diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index 0a57d0f..6c5923e 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -76,7 +76,7 @@ Contributing to the Elasticsearch codebase
 
 **Repository:** [https://github.com/elasticsearch/elasticsearch](https://github.com/elastic/elasticsearch)
 
-Make sure you have [Maven](http://maven.apache.org) installed, as Elasticsearch uses it as its build system. Integration with IntelliJ and Eclipse should work out of the box. Eclipse users can automatically configure their IDE by running `mvn eclipse:eclipse` and then importing the project into their workspace: `File > Import > Existing project into workspace`.
+Make sure you have [Maven](http://maven.apache.org) installed, as Elasticsearch uses it as its build system. Integration with IntelliJ and Eclipse should work out of the box. Eclipse users can automatically configure their IDE by running `mvn eclipse:eclipse` and then importing the project into their workspace: `File > Import > Existing project into workspace` and make sure to select `Search for nested projects...` option as Elasticsearch is a multi-module maven project. Additionally you will want to ensure that Eclipse is using 2048m of heap by modifying `eclipse.ini` accordingly to avoid GC overhead errors. Please make sure the [m2e-connector](http://marketplace.eclipse.org/content/m2e-connector-maven-dependency-plugin) is not installed in your Eclipse distribution as it will interfere with setup performed by `mvn eclipse:eclipse`.
 
 Elasticsearch also works perfectly with Eclipse's [m2e](http://www.eclipse.org/m2e/).  Once you've installed m2e you can import Elasticsearch as an `Existing Maven Project`.
 
diff --git a/TESTING.asciidoc b/TESTING.asciidoc
index 3b45c4e..7a717c6 100644
--- a/TESTING.asciidoc
+++ b/TESTING.asciidoc
@@ -445,12 +445,12 @@ These are the linux flavors the Vagrantfile currently supports:
 * centos-7
 * fedora-22
 * oel-7 aka Oracle Enterprise Linux 7
+* sles-12
 
 We're missing the following from the support matrix because there aren't high
 quality boxes available in vagrant atlas:
 
 * sles-11
-* sles-12
 * opensuse-13
 * oel-6
 
diff --git a/Vagrantfile b/Vagrantfile
index 4c86d02..ab0e322 100644
--- a/Vagrantfile
+++ b/Vagrantfile
@@ -71,13 +71,22 @@ Vagrant.configure(2) do |config|
   config.vm.define "opensuse-13" do |config|
     config.vm.box = "chef/opensuse-13"
     config.vm.box_url = "http://opscode-vm-bento.s3.amazonaws.com/vagrant/virtualbox/opscode_opensuse-13.2-x86_64_chef-provisionerless.box"
-    suse_common config
+    opensuse_common config
+  end
+  # The SLES boxes are not considered to be highest quality, but seem to be sufficient for a test run
+  config.vm.define "sles-12" do |config|
+    config.vm.box = "idar/sles12"
+    sles_common config
   end
   # Switch the default share for the project root from /vagrant to
   # /elasticsearch because /vagrant is confusing when there is a project inside
   # the elasticsearch project called vagrant....
   config.vm.synced_folder ".", "/vagrant", disabled: true
   config.vm.synced_folder ".", "/elasticsearch"
+  config.vm.provider "virtualbox" do |v|
+    # Give the boxes 2GB so they can run our tests if they have to.
+    v.memory = 2048
+  end
   if Vagrant.has_plugin?("vagrant-cachier")
     config.cache.scope = :box
   end
@@ -127,9 +136,7 @@ def deb_common(config, add_openjdk_repository_command, openjdk_list)
       ls /etc/apt/sources.list.d/#{openjdk_list}.list > /dev/null 2>&1 ||
         (echo "Importing java-8 ppa" &&
           #{add_openjdk_repository_command} &&
-          apt-get update -o \
-            Dir::Etc::sourcelist="$(ls /etc/apt/sources.list.d/#{openjdk_list}.list)" \
-            -o Dir::Etc::sourceparts="-" -o APT::Get::List-Cleanup="0")
+          apt-get update)
 SHELL
   )
 end
@@ -155,12 +162,29 @@ def dnf_common(config)
   end
 end
 
-def suse_common(config)
+def opensuse_common(config)
+  suse_common config, ''
+end
+
+def suse_common(config, extra)
   provision(config,
     update_command: "zypper --non-interactive list-updates",
     update_tracking_file: "/var/cache/zypp/packages/last_update",
     install_command: "zypper --non-interactive --quiet install --no-recommends",
-    java_package: "java-1_8_0-openjdk-devel")
+    java_package: "java-1_8_0-openjdk-devel",
+    extra: extra)
+end
+
+def sles_common(config)
+  extra = <<-SHELL
+    zypper rr systemsmanagement_puppet
+    zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD1/ dvd1 || true
+    zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD2/ dvd2 || true
+    zypper addrepo http://download.opensuse.org/repositories/Java:Factory/SLE_12/Java:Factory.repo || true
+    zypper --no-gpg-checks --non-interactive refresh
+    zypper --non-interactive install git-core
+SHELL
+  suse_common config, extra
 end
 
 # Register the main box provisioning script.
diff --git a/core/pom.xml b/core/pom.xml
index 16f3c7f..3646db5 100644
--- a/core/pom.xml
+++ b/core/pom.xml
@@ -87,11 +87,6 @@
             <artifactId>lucene-spatial</artifactId>
         </dependency>
         <dependency>
-            <groupId>org.apache.lucene</groupId>
-            <artifactId>lucene-expressions</artifactId>
-            <optional>true</optional>
-        </dependency>
-        <dependency>
             <groupId>com.spatial4j</groupId>
             <artifactId>spatial4j</artifactId>
             <optional>true</optional>
diff --git a/core/src/main/java/org/elasticsearch/ElasticsearchException.java b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
index 5d35bce..c82cab9 100644
--- a/core/src/main/java/org/elasticsearch/ElasticsearchException.java
+++ b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
@@ -463,154 +463,151 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
         // change due to refactorings etc. like renaming we have to keep the ordinal <--> class mapping
         // to deserialize the exception coming from another node or from an corruption marker on
         // a corrupted index.
+        // NOTE: ONLY APPEND TO THE END and NEVER REMOVE EXCEPTIONS IN MINOR VERSIONS
         final Map<Class<? extends ElasticsearchException>, Integer> exceptions = new HashMap<>();
-        exceptions.put(org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException.class, 0);
-        exceptions.put(org.elasticsearch.search.dfs.DfsPhaseExecutionException.class, 1);
-        exceptions.put(org.elasticsearch.common.util.CancellableThreads.ExecutionCancelledException.class, 2);
-        exceptions.put(org.elasticsearch.discovery.MasterNotDiscoveredException.class, 3);
-        exceptions.put(org.elasticsearch.ElasticsearchSecurityException.class, 4);
-        exceptions.put(org.elasticsearch.index.snapshots.IndexShardRestoreException.class, 5);
-        exceptions.put(org.elasticsearch.indices.IndexClosedException.class, 6);
-        exceptions.put(org.elasticsearch.http.BindHttpException.class, 7);
-        exceptions.put(org.elasticsearch.action.search.ReduceSearchPhaseException.class, 8);
-        exceptions.put(org.elasticsearch.node.NodeClosedException.class, 9);
-        exceptions.put(org.elasticsearch.index.engine.SnapshotFailedEngineException.class, 10);
-        exceptions.put(org.elasticsearch.index.shard.ShardNotFoundException.class, 11);
-        exceptions.put(org.elasticsearch.transport.ConnectTransportException.class, 12);
-        exceptions.put(org.elasticsearch.transport.NotSerializableTransportException.class, 13);
-        exceptions.put(org.elasticsearch.transport.ResponseHandlerFailureTransportException.class, 14);
-        exceptions.put(org.elasticsearch.indices.IndexCreationException.class, 15);
-        exceptions.put(org.elasticsearch.index.IndexNotFoundException.class, 16);
-        exceptions.put(org.elasticsearch.cluster.routing.IllegalShardRoutingStateException.class, 17);
-        exceptions.put(org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException.class, 18);
-        exceptions.put(org.elasticsearch.ResourceNotFoundException.class, 19);
-        exceptions.put(org.elasticsearch.transport.ActionTransportException.class, 20);
-        exceptions.put(org.elasticsearch.ElasticsearchGenerationException.class, 21);
-        exceptions.put(org.elasticsearch.index.engine.CreateFailedEngineException.class, 22);
-        exceptions.put(org.elasticsearch.index.shard.IndexShardStartedException.class, 23);
-        exceptions.put(org.elasticsearch.search.SearchContextMissingException.class, 24);
-        exceptions.put(org.elasticsearch.script.ScriptException.class, 25);
-        exceptions.put(org.elasticsearch.index.shard.TranslogRecoveryPerformer.BatchOperationException.class, 26);
-        exceptions.put(org.elasticsearch.snapshots.SnapshotCreationException.class, 27);
-        exceptions.put(org.elasticsearch.index.engine.DeleteFailedEngineException.class, 28);
-        exceptions.put(org.elasticsearch.index.engine.DocumentMissingException.class, 29);
-        exceptions.put(org.elasticsearch.snapshots.SnapshotException.class, 30);
-        exceptions.put(org.elasticsearch.indices.InvalidAliasNameException.class, 31);
-        exceptions.put(org.elasticsearch.indices.InvalidIndexNameException.class, 32);
-        exceptions.put(org.elasticsearch.indices.IndexPrimaryShardNotAllocatedException.class, 33);
-        exceptions.put(org.elasticsearch.transport.TransportException.class, 34);
-        exceptions.put(org.elasticsearch.ElasticsearchParseException.class, 35);
-        exceptions.put(org.elasticsearch.search.SearchException.class, 36);
-        exceptions.put(org.elasticsearch.index.mapper.MapperException.class, 37);
-        exceptions.put(org.elasticsearch.indices.InvalidTypeNameException.class, 38);
-        exceptions.put(org.elasticsearch.snapshots.SnapshotRestoreException.class, 39);
-        exceptions.put(org.elasticsearch.common.ParsingException.class, 40);
-        exceptions.put(org.elasticsearch.index.shard.IndexShardClosedException.class, 41);
-        exceptions.put(org.elasticsearch.script.expression.ExpressionScriptCompilationException.class, 42);
-        exceptions.put(org.elasticsearch.indices.recovery.RecoverFilesRecoveryException.class, 43);
-        exceptions.put(org.elasticsearch.index.translog.TruncatedTranslogException.class, 44);
-        exceptions.put(org.elasticsearch.indices.recovery.RecoveryFailedException.class, 45);
-        exceptions.put(org.elasticsearch.index.shard.IndexShardRelocatedException.class, 46);
-        exceptions.put(org.elasticsearch.transport.NodeShouldNotConnectException.class, 47);
-        exceptions.put(org.elasticsearch.indices.IndexTemplateAlreadyExistsException.class, 48);
-        exceptions.put(org.elasticsearch.index.translog.TranslogCorruptedException.class, 49);
-        exceptions.put(org.elasticsearch.cluster.block.ClusterBlockException.class, 50);
-        exceptions.put(org.elasticsearch.search.fetch.FetchPhaseExecutionException.class, 51);
-        exceptions.put(org.elasticsearch.index.IndexShardAlreadyExistsException.class, 52);
-        exceptions.put(org.elasticsearch.index.engine.VersionConflictEngineException.class, 53);
-        exceptions.put(org.elasticsearch.index.engine.EngineException.class, 54);
-        exceptions.put(org.elasticsearch.index.engine.DocumentAlreadyExistsException.class, 55);
-        exceptions.put(org.elasticsearch.action.NoSuchNodeException.class, 56);
-        exceptions.put(org.elasticsearch.common.settings.SettingsException.class, 57);
-        exceptions.put(org.elasticsearch.indices.IndexTemplateMissingException.class, 58);
-        exceptions.put(org.elasticsearch.transport.SendRequestTransportException.class, 59);
-        exceptions.put(org.elasticsearch.common.util.concurrent.EsRejectedExecutionException.class, 60);
-        exceptions.put(org.elasticsearch.common.lucene.Lucene.EarlyTerminationException.class, 61);
-        exceptions.put(org.elasticsearch.cluster.routing.RoutingValidationException.class, 62);
-        exceptions.put(org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper.class, 63);
-        exceptions.put(org.elasticsearch.indices.AliasFilterParsingException.class, 64);
-        exceptions.put(org.elasticsearch.index.engine.DeleteByQueryFailedEngineException.class, 65);
-        exceptions.put(org.elasticsearch.gateway.GatewayException.class, 66);
-        exceptions.put(org.elasticsearch.index.shard.IndexShardNotRecoveringException.class, 67);
-        exceptions.put(org.elasticsearch.http.HttpException.class, 68);
-        exceptions.put(org.elasticsearch.ElasticsearchException.class, 69);
-        exceptions.put(org.elasticsearch.snapshots.SnapshotMissingException.class, 70);
-        exceptions.put(org.elasticsearch.action.PrimaryMissingActionException.class, 71);
-        exceptions.put(org.elasticsearch.action.FailedNodeException.class, 72);
-        exceptions.put(org.elasticsearch.search.SearchParseException.class, 73);
-        exceptions.put(org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException.class, 74);
-        exceptions.put(org.elasticsearch.common.blobstore.BlobStoreException.class, 75);
-        exceptions.put(org.elasticsearch.cluster.IncompatibleClusterStateVersionException.class, 76);
-        exceptions.put(org.elasticsearch.index.engine.RecoveryEngineException.class, 77);
-        exceptions.put(org.elasticsearch.common.util.concurrent.UncategorizedExecutionException.class, 78);
-        exceptions.put(org.elasticsearch.action.TimestampParsingException.class, 79);
-        exceptions.put(org.elasticsearch.action.RoutingMissingException.class, 80);
-        exceptions.put(org.elasticsearch.index.engine.IndexFailedEngineException.class, 81);
-        exceptions.put(org.elasticsearch.index.snapshots.IndexShardRestoreFailedException.class, 82);
-        exceptions.put(org.elasticsearch.repositories.RepositoryException.class, 83);
-        exceptions.put(org.elasticsearch.transport.ReceiveTimeoutTransportException.class, 84);
-        exceptions.put(org.elasticsearch.transport.NodeDisconnectedException.class, 85);
-        exceptions.put(org.elasticsearch.index.AlreadyExpiredException.class, 86);
-        exceptions.put(org.elasticsearch.search.aggregations.AggregationExecutionException.class, 87);
-        exceptions.put(org.elasticsearch.index.mapper.MergeMappingException.class, 88);
-        exceptions.put(org.elasticsearch.indices.InvalidIndexTemplateException.class, 89);
-        exceptions.put(org.elasticsearch.percolator.PercolateException.class, 90);
-        exceptions.put(org.elasticsearch.index.engine.RefreshFailedEngineException.class, 91);
-        exceptions.put(org.elasticsearch.search.aggregations.AggregationInitializationException.class, 92);
-        exceptions.put(org.elasticsearch.indices.recovery.DelayRecoveryException.class, 93);
-        exceptions.put(org.elasticsearch.search.warmer.IndexWarmerMissingException.class, 94);
-        exceptions.put(org.elasticsearch.client.transport.NoNodeAvailableException.class, 95);
-        exceptions.put(org.elasticsearch.script.groovy.GroovyScriptCompilationException.class, 96);
-        exceptions.put(org.elasticsearch.snapshots.InvalidSnapshotNameException.class, 97);
-        exceptions.put(org.elasticsearch.index.shard.IllegalIndexShardStateException.class, 98);
-        exceptions.put(org.elasticsearch.index.snapshots.IndexShardSnapshotException.class, 99);
-        exceptions.put(org.elasticsearch.index.shard.IndexShardNotStartedException.class, 100);
-        exceptions.put(org.elasticsearch.action.search.SearchPhaseExecutionException.class, 101);
-        exceptions.put(org.elasticsearch.transport.ActionNotFoundTransportException.class, 102);
-        exceptions.put(org.elasticsearch.transport.TransportSerializationException.class, 103);
-        exceptions.put(org.elasticsearch.transport.RemoteTransportException.class, 104);
-        exceptions.put(org.elasticsearch.index.engine.EngineCreationFailureException.class, 105);
-        exceptions.put(org.elasticsearch.cluster.routing.RoutingException.class, 106);
-        exceptions.put(org.elasticsearch.index.shard.IndexShardRecoveryException.class, 107);
-        exceptions.put(org.elasticsearch.repositories.RepositoryMissingException.class, 108);
-        exceptions.put(org.elasticsearch.script.expression.ExpressionScriptExecutionException.class, 109);
-        exceptions.put(org.elasticsearch.index.percolator.PercolatorException.class, 110);
-        exceptions.put(org.elasticsearch.index.engine.DocumentSourceMissingException.class, 111);
-        exceptions.put(org.elasticsearch.index.engine.FlushNotAllowedEngineException.class, 112);
-        exceptions.put(org.elasticsearch.common.settings.NoClassSettingsException.class, 113);
-        exceptions.put(org.elasticsearch.transport.BindTransportException.class, 114);
-        exceptions.put(org.elasticsearch.rest.action.admin.indices.alias.delete.AliasesNotFoundException.class, 115);
-        exceptions.put(org.elasticsearch.index.shard.IndexShardRecoveringException.class, 116);
-        exceptions.put(org.elasticsearch.index.translog.TranslogException.class, 117);
-        exceptions.put(org.elasticsearch.cluster.metadata.ProcessClusterEventTimeoutException.class, 118);
-        exceptions.put(org.elasticsearch.action.support.replication.TransportReplicationAction.RetryOnPrimaryException.class, 119);
-        exceptions.put(org.elasticsearch.ElasticsearchTimeoutException.class, 120);
-        exceptions.put(org.elasticsearch.search.query.QueryPhaseExecutionException.class, 121);
-        exceptions.put(org.elasticsearch.repositories.RepositoryVerificationException.class, 122);
-        exceptions.put(org.elasticsearch.search.aggregations.InvalidAggregationPathException.class, 123);
-        exceptions.put(org.elasticsearch.script.groovy.GroovyScriptExecutionException.class, 124);
-        exceptions.put(org.elasticsearch.indices.IndexAlreadyExistsException.class, 125);
-        exceptions.put(org.elasticsearch.script.Script.ScriptParseException.class, 126);
-        exceptions.put(org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.HttpOnTransportException.class, 127);
-        exceptions.put(org.elasticsearch.index.mapper.MapperParsingException.class, 128);
-        exceptions.put(org.elasticsearch.search.SearchContextException.class, 129);
-        exceptions.put(org.elasticsearch.search.builder.SearchSourceBuilderException.class, 130);
-        exceptions.put(org.elasticsearch.index.engine.EngineClosedException.class, 131);
-        exceptions.put(org.elasticsearch.action.NoShardAvailableActionException.class, 132);
-        exceptions.put(org.elasticsearch.action.UnavailableShardsException.class, 133);
-        exceptions.put(org.elasticsearch.index.engine.FlushFailedEngineException.class, 134);
-        exceptions.put(org.elasticsearch.common.breaker.CircuitBreakingException.class, 135);
-        exceptions.put(org.elasticsearch.transport.NodeNotConnectedException.class, 136);
-        exceptions.put(org.elasticsearch.index.mapper.StrictDynamicMappingException.class, 137);
-        exceptions.put(org.elasticsearch.action.support.replication.TransportReplicationAction.RetryOnReplicaException.class, 138);
-        exceptions.put(org.elasticsearch.indices.TypeMissingException.class, 139);
+        exceptions.put(org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.search.dfs.DfsPhaseExecutionException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.common.util.CancellableThreads.ExecutionCancelledException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.discovery.MasterNotDiscoveredException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.ElasticsearchSecurityException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.snapshots.IndexShardRestoreException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.indices.IndexClosedException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.http.BindHttpException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.action.search.ReduceSearchPhaseException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.node.NodeClosedException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.engine.SnapshotFailedEngineException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.shard.ShardNotFoundException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.transport.ConnectTransportException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.transport.NotSerializableTransportException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.transport.ResponseHandlerFailureTransportException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.indices.IndexCreationException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.IndexNotFoundException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.cluster.routing.IllegalShardRoutingStateException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.ResourceNotFoundException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.transport.ActionTransportException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.ElasticsearchGenerationException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.engine.CreateFailedEngineException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.shard.IndexShardStartedException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.search.SearchContextMissingException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.script.ScriptException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.shard.TranslogRecoveryPerformer.BatchOperationException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.snapshots.SnapshotCreationException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.engine.DeleteFailedEngineException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.engine.DocumentMissingException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.snapshots.SnapshotException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.indices.InvalidAliasNameException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.indices.InvalidIndexNameException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.indices.IndexPrimaryShardNotAllocatedException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.transport.TransportException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.ElasticsearchParseException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.search.SearchException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.mapper.MapperException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.indices.InvalidTypeNameException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.snapshots.SnapshotRestoreException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.common.ParsingException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.shard.IndexShardClosedException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.indices.recovery.RecoverFilesRecoveryException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.translog.TruncatedTranslogException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.indices.recovery.RecoveryFailedException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.shard.IndexShardRelocatedException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.transport.NodeShouldNotConnectException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.indices.IndexTemplateAlreadyExistsException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.translog.TranslogCorruptedException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.cluster.block.ClusterBlockException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.search.fetch.FetchPhaseExecutionException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.IndexShardAlreadyExistsException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.engine.VersionConflictEngineException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.engine.EngineException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.engine.DocumentAlreadyExistsException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.action.NoSuchNodeException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.common.settings.SettingsException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.indices.IndexTemplateMissingException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.transport.SendRequestTransportException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.common.util.concurrent.EsRejectedExecutionException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.common.lucene.Lucene.EarlyTerminationException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.cluster.routing.RoutingValidationException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper.class, exceptions.size());
+        exceptions.put(org.elasticsearch.indices.AliasFilterParsingException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.engine.DeleteByQueryFailedEngineException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.gateway.GatewayException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.shard.IndexShardNotRecoveringException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.http.HttpException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.ElasticsearchException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.snapshots.SnapshotMissingException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.action.PrimaryMissingActionException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.action.FailedNodeException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.search.SearchParseException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.common.blobstore.BlobStoreException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.cluster.IncompatibleClusterStateVersionException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.engine.RecoveryEngineException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.common.util.concurrent.UncategorizedExecutionException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.action.TimestampParsingException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.action.RoutingMissingException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.engine.IndexFailedEngineException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.snapshots.IndexShardRestoreFailedException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.repositories.RepositoryException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.transport.ReceiveTimeoutTransportException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.transport.NodeDisconnectedException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.AlreadyExpiredException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.search.aggregations.AggregationExecutionException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.mapper.MergeMappingException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.indices.InvalidIndexTemplateException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.percolator.PercolateException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.engine.RefreshFailedEngineException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.search.aggregations.AggregationInitializationException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.indices.recovery.DelayRecoveryException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.search.warmer.IndexWarmerMissingException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.client.transport.NoNodeAvailableException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.script.groovy.GroovyScriptCompilationException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.snapshots.InvalidSnapshotNameException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.shard.IllegalIndexShardStateException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.snapshots.IndexShardSnapshotException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.shard.IndexShardNotStartedException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.action.search.SearchPhaseExecutionException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.transport.ActionNotFoundTransportException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.transport.TransportSerializationException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.transport.RemoteTransportException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.engine.EngineCreationFailureException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.cluster.routing.RoutingException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.shard.IndexShardRecoveryException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.repositories.RepositoryMissingException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.percolator.PercolatorException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.engine.DocumentSourceMissingException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.engine.FlushNotAllowedEngineException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.common.settings.NoClassSettingsException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.transport.BindTransportException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.rest.action.admin.indices.alias.delete.AliasesNotFoundException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.shard.IndexShardRecoveringException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.translog.TranslogException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.cluster.metadata.ProcessClusterEventTimeoutException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.action.support.replication.TransportReplicationAction.RetryOnPrimaryException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.ElasticsearchTimeoutException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.search.query.QueryPhaseExecutionException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.repositories.RepositoryVerificationException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.search.aggregations.InvalidAggregationPathException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.script.groovy.GroovyScriptExecutionException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.indices.IndexAlreadyExistsException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.script.Script.ScriptParseException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.HttpOnTransportException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.mapper.MapperParsingException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.search.SearchContextException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.search.builder.SearchSourceBuilderException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.engine.EngineClosedException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.action.NoShardAvailableActionException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.action.UnavailableShardsException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.engine.FlushFailedEngineException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.common.breaker.CircuitBreakingException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.transport.NodeNotConnectedException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.mapper.StrictDynamicMappingException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.action.support.replication.TransportReplicationAction.RetryOnReplicaException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.indices.TypeMissingException.class, exceptions.size());
         // added in 3.x
-        exceptions.put(org.elasticsearch.discovery.Discovery.FailedToCommitClusterStateException.class, 140);
-        exceptions.put(org.elasticsearch.index.query.QueryShardException.class, 141);
-
-        final int maxOrd = 141;
-        assert exceptions.size() == maxOrd + 1;
-        Constructor<? extends ElasticsearchException>[] idToSupplier = new Constructor[maxOrd + 1];
+        exceptions.put(org.elasticsearch.discovery.Discovery.FailedToCommitClusterStateException.class, exceptions.size());
+        exceptions.put(org.elasticsearch.index.query.QueryShardException.class, exceptions.size());
+        // NOTE: ONLY APPEND TO THE END and NEVER REMOVE EXCEPTIONS IN MINOR VERSIONS
+        Constructor<? extends ElasticsearchException>[] idToSupplier = new Constructor[exceptions.size()];
         for (Map.Entry<Class<? extends ElasticsearchException>, Integer> e : exceptions.entrySet()) {
             try {
                 Constructor<? extends ElasticsearchException> constructor = e.getKey().getDeclaredConstructor(StreamInput.class);
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java
index f291bd6..cb7b240 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java
@@ -73,7 +73,7 @@ public class TransportClusterHealthAction extends TransportMasterNodeReadAction<
     protected void masterOperation(final ClusterHealthRequest request, final ClusterState unusedState, final ActionListener<ClusterHealthResponse> listener) {
         if (request.waitForEvents() != null) {
             final long endTimeMS = TimeValue.nsecToMSec(System.nanoTime()) + request.timeout().millis();
-            clusterService.submitStateUpdateTask("cluster_health (wait_for_events [" + request.waitForEvents() + "])", request.waitForEvents(), new ProcessedClusterStateUpdateTask() {
+            clusterService.submitStateUpdateTask("cluster_health (wait_for_events [" + request.waitForEvents() + "])", request.waitForEvents(), new ClusterStateUpdateTask() {
                 @Override
                 public ClusterState execute(ClusterState currentState) {
                     return currentState;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java
index 5ac72d7..908a25a 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java
@@ -42,9 +42,8 @@ import java.util.Map;
  * Node information (static, does not change over time).
  */
 public class NodeInfo extends BaseNodeResponse {
-
     @Nullable
-    private ImmutableMap<String, String> serviceAttributes;
+    private Map<String, String> serviceAttributes;
 
     private Version version;
     private Build build;
@@ -119,7 +118,7 @@ public class NodeInfo extends BaseNodeResponse {
      * The service attributes of the node.
      */
     @Nullable
-    public ImmutableMap<String, String> getServiceAttributes() {
+    public Map<String, String> getServiceAttributes() {
         return this.serviceAttributes;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java
index 5de9603..2ecd4fb 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.action.admin.cluster.snapshots.status;
 
 import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.ActionRequest;
 import org.elasticsearch.action.FailedNodeException;
@@ -46,10 +47,13 @@ import org.elasticsearch.transport.TransportService;
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicReferenceArray;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * Transport client that collects snapshot shard statuses from data nodes
  */
@@ -104,7 +108,7 @@ public class TransportNodesSnapshotsStatus extends TransportNodesAction<Transpor
 
     @Override
     protected NodeSnapshotStatus nodeOperation(NodeRequest request) {
-        ImmutableMap.Builder<SnapshotId, ImmutableMap<ShardId, SnapshotIndexShardStatus>> snapshotMapBuilder = ImmutableMap.builder();
+        Map<SnapshotId, Map<ShardId, SnapshotIndexShardStatus>> snapshotMapBuilder = new HashMap<>();
         try {
             String nodeId = clusterService.localNode().id();
             for (SnapshotId snapshotId : request.snapshotIds) {
@@ -112,7 +116,7 @@ public class TransportNodesSnapshotsStatus extends TransportNodesAction<Transpor
                 if (shardsStatus == null) {
                     continue;
                 }
-                ImmutableMap.Builder<ShardId, SnapshotIndexShardStatus> shardMapBuilder = ImmutableMap.builder();
+                Map<ShardId, SnapshotIndexShardStatus> shardMapBuilder = new HashMap<>();
                 for (Map.Entry<ShardId, IndexShardSnapshotStatus> shardEntry : shardsStatus.entrySet()) {
                     SnapshotIndexShardStatus shardStatus;
                     IndexShardSnapshotStatus.Stage stage = shardEntry.getValue().stage();
@@ -124,9 +128,9 @@ public class TransportNodesSnapshotsStatus extends TransportNodesAction<Transpor
                     }
                     shardMapBuilder.put(shardEntry.getKey(), shardStatus);
                 }
-                snapshotMapBuilder.put(snapshotId, shardMapBuilder.build());
+                snapshotMapBuilder.put(snapshotId, unmodifiableMap(shardMapBuilder));
             }
-            return new NodeSnapshotStatus(clusterService.localNode(), snapshotMapBuilder.build());
+            return new NodeSnapshotStatus(clusterService.localNode(), unmodifiableMap(snapshotMapBuilder));
         } catch (Exception e) {
             throw new ElasticsearchException("failed to load metadata", e);
         }
@@ -241,17 +245,17 @@ public class TransportNodesSnapshotsStatus extends TransportNodesAction<Transpor
 
     public static class NodeSnapshotStatus extends BaseNodeResponse {
 
-        private ImmutableMap<SnapshotId, ImmutableMap<ShardId, SnapshotIndexShardStatus>> status;
+        private Map<SnapshotId, Map<ShardId, SnapshotIndexShardStatus>> status;
 
         NodeSnapshotStatus() {
         }
 
-        public NodeSnapshotStatus(DiscoveryNode node, ImmutableMap<SnapshotId, ImmutableMap<ShardId, SnapshotIndexShardStatus>> status) {
+        public NodeSnapshotStatus(DiscoveryNode node, Map<SnapshotId, Map<ShardId, SnapshotIndexShardStatus>> status) {
             super(node);
             this.status = status;
         }
 
-        public ImmutableMap<SnapshotId, ImmutableMap<ShardId, SnapshotIndexShardStatus>> status() {
+        public Map<SnapshotId, Map<ShardId, SnapshotIndexShardStatus>> status() {
             return status;
         }
 
@@ -259,19 +263,19 @@ public class TransportNodesSnapshotsStatus extends TransportNodesAction<Transpor
         public void readFrom(StreamInput in) throws IOException {
             super.readFrom(in);
             int numberOfSnapshots = in.readVInt();
-            ImmutableMap.Builder<SnapshotId, ImmutableMap<ShardId, SnapshotIndexShardStatus>> snapshotMapBuilder = ImmutableMap.builder();
+            Map<SnapshotId, Map<ShardId, SnapshotIndexShardStatus>> snapshotMapBuilder = new HashMap<>(numberOfSnapshots);
             for (int i = 0; i < numberOfSnapshots; i++) {
                 SnapshotId snapshotId = SnapshotId.readSnapshotId(in);
-                ImmutableMap.Builder<ShardId, SnapshotIndexShardStatus> shardMapBuilder = ImmutableMap.builder();
                 int numberOfShards = in.readVInt();
+                Map<ShardId, SnapshotIndexShardStatus> shardMapBuilder = new HashMap<>(numberOfShards);
                 for (int j = 0; j < numberOfShards; j++) {
                     ShardId shardId =  ShardId.readShardId(in);
                     SnapshotIndexShardStatus status = SnapshotIndexShardStatus.readShardSnapshotStatus(in);
                     shardMapBuilder.put(shardId, status);
                 }
-                snapshotMapBuilder.put(snapshotId, shardMapBuilder.build());
+                snapshotMapBuilder.put(snapshotId, unmodifiableMap(shardMapBuilder));
             }
-            status = snapshotMapBuilder.build();
+            status = unmodifiableMap(snapshotMapBuilder);
         }
 
         @Override
@@ -279,10 +283,10 @@ public class TransportNodesSnapshotsStatus extends TransportNodesAction<Transpor
             super.writeTo(out);
             if (status != null) {
                 out.writeVInt(status.size());
-                for (ImmutableMap.Entry<SnapshotId, ImmutableMap<ShardId, SnapshotIndexShardStatus>> entry : status.entrySet()) {
+                for (Map.Entry<SnapshotId, Map<ShardId, SnapshotIndexShardStatus>> entry : status.entrySet()) {
                     entry.getKey().writeTo(out);
                     out.writeVInt(entry.getValue().size());
-                    for (ImmutableMap.Entry<ShardId, SnapshotIndexShardStatus> shardEntry : entry.getValue().entrySet()) {
+                    for (Map.Entry<ShardId, SnapshotIndexShardStatus> shardEntry : entry.getValue().entrySet()) {
                         shardEntry.getKey().writeTo(out);
                         shardEntry.getValue().writeTo(out);
                     }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java
index fb44483..5af92dc 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java
@@ -157,7 +157,7 @@ public class TransportSnapshotsStatusAction extends TransportMasterNodeAction<Sn
                         // We should have information about this shard from the shard:
                         TransportNodesSnapshotsStatus.NodeSnapshotStatus nodeStatus = nodeSnapshotStatusMap.get(status.nodeId());
                         if (nodeStatus != null) {
-                            ImmutableMap<ShardId, SnapshotIndexShardStatus> shardStatues = nodeStatus.status().get(entry.snapshotId());
+                            Map<ShardId, SnapshotIndexShardStatus> shardStatues = nodeStatus.status().get(entry.snapshotId());
                             if (shardStatues != null) {
                                 SnapshotIndexShardStatus shardStatus = shardStatues.get(shardEntry.getKey());
                                 if (shardStatus != null) {
@@ -204,7 +204,7 @@ public class TransportSnapshotsStatusAction extends TransportMasterNodeAction<Sn
                     Snapshot snapshot = snapshotsService.snapshot(snapshotId);
                     List<SnapshotIndexShardStatus> shardStatusBuilder = new ArrayList<>();
                     if (snapshot.state().completed()) {
-                        ImmutableMap<ShardId, IndexShardSnapshotStatus> shardStatues = snapshotsService.snapshotShards(snapshotId);
+                        Map<ShardId, IndexShardSnapshotStatus> shardStatues = snapshotsService.snapshotShards(snapshotId);
                         for (ImmutableMap.Entry<ShardId, IndexShardSnapshotStatus> shardStatus : shardStatues.entrySet()) {
                             shardStatusBuilder.add(new SnapshotIndexShardStatus(shardStatus.getKey(), shardStatus.getValue()));
                         }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java
index 6eac403..0930f8f 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java
@@ -122,7 +122,7 @@ public class GetIndexResponse extends ActionResponse {
                         in.readString(),
                         in.readStringArray(),
                         in.readOptionalBoolean(),
-                        in.readBoolean() ? new IndexWarmersMetaData.SearchSource(in) : null)
+                        in.readBytesReference())
                 );
             }
             warmersMapBuilder.put(key, Collections.unmodifiableList(warmerEntryBuilder));
@@ -173,11 +173,7 @@ public class GetIndexResponse extends ActionResponse {
                 out.writeString(warmerEntry.name());
                 out.writeStringArray(warmerEntry.types());
                 out.writeOptionalBoolean(warmerEntry.requestCache());
-                boolean hasSource = warmerEntry.source() != null;
-                out.writeBoolean(hasSource);
-                if (hasSource) {
-                    warmerEntry.source().writeTo(out);
-                }
+                out.writeBytesReference(warmerEntry.source());
             }
         }
         out.writeVInt(mappings.size());
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java
index b92e60d..24820ba 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.action.admin.indices.mapping.get;
 
 import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
@@ -31,14 +32,17 @@ import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.index.mapper.Mapper;
 
 import java.io.IOException;
+import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /** Response object for {@link GetFieldMappingsRequest} API */
 public class GetFieldMappingsResponse extends ActionResponse implements ToXContent {
 
-    private ImmutableMap<String, ImmutableMap<String, ImmutableMap<String, FieldMappingMetaData>>> mappings = ImmutableMap.of();
+    private Map<String, Map<String, Map<String, FieldMappingMetaData>>> mappings = ImmutableMap.of();
 
-    GetFieldMappingsResponse(ImmutableMap<String, ImmutableMap<String, ImmutableMap<String, FieldMappingMetaData>>> mappings) {
+    GetFieldMappingsResponse(Map<String, Map<String, Map<String, FieldMappingMetaData>>> mappings) {
         this.mappings = mappings;
     }
 
@@ -46,7 +50,7 @@ public class GetFieldMappingsResponse extends ActionResponse implements ToXConte
     }
 
     /** returns the retrieved field mapping. The return map keys are index, type, field (as specified in the request). */
-    public ImmutableMap<String, ImmutableMap<String, ImmutableMap<String, FieldMappingMetaData>>> mappings() {
+    public Map<String, Map<String, Map<String, FieldMappingMetaData>>> mappings() {
         return mappings;
     }
 
@@ -57,11 +61,11 @@ public class GetFieldMappingsResponse extends ActionResponse implements ToXConte
      * @return FieldMappingMetaData for the requested field or null if not found.
      */
     public FieldMappingMetaData fieldMappings(String index, String type, String field) {
-        ImmutableMap<String, ImmutableMap<String, FieldMappingMetaData>> indexMapping = mappings.get(index);
+        Map<String, Map<String, FieldMappingMetaData>> indexMapping = mappings.get(index);
         if (indexMapping == null) {
             return null;
         }
-        ImmutableMap<String, FieldMappingMetaData> typeMapping = indexMapping.get(type);
+        Map<String, FieldMappingMetaData> typeMapping = indexMapping.get(type);
         if (typeMapping == null) {
             return null;
         }
@@ -70,10 +74,10 @@ public class GetFieldMappingsResponse extends ActionResponse implements ToXConte
 
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        for (Map.Entry<String, ImmutableMap<String, ImmutableMap<String, FieldMappingMetaData>>> indexEntry : mappings.entrySet()) {
+        for (Map.Entry<String, Map<String, Map<String, FieldMappingMetaData>>> indexEntry : mappings.entrySet()) {
             builder.startObject(indexEntry.getKey(), XContentBuilder.FieldCaseConversion.NONE);
             builder.startObject("mappings");
-            for (Map.Entry<String, ImmutableMap<String, FieldMappingMetaData>> typeEntry : indexEntry.getValue().entrySet()) {
+            for (Map.Entry<String, Map<String, FieldMappingMetaData>> typeEntry : indexEntry.getValue().entrySet()) {
                 builder.startObject(typeEntry.getKey(), XContentBuilder.FieldCaseConversion.NONE);
                 for (Map.Entry<String, FieldMappingMetaData> fieldEntry : typeEntry.getValue().entrySet()) {
                     builder.startObject(fieldEntry.getKey());
@@ -128,33 +132,33 @@ public class GetFieldMappingsResponse extends ActionResponse implements ToXConte
     public void readFrom(StreamInput in) throws IOException {
         super.readFrom(in);
         int size = in.readVInt();
-        ImmutableMap.Builder<String, ImmutableMap<String, ImmutableMap<String, FieldMappingMetaData>>> indexMapBuilder = ImmutableMap.builder();
+        Map<String, Map<String, Map<String, FieldMappingMetaData>>> indexMapBuilder = new HashMap<>(size);
         for (int i = 0; i < size; i++) {
             String index = in.readString();
             int typesSize = in.readVInt();
-            ImmutableMap.Builder<String, ImmutableMap<String, FieldMappingMetaData>> typeMapBuilder = ImmutableMap.builder();
+            Map<String, Map<String, FieldMappingMetaData>> typeMapBuilder = new HashMap<>(typesSize);
             for (int j = 0; j < typesSize; j++) {
                 String type = in.readString();
-                ImmutableMap.Builder<String, FieldMappingMetaData> fieldMapBuilder = ImmutableMap.builder();
                 int fieldSize = in.readVInt();
+                Map<String, FieldMappingMetaData> fieldMapBuilder = new HashMap<>(fieldSize);
                 for (int k = 0; k < fieldSize; k++) {
                     fieldMapBuilder.put(in.readString(), new FieldMappingMetaData(in.readString(), in.readBytesReference()));
                 }
-                typeMapBuilder.put(type, fieldMapBuilder.build());
+                typeMapBuilder.put(type, unmodifiableMap(fieldMapBuilder));
             }
-            indexMapBuilder.put(index, typeMapBuilder.build());
+            indexMapBuilder.put(index, unmodifiableMap(typeMapBuilder));
         }
-        mappings = indexMapBuilder.build();
+        mappings = unmodifiableMap(indexMapBuilder);
     }
 
     @Override
     public void writeTo(StreamOutput out) throws IOException {
         super.writeTo(out);
         out.writeVInt(mappings.size());
-        for (Map.Entry<String, ImmutableMap<String, ImmutableMap<String, FieldMappingMetaData>>> indexEntry : mappings.entrySet()) {
+        for (Map.Entry<String, Map<String, Map<String, FieldMappingMetaData>>> indexEntry : mappings.entrySet()) {
             out.writeString(indexEntry.getKey());
             out.writeVInt(indexEntry.getValue().size());
-            for (Map.Entry<String, ImmutableMap<String, FieldMappingMetaData>> typeEntry : indexEntry.getValue().entrySet()) {
+            for (Map.Entry<String, Map<String, FieldMappingMetaData>> typeEntry : indexEntry.getValue().entrySet()) {
                 out.writeString(typeEntry.getKey());
                 out.writeVInt(typeEntry.getValue().size());
                 for (Map.Entry<String, FieldMappingMetaData> fieldEntry : typeEntry.getValue().entrySet()) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java
index 19f1ec3..060d94a 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.action.admin.indices.mapping.get;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.HandledTransportAction;
@@ -32,6 +31,7 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
+import java.util.Map;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReferenceArray;
 
@@ -88,7 +88,7 @@ public class TransportGetFieldMappingsAction extends HandledTransportAction<GetF
     }
 
     private GetFieldMappingsResponse merge(AtomicReferenceArray<Object> indexResponses) {
-        MapBuilder<String, ImmutableMap<String, ImmutableMap<String, GetFieldMappingsResponse.FieldMappingMetaData>>> mergedResponses = MapBuilder.newMapBuilder();
+        MapBuilder<String, Map<String, Map<String, GetFieldMappingsResponse.FieldMappingMetaData>>> mergedResponses = MapBuilder.newMapBuilder();
         for (int i = 0; i < indexResponses.length(); i++) {
             Object element = indexResponses.get(i);
             if (element instanceof GetFieldMappingsResponse) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java
index 62183a0..a7b780a 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.action.admin.indices.mapping.get;
 
 import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.admin.indices.mapping.get.GetFieldMappingsResponse.FieldMappingMetaData;
 import org.elasticsearch.action.support.ActionFilters;
@@ -52,6 +53,7 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Iterator;
+import java.util.Map;
 import java.util.stream.Collectors;
 
 import static org.elasticsearch.common.util.CollectionUtils.newLinkedList;
@@ -105,7 +107,7 @@ public class TransportGetFieldMappingsIndexAction extends TransportSingleShardAc
             }
         }
 
-        MapBuilder<String, ImmutableMap<String, FieldMappingMetaData>> typeMappings = new MapBuilder<>();
+        MapBuilder<String, Map<String, FieldMappingMetaData>> typeMappings = new MapBuilder<>();
         for (String type : typeIntersection) {
             DocumentMapper documentMapper = indexService.mapperService().documentMapper(type);
             ImmutableMap<String, FieldMappingMetaData> fieldMapping = findFieldMappingsByType(documentMapper, request);
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java
index 4719cb8..3ed444c 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java
@@ -20,12 +20,12 @@
 package org.elasticsearch.action.admin.indices.warmer.get;
 
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
+import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionResponse;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 
 import java.io.IOException;
@@ -69,7 +69,7 @@ public class GetWarmersResponse extends ActionResponse {
             for (int j = 0; j < valueSize; j++) {
                 String name = in.readString();
                 String[] types = in.readStringArray();
-                IndexWarmersMetaData.SearchSource source = new IndexWarmersMetaData.SearchSource(in);
+                BytesReference source = in.readBytesReference();
                 Boolean queryCache = null;
                 queryCache = in.readOptionalBoolean();
                 warmerEntryBuilder.add(new IndexWarmersMetaData.Entry(
@@ -94,7 +94,7 @@ public class GetWarmersResponse extends ActionResponse {
             for (IndexWarmersMetaData.Entry warmerEntry : indexEntry.value) {
                 out.writeString(warmerEntry.name());
                 out.writeStringArray(warmerEntry.types());
-                warmerEntry.source().writeTo(out);
+                out.writeBytesReference(warmerEntry.source());
                 out.writeOptionalBoolean(warmerEntry.requestCache());
             }
         }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java
index d72be81..18246f6 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java
@@ -38,7 +38,6 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.IndexNotFoundException;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
@@ -115,9 +114,11 @@ public class TransportPutWarmerAction extends TransportMasterNodeAction<PutWarme
                         MetaData metaData = currentState.metaData();
                         String[] concreteIndices = indexNameExpressionResolver.concreteIndices(currentState, request.searchRequest().indicesOptions(), request.searchRequest().indices());
 
-                        IndexWarmersMetaData.SearchSource source = null;
-                        if (request.searchRequest().source() != null) {
-                            source = new IndexWarmersMetaData.SearchSource(request.searchRequest().source());
+                        BytesReference source = null;
+                        if (request.searchRequest().source() != null && request.searchRequest().source().length() > 0) {
+                            source = request.searchRequest().source();
+                        } else if (request.searchRequest().extraSource() != null && request.searchRequest().extraSource().length() > 0) {
+                            source = request.searchRequest().extraSource();
                         }
 
                         // now replace it on the metadata
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java b/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
index 9e3af1f..eae25b7 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
@@ -304,7 +304,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
             assert preVersionTypes[requestIndex] != null;
         }
 
-        processAfter(request, indexShard, location);
+        processAfter(request.refresh(), indexShard, location);
         BulkItemResponse[] responses = new BulkItemResponse[request.items().length];
         BulkItemRequest[] items = request.items();
         for (int i = 0; i < items.length; i++) {
@@ -500,21 +500,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
             }
         }
 
-       processAfter(request, indexShard, location);
-    }
-
-    private void processAfter(BulkShardRequest request, IndexShard indexShard, Translog.Location location) {
-        if (request.refresh()) {
-            try {
-                indexShard.refresh("refresh_flag_bulk");
-            } catch (Throwable e) {
-                // ignore
-            }
-        }
-
-        if (indexShard.getTranslogDurability() == Translog.Durabilty.REQUEST && location != null) {
-            indexShard.sync(location);
-        }
+       processAfter(request.refresh(), indexShard, location);
     }
 
     private void applyVersion(BulkItemRequest item, long version, VersionType versionType) {
diff --git a/core/src/main/java/org/elasticsearch/action/count/CountRequest.java b/core/src/main/java/org/elasticsearch/action/count/CountRequest.java
index 80c03e6..05e193a 100644
--- a/core/src/main/java/org/elasticsearch/action/count/CountRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/count/CountRequest.java
@@ -33,7 +33,6 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
@@ -45,6 +44,9 @@ import static org.elasticsearch.search.internal.SearchContext.DEFAULT_TERMINATE_
 /**
  * A request to count the number of documents matching a specific query. Best created with
  * {@link org.elasticsearch.client.Requests#countRequest(String...)}.
+ * <p>
+ * The request requires the query source to be set either using {@link #source(QuerySourceBuilder)},
+ * or {@link #source(byte[])}.
  *
  * @see CountResponse
  * @see org.elasticsearch.client.Client#count(CountRequest)
@@ -62,12 +64,12 @@ public class CountRequest extends BroadcastRequest<CountRequest> {
     @Nullable
     private String preference;
 
+    private BytesReference source;
+
     private String[] types = Strings.EMPTY_ARRAY;
 
     private int terminateAfter = DEFAULT_TERMINATE_AFTER;
 
-    private SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
-
     /**
      * Constructs a new count request against the provided indices. No indices provided means it will
      * run against all indices.
@@ -92,21 +94,67 @@ public class CountRequest extends BroadcastRequest<CountRequest> {
         return this;
     }
 
+    /**
+     * The source to execute.
+     */
+    public BytesReference source() {
+        return source;
+    }
 
     /**
-     * The query to execute
+     * The source to execute.
      */
-    public CountRequest query(QueryBuilder queryBuilder) {
-        this.searchSourceBuilder = new SearchSourceBuilder().query(queryBuilder);
+    public CountRequest source(QuerySourceBuilder sourceBuilder) {
+        this.source = sourceBuilder.buildAsBytes(Requests.CONTENT_TYPE);
         return this;
     }
 
-    public CountRequest searchSource(SearchSourceBuilder searchSourceBuilder) {
-        this.searchSourceBuilder = searchSourceBuilder;
+    /**
+     * The source to execute in the form of a map.
+     */
+    @SuppressWarnings("unchecked")
+    public CountRequest source(Map querySource) {
+        try {
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(querySource);
+            return source(builder);
+        } catch (IOException e) {
+            throw new ElasticsearchGenerationException("Failed to generate [" + querySource + "]", e);
+        }
+    }
+
+    public CountRequest source(XContentBuilder builder) {
+        this.source = builder.bytes();
         return this;
     }
 
+    /**
+     * The source to execute. It is preferable to use either {@link #source(byte[])}
+     * or {@link #source(QuerySourceBuilder)}.
+     */
+    public CountRequest source(String querySource) {
+        this.source = new BytesArray(querySource);
+        return this;
+    }
 
+    /**
+     * The source to execute.
+     */
+    public CountRequest source(byte[] querySource) {
+        return source(querySource, 0, querySource.length);
+    }
+
+    /**
+     * The source to execute.
+     */
+    public CountRequest source(byte[] querySource, int offset, int length) {
+        return source(new BytesArray(querySource, offset, length));
+    }
+
+    public CountRequest source(BytesReference querySource) {
+        this.source = querySource;
+        return this;
+    }
 
     /**
      * The types of documents the query will run against. Defaults to all types.
@@ -184,7 +232,7 @@ public class CountRequest extends BroadcastRequest<CountRequest> {
     public String toString() {
         String sSource = "_na_";
         try {
-            sSource = XContentHelper.toString(searchSourceBuilder);
+            sSource = XContentHelper.convertToJson(source, false);
         } catch (Exception e) {
             // ignore
         }
@@ -192,6 +240,13 @@ public class CountRequest extends BroadcastRequest<CountRequest> {
     }
 
     public SearchRequest toSearchRequest() {
+        SearchRequest searchRequest = new SearchRequest(indices());
+        searchRequest.indicesOptions(indicesOptions());
+        searchRequest.types(types());
+        searchRequest.routing(routing());
+        searchRequest.preference(preference());
+        searchRequest.source(source());
+        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
         searchSourceBuilder.size(0);
         if (minScore() != DEFAULT_MIN_SCORE) {
             searchSourceBuilder.minScore(minScore());
@@ -199,16 +254,7 @@ public class CountRequest extends BroadcastRequest<CountRequest> {
         if (terminateAfter() != DEFAULT_TERMINATE_AFTER) {
             searchSourceBuilder.terminateAfter(terminateAfter());
         }
-        SearchRequest searchRequest = new SearchRequest(indices());
-        searchRequest.source(searchSourceBuilder);
-        searchRequest.indicesOptions(indicesOptions());
-        searchRequest.types(types());
-        searchRequest.routing(routing());
-        searchRequest.preference(preference());
+        searchRequest.extraSource(searchSourceBuilder);
         return searchRequest;
     }
-
-    SearchSourceBuilder sourceBuilder() {
-        return searchSourceBuilder;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java
index 068e197..54c60e5 100644
--- a/core/src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java
@@ -31,13 +31,14 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 /**
  * A count action request builder.
  */
 public class CountRequestBuilder extends BroadcastOperationRequestBuilder<CountRequest, CountResponse, CountRequestBuilder> {
 
+    private QuerySourceBuilder sourceBuilder;
+
     public CountRequestBuilder(ElasticsearchClient client, CountAction action) {
         super(client, action, new CountRequest());
     }
@@ -87,18 +88,44 @@ public class CountRequestBuilder extends BroadcastOperationRequestBuilder<CountR
     }
 
     /**
+     * The query source to execute.
+     *
+     * @see org.elasticsearch.index.query.QueryBuilders
+     */
+    public CountRequestBuilder setQuery(QueryBuilder queryBuilder) {
+        sourceBuilder().setQuery(queryBuilder);
+        return this;
+    }
+
+    /**
+     * The query binary to execute
+     */
+    public CountRequestBuilder setQuery(BytesReference queryBinary) {
+        sourceBuilder().setQuery(queryBinary);
+        return this;
+    }
+
+    /**
+     * Constructs a new builder with a raw search query.
+     */
+    public CountRequestBuilder setQuery(XContentBuilder query) {
+        return setQuery(query.bytes());
+    }
+
+
+    /**
      * The source to execute.
      */
-    public CountRequestBuilder setSource(SearchSourceBuilder source) {
-        request().searchSource(source);
+    public CountRequestBuilder setSource(BytesReference source) {
+        request().source(source);
         return this;
     }
 
     /**
      * The query source to execute.
      */
-    public CountRequestBuilder setQuery(QueryBuilder<?> builder) {
-        request.query(builder);
+    public CountRequestBuilder setSource(byte[] querySource) {
+        request.source(querySource);
         return this;
     }
 
@@ -108,6 +135,21 @@ public class CountRequestBuilder extends BroadcastOperationRequestBuilder<CountR
     }
 
     @Override
+    protected CountRequest beforeExecute(CountRequest request) {
+        if (sourceBuilder != null) {
+            request.source(sourceBuilder);
+        }
+        return request;
+    }
+
+    private QuerySourceBuilder sourceBuilder() {
+        if (sourceBuilder == null) {
+            sourceBuilder = new QuerySourceBuilder();
+        }
+        return sourceBuilder;
+    }
+
+    @Override
     public void execute(ActionListener<CountResponse> listener) {
         CountRequest countRequest = beforeExecute(request);
         client.execute(SearchAction.INSTANCE, countRequest.toSearchRequest(), new DelegatingActionListener<SearchResponse, CountResponse>(listener) {
@@ -120,8 +162,15 @@ public class CountRequestBuilder extends BroadcastOperationRequestBuilder<CountR
 
     @Override
     public String toString() {
-        if (request.sourceBuilder() != null) {
-            return request.sourceBuilder().toString();
+        if (sourceBuilder != null) {
+            return sourceBuilder.toString();
+        }
+        if (request.source() != null) {
+            try {
+                return XContentHelper.convertToJson(request.source().toBytesArray(), false, true);
+            } catch (Exception e) {
+                return "{ \"error\" : \"" + ExceptionsHelper.detailedMessage(e) + "\"}";
+            }
         }
         return new QuerySourceBuilder().toString();
     }
diff --git a/core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java b/core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java
index c105fe1..0bbc37f 100644
--- a/core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java
+++ b/core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java
@@ -138,8 +138,7 @@ public class TransportDeleteAction extends TransportReplicationAction<DeleteRequ
         request.version(delete.version());
 
         assert request.versionType().validateVersionForWrites(request.version());
-
-        processAfter(request, indexShard, delete.getTranslogLocation());
+        processAfter(request.refresh(), indexShard, delete.getTranslogLocation());
 
         DeleteResponse response = new DeleteResponse(shardRequest.shardId.getIndex(), request.type(), request.id(), delete.version(), delete.found());
         return new Tuple<>(response, shardRequest.request);
@@ -151,7 +150,7 @@ public class TransportDeleteAction extends TransportReplicationAction<DeleteRequ
         Engine.Delete delete = indexShard.prepareDelete(request.type(), request.id(), request.version(), request.versionType(), Engine.Operation.Origin.REPLICA);
 
         indexShard.delete(delete);
-        processAfter(request, indexShard, delete.getTranslogLocation());
+        processAfter(request.refresh(), indexShard, delete.getTranslogLocation());
     }
 
     @Override
@@ -159,18 +158,4 @@ public class TransportDeleteAction extends TransportReplicationAction<DeleteRequ
         return clusterService.operationRouting()
                 .deleteShards(clusterService.state(), request.concreteIndex(), request.request().type(), request.request().id(), request.request().routing());
     }
-
-    private void processAfter(DeleteRequest request, IndexShard indexShard, Translog.Location location) {
-        if (request.refresh()) {
-            try {
-                indexShard.refresh("refresh_flag_delete");
-            } catch (Throwable e) {
-                // ignore
-            }
-        }
-
-        if (indexShard.getTranslogDurability() == Translog.Durabilty.REQUEST && location != null) {
-            indexShard.sync(location);
-        }
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java b/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
index 60dfa9e..348dae8 100644
--- a/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
@@ -170,7 +170,7 @@ public class TransportIndexAction extends TransportReplicationAction<IndexReques
         final WriteResult<IndexResponse> result = executeIndexRequestOnPrimary(null, request, indexShard);
         final IndexResponse response = result.response;
         final Translog.Location location = result.location;
-        processAfter(request, indexShard, location);
+        processAfter(request.refresh(), indexShard, location);
         return new Tuple<>(response, shardRequest.request);
     }
 
@@ -193,20 +193,7 @@ public class TransportIndexAction extends TransportReplicationAction<IndexReques
             throw new RetryOnReplicaException(shardId, "Mappings are not available on the replica yet, triggered update: " + update);
         }
         operation.execute(indexShard);
-        processAfter(request, indexShard, operation.getTranslogLocation());
+        processAfter(request.refresh(), indexShard, operation.getTranslogLocation());
     }
 
-    private void processAfter(IndexRequest request, IndexShard indexShard, Translog.Location location) {
-        if (request.refresh()) {
-            try {
-                indexShard.refresh("refresh_flag_index");
-            } catch (Throwable e) {
-                // ignore
-            }
-        }
-
-        if (indexShard.getTranslogDurability() == Translog.Durabilty.REQUEST && location != null) {
-            indexShard.sync(location);
-        }
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java b/core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java
index a3236e9..d754d96 100644
--- a/core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java
@@ -24,14 +24,22 @@ import org.elasticsearch.action.ActionRequestValidationException;
 import org.elasticsearch.action.CompositeIndicesRequest;
 import org.elasticsearch.action.IndicesRequest;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.XContent;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
+import java.util.Map;
 
 import static org.elasticsearch.action.ValidateActions.addValidationError;
+import static org.elasticsearch.common.xcontent.support.XContentMapValues.*;
 
 /**
  * A multi search API request.
@@ -60,6 +68,107 @@ public class MultiSearchRequest extends ActionRequest<MultiSearchRequest> implem
         return this;
     }
 
+    public MultiSearchRequest add(byte[] data, int from, int length,
+            boolean isTemplateRequest, @Nullable String[] indices, @Nullable String[] types, @Nullable String searchType) throws Exception {
+        return add(new BytesArray(data, from, length), isTemplateRequest, indices, types, searchType, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true);
+    }
+
+    public MultiSearchRequest add(BytesReference data, boolean isTemplateRequest, @Nullable String[] indices, @Nullable String[] types, @Nullable String searchType, IndicesOptions indicesOptions) throws Exception {
+        return add(data, isTemplateRequest, indices, types, searchType, null, indicesOptions, true);
+    }
+
+    public MultiSearchRequest add(BytesReference data, boolean isTemplateRequest, @Nullable String[] indices, @Nullable String[] types, @Nullable String searchType, @Nullable String routing, IndicesOptions indicesOptions, boolean allowExplicitIndex) throws Exception {
+        XContent xContent = XContentFactory.xContent(data);
+        int from = 0;
+        int length = data.length();
+        byte marker = xContent.streamSeparator();
+        while (true) {
+            int nextMarker = findNextMarker(marker, from, data, length);
+            if (nextMarker == -1) {
+                break;
+            }
+            // support first line with \n
+            if (nextMarker == 0) {
+                from = nextMarker + 1;
+                continue;
+            }
+
+            SearchRequest searchRequest = new SearchRequest();
+            if (indices != null) {
+                searchRequest.indices(indices);
+            }
+            if (indicesOptions != null) {
+                searchRequest.indicesOptions(indicesOptions);
+            }
+            if (types != null && types.length > 0) {
+                searchRequest.types(types);
+            }
+            if (routing != null) {
+                searchRequest.routing(routing);
+            }
+            searchRequest.searchType(searchType);
+
+            IndicesOptions defaultOptions = IndicesOptions.strictExpandOpenAndForbidClosed();
+
+
+            // now parse the action
+            if (nextMarker - from > 0) {
+                try (XContentParser parser = xContent.createParser(data.slice(from, nextMarker - from))) {
+                    Map<String, Object> source = parser.map();
+                    for (Map.Entry<String, Object> entry : source.entrySet()) {
+                        Object value = entry.getValue();
+                        if ("index".equals(entry.getKey()) || "indices".equals(entry.getKey())) {
+                            if (!allowExplicitIndex) {
+                                throw new IllegalArgumentException("explicit index in multi percolate is not allowed");
+                            }
+                            searchRequest.indices(nodeStringArrayValue(value));
+                        } else if ("type".equals(entry.getKey()) || "types".equals(entry.getKey())) {
+                            searchRequest.types(nodeStringArrayValue(value));
+                        } else if ("search_type".equals(entry.getKey()) || "searchType".equals(entry.getKey())) {
+                            searchRequest.searchType(nodeStringValue(value, null));
+                        } else if ("request_cache".equals(entry.getKey()) || "requestCache".equals(entry.getKey())) {
+                            searchRequest.requestCache(nodeBooleanValue(value));
+                        } else if ("preference".equals(entry.getKey())) {
+                            searchRequest.preference(nodeStringValue(value, null));
+                        } else if ("routing".equals(entry.getKey())) {
+                            searchRequest.routing(nodeStringValue(value, null));
+                        }
+                    }
+                    defaultOptions = IndicesOptions.fromMap(source, defaultOptions);
+                }
+            }
+            searchRequest.indicesOptions(defaultOptions);
+
+            // move pointers
+            from = nextMarker + 1;
+            // now for the body
+            nextMarker = findNextMarker(marker, from, data, length);
+            if (nextMarker == -1) {
+                break;
+            }
+            if (isTemplateRequest) {
+                searchRequest.templateSource(data.slice(from,  nextMarker - from));
+            } else {
+                searchRequest.source(data.slice(from, nextMarker - from));
+            }
+            // move pointers
+            from = nextMarker + 1;
+
+            add(searchRequest);
+        }
+
+        return this;
+    }
+
+    private int findNextMarker(byte marker, int from, BytesReference data, int length) {
+        for (int i = from; i < length; i++) {
+            if (data.get(i) == marker) {
+                return i;
+            }
+        }
+        return -1;
+    }
+
     public List<SearchRequest> requests() {
         return this.requests;
     }
diff --git a/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java b/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java
index 1fabe31..9348185 100644
--- a/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java
@@ -19,21 +19,31 @@
 
 package org.elasticsearch.action.search;
 
+import org.elasticsearch.ElasticsearchGenerationException;
 import org.elasticsearch.action.ActionRequest;
 import org.elasticsearch.action.ActionRequestValidationException;
 import org.elasticsearch.action.IndicesRequest;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.Template;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.Scroll;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
+import java.util.Map;
 
 import static org.elasticsearch.search.Scroll.readScroll;
 
@@ -43,7 +53,9 @@ import static org.elasticsearch.search.Scroll.readScroll;
  * <p>
  * Note, the search {@link #source(org.elasticsearch.search.builder.SearchSourceBuilder)}
  * is required. The search source is the different search options, including aggregations and such.
- * </p>
+ * <p>
+ * There is an option to specify an addition search source using the {@link #extraSource(org.elasticsearch.search.builder.SearchSourceBuilder)}.
+ *
  * @see org.elasticsearch.client.Requests#searchRequest(String...)
  * @see org.elasticsearch.client.Client#search(SearchRequest)
  * @see SearchResponse
@@ -59,8 +71,12 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
     @Nullable
     private String preference;
 
-    private SearchSourceBuilder source;
+    private BytesReference templateSource;
+    private Template template;
 
+    private BytesReference source;
+
+    private BytesReference extraSource;
     private Boolean requestCache;
 
     private Scroll scroll;
@@ -71,8 +87,6 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
 
     private IndicesOptions indicesOptions = DEFAULT_INDICES_OPTIONS;
 
-    private Template template;
-
     public SearchRequest() {
     }
 
@@ -86,8 +100,10 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
         this.indices = searchRequest.indices;
         this.routing = searchRequest.routing;
         this.preference = searchRequest.preference;
+        this.templateSource = searchRequest.templateSource;
         this.template = searchRequest.template;
         this.source = searchRequest.source;
+        this.extraSource = searchRequest.extraSource;
         this.requestCache = searchRequest.requestCache;
         this.scroll = searchRequest.scroll;
         this.types = searchRequest.types;
@@ -113,9 +129,9 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
     /**
      * Constructs a new search request against the provided indices with the given search source.
      */
-    public SearchRequest(String[] indices, SearchSourceBuilder source) {
+    public SearchRequest(String[] indices, byte[] source) {
         indices(indices);
-        this.source = source;
+        this.source = new BytesArray(source);
     }
 
     @Override
@@ -231,17 +247,60 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
      * The source of the search request.
      */
     public SearchRequest source(SearchSourceBuilder sourceBuilder) {
-        this.source = sourceBuilder;
+        this.source = sourceBuilder.buildAsBytes(Requests.CONTENT_TYPE);
+        return this;
+    }
+
+    /**
+     * The search source to execute.
+     */
+    public SearchRequest source(BytesReference source) {
+        this.source = source;
         return this;
     }
 
+
     /**
      * The search source to execute.
      */
-    public SearchSourceBuilder source() {
+    public BytesReference source() {
         return source;
     }
 
+    /**
+     * The search source template to execute.
+     */
+    public BytesReference templateSource() {
+        return templateSource;
+    }
+
+    /**
+     * Allows to provide additional source that will be used as well.
+     */
+    public SearchRequest extraSource(SearchSourceBuilder sourceBuilder) {
+        if (sourceBuilder == null) {
+            extraSource = null;
+            return this;
+        }
+        this.extraSource = sourceBuilder.buildAsBytes(Requests.CONTENT_TYPE);
+        return this;
+    }
+
+    /**
+     * Allows to provide template as source.
+     */
+    public SearchRequest templateSource(BytesReference template) {
+        this.templateSource = template;
+        return this;
+    }
+
+    /**
+     * The template of the search request.
+     */
+    public SearchRequest templateSource(String template) {
+        this.templateSource = new BytesArray(template);
+        return this;
+    }
 
     /**
      * The stored template
@@ -258,6 +317,88 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
     }
 
     /**
+     * The name of the stored template
+     * 
+     * @deprecated use {@link #template(Template)} instead.
+     */
+    @Deprecated
+    public void templateName(String templateName) {
+        updateOrCreateScript(templateName, null, null, null);
+    }
+
+    /**
+     * The type of the stored template
+     * 
+     * @deprecated use {@link #template(Template)} instead.
+     */
+    @Deprecated
+    public void templateType(ScriptService.ScriptType templateType) {
+        updateOrCreateScript(null, templateType, null, null);
+    }
+
+    /**
+     * Template parameters used for rendering
+     * 
+     * @deprecated use {@link #template(Template)} instead.
+     */
+    @Deprecated
+    public void templateParams(Map<String, Object> params) {
+        updateOrCreateScript(null, null, null, params);
+    }
+
+    /**
+     * The name of the stored template
+     * 
+     * @deprecated use {@link #template()} instead.
+     */
+    @Deprecated
+    public String templateName() {
+        return template == null ? null : template.getScript();
+    }
+
+    /**
+     * The name of the stored template
+     * 
+     * @deprecated use {@link #template()} instead.
+     */
+    @Deprecated
+    public ScriptService.ScriptType templateType() {
+        return template == null ? null : template.getType();
+    }
+
+    /**
+     * Template parameters used for rendering
+     * 
+     * @deprecated use {@link #template()} instead.
+     */
+    @Deprecated
+    public Map<String, Object> templateParams() {
+        return template == null ? null : template.getParams();
+    }
+
+    private void updateOrCreateScript(String templateContent, ScriptType type, String lang, Map<String, Object> params) {
+        Template template = template();
+        if (template == null) {
+            template = new Template(templateContent == null ? "" : templateContent, type == null ? ScriptType.INLINE : type, lang, null,
+                    params);
+        } else {
+            String newTemplateContent = templateContent == null ? template.getScript() : templateContent;
+            ScriptType newTemplateType = type == null ? template.getType() : type;
+            String newTemplateLang = lang == null ? template.getLang() : lang;
+            Map<String, Object> newTemplateParams = params == null ? template.getParams() : params;
+            template = new Template(newTemplateContent, newTemplateType, MustacheScriptEngineService.NAME, null, newTemplateParams);
+        }
+        template(template);
+    }
+
+    /**
+     * Additional search source to execute.
+     */
+    public BytesReference extraSource() {
+        return this.extraSource;
+    }
+
+    /**
      * The tye of search to execute.
      */
     public SearchType searchType() {
@@ -331,15 +472,18 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
         if (in.readBoolean()) {
             scroll = readScroll(in);
         }
-        if (in.readBoolean()) {
-            source = SearchSourceBuilder.PROTOTYPE.readFrom(in);
-        }
+
+        source = in.readBytesReference();
+        extraSource = in.readBytesReference();
 
         types = in.readStringArray();
         indicesOptions = IndicesOptions.readIndicesOptions(in);
 
+        templateSource = in.readBytesReference();
+        if (in.readBoolean()) {
+            template = Template.readTemplate(in);
+        }
         requestCache = in.readOptionalBoolean();
-        template = in.readOptionalStreamable(new Template());
     }
 
     @Override
@@ -361,15 +505,18 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
             out.writeBoolean(true);
             scroll.writeTo(out);
         }
-        if (source == null) {
-            out.writeBoolean(false);
-        } else {
-            out.writeBoolean(true);
-            source.writeTo(out);
-        }
+        out.writeBytesReference(source);
+        out.writeBytesReference(extraSource);
         out.writeStringArray(types);
         indicesOptions.writeIndicesOptions(out);
+
+        out.writeBytesReference(templateSource);
+        boolean hasTemplate = template != null;
+        out.writeBoolean(hasTemplate);
+        if (hasTemplate) {
+            template.writeTo(out);
+        }
+
         out.writeOptionalBoolean(requestCache);
-        out.writeOptionalStreamable(template);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
index 75392c7..a570080 100644
--- a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
@@ -26,9 +26,11 @@ import org.elasticsearch.client.ElasticsearchClient;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.script.Script;
+import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
 import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
@@ -40,7 +42,7 @@ import org.elasticsearch.search.sort.SortBuilder;
 import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.search.suggest.SuggestBuilder;
 
-import java.util.Arrays;
+import java.util.Map;
 
 /**
  * A search action request builder.
@@ -121,6 +123,14 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     }
 
     /**
+     * An optional timeout to control how long search is allowed to take.
+     */
+    public SearchRequestBuilder setTimeout(String timeout) {
+        sourceBuilder().timeout(timeout);
+        return this;
+    }
+
+    /**
      * An optional document count, upon collecting which the search
      * query will early terminate
      */
@@ -170,16 +180,118 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
      *
      * @see org.elasticsearch.index.query.QueryBuilders
      */
-    public SearchRequestBuilder setQuery(QueryBuilder<?> queryBuilder) {
+    public SearchRequestBuilder setQuery(QueryBuilder queryBuilder) {
         sourceBuilder().query(queryBuilder);
         return this;
     }
 
     /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(String query) {
+        sourceBuilder().query(query);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(BytesReference queryBinary) {
+        sourceBuilder().query(queryBinary);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(byte[] queryBinary) {
+        sourceBuilder().query(queryBinary);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(byte[] queryBinary, int queryBinaryOffset, int queryBinaryLength) {
+        sourceBuilder().query(queryBinary, queryBinaryOffset, queryBinaryLength);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(XContentBuilder query) {
+        sourceBuilder().query(query);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(Map query) {
+        sourceBuilder().query(query);
+        return this;
+    }
+
+    /**
      * Sets a filter that will be executed after the query has been executed and only has affect on the search hits
      * (not aggregations). This filter is always executed as last filtering mechanism.
      */
-    public SearchRequestBuilder setPostFilter(QueryBuilder<?> postFilter) {
+    public SearchRequestBuilder setPostFilter(QueryBuilder postFilter) {
+        sourceBuilder().postFilter(postFilter);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(String postFilter) {
+        sourceBuilder().postFilter(postFilter);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(BytesReference postFilter) {
+        sourceBuilder().postFilter(postFilter);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(byte[] postFilter) {
+        sourceBuilder().postFilter(postFilter);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(byte[] postFilter, int postFilterOffset, int postFilterLength) {
+        sourceBuilder().postFilter(postFilter, postFilterOffset, postFilterLength);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(XContentBuilder postFilter) {
+        sourceBuilder().postFilter(postFilter);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(Map postFilter) {
         sourceBuilder().postFilter(postFilter);
         return this;
     }
@@ -353,7 +465,7 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
      * the source of the document will be returned.
      */
     public SearchRequestBuilder addFields(String... fields) {
-        sourceBuilder().fields(Arrays.asList(fields));
+        sourceBuilder().fields(fields);
         return this;
     }
 
@@ -365,23 +477,267 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
         return this;
     }
 
-    public SearchRequestBuilder highlighter(HighlightBuilder highlightBuilder) {
-        sourceBuilder().highlighter(highlightBuilder);
+    /**
+     * Sets a raw (xcontent) binary representation of addAggregation to use.
+     */
+    public SearchRequestBuilder setAggregations(BytesReference aggregations) {
+        sourceBuilder().aggregations(aggregations);
+        return this;
+    }
+
+    /**
+     * Sets a raw (xcontent) binary representation of addAggregation to use.
+     */
+    public SearchRequestBuilder setAggregations(byte[] aggregations) {
+        sourceBuilder().aggregations(aggregations);
+        return this;
+    }
+
+    /**
+     * Sets a raw (xcontent) binary representation of addAggregation to use.
+     */
+    public SearchRequestBuilder setAggregations(byte[] aggregations, int aggregationsOffset, int aggregationsLength) {
+        sourceBuilder().aggregations(aggregations, aggregationsOffset, aggregationsLength);
+        return this;
+    }
+
+    /**
+     * Sets a raw (xcontent) binary representation of addAggregation to use.
+     */
+    public SearchRequestBuilder setAggregations(XContentBuilder aggregations) {
+        sourceBuilder().aggregations(aggregations);
+        return this;
+    }
+
+    /**
+     * Sets a raw (xcontent) binary representation of addAggregation to use.
+     */
+    public SearchRequestBuilder setAggregations(Map aggregations) {
+        sourceBuilder().aggregations(aggregations);
+        return this;
+    }
+
+    /**
+     * Adds a field to be highlighted with default fragment size of 100 characters, and
+     * default number of fragments of 5.
+     *
+     * @param name The field to highlight
+     */
+    public SearchRequestBuilder addHighlightedField(String name) {
+        highlightBuilder().field(name);
+        return this;
+    }
+
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters), and
+     * default number of fragments of 5.
+     *
+     * @param name         The field to highlight
+     * @param fragmentSize The size of a fragment in characters
+     */
+    public SearchRequestBuilder addHighlightedField(String name, int fragmentSize) {
+        highlightBuilder().field(name, fragmentSize);
+        return this;
+    }
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters), and
+     * a provided (maximum) number of fragments.
+     *
+     * @param name              The field to highlight
+     * @param fragmentSize      The size of a fragment in characters
+     * @param numberOfFragments The (maximum) number of fragments
+     */
+    public SearchRequestBuilder addHighlightedField(String name, int fragmentSize, int numberOfFragments) {
+        highlightBuilder().field(name, fragmentSize, numberOfFragments);
+        return this;
+    }
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters),
+     * a provided (maximum) number of fragments and an offset for the highlight.
+     *
+     * @param name              The field to highlight
+     * @param fragmentSize      The size of a fragment in characters
+     * @param numberOfFragments The (maximum) number of fragments
+     */
+    public SearchRequestBuilder addHighlightedField(String name, int fragmentSize, int numberOfFragments,
+                                                    int fragmentOffset) {
+        highlightBuilder().field(name, fragmentSize, numberOfFragments, fragmentOffset);
+        return this;
+    }
+
+    /**
+     * Adds a highlighted field.
+     */
+    public SearchRequestBuilder addHighlightedField(HighlightBuilder.Field field) {
+        highlightBuilder().field(field);
+        return this;
+    }
+
+    /**
+     * Set a tag scheme that encapsulates a built in pre and post tags. The allows schemes
+     * are <tt>styled</tt> and <tt>default</tt>.
+     *
+     * @param schemaName The tag scheme name
+     */
+    public SearchRequestBuilder setHighlighterTagsSchema(String schemaName) {
+        highlightBuilder().tagsSchema(schemaName);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterFragmentSize(Integer fragmentSize) {
+        highlightBuilder().fragmentSize(fragmentSize);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterNumOfFragments(Integer numOfFragments) {
+        highlightBuilder().numOfFragments(numOfFragments);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterFilter(Boolean highlightFilter) {
+        highlightBuilder().highlightFilter(highlightFilter);
+        return this;
+    }
+
+    /**
+     * The encoder to set for highlighting
+     */
+    public SearchRequestBuilder setHighlighterEncoder(String encoder) {
+        highlightBuilder().encoder(encoder);
+        return this;
+    }
+
+    /**
+     * Explicitly set the pre tags that will be used for highlighting.
+     */
+    public SearchRequestBuilder setHighlighterPreTags(String... preTags) {
+        highlightBuilder().preTags(preTags);
+        return this;
+    }
+
+    /**
+     * Explicitly set the post tags that will be used for highlighting.
+     */
+    public SearchRequestBuilder setHighlighterPostTags(String... postTags) {
+        highlightBuilder().postTags(postTags);
+        return this;
+    }
+
+    /**
+     * The order of fragments per field. By default, ordered by the order in the
+     * highlighted text. Can be <tt>score</tt>, which then it will be ordered
+     * by score of the fragments.
+     */
+    public SearchRequestBuilder setHighlighterOrder(String order) {
+        highlightBuilder().order(order);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterRequireFieldMatch(boolean requireFieldMatch) {
+        highlightBuilder().requireFieldMatch(requireFieldMatch);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterBoundaryMaxScan(Integer boundaryMaxScan) {
+        highlightBuilder().boundaryMaxScan(boundaryMaxScan);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterBoundaryChars(char[] boundaryChars) {
+        highlightBuilder().boundaryChars(boundaryChars);
         return this;
     }
 
     /**
-     * Delegates to
-     * {@link org.elasticsearch.search.suggest.SuggestBuilder#addSuggestion(org.elasticsearch.search.suggest.SuggestBuilder.SuggestionBuilder)}
-     * .
+     * The highlighter type to use.
      */
-    public SearchRequestBuilder suggest(SuggestBuilder suggestBuilder) {
-        sourceBuilder().suggest(suggestBuilder);
+    public SearchRequestBuilder setHighlighterType(String type) {
+        highlightBuilder().highlighterType(type);
         return this;
     }
 
-    public SearchRequestBuilder innerHits(InnerHitsBuilder innerHitsBuilder) {
-        sourceBuilder().innerHits(innerHitsBuilder);
+    public SearchRequestBuilder setHighlighterFragmenter(String fragmenter) {
+        highlightBuilder().fragmenter(fragmenter);
+        return this;
+    }
+
+    /**
+     * Sets a query to be used for highlighting all fields instead of the search query.
+     */
+    public SearchRequestBuilder setHighlighterQuery(QueryBuilder highlightQuery) {
+        highlightBuilder().highlightQuery(highlightQuery);
+        return this;
+    }
+
+    /**
+     * Sets the size of the fragment to return from the beginning of the field if there are no matches to
+     * highlight and the field doesn't also define noMatchSize.
+     *
+     * @param noMatchSize integer to set or null to leave out of request.  default is null.
+     * @return this builder for chaining
+     */
+    public SearchRequestBuilder setHighlighterNoMatchSize(Integer noMatchSize) {
+        highlightBuilder().noMatchSize(noMatchSize);
+        return this;
+    }
+
+    /**
+     * Sets the maximum number of phrases the fvh will consider if the field doesn't also define phraseLimit.
+     */
+    public SearchRequestBuilder setHighlighterPhraseLimit(Integer phraseLimit) {
+        highlightBuilder().phraseLimit(phraseLimit);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterOptions(Map<String, Object> options) {
+        highlightBuilder().options(options);
+        return this;
+    }
+
+    /**
+     * Forces to highlight fields based on the source even if fields are stored separately.
+     */
+    public SearchRequestBuilder setHighlighterForceSource(Boolean forceSource) {
+        highlightBuilder().forceSource(forceSource);
+        return this;
+    }
+
+    /**
+     * Send the fields to be highlighted using a syntax that is specific about the order in which they should be highlighted.
+     *
+     * @return this for chaining
+     */
+    public SearchRequestBuilder setHighlighterExplicitFieldOrder(boolean explicitFieldOrder) {
+        highlightBuilder().useExplicitFieldOrder(explicitFieldOrder);
+        return this;
+    }
+
+    public SearchRequestBuilder addParentChildInnerHits(String name, String type,  InnerHitsBuilder.InnerHit innerHit) {
+        innerHitsBuilder().addParentChildInnerHits(name, type, innerHit);
+        return this;
+    }
+
+    public SearchRequestBuilder addNestedInnerHits(String name, String path,  InnerHitsBuilder.InnerHit innerHit) {
+        innerHitsBuilder().addNestedInnerHits(name, path, innerHit);
+        return this;
+    }
+
+    /**
+     * Delegates to {@link org.elasticsearch.search.suggest.SuggestBuilder#setText(String)}.
+     */
+    public SearchRequestBuilder setSuggestText(String globalText) {
+        suggestBuilder().setText(globalText);
+        return this;
+    }
+
+    /**
+     * Delegates to {@link org.elasticsearch.search.suggest.SuggestBuilder#addSuggestion(org.elasticsearch.search.suggest.SuggestBuilder.SuggestionBuilder)}.
+     */
+    public SearchRequestBuilder addSuggestion(SuggestBuilder.SuggestionBuilder<?> suggestion) {
+        suggestBuilder().addSuggestion(suggestion);
         return this;
     }
 
@@ -455,7 +811,9 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     }
 
     /**
-     * Sets the source of the request as a SearchSourceBuilder.
+     * Sets the source of the request as a SearchSourceBuilder. Note, settings anything other
+     * than the search type will cause this source to be overridden, consider using
+     * {@link #setExtraSource(SearchSourceBuilder)} instead.
      */
     public SearchRequestBuilder setSource(SearchSourceBuilder source) {
         request.source(source);
@@ -463,6 +821,26 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     }
 
     /**
+     * Sets the source of the request as a json string. Note, settings anything other
+     * than the search type will cause this source to be overridden, consider using
+     * {@link #setExtraSource(SearchSourceBuilder)} instead.
+     */
+    public SearchRequestBuilder setSource(BytesReference source) {
+        request.source(source);
+        return this;
+    }
+
+    /**
+     * Sets the an addtional source of the request as a SearchSourceBuilder. All values and
+     * settings set on the extra source will override the corresponding settings on the specified
+     * source.
+     */
+    public SearchRequestBuilder setExtraSource(SearchSourceBuilder source) {
+        request.extraSource(source);
+        return this;
+    }
+
+    /**
      * template stuff
      */
     public SearchRequestBuilder setTemplate(Template template) {
@@ -470,6 +848,16 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
         return this;
     }
 
+    public SearchRequestBuilder setTemplateSource(String source) {
+        request.templateSource(source);
+        return this;
+    }
+
+    public SearchRequestBuilder setTemplateSource(BytesReference source) {
+        request.templateSource(source);
+        return this;
+    }
+
     /**
      * Sets if this request should use the request cache or not, assuming that it can (for
      * example, if "now" is used, it will never be cached). By default (not set, or null,
@@ -504,7 +892,7 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
         }
         if (request.source() != null) {
             try {
-                return XContentHelper.toString(request.source());
+                return XContentHelper.convertToJson(request.source().toBytesArray(), false, true);
             } catch (Exception e) {
                 return "{ \"error\" : \"" + ExceptionsHelper.detailedMessage(e) + "\"}";
             }
@@ -534,4 +922,16 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
         }
         return sourceBuilder;
     }
+
+    private HighlightBuilder highlightBuilder() {
+        return sourceBuilder().highlighter();
+    }
+
+    private InnerHitsBuilder innerHitsBuilder() {
+        return sourceBuilder().innerHitsBuilder();
+    }
+
+    private SuggestBuilder suggestBuilder() {
+        return sourceBuilder().suggest();
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
index 6e2ea50..dc6a89f 100644
--- a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
@@ -1082,4 +1082,18 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
 
         return new WriteResult(new IndexResponse(shardId.getIndex(), request.type(), request.id(), request.version(), created), operation.getTranslogLocation());
     }
+
+    protected final void processAfter(boolean refresh, IndexShard indexShard, Translog.Location location) {
+        if (refresh) {
+            try {
+                indexShard.refresh("refresh_flag_index");
+            } catch (Throwable e) {
+                // ignore
+            }
+        }
+        if (indexShard.getTranslogDurability() == Translog.Durabilty.REQUEST && location != null) {
+            indexShard.sync(location);
+        }
+        indexShard.maybeFlush();
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java b/core/src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java
index 1c89ee1..86d575d 100644
--- a/core/src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java
@@ -27,6 +27,7 @@ import org.elasticsearch.action.search.SearchRequest;
 import org.elasticsearch.action.support.broadcast.BroadcastRequest;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.index.query.BoolQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
@@ -102,7 +103,12 @@ public class DfsOnlyRequest extends BroadcastRequest<DfsOnlyRequest> {
 
     @Override
     public String toString() {
-        String sSource = searchRequest.source().toString();
+        String sSource = "_na_";
+        try {
+            sSource = XContentHelper.convertToJson(searchRequest.source(), false);
+        } catch (IOException e) {
+            // ignore
+        }
         return "[" + Arrays.toString(indices) + "]" + Arrays.toString(types()) + ", source[" + sSource + "]";
     }
 
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
index 86e9c2f..c9c3eeb 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
@@ -79,7 +79,7 @@ final class Bootstrap {
     }
     
     /** initialize native resources */
-    public static void initializeNatives(boolean mlockAll, boolean ctrlHandler) {
+    public static void initializeNatives(boolean mlockAll, boolean seccomp, boolean ctrlHandler) {
         final ESLogger logger = Loggers.getLogger(Bootstrap.class);
         
         // check if the user is running as root, and bail
@@ -91,6 +91,11 @@ final class Bootstrap {
             }
         }
         
+        // enable secure computing mode
+        if (seccomp) {
+            Natives.trySeccomp();
+        }
+        
         // mlockall if requested
         if (mlockAll) {
             if (Constants.WINDOWS) {
@@ -134,7 +139,8 @@ final class Bootstrap {
 
     private void setup(boolean addShutdownHook, Settings settings, Environment environment) throws Exception {
         initializeNatives(settings.getAsBoolean("bootstrap.mlockall", false),
-                settings.getAsBoolean("bootstrap.ctrlhandler", true));
+                          settings.getAsBoolean("bootstrap.seccomp", true),
+                          settings.getAsBoolean("bootstrap.ctrlhandler", true));
 
         // initialize probes before the security manager is installed
         initializeProbes();
@@ -218,6 +224,9 @@ final class Bootstrap {
      * to startup elasticsearch.
      */
     static void init(String[] args) throws Throwable {
+        // Set the system property before anything has a chance to trigger its use
+        System.setProperty("es.logger.prefix", "");
+
         BootstrapCLIParser bootstrapCLIParser = new BootstrapCLIParser();
         CliTool.ExitStatus status = bootstrapCLIParser.execute(args);
 
@@ -225,7 +234,6 @@ final class Bootstrap {
             System.exit(status.status());
         }
 
-        System.setProperty("es.logger.prefix", "");
         INSTANCE = new Bootstrap();
 
         boolean foreground = !"false".equals(System.getProperty("es.foreground", System.getProperty("es-foreground")));
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/BootstrapCLIParser.java b/core/src/main/java/org/elasticsearch/bootstrap/BootstrapCLIParser.java
index 9ce2dba..03ec082 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/BootstrapCLIParser.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/BootstrapCLIParser.java
@@ -128,7 +128,13 @@ final class BootstrapCLIParser extends CliTool {
             while (iterator.hasNext()) {
                 String arg = iterator.next();
                 if (!arg.startsWith("--")) {
-                    throw new IllegalArgumentException("Parameter [" + arg + "]does not start with --");
+                    if (arg.startsWith("-D") || arg.startsWith("-d") || arg.startsWith("-p")) {
+                        throw new IllegalArgumentException(
+                                "Parameter [" + arg + "] starting with \"-D\", \"-d\" or \"-p\" must be before any parameters starting with --"
+                        );
+                    } else {
+                        throw new IllegalArgumentException("Parameter [" + arg + "]does not start with --");
+                    }
                 }
                 // if there is no = sign, we have to get the next argu
                 arg = arg.replace("--", "");
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/BootstrapInfo.java b/core/src/main/java/org/elasticsearch/bootstrap/BootstrapInfo.java
index 829c45f..76485bb 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/BootstrapInfo.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/BootstrapInfo.java
@@ -43,4 +43,11 @@ public final class BootstrapInfo {
     public static boolean isMemoryLocked() {
         return Natives.isMemoryLocked();
     }
+    
+    /**
+     * Returns true if secure computing mode is enabled (linux/amd64 only)
+     */
+    public static boolean isSeccompInstalled() {
+        return Natives.isSeccompInstalled();
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java b/core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java
index 7a76d37..647f923 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java
@@ -41,6 +41,8 @@ class JNANatives {
 
     // Set to true, in case native mlockall call was successful
     static boolean LOCAL_MLOCKALL = false;
+    // Set to true, in case native seccomp call was successful
+    static boolean LOCAL_SECCOMP = false;
 
     static void tryMlockall() {
         int errno = Integer.MIN_VALUE;
@@ -170,4 +172,19 @@ class JNANatives {
         }
     }
 
+    static void trySeccomp() {
+        if (Constants.LINUX && "amd64".equals(Constants.OS_ARCH)) {
+            try {
+                Seccomp.installFilter();
+                LOCAL_SECCOMP = true;
+            } catch (Exception e) {
+                // this is likely to happen unless the kernel is newish, its a best effort at the moment
+                // so we log stacktrace at debug for now...
+                if (logger.isDebugEnabled()) {
+                    logger.debug("unable to install seccomp filter", e);
+                }
+                logger.warn("unable to install seccomp filter: " + e.getMessage());
+            }
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Natives.java b/core/src/main/java/org/elasticsearch/bootstrap/Natives.java
index 05b0088..1bf9bb8 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Natives.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Natives.java
@@ -88,4 +88,19 @@ final class Natives {
         }
         return JNANatives.LOCAL_MLOCKALL;
     }
+    
+    static void trySeccomp() {
+        if (!JNA_AVAILABLE) {
+            logger.warn("cannot install seccomp filters because JNA is not available");
+            return;
+        }
+        JNANatives.trySeccomp();
+    }
+    
+    static boolean isSeccompInstalled() {
+        if (!JNA_AVAILABLE) {
+            return false;
+        }
+        return JNANatives.LOCAL_SECCOMP;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Seccomp.java b/core/src/main/java/org/elasticsearch/bootstrap/Seccomp.java
new file mode 100644
index 0000000..409ebc2
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Seccomp.java
@@ -0,0 +1,271 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.bootstrap;
+
+import com.sun.jna.Library;
+import com.sun.jna.Memory;
+import com.sun.jna.Native;
+import com.sun.jna.Pointer;
+import com.sun.jna.Structure;
+
+import org.apache.lucene.util.Constants;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.Loggers;
+
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.util.Arrays;
+import java.util.List;
+
+/** 
+ * Installs a limited form of Linux secure computing mode (filter mode).
+ * This filters system calls to block process execution.
+ * <p>
+ * This is only supported on the amd64 architecture, on Linux kernels 3.5 or above, and requires
+ * {@code CONFIG_SECCOMP} and {@code CONFIG_SECCOMP_FILTER} compiled into the kernel.
+ * <p>
+ * Filters are installed using either {@code seccomp(2)} (3.17+) or {@code prctl(2)} (3.5+). {@code seccomp(2)}
+ * is preferred, as it allows filters to be applied to any existing threads in the process, and one motivation
+ * here is to protect against bugs in the JVM. Otherwise, code will fall back to the {@code prctl(2)} method 
+ * which will at least protect elasticsearch application threads.
+ * <p>
+ * The filters will return {@code EACCES} (Access Denied) for the following system calls:
+ * <ul>
+ *   <li>{@code execve}</li>
+ *   <li>{@code fork}</li>
+ *   <li>{@code vfork}</li>
+ * </ul>
+ * <p>
+ * This is not intended as a sandbox. It is another level of security, mostly intended to annoy
+ * security researchers and make their lives more difficult in achieving "remote execution" exploits.
+ * @see <a href="http://www.kernel.org/doc/Documentation/prctl/seccomp_filter.txt">
+ *      http://www.kernel.org/doc/Documentation/prctl/seccomp_filter.txt</a>
+ */
+// only supported on linux/amd64
+// not an example of how to write code!!!
+final class Seccomp {
+    private static final ESLogger logger = Loggers.getLogger(Seccomp.class);
+
+    /** we use an explicit interface for native methods, for varargs support */
+    static interface LinuxLibrary extends Library {
+        /** 
+         * maps to prctl(2) 
+         */
+        int prctl(int option, long arg2, long arg3, long arg4, long arg5);
+        /** 
+         * used to call seccomp(2), its too new... 
+         * this is the only way, DONT use it on some other architecture unless you know wtf you are doing 
+         */
+        long syscall(long number, Object... args);
+    };
+
+    // null if something goes wrong.
+    static final LinuxLibrary libc;
+
+    static {
+        LinuxLibrary lib = null;
+        try {
+            lib = (LinuxLibrary) Native.loadLibrary("c", LinuxLibrary.class);
+        } catch (UnsatisfiedLinkError e) {
+            logger.warn("unable to link C library. native methods (seccomp) will be disabled.", e);
+        }
+        libc = lib;
+    }
+    
+    /** the preferred method is seccomp(2), since we can apply to all threads of the process */
+    static final int SECCOMP_SYSCALL_NR        = 317;   // since Linux 3.17
+    static final int SECCOMP_SET_MODE_FILTER   =   1;   // since Linux 3.17
+    static final int SECCOMP_FILTER_FLAG_TSYNC =   1;   // since Linux 3.17
+
+    /** otherwise, we can use prctl(2), which will at least protect ES application threads */
+    static final int PR_GET_NO_NEW_PRIVS       =  39;   // since Linux 3.5
+    static final int PR_SET_NO_NEW_PRIVS       =  38;   // since Linux 3.5
+    static final int PR_GET_SECCOMP            =  21;   // since Linux 2.6.23
+    static final int PR_SET_SECCOMP            =  22;   // since Linux 2.6.23
+    static final int SECCOMP_MODE_FILTER       =   2;   // since Linux Linux 3.5
+    
+    /** corresponds to struct sock_filter */
+    static final class SockFilter {
+        short code; // insn
+        byte jt;    // number of insn to jump (skip) if true
+        byte jf;    // number of insn to jump (skip) if false
+        int k;      // additional data
+
+        SockFilter(short code, byte jt, byte jf, int k) {
+            this.code = code;
+            this.jt = jt;
+            this.jf = jf;
+            this.k = k;
+        }
+    }
+    
+    /** corresponds to struct sock_fprog */
+    public static final class SockFProg extends Structure implements Structure.ByReference {
+        public short   len;           // number of filters
+        public Pointer filter;        // filters
+        
+        public SockFProg(SockFilter filters[]) {
+            len = (short) filters.length;
+            // serialize struct sock_filter * explicitly, its less confusing than the JNA magic we would need
+            Memory filter = new Memory(len * 8);
+            ByteBuffer bbuf = filter.getByteBuffer(0, len * 8);
+            bbuf.order(ByteOrder.nativeOrder()); // little endian
+            for (SockFilter f : filters) {
+                bbuf.putShort(f.code);
+                bbuf.put(f.jt);
+                bbuf.put(f.jf);
+                bbuf.putInt(f.k);
+            }
+            this.filter = filter;
+        }
+        
+        @Override
+        protected List<String> getFieldOrder() {
+            return Arrays.asList(new String[] { "len", "filter" });
+        }
+    }
+    
+    // BPF "macros" and constants
+    static final int BPF_LD  = 0x00;
+    static final int BPF_W   = 0x00;
+    static final int BPF_ABS = 0x20;
+    static final int BPF_JMP = 0x05;
+    static final int BPF_JEQ = 0x10;
+    static final int BPF_JGE = 0x30;
+    static final int BPF_JGT = 0x20;
+    static final int BPF_RET = 0x06;
+    static final int BPF_K   = 0x00;
+    
+    static SockFilter BPF_STMT(int code, int k) {
+        return new SockFilter((short) code, (byte) 0, (byte) 0, k);
+    }
+    
+    static SockFilter BPF_JUMP(int code, int k, int jt, int jf) {
+        return new SockFilter((short) code, (byte) jt, (byte) jf, k);
+    }
+    
+    static final int AUDIT_ARCH_X86_64 = 0xC000003E;
+    static final int SECCOMP_RET_ERRNO = 0x00050000;
+    static final int SECCOMP_RET_DATA  = 0x0000FFFF;
+    static final int SECCOMP_RET_ALLOW = 0x7FFF0000;
+
+    // some errno constants for error checking/handling
+    static final int EACCES = 0x0D;
+    static final int EFAULT = 0x0E;
+    static final int EINVAL = 0x16;
+    static final int ENOSYS = 0x26;
+
+    // offsets (arch dependent) that our BPF checks
+    static final int SECCOMP_DATA_NR_OFFSET   = 0x00;
+    static final int SECCOMP_DATA_ARCH_OFFSET = 0x04;
+    
+    // currently this range is blocked (inclusive):
+    // execve is really the only one needed but why let someone fork a 30G heap? (not really what happens)
+    // ...
+    // 57: fork
+    // 58: vfork
+    // 59: execve
+    // ...
+    static final int BLACKLIST_START = 57;
+    static final int BLACKLIST_END   = 59;
+    
+    // TODO: execveat()? its less of a risk since the jvm does not use it...
+
+    /** try to install our filters */
+    static void installFilter() {
+        // first be defensive: we can give nice errors this way, at the very least.
+        // also, some of these security features get backported to old versions, checking kernel version here is a big no-no! 
+        boolean supported = Constants.LINUX && "amd64".equals(Constants.OS_ARCH);
+        if (supported == false) {
+            throw new IllegalStateException("bug: should not be trying to initialize seccomp for an unsupported architecture");
+        }
+        
+        // we couldn't link methods, could be some really ancient kernel (e.g. < 2.1.57) or some bug
+        if (libc == null) {
+            throw new UnsupportedOperationException("seccomp unavailable: could not link methods. requires kernel 3.5+ with CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER compiled in");
+        }
+
+        // check for kernel version
+        if (libc.prctl(PR_GET_NO_NEW_PRIVS, 0, 0, 0, 0) < 0) {
+            int errno = Native.getLastError();
+            switch (errno) {
+                case ENOSYS: throw new UnsupportedOperationException("seccomp unavailable: requires kernel 3.5+ with CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER compiled in");
+                default: throw new UnsupportedOperationException("prctl(PR_GET_NO_NEW_PRIVS): " + JNACLibrary.strerror(errno));
+            }
+        }
+        // check for SECCOMP
+        if (libc.prctl(PR_GET_SECCOMP, 0, 0, 0, 0) < 0) {
+            int errno = Native.getLastError();
+            switch (errno) {
+                case EINVAL: throw new UnsupportedOperationException("seccomp unavailable: CONFIG_SECCOMP not compiled into kernel, CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER are needed");
+                default: throw new UnsupportedOperationException("prctl(PR_GET_SECCOMP): " + JNACLibrary.strerror(errno));
+            }
+        }
+        // check for SECCOMP_MODE_FILTER
+        if (libc.prctl(PR_SET_SECCOMP, SECCOMP_MODE_FILTER, 0, 0, 0) < 0) {
+            int errno = Native.getLastError();
+            switch (errno) {
+                case EFAULT: break; // available
+                case EINVAL: throw new UnsupportedOperationException("seccomp unavailable: CONFIG_SECCOMP_FILTER not compiled into kernel, CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER are needed");
+                default: throw new UnsupportedOperationException("prctl(PR_SET_SECCOMP): " + JNACLibrary.strerror(errno));
+            }
+        }
+
+        // ok, now set PR_SET_NO_NEW_PRIVS, needed to be able to set a seccomp filter as ordinary user
+        if (libc.prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0) < 0) {
+            throw new UnsupportedOperationException("prctl(PR_SET_NO_NEW_PRIVS): " + JNACLibrary.strerror(Native.getLastError()));
+        }
+        
+        // BPF installed to check arch, then syscall range. See https://www.kernel.org/doc/Documentation/prctl/seccomp_filter.txt for details.
+        SockFilter insns[] = {
+          /* 1 */ BPF_STMT(BPF_LD  + BPF_W   + BPF_ABS, SECCOMP_DATA_ARCH_OFFSET),               // if (arch != amd64) goto fail;
+          /* 2 */ BPF_JUMP(BPF_JMP + BPF_JEQ + BPF_K,   AUDIT_ARCH_X86_64, 0, 3),                //
+          /* 3 */ BPF_STMT(BPF_LD  + BPF_W   + BPF_ABS, SECCOMP_DATA_NR_OFFSET),                 // if (syscall < BLACKLIST_START) goto pass;
+          /* 4 */ BPF_JUMP(BPF_JMP + BPF_JGE + BPF_K,   BLACKLIST_START, 0, 2),                  //
+          /* 5 */ BPF_JUMP(BPF_JMP + BPF_JGT + BPF_K,   BLACKLIST_END, 1, 0),                    // if (syscall > BLACKLIST_END) goto pass;
+          /* 6 */ BPF_STMT(BPF_RET + BPF_K, SECCOMP_RET_ERRNO | (EACCES & SECCOMP_RET_DATA)),    // fail: return EACCES;
+          /* 7 */ BPF_STMT(BPF_RET + BPF_K, SECCOMP_RET_ALLOW)                                   // pass: return OK;
+        };
+        
+        // seccomp takes a long, so we pass it one explicitly to keep the JNA simple
+        SockFProg prog = new SockFProg(insns);
+        prog.write();
+        long pointer = Pointer.nativeValue(prog.getPointer());
+
+        // install filter, if this works, after this there is no going back!
+        // first try it with seccomp(SECCOMP_SET_MODE_FILTER), falling back to prctl()
+        if (libc.syscall(SECCOMP_SYSCALL_NR, SECCOMP_SET_MODE_FILTER, SECCOMP_FILTER_FLAG_TSYNC, pointer) != 0) {
+            int errno1 = Native.getLastError();
+            if (logger.isDebugEnabled()) {
+                logger.debug("seccomp(SECCOMP_SET_MODE_FILTER): " + JNACLibrary.strerror(errno1) + ", falling back to prctl(PR_SET_SECCOMP)...");
+            }
+            if (libc.prctl(PR_SET_SECCOMP, SECCOMP_MODE_FILTER, pointer, 0, 0) < 0) {
+                int errno2 = Native.getLastError();
+                throw new UnsupportedOperationException("seccomp(SECCOMP_SET_MODE_FILTER): " + JNACLibrary.strerror(errno1) + 
+                                                        ", prctl(PR_SET_SECCOMP): " + JNACLibrary.strerror(errno2));
+            }
+        }
+        
+        // now check that the filter was really installed, we should be in filter mode.
+        if (libc.prctl(PR_GET_SECCOMP, 0, 0, 0, 0) != 2) {
+            throw new UnsupportedOperationException("seccomp filter installation did not really succeed. seccomp(PR_GET_SECCOMP): " + JNACLibrary.strerror(Native.getLastError()));
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/cluster/AckedClusterStateUpdateTask.java b/core/src/main/java/org/elasticsearch/cluster/AckedClusterStateUpdateTask.java
index 087bd1c..21c6cd5 100644
--- a/core/src/main/java/org/elasticsearch/cluster/AckedClusterStateUpdateTask.java
+++ b/core/src/main/java/org/elasticsearch/cluster/AckedClusterStateUpdateTask.java
@@ -28,7 +28,7 @@ import org.elasticsearch.common.unit.TimeValue;
  * An extension interface to {@link ClusterStateUpdateTask} that allows to be notified when
  * all the nodes have acknowledged a cluster state update request
  */
-public abstract class AckedClusterStateUpdateTask<Response> extends TimeoutClusterStateUpdateTask {
+public abstract class AckedClusterStateUpdateTask<Response> extends ClusterStateUpdateTask {
 
     private final ActionListener<Response> listener;
     private final AckedRequest request;
@@ -73,11 +73,6 @@ public abstract class AckedClusterStateUpdateTask<Response> extends TimeoutClust
         listener.onFailure(t);
     }
 
-    @Override
-    public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {
-
-    }
-
     /**
      * Acknowledgement timeout, maximum time interval to wait for acknowledgements
      */
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java b/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java
index 097ad61..581bee1 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java
@@ -81,7 +81,6 @@ import org.elasticsearch.index.shard.MergePolicyConfig;
 import org.elasticsearch.index.shard.MergeSchedulerConfig;
 import org.elasticsearch.index.store.IndexStore;
 import org.elasticsearch.index.translog.TranslogConfig;
-import org.elasticsearch.index.translog.TranslogService;
 import org.elasticsearch.indices.IndicesWarmer;
 import org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService;
 import org.elasticsearch.indices.cache.request.IndicesRequestCache;
@@ -259,11 +258,9 @@ public class ClusterModule extends AbstractModule {
         registerIndexDynamicSetting(MergePolicyConfig.INDEX_MERGE_POLICY_SEGMENTS_PER_TIER, Validator.DOUBLE_GTE_2);
         registerIndexDynamicSetting(MergePolicyConfig.INDEX_MERGE_POLICY_RECLAIM_DELETES_WEIGHT, Validator.NON_NEGATIVE_DOUBLE);
         registerIndexDynamicSetting(MergePolicyConfig.INDEX_COMPOUND_FORMAT, Validator.EMPTY);
-        registerIndexDynamicSetting(TranslogService.INDEX_TRANSLOG_FLUSH_INTERVAL, Validator.TIME);
-        registerIndexDynamicSetting(TranslogService.INDEX_TRANSLOG_FLUSH_THRESHOLD_OPS, Validator.INTEGER);
-        registerIndexDynamicSetting(TranslogService.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE, Validator.BYTES_SIZE);
-        registerIndexDynamicSetting(TranslogService.INDEX_TRANSLOG_FLUSH_THRESHOLD_PERIOD, Validator.TIME);
-        registerIndexDynamicSetting(TranslogService.INDEX_TRANSLOG_DISABLE_FLUSH, Validator.EMPTY);
+        registerIndexDynamicSetting(IndexShard.INDEX_TRANSLOG_FLUSH_THRESHOLD_OPS, Validator.INTEGER);
+        registerIndexDynamicSetting(IndexShard.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE, Validator.BYTES_SIZE);
+        registerIndexDynamicSetting(IndexShard.INDEX_TRANSLOG_DISABLE_FLUSH, Validator.EMPTY);
         registerIndexDynamicSetting(TranslogConfig.INDEX_TRANSLOG_DURABILITY, Validator.EMPTY);
         registerIndexDynamicSetting(IndicesWarmer.INDEX_WARMER_ENABLED, Validator.EMPTY);
         registerIndexDynamicSetting(IndicesRequestCache.INDEX_CACHE_REQUEST_ENABLED, Validator.BOOLEAN);
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterStateNonMasterUpdateTask.java b/core/src/main/java/org/elasticsearch/cluster/ClusterStateNonMasterUpdateTask.java
deleted file mode 100644
index 48afbb8..0000000
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterStateNonMasterUpdateTask.java
+++ /dev/null
@@ -1,32 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cluster;
-
-/**
- * This is a marker interface to indicate that the task should be executed
- * even if the current node is not a master.
- */
-public abstract class ClusterStateNonMasterUpdateTask extends ClusterStateUpdateTask {
-
-    @Override
-    public boolean runOnlyOnMaster() {
-        return false;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterStateUpdateTask.java b/core/src/main/java/org/elasticsearch/cluster/ClusterStateUpdateTask.java
index e1a17c2..c0f1438 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterStateUpdateTask.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterStateUpdateTask.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.cluster;
 
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;
 
 /**
@@ -51,4 +53,22 @@ abstract public class ClusterStateUpdateTask {
     public void onNoLongerMaster(String source) {
         onFailure(source, new EsRejectedExecutionException("no longer master. source: [" + source + "]"));
     }
+
+    /**
+     * Called when the result of the {@link #execute(ClusterState)} have been processed
+     * properly by all listeners.
+     */
+    public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {
+    }
+
+    /**
+     * If the cluster state update task wasn't processed by the provided timeout, call
+     * {@link #onFailure(String, Throwable)}. May return null to indicate no timeout is needed (default).
+     */
+    @Nullable
+    public TimeValue timeout() {
+        return null;
+    }
+
+
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/DiffableUtils.java b/core/src/main/java/org/elasticsearch/cluster/DiffableUtils.java
index ce583e3..84e0021 100644
--- a/core/src/main/java/org/elasticsearch/cluster/DiffableUtils.java
+++ b/core/src/main/java/org/elasticsearch/cluster/DiffableUtils.java
@@ -21,7 +21,7 @@ package org.elasticsearch.cluster;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -45,11 +45,11 @@ public final class DiffableUtils {
     }
 
     /**
-     * Calculates diff between two ImmutableMaps of Diffable objects
+     * Calculates diff between two Maps of Diffable objects.
      */
-    public static <T extends Diffable<T>> Diff<ImmutableMap<String, T>> diff(ImmutableMap<String, T> before, ImmutableMap<String, T> after) {
+    public static <T extends Diffable<T>> Diff<Map<String, T>> diff(Map<String, T> before, Map<String, T> after) {
         assert after != null && before != null;
-        return new ImmutableMapDiff<>(before, after);
+        return new JdkMapDiff<>(before, after);
     }
 
     /**
@@ -60,10 +60,10 @@ public final class DiffableUtils {
     }
 
     /**
-     * Loads an object that represents difference between two ImmutableMaps
+     * Loads an object that represents difference between two Maps.
      */
-    public static <T extends Diffable<T>> Diff<ImmutableMap<String, T>> readImmutableMapDiff(StreamInput in, KeyedReader<T> keyedReader) throws IOException {
-        return new ImmutableMapDiff<>(in, keyedReader);
+    public static <T extends Diffable<T>> Diff<Map<String, T>> readJdkMapDiff(StreamInput in, KeyedReader<T> keyedReader) throws IOException {
+        return new JdkMapDiff<>(in, keyedReader);
     }
 
     /**
@@ -74,10 +74,10 @@ public final class DiffableUtils {
     }
 
     /**
-     * Loads an object that represents difference between two ImmutableMaps
+     * Loads an object that represents difference between two Maps.
      */
-    public static <T extends Diffable<T>> Diff<ImmutableMap<String, T>> readImmutableMapDiff(StreamInput in, T proto) throws IOException {
-        return new ImmutableMapDiff<>(in, new PrototypeReader<>(proto));
+    public static <T extends Diffable<T>> Diff<Map<String, T>> readJdkMapDiff(StreamInput in, T proto) throws IOException {
+        return new JdkMapDiff<>(in, new PrototypeReader<>(proto));
     }
 
     /**
@@ -121,24 +121,24 @@ public final class DiffableUtils {
     }
 
     /**
-     * Represents differences between two ImmutableMaps of diffable objects
+     * Represents differences between two Maps of Diffable objects.
      *
      * @param <T> the diffable object
      */
-    private static class ImmutableMapDiff<T extends Diffable<T>> extends MapDiff<T, ImmutableMap<String, T>> {
+    private static class JdkMapDiff<T extends Diffable<T>> extends MapDiff<T, Map<String, T>> {
 
-        protected ImmutableMapDiff(StreamInput in, KeyedReader<T> reader) throws IOException {
+        protected JdkMapDiff(StreamInput in, KeyedReader<T> reader) throws IOException {
             super(in, reader);
         }
 
-        public ImmutableMapDiff(ImmutableMap<String, T> before, ImmutableMap<String, T> after) {
+        public JdkMapDiff(Map<String, T> before, Map<String, T> after) {
             assert after != null && before != null;
             for (String key : before.keySet()) {
                 if (!after.containsKey(key)) {
                     deletes.add(key);
                 }
             }
-            for (ImmutableMap.Entry<String, T> partIter : after.entrySet()) {
+            for (Map.Entry<String, T> partIter : after.entrySet()) {
                 T beforePart = before.get(partIter.getKey());
                 if (beforePart == null) {
                     adds.put(partIter.getKey(), partIter.getValue());
@@ -149,8 +149,8 @@ public final class DiffableUtils {
         }
 
         @Override
-        public ImmutableMap<String, T> apply(ImmutableMap<String, T> map) {
-            HashMap<String, T> builder = new HashMap<>();
+        public Map<String, T> apply(Map<String, T> map) {
+            Map<String, T> builder = new HashMap<>();
             builder.putAll(map);
 
             for (String part : deletes) {
@@ -164,7 +164,7 @@ public final class DiffableUtils {
             for (Map.Entry<String, T> additon : adds.entrySet()) {
                 builder.put(additon.getKey(), additon.getValue());
             }
-            return ImmutableMap.copyOf(builder);
+            return builder;
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/ProcessedClusterStateNonMasterUpdateTask.java b/core/src/main/java/org/elasticsearch/cluster/ProcessedClusterStateNonMasterUpdateTask.java
deleted file mode 100644
index 4af05b4..0000000
--- a/core/src/main/java/org/elasticsearch/cluster/ProcessedClusterStateNonMasterUpdateTask.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.cluster;
-
-/**
- * A combination between {@link org.elasticsearch.cluster.ProcessedClusterStateUpdateTask} and
- * {@link org.elasticsearch.cluster.ClusterStateNonMasterUpdateTask} to allow easy creation of anonymous classes
- */
-abstract public class ProcessedClusterStateNonMasterUpdateTask extends ProcessedClusterStateUpdateTask {
-
-    @Override
-    public boolean runOnlyOnMaster() {
-        return false;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/cluster/ProcessedClusterStateUpdateTask.java b/core/src/main/java/org/elasticsearch/cluster/ProcessedClusterStateUpdateTask.java
deleted file mode 100644
index 2d703ed..0000000
--- a/core/src/main/java/org/elasticsearch/cluster/ProcessedClusterStateUpdateTask.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cluster;
-
-/**
- * An extension interface to {@link ClusterStateUpdateTask} that allows to be notified when
- * the cluster state update has been processed.
- */
-public abstract class ProcessedClusterStateUpdateTask extends ClusterStateUpdateTask {
-
-    /**
-     * Called when the result of the {@link #execute(ClusterState)} have been processed
-     * properly by all listeners.
-     */
-    public abstract void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState);
-}
diff --git a/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java b/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java
index 5776fe6..82ba28d 100644
--- a/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java
+++ b/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java
@@ -112,7 +112,7 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
     public static class Entry {
         private final State state;
         private final SnapshotId snapshotId;
-        private final ImmutableMap<ShardId, ShardRestoreStatus> shards;
+        private final Map<ShardId, ShardRestoreStatus> shards;
         private final List<String> indices;
 
         /**
@@ -148,7 +148,7 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
          *
          * @return list of shards
          */
-        public ImmutableMap<ShardId, ShardRestoreStatus> shards() {
+        public Map<ShardId, ShardRestoreStatus> shards() {
             return this.shards;
         }
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java b/core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java
index 3385cfc..83c663a 100644
--- a/core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java
+++ b/core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java
@@ -38,6 +38,8 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * Meta data about snapshots that are currently executing
  */
@@ -67,12 +69,12 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
         private final State state;
         private final SnapshotId snapshotId;
         private final boolean includeGlobalState;
-        private final ImmutableMap<ShardId, ShardSnapshotStatus> shards;
+        private final Map<ShardId, ShardSnapshotStatus> shards;
         private final List<String> indices;
-        private final ImmutableMap<String, List<ShardId>> waitingIndices;
+        private final Map<String, List<ShardId>> waitingIndices;
         private final long startTime;
 
-        public Entry(SnapshotId snapshotId, boolean includeGlobalState, State state, List<String> indices, long startTime, ImmutableMap<ShardId, ShardSnapshotStatus> shards) {
+        public Entry(SnapshotId snapshotId, boolean includeGlobalState, State state, List<String> indices, long startTime, Map<ShardId, ShardSnapshotStatus> shards) {
             this.state = state;
             this.snapshotId = snapshotId;
             this.includeGlobalState = includeGlobalState;
@@ -82,16 +84,16 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
                 this.shards = ImmutableMap.of();
                 this.waitingIndices = ImmutableMap.of();
             } else {
-                this.shards = shards;
+                this.shards = unmodifiableMap(shards);
                 this.waitingIndices = findWaitingIndices(shards);
             }
         }
 
-        public Entry(Entry entry, State state, ImmutableMap<ShardId, ShardSnapshotStatus> shards) {
+        public Entry(Entry entry, State state, Map<ShardId, ShardSnapshotStatus> shards) {
             this(entry.snapshotId, entry.includeGlobalState, state, entry.indices, entry.startTime, shards);
         }
 
-        public Entry(Entry entry, ImmutableMap<ShardId, ShardSnapshotStatus> shards) {
+        public Entry(Entry entry, Map<ShardId, ShardSnapshotStatus> shards) {
             this(entry, entry.state, shards);
         }
 
@@ -99,7 +101,7 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
             return this.snapshotId;
         }
 
-        public ImmutableMap<ShardId, ShardSnapshotStatus> shards() {
+        public Map<ShardId, ShardSnapshotStatus> shards() {
             return this.shards;
         }
 
@@ -111,7 +113,7 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
             return indices;
         }
 
-        public ImmutableMap<String, List<ShardId>> waitingIndices() {
+        public Map<String, List<ShardId>> waitingIndices() {
             return waitingIndices;
         }
 
@@ -153,7 +155,7 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
             return result;
         }
 
-        private ImmutableMap<String, List<ShardId>> findWaitingIndices(ImmutableMap<ShardId, ShardSnapshotStatus> shards) {
+        private ImmutableMap<String, List<ShardId>> findWaitingIndices(Map<ShardId, ShardSnapshotStatus> shards) {
             Map<String, List<ShardId>> waitingIndicesMap = new HashMap<>();
             for (ImmutableMap.Entry<ShardId, ShardSnapshotStatus> entry : shards.entrySet()) {
                 if (entry.getValue().state() == State.WAITING) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/TimeoutClusterStateUpdateTask.java b/core/src/main/java/org/elasticsearch/cluster/TimeoutClusterStateUpdateTask.java
deleted file mode 100644
index 1ae767c..0000000
--- a/core/src/main/java/org/elasticsearch/cluster/TimeoutClusterStateUpdateTask.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cluster;
-
-import org.elasticsearch.common.unit.TimeValue;
-
-/**
- * An extension interface to {@link org.elasticsearch.cluster.ClusterStateUpdateTask} that allows to associate
- * a timeout.
- */
-abstract public class TimeoutClusterStateUpdateTask extends ProcessedClusterStateUpdateTask {
-
-    /**
-     * If the cluster state update task wasn't processed by the provided timeout, call
-     * {@link #onFailure(String, Throwable)}
-     */
-    abstract public TimeValue timeout();
-}
diff --git a/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java b/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java
index 62fb061..02867c9 100644
--- a/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java
+++ b/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java
@@ -23,11 +23,10 @@ import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.ClusterStateUpdateTask;
-import org.elasticsearch.cluster.ProcessedClusterStateUpdateTask;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.routing.*;
+import org.elasticsearch.cluster.routing.RoutingService;
+import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.allocation.AllocationService;
 import org.elasticsearch.cluster.routing.allocation.FailedRerouteAllocation;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
@@ -37,7 +36,6 @@ import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.threadpool.ThreadPool;
@@ -129,7 +127,7 @@ public class ShardStateAction extends AbstractComponent {
     private void handleShardFailureOnMaster(final ShardRoutingEntry shardRoutingEntry) {
         logger.warn("{} received shard failed for {}", shardRoutingEntry.failure, shardRoutingEntry.shardRouting.shardId(), shardRoutingEntry);
         failedShardQueue.add(shardRoutingEntry);
-        clusterService.submitStateUpdateTask("shard-failed (" + shardRoutingEntry.shardRouting + "), message [" + shardRoutingEntry.message + "]", Priority.HIGH, new ProcessedClusterStateUpdateTask() {
+        clusterService.submitStateUpdateTask("shard-failed (" + shardRoutingEntry.shardRouting + "), message [" + shardRoutingEntry.message + "]", Priority.HIGH, new ClusterStateUpdateTask() {
 
             @Override
             public ClusterState execute(ClusterState currentState) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/block/ClusterBlocks.java b/core/src/main/java/org/elasticsearch/cluster/block/ClusterBlocks.java
index 8d902f6..2352a36 100644
--- a/core/src/main/java/org/elasticsearch/cluster/block/ClusterBlocks.java
+++ b/core/src/main/java/org/elasticsearch/cluster/block/ClusterBlocks.java
@@ -45,11 +45,11 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
 
     private final ImmutableSet<ClusterBlock> global;
 
-    private final ImmutableMap<String, ImmutableSet<ClusterBlock>> indicesBlocks;
+    private final Map<String, ImmutableSet<ClusterBlock>> indicesBlocks;
 
     private final ImmutableLevelHolder[] levelHolders;
 
-    ClusterBlocks(ImmutableSet<ClusterBlock> global, ImmutableMap<String, ImmutableSet<ClusterBlock>> indicesBlocks) {
+    ClusterBlocks(ImmutableSet<ClusterBlock> global, Map<String, ImmutableSet<ClusterBlock>> indicesBlocks) {
         this.global = global;
         this.indicesBlocks = indicesBlocks;
 
@@ -83,7 +83,7 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
         return global;
     }
 
-    public ImmutableMap<String, ImmutableSet<ClusterBlock>> indices() {
+    public Map<String, ImmutableSet<ClusterBlock>> indices() {
         return indicesBlocks;
     }
 
@@ -91,7 +91,7 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
         return levelHolders[level.id()].global();
     }
 
-    public ImmutableMap<String, ImmutableSet<ClusterBlock>> indices(ClusterBlockLevel level) {
+    public Map<String, ImmutableSet<ClusterBlock>> indices(ClusterBlockLevel level) {
         return levelHolders[level.id()].indices();
     }
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
index 5d2fe42..6ea1d0e 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
@@ -21,7 +21,6 @@ package org.elasticsearch.cluster.metadata;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.Diff;
@@ -41,11 +40,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.settings.loader.SettingsLoader;
-import org.elasticsearch.common.xcontent.FromXContentBuilder;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
@@ -61,14 +56,12 @@ import java.util.Map;
 
 import static org.elasticsearch.cluster.node.DiscoveryNodeFilters.OpType.AND;
 import static org.elasticsearch.cluster.node.DiscoveryNodeFilters.OpType.OR;
-import static org.elasticsearch.common.settings.Settings.readSettingsFromStream;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.settings.Settings.writeSettingsToStream;
+import static org.elasticsearch.common.settings.Settings.*;
 
 /**
  *
  */
-public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuilder<IndexMetaData>, ToXContent {
+public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuilder<IndexMetaData>, ToXContent  {
 
     public static final IndexMetaData PROTO = IndexMetaData.builder("")
             .settings(Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT))
@@ -531,8 +524,7 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
     }
 
     @Override
-    public IndexMetaData fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
-            throws IOException {
+    public IndexMetaData fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
         return Builder.fromXContent(parser);
     }
 
@@ -713,7 +705,7 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
         public int numberOfReplicas() {
             return settings.getAsInt(SETTING_NUMBER_OF_REPLICAS, -1);
         }
-
+        
         public Builder creationDate(long creationDate) {
             settings = settingsBuilder().put(settings).put(SETTING_CREATION_DATE, creationDate).build();
             return this;
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java
index 10bc321..d7b2e47 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java
@@ -22,7 +22,7 @@ package org.elasticsearch.cluster.metadata;
 import org.elasticsearch.action.support.master.MasterNodeRequest;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.TimeoutClusterStateUpdateTask;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.cluster.action.index.NodeIndexDeletedAction;
 import org.elasticsearch.cluster.block.ClusterBlocks;
 import org.elasticsearch.cluster.routing.RoutingTable;
@@ -100,7 +100,7 @@ public class MetaDataDeleteIndexService extends AbstractComponent {
 
     private void deleteIndex(final Request request, final Listener userListener, Semaphore mdLock) {
         final DeleteIndexListener listener = new DeleteIndexListener(mdLock, userListener);
-        clusterService.submitStateUpdateTask("delete-index [" + request.index + "]", Priority.URGENT, new TimeoutClusterStateUpdateTask() {
+        clusterService.submitStateUpdateTask("delete-index [" + request.index + "]", Priority.URGENT, new ClusterStateUpdateTask() {
 
             @Override
             public TimeValue timeout() {
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java
index f5b163c..13823e8 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java
@@ -23,7 +23,7 @@ import org.elasticsearch.action.admin.indices.alias.Alias;
 import org.elasticsearch.action.support.master.MasterNodeRequest;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.TimeoutClusterStateUpdateTask;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.ValidationException;
@@ -36,13 +36,7 @@ import org.elasticsearch.indices.IndexTemplateAlreadyExistsException;
 import org.elasticsearch.indices.IndexTemplateMissingException;
 import org.elasticsearch.indices.InvalidIndexTemplateException;
 
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Set;
+import java.util.*;
 
 /**
  * Service responsible for submitting index templates updates
@@ -62,7 +56,7 @@ public class MetaDataIndexTemplateService extends AbstractComponent {
     }
 
     public void removeTemplates(final RemoveRequest request, final RemoveListener listener) {
-        clusterService.submitStateUpdateTask("remove-index-template [" + request.name + "]", Priority.URGENT, new TimeoutClusterStateUpdateTask() {
+        clusterService.submitStateUpdateTask("remove-index-template [" + request.name + "]", Priority.URGENT, new ClusterStateUpdateTask() {
 
             @Override
             public TimeValue timeout() {
@@ -149,7 +143,7 @@ public class MetaDataIndexTemplateService extends AbstractComponent {
         }
         final IndexTemplateMetaData template = templateBuilder.build();
 
-        clusterService.submitStateUpdateTask("create-index-template [" + request.name + "], cause [" + request.cause + "]", Priority.URGENT, new TimeoutClusterStateUpdateTask() {
+        clusterService.submitStateUpdateTask("create-index-template [" + request.name + "], cause [" + request.cause + "]", Priority.URGENT, new ClusterStateUpdateTask() {
 
             @Override
             public TimeValue timeout() {
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java
index c8b78b0..c9662e7 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java
@@ -25,7 +25,7 @@ import org.elasticsearch.action.admin.indices.mapping.put.PutMappingClusterState
 import org.elasticsearch.cluster.AckedClusterStateUpdateTask;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ProcessedClusterStateUpdateTask;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.collect.Tuple;
@@ -43,14 +43,7 @@ import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.indices.InvalidTypeNameException;
 import org.elasticsearch.percolator.PercolatorService;
 
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
+import java.util.*;
 /**
  * Service responsible for submitting mapping changes
  */
@@ -301,7 +294,7 @@ public class MetaDataMappingService extends AbstractComponent {
             insertOrder = ++refreshOrUpdateInsertOrder;
             refreshOrUpdateQueue.add(new RefreshTask(index, indexUUID, types));
         }
-        clusterService.submitStateUpdateTask("refresh-mapping [" + index + "][" + Arrays.toString(types) + "]", Priority.HIGH, new ProcessedClusterStateUpdateTask() {
+        clusterService.submitStateUpdateTask("refresh-mapping [" + index + "][" + Arrays.toString(types) + "]", Priority.HIGH, new ClusterStateUpdateTask() {
             private volatile List<MappingTask> allTasks;
 
             @Override
diff --git a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
index ca1ccbd..ebf1bcb 100644
--- a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
+++ b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
@@ -100,7 +100,7 @@ public class DiscoveryNode implements Streamable, ToXContent {
     private String hostName;
     private String hostAddress;
     private TransportAddress address;
-    private ImmutableMap<String, String> attributes;
+    private Map<String, String> attributes;
     private Version version = Version.CURRENT;
 
     DiscoveryNode() {
@@ -120,7 +120,7 @@ public class DiscoveryNode implements Streamable, ToXContent {
      * @param version the version of the node.
      */
     public DiscoveryNode(String nodeId, TransportAddress address, Version version) {
-        this("", nodeId, address, ImmutableMap.<String, String>of(), version);
+        this("", nodeId, address, Collections.emptyMap(), version);
     }
 
     /**
@@ -230,14 +230,14 @@ public class DiscoveryNode implements Streamable, ToXContent {
     /**
      * The node attributes.
      */
-    public ImmutableMap<String, String> attributes() {
+    public Map<String, String> attributes() {
         return this.attributes;
     }
 
     /**
      * The node attributes.
      */
-    public ImmutableMap<String, String> getAttributes() {
+    public Map<String, String> getAttributes() {
         return attributes();
     }
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
index 32c50b3..7a8c33e 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
@@ -21,6 +21,7 @@ package org.elasticsearch.cluster.routing;
 
 import com.carrotsearch.hppc.IntSet;
 import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.cluster.Diff;
 import org.elasticsearch.cluster.Diffable;
 import org.elasticsearch.cluster.DiffableUtils;
@@ -32,9 +33,16 @@ import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.index.IndexNotFoundException;
 
 import java.io.IOException;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
 import java.util.function.Predicate;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * Represents a global cluster-wide routing table for all indices including the
  * version of the current routing state.
@@ -50,11 +58,11 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
     private final long version;
 
     // index to IndexRoutingTable map
-    private final ImmutableMap<String, IndexRoutingTable> indicesRouting;
+    private final Map<String, IndexRoutingTable> indicesRouting;
 
     RoutingTable(long version, Map<String, IndexRoutingTable> indicesRouting) {
         this.version = version;
-        this.indicesRouting = ImmutableMap.copyOf(indicesRouting);
+        this.indicesRouting = unmodifiableMap(indicesRouting);
     }
 
     /**
@@ -304,7 +312,7 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
 
         private final long version;
 
-        private final Diff<ImmutableMap<String, IndexRoutingTable>> indicesRouting;
+        private final Diff<Map<String, IndexRoutingTable>> indicesRouting;
 
         public RoutingTableDiff(RoutingTable before, RoutingTable after) {
             version = after.version;
@@ -313,7 +321,7 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
 
         public RoutingTableDiff(StreamInput in) throws IOException {
             version = in.readLong();
-            indicesRouting = DiffableUtils.readImmutableMapDiff(in, IndexRoutingTable.PROTO);
+            indicesRouting = DiffableUtils.readJdkMapDiff(in, IndexRoutingTable.PROTO);
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java b/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
index 2b8a180..74bfac0 100644
--- a/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
@@ -41,8 +41,8 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.text.StringText;
 import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.common.util.concurrent.*;
+import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.discovery.Discovery;
 import org.elasticsearch.discovery.DiscoveryService;
 import org.elasticsearch.node.settings.NodeSettingsService;
@@ -275,15 +275,14 @@ public class InternalClusterService extends AbstractLifecycleComponent<ClusterSe
         }
         try {
             final UpdateTask task = new UpdateTask(source, priority, updateTask);
-            if (updateTask instanceof TimeoutClusterStateUpdateTask) {
-                final TimeoutClusterStateUpdateTask timeoutUpdateTask = (TimeoutClusterStateUpdateTask) updateTask;
-                updateTasksExecutor.execute(task, threadPool.scheduler(), timeoutUpdateTask.timeout(), new Runnable() {
+            if (updateTask.timeout() != null) {
+                updateTasksExecutor.execute(task, threadPool.scheduler(), updateTask.timeout(), new Runnable() {
                     @Override
                     public void run() {
                         threadPool.generic().execute(new Runnable() {
                             @Override
                             public void run() {
-                                timeoutUpdateTask.onFailure(task.source(), new ProcessClusterEventTimeoutException(timeoutUpdateTask.timeout(), task.source()));
+                                updateTask.onFailure(task.source(), new ProcessClusterEventTimeoutException(updateTask.timeout(), task.source()));
                             }
                         });
                     }
@@ -401,9 +400,7 @@ public class InternalClusterService extends AbstractLifecycleComponent<ClusterSe
                     //no need to wait for ack if nothing changed, the update can be counted as acknowledged
                     ((AckedClusterStateUpdateTask) updateTask).onAllNodesAcked(null);
                 }
-                if (updateTask instanceof ProcessedClusterStateUpdateTask) {
-                    ((ProcessedClusterStateUpdateTask) updateTask).clusterStateProcessed(source, previousClusterState, newClusterState);
-                }
+                updateTask.clusterStateProcessed(source, previousClusterState, newClusterState);
                 TimeValue executionTime = TimeValue.timeValueMillis(Math.max(0, TimeValue.nsecToMSec(System.nanoTime() - startTimeNS)));
                 logger.debug("processing [{}]: took {} no change in cluster_state", source, executionTime);
                 warnAboutSlowTaskIfNeeded(executionTime, source);
@@ -526,9 +523,7 @@ public class InternalClusterService extends AbstractLifecycleComponent<ClusterSe
                     }
                 }
 
-                if (updateTask instanceof ProcessedClusterStateUpdateTask) {
-                    ((ProcessedClusterStateUpdateTask) updateTask).clusterStateProcessed(source, previousClusterState, newClusterState);
-                }
+                updateTask.clusterStateProcessed(source, previousClusterState, newClusterState);
 
                 TimeValue executionTime = TimeValue.timeValueMillis(Math.max(0, TimeValue.nsecToMSec(System.nanoTime() - startTimeNS)));
                 logger.debug("processing [{}]: took {} done applying updated cluster_state (version: {}, uuid: {})", source, executionTime, newClusterState.version(), newClusterState.stateUUID());
diff --git a/core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java b/core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java
index 1477179..add383b 100644
--- a/core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java
+++ b/core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java
@@ -311,10 +311,6 @@ public class PagedBytesReference implements BytesReference {
             return true;
         }
 
-        if (obj == null) {
-            return false;
-        }
-
         if (!(obj instanceof PagedBytesReference)) {
             return BytesReference.Helper.bytesEqual(this, (BytesReference) obj);
         }
diff --git a/core/src/main/java/org/elasticsearch/common/collect/CopyOnWriteHashSet.java b/core/src/main/java/org/elasticsearch/common/collect/CopyOnWriteHashSet.java
deleted file mode 100644
index 3e2f1dd..0000000
--- a/core/src/main/java/org/elasticsearch/common/collect/CopyOnWriteHashSet.java
+++ /dev/null
@@ -1,109 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.collect;
-
-import com.google.common.collect.ForwardingSet;
-
-import java.util.AbstractMap;
-import java.util.Collection;
-import java.util.Set;
-
-/**
- * {@link Set} implementation based on {@link CopyOnWriteHashMap}.
- * Null values are not supported.
- */
-public class CopyOnWriteHashSet<T> extends ForwardingSet<T> {
-
-    /**
-     * Return a copy of the provided set.
-     */
-    public static <T> CopyOnWriteHashSet<T> copyOf(Collection<? extends T> set) {
-        if (set instanceof CopyOnWriteHashSet) {
-            // no need to copy in that case
-            @SuppressWarnings("unchecked")
-            final CopyOnWriteHashSet<T> cowSet = (CopyOnWriteHashSet<T>) set;
-            return cowSet;
-        } else {
-            return new CopyOnWriteHashSet<T>().copyAndAddAll(set);
-        }
-    }
-
-    private final CopyOnWriteHashMap<T, Boolean> map;
-
-    /** Create a new empty set. */
-    public CopyOnWriteHashSet() {
-        this(new CopyOnWriteHashMap<T, Boolean>());
-    }
-
-    private CopyOnWriteHashSet(CopyOnWriteHashMap<T, Boolean> map) {
-        this.map = map;
-    }
-
-    @Override
-    protected Set<T> delegate() {
-        return map.keySet();
-    }
-
-    /**
-     * Copy the current set and return a copy that contains or replaces <code>entry</code>.
-     */
-    public CopyOnWriteHashSet<T> copyAndAdd(T entry) {
-        return new CopyOnWriteHashSet<>(map.copyAndPut(entry, true));
-    }
-
-    /**
-     * Copy the current set and return a copy that is the union of the current
-     * set and <code>entries</code>, potentially replacing existing entries in
-     * case of equality.
-     */
-    public CopyOnWriteHashSet<T> copyAndAddAll(Collection<? extends T> entries) {
-        CopyOnWriteHashMap<T, Boolean> updated = this.map.copyAndPutAll(entries.stream().map(
-                p -> new AbstractMap.SimpleImmutableEntry<T, Boolean>(p, true)
-        ));
-        return new CopyOnWriteHashSet<>(updated);
-    }
-
-    /**
-     * Copy the current set and return a copy that removes <code>entry</code>
-     * if it exists.
-     */
-    public CopyOnWriteHashSet<T> copyAndRemove(Object entry) {
-        final CopyOnWriteHashMap<T, Boolean> updated = map.copyAndRemove(entry);
-        if (updated == map) {
-            return this;
-        } else {
-            return new CopyOnWriteHashSet<>(updated);
-        }
-    }
-
-    /**
-     * Copy the current set and return a copy that is the difference of the current
-     * set and <code>entries</code>.
-     */
-    public CopyOnWriteHashSet<T> copyAndRemoveAll(Collection<?> entries) {
-        CopyOnWriteHashMap<T, Boolean> updated = this.map.copyAndRemoveAll(entries);
-        if (updated == map) {
-            return this;
-        } else {
-            return new CopyOnWriteHashSet<>(updated);
-        }
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java b/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java
index 685073b..29f7886 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java
@@ -51,7 +51,7 @@ public final class PrivateElementsImpl implements PrivateElements {
     /**
      * lazily instantiated
      */
-    private ImmutableMap<Key<?>, Object> exposedKeysToSources;
+    private Map<Key<?>, Object> exposedKeysToSources;
     private Injector injector;
 
     public PrivateElementsImpl(Object source) {
diff --git a/core/src/main/java/org/elasticsearch/common/transport/BoundTransportAddress.java b/core/src/main/java/org/elasticsearch/common/transport/BoundTransportAddress.java
index f3bbeb0..28e4506 100644
--- a/core/src/main/java/org/elasticsearch/common/transport/BoundTransportAddress.java
+++ b/core/src/main/java/org/elasticsearch/common/transport/BoundTransportAddress.java
@@ -26,28 +26,31 @@ import org.elasticsearch.common.io.stream.Streamable;
 import java.io.IOException;
 
 /**
- * A bounded transport address is a tuple of two {@link TransportAddress}, one that represents
- * the address the transport is bounded on, the the published one represents the one clients should
- * communicate on.
+ * A bounded transport address is a tuple of {@link TransportAddress}, one array that represents
+ * the addresses the transport is bound to, and the other is the published one that represents the address clients
+ * should communicate on.
  *
  *
  */
 public class BoundTransportAddress implements Streamable {
 
-    private TransportAddress boundAddress;
+    private TransportAddress[] boundAddresses;
 
     private TransportAddress publishAddress;
 
     BoundTransportAddress() {
     }
 
-    public BoundTransportAddress(TransportAddress boundAddress, TransportAddress publishAddress) {
-        this.boundAddress = boundAddress;
+    public BoundTransportAddress(TransportAddress[] boundAddresses, TransportAddress publishAddress) {
+        if (boundAddresses == null || boundAddresses.length < 1) {
+            throw new IllegalArgumentException("at least one bound address must be provided");
+        }
+        this.boundAddresses = boundAddresses;
         this.publishAddress = publishAddress;
     }
 
-    public TransportAddress boundAddress() {
-        return boundAddress;
+    public TransportAddress[] boundAddresses() {
+        return boundAddresses;
     }
 
     public TransportAddress publishAddress() {
@@ -62,18 +65,38 @@ public class BoundTransportAddress implements Streamable {
 
     @Override
     public void readFrom(StreamInput in) throws IOException {
-        boundAddress = TransportAddressSerializers.addressFromStream(in);
+        int boundAddressLength = in.readInt();
+        boundAddresses = new TransportAddress[boundAddressLength];
+        for (int i = 0; i < boundAddressLength; i++) {
+            boundAddresses[i] = TransportAddressSerializers.addressFromStream(in);
+        }
         publishAddress = TransportAddressSerializers.addressFromStream(in);
     }
 
     @Override
     public void writeTo(StreamOutput out) throws IOException {
-        TransportAddressSerializers.addressToStream(out, boundAddress);
+        out.writeInt(boundAddresses.length);
+        for (TransportAddress address : boundAddresses) {
+            TransportAddressSerializers.addressToStream(out, address);
+        }
         TransportAddressSerializers.addressToStream(out, publishAddress);
     }
 
     @Override
     public String toString() {
-        return "bound_address {" + boundAddress + "}, publish_address {" + publishAddress + "}";
+        StringBuilder builder = new StringBuilder("publish_address {");
+        builder.append(publishAddress);
+        builder.append("}, bound_addresses ");
+        boolean firstAdded = false;
+        for (TransportAddress address : boundAddresses) {
+            if (firstAdded) {
+                builder.append(", ");
+            } else {
+                firstAdded = true;
+            }
+
+            builder.append("{").append(address).append("}");
+        }
+        return builder.toString();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java b/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java
index a84ee83..a17ef93 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java
@@ -21,7 +21,6 @@ package org.elasticsearch.common.xcontent;
 
 import org.elasticsearch.common.bytes.BytesReference;
 
-import java.io.Closeable;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.OutputStream;
@@ -29,7 +28,7 @@ import java.io.OutputStream;
 /**
  *
  */
-public interface XContentGenerator extends Closeable {
+public interface XContentGenerator {
 
     XContentType contentType();
 
diff --git a/core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java b/core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java
index 7c3d4c1..0979554 100644
--- a/core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java
+++ b/core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java
@@ -124,7 +124,13 @@ public class LocalDiscovery extends AbstractLifecycleComponent<Discovery> implem
                 // we are the first master (and the master)
                 master = true;
                 final LocalDiscovery master = firstMaster;
-                clusterService.submitStateUpdateTask("local-disco-initial_connect(master)", new ProcessedClusterStateNonMasterUpdateTask() {
+                clusterService.submitStateUpdateTask("local-disco-initial_connect(master)", new ClusterStateUpdateTask() {
+
+                    @Override
+                    public boolean runOnlyOnMaster() {
+                        return false;
+                    }
+
                     @Override
                     public ClusterState execute(ClusterState currentState) {
                         DiscoveryNodes.Builder nodesBuilder = DiscoveryNodes.builder();
@@ -150,7 +156,12 @@ public class LocalDiscovery extends AbstractLifecycleComponent<Discovery> implem
             } else if (firstMaster != null) {
                 // update as fast as we can the local node state with the new metadata (so we create indices for example)
                 final ClusterState masterState = firstMaster.clusterService.state();
-                clusterService.submitStateUpdateTask("local-disco(detected_master)", new ClusterStateNonMasterUpdateTask() {
+                clusterService.submitStateUpdateTask("local-disco(detected_master)", new ClusterStateUpdateTask() {
+                    @Override
+                    public boolean runOnlyOnMaster() {
+                        return false;
+                    }
+
                     @Override
                     public ClusterState execute(ClusterState currentState) {
                         // make sure we have the local node id set, we might need it as a result of the new metadata
@@ -166,7 +177,12 @@ public class LocalDiscovery extends AbstractLifecycleComponent<Discovery> implem
 
                 // tell the master to send the fact that we are here
                 final LocalDiscovery master = firstMaster;
-                firstMaster.clusterService.submitStateUpdateTask("local-disco-receive(from node[" + localNode + "])", new ProcessedClusterStateNonMasterUpdateTask() {
+                firstMaster.clusterService.submitStateUpdateTask("local-disco-receive(from node[" + localNode + "])", new ClusterStateUpdateTask() {
+                    @Override
+                    public boolean runOnlyOnMaster() {
+                        return false;
+                    }
+
                     @Override
                     public ClusterState execute(ClusterState currentState) {
                         DiscoveryNodes.Builder nodesBuilder = DiscoveryNodes.builder();
@@ -230,7 +246,12 @@ public class LocalDiscovery extends AbstractLifecycleComponent<Discovery> implem
                 }
 
                 final LocalDiscovery master = firstMaster;
-                master.clusterService.submitStateUpdateTask("local-disco-update", new ClusterStateNonMasterUpdateTask() {
+                master.clusterService.submitStateUpdateTask("local-disco-update", new ClusterStateUpdateTask() {
+                    @Override
+                    public boolean runOnlyOnMaster() {
+                        return false;
+                    }
+
                     @Override
                     public ClusterState execute(ClusterState currentState) {
                         DiscoveryNodes newNodes = currentState.nodes().removeDeadMembers(newMembers, master.localNode.id());
@@ -351,7 +372,12 @@ public class LocalDiscovery extends AbstractLifecycleComponent<Discovery> implem
                     assert nodeSpecificClusterState.nodes().masterNode() != null : "received a cluster state without a master";
                     assert !nodeSpecificClusterState.blocks().hasGlobalBlock(discoverySettings.getNoMasterBlock()) : "received a cluster state with a master block";
 
-                    discovery.clusterService.submitStateUpdateTask("local-disco-receive(from master)", new ProcessedClusterStateNonMasterUpdateTask() {
+                    discovery.clusterService.submitStateUpdateTask("local-disco-receive(from master)", new ClusterStateUpdateTask() {
+                        @Override
+                        public boolean runOnlyOnMaster() {
+                            return false;
+                        }
+
                         @Override
                         public ClusterState execute(ClusterState currentState) {
                             if (currentState.supersedes(nodeSpecificClusterState)) {
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java b/core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java
index 6aa6ac5..64f88d5 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java
@@ -21,8 +21,7 @@ package org.elasticsearch.discovery.zen;
 import org.elasticsearch.ElasticsearchTimeoutException;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ClusterStateNonMasterUpdateTask;
-import org.elasticsearch.cluster.ProcessedClusterStateUpdateTask;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.cluster.block.ClusterBlocks;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
@@ -133,7 +132,13 @@ public class NodeJoinController extends AbstractComponent {
 
     /** utility method to fail the given election context under the cluster state thread */
     private void failContext(final ElectionContext context, final String reason, final Throwable throwable) {
-        clusterService.submitStateUpdateTask("zen-disco-join(failure [" + reason + "])", Priority.IMMEDIATE, new ClusterStateNonMasterUpdateTask() {
+        clusterService.submitStateUpdateTask("zen-disco-join(failure [" + reason + "])", Priority.IMMEDIATE, new ClusterStateUpdateTask() {
+
+            @Override
+            public boolean runOnlyOnMaster() {
+                return false;
+            }
+
             @Override
             public ClusterState execute(ClusterState currentState) throws Exception {
                 context.onFailure(throwable);
@@ -343,7 +348,7 @@ public class NodeJoinController extends AbstractComponent {
      * Processes any pending joins via a ClusterState update task.
      * Note: this task automatically fails (and fails all pending joins) if the current node is not marked as master
      */
-    class ProcessJoinsTask extends ProcessedClusterStateUpdateTask {
+    class ProcessJoinsTask extends ClusterStateUpdateTask {
 
         private final List<MembershipAction.JoinCallback> joinCallbacksToRespondTo = new ArrayList<>();
         private boolean nodeAdded = false;
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java b/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
index 204f9e9..7255aa8 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
@@ -76,7 +76,7 @@ import static org.elasticsearch.common.unit.TimeValue.timeValueSeconds;
 public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implements Discovery, PingContextProvider {
 
     public final static String SETTING_REJOIN_ON_MASTER_GONE = "discovery.zen.rejoin_on_master_gone";
-    public final static String SETTING_PING_TIMEOUT = "discovery.zen.ping.timeout";
+    public final static String SETTING_PING_TIMEOUT = "discovery.zen.ping_timeout";
     public final static String SETTING_JOIN_TIMEOUT = "discovery.zen.join_timeout";
     public final static String SETTING_JOIN_RETRY_ATTEMPTS = "discovery.zen.join_retry_attempts";
     public final static String SETTING_JOIN_RETRY_DELAY = "discovery.zen.join_retry_delay";
@@ -150,10 +150,7 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         this.discoverySettings = discoverySettings;
         this.pingService = pingService;
         this.electMaster = electMasterService;
-        TimeValue pingTimeout = this.settings.getAsTime("discovery.zen.initial_ping_timeout", timeValueSeconds(3));
-        pingTimeout = this.settings.getAsTime("discovery.zen.ping_timeout", pingTimeout);
-        pingTimeout = settings.getAsTime("discovery.zen.ping_timeout", pingTimeout);
-        this.pingTimeout = settings.getAsTime(SETTING_PING_TIMEOUT, pingTimeout);
+        this.pingTimeout = settings.getAsTime(SETTING_PING_TIMEOUT, timeValueSeconds(3));
 
         this.joinTimeout = settings.getAsTime(SETTING_JOIN_TIMEOUT, TimeValue.timeValueMillis(this.pingTimeout.millis() * 20));
         this.joinRetryAttempts = settings.getAsInt(SETTING_JOIN_RETRY_ATTEMPTS, 3);
@@ -173,7 +170,7 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
             throw new IllegalArgumentException("'" + SETTING_MAX_PINGS_FROM_ANOTHER_MASTER + "' must be a positive number. got [" + this.maxPingsFromAnotherMaster + "]");
         }
 
-        logger.debug("using ping.timeout [{}], join.timeout [{}], master_election.filter_client [{}], master_election.filter_data [{}]", pingTimeout, joinTimeout, masterElectionFilterClientNodes, masterElectionFilterDataNodes);
+        logger.debug("using ping_timeout [{}], join.timeout [{}], master_election.filter_client [{}], master_election.filter_data [{}]", this.pingTimeout, joinTimeout, masterElectionFilterClientNodes, masterElectionFilterDataNodes);
 
         nodeSettingsService.addListener(new ApplySettings());
 
@@ -210,7 +207,13 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         this.nodeJoinController = new NodeJoinController(clusterService, routingService, discoverySettings, settings);
 
         // start the join thread from a cluster state update. See {@link JoinThreadControl} for details.
-        clusterService.submitStateUpdateTask("initial_join", new ClusterStateNonMasterUpdateTask() {
+        clusterService.submitStateUpdateTask("initial_join", new ClusterStateUpdateTask() {
+
+            @Override
+            public boolean runOnlyOnMaster() {
+                return false;
+            }
+
             @Override
             public ClusterState execute(ClusterState currentState) throws Exception {
                 // do the join on a different thread, the DiscoveryService waits for 30s anyhow till it is discovered
@@ -397,7 +400,12 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
 
             // finalize join through the cluster state update thread
             final DiscoveryNode finalMasterNode = masterNode;
-            clusterService.submitStateUpdateTask("finalize_join (" + masterNode + ")", new ClusterStateNonMasterUpdateTask() {
+            clusterService.submitStateUpdateTask("finalize_join (" + masterNode + ")", new ClusterStateUpdateTask() {
+                @Override
+                public boolean runOnlyOnMaster() {
+                    return false;
+                }
+
                 @Override
                 public ClusterState execute(ClusterState currentState) throws Exception {
                     if (!success) {
@@ -523,7 +531,7 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
             // nothing to do here...
             return;
         }
-        clusterService.submitStateUpdateTask("zen-disco-node_failed(" + node + "), reason " + reason, Priority.IMMEDIATE, new ProcessedClusterStateUpdateTask() {
+        clusterService.submitStateUpdateTask("zen-disco-node_failed(" + node + "), reason " + reason, Priority.IMMEDIATE, new ClusterStateUpdateTask() {
             @Override
             public ClusterState execute(ClusterState currentState) {
                 if (currentState.nodes().get(node.id()) == null) {
@@ -570,7 +578,7 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
             // We only set the new value. If the master doesn't see enough nodes it will revoke it's mastership.
             return;
         }
-        clusterService.submitStateUpdateTask("zen-disco-minimum_master_nodes_changed", Priority.IMMEDIATE, new ProcessedClusterStateUpdateTask() {
+        clusterService.submitStateUpdateTask("zen-disco-minimum_master_nodes_changed", Priority.IMMEDIATE, new ClusterStateUpdateTask() {
             @Override
             public ClusterState execute(ClusterState currentState) {
                 // check if we have enough master nodes, if not, we need to move into joining the cluster again
@@ -610,7 +618,13 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
 
         logger.info("master_left [{}], reason [{}]", masterNode, reason);
 
-        clusterService.submitStateUpdateTask("zen-disco-master_failed (" + masterNode + ")", Priority.IMMEDIATE, new ProcessedClusterStateNonMasterUpdateTask() {
+        clusterService.submitStateUpdateTask("zen-disco-master_failed (" + masterNode + ")", Priority.IMMEDIATE, new ClusterStateUpdateTask() {
+
+            @Override
+            public boolean runOnlyOnMaster() {
+                return false;
+            }
+
             @Override
             public ClusterState execute(ClusterState currentState) {
                 if (!masterNode.id().equals(currentState.nodes().masterNodeId())) {
@@ -671,7 +685,12 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
     }
 
     void processNextPendingClusterState(String reason) {
-        clusterService.submitStateUpdateTask("zen-disco-receive(from master [" + reason + "])", Priority.URGENT, new ProcessedClusterStateNonMasterUpdateTask() {
+        clusterService.submitStateUpdateTask("zen-disco-receive(from master [" + reason + "])", Priority.URGENT, new ClusterStateUpdateTask() {
+            @Override
+            public boolean runOnlyOnMaster() {
+                return false;
+            }
+
             ClusterState newClusterState = null;
 
             @Override
@@ -1086,7 +1105,13 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
     class RejoinClusterRequestHandler implements TransportRequestHandler<RejoinClusterRequest> {
         @Override
         public void messageReceived(final RejoinClusterRequest request, final TransportChannel channel) throws Exception {
-            clusterService.submitStateUpdateTask("received a request to rejoin the cluster from [" + request.fromNodeId + "]", Priority.IMMEDIATE, new ClusterStateNonMasterUpdateTask() {
+            clusterService.submitStateUpdateTask("received a request to rejoin the cluster from [" + request.fromNodeId + "]", Priority.IMMEDIATE, new ClusterStateUpdateTask() {
+
+                @Override
+                public boolean runOnlyOnMaster() {
+                    return false;
+                }
+
                 @Override
                 public ClusterState execute(ClusterState currentState) {
                     try {
@@ -1098,11 +1123,6 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
                 }
 
                 @Override
-                public void onNoLongerMaster(String source) {
-                    // already logged
-                }
-
-                @Override
                 public void onFailure(String source, Throwable t) {
                     logger.error("unexpected failure during [{}]", t, source);
                 }
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java b/core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java
index b20ed81..3163e06 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java
@@ -23,7 +23,7 @@ import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ProcessedClusterStateNonMasterUpdateTask;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.common.Nullable;
@@ -342,15 +342,12 @@ public class MasterFaultDetection extends FaultDetection {
 
             if (!nodes.localNodeMaster() || !nodes.nodeExists(request.nodeId)) {
                 logger.trace("checking ping from [{}] under a cluster state thread", request.nodeId);
-                clusterService.submitStateUpdateTask("master ping (from: [" + request.nodeId + "])", new ProcessedClusterStateNonMasterUpdateTask() {
+                clusterService.submitStateUpdateTask("master ping (from: [" + request.nodeId + "])", new ClusterStateUpdateTask() {
 
                     @Override
                     public ClusterState execute(ClusterState currentState) throws Exception {
                         // if we are no longer master, fail...
                         DiscoveryNodes nodes = currentState.nodes();
-                        if (!nodes.localNodeMaster()) {
-                            throw new NotMasterException("local node is not master");
-                        }
                         if (!nodes.nodeExists(request.nodeId)) {
                             throw new NodeDoesNotExistOnMasterException();
                         }
@@ -358,6 +355,11 @@ public class MasterFaultDetection extends FaultDetection {
                     }
 
                     @Override
+                    public void onNoLongerMaster(String source) {
+                        onFailure(source, new NotMasterException("local node is not master"));
+                    }
+
+                    @Override
                     public void onFailure(String source, @Nullable Throwable t) {
                         if (t == null) {
                             t = new ElasticsearchException("unknown error while processing ping");
diff --git a/core/src/main/java/org/elasticsearch/gateway/GatewayService.java b/core/src/main/java/org/elasticsearch/gateway/GatewayService.java
index 7f51893..742f789 100644
--- a/core/src/main/java/org/elasticsearch/gateway/GatewayService.java
+++ b/core/src/main/java/org/elasticsearch/gateway/GatewayService.java
@@ -214,7 +214,7 @@ public class GatewayService extends AbstractLifecycleComponent<GatewayService> i
         @Override
         public void onSuccess(final ClusterState recoveredState) {
             logger.trace("successful state recovery, importing cluster state...");
-            clusterService.submitStateUpdateTask("local-gateway-elected-state", new ProcessedClusterStateUpdateTask() {
+            clusterService.submitStateUpdateTask("local-gateway-elected-state", new ClusterStateUpdateTask() {
                 @Override
                 public ClusterState execute(ClusterState currentState) {
                     assert currentState.metaData().indices().isEmpty();
diff --git a/core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java b/core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java
index 24292fc..1ab7a56 100644
--- a/core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java
+++ b/core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java
@@ -21,7 +21,7 @@ package org.elasticsearch.gateway;
 
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ProcessedClusterStateUpdateTask;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.cluster.block.ClusterBlocks;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
@@ -112,7 +112,7 @@ public class LocalAllocateDangledIndices extends AbstractComponent {
             for (int i = 0; i < request.indices.length; i++) {
                 indexNames[i] = request.indices[i].index();
             }
-            clusterService.submitStateUpdateTask("allocation dangled indices " + Arrays.toString(indexNames), new ProcessedClusterStateUpdateTask() {
+            clusterService.submitStateUpdateTask("allocation dangled indices " + Arrays.toString(indexNames), new ClusterStateUpdateTask() {
                 @Override
                 public ClusterState execute(ClusterState currentState) {
                     if (currentState.blocks().disableStatePersistence()) {
diff --git a/core/src/main/java/org/elasticsearch/http/HttpInfo.java b/core/src/main/java/org/elasticsearch/http/HttpInfo.java
index 6b425d6..df51de9 100644
--- a/core/src/main/java/org/elasticsearch/http/HttpInfo.java
+++ b/core/src/main/java/org/elasticsearch/http/HttpInfo.java
@@ -57,7 +57,7 @@ public class HttpInfo implements Streamable, ToXContent {
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject(Fields.HTTP);
-        builder.field(Fields.BOUND_ADDRESS, address.boundAddress().toString());
+        builder.array(Fields.BOUND_ADDRESS, (Object[]) address.boundAddresses());
         builder.field(Fields.PUBLISH_ADDRESS, address.publishAddress().toString());
         builder.byteSizeField(Fields.MAX_CONTENT_LENGTH_IN_BYTES, Fields.MAX_CONTENT_LENGTH, maxContentLength);
         builder.endObject();
diff --git a/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java b/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java
index 634e38f..a794f52 100644
--- a/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java
+++ b/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java
@@ -28,10 +28,7 @@ import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.common.network.NetworkService;
 import org.elasticsearch.common.network.NetworkUtils;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.transport.BoundTransportAddress;
-import org.elasticsearch.common.transport.InetSocketTransportAddress;
-import org.elasticsearch.common.transport.NetworkExceptionHelper;
-import org.elasticsearch.common.transport.PortsRange;
+import org.elasticsearch.common.transport.*;
 import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.util.BigArrays;
@@ -52,10 +49,8 @@ import org.jboss.netty.handler.timeout.ReadTimeoutException;
 import java.io.IOException;
 import java.net.InetAddress;
 import java.net.InetSocketAddress;
-import java.net.SocketAddress;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Map;
 import java.util.concurrent.Executors;
 import java.util.concurrent.atomic.AtomicReference;
 
@@ -255,12 +250,13 @@ public class NettyHttpServerTransport extends AbstractLifecycleComponent<HttpSer
         } catch (IOException e) {
             throw new BindHttpException("Failed to resolve host [" + bindHost + "]", e);
         }
-        
+
+        List<InetSocketTransportAddress> boundAddresses = new ArrayList<>(hostAddresses.length);
         for (InetAddress address : hostAddresses) {
-            bindAddress(address);
+            boundAddresses.add(bindAddress(address));
         }
 
-        InetSocketAddress boundAddress = (InetSocketAddress) serverChannels.get(0).getLocalAddress();
+        InetSocketTransportAddress boundAddress = boundAddresses.get(0);
         InetSocketAddress publishAddress;
         if (0 == publishPort) {
             publishPort = boundAddress.getPort();
@@ -270,10 +266,10 @@ public class NettyHttpServerTransport extends AbstractLifecycleComponent<HttpSer
         } catch (Exception e) {
             throw new BindTransportException("Failed to resolve publish address", e);
         }
-        this.boundAddress = new BoundTransportAddress(new InetSocketTransportAddress(boundAddress), new InetSocketTransportAddress(publishAddress));
+        this.boundAddress = new BoundTransportAddress(boundAddresses.toArray(new TransportAddress[boundAddresses.size()]), new InetSocketTransportAddress(publishAddress));
     }
     
-    private void bindAddress(final InetAddress hostAddress) {
+    private InetSocketTransportAddress bindAddress(final InetAddress hostAddress) {
         PortsRange portsRange = new PortsRange(port);
         final AtomicReference<Exception> lastException = new AtomicReference<>();
         final AtomicReference<InetSocketAddress> boundSocket = new AtomicReference<>();
@@ -296,7 +292,11 @@ public class NettyHttpServerTransport extends AbstractLifecycleComponent<HttpSer
         if (!success) {
             throw new BindHttpException("Failed to bind to [" + port + "]", lastException.get());
         }
-        logger.info("Bound http to address {{}}", NetworkAddress.format(boundSocket.get()));
+
+        if (logger.isDebugEnabled()) {
+            logger.debug("Bound http to address {{}}", NetworkAddress.format(boundSocket.get()));
+        }
+        return new InetSocketTransportAddress(boundSocket.get());
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/index/IndexService.java b/core/src/main/java/org/elasticsearch/index/IndexService.java
index cdfe7b0..cf43e6c 100644
--- a/core/src/main/java/org/elasticsearch/index/IndexService.java
+++ b/core/src/main/java/org/elasticsearch/index/IndexService.java
@@ -59,7 +59,6 @@ import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.index.store.IndexStore;
 import org.elasticsearch.index.store.Store;
 import org.elasticsearch.index.store.StoreModule;
-import org.elasticsearch.index.translog.TranslogService;
 import org.elasticsearch.indices.IndicesLifecycle;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.indices.InternalIndicesLifecycle;
@@ -435,9 +434,6 @@ public class IndexService extends AbstractIndexComponent implements IndexCompone
                         logger.debug("[{}] failed to clean plugin shard service [{}]", e, shardId, closeable);
                     }
                 }
-                // now we can close the translog service, we need to close it before the we close the shard
-                // note the that the translog service is not there for shadow replicas
-                closeInjectorOptionalResource(sId, shardInjector, TranslogService.class);
                 // this logic is tricky, we want to close the engine so we rollback the changes done to it
                 // and close the shard so no operations are allowed to it
                 if (indexShard != null) {
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java
index 222d262..4111786 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java
@@ -231,7 +231,8 @@ public class GeoPointFieldMapper extends FieldMapper implements ArrayValueMapper
             StringFieldMapper geohashMapper = null;
             if (enableGeoHash || enableGeohashPrefix) {
                 // TODO: possible also implicitly enable geohash if geohash precision is set
-                geohashMapper = stringField(Names.GEOHASH).index(true).tokenized(false).includeInAll(false).omitNorms(true).indexOptions(IndexOptions.DOCS).build(context);
+                geohashMapper = stringField(Names.GEOHASH).index(true).tokenized(false).includeInAll(false).store(fieldType.stored())
+                        .omitNorms(true).indexOptions(IndexOptions.DOCS).build(context);
                 geoPointFieldType.setGeohashEnabled(geohashMapper.fieldType(), geoHashPrecision, enableGeohashPrefix);
             }
             context.path().remove();
@@ -678,7 +679,7 @@ public class GeoPointFieldMapper extends FieldMapper implements ArrayValueMapper
 
     private void addGeohashField(ParseContext context, String geohash) throws IOException {
         int len = Math.min(fieldType().geohashPrecision(), geohash.length());
-        int min = fieldType().isGeohashPrefixEnabled() ? 1 : geohash.length();
+        int min = fieldType().isGeohashPrefixEnabled() ? 1 : len;
 
         for (int i = len; i >= min; i--) {
             // side effect of this call is adding the field
diff --git a/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java b/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
index 9dc25b2..06d04f6 100644
--- a/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
+++ b/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
@@ -184,7 +184,6 @@ public class PercolatorQueriesRegistry extends AbstractIndexShardComponent imple
         }
     }
 
-    //norelease this method parses from xcontent to lucene query, need to re-investigate how to split context here
     private Query parseQuery(String type, XContentParser parser) {
         String[] previousTypes = null;
         if (type != null) {
diff --git a/core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java
index 3d454e3..560476a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java
@@ -156,10 +156,7 @@ public abstract class AbstractQueryBuilder<QB extends AbstractQueryBuilder> exte
         return queryBuilder;
     }
 
-    //norelease make this abstract once all builders implement doReadFrom themselves
-    protected QB doReadFrom(StreamInput in) throws IOException {
-        throw new UnsupportedOperationException();
-    }
+    protected abstract QB doReadFrom(StreamInput in) throws IOException;
 
     @Override
     public final void writeTo(StreamOutput out) throws IOException {
@@ -168,10 +165,7 @@ public abstract class AbstractQueryBuilder<QB extends AbstractQueryBuilder> exte
         out.writeOptionalString(queryName);
     }
 
-    //norelease make this abstract once all builders implement doWriteTo themselves
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        throw new UnsupportedOperationException();
-    }
+    protected abstract void doWriteTo(StreamOutput out) throws IOException;
 
     protected final QueryValidationException addValidationError(String validationError, QueryValidationException validationException) {
         return QueryValidationException.addValidationError(getName(), validationError, validationException);
@@ -195,20 +189,14 @@ public abstract class AbstractQueryBuilder<QB extends AbstractQueryBuilder> exte
     /**
      * Indicates whether some other {@link QueryBuilder} object of the same type is "equal to" this one.
      */
-    //norelease to be made abstract once all queries are refactored
-    protected boolean doEquals(QB other) {
-        return super.equals(other);
-    }
+    protected abstract boolean doEquals(QB other);
 
     @Override
     public final int hashCode() {
         return Objects.hash(getClass(), queryName, boost, doHashCode());
     }
 
-    //norelease to be made abstract once all queries are refactored
-    protected int doHashCode() {
-        return super.hashCode();
-    }
+    protected abstract int doHashCode();
 
     /**
      * This helper method checks if the object passed in is a string, if so it
@@ -275,20 +263,4 @@ public abstract class AbstractQueryBuilder<QB extends AbstractQueryBuilder> exte
         }
         return queries;
     }
-
-    protected final void writeOptionalQuery(StreamOutput out, QueryBuilder query) throws IOException {
-        if (query == null) {
-            out.writeBoolean(false);
-        } else {
-            out.writeBoolean(true);
-            out.writeQuery(query);
-        }
-    }
-
-    protected final QueryBuilder readOptionalQuery(StreamInput in) throws IOException {
-        if (in.readBoolean()) {
-            return in.readQuery();
-        }
-        return null;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
index c726992..4ad63eb 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
@@ -22,7 +22,6 @@ package org.elasticsearch.index.query;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Numbers;
-import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -47,10 +46,6 @@ import java.util.Objects;
 public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBoundingBoxQueryBuilder> {
     /** Name of the query. */
     public static final String NAME = "geo_bbox";
-    /** Default for geo point coerce (as of this writing false). */
-    public static final boolean DEFAULT_COERCE = false;
-    /** Default for skipping geo point validation (as of this writing false). */
-    public static final boolean DEFAULT_IGNORE_MALFORMED = false;
     /** Default type for executing this query (memory as of this writing). */
     public static final GeoExecType DEFAULT_TYPE = GeoExecType.MEMORY;
     /** Needed for serialization. */
@@ -62,10 +57,8 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
     private GeoPoint topLeft = new GeoPoint(Double.NaN, Double.NaN);
     /** Bottom right corner coordinates of bounding box.*/
     private GeoPoint bottomRight = new GeoPoint(Double.NaN, Double.NaN);
-    /** Whether or not to infer correct coordinates for wrapping bounding boxes.*/
-    private boolean coerce = DEFAULT_COERCE;
-    /** Whether or not to skip geo point validation. */
-    private boolean ignoreMalformed = DEFAULT_IGNORE_MALFORMED;
+    /** How to deal with incorrect coordinates.*/
+    private GeoValidationMethod validationMethod = GeoValidationMethod.DEFAULT;
     /** How the query should be run. */
     private GeoExecType type = DEFAULT_TYPE;
 
@@ -88,7 +81,7 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
      * @param right The right longitude
      */
     public GeoBoundingBoxQueryBuilder setCorners(double top, double left, double bottom, double right) {
-        if (!ignoreMalformed) {
+        if (GeoValidationMethod.isIgnoreMalformed(validationMethod) == false) {
             if (Numbers.isValidDouble(top) == false) {
                 throw new IllegalArgumentException("top latitude is invalid: " + top);
             }
@@ -165,38 +158,20 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
     }
 
     /**
-     * Specify whether or not to try and fix broken/wrapping bounding boxes.
-     * If set to true, also enables ignoreMalformed thus disabling geo point
-     * validation altogether.
-     **/
-    public GeoBoundingBoxQueryBuilder coerce(boolean coerce) {
-        if (coerce) {
-            this.ignoreMalformed = true;
-        }
-        this.coerce = coerce;
-        return this;
-    }
-
-    /** Returns whether or not to try and fix broken/wrapping bounding boxes. */
-    public boolean coerce() {
-        return this.coerce;
-    }
-
-    /**
      * Specify whether or not to ignore validation errors of bounding boxes.
      * Can only be set if coerce set to false, otherwise calling this
      * method has no effect.
      **/
-    public GeoBoundingBoxQueryBuilder ignoreMalformed(boolean ignoreMalformed) {
-        if (coerce == false) {
-            this.ignoreMalformed = ignoreMalformed;
-        }
+    public GeoBoundingBoxQueryBuilder setValidationMethod(GeoValidationMethod method) {
+        this.validationMethod = method;
         return this;
     }
-
-    /** Returns whether or not to skip bounding box validation. */
-    public boolean ignoreMalformed() {
-        return ignoreMalformed;
+    
+    /**
+     * Returns geo coordinate validation method to use.
+     * */
+    public GeoValidationMethod getValidationMethod() {
+        return this.validationMethod;
     }
 
     /**
@@ -230,7 +205,7 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
 
     QueryValidationException checkLatLon(boolean indexCreatedBeforeV2_0) {
         // validation was not available prior to 2.x, so to support bwc percolation queries we only ignore_malformed on 2.x created indexes
-        if (ignoreMalformed || indexCreatedBeforeV2_0) {
+        if (GeoValidationMethod.isIgnoreMalformed(validationMethod) == true || indexCreatedBeforeV2_0) {
             return null;
         }
 
@@ -264,7 +239,7 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
 
         GeoPoint luceneTopLeft = new GeoPoint(topLeft);
         GeoPoint luceneBottomRight = new GeoPoint(bottomRight);
-        if (coerce) {
+        if (GeoValidationMethod.isCoerce(validationMethod)) {
             // Special case: if the difference between the left and right is 360 and the right is greater than the left, we are asking for
             // the complete longitude range so need to set longitude to the complete longditude range
             double right = luceneBottomRight.getLon();
@@ -313,8 +288,7 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
         builder.array(GeoBoundingBoxQueryParser.TOP_LEFT, topLeft.getLon(), topLeft.getLat());
         builder.array(GeoBoundingBoxQueryParser.BOTTOM_RIGHT, bottomRight.getLon(), bottomRight.getLat());
         builder.endObject();
-        builder.field("coerce", coerce);
-        builder.field("ignore_malformed", ignoreMalformed);
+        builder.field("validation_method", validationMethod);
         builder.field("type", type);
 
         printBoostAndQueryName(builder);
@@ -327,14 +301,13 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
         return Objects.equals(topLeft, other.topLeft) &&
                 Objects.equals(bottomRight, other.bottomRight) &&
                 Objects.equals(type, other.type) &&
-                Objects.equals(coerce, other.coerce) &&
-                Objects.equals(ignoreMalformed, other.ignoreMalformed) &&
+                Objects.equals(validationMethod, other.validationMethod) &&
                 Objects.equals(fieldName, other.fieldName);
     }
 
     @Override
     public int doHashCode() {
-        return Objects.hash(topLeft, bottomRight, type, coerce, ignoreMalformed, fieldName);
+        return Objects.hash(topLeft, bottomRight, type, validationMethod, fieldName);
     }
 
     @Override
@@ -344,8 +317,7 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
         geo.topLeft = geo.topLeft.readFrom(in);
         geo.bottomRight = geo.bottomRight.readFrom(in);
         geo.type = GeoExecType.readTypeFrom(in);
-        geo.coerce = in.readBoolean();
-        geo.ignoreMalformed = in.readBoolean();
+        geo.validationMethod = GeoValidationMethod.readGeoValidationMethodFrom(in);
         return geo;
     }
 
@@ -355,8 +327,7 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
         topLeft.writeTo(out);
         bottomRight.writeTo(out);
         type.writeTo(out);
-        out.writeBoolean(coerce);
-        out.writeBoolean(ignoreMalformed);
+        validationMethod.writeTo(out);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java
index 638968f..afa8853 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java
@@ -80,8 +80,9 @@ public class GeoBoundingBoxQueryParser implements QueryParser<GeoBoundingBoxQuer
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token;
-        boolean coerce = GeoBoundingBoxQueryBuilder.DEFAULT_COERCE;
-        boolean ignoreMalformed = GeoBoundingBoxQueryBuilder.DEFAULT_IGNORE_MALFORMED;
+        boolean coerce = GeoValidationMethod.DEFAULT_LENIENT_PARSING;
+        boolean ignoreMalformed = GeoValidationMethod.DEFAULT_LENIENT_PARSING;
+        GeoValidationMethod validationMethod = null;
 
         GeoPoint sparse = new GeoPoint();
 
@@ -144,6 +145,8 @@ public class GeoBoundingBoxQueryParser implements QueryParser<GeoBoundingBoxQuer
                     if (coerce) {
                         ignoreMalformed = true;
                     }
+                } else if ("validation_method".equals(currentFieldName)) {
+                    validationMethod = GeoValidationMethod.fromString(parser.text());
                 } else if ("type".equals(currentFieldName)) {
                     type = parser.text();
                 } else if ("ignore_malformed".equals(currentFieldName)) {
@@ -161,8 +164,12 @@ public class GeoBoundingBoxQueryParser implements QueryParser<GeoBoundingBoxQuer
         builder.queryName(queryName);
         builder.boost(boost);
         builder.type(GeoExecType.fromString(type));
-        builder.coerce(coerce);
-        builder.ignoreMalformed(ignoreMalformed);
+        if (validationMethod != null) {
+            // ignore deprecated coerce/ignoreMalformed settings if validationMethod is set
+            builder.setValidationMethod(validationMethod);
+        } else {
+            builder.setValidationMethod(GeoValidationMethod.infer(coerce, ignoreMalformed));
+        }
         return builder;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
index e382a9e..7d6066e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
@@ -56,10 +56,6 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
     public static final GeoDistance DEFAULT_GEO_DISTANCE = GeoDistance.DEFAULT;
     /** Default for optimising query through pre computed bounding box query. */
     public static final String DEFAULT_OPTIMIZE_BBOX = "memory";
-    /** Default for coercing lon/lat values to a standard coordinate system */
-    public static final boolean DEFAULT_COERCE = false;
-    /** Default for accepting accept geo points with invalid latitude or longitude */
-    public static final boolean DEFAULT_IGNORE_MALFORMED = false;
 
     private final String fieldName;
     /** Distance from center to cover. */
@@ -70,10 +66,8 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
     private GeoDistance geoDistance = DEFAULT_GEO_DISTANCE;
     /** Whether or not to use a bbox for pre-filtering. TODO change to enum? */
     private String optimizeBbox = DEFAULT_OPTIMIZE_BBOX;
-    /** Whether or not to normalize longitude and latitude values to a standard coordinate system */
-    private boolean coerce = DEFAULT_COERCE;
-    /** Whether or not to accept geo points with invalid latitude or longitude */
-    private boolean ignoreMalformed = DEFAULT_IGNORE_MALFORMED;
+    /** How strict should geo coordinate validation be? */
+    private GeoValidationMethod validationMethod = GeoValidationMethod.DEFAULT;
 
     static final GeoDistanceQueryBuilder PROTOTYPE = new GeoDistanceQueryBuilder("_na_");
 
@@ -197,19 +191,14 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
         return this.optimizeBbox;
     }
 
-    public GeoDistanceQueryBuilder coerce(boolean coerce) {
-        this.coerce = coerce;
-        if (this.coerce) {
-            this.ignoreMalformed = true;
-        }
-        return this;
+    /** Set validaton method for geo coordinates. */
+    public void setValidationMethod(GeoValidationMethod method) {
+        this.validationMethod = method;
     }
 
-    public GeoDistanceQueryBuilder ignoreMalformed(boolean ignoreMalformed) {
-        if (coerce == false) {
-            this.ignoreMalformed = ignoreMalformed;
-        }
-        return this;
+    /** Returns validation method for geo coordinates. */
+    public GeoValidationMethod getValidationMethod() {
+        return this.validationMethod;
     }
 
     @Override
@@ -219,8 +208,8 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
             throw new QueryShardException(shardContext, "couldn't validate latitude/ longitude values", exception);
         }
 
-        if (coerce) {
-            GeoUtils.normalizePoint(center, coerce, coerce);
+        if (GeoValidationMethod.isCoerce(validationMethod)) {
+            GeoUtils.normalizePoint(center, true, true);
         }
 
         double normDistance = geoDistance.normalize(this.distance, DistanceUnit.DEFAULT);
@@ -246,23 +235,21 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
         builder.field("distance", distance);
         builder.field("distance_type", geoDistance.name().toLowerCase(Locale.ROOT));
         builder.field("optimize_bbox", optimizeBbox);
-        builder.field("coerce", coerce);
-        builder.field("ignore_malformed", ignoreMalformed);
+        builder.field("validation_method", validationMethod);
         printBoostAndQueryName(builder);
         builder.endObject();
     }
 
     @Override
     public int doHashCode() {
-        return Objects.hash(center, geoDistance, optimizeBbox, distance, coerce, ignoreMalformed);
+        return Objects.hash(center, geoDistance, optimizeBbox, distance, validationMethod);
     }
 
     @Override
     public boolean doEquals(GeoDistanceQueryBuilder other) {
         return Objects.equals(fieldName, other.fieldName) &&
                 (distance == other.distance) &&
-                (coerce == other.coerce) &&
-                (ignoreMalformed == other.ignoreMalformed) &&
+                Objects.equals(validationMethod, other.validationMethod) &&
                 Objects.equals(center, other.center) &&
                 Objects.equals(optimizeBbox, other.optimizeBbox) &&
                 Objects.equals(geoDistance, other.geoDistance);
@@ -273,8 +260,7 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
         String fieldName = in.readString();
         GeoDistanceQueryBuilder result = new GeoDistanceQueryBuilder(fieldName);
         result.distance = in.readDouble();
-        result.coerce = in.readBoolean();
-        result.ignoreMalformed = in.readBoolean();
+        result.validationMethod = GeoValidationMethod.readGeoValidationMethodFrom(in);
         result.center = GeoPoint.readGeoPointFrom(in);
         result.optimizeBbox = in.readString();
         result.geoDistance = GeoDistance.readGeoDistanceFrom(in);
@@ -285,8 +271,7 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
     protected void doWriteTo(StreamOutput out) throws IOException {
         out.writeString(fieldName);
         out.writeDouble(distance);
-        out.writeBoolean(coerce);
-        out.writeBoolean(ignoreMalformed);
+        validationMethod.writeTo(out);
         center.writeTo(out);
         out.writeString(optimizeBbox);
         geoDistance.writeTo(out);
@@ -294,7 +279,7 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
 
     private QueryValidationException checkLatLon(boolean indexCreatedBeforeV2_0) {
         // validation was not available prior to 2.x, so to support bwc percolation queries we only ignore_malformed on 2.x created indexes
-        if (ignoreMalformed || indexCreatedBeforeV2_0) {
+        if (GeoValidationMethod.isIgnoreMalformed(validationMethod) || indexCreatedBeforeV2_0) {
             return null;
         }
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
index a2bbba5..da9a7c2 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
@@ -61,8 +61,9 @@ public class GeoDistanceQueryParser implements QueryParser<GeoDistanceQueryBuild
         DistanceUnit unit = GeoDistanceQueryBuilder.DEFAULT_DISTANCE_UNIT;
         GeoDistance geoDistance = GeoDistanceQueryBuilder.DEFAULT_GEO_DISTANCE;
         String optimizeBbox = GeoDistanceQueryBuilder.DEFAULT_OPTIMIZE_BBOX;
-        boolean coerce = GeoDistanceQueryBuilder.DEFAULT_COERCE;
-        boolean ignoreMalformed = GeoDistanceQueryBuilder.DEFAULT_IGNORE_MALFORMED;
+        boolean coerce = GeoValidationMethod.DEFAULT_LENIENT_PARSING;
+        boolean ignoreMalformed = GeoValidationMethod.DEFAULT_LENIENT_PARSING;
+        GeoValidationMethod validationMethod = null;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -75,6 +76,7 @@ public class GeoDistanceQueryParser implements QueryParser<GeoDistanceQueryBuild
             } else if (token == XContentParser.Token.START_OBJECT) {
                 // the json in the format of -> field : { lat : 30, lon : 12 }
                 String currentName = parser.currentName();
+                assert currentFieldName != null;
                 fieldName = currentFieldName;
                 while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                     if (token == XContentParser.Token.FIELD_NAME) {
@@ -125,6 +127,8 @@ public class GeoDistanceQueryParser implements QueryParser<GeoDistanceQueryBuild
                     }
                 } else if ("ignore_malformed".equals(currentFieldName)) {
                     ignoreMalformed = parser.booleanValue();
+                } else if ("validation_method".equals(currentFieldName)) {
+                    validationMethod = GeoValidationMethod.fromString(parser.text());
                 } else {
                     point.resetFromString(parser.text());
                     fieldName = currentFieldName;
@@ -143,8 +147,11 @@ public class GeoDistanceQueryParser implements QueryParser<GeoDistanceQueryBuild
             qb.distance((String) vDistance, unit);
         }
         qb.point(point);
-        qb.coerce(coerce);
-        qb.ignoreMalformed(ignoreMalformed);
+        if (validationMethod != null) {
+            qb.setValidationMethod(validationMethod);
+        } else {
+            qb.setValidationMethod(GeoValidationMethod.infer(coerce, ignoreMalformed));
+        }
         qb.optimizeBbox(optimizeBbox);
         qb.geoDistance(geoDistance);
         qb.boost(boost);
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
index bc58057..9a224fa 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
@@ -46,8 +46,6 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
     public static final GeoDistance DEFAULT_GEO_DISTANCE = GeoDistance.DEFAULT;
     public static final DistanceUnit DEFAULT_UNIT = DistanceUnit.DEFAULT;
     public static final String DEFAULT_OPTIMIZE_BBOX = "memory";
-    public static final boolean DEFAULT_COERCE = false;
-    public static final boolean DEFAULT_IGNORE_MALFORMED = false;
 
     private final String fieldName;
 
@@ -64,9 +62,7 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
 
     private String optimizeBbox = DEFAULT_OPTIMIZE_BBOX;
 
-    private boolean coerce = DEFAULT_COERCE;
-
-    private boolean ignoreMalformed = DEFAULT_IGNORE_MALFORMED;
+    private GeoValidationMethod validationMethod = GeoValidationMethod.DEFAULT;
 
     static final GeoDistanceRangeQueryBuilder PROTOTYPE = new GeoDistanceRangeQueryBuilder("_na_", new GeoPoint());
 
@@ -199,27 +195,15 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
         return optimizeBbox;
     }
 
-    public GeoDistanceRangeQueryBuilder coerce(boolean coerce) {
-        if (coerce) {
-            this.ignoreMalformed = true;
-        }
-        this.coerce = coerce;
-        return this;
-    }
-
-    public boolean coerce() {
-        return this.coerce;
-    }
-
-    public GeoDistanceRangeQueryBuilder ignoreMalformed(boolean ignoreMalformed) {
-        if (coerce == false) {
-            this.ignoreMalformed = ignoreMalformed;
-        }
+    /** Set validation method for coordinates. */
+    public GeoDistanceRangeQueryBuilder setValidationMethod(GeoValidationMethod method) {
+        this.validationMethod = method;
         return this;
     }
-
-    public boolean ignoreMalformed() {
-        return ignoreMalformed;
+    
+    /** Returns validation method for coordinates. */
+    public GeoValidationMethod getValidationMethod(GeoValidationMethod method) {
+        return this.validationMethod;
     }
 
     @Override
@@ -228,7 +212,7 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
         final boolean indexCreatedBeforeV2_0 = context.indexVersionCreated().before(Version.V_2_0_0);
         // validation was not available prior to 2.x, so to support bwc
         // percolation queries we only ignore_malformed on 2.x created indexes
-        if (!indexCreatedBeforeV2_0 && !ignoreMalformed) {
+        if (!indexCreatedBeforeV2_0 && !GeoValidationMethod.isIgnoreMalformed(validationMethod)) {
             if (!GeoUtils.isValidLatitude(point.lat())) {
                 throw new QueryShardException(context, "illegal latitude value [{}] for [{}]", point.lat(), NAME);
             }
@@ -237,8 +221,8 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
             }
         }
 
-        if (coerce) {
-            GeoUtils.normalizePoint(point, coerce, coerce);
+        if (GeoValidationMethod.isCoerce(validationMethod)) {
+            GeoUtils.normalizePoint(point, true, true);
         }
 
         Double fromValue = null;
@@ -285,8 +269,7 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
         builder.field(GeoDistanceRangeQueryParser.UNIT_FIELD.getPreferredName(), unit);
         builder.field(GeoDistanceRangeQueryParser.DISTANCE_TYPE_FIELD.getPreferredName(), geoDistance.name().toLowerCase(Locale.ROOT));
         builder.field(GeoDistanceRangeQueryParser.OPTIMIZE_BBOX_FIELD.getPreferredName(), optimizeBbox);
-        builder.field(GeoDistanceRangeQueryParser.COERCE_FIELD.getPreferredName(), coerce);
-        builder.field(GeoDistanceRangeQueryParser.IGNORE_MALFORMED_FIELD.getPreferredName(), ignoreMalformed);
+        builder.field(GeoDistanceRangeQueryParser.VALIDATION_METHOD.getPreferredName(), validationMethod);
         printBoostAndQueryName(builder);
         builder.endObject();
     }
@@ -301,8 +284,7 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
         queryBuilder.unit = DistanceUnit.valueOf(in.readString());
         queryBuilder.geoDistance = GeoDistance.readGeoDistanceFrom(in);
         queryBuilder.optimizeBbox = in.readString();
-        queryBuilder.coerce = in.readBoolean();
-        queryBuilder.ignoreMalformed = in.readBoolean();
+        queryBuilder.validationMethod = GeoValidationMethod.readGeoValidationMethodFrom(in);
         return queryBuilder;
     }
 
@@ -317,49 +299,25 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
         out.writeString(unit.name());
         geoDistance.writeTo(out);;
         out.writeString(optimizeBbox);
-        out.writeBoolean(coerce);
-        out.writeBoolean(ignoreMalformed);
+        validationMethod.writeTo(out);
     }
 
     @Override
     protected boolean doEquals(GeoDistanceRangeQueryBuilder other) {
-        if (!Objects.equals(fieldName, other.fieldName)) {
-            return false;
-        }
-        if (!Objects.equals(point, other.point)) {
-            return false;
-        }
-        if (!Objects.equals(from, other.from)) {
-            return false;
-        }
-        if (!Objects.equals(to, other.to)) {
-            return false;
-        }
-        if (!Objects.equals(includeUpper, other.includeUpper)) {
-            return false;
-        }
-        if (!Objects.equals(includeLower, other.includeLower)) {
-            return false;
-        }
-        if (!Objects.equals(geoDistance, other.geoDistance)) {
-            return false;
-        }
-        if (!Objects.equals(optimizeBbox, other.optimizeBbox)) {
-            return false;
-        }
-        if (!Objects.equals(coerce, other.coerce)) {
-            return false;
-        }
-        if (!Objects.equals(ignoreMalformed, other.ignoreMalformed)) {
-            return false;
-        }
-        return true;
+        return ((Objects.equals(fieldName, other.fieldName)) && 
+                (Objects.equals(point, other.point)) &&
+                (Objects.equals(from, other.from)) &&
+                (Objects.equals(to, other.to)) &&
+                (Objects.equals(includeUpper, other.includeUpper)) &&
+                (Objects.equals(includeLower, other.includeLower)) &&
+                (Objects.equals(geoDistance, other.geoDistance)) &&
+                (Objects.equals(optimizeBbox, other.optimizeBbox)) &&
+                (Objects.equals(validationMethod, other.validationMethod)));
     }
 
     @Override
     protected int doHashCode() {
-        return Objects.hash(fieldName, point, from, to, includeUpper, includeLower, geoDistance, optimizeBbox, coerce,
-                ignoreMalformed);
+        return Objects.hash(fieldName, point, from, to, includeUpper, includeLower, geoDistance, optimizeBbox, validationMethod);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
index 2032171..4762e6f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
@@ -54,6 +54,7 @@ public class GeoDistanceRangeQueryParser implements QueryParser<GeoDistanceRange
     public static final ParseField OPTIMIZE_BBOX_FIELD = new ParseField("optimize_bbox");
     public static final ParseField COERCE_FIELD = new ParseField("coerce", "normalize");
     public static final ParseField IGNORE_MALFORMED_FIELD = new ParseField("ignore_malformed");
+    public static final ParseField VALIDATION_METHOD = new ParseField("validation_method");
 
     @Override
     public String[] names() {
@@ -83,8 +84,10 @@ public class GeoDistanceRangeQueryParser implements QueryParser<GeoDistanceRange
         DistanceUnit unit = null;
         GeoDistance geoDistance = null;
         String optimizeBbox = null;
-        Boolean coerce = null;
-        Boolean ignoreMalformed = null;
+        boolean coerce = GeoValidationMethod.DEFAULT_LENIENT_PARSING;
+        boolean ignoreMalformed = GeoValidationMethod.DEFAULT_LENIENT_PARSING;
+        GeoValidationMethod validationMethod = null;
+
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -183,6 +186,8 @@ public class GeoDistanceRangeQueryParser implements QueryParser<GeoDistanceRange
                     coerce = parser.booleanValue();
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, IGNORE_MALFORMED_FIELD)) {
                     ignoreMalformed = parser.booleanValue();
+                } else if (parseContext.parseFieldMatcher().match(currentFieldName, VALIDATION_METHOD)) {
+                    validationMethod = GeoValidationMethod.fromString(parser.text());
                 } else {
                     if (point == null) {
                         point = new GeoPoint();
@@ -238,12 +243,11 @@ public class GeoDistanceRangeQueryParser implements QueryParser<GeoDistanceRange
             queryBuilder.optimizeBbox(optimizeBbox);
         }
 
-        if (coerce != null) {
-            queryBuilder.coerce(coerce);
-        }
-
-        if (ignoreMalformed != null) {
-            queryBuilder.ignoreMalformed(ignoreMalformed);
+        if (validationMethod != null) {
+            // if validation method is set explicitly ignore deprecated coerce/ignore malformed fields if any
+            queryBuilder.setValidationMethod(validationMethod);
+        } else {
+            queryBuilder.setValidationMethod(GeoValidationMethod.infer(coerce, ignoreMalformed));
         }
         return queryBuilder;
     }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
index a4cb5d7..35b7cba 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
@@ -51,9 +51,7 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
 
     private final List<GeoPoint> shell;
 
-    private boolean coerce = false;
-
-    private boolean ignoreMalformed = false;
+    private GeoValidationMethod validationMethod = GeoValidationMethod.DEFAULT;
 
     public GeoPolygonQueryBuilder(String fieldName, List<GeoPoint> points) {
         if (Strings.isEmpty(fieldName)) {
@@ -85,27 +83,15 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
         return shell;
     }
 
-    public GeoPolygonQueryBuilder coerce(boolean coerce) {
-        if (coerce) {
-            this.ignoreMalformed = true;
-        }
-        this.coerce = coerce;
-        return this;
-    }
-
-    public boolean coerce() {
-        return this.coerce;
-    }
-
-    public GeoPolygonQueryBuilder ignoreMalformed(boolean ignoreMalformed) {
-        if (coerce == false) {
-            this.ignoreMalformed = ignoreMalformed;
-        }
+    /** Sets the validation method to use for geo coordinates. */
+    public GeoPolygonQueryBuilder setValidationMethod(GeoValidationMethod method) {
+        this.validationMethod = method;
         return this;
     }
 
-    public boolean ignoreMalformed() {
-        return ignoreMalformed;
+    /** Returns the validation method to use for geo coordinates. */
+    public GeoValidationMethod getValidationMethod() {
+        return this.validationMethod;
     }
 
     @Override
@@ -118,7 +104,7 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
         final boolean indexCreatedBeforeV2_0 = context.indexVersionCreated().before(Version.V_2_0_0);
         // validation was not available prior to 2.x, so to support bwc
         // percolation queries we only ignore_malformed on 2.x created indexes
-        if (!indexCreatedBeforeV2_0 && !ignoreMalformed) {
+        if (!indexCreatedBeforeV2_0 && !GeoValidationMethod.isIgnoreMalformed(validationMethod)) {
             for (GeoPoint point : shell) {
                 if (!GeoUtils.isValidLatitude(point.lat())) {
                     throw new QueryShardException(context, "illegal latitude value [{}] for [{}]", point.lat(),
@@ -131,9 +117,9 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
             }
         }
 
-        if (coerce) {
+        if (GeoValidationMethod.isCoerce(validationMethod)) {
             for (GeoPoint point : shell) {
-                GeoUtils.normalizePoint(point, coerce, coerce);
+                GeoUtils.normalizePoint(point, true, true);
             }
         }
 
@@ -161,8 +147,8 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
         builder.endArray();
         builder.endObject();
 
-        builder.field(GeoPolygonQueryParser.COERCE_FIELD.getPreferredName(), coerce);
-        builder.field(GeoPolygonQueryParser.IGNORE_MALFORMED_FIELD.getPreferredName(), ignoreMalformed);
+        builder.field(GeoPolygonQueryParser.COERCE_FIELD.getPreferredName(), GeoValidationMethod.isCoerce(validationMethod));
+        builder.field(GeoPolygonQueryParser.IGNORE_MALFORMED_FIELD.getPreferredName(), GeoValidationMethod.isIgnoreMalformed(validationMethod));
 
         printBoostAndQueryName(builder);
         builder.endObject();
@@ -177,8 +163,7 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
             shell.add(GeoPoint.readGeoPointFrom(in));
         }
         GeoPolygonQueryBuilder builder = new GeoPolygonQueryBuilder(fieldName, shell);
-        builder.coerce = in.readBoolean();
-        builder.ignoreMalformed = in.readBoolean();
+        builder.validationMethod = GeoValidationMethod.readGeoValidationMethodFrom(in);
         return builder;
     }
 
@@ -189,21 +174,19 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
         for (GeoPoint point : shell) {
             point.writeTo(out);
         }
-        out.writeBoolean(coerce);
-        out.writeBoolean(ignoreMalformed);
+        validationMethod.writeTo(out);
     }
 
     @Override
     protected boolean doEquals(GeoPolygonQueryBuilder other) {
-        return Objects.equals(coerce, other.coerce)
+        return Objects.equals(validationMethod, other.validationMethod)
                 && Objects.equals(fieldName, other.fieldName)
-                && Objects.equals(ignoreMalformed, other.ignoreMalformed)
                 && Objects.equals(shell, other.shell);
     }
 
     @Override
     protected int doHashCode() {
-        return Objects.hash(coerce, fieldName, ignoreMalformed, shell);
+        return Objects.hash(validationMethod, fieldName, shell);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java
index 1f0e284..3298a0f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java
@@ -46,6 +46,7 @@ public class GeoPolygonQueryParser implements QueryParser<GeoPolygonQueryBuilder
 
     public static final ParseField COERCE_FIELD = new ParseField("coerce", "normalize");
     public static final ParseField IGNORE_MALFORMED_FIELD = new ParseField("ignore_malformed");
+    public static final ParseField VALIDATION_METHOD = new ParseField("validation_method");
     public static final ParseField POINTS_FIELD = new ParseField("points");
 
     @Override
@@ -62,8 +63,9 @@ public class GeoPolygonQueryParser implements QueryParser<GeoPolygonQueryBuilder
         List<GeoPoint> shell = null;
 
         Float boost = null;
-        Boolean coerce = null;
-        Boolean ignoreMalformed = null;
+        boolean coerce = GeoValidationMethod.DEFAULT_LENIENT_PARSING;
+        boolean ignoreMalformed = GeoValidationMethod.DEFAULT_LENIENT_PARSING;
+        GeoValidationMethod validationMethod = null;
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token;
@@ -106,6 +108,8 @@ public class GeoPolygonQueryParser implements QueryParser<GeoPolygonQueryBuilder
                     }
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, IGNORE_MALFORMED_FIELD)) {
                     ignoreMalformed = parser.booleanValue();
+                } else if (parseContext.parseFieldMatcher().match(currentFieldName, VALIDATION_METHOD)) {
+                    validationMethod = GeoValidationMethod.fromString(parser.text());
                 } else {
                     throw new ParsingException(parser.getTokenLocation(), "[geo_polygon] query does not support [" + currentFieldName + "]");
                 }
@@ -114,12 +118,13 @@ public class GeoPolygonQueryParser implements QueryParser<GeoPolygonQueryBuilder
             }
         }
         GeoPolygonQueryBuilder builder = new GeoPolygonQueryBuilder(fieldName, shell);
-        if (coerce != null) {
-            builder.coerce(coerce);
-        }
-        if (ignoreMalformed != null) {
-            builder.ignoreMalformed(ignoreMalformed);
+        if (validationMethod != null) {
+            // if GeoValidationMethod was explicitly set ignore deprecated coerce and ignoreMalformed settings
+            builder.setValidationMethod(validationMethod);
+        } else {
+            builder.setValidationMethod(GeoValidationMethod.infer(coerce, ignoreMalformed));
         }
+
         if (queryName != null) {
             builder.queryName(queryName);
         }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
index 7fddc48..31bc889 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
@@ -182,6 +182,10 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
      * @return this
      */
     public GeoShapeQueryBuilder strategy(SpatialStrategy strategy) {
+        if (strategy != null && strategy == SpatialStrategy.TERM && relation != ShapeRelation.INTERSECTS) {
+            throw new IllegalArgumentException("strategy [" + strategy.getStrategyName() + "] only supports relation ["
+                    + ShapeRelation.INTERSECTS.getRelationName() + "] found relation [" + relation.getRelationName() + "]");
+        }
         this.strategy = strategy;
         return this;
     }
@@ -240,6 +244,10 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
         if (relation == null) {
             throw new IllegalArgumentException("No Shape Relation defined");
         }
+        if (strategy != null && strategy == SpatialStrategy.TERM && relation != ShapeRelation.INTERSECTS) {
+            throw new IllegalArgumentException("current strategy [" + strategy.getStrategyName() + "] only supports relation ["
+                    + ShapeRelation.INTERSECTS.getRelationName() + "] found relation [" + relation.getRelationName() + "]");
+        }
         this.relation = relation;
         return this;
     }
@@ -251,17 +259,6 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
         return relation;
     }
 
-    public QueryValidationException validate() {
-        QueryValidationException errors = null;
-        // TODO did we validate this before the refactoring and can we do this in setters at all?
-        if (strategy != null && strategy == SpatialStrategy.TERM && relation != ShapeRelation.INTERSECTS) {
-            errors = QueryValidationException.addValidationError(NAME,
-                    "strategy [" + strategy.getStrategyName() + "] only supports relation ["
-                    + ShapeRelation.INTERSECTS.getRelationName() + "] found relation [" + relation.getRelationName() + "]", errors);
-        }
-        return errors;
-    }
-
     @Override
     protected Query doToQuery(QueryShardContext context) throws IOException {
         ShapeBuilder shape;
@@ -432,9 +429,7 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
             }
         }
         builder.relation = ShapeRelation.DISJOINT.readFrom(in);
-        if (in.readBoolean()) {
-            builder.strategy = SpatialStrategy.RECURSIVE.readFrom(in);
-        }
+        builder.strategy = SpatialStrategy.RECURSIVE.readFrom(in);
         return builder;
     }
 
@@ -452,11 +447,7 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
             out.writeOptionalString(indexedShapePath);
         }
         relation.writeTo(out);
-        boolean hasStrategy = strategy != null;
-        out.writeBoolean(hasStrategy);
-        if (hasStrategy) {
-            strategy.writeTo(out);
-        }
+        strategy.writeTo(out);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoValidationMethod.java b/core/src/main/java/org/elasticsearch/index/query/GeoValidationMethod.java
new file mode 100644
index 0000000..aa7bd08
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoValidationMethod.java
@@ -0,0 +1,89 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.query;
+
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.io.stream.Writeable;
+import org.elasticsearch.common.util.CollectionUtils;
+
+import java.io.IOException;
+
+/**
+ * This enum is used to determine how to deal with invalid geo coordinates in geo related
+ * queries:
+ * 
+ *  On STRICT validation invalid coordinates cause an exception to be thrown.
+ *  On IGNORE_MALFORMED invalid coordinates are being accepted.
+ *  On COERCE invalid coordinates are being corrected to the most likely valid coordinate.
+ * */
+public enum GeoValidationMethod implements Writeable<GeoValidationMethod>{
+    COERCE, IGNORE_MALFORMED, STRICT;
+
+    public static final GeoValidationMethod DEFAULT = STRICT;
+    public static final boolean DEFAULT_LENIENT_PARSING = (DEFAULT != STRICT);
+    private static final GeoValidationMethod PROTOTYPE = DEFAULT;
+
+    @Override
+    public GeoValidationMethod readFrom(StreamInput in) throws IOException {
+        return GeoValidationMethod.values()[in.readVInt()];
+    }
+
+    public static GeoValidationMethod readGeoValidationMethodFrom(StreamInput in) throws IOException {
+        return PROTOTYPE.readFrom(in);
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeVInt(this.ordinal());
+    }
+
+    public static GeoValidationMethod fromString(String op) {
+        for (GeoValidationMethod method : GeoValidationMethod.values()) {
+            if (method.name().equalsIgnoreCase(op)) {
+                return method;
+            }
+        }
+        throw new IllegalArgumentException("operator needs to be either " + CollectionUtils.arrayAsArrayList(GeoValidationMethod.values())
+                + ", but not [" + op + "]");
+    }
+    
+    /** Returns whether or not to skip bounding box validation. */
+    public static boolean isIgnoreMalformed(GeoValidationMethod method) {
+        return (method == GeoValidationMethod.IGNORE_MALFORMED || method == GeoValidationMethod.COERCE);
+    }
+
+    /** Returns whether or not to try and fix broken/wrapping bounding boxes. */
+    public static boolean isCoerce(GeoValidationMethod method) {
+        return method == GeoValidationMethod.COERCE;
+    }
+
+    /** Returns validation method corresponding to given coerce and ignoreMalformed values. */
+    public static GeoValidationMethod infer(boolean coerce, boolean ignoreMalformed) {
+        if (coerce) {
+            return GeoValidationMethod.COERCE;
+        } else if (ignoreMalformed) {
+            return GeoValidationMethod.IGNORE_MALFORMED;
+        } else {
+            return GeoValidationMethod.STRICT;
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
index 1c3b912..b85db4b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
@@ -31,12 +31,7 @@ import org.elasticsearch.index.mapper.Uid;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 
 import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.Objects;
-import java.util.Set;
+import java.util.*;
 
 /**
  * A query that will return only documents matching specific ids (and a type).
@@ -149,14 +144,14 @@ public class IdsQueryBuilder extends AbstractQueryBuilder<IdsQueryBuilder> {
 
     @Override
     protected IdsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        IdsQueryBuilder idsQueryBuilder = new IdsQueryBuilder(in.readOptionalStringArray());
+        IdsQueryBuilder idsQueryBuilder = new IdsQueryBuilder(in.readStringArray());
         idsQueryBuilder.addIds(in.readStringArray());
         return idsQueryBuilder;
     }
 
     @Override
     protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeOptionalStringArray(types);
+        out.writeStringArray(types);
         out.writeStringArray(ids.toArray(new String[ids.size()]));
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java b/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
index 7c07de0..d4f7491 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
@@ -149,30 +149,12 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         return this.queryStringLenient;
     }
 
-    public IndicesQueriesRegistry indicesQueriesRegistry() {
+    IndicesQueriesRegistry indicesQueriesRegistry() {
         return indicesQueriesRegistry;
     }
 
-    //norelease this needs to go away
-    public ParsedQuery parse(QueryBuilder<?> queryBuilder) {
-        QueryShardContext context = cache.get();
-        context.reset();
-        context.parseFieldMatcher(parseFieldMatcher);
-        try {
-            return innerParse(context, queryBuilder);
-        } catch (ParsingException e) {
-            throw e;
-        } catch (Exception e) {
-            throw new QueryShardException(context, "failed to create query: {}", e, queryBuilder);
-        }
-    }
-
     public ParsedQuery parse(BytesReference source) {
-        return parse(cache.get(), source);
-    }
-
-    //norelease
-    public ParsedQuery parse(QueryShardContext context, BytesReference source) {
+        QueryShardContext context = cache.get();
         XContentParser parser = null;
         try {
             parser = XContentFactory.xContent(source).createParser(source);
@@ -200,11 +182,11 @@ public class IndexQueryParserService extends AbstractIndexComponent {
      * Parses an inner filter, returning null if the filter should be ignored.
      */
     @Nullable
-    //norelease
     public ParsedQuery parseInnerFilter(XContentParser parser) throws IOException {
         QueryShardContext context = cache.get();
         context.reset(parser);
         try {
+            context.parseFieldMatcher(parseFieldMatcher);
             Query filter = context.parseContext().parseInnerQueryBuilder().toFilter(context);
             if (filter == null) {
                 return null;
@@ -216,13 +198,6 @@ public class IndexQueryParserService extends AbstractIndexComponent {
     }
 
     @Nullable
-    public QueryBuilder<?> parseInnerQueryBuilder(QueryParseContext parseContext) throws IOException {
-        parseContext.parseFieldMatcher(parseFieldMatcher);
-        return parseContext.parseInnerQueryBuilder();
-    }
-
-    @Nullable
-    //norelease
     public Query parseInnerQuery(QueryShardContext context) throws IOException {
         Query query = context.parseContext().parseInnerQueryBuilder().toQuery(context);
         if (query == null) {
@@ -279,25 +254,20 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         }
     }
 
-    //norelease
     private ParsedQuery innerParse(QueryShardContext context, XContentParser parser) throws IOException, QueryShardException {
         context.reset(parser);
         try {
             context.parseFieldMatcher(parseFieldMatcher);
-            return innerParse(context, context.parseContext().parseInnerQueryBuilder());
+            Query query = context.parseContext().parseInnerQueryBuilder().toQuery(context);
+            if (query == null) {
+                query = Queries.newMatchNoDocsQuery();
+            }
+            return new ParsedQuery(query, context.copyNamedQueries());
         } finally {
             context.reset(null);
         }
     }
 
-    private static ParsedQuery innerParse(QueryShardContext context, QueryBuilder<?> queryBuilder) throws IOException, QueryShardException {
-        Query query = queryBuilder.toQuery(context);
-        if (query == null) {
-            query = Queries.newMatchNoDocsQuery();
-        }
-        return new ParsedQuery(query, context.copyNamedQueries());
-    }
-
     public ParseFieldMatcher parseFieldMatcher() {
         return parseFieldMatcher;
     }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java
index 3bc9bcb..2fde316 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java
@@ -20,11 +20,8 @@
 package org.elasticsearch.index.query;
 
 import org.apache.lucene.search.Query;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.NamedWriteable;
 import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentType;
 
 import java.io.IOException;
 
@@ -51,14 +48,6 @@ public interface QueryBuilder<QB extends QueryBuilder> extends NamedWriteable<QB
     Query toFilter(QueryShardContext context) throws IOException;
 
     /**
-     * Returns a {@link org.elasticsearch.common.bytes.BytesReference}
-     * containing the {@link ToXContent} output in binary format.
-     * Builds the request based on the default {@link XContentType}, either {@link Requests#CONTENT_TYPE} or provided as a constructor argument
-     */
-    //norelease once we move to serializing queries over the wire in Streamable format, this method shouldn't be needed anymore
-    BytesReference buildAsBytes();
-
-    /**
      * Sets the arbitrary name to be assigned to the query (see named queries).
      */
     QB queryName(String queryName);
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java b/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java
index 5accd28..de85abd 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java
@@ -33,21 +33,12 @@ public class QueryParseContext {
     private static final ParseField CACHE_KEY = new ParseField("_cache_key").withAllDeprecated("Filters are always used as cache keys");
 
     private XContentParser parser;
-    //norelease this flag is also used in the QueryShardContext, we need to make sure we set it there correctly in doToQuery()
     private ParseFieldMatcher parseFieldMatcher = ParseFieldMatcher.EMPTY;
 
-    //norelease this can eventually be deleted when context() method goes away
-    private final QueryShardContext shardContext;
     private IndicesQueriesRegistry indicesQueriesRegistry;
 
     public QueryParseContext(IndicesQueriesRegistry registry) {
         this.indicesQueriesRegistry = registry;
-        this.shardContext = null;
-    }
-
-    QueryParseContext(QueryShardContext context) {
-        this.shardContext = context;
-        this.indicesQueriesRegistry = context.indexQueryParserService().indicesQueriesRegistry();
     }
 
     public void reset(XContentParser jp) {
@@ -129,7 +120,7 @@ public class QueryParseContext {
      * @param name the name of the parser to retrieve
      * @return the query parser
      */
-    QueryParser queryParser(String name) {
+    private QueryParser queryParser(String name) {
         return indicesQueriesRegistry.queryParsers().get(name);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java b/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
index 38d27f3..23fc2bb 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
@@ -89,15 +89,12 @@ public class QueryShardContext {
 
     private final MapperQueryParser queryParser = new MapperQueryParser(this);
 
-    private ParseFieldMatcher parseFieldMatcher;
-
     private boolean allowUnmappedFields;
 
     private boolean mapUnmappedFieldAsString;
 
     private NestedScope nestedScope;
 
-    //norelease this should be possible to remove once query context are completely separated
     private QueryParseContext parseContext;
 
     boolean isFilter;
@@ -106,17 +103,15 @@ public class QueryShardContext {
         this.index = index;
         this.indexVersionCreated = Version.indexCreated(indexQueryParser.indexSettings());
         this.indexQueryParser = indexQueryParser;
-        this.parseContext = new QueryParseContext(this);
+        this.parseContext = new QueryParseContext(indexQueryParser.indicesQueriesRegistry());
     }
 
     public void parseFieldMatcher(ParseFieldMatcher parseFieldMatcher) {
-        //norelease ParseFieldMatcher is currently duplicated, this should be cleaned up
-        this.parseFieldMatcher = parseFieldMatcher;
         this.parseContext.parseFieldMatcher(parseFieldMatcher);
     }
 
     public ParseFieldMatcher parseFieldMatcher() {
-        return parseFieldMatcher;
+        return parseContext.parseFieldMatcher();
     }
 
     public void reset() {
@@ -127,7 +122,6 @@ public class QueryShardContext {
         this.nestedScope = new NestedScope();
     }
 
-    //norelease remove parser argument once query contexts are separated
     public void reset(XContentParser jp) {
         this.reset();
         this.parseContext.reset(jp);
@@ -137,7 +131,6 @@ public class QueryShardContext {
         return this.index;
     }
 
-    //norelease we might be able to avoid exposing the service to the outside world once all queries are refactored
     public IndexQueryParserService indexQueryParserService() {
         return indexQueryParser;
     }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
index 3c72adf..0df2460 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
@@ -18,25 +18,16 @@
  */
 package org.elasticsearch.index.query;
 
-import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.HasContextAndHeaders;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.lease.Releasables;
-import org.elasticsearch.common.xcontent.XContent;
-import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.script.*;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.script.Template;
 
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
-import static org.elasticsearch.common.Strings.hasLength;
-
 /**
  * In the simplest case, parse template string and variables from the request,
  * compile the template and execute the template against the given variables.
@@ -99,6 +90,4 @@ public class TemplateQueryParser implements QueryParser<TemplateQueryBuilder> {
     public TemplateQueryBuilder getBuilderPrototype() {
         return TemplateQueryBuilder.PROTOTYPE;
     }
-
-
 }
diff --git a/core/src/main/java/org/elasticsearch/index/search/stats/SearchSlowLog.java b/core/src/main/java/org/elasticsearch/index/search/stats/SearchSlowLog.java
index 108dab4..cfb7402 100644
--- a/core/src/main/java/org/elasticsearch/index/search/stats/SearchSlowLog.java
+++ b/core/src/main/java/org/elasticsearch/index/search/stats/SearchSlowLog.java
@@ -189,11 +189,24 @@ public final class SearchSlowLog{
                 sb.append("], ");
             }
             sb.append("search_type[").append(context.searchType()).append("], total_shards[").append(context.numberOfShards()).append("], ");
-            if (context.request().source() != null) {
-                sb.append("source[").append(context.request().source()).append("], ");
+            if (context.request().source() != null && context.request().source().length() > 0) {
+                try {
+                    sb.append("source[").append(XContentHelper.convertToJson(context.request().source(), reformat)).append("], ");
+                } catch (IOException e) {
+                    sb.append("source[_failed_to_convert_], ");
+                }
             } else {
                 sb.append("source[], ");
             }
+            if (context.request().extraSource() != null && context.request().extraSource().length() > 0) {
+                try {
+                    sb.append("extra_source[").append(XContentHelper.convertToJson(context.request().extraSource(), reformat)).append("], ");
+                } catch (IOException e) {
+                    sb.append("extra_source[_failed_to_convert_], ");
+                }
+            } else {
+                sb.append("extra_source[], ");
+            }
             return sb.toString();
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/index/shard/AbstractIndexShardComponent.java b/core/src/main/java/org/elasticsearch/index/shard/AbstractIndexShardComponent.java
index bb19dd4..76ae655 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/AbstractIndexShardComponent.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/AbstractIndexShardComponent.java
@@ -55,4 +55,6 @@ public abstract class AbstractIndexShardComponent implements IndexShardComponent
     public String nodeName() {
         return indexSettings.get("name", "");
     }
+
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
index a2f5b9d..137226d 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -46,10 +46,12 @@ import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.metrics.MeanMetric;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.concurrent.AbstractRefCounted;
+import org.elasticsearch.common.util.concurrent.AbstractRunnable;
 import org.elasticsearch.common.util.concurrent.FutureUtils;
 import org.elasticsearch.gateway.MetaDataStateFormat;
 import org.elasticsearch.index.IndexService;
@@ -114,7 +116,10 @@ import java.util.*;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.ScheduledFuture;
 import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicReference;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
 
 public class IndexShard extends AbstractIndexShardComponent {
 
@@ -176,12 +181,20 @@ public class IndexShard extends AbstractIndexShardComponent {
 
     private final ShardEngineFailListener failedEngineListener = new ShardEngineFailListener();
     private volatile boolean flushOnClose = true;
+    private volatile int flushThresholdOperations;
+    private volatile ByteSizeValue flushThresholdSize;
+    private volatile boolean disableFlush;
 
     /**
      * Index setting to control if a flush is executed before engine is closed
      * This setting is realtime updateable.
      */
     public static final String INDEX_FLUSH_ON_CLOSE = "index.flush_on_close";
+    public static final String INDEX_TRANSLOG_FLUSH_THRESHOLD_OPS = "index.translog.flush_threshold_ops";
+    public static final String INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE = "index.translog.flush_threshold_size";
+    public static final String INDEX_TRANSLOG_DISABLE_FLUSH = "index.translog.disable_flush";
+
+
     private final ShardPath path;
 
     private final IndexShardOperationCounter indexShardOperationCounter;
@@ -251,8 +264,10 @@ public class IndexShard extends AbstractIndexShardComponent {
             cachingPolicy = new UsageTrackingQueryCachingPolicy();
         }
         this.engineConfig = newEngineConfig(translogConfig, cachingPolicy);
+        this.flushThresholdOperations = indexSettings.getAsInt(INDEX_TRANSLOG_FLUSH_THRESHOLD_OPS, indexSettings.getAsInt("index.translog.flush_threshold", Integer.MAX_VALUE));
+        this.flushThresholdSize = indexSettings.getAsBytesSize(INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE, new ByteSizeValue(512, ByteSizeUnit.MB));
+        this.disableFlush = indexSettings.getAsBoolean(INDEX_TRANSLOG_DISABLE_FLUSH, false);
         this.indexShardOperationCounter = new IndexShardOperationCounter(logger, shardId);
-
     }
 
     public Store store() {
@@ -1051,6 +1066,25 @@ public class IndexShard extends AbstractIndexShardComponent {
         storeRecoveryService.recover(this, shouldExist, recoveryListener);
     }
 
+    /**
+     * Returns <code>true</code> iff this shard needs to be flushed due to too many translog operation or a too large transaction log.
+     * Otherwise <code>false</code>.
+     */
+    boolean shouldFlush() {
+        if (disableFlush == false) {
+            Engine engine = engineUnsafe();
+            if (engine != null) {
+                try {
+                    Translog translog = engine.getTranslog();
+                    return translog.totalOperations() > flushThresholdOperations || translog.sizeInBytes() > flushThresholdSize.bytes();
+                } catch (AlreadyClosedException ex) {
+                    // that's fine we are already close - no need to flush
+                }
+            }
+        }
+        return false;
+    }
+
     private class ApplyRefreshSettings implements IndexSettingsService.Listener {
         @Override
         public void onRefreshSettings(Settings settings) {
@@ -1059,6 +1093,22 @@ public class IndexShard extends AbstractIndexShardComponent {
                 if (state() == IndexShardState.CLOSED) { // no need to update anything if we are closed
                     return;
                 }
+                int flushThresholdOperations = settings.getAsInt(INDEX_TRANSLOG_FLUSH_THRESHOLD_OPS, IndexShard.this.flushThresholdOperations);
+                if (flushThresholdOperations != IndexShard.this.flushThresholdOperations) {
+                    logger.info("updating flush_threshold_ops from [{}] to [{}]", IndexShard.this.flushThresholdOperations, flushThresholdOperations);
+                    IndexShard.this.flushThresholdOperations = flushThresholdOperations;
+                }
+                ByteSizeValue flushThresholdSize = settings.getAsBytesSize(INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE, IndexShard.this.flushThresholdSize);
+                if (!flushThresholdSize.equals(IndexShard.this.flushThresholdSize)) {
+                    logger.info("updating flush_threshold_size from [{}] to [{}]", IndexShard.this.flushThresholdSize, flushThresholdSize);
+                    IndexShard.this.flushThresholdSize = flushThresholdSize;
+                }
+                boolean disableFlush = settings.getAsBoolean(INDEX_TRANSLOG_DISABLE_FLUSH, IndexShard.this.disableFlush);
+                if (disableFlush != IndexShard.this.disableFlush) {
+                    logger.info("updating disable_flush from [{}] to [{}]", IndexShard.this.disableFlush, disableFlush);
+                    IndexShard.this.disableFlush = disableFlush;
+                }
+
                 final EngineConfig config = engineConfig;
                 final boolean flushOnClose = settings.getAsBoolean(INDEX_FLUSH_ON_CLOSE, IndexShard.this.flushOnClose);
                 if (flushOnClose != IndexShard.this.flushOnClose) {
@@ -1438,4 +1488,48 @@ public class IndexShard extends AbstractIndexShardComponent {
         }
     }
 
+    private final AtomicBoolean asyncFlushRunning = new AtomicBoolean();
+
+    /**
+     * Schedules a flush if needed but won't schedule more than one flush concurrently. The flush will be executed on the
+     * Flush thread-pool asynchronously.
+     * @return <code>true</code> if a new flush is scheduled otherwise <code>false</code>.
+     */
+    public boolean maybeFlush() {
+        if (shouldFlush()) {
+            if (asyncFlushRunning.compareAndSet(false, true)) { // we can't use a lock here since we "release" in a different thread
+                if (shouldFlush() == false) {
+                    // we have to check again since otherwise there is a race when a thread passes
+                    // the first shouldFlush() check next to another thread which flushes fast enough
+                    // to finish before the current thread could flip the asyncFlushRunning flag.
+                    // in that situation we have an extra unexpected flush.
+                    asyncFlushRunning.compareAndSet(true, false);
+                } else {
+                    logger.debug("submitting async flush request");
+                    final AbstractRunnable abstractRunnable = new AbstractRunnable() {
+                        @Override
+                        public void onFailure(Throwable t) {
+                            if (state != IndexShardState.CLOSED) {
+                                logger.warn("failed to flush index", t);
+                            }
+                        }
+
+                        @Override
+                        protected void doRun() throws Exception {
+                            flush(new FlushRequest());
+                        }
+
+                        @Override
+                        public void onAfter() {
+                            asyncFlushRunning.compareAndSet(true, false);
+                            maybeFlush(); // fire a flush up again if we have filled up the limits such that shouldFlush() returns true
+                        }
+                    };
+                    threadPool.executor(ThreadPool.Names.FLUSH).execute(abstractRunnable);
+                    return true;
+                }
+            }
+        }
+        return false;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShardModule.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShardModule.java
index 0851b1c..08cf9b2 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexShardModule.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShardModule.java
@@ -20,18 +20,15 @@
 package org.elasticsearch.index.shard;
 
 import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.Classes;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.multibindings.Multibinder;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.cache.query.index.IndexQueryCache;
 import org.elasticsearch.index.engine.IndexSearcherWrapper;
 import org.elasticsearch.index.engine.IndexSearcherWrappingService;
 import org.elasticsearch.index.engine.EngineFactory;
 import org.elasticsearch.index.engine.InternalEngineFactory;
 import org.elasticsearch.index.percolator.stats.ShardPercolateService;
 import org.elasticsearch.index.termvectors.ShardTermVectorsService;
-import org.elasticsearch.index.translog.TranslogService;
 
 /**
  * The {@code IndexShardModule} module is responsible for binding the correct
@@ -68,7 +65,6 @@ public class IndexShardModule extends AbstractModule {
             bind(IndexShard.class).to(ShadowIndexShard.class).asEagerSingleton();
         } else {
             bind(IndexShard.class).asEagerSingleton();
-            bind(TranslogService.class).asEagerSingleton();
         }
 
         bind(EngineFactory.class).to(engineFactoryImpl);
diff --git a/core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java
index 9e8776d..502724e 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java
@@ -108,6 +108,12 @@ public final class ShadowIndexShard extends IndexShard {
         return engineFactory.newReadOnlyEngine(config);
     }
 
+    @Override
+    public boolean shouldFlush() {
+        // we don't need to flush since we don't write - all dominated by the primary
+        return false;
+    }
+
     public boolean allowsPrimaryPromotion() {
         return false;
     }
diff --git a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
index bf8c1a8..ad3b9c9 100644
--- a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
+++ b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
@@ -49,8 +49,8 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
     public static final BlobStoreIndexShardSnapshots PROTO = new BlobStoreIndexShardSnapshots();
 
     private final List<SnapshotFiles> shardSnapshots;
-    private final ImmutableMap<String, FileInfo> files;
-    private final ImmutableMap<String, List<FileInfo>> physicalFiles;
+    private final Map<String, FileInfo> files;
+    private final Map<String, List<FileInfo>> physicalFiles;
 
     public BlobStoreIndexShardSnapshots(List<SnapshotFiles> shardSnapshots) {
         this.shardSnapshots = Collections.unmodifiableList(new ArrayList<>(shardSnapshots));
@@ -108,8 +108,8 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
 
     private BlobStoreIndexShardSnapshots() {
         shardSnapshots = Collections.emptyList();
-        files = ImmutableMap.of();
-        physicalFiles = ImmutableMap.of();
+        files = Collections.emptyMap();
+        physicalFiles = Collections.emptyMap();
     }
 
 
diff --git a/core/src/main/java/org/elasticsearch/index/store/Store.java b/core/src/main/java/org/elasticsearch/index/store/Store.java
index 09f8ec2..0136c73 100644
--- a/core/src/main/java/org/elasticsearch/index/store/Store.java
+++ b/core/src/main/java/org/elasticsearch/index/store/Store.java
@@ -737,7 +737,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
 
         public static final MetadataSnapshot EMPTY = new MetadataSnapshot();
 
-        private final ImmutableMap<String, String> commitUserData;
+        private final Map<String, String> commitUserData;
 
         private final long numDocs;
 
diff --git a/core/src/main/java/org/elasticsearch/index/translog/TranslogService.java b/core/src/main/java/org/elasticsearch/index/translog/TranslogService.java
deleted file mode 100644
index 8897227..0000000
--- a/core/src/main/java/org/elasticsearch/index/translog/TranslogService.java
+++ /dev/null
@@ -1,207 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.translog;
-
-import org.elasticsearch.action.admin.indices.flush.FlushRequest;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.util.concurrent.FutureUtils;
-import org.elasticsearch.index.engine.FlushNotAllowedEngineException;
-import org.elasticsearch.index.settings.IndexSettings;
-import org.elasticsearch.index.settings.IndexSettingsService;
-import org.elasticsearch.index.shard.*;
-import org.elasticsearch.threadpool.ThreadPool;
-
-import java.io.Closeable;
-import java.util.concurrent.ScheduledFuture;
-import java.util.concurrent.ThreadLocalRandom;
-
-import static org.elasticsearch.common.unit.TimeValue.timeValueMillis;
-
-/**
- *
- */
-public class TranslogService extends AbstractIndexShardComponent implements Closeable {
-
-
-    public static final String INDEX_TRANSLOG_FLUSH_INTERVAL = "index.translog.interval";
-    public static final String INDEX_TRANSLOG_FLUSH_THRESHOLD_OPS = "index.translog.flush_threshold_ops";
-    public static final String INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE = "index.translog.flush_threshold_size";
-    public static final String INDEX_TRANSLOG_FLUSH_THRESHOLD_PERIOD =  "index.translog.flush_threshold_period";
-    public static final String INDEX_TRANSLOG_DISABLE_FLUSH = "index.translog.disable_flush";
-
-    private final ThreadPool threadPool;
-    private final IndexSettingsService indexSettingsService;
-    private final IndexShard indexShard;
-
-    private volatile TimeValue interval;
-    private volatile int flushThresholdOperations;
-    private volatile ByteSizeValue flushThresholdSize;
-    private volatile TimeValue flushThresholdPeriod;
-    private volatile boolean disableFlush;
-    private volatile ScheduledFuture future;
-
-    private final ApplySettings applySettings = new ApplySettings();
-
-    @Inject
-    public TranslogService(ShardId shardId, @IndexSettings Settings indexSettings, IndexSettingsService indexSettingsService, ThreadPool threadPool, IndexShard indexShard) {
-        super(shardId, indexSettings);
-        this.threadPool = threadPool;
-        this.indexSettingsService = indexSettingsService;
-        this.indexShard = indexShard;
-        this.flushThresholdOperations = indexSettings.getAsInt(INDEX_TRANSLOG_FLUSH_THRESHOLD_OPS, indexSettings.getAsInt("index.translog.flush_threshold", Integer.MAX_VALUE));
-        this.flushThresholdSize = indexSettings.getAsBytesSize(INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE, new ByteSizeValue(512, ByteSizeUnit.MB));
-        this.flushThresholdPeriod = indexSettings.getAsTime(INDEX_TRANSLOG_FLUSH_THRESHOLD_PERIOD, TimeValue.timeValueMinutes(30));
-        this.interval = indexSettings.getAsTime(INDEX_TRANSLOG_FLUSH_INTERVAL, timeValueMillis(5000));
-        this.disableFlush = indexSettings.getAsBoolean(INDEX_TRANSLOG_DISABLE_FLUSH, false);
-        logger.debug("interval [{}], flush_threshold_ops [{}], flush_threshold_size [{}], flush_threshold_period [{}]", interval, flushThresholdOperations, flushThresholdSize, flushThresholdPeriod);
-
-        this.future = threadPool.schedule(interval, ThreadPool.Names.SAME, new TranslogBasedFlush());
-
-        indexSettingsService.addListener(applySettings);
-    }
-
-    @Override
-    public void close() {
-        indexSettingsService.removeListener(applySettings);
-        FutureUtils.cancel(this.future);
-    }
-
-
-
-    class ApplySettings implements IndexSettingsService.Listener {
-        @Override
-        public void onRefreshSettings(Settings settings) {
-            int flushThresholdOperations = settings.getAsInt(INDEX_TRANSLOG_FLUSH_THRESHOLD_OPS, TranslogService.this.flushThresholdOperations);
-            if (flushThresholdOperations != TranslogService.this.flushThresholdOperations) {
-                logger.info("updating flush_threshold_ops from [{}] to [{}]", TranslogService.this.flushThresholdOperations, flushThresholdOperations);
-                TranslogService.this.flushThresholdOperations = flushThresholdOperations;
-            }
-            ByteSizeValue flushThresholdSize = settings.getAsBytesSize(INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE, TranslogService.this.flushThresholdSize);
-            if (!flushThresholdSize.equals(TranslogService.this.flushThresholdSize)) {
-                logger.info("updating flush_threshold_size from [{}] to [{}]", TranslogService.this.flushThresholdSize, flushThresholdSize);
-                TranslogService.this.flushThresholdSize = flushThresholdSize;
-            }
-            TimeValue flushThresholdPeriod = settings.getAsTime(INDEX_TRANSLOG_FLUSH_THRESHOLD_PERIOD, TranslogService.this.flushThresholdPeriod);
-            if (!flushThresholdPeriod.equals(TranslogService.this.flushThresholdPeriod)) {
-                logger.info("updating flush_threshold_period from [{}] to [{}]", TranslogService.this.flushThresholdPeriod, flushThresholdPeriod);
-                TranslogService.this.flushThresholdPeriod = flushThresholdPeriod;
-            }
-            TimeValue interval = settings.getAsTime(INDEX_TRANSLOG_FLUSH_INTERVAL, TranslogService.this.interval);
-            if (!interval.equals(TranslogService.this.interval)) {
-                logger.info("updating interval from [{}] to [{}]", TranslogService.this.interval, interval);
-                TranslogService.this.interval = interval;
-            }
-            boolean disableFlush = settings.getAsBoolean(INDEX_TRANSLOG_DISABLE_FLUSH, TranslogService.this.disableFlush);
-            if (disableFlush != TranslogService.this.disableFlush) {
-                logger.info("updating disable_flush from [{}] to [{}]", TranslogService.this.disableFlush, disableFlush);
-                TranslogService.this.disableFlush = disableFlush;
-            }
-        }
-    }
-
-    private TimeValue computeNextInterval() {
-        return new TimeValue(interval.millis() + (ThreadLocalRandom.current().nextLong(interval.millis())));
-    }
-
-    private class TranslogBasedFlush implements Runnable {
-
-        private volatile long lastFlushTime = System.currentTimeMillis();
-
-        @Override
-        public void run() {
-            if (indexShard.state() == IndexShardState.CLOSED) {
-                return;
-            }
-
-            // flush is disabled, but still reschedule
-            if (disableFlush) {
-                reschedule();
-                return;
-            }
-            Translog translog = indexShard.engine().getTranslog();
-            if (translog == null) {
-                reschedule();
-                return;
-            }
-            int currentNumberOfOperations = translog.totalOperations();
-            if (currentNumberOfOperations == 0) {
-                reschedule();
-                return;
-            }
-
-            if (flushThresholdOperations > 0) {
-                if (currentNumberOfOperations > flushThresholdOperations) {
-                    logger.trace("flushing translog, operations [{}], breached [{}]", currentNumberOfOperations, flushThresholdOperations);
-                    asyncFlushAndReschedule();
-                    return;
-                }
-            }
-
-            if (flushThresholdSize.bytes() > 0) {
-                long sizeInBytes = translog.sizeInBytes();
-                if (sizeInBytes > flushThresholdSize.bytes()) {
-                    logger.trace("flushing translog, size [{}], breached [{}]", new ByteSizeValue(sizeInBytes), flushThresholdSize);
-                    asyncFlushAndReschedule();
-                    return;
-                }
-            }
-
-            if (flushThresholdPeriod.millis() > 0) {
-                if ((threadPool.estimatedTimeInMillis() - lastFlushTime) > flushThresholdPeriod.millis()) {
-                    logger.trace("flushing translog, last_flush_time [{}], breached [{}]", lastFlushTime, flushThresholdPeriod);
-                    asyncFlushAndReschedule();
-                    return;
-                }
-            }
-
-            reschedule();
-        }
-
-        private void reschedule() {
-            future = threadPool.schedule(computeNextInterval(), ThreadPool.Names.SAME, this);
-        }
-
-        private void asyncFlushAndReschedule() {
-            threadPool.executor(ThreadPool.Names.FLUSH).execute(new Runnable() {
-                @Override
-                public void run() {
-                    try {
-                        indexShard.flush(new FlushRequest());
-                    } catch (IllegalIndexShardStateException e) {
-                        // we are being closed, or in created state, ignore
-                    } catch (FlushNotAllowedEngineException e) {
-                        // ignore this exception, we are not allowed to perform flush
-                    } catch (Throwable e) {
-                        logger.warn("failed to flush shard on translog threshold", e);
-                    }
-                    lastFlushTime = threadPool.estimatedTimeInMillis();
-
-                    if (indexShard.state() != IndexShardState.CLOSED) {
-                        future = threadPool.schedule(computeNextInterval(), ThreadPool.Names.SAME, TranslogBasedFlush.this);
-                    }
-                }
-            });
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java b/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
index 5048c7d..4ab4691 100644
--- a/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
+++ b/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
@@ -207,6 +207,11 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
      * Can the shard request be cached at all?
      */
     public boolean canCache(ShardSearchRequest request, SearchContext context) {
+        // TODO: for now, template is not supported, though we could use the generated bytes as the key
+        if (hasLength(request.templateSource())) {
+            return false;
+        }
+
         // for now, only enable it for requests with no hits
         if (context.size() != 0) {
             return false;
diff --git a/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java b/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java
index 2bf6946..f540208 100644
--- a/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java
+++ b/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java
@@ -38,12 +38,7 @@ import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.monitor.jvm.JvmInfo;
 import org.elasticsearch.threadpool.ThreadPool;
 
-import java.util.ArrayList;
-import java.util.EnumSet;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 import java.util.concurrent.ScheduledFuture;
 
 /**
@@ -258,7 +253,7 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
                     }
 
                     // consider shard inactive if it has same translogFileGeneration and no operations for a long time
-                    if (status.translogId == translog.currentFileGeneration() && translog.totalOperations() == 0) {
+                    if (status.translogId == translog.currentFileGeneration() && translog.totalOperations() == status.translogNumberOfOperations) {
                         if (status.timeMS == -1) {
                             // first time we noticed the shard become idle
                             status.timeMS = timeMS;
@@ -282,6 +277,7 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
                         status.timeMS = -1;
                     }
                     status.translogId = translog.currentFileGeneration();
+                    status.translogNumberOfOperations = translog.totalOperations();
 
                     if (status.activeIndexing) {
                         activeShards++;
@@ -376,6 +372,7 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
 
     private static class ShardIndexingStatus {
         long translogId = -1;
+        long translogNumberOfOperations = -1;
         boolean activeIndexing = true;
         long timeMS = -1; // contains the first time we saw this shard with no operations done on it
     }
diff --git a/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java b/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java
index f7c7f16..45b19ae 100644
--- a/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java
+++ b/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java
@@ -287,7 +287,12 @@ public class IndicesStore extends AbstractComponent implements ClusterStateListe
                 return;
             }
 
-            clusterService.submitStateUpdateTask("indices_store ([" + shardId + "] active fully on other nodes)", new ClusterStateNonMasterUpdateTask() {
+            clusterService.submitStateUpdateTask("indices_store ([" + shardId + "] active fully on other nodes)", new ClusterStateUpdateTask() {
+                @Override
+                public boolean runOnlyOnMaster() {
+                    return false;
+                }
+
                 @Override
                 public ClusterState execute(ClusterState currentState) throws Exception {
                     if (clusterState.getVersion() != currentState.getVersion()) {
diff --git a/core/src/main/java/org/elasticsearch/monitor/jvm/DeadlockAnalyzer.java b/core/src/main/java/org/elasticsearch/monitor/jvm/DeadlockAnalyzer.java
index fe3cbc1..58e7189 100644
--- a/core/src/main/java/org/elasticsearch/monitor/jvm/DeadlockAnalyzer.java
+++ b/core/src/main/java/org/elasticsearch/monitor/jvm/DeadlockAnalyzer.java
@@ -70,7 +70,7 @@ public class DeadlockAnalyzer {
     }
 
 
-    private Set<LinkedHashSet<ThreadInfo>> calculateCycles(ImmutableMap<Long, ThreadInfo> threadInfoMap) {
+    private Set<LinkedHashSet<ThreadInfo>> calculateCycles(Map<Long, ThreadInfo> threadInfoMap) {
         Set<LinkedHashSet<ThreadInfo>> cycles = new HashSet<>();
         for (Map.Entry<Long, ThreadInfo> entry : threadInfoMap.entrySet()) {
             LinkedHashSet<ThreadInfo> cycle = new LinkedHashSet<>();
diff --git a/core/src/main/java/org/elasticsearch/node/service/NodeService.java b/core/src/main/java/org/elasticsearch/node/service/NodeService.java
index 1964753..369f7e0 100644
--- a/core/src/main/java/org/elasticsearch/node/service/NodeService.java
+++ b/core/src/main/java/org/elasticsearch/node/service/NodeService.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.node.service;
 
 import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.Build;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;
@@ -41,6 +42,7 @@ import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
 import java.io.IOException;
+import java.util.Map;
 
 /**
  */
@@ -101,7 +103,7 @@ public class NodeService extends AbstractComponent {
     /**
      * Attributes different services in the node can add to be reported as part of the node info (for example).
      */
-    public ImmutableMap<String, String> attributes() {
+    public Map<String, String> attributes() {
         return this.serviceAttributes;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
index 7252190..9d3aac4 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
@@ -84,6 +84,7 @@ public class PluginManager {
                     "discovery-azure",
                     "discovery-ec2",
                     "discovery-multicast",
+                    "lang-expression",
                     "lang-javascript",
                     "lang-python",
                     "mapper-murmur3",
diff --git a/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java b/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java
index 9cea2e9..d485e47 100644
--- a/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java
+++ b/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java
@@ -58,7 +58,7 @@ public class RepositoriesService extends AbstractComponent implements ClusterSta
 
     private final VerifyNodeRepositoryAction verifyAction;
 
-    private volatile ImmutableMap<String, RepositoryHolder> repositories = ImmutableMap.of();
+    private volatile Map<String, RepositoryHolder> repositories = ImmutableMap.of();
 
     @Inject
     public RepositoriesService(Settings settings, ClusterService clusterService, TransportService transportService, RepositoryTypesRegistry typesRegistry, Injector injector) {
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetFieldMappingAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetFieldMappingAction.java
index eb53129..7594a09 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetFieldMappingAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetFieldMappingAction.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.rest.action.admin.indices.mapping.get;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.action.admin.indices.mapping.get.GetFieldMappingsRequest;
 import org.elasticsearch.action.admin.indices.mapping.get.GetFieldMappingsResponse;
 import org.elasticsearch.action.admin.indices.mapping.get.GetFieldMappingsResponse.FieldMappingMetaData;
@@ -29,7 +28,13 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.rest.*;
+import org.elasticsearch.rest.BaseRestHandler;
+import org.elasticsearch.rest.BytesRestResponse;
+import org.elasticsearch.rest.RestChannel;
+import org.elasticsearch.rest.RestController;
+import org.elasticsearch.rest.RestRequest;
+import org.elasticsearch.rest.RestResponse;
+import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.rest.action.support.RestBuilderListener;
 
 import java.io.IOException;
@@ -64,11 +69,9 @@ public class RestGetFieldMappingAction extends BaseRestHandler {
         getMappingsRequest.indicesOptions(IndicesOptions.fromRequest(request, getMappingsRequest.indicesOptions()));
         getMappingsRequest.local(request.paramAsBoolean("local", getMappingsRequest.local()));
         client.admin().indices().getFieldMappings(getMappingsRequest, new RestBuilderListener<GetFieldMappingsResponse>(channel) {
-
-            @SuppressWarnings("unchecked")
             @Override
             public RestResponse buildResponse(GetFieldMappingsResponse response, XContentBuilder builder) throws Exception {
-                ImmutableMap<String, ImmutableMap<String, ImmutableMap<String, FieldMappingMetaData>>> mappingsByIndex = response.mappings();
+                Map<String, Map<String, Map<String, FieldMappingMetaData>>> mappingsByIndex = response.mappings();
 
                 boolean isPossibleSingleFieldRequest = indices.length == 1 && types.length == 1 && fields.length == 1;
                 if (isPossibleSingleFieldRequest && isFieldMappingMissingField(mappingsByIndex)) {
@@ -91,13 +94,13 @@ public class RestGetFieldMappingAction extends BaseRestHandler {
      * Helper method to find out if the only included fieldmapping metadata is typed NULL, which means
      * that type and index exist, but the field did not
      */
-    private boolean isFieldMappingMissingField(ImmutableMap<String, ImmutableMap<String, ImmutableMap<String, FieldMappingMetaData>>> mappingsByIndex) throws IOException {
+    private boolean isFieldMappingMissingField(Map<String, Map<String, Map<String, FieldMappingMetaData>>> mappingsByIndex) throws IOException {
         if (mappingsByIndex.size() != 1) {
             return false;
         }
 
-        for (ImmutableMap<String, ImmutableMap<String, FieldMappingMetaData>> value : mappingsByIndex.values()) {
-            for (ImmutableMap<String, FieldMappingMetaData> fieldValue : value.values()) {
+        for (Map<String, Map<String, FieldMappingMetaData>> value : mappingsByIndex.values()) {
+            for (Map<String, FieldMappingMetaData> fieldValue : value.values()) {
                 for (Map.Entry<String, FieldMappingMetaData> fieldMappingMetaDataEntry : fieldValue.entrySet()) {
                     if (fieldMappingMetaDataEntry.getValue().isNull()) {
                         return true;
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java
index d67635c..6766196 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java
@@ -29,13 +29,7 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.BytesRestResponse;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.RestResponse;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestBuilderListener;
 
@@ -67,11 +61,9 @@ public class RestValidateQueryAction extends BaseRestHandler {
         if (RestActions.hasBodyContent(request)) {
             validateQueryRequest.source(RestActions.getRestContent(request));
         } else {
-            QueryBuilder<?> queryBuilder = RestActions.parseQuerySource(request);
-            if (queryBuilder != null) {
-                QuerySourceBuilder querySourceBuilder = new QuerySourceBuilder();
-                querySourceBuilder.setQuery(queryBuilder);
-                validateQueryRequest.source(querySourceBuilder.buildAsBytes());
+            QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+            if (querySourceBuilder != null) {
+                validateQueryRequest.source(querySourceBuilder);
             }
         }
         validateQueryRequest.types(Strings.splitStringByCommaToArray(request.param("type")));
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java
index c50cf18..4c421cc 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java
@@ -19,27 +19,17 @@
 package org.elasticsearch.rest.action.admin.indices.warmer.put;
 
 import org.elasticsearch.action.admin.indices.warmer.put.PutWarmerRequest;
+import org.elasticsearch.action.admin.indices.warmer.put.PutWarmerResponse;
 import org.elasticsearch.action.search.SearchRequest;
 import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.AcknowledgedRestListener;
-import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 
-import java.io.IOException;
-
 import static org.elasticsearch.rest.RestRequest.Method.POST;
 import static org.elasticsearch.rest.RestRequest.Method.PUT;
 
@@ -47,12 +37,9 @@ import static org.elasticsearch.rest.RestRequest.Method.PUT;
  */
 public class RestPutWarmerAction extends BaseRestHandler {
 
-    private final IndicesQueriesRegistry queryRegistry;
-
     @Inject
-    public RestPutWarmerAction(Settings settings, RestController controller, Client client, IndicesQueriesRegistry queryRegistry) {
+    public RestPutWarmerAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
-        this.queryRegistry = queryRegistry;
         controller.registerHandler(PUT, "/_warmer/{name}", this);
         controller.registerHandler(PUT, "/{index}/_warmer/{name}", this);
         controller.registerHandler(PUT, "/{index}/{type}/_warmer/{name}", this);
@@ -71,17 +58,12 @@ public class RestPutWarmerAction extends BaseRestHandler {
     }
 
     @Override
-    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) throws IOException {
+    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) {
         PutWarmerRequest putWarmerRequest = new PutWarmerRequest(request.param("name"));
-
-        BytesReference sourceBytes = RestActions.getRestContent(request);
-        XContentParser parser = XContentFactory.xContent(sourceBytes).createParser(sourceBytes);
-        QueryParseContext queryParseContext = new QueryParseContext(queryRegistry);
-        queryParseContext.reset(parser);
-        SearchSourceBuilder source = SearchSourceBuilder.PROTOTYPE.fromXContent(parser, queryParseContext);
         SearchRequest searchRequest = new SearchRequest(Strings.splitStringByCommaToArray(request.param("index")))
                 .types(Strings.splitStringByCommaToArray(request.param("type")))
-                .requestCache(request.paramAsBoolean("request_cache", null)).source(source);
+                .requestCache(request.paramAsBoolean("request_cache", null))
+                .source(request.content());
         searchRequest.indicesOptions(IndicesOptions.fromRequest(request, searchRequest.indicesOptions()));
         putWarmerRequest.searchRequest(searchRequest);
         putWarmerRequest.timeout(request.paramAsTime("timeout", putWarmerRequest.timeout()));
diff --git a/core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java b/core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java
index 86029e5..72057a9 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java
@@ -19,48 +19,32 @@
 
 package org.elasticsearch.rest.action.cat;
 
-import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.count.CountRequest;
 import org.elasticsearch.action.count.CountResponse;
 import org.elasticsearch.action.support.QuerySourceBuilder;
-import org.elasticsearch.bootstrap.Elasticsearch;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.Table;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.RestResponse;
-import org.elasticsearch.rest.action.search.RestSearchAction;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestResponseListener;
 import org.elasticsearch.rest.action.support.RestTable;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.joda.time.format.DateTimeFormat;
 import org.joda.time.format.DateTimeFormatter;
 
-import java.io.IOException;
 import java.util.concurrent.TimeUnit;
 
 import static org.elasticsearch.rest.RestRequest.Method.GET;
 
 public class RestCountAction extends AbstractCatAction {
 
-    private final IndicesQueriesRegistry indicesQueriesRegistry;
-
     @Inject
-    public RestCountAction(Settings settings, RestController restController, RestController controller, Client client, IndicesQueriesRegistry indicesQueriesRegistry) {
+    public RestCountAction(Settings settings, RestController restController, RestController controller, Client client) {
         super(settings, controller, client);
         restController.registerHandler(GET, "/_cat/count", this);
         restController.registerHandler(GET, "/_cat/count/{index}", this);
-        this.indicesQueriesRegistry = indicesQueriesRegistry;
     }
 
     @Override
@@ -75,22 +59,14 @@ public class RestCountAction extends AbstractCatAction {
         CountRequest countRequest = new CountRequest(indices);
         String source = request.param("source");
         if (source != null) {
-            try (XContentParser requestParser = XContentFactory.xContent(source).createParser(source)) {
-                QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
-                context.reset(requestParser);
-                final SearchSourceBuilder builder = SearchSourceBuilder.PROTOTYPE.fromXContent(requestParser, context);
-                countRequest.searchSource(builder);
-            } catch (IOException e) {
-                throw new ElasticsearchException("failed to parse source", e);
-            }
+            countRequest.source(source);
         } else {
-            QueryBuilder<?> queryBuilder = RestActions.parseQuerySource(request);
-            if (queryBuilder != null) {
-                QuerySourceBuilder querySourceBuilder = new QuerySourceBuilder();
-                querySourceBuilder.setQuery(queryBuilder);
-                countRequest.query(queryBuilder);
+            QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+            if (querySourceBuilder != null) {
+                countRequest.source(querySourceBuilder);
             }
         }
+
         client.count(countRequest, new RestResponseListener<CountResponse>(channel) {
             @Override
             public RestResponse buildResponse(CountResponse countResponse) throws Exception {
diff --git a/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodeAttrsAction.java b/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodeAttrsAction.java
index c9d3a2d..4193208 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodeAttrsAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodeAttrsAction.java
@@ -18,7 +18,6 @@
  */
 
 package org.elasticsearch.rest.action.cat;
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;
 import org.elasticsearch.action.admin.cluster.node.info.NodesInfoRequest;
 import org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse;
@@ -34,11 +33,16 @@ import org.elasticsearch.common.Table;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.InetSocketTransportAddress;
-import org.elasticsearch.rest.*;
+import org.elasticsearch.rest.RestChannel;
+import org.elasticsearch.rest.RestController;
+import org.elasticsearch.rest.RestRequest;
+import org.elasticsearch.rest.RestResponse;
 import org.elasticsearch.rest.action.support.RestActionListener;
 import org.elasticsearch.rest.action.support.RestResponseListener;
 import org.elasticsearch.rest.action.support.RestTable;
 
+import java.util.Map;
+
 import static org.elasticsearch.rest.RestRequest.Method.GET;
 
 public class RestNodeAttrsAction extends AbstractCatAction {
@@ -107,7 +111,7 @@ public class RestNodeAttrsAction extends AbstractCatAction {
 
         for (DiscoveryNode node : nodes) {
             NodeInfo info = nodesInfo.getNodesMap().get(node.id());
-            ImmutableMap<String, String> attrs = node.getAttributes();
+            Map<String, String> attrs = node.getAttributes();
             for(String att : attrs.keySet()) {
                 table.startRow();
                 table.addCell(node.name());
diff --git a/core/src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java b/core/src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java
index c0548ce..677f3af 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java
@@ -19,49 +19,32 @@
 
 package org.elasticsearch.rest.action.count;
 
-import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.count.CountRequest;
 import org.elasticsearch.action.count.CountResponse;
 import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.BytesRestResponse;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.RestResponse;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestBuilderListener;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-
-import java.io.IOException;
 
 import static org.elasticsearch.action.count.CountRequest.DEFAULT_MIN_SCORE;
+import static org.elasticsearch.search.internal.SearchContext.DEFAULT_TERMINATE_AFTER;
 import static org.elasticsearch.rest.RestRequest.Method.GET;
 import static org.elasticsearch.rest.RestRequest.Method.POST;
 import static org.elasticsearch.rest.action.support.RestActions.buildBroadcastShardsHeader;
-import static org.elasticsearch.search.internal.SearchContext.DEFAULT_TERMINATE_AFTER;
 
 /**
  *
  */
 public class RestCountAction extends BaseRestHandler {
 
-    private final IndicesQueriesRegistry indicesQueriesRegistry;
-
     @Inject
-    public RestCountAction(Settings settings, RestController controller, Client client, IndicesQueriesRegistry indicesQueriesRegistry) {
+    public RestCountAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
         controller.registerHandler(POST, "/_count", this);
         controller.registerHandler(GET, "/_count", this);
@@ -69,7 +52,6 @@ public class RestCountAction extends BaseRestHandler {
         controller.registerHandler(GET, "/{index}/_count", this);
         controller.registerHandler(POST, "/{index}/{type}/_count", this);
         controller.registerHandler(GET, "/{index}/{type}/_count", this);
-        this.indicesQueriesRegistry = indicesQueriesRegistry;
     }
 
     @Override
@@ -77,19 +59,11 @@ public class RestCountAction extends BaseRestHandler {
         CountRequest countRequest = new CountRequest(Strings.splitStringByCommaToArray(request.param("index")));
         countRequest.indicesOptions(IndicesOptions.fromRequest(request, countRequest.indicesOptions()));
         if (RestActions.hasBodyContent(request)) {
-            BytesReference restContent = RestActions.getRestContent(request);
-            try (XContentParser requestParser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
-                context.reset(requestParser);
-                final SearchSourceBuilder builder = SearchSourceBuilder.PROTOTYPE.fromXContent(requestParser, context);
-                countRequest.searchSource(builder);
-            } catch (IOException e) {
-                throw new ElasticsearchException("failed to parse source", e);
-            }
+            countRequest.source(RestActions.getRestContent(request));
         } else {
-            QueryBuilder<?> queryBuilder = RestActions.parseQuerySource(request);
-            if (queryBuilder != null) {
-                countRequest.query(queryBuilder);
+            QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+            if (querySourceBuilder != null) {
+                countRequest.source(querySourceBuilder);
             }
         }
         countRequest.routing(request.param("routing"));
diff --git a/core/src/main/java/org/elasticsearch/rest/action/exists/RestExistsAction.java b/core/src/main/java/org/elasticsearch/rest/action/exists/RestExistsAction.java
index 1fc8400..7cfe7ca 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/exists/RestExistsAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/exists/RestExistsAction.java
@@ -27,14 +27,7 @@ import org.elasticsearch.client.Client;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.BytesRestResponse;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.RestResponse;
-import org.elasticsearch.rest.RestStatus;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestBuilderListener;
 
@@ -58,11 +51,9 @@ public class RestExistsAction extends BaseRestHandler {
         if (RestActions.hasBodyContent(request)) {
             existsRequest.source(RestActions.getRestContent(request));
         } else {
-            QueryBuilder<?> queryBuilder = RestActions.parseQuerySource(request);
-            if (queryBuilder != null) {
-                QuerySourceBuilder querySourceBuilder = new QuerySourceBuilder();
-                querySourceBuilder.setQuery(queryBuilder);
-                existsRequest.source(querySourceBuilder.buildAsBytes());
+            QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+            if (querySourceBuilder != null) {
+                existsRequest.source(querySourceBuilder);
             }
         }
         existsRequest.routing(request.param("routing"));
diff --git a/core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java b/core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java
index 31bf0ab..af1f2f4 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java
@@ -20,31 +20,16 @@
 package org.elasticsearch.rest.action.search;
 
 import org.elasticsearch.action.search.MultiSearchRequest;
-import org.elasticsearch.action.search.SearchRequest;
+import org.elasticsearch.action.search.MultiSearchResponse;
 import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.client.Client;
-import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContent;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.TemplateQueryParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestToXContentListener;
-import org.elasticsearch.script.Template;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 
-import java.util.Map;
-
-import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeBooleanValue;
-import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeStringArrayValue;
-import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeStringValue;
 import static org.elasticsearch.rest.RestRequest.Method.GET;
 import static org.elasticsearch.rest.RestRequest.Method.POST;
 
@@ -53,11 +38,9 @@ import static org.elasticsearch.rest.RestRequest.Method.POST;
 public class RestMultiSearchAction extends BaseRestHandler {
 
     private final boolean allowExplicitIndex;
-    private final IndicesQueriesRegistry indicesQueriesRegistry;
-
 
     @Inject
-    public RestMultiSearchAction(Settings settings, RestController controller, Client client, IndicesQueriesRegistry indicesQueriesRegistry) {
+    public RestMultiSearchAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
 
         controller.registerHandler(GET, "/_msearch", this);
@@ -75,7 +58,6 @@ public class RestMultiSearchAction extends BaseRestHandler {
         controller.registerHandler(POST, "/{index}/{type}/_msearch/template", this);
 
         this.allowExplicitIndex = settings.getAsBoolean("rest.action.multi.allow_explicit_index", true);
-        this.indicesQueriesRegistry = indicesQueriesRegistry;
     }
 
     @Override
@@ -87,117 +69,12 @@ public class RestMultiSearchAction extends BaseRestHandler {
         String path = request.path();
         boolean isTemplateRequest = isTemplateRequest(path);
         IndicesOptions indicesOptions = IndicesOptions.fromRequest(request, multiSearchRequest.indicesOptions());
-        parseRequest(multiSearchRequest, RestActions.getRestContent(request), isTemplateRequest, indices, types, request.param("search_type"), request.param("routing"), indicesOptions, allowExplicitIndex, indicesQueriesRegistry);
-        client.multiSearch(multiSearchRequest, new RestToXContentListener<>(channel));
+        multiSearchRequest.add(RestActions.getRestContent(request), isTemplateRequest, indices, types, request.param("search_type"), request.param("routing"), indicesOptions, allowExplicitIndex);
+
+        client.multiSearch(multiSearchRequest, new RestToXContentListener<MultiSearchResponse>(channel));
     }
 
     private boolean isTemplateRequest(String path) {
         return (path != null && path.endsWith("/template"));
     }
-
-    public static MultiSearchRequest parseRequest(MultiSearchRequest msr, BytesReference data, boolean isTemplateRequest,
-                                                   @Nullable String[] indices,
-                                                   @Nullable String[] types,
-                                                   @Nullable String searchType,
-                                                   @Nullable String routing,
-                                                   IndicesOptions indicesOptions,
-                                                   boolean allowExplicitIndex, IndicesQueriesRegistry indicesQueriesRegistry) throws Exception {
-        XContent xContent = XContentFactory.xContent(data);
-        int from = 0;
-        int length = data.length();
-        byte marker = xContent.streamSeparator();
-        final QueryParseContext queryParseContext = new QueryParseContext(indicesQueriesRegistry);
-        while (true) {
-            int nextMarker = findNextMarker(marker, from, data, length);
-            if (nextMarker == -1) {
-                break;
-            }
-            // support first line with \n
-            if (nextMarker == 0) {
-                from = nextMarker + 1;
-                continue;
-            }
-
-            SearchRequest searchRequest = new SearchRequest();
-            if (indices != null) {
-                searchRequest.indices(indices);
-            }
-            if (indicesOptions != null) {
-                searchRequest.indicesOptions(indicesOptions);
-            }
-            if (types != null && types.length > 0) {
-                searchRequest.types(types);
-            }
-            if (routing != null) {
-                searchRequest.routing(routing);
-            }
-            searchRequest.searchType(searchType);
-
-            IndicesOptions defaultOptions = IndicesOptions.strictExpandOpenAndForbidClosed();
-
-
-            // now parse the action
-            if (nextMarker - from > 0) {
-                try (XContentParser parser = xContent.createParser(data.slice(from, nextMarker - from))) {
-                    Map<String, Object> source = parser.map();
-                    for (Map.Entry<String, Object> entry : source.entrySet()) {
-                        Object value = entry.getValue();
-                        if ("index".equals(entry.getKey()) || "indices".equals(entry.getKey())) {
-                            if (!allowExplicitIndex) {
-                                throw new IllegalArgumentException("explicit index in multi percolate is not allowed");
-                            }
-                            searchRequest.indices(nodeStringArrayValue(value));
-                        } else if ("type".equals(entry.getKey()) || "types".equals(entry.getKey())) {
-                            searchRequest.types(nodeStringArrayValue(value));
-                        } else if ("search_type".equals(entry.getKey()) || "searchType".equals(entry.getKey())) {
-                            searchRequest.searchType(nodeStringValue(value, null));
-                        } else if ("request_cache".equals(entry.getKey()) || "requestCache".equals(entry.getKey())) {
-                            searchRequest.requestCache(nodeBooleanValue(value));
-                        } else if ("preference".equals(entry.getKey())) {
-                            searchRequest.preference(nodeStringValue(value, null));
-                        } else if ("routing".equals(entry.getKey())) {
-                            searchRequest.routing(nodeStringValue(value, null));
-                        }
-                    }
-                    defaultOptions = IndicesOptions.fromMap(source, defaultOptions);
-                }
-            }
-            searchRequest.indicesOptions(defaultOptions);
-
-            // move pointers
-            from = nextMarker + 1;
-            // now for the body
-            nextMarker = findNextMarker(marker, from, data, length);
-            if (nextMarker == -1) {
-                break;
-            }
-            final BytesReference slice = data.slice(from, nextMarker - from);
-            if (isTemplateRequest) {
-                try (XContentParser parser = XContentFactory.xContent(slice).createParser(slice)) {
-                    queryParseContext.reset(parser);
-                    Template template = TemplateQueryParser.parse(parser, queryParseContext.parseFieldMatcher(), "params", "template");
-                    searchRequest.template(template);
-                }
-            } else {
-                try (XContentParser requestParser = XContentFactory.xContent(slice).createParser(slice)) {
-                    queryParseContext.reset(requestParser);
-                    searchRequest.source(SearchSourceBuilder.PROTOTYPE.fromXContent(requestParser, queryParseContext));
-                }
-            }
-            // move pointers
-            from = nextMarker + 1;
-
-            msr.add(searchRequest);
-        }
-        return msr;
-    }
-
-    private static int findNextMarker(byte marker, int from, BytesReference data, int length) {
-        for (int i = from; i < length; i++) {
-            if (data.get(i) == marker) {
-                return i;
-            }
-        }
-        return -1;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java b/core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java
index 3d8d37e..03a33e0 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java
@@ -20,20 +20,15 @@
 package org.elasticsearch.rest.action.search;
 
 import org.elasticsearch.action.search.SearchRequest;
+import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.TemplateQueryParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.rest.BaseRestHandler;
 import org.elasticsearch.rest.RestChannel;
 import org.elasticsearch.rest.RestController;
@@ -41,15 +36,11 @@ import org.elasticsearch.rest.RestRequest;
 import org.elasticsearch.rest.action.exists.RestExistsAction;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
-import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.search.suggest.SuggestBuilder;
-
-import java.io.IOException;
 
 import static org.elasticsearch.common.unit.TimeValue.parseTimeValue;
 import static org.elasticsearch.rest.RestRequest.Method.GET;
@@ -61,12 +52,9 @@ import static org.elasticsearch.search.suggest.SuggestBuilders.termSuggestion;
  */
 public class RestSearchAction extends BaseRestHandler {
 
-    private final IndicesQueriesRegistry queryRegistry;
-
     @Inject
-    public RestSearchAction(Settings settings, RestController controller, Client client, IndicesQueriesRegistry queryRegistry) {
+    public RestSearchAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
-        this.queryRegistry = queryRegistry;
         controller.registerHandler(GET, "/_search", this);
         controller.registerHandler(POST, "/_search", this);
         controller.registerHandler(GET, "/{index}/_search", this);
@@ -90,37 +78,24 @@ public class RestSearchAction extends BaseRestHandler {
     }
 
     @Override
-    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) throws IOException {
+    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) {
         SearchRequest searchRequest;
-        searchRequest = RestSearchAction.parseSearchRequest(queryRegistry, request, parseFieldMatcher);
-        client.search(searchRequest, new RestStatusToXContentListener<>(channel));
+        searchRequest = RestSearchAction.parseSearchRequest(request, parseFieldMatcher);
+        client.search(searchRequest, new RestStatusToXContentListener<SearchResponse>(channel));
     }
 
-    public static SearchRequest parseSearchRequest(IndicesQueriesRegistry indicesQueriesRegistry,  RestRequest request, ParseFieldMatcher parseFieldMatcher) throws IOException {
+    public static SearchRequest parseSearchRequest(RestRequest request, ParseFieldMatcher parseFieldMatcher) {
         String[] indices = Strings.splitStringByCommaToArray(request.param("index"));
         SearchRequest searchRequest = new SearchRequest(indices);
         // get the content, and put it in the body
         // add content/source as template if template flag is set
         boolean isTemplateRequest = request.path().endsWith("/template");
-        final SearchSourceBuilder builder;
         if (RestActions.hasBodyContent(request)) {
-            BytesReference restContent = RestActions.getRestContent(request);
-            QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
             if (isTemplateRequest) {
-                try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                    context.reset(parser);
-                    Template template = TemplateQueryParser.parse(parser, context.parseFieldMatcher(), "params", "template");
-                    searchRequest.template(template);
-                }
-                builder = null;
+                searchRequest.templateSource(RestActions.getRestContent(request));
             } else {
-                try (XContentParser requestParser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                    context.reset(requestParser);
-                    builder = SearchSourceBuilder.PROTOTYPE.fromXContent(requestParser, context);
-                }
+                searchRequest.source(RestActions.getRestContent(request));
             }
-        } else {
-            builder = null;
         }
 
         // do not allow 'query_and_fetch' or 'dfs_query_and_fetch' search types
@@ -133,15 +108,8 @@ public class RestSearchAction extends BaseRestHandler {
         } else {
             searchRequest.searchType(searchType);
         }
-        if (builder == null) {
-            SearchSourceBuilder extraBuilder = new SearchSourceBuilder();
-            if (parseSearchSource(extraBuilder, request)) {
-                searchRequest.source(extraBuilder);
-            }
-        } else {
-            parseSearchSource(builder, request);
-            searchRequest.source(builder);
-        }
+
+        searchRequest.extraSource(parseSearchSource(request));
         searchRequest.requestCache(request.paramAsBoolean("request_cache", null));
 
         String scroll = request.param("scroll");
@@ -157,89 +125,111 @@ public class RestSearchAction extends BaseRestHandler {
         return searchRequest;
     }
 
-    public static boolean parseSearchSource(final SearchSourceBuilder searchSourceBuilder, RestRequest request) {
+    public static SearchSourceBuilder parseSearchSource(RestRequest request) {
+        SearchSourceBuilder searchSourceBuilder = null;
 
-        boolean modified = false;
-        QueryBuilder<?> queryBuilder = RestActions.parseQuerySource(request);
-        if (queryBuilder != null) {
-            searchSourceBuilder.query(queryBuilder);
-            modified = true;
+        QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+        if (querySourceBuilder != null) {
+            searchSourceBuilder = new SearchSourceBuilder();
+            searchSourceBuilder.query(querySourceBuilder);
         }
 
         int from = request.paramAsInt("from", -1);
         if (from != -1) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.from(from);
-            modified = true;
         }
         int size = request.paramAsInt("size", -1);
         if (size != -1) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.size(size);
-            modified = true;
         }
 
         if (request.hasParam("explain")) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.explain(request.paramAsBoolean("explain", null));
-            modified = true;
         }
         if (request.hasParam("version")) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.version(request.paramAsBoolean("version", null));
-            modified = true;
         }
         if (request.hasParam("timeout")) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.timeout(request.paramAsTime("timeout", null));
-            modified = true;
         }
         if (request.hasParam("terminate_after")) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             int terminateAfter = request.paramAsInt("terminate_after",
                     SearchContext.DEFAULT_TERMINATE_AFTER);
             if (terminateAfter < 0) {
                 throw new IllegalArgumentException("terminateAfter must be > 0");
             } else if (terminateAfter > 0) {
                 searchSourceBuilder.terminateAfter(terminateAfter);
-                modified = true;
             }
         }
 
         String sField = request.param("fields");
         if (sField != null) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             if (!Strings.hasText(sField)) {
                 searchSourceBuilder.noFields();
-                modified = true;
             } else {
                 String[] sFields = Strings.splitStringByCommaToArray(sField);
                 if (sFields != null) {
                     for (String field : sFields) {
                         searchSourceBuilder.field(field);
-                        modified = true;
                     }
                 }
             }
         }
         String sFieldDataFields = request.param("fielddata_fields");
         if (sFieldDataFields != null) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             if (Strings.hasText(sFieldDataFields)) {
                 String[] sFields = Strings.splitStringByCommaToArray(sFieldDataFields);
                 if (sFields != null) {
                     for (String field : sFields) {
                         searchSourceBuilder.fieldDataField(field);
-                        modified = true;
                     }
                 }
             }
         }
         FetchSourceContext fetchSourceContext = FetchSourceContext.parseFromRestRequest(request);
         if (fetchSourceContext != null) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.fetchSource(fetchSourceContext);
-            modified = true;
         }
 
         if (request.hasParam("track_scores")) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.trackScores(request.paramAsBoolean("track_scores", false));
-            modified = true;
         }
 
         String sSorts = request.param("sort");
         if (sSorts != null) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             String[] sorts = Strings.splitStringByCommaToArray(sSorts);
             for (String sort : sorts) {
                 int delimiter = sort.lastIndexOf(":");
@@ -248,33 +238,37 @@ public class RestSearchAction extends BaseRestHandler {
                     String reverse = sort.substring(delimiter + 1);
                     if ("asc".equals(reverse)) {
                         searchSourceBuilder.sort(sortField, SortOrder.ASC);
-                        modified = true;
                     } else if ("desc".equals(reverse)) {
                         searchSourceBuilder.sort(sortField, SortOrder.DESC);
-                        modified = true;
                     }
                 } else {
                     searchSourceBuilder.sort(sort);
-                    modified = true;
                 }
             }
         }
 
         String sStats = request.param("stats");
         if (sStats != null) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.stats(Strings.splitStringByCommaToArray(sStats));
-            modified = true;
         }
 
         String suggestField = request.param("suggest_field");
         if (suggestField != null) {
             String suggestText = request.param("suggest_text", request.param("q"));
             int suggestSize = request.paramAsInt("suggest_size", 5);
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             String suggestMode = request.param("suggest_mode");
-            searchSourceBuilder.suggest(new SuggestBuilder().addSuggestion(
-                    termSuggestion(suggestField).field(suggestField).text(suggestText).size(suggestSize).suggestMode(suggestMode)));
-            modified = true;
+            searchSourceBuilder.suggest().addSuggestion(
+                    termSuggestion(suggestField).field(suggestField).text(suggestText).size(suggestSize)
+                            .suggestMode(suggestMode)
+            );
         }
-        return modified;
+
+        return searchSourceBuilder;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java b/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
index 464870a..674aa69 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
@@ -21,17 +21,13 @@ package org.elasticsearch.rest.action.support;
 
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ShardOperationFailedException;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.action.support.broadcast.BroadcastResponse;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.lucene.uid.Versions;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.index.query.Operator;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.QueryStringQueryBuilder;
 import org.elasticsearch.rest.RestRequest;
@@ -89,7 +85,7 @@ public class RestActions {
         builder.endObject();
     }
 
-    public static QueryBuilder<?> parseQuerySource(RestRequest request) {
+    public static QuerySourceBuilder parseQuerySource(RestRequest request) {
         String queryString = request.param("q");
         if (queryString == null) {
             return null;
@@ -104,7 +100,7 @@ public class RestActions {
         if (defaultOperator != null) {
             queryBuilder.defaultOperator(Operator.fromString(defaultOperator));
         }
-        return queryBuilder;
+        return new QuerySourceBuilder().setQuery(queryBuilder);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java b/core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java
index e9681c7..acd050d 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java
@@ -23,6 +23,7 @@ import com.google.common.collect.ImmutableCollection;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 
+import java.util.Collection;
 import java.util.HashMap;
 import java.util.Map;
 
@@ -36,7 +37,7 @@ public final class ScriptContextRegistry {
 
     private final ImmutableMap<String, ScriptContext> scriptContexts;
 
-    public ScriptContextRegistry(Iterable<ScriptContext.Plugin> customScriptContexts) {
+    public ScriptContextRegistry(Collection<ScriptContext.Plugin> customScriptContexts) {
         Map<String, ScriptContext> scriptContexts = new HashMap<>();
         for (ScriptContext.Standard scriptContext : ScriptContext.Standard.values()) {
             scriptContexts.put(scriptContext.getKey(), scriptContext);
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptModule.java b/core/src/main/java/org/elasticsearch/script/ScriptModule.java
index 7d91f36..77ed2b8 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptModule.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptModule.java
@@ -24,7 +24,6 @@ import org.elasticsearch.common.inject.multibindings.MapBinder;
 import org.elasticsearch.common.inject.multibindings.Multibinder;
 import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.script.expression.ExpressionScriptEngineService;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
 import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 
@@ -92,13 +91,6 @@ public class ScriptModule extends AbstractModule {
             Loggers.getLogger(ScriptService.class, settings).debug("failed to load mustache", t);
         }
 
-        try {
-            Class.forName("org.apache.lucene.expressions.Expression");
-            multibinder.addBinding().to(ExpressionScriptEngineService.class).asEagerSingleton();
-        } catch (Throwable t) {
-            Loggers.getLogger(ScriptService.class, settings).debug("failed to load lucene expressions", t);
-        }
-
         for (Class<? extends ScriptEngineService> scriptEngine : scriptEngines) {
             multibinder.addBinding().to(scriptEngine).asEagerSingleton();
         }
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptService.java b/core/src/main/java/org/elasticsearch/script/ScriptService.java
index 6081041..4ffd4c6 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptService.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptService.java
@@ -58,7 +58,6 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.query.TemplateQueryParser;
-import org.elasticsearch.script.expression.ExpressionScriptEngineService;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.lookup.SearchLookup;
@@ -245,8 +244,9 @@ public class ScriptService extends AbstractComponent implements Closeable {
             throw new ScriptException("scripts of type [" + script.getType() + "], operation [" + scriptContext.getKey() + "] and lang [" + lang + "] are disabled");
         }
 
+        // TODO: fix this through some API or something, thats wrong
         // special exception to prevent expressions from compiling as update or mapping scripts
-        boolean expression = scriptEngineService instanceof ExpressionScriptEngineService;
+        boolean expression = "expression".equals(script.getLang());
         boolean notSupported = scriptContext.getKey().equals(ScriptContext.Standard.UPDATE.getKey()) ||
                                scriptContext.getKey().equals(ScriptContext.Standard.MAPPING.getKey());
         if (expression && notSupported) {
diff --git a/core/src/main/java/org/elasticsearch/script/Template.java b/core/src/main/java/org/elasticsearch/script/Template.java
index babe488..293a8b3 100644
--- a/core/src/main/java/org/elasticsearch/script/Template.java
+++ b/core/src/main/java/org/elasticsearch/script/Template.java
@@ -46,7 +46,7 @@ public class Template extends Script {
     /**
      * Constructor for simple inline template. The template will have no lang,
      * content type or params set.
-     *
+     * 
      * @param template
      *            The inline template.
      */
@@ -56,7 +56,7 @@ public class Template extends Script {
 
     /**
      * Constructor for Template.
-     *
+     * 
      * @param template
      *            The cache key of the template to be compiled/executed. For
      *            inline templates this is the actual templates source code. For
@@ -73,13 +73,13 @@ public class Template extends Script {
      */
     public Template(String template, ScriptType type, @Nullable String lang, @Nullable XContentType xContentType,
             @Nullable Map<String, Object> params) {
-        super(template, type, lang == null ? MustacheScriptEngineService.NAME : lang, params);
+        super(template, type, lang, params);
         this.contentType = xContentType;
     }
 
     /**
      * Method for getting the {@link XContentType} of the template.
-     *
+     * 
      * @return The {@link XContentType} of the template.
      */
     public XContentType getContentType() {
diff --git a/core/src/main/java/org/elasticsearch/script/expression/CountMethodFunctionValues.java b/core/src/main/java/org/elasticsearch/script/expression/CountMethodFunctionValues.java
deleted file mode 100644
index 818404e..0000000
--- a/core/src/main/java/org/elasticsearch/script/expression/CountMethodFunctionValues.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.expression;
-
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
-import org.elasticsearch.index.fielddata.AtomicNumericFieldData;
-import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
-
-/**
- * FunctionValues to get the count of the number of values in a field for a document.
- */
-public class CountMethodFunctionValues extends DoubleDocValues {
-    SortedNumericDoubleValues values;
-
-    CountMethodFunctionValues(ValueSource parent, AtomicNumericFieldData fieldData) {
-        super(parent);
-
-        values = fieldData.getDoubleValues();
-    }
-
-    @Override
-    public double doubleVal(int doc) {
-        values.setDocument(doc);
-        return values.count();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/script/expression/CountMethodValueSource.java b/core/src/main/java/org/elasticsearch/script/expression/CountMethodValueSource.java
deleted file mode 100644
index 0fa83d9..0000000
--- a/core/src/main/java/org/elasticsearch/script/expression/CountMethodValueSource.java
+++ /dev/null
@@ -1,73 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.expression;
-
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.elasticsearch.index.fielddata.AtomicFieldData;
-import org.elasticsearch.index.fielddata.AtomicNumericFieldData;
-import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.search.MultiValueMode;
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.Objects;
-
-/**
- * A ValueSource to create FunctionValues to get the count of the number of values in a field for a document.
- */
-public class CountMethodValueSource extends ValueSource {
-    protected IndexFieldData<?> fieldData;
-
-    protected CountMethodValueSource(IndexFieldData<?> fieldData) {
-        Objects.requireNonNull(fieldData);
-
-        this.fieldData = fieldData;
-    }
-
-    @Override
-    public FunctionValues getValues(Map context, LeafReaderContext leaf) throws IOException {
-        AtomicFieldData leafData = fieldData.load(leaf);
-        assert(leafData instanceof AtomicNumericFieldData);
-
-        return new CountMethodFunctionValues(this, (AtomicNumericFieldData)leafData);
-    }
-
-    @Override
-    public boolean equals(Object o) {
-        if (this == o) return true;
-        if (o == null || getClass() != o.getClass()) return false;
-
-        FieldDataValueSource that = (FieldDataValueSource) o;
-
-        return fieldData.equals(that.fieldData);
-    }
-
-    @Override
-    public int hashCode() {
-        return fieldData.hashCode();
-    }
-
-    @Override
-    public String description() {
-        return "count: field(" + fieldData.getFieldNames().toString() + ")";
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/script/expression/DateMethodFunctionValues.java b/core/src/main/java/org/elasticsearch/script/expression/DateMethodFunctionValues.java
deleted file mode 100644
index f7198fc..0000000
--- a/core/src/main/java/org/elasticsearch/script/expression/DateMethodFunctionValues.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.expression;
-
-import java.util.Calendar;
-import java.util.Locale;
-import java.util.TimeZone;
-
-import org.apache.lucene.queries.function.ValueSource;
-import org.elasticsearch.index.fielddata.AtomicNumericFieldData;
-import org.elasticsearch.search.MultiValueMode;
-
-class DateMethodFunctionValues extends FieldDataFunctionValues {
-    private final int calendarType;
-    private final Calendar calendar;
-
-    DateMethodFunctionValues(ValueSource parent, MultiValueMode multiValueMode,  AtomicNumericFieldData data, int calendarType) {
-        super(parent, multiValueMode, data);
-
-        this.calendarType = calendarType;
-        calendar = Calendar.getInstance(TimeZone.getTimeZone("UTC"), Locale.ROOT);
-    }
-
-    @Override
-    public double doubleVal(int docId) {
-        long millis = (long)dataAccessor.get(docId);
-        calendar.setTimeInMillis(millis);
-        return calendar.get(calendarType);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/script/expression/DateMethodValueSource.java b/core/src/main/java/org/elasticsearch/script/expression/DateMethodValueSource.java
deleted file mode 100644
index 522b546..0000000
--- a/core/src/main/java/org/elasticsearch/script/expression/DateMethodValueSource.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.expression;
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.Objects;
-
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.queries.function.FunctionValues;
-
-import org.elasticsearch.index.fielddata.AtomicFieldData;
-import org.elasticsearch.index.fielddata.AtomicNumericFieldData;
-import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.search.MultiValueMode;
-
-class DateMethodValueSource extends FieldDataValueSource {
-
-    protected final String methodName;
-    protected final int calendarType;
-
-    DateMethodValueSource(IndexFieldData<?> indexFieldData, MultiValueMode multiValueMode, String methodName, int calendarType) {
-        super(indexFieldData, multiValueMode);
-
-        Objects.requireNonNull(methodName);
-
-        this.methodName = methodName;
-        this.calendarType = calendarType;
-    }
-
-    @Override
-    public FunctionValues getValues(Map context, LeafReaderContext leaf) throws IOException {
-        AtomicFieldData leafData = fieldData.load(leaf);
-        assert(leafData instanceof AtomicNumericFieldData);
-
-        return new DateMethodFunctionValues(this, multiValueMode, (AtomicNumericFieldData)leafData, calendarType);
-    }
-
-    @Override
-    public String description() {
-        return methodName + ": field(" + fieldData.getFieldNames().toString() + ")";
-    }
-
-    @Override
-    public boolean equals(Object o) {
-        if (this == o) return true;
-        if (o == null || getClass() != o.getClass()) return false;
-        if (!super.equals(o)) return false;
-
-        DateMethodValueSource that = (DateMethodValueSource) o;
-
-        if (calendarType != that.calendarType) return false;
-        return methodName.equals(that.methodName);
-
-    }
-
-    @Override
-    public int hashCode() {
-        int result = super.hashCode();
-        result = 31 * result + methodName.hashCode();
-        result = 31 * result + calendarType;
-        return result;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/script/expression/ExpressionExecutableScript.java b/core/src/main/java/org/elasticsearch/script/expression/ExpressionExecutableScript.java
deleted file mode 100644
index d4ae9e1..0000000
--- a/core/src/main/java/org/elasticsearch/script/expression/ExpressionExecutableScript.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.expression;
-
-import org.apache.lucene.expressions.Expression;
-import org.elasticsearch.script.CompiledScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.ScriptException;
-
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * A bridge to evaluate an {@link Expression} against a map of variables in the context
- * of an {@link ExecutableScript}.
- */
-public class ExpressionExecutableScript implements ExecutableScript {
-
-    private final int NO_DOCUMENT = -1;
-
-    public final CompiledScript compiledScript;
-    public final Map<String, ReplaceableConstFunctionValues> functionValuesMap;
-    public final ReplaceableConstFunctionValues[] functionValuesArray;
-
-    public ExpressionExecutableScript(CompiledScript compiledScript, Map<String, Object> vars) {
-        this.compiledScript = compiledScript;
-        Expression expression = (Expression)this.compiledScript.compiled();
-        int functionValuesLength = expression.variables.length;
-
-        if (vars.size() != functionValuesLength) {
-            throw new ScriptException("Error using " + compiledScript + ". " +
-                    "The number of variables in an executable expression script [" +
-                    functionValuesLength + "] must match the number of variables in the variable map" +
-                    " [" + vars.size() + "].");
-        }
-
-        functionValuesArray = new ReplaceableConstFunctionValues[functionValuesLength];
-        functionValuesMap = new HashMap<>();
-
-        for (int functionValuesIndex = 0; functionValuesIndex < functionValuesLength; ++functionValuesIndex) {
-            String variableName = expression.variables[functionValuesIndex];
-            functionValuesArray[functionValuesIndex] = new ReplaceableConstFunctionValues();
-            functionValuesMap.put(variableName, functionValuesArray[functionValuesIndex]);
-        }
-
-        for (String varsName : vars.keySet()) {
-            setNextVar(varsName, vars.get(varsName));
-        }
-    }
-
-    @Override
-    public void setNextVar(String name, Object value) {
-        if (functionValuesMap.containsKey(name)) {
-            if (value instanceof Number) {
-                double doubleValue = ((Number)value).doubleValue();
-                functionValuesMap.get(name).setValue(doubleValue);
-            } else {
-                throw new ScriptException("Error using " + compiledScript + ". " +
-                        "Executable expressions scripts can only process numbers." +
-                        "  The variable [" + name + "] is not a number.");
-            }
-        } else {
-            throw new ScriptException("Error using " + compiledScript + ". " +
-                    "The variable [" + name + "] does not exist in the executable expressions script.");
-        }
-    }
-
-    @Override
-    public Object run() {
-        try {
-            return ((Expression) compiledScript.compiled()).evaluate(NO_DOCUMENT, functionValuesArray);
-        } catch (Exception exception) {
-            throw new ScriptException("Error evaluating " + compiledScript, exception);
-        }
-    }
-
-    @Override
-    public Object unwrap(Object value) {
-        return value;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptCompilationException.java b/core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptCompilationException.java
deleted file mode 100644
index f7e9010..0000000
--- a/core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptCompilationException.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.expression;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-
-import java.io.IOException;
-import java.text.ParseException;
-
-/**
- * Exception representing a compilation error in an expression.
- */
-public class ExpressionScriptCompilationException extends ElasticsearchException {
-    public ExpressionScriptCompilationException(String msg, ParseException e) {
-        super(msg, e);
-    }
-    public ExpressionScriptCompilationException(String msg) {
-        super(msg);
-    }
-
-    public ExpressionScriptCompilationException(StreamInput in) throws IOException {
-        super(in);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java b/core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java
deleted file mode 100644
index 9cd7da3..0000000
--- a/core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java
+++ /dev/null
@@ -1,244 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.expression;
-
-import org.apache.lucene.expressions.Expression;
-import org.apache.lucene.expressions.SimpleBindings;
-import org.apache.lucene.expressions.js.JavascriptCompiler;
-import org.apache.lucene.expressions.js.VariableContext;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.valuesource.DoubleConstValueSource;
-import org.apache.lucene.search.SortField;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.mapper.core.DateFieldMapper;
-import org.elasticsearch.index.mapper.core.NumberFieldMapper;
-import org.elasticsearch.script.CompiledScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.ScriptEngineService;
-import org.elasticsearch.script.ScriptException;
-import org.elasticsearch.script.SearchScript;
-import org.elasticsearch.search.MultiValueMode;
-import org.elasticsearch.search.lookup.SearchLookup;
-
-import java.text.ParseException;
-import java.util.Calendar;
-import java.util.Map;
-
-/**
- * Provides the infrastructure for Lucene expressions as a scripting language for Elasticsearch.  Only
- * {@link SearchScript}s are supported.
- */
-public class ExpressionScriptEngineService extends AbstractComponent implements ScriptEngineService {
-
-    public static final String NAME = "expression";
-
-    protected static final String GET_YEAR_METHOD         = "getYear";
-    protected static final String GET_MONTH_METHOD        = "getMonth";
-    protected static final String GET_DAY_OF_MONTH_METHOD = "getDayOfMonth";
-    protected static final String GET_HOUR_OF_DAY_METHOD  = "getHourOfDay";
-    protected static final String GET_MINUTES_METHOD      = "getMinutes";
-    protected static final String GET_SECONDS_METHOD      = "getSeconds";
-
-    protected static final String MINIMUM_METHOD          = "min";
-    protected static final String MAXIMUM_METHOD          = "max";
-    protected static final String AVERAGE_METHOD          = "avg";
-    protected static final String MEDIAN_METHOD           = "median";
-    protected static final String SUM_METHOD              = "sum";
-    protected static final String COUNT_METHOD            = "count";
-
-    @Inject
-    public ExpressionScriptEngineService(Settings settings) {
-        super(settings);
-    }
-
-    @Override
-    public String[] types() {
-        return new String[]{NAME};
-    }
-
-    @Override
-    public String[] extensions() {
-        return new String[]{NAME};
-    }
-
-    @Override
-    public boolean sandboxed() {
-        return true;
-    }
-
-    @Override
-    public Object compile(String script) {
-        try {
-            // NOTE: validation is delayed to allow runtime vars, and we don't have access to per index stuff here
-            return JavascriptCompiler.compile(script);
-        } catch (ParseException e) {
-            throw new ExpressionScriptCompilationException("Failed to parse expression: " + script, e);
-        }
-    }
-
-    @Override
-    public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, @Nullable Map<String, Object> vars) {
-        try {
-            Expression expr = (Expression)compiledScript.compiled();
-            MapperService mapper = lookup.doc().mapperService();
-            // NOTE: if we need to do anything complicated with bindings in the future, we can just extend Bindings,
-            // instead of complicating SimpleBindings (which should stay simple)
-            SimpleBindings bindings = new SimpleBindings();
-            ReplaceableConstValueSource specialValue = null;
-
-            for (String variable : expr.variables) {
-                if (variable.equals("_score")) {
-                    bindings.add(new SortField("_score", SortField.Type.SCORE));
-                } else if (variable.equals("_value")) {
-                    specialValue = new ReplaceableConstValueSource();
-                    bindings.add("_value", specialValue);
-                    // noop: _value is special for aggregations, and is handled in ExpressionScriptBindings
-                    // TODO: if some uses it in a scoring expression, they will get a nasty failure when evaluating...need a
-                    // way to know this is for aggregations and so _value is ok to have...
-
-                } else if (vars != null && vars.containsKey(variable)) {
-                    // TODO: document and/or error if vars contains _score?
-                    // NOTE: by checking for the variable in vars first, it allows masking document fields with a global constant,
-                    // but if we were to reverse it, we could provide a way to supply dynamic defaults for documents missing the field?
-                    Object value = vars.get(variable);
-                    if (value instanceof Number) {
-                        bindings.add(variable, new DoubleConstValueSource(((Number) value).doubleValue()));
-                    } else {
-                        throw new ExpressionScriptCompilationException("Parameter [" + variable + "] must be a numeric type");
-                    }
-
-                } else {
-                    String fieldname = null;
-                    String methodname = null;
-                    VariableContext[] parts = VariableContext.parse(variable);
-                    if (parts[0].text.equals("doc") == false) {
-                        throw new ExpressionScriptCompilationException("Unknown variable [" + parts[0].text + "] in expression");
-                    }
-                    if (parts.length < 2 || parts[1].type != VariableContext.Type.STR_INDEX) {
-                        throw new ExpressionScriptCompilationException("Variable 'doc' in expression must be used with a specific field like: doc['myfield']");
-                    } else {
-                        fieldname = parts[1].text;
-                    }
-                    if (parts.length == 3) {
-                        if (parts[2].type == VariableContext.Type.METHOD) {
-                            methodname = parts[2].text;
-                        } else if (parts[2].type != VariableContext.Type.MEMBER || !"value".equals(parts[2].text)) {
-                            throw new ExpressionScriptCompilationException("Only the member variable [value] or member methods may be accessed on a field when not accessing the field directly");
-                        }
-                    }
-                    if (parts.length > 3) {
-                        throw new ExpressionScriptCompilationException("Variable [" + variable + "] does not follow an allowed format of either doc['field'] or doc['field'].method()");
-                    }
-
-                    MappedFieldType fieldType = mapper.smartNameFieldType(fieldname);
-
-                    if (fieldType == null) {
-                        throw new ExpressionScriptCompilationException("Field [" + fieldname + "] used in expression does not exist in mappings");
-                    }
-                    if (fieldType.isNumeric() == false) {
-                        // TODO: more context (which expression?)
-                        throw new ExpressionScriptCompilationException("Field [" + fieldname + "] used in expression must be numeric");
-                    }
-
-                    IndexFieldData<?> fieldData = lookup.doc().fieldDataService().getForField((NumberFieldMapper.NumberFieldType) fieldType);
-                    if (methodname == null) {
-                        bindings.add(variable, new FieldDataValueSource(fieldData, MultiValueMode.MIN));
-                    } else {
-                        bindings.add(variable, getMethodValueSource(fieldType, fieldData, fieldname, methodname));
-                    }
-                }
-            }
-
-            final boolean needsScores = expr.getSortField(bindings, false).needsScores();
-            return new ExpressionSearchScript(compiledScript, bindings, specialValue, needsScores);
-        } catch (Exception exception) {
-            throw new ScriptException("Error during search with " + compiledScript, exception);
-        }
-    }
-
-    protected ValueSource getMethodValueSource(MappedFieldType fieldType, IndexFieldData<?> fieldData, String fieldName, String methodName) {
-        switch (methodName) {
-            case GET_YEAR_METHOD:
-                return getDateMethodValueSource(fieldType, fieldData, fieldName, methodName, Calendar.YEAR);
-            case GET_MONTH_METHOD:
-                return getDateMethodValueSource(fieldType, fieldData, fieldName, methodName, Calendar.MONTH);
-            case GET_DAY_OF_MONTH_METHOD:
-                return getDateMethodValueSource(fieldType, fieldData, fieldName, methodName, Calendar.DAY_OF_MONTH);
-            case GET_HOUR_OF_DAY_METHOD:
-                return getDateMethodValueSource(fieldType, fieldData, fieldName, methodName, Calendar.HOUR_OF_DAY);
-            case GET_MINUTES_METHOD:
-                return getDateMethodValueSource(fieldType, fieldData, fieldName, methodName, Calendar.MINUTE);
-            case GET_SECONDS_METHOD:
-                return getDateMethodValueSource(fieldType, fieldData, fieldName, methodName, Calendar.SECOND);
-            case MINIMUM_METHOD:
-                return new FieldDataValueSource(fieldData, MultiValueMode.MIN);
-            case MAXIMUM_METHOD:
-                return new FieldDataValueSource(fieldData, MultiValueMode.MAX);
-            case AVERAGE_METHOD:
-                return new FieldDataValueSource(fieldData, MultiValueMode.AVG);
-            case MEDIAN_METHOD:
-                return new FieldDataValueSource(fieldData, MultiValueMode.MEDIAN);
-            case SUM_METHOD:
-                return new FieldDataValueSource(fieldData, MultiValueMode.SUM);
-            case COUNT_METHOD:
-                return new CountMethodValueSource(fieldData);
-            default:
-                throw new IllegalArgumentException("Member method [" + methodName + "] does not exist.");
-        }
-    }
-
-    protected ValueSource getDateMethodValueSource(MappedFieldType fieldType, IndexFieldData<?> fieldData, String fieldName, String methodName, int calendarType) {
-        if (!(fieldType instanceof DateFieldMapper.DateFieldType)) {
-            throw new IllegalArgumentException("Member method [" + methodName + "] can only be used with a date field type, not the field [" + fieldName + "].");
-        }
-
-        return new DateMethodValueSource(fieldData, MultiValueMode.MIN, methodName, calendarType);
-    }
-
-    @Override
-    public ExecutableScript executable(CompiledScript compiledScript, Map<String, Object> vars) {
-        return new ExpressionExecutableScript(compiledScript, vars);
-    }
-
-    @Override
-    public Object execute(CompiledScript compiledScript, Map<String, Object> vars) {
-        ExpressionExecutableScript expressionExecutableScript = new ExpressionExecutableScript(compiledScript, vars);
-        return expressionExecutableScript.run();
-    }
-
-    @Override
-    public Object unwrap(Object value) {
-        return value;
-    }
-
-    @Override
-    public void close() {}
-
-    @Override
-    public void scriptRemoved(CompiledScript script) {
-        // Nothing to do
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptExecutionException.java b/core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptExecutionException.java
deleted file mode 100644
index 435b122..0000000
--- a/core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptExecutionException.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.expression;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-
-import java.io.IOException;
-
-/**
- * Exception used to wrap exceptions occuring while running expressions.
- */
-public class ExpressionScriptExecutionException extends ElasticsearchException {
-    public ExpressionScriptExecutionException(String msg, Throwable cause) {
-        super(msg, cause);
-    }
-
-    public ExpressionScriptExecutionException(StreamInput in) throws IOException {
-        super(in);
-    }
-
-    public ExpressionScriptExecutionException(String msg) {
-        super(msg);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/script/expression/ExpressionSearchScript.java b/core/src/main/java/org/elasticsearch/script/expression/ExpressionSearchScript.java
deleted file mode 100644
index 2825e55..0000000
--- a/core/src/main/java/org/elasticsearch/script/expression/ExpressionSearchScript.java
+++ /dev/null
@@ -1,131 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.expression;
-
-import org.apache.lucene.expressions.Bindings;
-import org.apache.lucene.expressions.Expression;
-import org.apache.lucene.expressions.SimpleBindings;
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.search.Scorer;
-import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.script.CompiledScript;
-import org.elasticsearch.script.LeafSearchScript;
-import org.elasticsearch.script.ScriptException;
-import org.elasticsearch.script.SearchScript;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Map;
-
-/**
- * A bridge to evaluate an {@link Expression} against {@link Bindings} in the context
- * of a {@link SearchScript}.
- */
-class ExpressionSearchScript implements SearchScript {
-
-    final CompiledScript compiledScript;
-    final SimpleBindings bindings;
-    final ValueSource source;
-    final ReplaceableConstValueSource specialValue; // _value
-    final boolean needsScores;
-    Scorer scorer;
-    int docid;
-
-    ExpressionSearchScript(CompiledScript c, SimpleBindings b, ReplaceableConstValueSource v, boolean needsScores) {
-        compiledScript = c;
-        bindings = b;
-        source = ((Expression)compiledScript.compiled()).getValueSource(bindings);
-        specialValue = v;
-        this.needsScores = needsScores;
-    }
-
-    @Override
-    public boolean needsScores() {
-        return needsScores;
-    }
-
-    @Override
-    public LeafSearchScript getLeafSearchScript(final LeafReaderContext leaf) throws IOException {
-        return new LeafSearchScript() {
-
-            FunctionValues values = source.getValues(Collections.singletonMap("scorer", Lucene.illegalScorer("Scores are not available in the current context")), leaf);
-
-            double evaluate() {
-                try {
-                    return values.doubleVal(docid);
-                } catch (Exception exception) {
-                    throw new ScriptException("Error evaluating " + compiledScript, exception);
-                }
-            }
-
-            @Override
-            public Object run() { return new Double(evaluate()); }
-
-            @Override
-            public float runAsFloat() { return (float)evaluate();}
-
-            @Override
-            public long runAsLong() { return (long)evaluate(); }
-
-            @Override
-            public double runAsDouble() { return evaluate(); }
-
-            @Override
-            public Object unwrap(Object value) { return value; }
-
-            @Override
-            public void setDocument(int d) {
-                docid = d;
-            }
-
-            @Override
-            public void setScorer(Scorer s) {
-                scorer = s;
-                try {
-                    // We have a new binding for the scorer so we need to reset the values
-                    values = source.getValues(Collections.singletonMap("scorer", scorer), leaf);
-                } catch (IOException e) {
-                    throw new IllegalStateException("Can't get values using " + compiledScript, e);
-                }
-            }
-
-            @Override
-            public void setSource(Map<String, Object> source) {
-                // noop: expressions don't use source data
-            }
-
-            @Override
-            public void setNextVar(String name, Object value) {
-                assert(specialValue != null);
-                // this should only be used for the special "_value" variable used in aggregations
-                assert(name.equals("_value"));
-
-                if (value instanceof Number) {
-                    specialValue.setValue(((Number)value).doubleValue());
-                } else {
-                    throw new ExpressionScriptExecutionException("Cannot use expression with text variable using " + compiledScript);
-                }
-            }
-        };
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/script/expression/FieldDataFunctionValues.java b/core/src/main/java/org/elasticsearch/script/expression/FieldDataFunctionValues.java
deleted file mode 100644
index b3e06d6..0000000
--- a/core/src/main/java/org/elasticsearch/script/expression/FieldDataFunctionValues.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.expression;
-
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
-import org.elasticsearch.index.fielddata.AtomicNumericFieldData;
-import org.elasticsearch.index.fielddata.NumericDoubleValues;
-import org.elasticsearch.search.MultiValueMode;
-
-/**
- * A {@link org.apache.lucene.queries.function.FunctionValues} which wrap field data.
- */
-class FieldDataFunctionValues extends DoubleDocValues {
-    NumericDoubleValues dataAccessor;
-
-    FieldDataFunctionValues(ValueSource parent, MultiValueMode m, AtomicNumericFieldData d) {
-        super(parent);
-        dataAccessor = m.select(d.getDoubleValues(), 0d);
-    }
-
-    @Override
-    public double doubleVal(int i) {
-        return dataAccessor.get(i);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/script/expression/FieldDataValueSource.java b/core/src/main/java/org/elasticsearch/script/expression/FieldDataValueSource.java
deleted file mode 100644
index 39386ee..0000000
--- a/core/src/main/java/org/elasticsearch/script/expression/FieldDataValueSource.java
+++ /dev/null
@@ -1,80 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.expression;
-
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.elasticsearch.index.fielddata.AtomicFieldData;
-import org.elasticsearch.index.fielddata.AtomicNumericFieldData;
-import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.search.MultiValueMode;
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.Objects;
-
-/**
- * A {@link ValueSource} wrapper for field data.
- */
-class FieldDataValueSource extends ValueSource {
-
-    protected IndexFieldData<?> fieldData;
-    protected MultiValueMode multiValueMode;
-
-    protected FieldDataValueSource(IndexFieldData<?> d, MultiValueMode m) {
-        Objects.requireNonNull(d);
-        Objects.requireNonNull(m);
-
-        fieldData = d;
-        multiValueMode = m;
-    }
-
-    @Override
-    public boolean equals(Object o) {
-        if (this == o) return true;
-        if (o == null || getClass() != o.getClass()) return false;
-
-        FieldDataValueSource that = (FieldDataValueSource) o;
-
-        if (!fieldData.equals(that.fieldData)) return false;
-        return multiValueMode == that.multiValueMode;
-
-    }
-
-    @Override
-    public int hashCode() {
-        int result = fieldData.hashCode();
-        result = 31 * result + multiValueMode.hashCode();
-        return result;
-    }
-
-    @Override
-    public FunctionValues getValues(Map context, LeafReaderContext leaf) throws IOException {
-        AtomicFieldData leafData = fieldData.load(leaf);
-        assert(leafData instanceof AtomicNumericFieldData);
-        return new FieldDataFunctionValues(this, multiValueMode, (AtomicNumericFieldData)leafData);
-    }
-
-    @Override
-    public String description() {
-        return "field(" + fieldData.getFieldNames().toString() + ")";
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/script/expression/ReplaceableConstFunctionValues.java b/core/src/main/java/org/elasticsearch/script/expression/ReplaceableConstFunctionValues.java
deleted file mode 100644
index b3ebfab..0000000
--- a/core/src/main/java/org/elasticsearch/script/expression/ReplaceableConstFunctionValues.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.expression;
-
-import org.apache.lucene.queries.function.FunctionValues;
-
-/**
- * A support class for an executable expression script that allows the double returned
- * by a {@link FunctionValues} to be modified.
- */
-public class ReplaceableConstFunctionValues extends FunctionValues {
-    private double value = 0;
-
-    public void setValue(double value) {
-        this.value = value;
-    }
-
-    @Override
-    public double doubleVal(int doc) {
-        return value;
-    }
-
-    @Override
-    public String toString(int i) {
-        return "ReplaceableConstFunctionValues: " + value;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/script/expression/ReplaceableConstValueSource.java b/core/src/main/java/org/elasticsearch/script/expression/ReplaceableConstValueSource.java
deleted file mode 100644
index c9bcc23..0000000
--- a/core/src/main/java/org/elasticsearch/script/expression/ReplaceableConstValueSource.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.expression;
-
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-
-import java.io.IOException;
-import java.util.Map;
-
-/**
- * A {@link ValueSource} which has a stub {@link FunctionValues} that holds a dynamically replaceable constant double.
- */
-class ReplaceableConstValueSource extends ValueSource {
-    final ReplaceableConstFunctionValues fv;
-
-    public ReplaceableConstValueSource() {
-        fv = new ReplaceableConstFunctionValues();
-    }
-
-    @Override
-    public FunctionValues getValues(Map map, LeafReaderContext atomicReaderContext) throws IOException {
-        return fv;
-    }
-
-    @Override
-    public boolean equals(Object o) {
-        return o == this;
-    }
-
-    @Override
-    public int hashCode() {
-        return System.identityHashCode(this);
-    }
-
-    @Override
-    public String description() {
-        return "replaceableConstDouble";
-    }
-
-    public void setValue(double v) {
-        fv.setValue(v);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/SearchService.java b/core/src/main/java/org/elasticsearch/search/SearchService.java
index a645621..59ec671 100644
--- a/core/src/main/java/org/elasticsearch/search/SearchService.java
+++ b/core/src/main/java/org/elasticsearch/search/SearchService.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.search;
 
-import com.carrotsearch.hppc.ObjectFloatHashMap;
 import com.carrotsearch.hppc.ObjectHashSet;
 import com.carrotsearch.hppc.ObjectSet;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
@@ -29,6 +28,7 @@ import org.apache.lucene.index.IndexOptions;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.search.TopDocs;
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
@@ -39,6 +39,7 @@ import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
@@ -46,8 +47,8 @@ import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.common.util.concurrent.ConcurrentMapLong;
 import org.elasticsearch.common.util.concurrent.FutureUtils;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentLocation;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.Index;
@@ -62,7 +63,7 @@ import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MappedFieldType.Loading;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.TemplateQueryParser;
 import org.elasticsearch.index.search.stats.ShardSearchStats;
 import org.elasticsearch.index.search.stats.StatsGroupsParseElement;
 import org.elasticsearch.index.settings.IndexSettings;
@@ -75,10 +76,11 @@ import org.elasticsearch.indices.IndicesWarmer.WarmerContext;
 import org.elasticsearch.indices.cache.request.IndicesRequestCache;
 import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.Script.ScriptParseException;
 import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.SearchScript;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.script.Template;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.dfs.DfsPhase;
 import org.elasticsearch.search.dfs.DfsSearchResult;
 import org.elasticsearch.search.fetch.FetchPhase;
@@ -86,10 +88,6 @@ import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.QueryFetchSearchResult;
 import org.elasticsearch.search.fetch.ScrollQueryFetchSearchResult;
 import org.elasticsearch.search.fetch.ShardFetchRequest;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsContext;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsContext.FieldDataField;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsFetchSubPhase;
-import org.elasticsearch.search.fetch.script.ScriptFieldsContext.ScriptField;
 import org.elasticsearch.search.internal.DefaultSearchContext;
 import org.elasticsearch.search.internal.InternalScrollSearchRequest;
 import org.elasticsearch.search.internal.ScrollContext;
@@ -105,7 +103,7 @@ import org.elasticsearch.search.query.ScrollQuerySearchResult;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 import org.elasticsearch.threadpool.ThreadPool;
 
-import java.util.Arrays;
+import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.CountDownLatch;
@@ -114,6 +112,7 @@ import java.util.concurrent.Executor;
 import java.util.concurrent.ScheduledFuture;
 import java.util.concurrent.atomic.AtomicLong;
 
+import static org.elasticsearch.common.Strings.hasLength;
 import static org.elasticsearch.common.unit.TimeValue.timeValueMillis;
 import static org.elasticsearch.common.unit.TimeValue.timeValueMinutes;
 
@@ -573,16 +572,10 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
                 context.scrollContext(new ScrollContext());
                 context.scrollContext().scroll = request.scroll();
             }
-            if (request.template() != null) {
-                ExecutableScript executable = this.scriptService.executable(request.template(), ScriptContext.Standard.SEARCH, context);
-                BytesReference run = (BytesReference) executable.run();
-                try (XContentParser parser = XContentFactory.xContent(run).createParser(run)) {
-                    QueryParseContext queryParseContext = new QueryParseContext(indexService.queryParserService().indicesQueriesRegistry());
-                    queryParseContext.reset(parser);
-                    parseSource(context, SearchSourceBuilder.PROTOTYPE.fromXContent(parser, queryParseContext));
-                }
-            }
+
+            parseTemplate(request, context);
             parseSource(context, request.source());
+            parseSource(context, request.extraSource());
 
             // if the from and size are still not set, default them
             if (context.from() == -1) {
@@ -671,229 +664,113 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
         }
     }
 
-    private void parseSource(SearchContext context, SearchSourceBuilder source) throws SearchParseException {
-        // nothing to parse...
-        if (source == null) {
-            return;
-        }
+    private void parseTemplate(ShardSearchRequest request, SearchContext searchContext) {
 
-        context.from(source.from());
-        context.size(source.size());
-        ObjectFloatHashMap<String> indexBoostMap = source.indexBoost();
-        if (indexBoostMap != null) {
-            Float indexBoost = indexBoostMap.get(context.shardTarget().index());
-            if (indexBoost != null) {
-                context.queryBoost(indexBoost);
-            }
-        }
-        if (source.query() != null) {
-            context.parsedQuery(context.queryParserService().parse(source.query()));
-        }
-        if (source.postFilter() != null) {
-            context.parsedPostFilter(context.queryParserService().parse(source.postFilter()));
-        }
-        if (source.sorts() != null) {
-            XContentParser completeSortParser = null;
-            try {
-                XContentBuilder completeSortBuilder = XContentFactory.jsonBuilder();
-                completeSortBuilder.startObject();
-                completeSortBuilder.startArray("sort");
-                for (BytesReference sort : source.sorts()) {
-                    XContentParser parser = XContentFactory.xContent(sort).createParser(sort);
-                    parser.nextToken();
-                    completeSortBuilder.copyCurrentStructure(parser);
-                }
-                completeSortBuilder.endArray();
-                completeSortBuilder.endObject();
-                BytesReference completeSortBytes = completeSortBuilder.bytes();
-                completeSortParser = XContentFactory.xContent(completeSortBytes).createParser(completeSortBytes);
-                completeSortParser.nextToken();
-                completeSortParser.nextToken();
-                completeSortParser.nextToken();
-                this.elementParsers.get("sort").parse(completeSortParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = completeSortParser != null ? completeSortParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse sort source [" + sSource + "]", location, e);
-            } // NORELEASE fix this to be more elegant
-        }
-        context.trackScores(source.trackScores());
-        if (source.minScore() != null) {
-            context.minimumScore(source.minScore());
-        }
-        context.timeoutInMillis(source.timeoutInMillis());
-        context.terminateAfter(source.terminateAfter());
-        if (source.aggregations() != null) {
-            XContentParser completeAggregationsParser = null;
-            try {
-                XContentBuilder completeAggregationsBuilder = XContentFactory.jsonBuilder();
-                completeAggregationsBuilder.startObject();
-                for (BytesReference agg : source.aggregations()) {
-                    XContentParser parser = XContentFactory.xContent(agg).createParser(agg);
-                    parser.nextToken();
-                    parser.nextToken();
-                    completeAggregationsBuilder.field(parser.currentName());
-                    parser.nextToken();
-                    completeAggregationsBuilder.copyCurrentStructure(parser);
-                }
-                completeAggregationsBuilder.endObject();
-                BytesReference completeAggregationsBytes = completeAggregationsBuilder.bytes();
-                completeAggregationsParser = XContentFactory.xContent(completeAggregationsBytes).createParser(completeAggregationsBytes);
-                completeAggregationsParser.nextToken();
-                this.elementParsers.get("aggregations").parse(completeAggregationsParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = completeAggregationsParser != null ? completeAggregationsParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse rescore source [" + sSource + "]", location, e);
-            } // NORELEASE fix this to be more elegant
-        }
-        if (source.suggest() != null) {
-            XContentParser suggestParser = null;
-            try {
-                suggestParser = XContentFactory.xContent(source.suggest()).createParser(source.suggest());
-                suggestParser.nextToken();
-                this.elementParsers.get("suggest").parse(suggestParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = suggestParser != null ? suggestParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse suggest source [" + sSource + "]", location, e);
+        BytesReference processedQuery;
+        if (request.template() != null) {
+            ExecutableScript executable = this.scriptService.executable(request.template(), ScriptContext.Standard.SEARCH, searchContext);
+            processedQuery = (BytesReference) executable.run();
+        } else {
+            if (!hasLength(request.templateSource())) {
+                return;
             }
-        }
-        if (source.rescores() != null) {
-            XContentParser completeRescoreParser = null;
+            XContentParser parser = null;
+            Template template = null;
+
             try {
-                XContentBuilder completeRescoreBuilder = XContentFactory.jsonBuilder();
-                completeRescoreBuilder.startObject();
-                completeRescoreBuilder.startArray("rescore");
-                for (BytesReference rescore : source.rescores()) {
-                    XContentParser parser = XContentFactory.xContent(rescore).createParser(rescore);
-                    parser.nextToken();
-                    completeRescoreBuilder.copyCurrentStructure(parser);
-                }
-                completeRescoreBuilder.endArray();
-                completeRescoreBuilder.endObject();
-                BytesReference completeRescoreBytes = completeRescoreBuilder.bytes();
-                completeRescoreParser = XContentFactory.xContent(completeRescoreBytes).createParser(completeRescoreBytes);
-                completeRescoreParser.nextToken();
-                completeRescoreParser.nextToken();
-                completeRescoreParser.nextToken();
-                this.elementParsers.get("rescore").parse(completeRescoreParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
+                parser = XContentFactory.xContent(request.templateSource()).createParser(request.templateSource());
+                template = TemplateQueryParser.parse(parser, searchContext.parseFieldMatcher(), "params", "template");
+
+                if (template.getType() == ScriptService.ScriptType.INLINE) {
+                    //Try to double parse for nested template id/file
+                    parser = null;
+                    try {
+                        ExecutableScript executable = this.scriptService.executable(template, ScriptContext.Standard.SEARCH, searchContext);
+                        processedQuery = (BytesReference) executable.run();
+                        parser = XContentFactory.xContent(processedQuery).createParser(processedQuery);
+                    } catch (ElasticsearchParseException epe) {
+                        //This was an non-nested template, the parse failure was due to this, it is safe to assume this refers to a file
+                        //for backwards compatibility and keep going
+                        template = new Template(template.getScript(), ScriptService.ScriptType.FILE, MustacheScriptEngineService.NAME,
+                                null, template.getParams());
+                        ExecutableScript executable = this.scriptService.executable(template, ScriptContext.Standard.SEARCH, searchContext);
+                        processedQuery = (BytesReference) executable.run();
+                    }
+                    if (parser != null) {
+                        try {
+                            Template innerTemplate = TemplateQueryParser.parse(parser, searchContext.parseFieldMatcher());
+                            if (hasLength(innerTemplate.getScript()) && !innerTemplate.getType().equals(ScriptService.ScriptType.INLINE)) {
+                                //An inner template referring to a filename or id
+                                template = new Template(innerTemplate.getScript(), innerTemplate.getType(),
+                                        MustacheScriptEngineService.NAME, null, template.getParams());
+                                ExecutableScript executable = this.scriptService.executable(template, ScriptContext.Standard.SEARCH,
+                                        searchContext);
+                                processedQuery = (BytesReference) executable.run();
+                            }
+                        } catch (ScriptParseException e) {
+                            // No inner template found, use original template from above
+                        }
+                    }
+                } else {
+                    ExecutableScript executable = this.scriptService.executable(template, ScriptContext.Standard.SEARCH, searchContext);
+                    processedQuery = (BytesReference) executable.run();
                 }
-                XContentLocation location = completeRescoreParser != null ? completeRescoreParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse rescore source [" + sSource + "]", location, e);
-            } // NORELEASE fix this to be more elegant
-        }
-        if (source.fields() != null) {
-            context.fieldNames().addAll(source.fields());
-        }
-        if (source.explain() != null) {
-            context.explain(source.explain());
-        }
-        if (source.fetchSource() != null) {
-            context.fetchSourceContext(source.fetchSource());
-        }
-        if (source.fieldDataFields() != null) {
-            FieldDataFieldsContext fieldDataFieldsContext = context.getFetchSubPhaseContext(FieldDataFieldsFetchSubPhase.CONTEXT_FACTORY);
-            for (String field : source.fieldDataFields()) {
-                fieldDataFieldsContext.add(new FieldDataField(field));
+            } catch (IOException e) {
+                throw new ElasticsearchParseException("Failed to parse template", e);
+            } finally {
+                Releasables.closeWhileHandlingException(parser);
             }
-            fieldDataFieldsContext.setHitExecutionNeeded(true);
-        }
-        if (source.highlighter() != null) {
-            XContentParser highlighterParser = null;
-            try {
-                highlighterParser = XContentFactory.xContent(source.highlighter()).createParser(source.highlighter());
-                this.elementParsers.get("highlight").parse(highlighterParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = highlighterParser != null ? highlighterParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse suggest source [" + sSource + "]", location, e);
+
+            if (!hasLength(template.getScript())) {
+                throw new ElasticsearchParseException("Template must have [template] field configured");
             }
         }
-        if (source.innerHits() != null) {
-            XContentParser innerHitsParser = null;
-            try {
-                innerHitsParser = XContentFactory.xContent(source.innerHits()).createParser(source.innerHits());
-                innerHitsParser.nextToken();
-                this.elementParsers.get("inner_hits").parse(innerHitsParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = innerHitsParser != null ? innerHitsParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse suggest source [" + sSource + "]", location, e);
-            }
+        request.source(processedQuery);
+    }
+
+    private void parseSource(SearchContext context, BytesReference source) throws SearchParseException {
+        // nothing to parse...
+        if (source == null || source.length() == 0) {
+            return;
         }
-        if (source.scriptFields() != null) {
-            for (org.elasticsearch.search.builder.SearchSourceBuilder.ScriptField field : source.scriptFields()) {
-                SearchScript searchScript = context.scriptService().search(context.lookup(), field.script(), ScriptContext.Standard.SEARCH);
-                context.scriptFields().add(new ScriptField(field.fieldName(), searchScript, false)); // NORELEASE need to have ignore_exception parsed somewhere
+        XContentParser parser = null;
+        try {
+            parser = XContentFactory.xContent(source).createParser(source);
+            XContentParser.Token token;
+            token = parser.nextToken();
+            if (token != XContentParser.Token.START_OBJECT) {
+                throw new ElasticsearchParseException("failed to parse search source. source must be an object, but found [{}] instead", token.name());
             }
-        }
-        if (source.ext() != null) {
-            XContentParser extParser = null;
-            try {
-                extParser = XContentFactory.xContent(source.ext()).createParser(source.ext());
-                XContentParser.Token token = extParser.nextToken();
-                String currentFieldName = null;
-                while ((token = extParser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                    if (token == XContentParser.Token.FIELD_NAME) {
-                        currentFieldName = extParser.currentName();
+            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                if (token == XContentParser.Token.FIELD_NAME) {
+                    String fieldName = parser.currentName();
+                    parser.nextToken();
+                    SearchParseElement element = elementParsers.get(fieldName);
+                    if (element == null) {
+                        throw new SearchParseException(context, "failed to parse search source. unknown search element [" + fieldName + "]", parser.getTokenLocation());
+                    }
+                    element.parse(parser, context);
+                } else {
+                    if (token == null) {
+                        throw new ElasticsearchParseException("failed to parse search source. end of query source reached but query is not complete.");
                     } else {
-                        SearchParseElement parseElement = this.elementParsers.get(currentFieldName);
-                        if (parseElement == null) {
-                            throw new SearchParseException(context, "Unknown element [" + currentFieldName + "] in [ext]",
-                                    extParser.getTokenLocation());
-                        } else {
-                            parseElement.parse(extParser, context);
-                        }
+                        throw new ElasticsearchParseException("failed to parse search source. expected field name but got [{}]", token);
                     }
                 }
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = extParser != null ? extParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse ext source [" + sSource + "]", location, e);
             }
-        }
-        if (source.version() != null) {
-            context.version(source.version());
-        }
-        if (source.stats() != null) {
-            context.groupStats(Arrays.asList(source.stats())); // NORELEASE stats should be a list in SearchSourceBuilder
+        } catch (Throwable e) {
+            String sSource = "_na_";
+            try {
+                sSource = XContentHelper.convertToJson(source, false);
+            } catch (Throwable e1) {
+                // ignore
+            }
+            XContentLocation location = parser != null ? parser.getTokenLocation() : null;
+            throw new SearchParseException(context, "failed to parse search source [" + sSource + "]", location, e);
+        } finally {
+            if (parser != null) {
+                parser.close();
+            }
         }
     }
 
@@ -1187,23 +1064,17 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
                         SearchContext context = null;
                         try {
                             long now = System.nanoTime();
-                            final IndexService indexService = indicesService.indexServiceSafe(indexShard.shardId().index().name());
                             ShardSearchRequest request = new ShardSearchLocalRequest(indexShard.shardId(), indexMetaData.numberOfShards(),
-                                    SearchType.QUERY_THEN_FETCH, entry.source().build(new QueryParseContext(indexService.queryParserService().indicesQueriesRegistry())), entry.types(), entry.requestCache());
+                                    SearchType.QUERY_THEN_FETCH, entry.source(), entry.types(), entry.requestCache());
                             context = createContext(request, warmerContext.searcher());
-                            // if we use sort, we need to do query to sort on
-                            // it and load relevant field data
-                            // if not, we might as well set size=0 (and cache
-                            // if needed)
+                            // if we use sort, we need to do query to sort on it and load relevant field data
+                            // if not, we might as well set size=0 (and cache if needed)
                             if (context.sort() == null) {
                                 context.size(0);
                             }
                             boolean canCache = indicesQueryCache.canCache(request, context);
-                            // early terminate when we can cache, since we
-                            // can only do proper caching on top level searcher
-                            // also, if we can't cache, and its top, we don't
-                            // need to execute it, since we already did when its
-                            // not top
+                            // early terminate when we can cache, since we can only do proper caching on top level searcher
+                            // also, if we can't cache, and its top, we don't need to execute it, since we already did when its not top
                             if (canCache != top) {
                                 return;
                             }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
index a5202c6..62bd22a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
@@ -19,8 +19,8 @@
 package org.elasticsearch.search.aggregations.metrics.tophits;
 
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
@@ -29,6 +29,7 @@ import org.elasticsearch.search.sort.SortBuilder;
 import org.elasticsearch.search.sort.SortOrder;
 
 import java.io.IOException;
+import java.util.Map;
 
 /**
  * Builder for the {@link TopHits} aggregation.
@@ -172,6 +173,185 @@ public class TopHitsBuilder extends AbstractAggregationBuilder {
         return this;
     }
 
+    /**
+     * Adds a field to be highlighted with default fragment size of 100 characters, and
+     * default number of fragments of 5.
+     *
+     * @param name The field to highlight
+     */
+    public TopHitsBuilder addHighlightedField(String name) {
+        highlightBuilder().field(name);
+        return this;
+    }
+
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters), and
+     * default number of fragments of 5.
+     *
+     * @param name         The field to highlight
+     * @param fragmentSize The size of a fragment in characters
+     */
+    public TopHitsBuilder addHighlightedField(String name, int fragmentSize) {
+        highlightBuilder().field(name, fragmentSize);
+        return this;
+    }
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters), and
+     * a provided (maximum) number of fragments.
+     *
+     * @param name              The field to highlight
+     * @param fragmentSize      The size of a fragment in characters
+     * @param numberOfFragments The (maximum) number of fragments
+     */
+    public TopHitsBuilder addHighlightedField(String name, int fragmentSize, int numberOfFragments) {
+        highlightBuilder().field(name, fragmentSize, numberOfFragments);
+        return this;
+    }
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters),
+     * a provided (maximum) number of fragments and an offset for the highlight.
+     *
+     * @param name              The field to highlight
+     * @param fragmentSize      The size of a fragment in characters
+     * @param numberOfFragments The (maximum) number of fragments
+     */
+    public TopHitsBuilder addHighlightedField(String name, int fragmentSize, int numberOfFragments,
+                                                    int fragmentOffset) {
+        highlightBuilder().field(name, fragmentSize, numberOfFragments, fragmentOffset);
+        return this;
+    }
+
+    /**
+     * Adds a highlighted field.
+     */
+    public TopHitsBuilder addHighlightedField(HighlightBuilder.Field field) {
+        highlightBuilder().field(field);
+        return this;
+    }
+
+    /**
+     * Set a tag scheme that encapsulates a built in pre and post tags. The allows schemes
+     * are <tt>styled</tt> and <tt>default</tt>.
+     *
+     * @param schemaName The tag scheme name
+     */
+    public TopHitsBuilder setHighlighterTagsSchema(String schemaName) {
+        highlightBuilder().tagsSchema(schemaName);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterFragmentSize(Integer fragmentSize) {
+        highlightBuilder().fragmentSize(fragmentSize);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterNumOfFragments(Integer numOfFragments) {
+        highlightBuilder().numOfFragments(numOfFragments);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterFilter(Boolean highlightFilter) {
+        highlightBuilder().highlightFilter(highlightFilter);
+        return this;
+    }
+
+    /**
+     * The encoder to set for highlighting
+     */
+    public TopHitsBuilder setHighlighterEncoder(String encoder) {
+        highlightBuilder().encoder(encoder);
+        return this;
+    }
+
+    /**
+     * Explicitly set the pre tags that will be used for highlighting.
+     */
+    public TopHitsBuilder setHighlighterPreTags(String... preTags) {
+        highlightBuilder().preTags(preTags);
+        return this;
+    }
+
+    /**
+     * Explicitly set the post tags that will be used for highlighting.
+     */
+    public TopHitsBuilder setHighlighterPostTags(String... postTags) {
+        highlightBuilder().postTags(postTags);
+        return this;
+    }
+
+    /**
+     * The order of fragments per field. By default, ordered by the order in the
+     * highlighted text. Can be <tt>score</tt>, which then it will be ordered
+     * by score of the fragments.
+     */
+    public TopHitsBuilder setHighlighterOrder(String order) {
+        highlightBuilder().order(order);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterRequireFieldMatch(boolean requireFieldMatch) {
+        highlightBuilder().requireFieldMatch(requireFieldMatch);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterBoundaryMaxScan(Integer boundaryMaxScan) {
+        highlightBuilder().boundaryMaxScan(boundaryMaxScan);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterBoundaryChars(char[] boundaryChars) {
+        highlightBuilder().boundaryChars(boundaryChars);
+        return this;
+    }
+
+    /**
+     * The highlighter type to use.
+     */
+    public TopHitsBuilder setHighlighterType(String type) {
+        highlightBuilder().highlighterType(type);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterFragmenter(String fragmenter) {
+        highlightBuilder().fragmenter(fragmenter);
+        return this;
+    }
+
+    /**
+     * Sets a query to be used for highlighting all fields instead of the search query.
+     */
+    public TopHitsBuilder setHighlighterQuery(QueryBuilder highlightQuery) {
+        highlightBuilder().highlightQuery(highlightQuery);
+        return this;
+    }
+
+    /**
+     * Sets the size of the fragment to return from the beginning of the field if there are no matches to
+     * highlight and the field doesn't also define noMatchSize.
+     * @param noMatchSize integer to set or null to leave out of request.  default is null.
+     * @return this builder for chaining
+     */
+    public TopHitsBuilder setHighlighterNoMatchSize(Integer noMatchSize) {
+        highlightBuilder().noMatchSize(noMatchSize);
+        return this;
+    }
+
+    /**
+     * Sets the maximum number of phrases the fvh will consider if the field doesn't also define phraseLimit.
+     */
+    public TopHitsBuilder setHighlighterPhraseLimit(Integer phraseLimit) {
+        highlightBuilder().phraseLimit(phraseLimit);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterOptions(Map<String, Object> options) {
+        highlightBuilder().options(options);
+        return this;
+    }
+
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject(getName()).field(type);
@@ -186,12 +366,7 @@ public class TopHitsBuilder extends AbstractAggregationBuilder {
         return sourceBuilder;
     }
 
-    public BytesReference highlighter() {
+    public HighlightBuilder highlightBuilder() {
         return sourceBuilder().highlighter();
     }
-
-    public TopHitsBuilder highlighter(HighlightBuilder highlightBuilder) {
-        sourceBuilder().highlighter(highlightBuilder);
-        return this;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java
index 6279ad1..669a223 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java
@@ -26,7 +26,6 @@ import org.elasticsearch.script.CompiledScript;
 import org.elasticsearch.script.ExecutableScript;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.expression.ExpressionScriptEngineService;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -105,7 +104,8 @@ public class BucketSelectorPipelineAggregator extends PipelineAggregator {
             ExecutableScript executableScript = reduceContext.scriptService().executable(compiledScript, vars);
             Object scriptReturnValue = executableScript.run();
             final boolean keepBucket;
-            if (ExpressionScriptEngineService.NAME.equals(script.getLang())) {
+            // TODO: WTF!!!!!
+            if ("expression".equals(script.getLang())) {
                 double scriptDoubleValue = (double) scriptReturnValue;
                 keepBucket = scriptDoubleValue == 1.0;
             } else {
diff --git a/core/src/main/java/org/elasticsearch/search/builder/OldSearchSourceBuilder.java b/core/src/main/java/org/elasticsearch/search/builder/OldSearchSourceBuilder.java
deleted file mode 100644
index 949628b..0000000
--- a/core/src/main/java/org/elasticsearch/search/builder/OldSearchSourceBuilder.java
+++ /dev/null
@@ -1,840 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.builder;
-
-import com.carrotsearch.hppc.ObjectFloatHashMap;
-
-import org.elasticsearch.ElasticsearchGenerationException;
-import org.elasticsearch.action.support.QuerySourceBuilder;
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
-import org.elasticsearch.search.highlight.HighlightBuilder;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.search.rescore.RescoreBuilder;
-import org.elasticsearch.search.sort.SortBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.search.suggest.SuggestBuilder;
-
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-/**
- * A search source builder allowing to easily build search source. Simple
- * construction using
- * {@link org.elasticsearch.search.builder.OldSearchSourceBuilder#searchSource()}
- */
-public class OldSearchSourceBuilder extends ToXContentToBytes {
-
-    /**
-     * A static factory method to construct a new search source.
-     */
-    public static OldSearchSourceBuilder searchSource() {
-        return new OldSearchSourceBuilder();
-    }
-
-    /**
-     * A static factory method to construct new search highlights.
-     */
-    public static HighlightBuilder highlight() {
-        return new HighlightBuilder();
-    }
-
-    private QuerySourceBuilder querySourceBuilder;
-
-    private QueryBuilder postQueryBuilder;
-
-    private BytesReference filterBinary;
-
-    private int from = -1;
-
-    private int size = -1;
-
-    private Boolean explain;
-
-    private Boolean version;
-
-    private List<SortBuilder> sorts;
-
-    private boolean trackScores = false;
-
-    private Float minScore;
-
-    private long timeoutInMillis = -1;
-    private int terminateAfter = SearchContext.DEFAULT_TERMINATE_AFTER;
-
-    private List<String> fieldNames;
-    private List<String> fieldDataFields;
-    private List<ScriptField> scriptFields;
-    private FetchSourceContext fetchSourceContext;
-
-    private List<AbstractAggregationBuilder> aggregations;
-    private BytesReference aggregationsBinary;
-
-    private HighlightBuilder highlightBuilder;
-
-    private SuggestBuilder suggestBuilder;
-
-    private InnerHitsBuilder innerHitsBuilder;
-
-    private List<RescoreBuilder> rescoreBuilders;
-    private Integer defaultRescoreWindowSize;
-
-    private ObjectFloatHashMap<String> indexBoost = null;
-
-    private String[] stats;
-
-    /**
-     * Constructs a new search source builder.
-     */
-    public OldSearchSourceBuilder() {
-    }
-
-    /**
-     * Sets the query provided as a {@link QuerySourceBuilder}
-     */
-    public OldSearchSourceBuilder query(QuerySourceBuilder querySourceBuilder) {
-        this.querySourceBuilder = querySourceBuilder;
-        return this;
-    }
-
-    /**
-     * Constructs a new search source builder with a search query.
-     *
-     * @see org.elasticsearch.index.query.QueryBuilders
-     */
-    public OldSearchSourceBuilder query(QueryBuilder query) {
-        if (this.querySourceBuilder == null) {
-            this.querySourceBuilder = new QuerySourceBuilder();
-        }
-        this.querySourceBuilder.setQuery(query);
-        return this;
-    }
-
-    /**
-     * Constructs a new search source builder with a raw search query.
-     */
-    public OldSearchSourceBuilder query(byte[] queryBinary) {
-        return query(queryBinary, 0, queryBinary.length);
-    }
-
-    /**
-     * Constructs a new search source builder with a raw search query.
-     */
-    public OldSearchSourceBuilder query(byte[] queryBinary, int queryBinaryOffset, int queryBinaryLength) {
-        return query(new BytesArray(queryBinary, queryBinaryOffset, queryBinaryLength));
-    }
-
-    /**
-     * Constructs a new search source builder with a raw search query.
-     */
-    public OldSearchSourceBuilder query(BytesReference queryBinary) {
-        if (this.querySourceBuilder == null) {
-            this.querySourceBuilder = new QuerySourceBuilder();
-        }
-        this.querySourceBuilder.setQuery(queryBinary);
-        return this;
-    }
-
-    /**
-     * Constructs a new search source builder with a raw search query.
-     */
-    public OldSearchSourceBuilder query(String queryString) {
-        return query(queryString.getBytes(StandardCharsets.UTF_8));
-    }
-
-    /**
-     * Constructs a new search source builder with a query from a builder.
-     */
-    public OldSearchSourceBuilder query(XContentBuilder query) {
-        return query(query.bytes());
-    }
-
-    /**
-     * Constructs a new search source builder with a query from a map.
-     */
-    @SuppressWarnings("unchecked")
-    public OldSearchSourceBuilder query(Map query) {
-        try {
-            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
-            builder.map(query);
-            return query(builder);
-        } catch (IOException e) {
-            throw new ElasticsearchGenerationException("Failed to generate [" + query + "]", e);
-        }
-    }
-
-    /**
-     * Sets a filter that will be executed after the query has been executed and
-     * only has affect on the search hits (not aggregations). This filter is
-     * always executed as last filtering mechanism.
-     */
-    public OldSearchSourceBuilder postFilter(QueryBuilder postFilter) {
-        this.postQueryBuilder = postFilter;
-        return this;
-    }
-
-    /**
-     * Sets a filter on the query executed that only applies to the search query
-     * (and not aggs for example).
-     */
-    public OldSearchSourceBuilder postFilter(String postFilterString) {
-        return postFilter(postFilterString.getBytes(StandardCharsets.UTF_8));
-    }
-
-    /**
-     * Sets a filter on the query executed that only applies to the search query
-     * (and not aggs for example).
-     */
-    public OldSearchSourceBuilder postFilter(byte[] postFilter) {
-        return postFilter(postFilter, 0, postFilter.length);
-    }
-
-    /**
-     * Sets a filter on the query executed that only applies to the search query
-     * (and not aggs for example).
-     */
-    public OldSearchSourceBuilder postFilter(byte[] postFilterBinary, int postFilterBinaryOffset, int postFilterBinaryLength) {
-        return postFilter(new BytesArray(postFilterBinary, postFilterBinaryOffset, postFilterBinaryLength));
-    }
-
-    /**
-     * Sets a filter on the query executed that only applies to the search query
-     * (and not aggs for example).
-     */
-    public OldSearchSourceBuilder postFilter(BytesReference postFilterBinary) {
-        this.filterBinary = postFilterBinary;
-        return this;
-    }
-
-    /**
-     * Constructs a new search source builder with a query from a builder.
-     */
-    public OldSearchSourceBuilder postFilter(XContentBuilder postFilter) {
-        return postFilter(postFilter.bytes());
-    }
-
-    /**
-     * Constructs a new search source builder with a query from a map.
-     */
-    @SuppressWarnings("unchecked")
-    public OldSearchSourceBuilder postFilter(Map postFilter) {
-        try {
-            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
-            builder.map(postFilter);
-            return postFilter(builder);
-        } catch (IOException e) {
-            throw new ElasticsearchGenerationException("Failed to generate [" + postFilter + "]", e);
-        }
-    }
-
-    /**
-     * From index to start the search from. Defaults to <tt>0</tt>.
-     */
-    public OldSearchSourceBuilder from(int from) {
-        this.from = from;
-        return this;
-    }
-
-    /**
-     * The number of search hits to return. Defaults to <tt>10</tt>.
-     */
-    public OldSearchSourceBuilder size(int size) {
-        this.size = size;
-        return this;
-    }
-
-    /**
-     * Sets the minimum score below which docs will be filtered out.
-     */
-    public OldSearchSourceBuilder minScore(float minScore) {
-        this.minScore = minScore;
-        return this;
-    }
-
-    /**
-     * Should each {@link org.elasticsearch.search.SearchHit} be returned with
-     * an explanation of the hit (ranking).
-     */
-    public OldSearchSourceBuilder explain(Boolean explain) {
-        this.explain = explain;
-        return this;
-    }
-
-    /**
-     * Should each {@link org.elasticsearch.search.SearchHit} be returned with a
-     * version associated with it.
-     */
-    public OldSearchSourceBuilder version(Boolean version) {
-        this.version = version;
-        return this;
-    }
-
-    /**
-     * An optional timeout to control how long search is allowed to take.
-     */
-    public OldSearchSourceBuilder timeout(TimeValue timeout) {
-        this.timeoutInMillis = timeout.millis();
-        return this;
-    }
-
-    /**
-     * An optional timeout to control how long search is allowed to take.
-     */
-    public OldSearchSourceBuilder timeout(String timeout) {
-        this.timeoutInMillis = TimeValue.parseTimeValue(timeout, null, getClass().getSimpleName() + ".timeout").millis();
-        return this;
-    }
-
-    /**
-     * An optional terminate_after to terminate the search after collecting
-     * <code>terminateAfter</code> documents
-     */
-    public OldSearchSourceBuilder terminateAfter(int terminateAfter) {
-        if (terminateAfter <= 0) {
-            throw new IllegalArgumentException("terminateAfter must be > 0");
-        }
-        this.terminateAfter = terminateAfter;
-        return this;
-    }
-
-    /**
-     * Adds a sort against the given field name and the sort ordering.
-     *
-     * @param name
-     *            The name of the field
-     * @param order
-     *            The sort ordering
-     */
-    public OldSearchSourceBuilder sort(String name, SortOrder order) {
-        return sort(SortBuilders.fieldSort(name).order(order));
-    }
-
-    /**
-     * Add a sort against the given field name.
-     *
-     * @param name
-     *            The name of the field to sort by
-     */
-    public OldSearchSourceBuilder sort(String name) {
-        return sort(SortBuilders.fieldSort(name));
-    }
-
-    /**
-     * Adds a sort builder.
-     */
-    public OldSearchSourceBuilder sort(SortBuilder sort) {
-        if (sorts == null) {
-            sorts = new ArrayList<>();
-        }
-        sorts.add(sort);
-        return this;
-    }
-
-    /**
-     * Applies when sorting, and controls if scores will be tracked as well.
-     * Defaults to <tt>false</tt>.
-     */
-    public OldSearchSourceBuilder trackScores(boolean trackScores) {
-        this.trackScores = trackScores;
-        return this;
-    }
-
-    /**
-     * Add an get to perform as part of the search.
-     */
-    public OldSearchSourceBuilder aggregation(AbstractAggregationBuilder aggregation) {
-        if (aggregations == null) {
-            aggregations = new ArrayList<>();
-        }
-        aggregations.add(aggregation);
-        return this;
-    }
-
-    /**
-     * Sets a raw (xcontent / json) addAggregation.
-     */
-    public OldSearchSourceBuilder aggregations(byte[] aggregationsBinary) {
-        return aggregations(aggregationsBinary, 0, aggregationsBinary.length);
-    }
-
-    /**
-     * Sets a raw (xcontent / json) addAggregation.
-     */
-    public OldSearchSourceBuilder aggregations(byte[] aggregationsBinary, int aggregationsBinaryOffset, int aggregationsBinaryLength) {
-        return aggregations(new BytesArray(aggregationsBinary, aggregationsBinaryOffset, aggregationsBinaryLength));
-    }
-
-    /**
-     * Sets a raw (xcontent / json) addAggregation.
-     */
-    public OldSearchSourceBuilder aggregations(BytesReference aggregationsBinary) {
-        this.aggregationsBinary = aggregationsBinary;
-        return this;
-    }
-
-    /**
-     * Sets a raw (xcontent / json) addAggregation.
-     */
-    public OldSearchSourceBuilder aggregations(XContentBuilder aggs) {
-        return aggregations(aggs.bytes());
-    }
-
-    /**
-     * Set the rescore window size for rescores that don't specify their window.
-     */
-    public OldSearchSourceBuilder defaultRescoreWindowSize(int defaultRescoreWindowSize) {
-        this.defaultRescoreWindowSize = defaultRescoreWindowSize;
-        return this;
-    }
-
-    /**
-     * Sets a raw (xcontent / json) addAggregation.
-     */
-    @SuppressWarnings("unchecked")
-    public OldSearchSourceBuilder aggregations(Map aggregations) {
-        try {
-            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
-            builder.map(aggregations);
-            return aggregations(builder);
-        } catch (IOException e) {
-            throw new ElasticsearchGenerationException("Failed to generate [" + aggregations + "]", e);
-        }
-    }
-
-    public HighlightBuilder highlighter() {
-        if (highlightBuilder == null) {
-            highlightBuilder = new HighlightBuilder();
-        }
-        return highlightBuilder;
-    }
-
-    /**
-     * Adds highlight to perform as part of the search.
-     */
-    public OldSearchSourceBuilder highlight(HighlightBuilder highlightBuilder) {
-        this.highlightBuilder = highlightBuilder;
-        return this;
-    }
-
-    public InnerHitsBuilder innerHitsBuilder() {
-        if (innerHitsBuilder == null) {
-            innerHitsBuilder = new InnerHitsBuilder();
-        }
-        return innerHitsBuilder;
-    }
-
-    public SuggestBuilder suggest() {
-        if (suggestBuilder == null) {
-            suggestBuilder = new SuggestBuilder("suggest");
-        }
-        return suggestBuilder;
-    }
-
-    public OldSearchSourceBuilder addRescorer(RescoreBuilder rescoreBuilder) {
-        if (rescoreBuilders == null) {
-            rescoreBuilders = new ArrayList<>();
-        }
-        rescoreBuilders.add(rescoreBuilder);
-        return this;
-    }
-
-    public OldSearchSourceBuilder clearRescorers() {
-        rescoreBuilders = null;
-        return this;
-    }
-
-    /**
-     * Indicates whether the response should contain the stored _source for
-     * every hit
-     */
-    public OldSearchSourceBuilder fetchSource(boolean fetch) {
-        if (this.fetchSourceContext == null) {
-            this.fetchSourceContext = new FetchSourceContext(fetch);
-        } else {
-            this.fetchSourceContext.fetchSource(fetch);
-        }
-        return this;
-    }
-
-    /**
-     * Indicate that _source should be returned with every hit, with an
-     * "include" and/or "exclude" set which can include simple wildcard
-     * elements.
-     *
-     * @param include
-     *            An optional include (optionally wildcarded) pattern to filter
-     *            the returned _source
-     * @param exclude
-     *            An optional exclude (optionally wildcarded) pattern to filter
-     *            the returned _source
-     */
-    public OldSearchSourceBuilder fetchSource(@Nullable String include, @Nullable String exclude) {
-        return fetchSource(include == null ? Strings.EMPTY_ARRAY : new String[] { include }, exclude == null ? Strings.EMPTY_ARRAY
-                : new String[] { exclude });
-    }
-
-    /**
-     * Indicate that _source should be returned with every hit, with an
-     * "include" and/or "exclude" set which can include simple wildcard
-     * elements.
-     *
-     * @param includes
-     *            An optional list of include (optionally wildcarded) pattern to
-     *            filter the returned _source
-     * @param excludes
-     *            An optional list of exclude (optionally wildcarded) pattern to
-     *            filter the returned _source
-     */
-    public OldSearchSourceBuilder fetchSource(@Nullable String[] includes, @Nullable String[] excludes) {
-        fetchSourceContext = new FetchSourceContext(includes, excludes);
-        return this;
-    }
-
-    /**
-     * Indicate how the _source should be fetched.
-     */
-    public OldSearchSourceBuilder fetchSource(@Nullable FetchSourceContext fetchSourceContext) {
-        this.fetchSourceContext = fetchSourceContext;
-        return this;
-    }
-
-    /**
-     * Sets no fields to be loaded, resulting in only id and type to be returned
-     * per field.
-     */
-    public OldSearchSourceBuilder noFields() {
-        this.fieldNames = Collections.emptyList();
-        return this;
-    }
-
-    /**
-     * Sets the fields to load and return as part of the search request. If none
-     * are specified, the source of the document will be returned.
-     */
-    public OldSearchSourceBuilder fields(List<String> fields) {
-        this.fieldNames = fields;
-        return this;
-    }
-
-    /**
-     * Adds the fields to load and return as part of the search request. If none
-     * are specified, the source of the document will be returned.
-     */
-    public OldSearchSourceBuilder fields(String... fields) {
-        if (fieldNames == null) {
-            fieldNames = new ArrayList<>();
-        }
-        Collections.addAll(fieldNames, fields);
-        return this;
-    }
-
-    /**
-     * Adds a field to load and return (note, it must be stored) as part of the
-     * search request. If none are specified, the source of the document will be
-     * return.
-     */
-    public OldSearchSourceBuilder field(String name) {
-        if (fieldNames == null) {
-            fieldNames = new ArrayList<>();
-        }
-        fieldNames.add(name);
-        return this;
-    }
-
-    /**
-     * Adds a field to load from the field data cache and return as part of the
-     * search request.
-     */
-    public OldSearchSourceBuilder fieldDataField(String name) {
-        if (fieldDataFields == null) {
-            fieldDataFields = new ArrayList<>();
-        }
-        fieldDataFields.add(name);
-        return this;
-    }
-
-    /**
-     * Adds a script field under the given name with the provided script.
-     *
-     * @param name
-     *            The name of the field
-     * @param script
-     *            The script
-     */
-    public OldSearchSourceBuilder scriptField(String name, Script script) {
-        if (scriptFields == null) {
-            scriptFields = new ArrayList<>();
-        }
-        scriptFields.add(new ScriptField(name, script));
-        return this;
-    }
-
-    /**
-     * Sets the boost a specific index will receive when the query is executeed
-     * against it.
-     *
-     * @param index
-     *            The index to apply the boost against
-     * @param indexBoost
-     *            The boost to apply to the index
-     */
-    public OldSearchSourceBuilder indexBoost(String index, float indexBoost) {
-        if (this.indexBoost == null) {
-            this.indexBoost = new ObjectFloatHashMap<>();
-        }
-        this.indexBoost.put(index, indexBoost);
-        return this;
-    }
-
-    /**
-     * The stats groups this request will be aggregated under.
-     */
-    public OldSearchSourceBuilder stats(String... statsGroups) {
-        this.stats = statsGroups;
-        return this;
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        innerToXContent(builder, params);
-        builder.endObject();
-        return builder;
-    }
-
-    public void innerToXContent(XContentBuilder builder, Params params) throws IOException {
-        if (from != -1) {
-            builder.field("from", from);
-        }
-        if (size != -1) {
-            builder.field("size", size);
-        }
-
-        if (timeoutInMillis != -1) {
-            builder.field("timeout", timeoutInMillis);
-        }
-
-        if (terminateAfter != SearchContext.DEFAULT_TERMINATE_AFTER) {
-            builder.field("terminate_after", terminateAfter);
-        }
-
-        if (querySourceBuilder != null) {
-            querySourceBuilder.innerToXContent(builder, params);
-        }
-
-        if (postQueryBuilder != null) {
-            builder.field("post_filter");
-            postQueryBuilder.toXContent(builder, params);
-        }
-
-        if (filterBinary != null) {
-            if (XContentFactory.xContentType(filterBinary) == builder.contentType()) {
-                builder.rawField("filter", filterBinary);
-            } else {
-                builder.field("filter_binary", filterBinary);
-            }
-        }
-
-        if (minScore != null) {
-            builder.field("min_score", minScore);
-        }
-
-        if (version != null) {
-            builder.field("version", version);
-        }
-
-        if (explain != null) {
-            builder.field("explain", explain);
-        }
-
-        if (fetchSourceContext != null) {
-            if (!fetchSourceContext.fetchSource()) {
-                builder.field("_source", false);
-            } else {
-                builder.startObject("_source");
-                builder.array("includes", fetchSourceContext.includes());
-                builder.array("excludes", fetchSourceContext.excludes());
-                builder.endObject();
-            }
-        }
-
-        if (fieldNames != null) {
-            if (fieldNames.size() == 1) {
-                builder.field("fields", fieldNames.get(0));
-            } else {
-                builder.startArray("fields");
-                for (String fieldName : fieldNames) {
-                    builder.value(fieldName);
-                }
-                builder.endArray();
-            }
-        }
-
-        if (fieldDataFields != null) {
-            builder.startArray("fielddata_fields");
-            for (String fieldName : fieldDataFields) {
-                builder.value(fieldName);
-            }
-            builder.endArray();
-        }
-
-        if (scriptFields != null) {
-            builder.startObject("script_fields");
-            for (ScriptField scriptField : scriptFields) {
-                builder.startObject(scriptField.fieldName());
-                builder.field("script", scriptField.script());
-                builder.endObject();
-            }
-            builder.endObject();
-        }
-
-        if (sorts != null) {
-            builder.startArray("sort");
-            for (SortBuilder sort : sorts) {
-                builder.startObject();
-                sort.toXContent(builder, params);
-                builder.endObject();
-            }
-            builder.endArray();
-        }
-
-        if (trackScores) {
-            builder.field("track_scores", true);
-        }
-
-        if (indexBoost != null) {
-            builder.startObject("indices_boost");
-            assert !indexBoost.containsKey(null);
-            final Object[] keys = indexBoost.keys;
-            final float[] values = indexBoost.values;
-            for (int i = 0; i < keys.length; i++) {
-                if (keys[i] != null) {
-                    builder.field((String) keys[i], values[i]);
-                }
-            }
-            builder.endObject();
-        }
-
-        if (aggregations != null) {
-            builder.field("aggregations");
-            builder.startObject();
-            for (AbstractAggregationBuilder aggregation : aggregations) {
-                aggregation.toXContent(builder, params);
-            }
-            builder.endObject();
-        }
-
-        if (aggregationsBinary != null) {
-            if (XContentFactory.xContentType(aggregationsBinary) == builder.contentType()) {
-                builder.rawField("aggregations", aggregationsBinary);
-            } else {
-                builder.field("aggregations_binary", aggregationsBinary);
-            }
-        }
-
-        if (highlightBuilder != null) {
-            highlightBuilder.toXContent(builder, params);
-        }
-
-        if (innerHitsBuilder != null) {
-            innerHitsBuilder.toXContent(builder, params);
-        }
-
-        if (suggestBuilder != null) {
-            suggestBuilder.toXContent(builder, params);
-        }
-
-        if (rescoreBuilders != null) {
-            // Strip empty rescoreBuilders from the request
-            Iterator<RescoreBuilder> itr = rescoreBuilders.iterator();
-            while (itr.hasNext()) {
-                if (itr.next().isEmpty()) {
-                    itr.remove();
-                }
-            }
-
-            // Now build the request taking care to skip empty lists and only send the object form
-            // if there is just one builder.
-            if (rescoreBuilders.size() == 1) {
-                builder.startObject("rescore");
-                rescoreBuilders.get(0).toXContent(builder, params);
-                if (rescoreBuilders.get(0).windowSize() == null && defaultRescoreWindowSize != null) {
-                    builder.field("window_size", defaultRescoreWindowSize);
-                }
-                builder.endObject();
-            } else if (!rescoreBuilders.isEmpty()) {
-                builder.startArray("rescore");
-                for (RescoreBuilder rescoreBuilder : rescoreBuilders) {
-                    builder.startObject();
-                    rescoreBuilder.toXContent(builder, params);
-                    if (rescoreBuilder.windowSize() == null && defaultRescoreWindowSize != null) {
-                        builder.field("window_size", defaultRescoreWindowSize);
-                    }
-                    builder.endObject();
-                }
-                builder.endArray();
-            }
-        }
-
-        if (stats != null) {
-            builder.startArray("stats");
-            for (String stat : stats) {
-                builder.value(stat);
-            }
-            builder.endArray();
-        }
-    }
-
-    private static class ScriptField {
-        private final String fieldName;
-        private final Script script;
-
-        private ScriptField(String fieldName, Script script) {
-            this.fieldName = fieldName;
-            this.script = script;
-        }
-
-        public String fieldName() {
-            return fieldName;
-        }
-
-        public Script script() {
-            return script;
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
index 914c964..3b87030 100644
--- a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
@@ -20,25 +20,19 @@
 package org.elasticsearch.search.builder;
 
 import com.carrotsearch.hppc.ObjectFloatHashMap;
-import com.carrotsearch.hppc.cursors.ObjectCursor;
-
+import java.nio.charset.StandardCharsets;
+import org.elasticsearch.ElasticsearchGenerationException;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.action.support.ToXContentToBytes;
+import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
@@ -53,49 +47,19 @@ import org.elasticsearch.search.suggest.SuggestBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.Collections;
+import java.util.Iterator;
 import java.util.List;
-import java.util.Objects;
+import java.util.Map;
 
 /**
  * A search source builder allowing to easily build search source. Simple
  * construction using
- * {@link org.elasticsearch.search.builder.NewSearchSourceBuilder#searchSource()}.
+ * {@link org.elasticsearch.search.builder.SearchSourceBuilder#searchSource()}.
  *
- * @see org.elasticsearch.action.search.SearchRequest#source(NewSearchSourceBuilder)
+ * @see org.elasticsearch.action.search.SearchRequest#source(SearchSourceBuilder)
  */
-/**
- *
- */
-public final class SearchSourceBuilder extends ToXContentToBytes implements Writeable<SearchSourceBuilder> {
-
-    public static final ParseField FROM_FIELD = new ParseField("from");
-    public static final ParseField SIZE_FIELD = new ParseField("size");
-    public static final ParseField TIMEOUT_FIELD = new ParseField("timeout");
-    public static final ParseField TERMINATE_AFTER_FIELD = new ParseField("terminate_after");
-    public static final ParseField QUERY_FIELD = new ParseField("query");
-    public static final ParseField POST_FILTER_FIELD = new ParseField("post_filter");
-    public static final ParseField MIN_SCORE_FIELD = new ParseField("min_score");
-    public static final ParseField VERSION_FIELD = new ParseField("version");
-    public static final ParseField EXPLAIN_FIELD = new ParseField("explain");
-    public static final ParseField _SOURCE_FIELD = new ParseField("_source");
-    public static final ParseField FIELDS_FIELD = new ParseField("fields");
-    public static final ParseField FIELDDATA_FIELDS_FIELD = new ParseField("fielddata_fields");
-    public static final ParseField SCRIPT_FIELDS_FIELD = new ParseField("script_fields");
-    public static final ParseField SCRIPT_FIELD = new ParseField("script");
-    public static final ParseField SORT_FIELD = new ParseField("sort");
-    public static final ParseField TRACK_SCORES_FIELD = new ParseField("track_scores");
-    public static final ParseField INDICES_BOOST_FIELD = new ParseField("indices_boost");
-    public static final ParseField AGGREGATIONS_FIELD = new ParseField("aggregations", "aggs");
-    public static final ParseField HIGHLIGHT_FIELD = new ParseField("highlight");
-    public static final ParseField INNER_HITS_FIELD = new ParseField("inner_hits");
-    public static final ParseField SUGGEST_FIELD = new ParseField("suggest");
-    public static final ParseField RESCORE_FIELD = new ParseField("rescore");
-    public static final ParseField STATS_FIELD = new ParseField("stats");
-    public static final ParseField EXT_FIELD = new ParseField("ext");
-
-    public static final SearchSourceBuilder PROTOTYPE = new SearchSourceBuilder();
+public class SearchSourceBuilder extends ToXContentToBytes {
 
     /**
      * A static factory method to construct a new search source.
@@ -111,9 +75,11 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         return new HighlightBuilder();
     }
 
-    private QueryBuilder<?> queryBuilder;
+    private QuerySourceBuilder querySourceBuilder;
+
+    private QueryBuilder postQueryBuilder;
 
-    private QueryBuilder<?> postQueryBuilder;
+    private BytesReference filterBinary;
 
     private int from = -1;
 
@@ -123,7 +89,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
 
     private Boolean version;
 
-    private List<BytesReference> sorts;
+    private List<SortBuilder> sorts;
 
     private boolean trackScores = false;
 
@@ -137,23 +103,22 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     private List<ScriptField> scriptFields;
     private FetchSourceContext fetchSourceContext;
 
-    private List<BytesReference> aggregations;
+    private List<AbstractAggregationBuilder> aggregations;
+    private BytesReference aggregationsBinary;
 
-    private BytesReference highlightBuilder;
+    private HighlightBuilder highlightBuilder;
 
-    private BytesReference suggestBuilder;
+    private SuggestBuilder suggestBuilder;
 
-    private BytesReference innerHitsBuilder;
+    private InnerHitsBuilder innerHitsBuilder;
 
-    private List<BytesReference> rescoreBuilders;
+    private List<RescoreBuilder> rescoreBuilders;
     private Integer defaultRescoreWindowSize;
 
     private ObjectFloatHashMap<String> indexBoost = null;
 
     private String[] stats;
 
-    private BytesReference ext = null;
-
     /**
      * Constructs a new search source builder.
      */
@@ -161,20 +126,77 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Sets the search query for this request.
+     * Sets the query provided as a {@link QuerySourceBuilder}
+     */
+    public SearchSourceBuilder query(QuerySourceBuilder querySourceBuilder) {
+        this.querySourceBuilder = querySourceBuilder;
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a search query.
      *
      * @see org.elasticsearch.index.query.QueryBuilders
      */
-    public SearchSourceBuilder query(QueryBuilder<?> query) {
-        this.queryBuilder = query;
+    public SearchSourceBuilder query(QueryBuilder query) {
+        if (this.querySourceBuilder == null) {
+            this.querySourceBuilder = new QuerySourceBuilder();
+        }
+        this.querySourceBuilder.setQuery(query);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchSourceBuilder query(byte[] queryBinary) {
+        return query(queryBinary, 0, queryBinary.length);
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchSourceBuilder query(byte[] queryBinary, int queryBinaryOffset, int queryBinaryLength) {
+        return query(new BytesArray(queryBinary, queryBinaryOffset, queryBinaryLength));
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchSourceBuilder query(BytesReference queryBinary) {
+        if (this.querySourceBuilder == null) {
+            this.querySourceBuilder = new QuerySourceBuilder();
+        }
+        this.querySourceBuilder.setQuery(queryBinary);
         return this;
     }
 
     /**
-     * Gets the query for this request
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchSourceBuilder query(String queryString) {
+        return query(queryString.getBytes(StandardCharsets.UTF_8));
+    }
+
+    /**
+     * Constructs a new search source builder with a query from a builder.
      */
-    public QueryBuilder<?> query() {
-        return queryBuilder;
+    public SearchSourceBuilder query(XContentBuilder query) {
+        return query(query.bytes());
+    }
+
+    /**
+     * Constructs a new search source builder with a query from a map.
+     */
+    @SuppressWarnings("unchecked")
+    public SearchSourceBuilder query(Map query) {
+        try {
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(query);
+            return query(builder);
+        } catch (IOException e) {
+            throw new ElasticsearchGenerationException("Failed to generate [" + query + "]", e);
+        }
     }
 
     /**
@@ -182,61 +204,87 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
      * only has affect on the search hits (not aggregations). This filter is
      * always executed as last filtering mechanism.
      */
-    public SearchSourceBuilder postFilter(QueryBuilder<?> postFilter) {
+    public SearchSourceBuilder postFilter(QueryBuilder postFilter) {
         this.postQueryBuilder = postFilter;
         return this;
     }
 
     /**
-     * Gets the post filter for this request
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
      */
-    public QueryBuilder<?> postFilter() {
-        return postQueryBuilder;
+    public SearchSourceBuilder postFilter(String postFilterString) {
+        return postFilter(postFilterString.getBytes(StandardCharsets.UTF_8));
     }
 
     /**
-     * From index to start the search from. Defaults to <tt>0</tt>.
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
      */
-    public SearchSourceBuilder from(int from) {
-        this.from = from;
-        return this;
+    public SearchSourceBuilder postFilter(byte[] postFilter) {
+        return postFilter(postFilter, 0, postFilter.length);
     }
 
     /**
-     * Gets the from index to start the search from.
-     **/
-    public int from() {
-        return from;
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchSourceBuilder postFilter(byte[] postFilterBinary, int postFilterBinaryOffset, int postFilterBinaryLength) {
+        return postFilter(new BytesArray(postFilterBinary, postFilterBinaryOffset, postFilterBinaryLength));
     }
 
     /**
-     * The number of search hits to return. Defaults to <tt>10</tt>.
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
      */
-    public SearchSourceBuilder size(int size) {
-        this.size = size;
+    public SearchSourceBuilder postFilter(BytesReference postFilterBinary) {
+        this.filterBinary = postFilterBinary;
         return this;
     }
 
     /**
-     * Gets the number of search hits to return.
+     * Constructs a new search source builder with a query from a builder.
      */
-    public int size() {
-        return size;
+    public SearchSourceBuilder postFilter(XContentBuilder postFilter) {
+        return postFilter(postFilter.bytes());
     }
 
     /**
-     * Sets the minimum score below which docs will be filtered out.
+     * Constructs a new search source builder with a query from a map.
      */
-    public SearchSourceBuilder minScore(float minScore) {
-        this.minScore = minScore;
+    @SuppressWarnings("unchecked")
+    public SearchSourceBuilder postFilter(Map postFilter) {
+        try {
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(postFilter);
+            return postFilter(builder);
+        } catch (IOException e) {
+            throw new ElasticsearchGenerationException("Failed to generate [" + postFilter + "]", e);
+        }
+    }
+
+    /**
+     * From index to start the search from. Defaults to <tt>0</tt>.
+     */
+    public SearchSourceBuilder from(int from) {
+        this.from = from;
+        return this;
+    }
+
+    /**
+     * The number of search hits to return. Defaults to <tt>10</tt>.
+     */
+    public SearchSourceBuilder size(int size) {
+        this.size = size;
         return this;
     }
 
     /**
-     * Gets the minimum score below which docs will be filtered out.
+     * Sets the minimum score below which docs will be filtered out.
      */
-    public Float minScore() {
-        return minScore;
+    public SearchSourceBuilder minScore(float minScore) {
+        this.minScore = minScore;
+        return this;
     }
 
     /**
@@ -249,14 +297,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Indicates whether each search hit will be returned with an explanation of
-     * the hit (ranking)
-     */
-    public Boolean explain() {
-        return explain;
-    }
-
-    /**
      * Should each {@link org.elasticsearch.search.SearchHit} be returned with a
      * version associated with it.
      */
@@ -266,14 +306,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Indicates whether the document's version will be included in the search
-     * hits.
-     */
-    public Boolean version() {
-        return version;
-    }
-
-    /**
      * An optional timeout to control how long search is allowed to take.
      */
     public SearchSourceBuilder timeout(TimeValue timeout) {
@@ -282,10 +314,11 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the timeout to control how long search is allowed to take.
+     * An optional timeout to control how long search is allowed to take.
      */
-    public long timeoutInMillis() {
-        return timeoutInMillis;
+    public SearchSourceBuilder timeout(String timeout) {
+        this.timeoutInMillis = TimeValue.parseTimeValue(timeout, null, getClass().getSimpleName() + ".timeout").millis();
+        return this;
     }
 
     /**
@@ -301,13 +334,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the number of documents to terminate after collecting.
-     */
-    public int terminateAfter() {
-        return terminateAfter;
-    }
-
-    /**
      * Adds a sort against the given field name and the sort ordering.
      *
      * @param name
@@ -333,26 +359,11 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
      * Adds a sort builder.
      */
     public SearchSourceBuilder sort(SortBuilder sort) {
-        try {
-            if (sorts == null) {
-                sorts = new ArrayList<>();
-            }
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            sort.toXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            sorts.add(builder.bytes());
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
+        if (sorts == null) {
+            sorts = new ArrayList<>();
         }
-    }
-
-    /**
-     * Gets the bytes representing the sort builders for this request.
-     */
-    public List<BytesReference> sorts() {
-        return sorts;
+        sorts.add(sort);
+        return this;
     }
 
     /**
@@ -365,128 +376,102 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Indicates whether scores will be tracked for this request.
+     * Add an get to perform as part of the search.
      */
-    public boolean trackScores() {
-        return trackScores;
+    public SearchSourceBuilder aggregation(AbstractAggregationBuilder aggregation) {
+        if (aggregations == null) {
+            aggregations = new ArrayList<>();
+        }
+        aggregations.add(aggregation);
+        return this;
     }
 
     /**
-     * Add an aggregation to perform as part of the search.
+     * Sets a raw (xcontent / json) addAggregation.
      */
-    public SearchSourceBuilder aggregation(AbstractAggregationBuilder aggregation) {
-        try {
-            if (aggregations == null) {
-                aggregations = new ArrayList<>();
-            }
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            aggregation.toXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            aggregations.add(builder.bytes());
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
-        }
+    public SearchSourceBuilder aggregations(byte[] aggregationsBinary) {
+        return aggregations(aggregationsBinary, 0, aggregationsBinary.length);
     }
 
     /**
-     * Gets the bytes representing the aggregation builders for this request.
+     * Sets a raw (xcontent / json) addAggregation.
      */
-    public List<BytesReference> aggregations() {
-        return aggregations;
+    public SearchSourceBuilder aggregations(byte[] aggregationsBinary, int aggregationsBinaryOffset, int aggregationsBinaryLength) {
+        return aggregations(new BytesArray(aggregationsBinary, aggregationsBinaryOffset, aggregationsBinaryLength));
     }
 
     /**
-     * Set the rescore window size for rescores that don't specify their window.
+     * Sets a raw (xcontent / json) addAggregation.
      */
-    public SearchSourceBuilder defaultRescoreWindowSize(int defaultRescoreWindowSize) {
-        this.defaultRescoreWindowSize = defaultRescoreWindowSize;
+    public SearchSourceBuilder aggregations(BytesReference aggregationsBinary) {
+        this.aggregationsBinary = aggregationsBinary;
         return this;
     }
 
     /**
-     * Get the rescore window size for rescores that don't specify their window.
+     * Sets a raw (xcontent / json) addAggregation.
      */
-    public int defaultRescoreWindowSize() {
-        return defaultRescoreWindowSize;
+    public SearchSourceBuilder aggregations(XContentBuilder aggs) {
+        return aggregations(aggs.bytes());
     }
 
     /**
-     * Adds highlight to perform as part of the search.
+     * Set the rescore window size for rescores that don't specify their window.
      */
-    public SearchSourceBuilder highlighter(HighlightBuilder highlightBuilder) {
-        try {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            highlightBuilder.innerXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            this.highlightBuilder = builder.bytes();
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
-        }
+    public SearchSourceBuilder defaultRescoreWindowSize(int defaultRescoreWindowSize) {
+        this.defaultRescoreWindowSize = defaultRescoreWindowSize;
+        return this;
     }
 
     /**
-     * Gets the bytes representing the hightlighter builder for this request.
+     * Sets a raw (xcontent / json) addAggregation.
      */
-    public BytesReference highlighter() {
-        return highlightBuilder;
-    }
-
-    public SearchSourceBuilder innerHits(InnerHitsBuilder innerHitsBuilder) {
+    @SuppressWarnings("unchecked")
+    public SearchSourceBuilder aggregations(Map aggregations) {
         try {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            innerHitsBuilder.innerXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            this.innerHitsBuilder = builder.bytes();
-            return this;
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(aggregations);
+            return aggregations(builder);
         } catch (IOException e) {
-            throw new RuntimeException(e);
+            throw new ElasticsearchGenerationException("Failed to generate [" + aggregations + "]", e);
+        }
+    }
+
+    public HighlightBuilder highlighter() {
+        if (highlightBuilder == null) {
+            highlightBuilder = new HighlightBuilder();
         }
+        return highlightBuilder;
     }
 
     /**
-     * Gets the bytes representing the inner hits builder for this request.
+     * Adds highlight to perform as part of the search.
      */
-    public BytesReference innerHits() {
-        return innerHitsBuilder;
+    public SearchSourceBuilder highlight(HighlightBuilder highlightBuilder) {
+        this.highlightBuilder = highlightBuilder;
+        return this;
     }
 
-    public SearchSourceBuilder suggest(SuggestBuilder suggestBuilder) {
-        try {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            suggestBuilder.toXContent(builder, EMPTY_PARAMS);
-            this.suggestBuilder = builder.bytes();
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
+    public InnerHitsBuilder innerHitsBuilder() {
+        if (innerHitsBuilder == null) {
+            innerHitsBuilder = new InnerHitsBuilder();
         }
+        return innerHitsBuilder;
     }
 
-    /**
-     * Gets the bytes representing the suggester builder for this request.
-     */
-    public BytesReference suggest() {
+    public SuggestBuilder suggest() {
+        if (suggestBuilder == null) {
+            suggestBuilder = new SuggestBuilder("suggest");
+        }
         return suggestBuilder;
     }
 
     public SearchSourceBuilder addRescorer(RescoreBuilder rescoreBuilder) {
-        try {
-            if (rescoreBuilders == null) {
-                rescoreBuilders = new ArrayList<>();
-            }
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            rescoreBuilder.toXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            rescoreBuilders.add(builder.bytes());
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
+        if (rescoreBuilders == null) {
+            rescoreBuilders = new ArrayList<>();
         }
+        rescoreBuilders.add(rescoreBuilder);
+        return this;
     }
 
     public SearchSourceBuilder clearRescorers() {
@@ -495,13 +480,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the bytes representing the rescore builders for this request.
-     */
-    public List<BytesReference> rescores() {
-        return rescoreBuilders;
-    }
-
-    /**
      * Indicates whether the response should contain the stored _source for
      * every hit
      */
@@ -557,23 +535,11 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the {@link FetchSourceContext} which defines how the _source should
-     * be fetched.
-     */
-    public FetchSourceContext fetchSource() {
-        return fetchSourceContext;
-    }
-
-    /**
-     * Adds a field to load and return (note, it must be stored) as part of the
-     * search request. If none are specified, the source of the document will be
-     * return.
+     * Sets no fields to be loaded, resulting in only id and type to be returned
+     * per field.
      */
-    public SearchSourceBuilder field(String name) {
-        if (fieldNames == null) {
-            fieldNames = new ArrayList<>();
-        }
-        fieldNames.add(name);
+    public SearchSourceBuilder noFields() {
+        this.fieldNames = Collections.emptyList();
         return this;
     }
 
@@ -587,19 +553,28 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Sets no fields to be loaded, resulting in only id and type to be returned
-     * per field.
+     * Adds the fields to load and return as part of the search request. If none
+     * are specified, the source of the document will be returned.
      */
-    public SearchSourceBuilder noFields() {
-        this.fieldNames = Collections.emptyList();
+    public SearchSourceBuilder fields(String... fields) {
+        if (fieldNames == null) {
+            fieldNames = new ArrayList<>();
+        }
+        Collections.addAll(fieldNames, fields);
         return this;
     }
 
     /**
-     * Gets the fields to load and return as part of the search request.
+     * Adds a field to load and return (note, it must be stored) as part of the
+     * search request. If none are specified, the source of the document will be
+     * return.
      */
-    public List<String> fields() {
-        return fieldNames;
+    public SearchSourceBuilder field(String name) {
+        if (fieldNames == null) {
+            fieldNames = new ArrayList<>();
+        }
+        fieldNames.add(name);
+        return this;
     }
 
     /**
@@ -615,13 +590,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the field-data fields.
-     */
-    public List<String> fieldDataFields() {
-        return fieldDataFields;
-    }
-
-    /**
      * Adds a script field under the given name with the provided script.
      *
      * @param name
@@ -638,13 +606,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the script fields.
-     */
-    public List<ScriptField> scriptFields() {
-        return scriptFields;
-    }
-
-    /**
      * Sets the boost a specific index will receive when the query is executeed
      * against it.
      *
@@ -662,14 +623,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the boost a specific indices will receive when the query is
-     * executeed against them.
-     */
-    public ObjectFloatHashMap<String> indexBoost() {
-        return indexBoost;
-    }
-
-    /**
      * The stats groups this request will be aggregated under.
      */
     public SearchSourceBuilder stats(String... statsGroups) {
@@ -677,224 +630,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         return this;
     }
 
-    /**
-     * The stats groups this request will be aggregated under.
-     */
-    public String[] stats() {
-        return stats;
-    }
-
-    public SearchSourceBuilder ext(XContentBuilder ext) {
-        this.ext = ext.bytes();
-        return this;
-    }
-
-    public BytesReference ext() {
-        return ext;
-    }
-
-    public SearchSourceBuilder fromXContent(XContentParser parser, QueryParseContext context) throws IOException {
-        SearchSourceBuilder builder = new SearchSourceBuilder();
-        XContentParser.Token token;
-        String currentFieldName = null;
-        if ((token = parser.nextToken()) != XContentParser.Token.START_OBJECT) {
-            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.START_OBJECT + "] but found [" + token + "]",
-                    parser.getTokenLocation());
-        }
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token.isValue()) {
-                if (context.parseFieldMatcher().match(currentFieldName, FROM_FIELD)) {
-                    builder.from = parser.intValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, SIZE_FIELD)) {
-                    builder.size = parser.intValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, TIMEOUT_FIELD)) {
-                    builder.timeoutInMillis = parser.longValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, TERMINATE_AFTER_FIELD)) {
-                    builder.terminateAfter = parser.intValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, MIN_SCORE_FIELD)) {
-                    builder.minScore = parser.floatValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, VERSION_FIELD)) {
-                    builder.version = parser.booleanValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, EXPLAIN_FIELD)) {
-                    builder.explain = parser.booleanValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, TRACK_SCORES_FIELD)) {
-                    builder.trackScores = parser.booleanValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, _SOURCE_FIELD)) {
-                    FetchSourceContext fetchSourceContext = FetchSourceContext.parse(parser, context);
-                    builder.fetchSourceContext = fetchSourceContext;
-                } else if (context.parseFieldMatcher().match(currentFieldName, FIELDS_FIELD)) {
-                    List<String> fieldNames = new ArrayList<>();
-                    fieldNames.add(parser.text());
-                    builder.fieldNames = fieldNames;
-                } else if (context.parseFieldMatcher().match(currentFieldName, SORT_FIELD)) {
-                    builder.sort(parser.text());
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else if (token == XContentParser.Token.START_OBJECT) {
-                if (context.parseFieldMatcher().match(currentFieldName, QUERY_FIELD)) {
-                    builder.queryBuilder = context.parseInnerQueryBuilder();
-                } else if (context.parseFieldMatcher().match(currentFieldName, POST_FILTER_FIELD)) {
-                    builder.postQueryBuilder = context.parseInnerQueryBuilder();
-                } else if (context.parseFieldMatcher().match(currentFieldName, _SOURCE_FIELD)) {
-                    FetchSourceContext fetchSourceContext = FetchSourceContext.parse(parser, context);
-                    builder.fetchSourceContext = fetchSourceContext;
-                } else if (context.parseFieldMatcher().match(currentFieldName, SCRIPT_FIELDS_FIELD)) {
-                    List<ScriptField> scriptFields = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        String scriptFieldName = parser.currentName();
-                        token = parser.nextToken();
-                        if (token == XContentParser.Token.START_OBJECT) {
-                            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                                if (token == XContentParser.Token.FIELD_NAME) {
-                                    currentFieldName = parser.currentName();
-                                } else if (token.isValue()) {
-                                    if (context.parseFieldMatcher().match(currentFieldName, SCRIPT_FIELD)) {
-                                        scriptFields
-                                                .add(new ScriptField(scriptFieldName, Script.parse(parser, context.parseFieldMatcher())));
-                                    } else {
-                                        throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName
-                                                + "].", parser.getTokenLocation());
-                                    }
-                                } else if (token == XContentParser.Token.START_OBJECT) {
-                                    if (context.parseFieldMatcher().match(currentFieldName, SCRIPT_FIELD)) {
-                                        scriptFields
-                                                .add(new ScriptField(scriptFieldName, Script.parse(parser, context.parseFieldMatcher())));
-                                    } else {
-                                        throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName
-                                                + "].", parser.getTokenLocation());
-                                    }
-                                } else {
-                                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName
-                                            + "].", parser.getTokenLocation());
-                                }
-                            }
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.START_OBJECT + "] in ["
-                                    + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
-                    }
-                    builder.scriptFields = scriptFields;
-                } else if (context.parseFieldMatcher().match(currentFieldName, INDICES_BOOST_FIELD)) {
-                    ObjectFloatHashMap<String> indexBoost = new ObjectFloatHashMap<String>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        if (token == XContentParser.Token.FIELD_NAME) {
-                            currentFieldName = parser.currentName();
-                        } else if (token.isValue()) {
-                            indexBoost.put(currentFieldName, parser.floatValue());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                    parser.getTokenLocation());
-                        }
-                    }
-                    builder.indexBoost = indexBoost;
-                } else if (context.parseFieldMatcher().match(currentFieldName, AGGREGATIONS_FIELD)) {
-                    List<BytesReference> aggregations = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        currentFieldName = parser.currentName();
-                        token = parser.nextToken();
-                        if (token == XContentParser.Token.START_OBJECT) {
-                            XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType());
-                            xContentBuilder.startObject();
-                            xContentBuilder.field(currentFieldName);
-                            xContentBuilder.copyCurrentStructure(parser);
-                            xContentBuilder.endObject();
-                            aggregations.add(xContentBuilder.bytes());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                    parser.getTokenLocation());
-                        }
-                    }
-                    builder.aggregations = aggregations;
-                } else if (context.parseFieldMatcher().match(currentFieldName, HIGHLIGHT_FIELD)) {
-                    XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                    builder.highlightBuilder = xContentBuilder.bytes();
-                } else if (context.parseFieldMatcher().match(currentFieldName, INNER_HITS_FIELD)) {
-                    XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                    builder.innerHitsBuilder = xContentBuilder.bytes();
-                } else if (context.parseFieldMatcher().match(currentFieldName, SUGGEST_FIELD)) {
-                    XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType());
-                    xContentBuilder.copyCurrentStructure(parser);
-                    builder.suggestBuilder = xContentBuilder.bytes();
-                } else if (context.parseFieldMatcher().match(currentFieldName, SORT_FIELD)) {
-                    List<BytesReference> sorts = new ArrayList<>();
-                    XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                    sorts.add(xContentBuilder.bytes());
-                    builder.sorts = sorts;
-                } else if (context.parseFieldMatcher().match(currentFieldName, EXT_FIELD)) {
-                    XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                    builder.ext = xContentBuilder.bytes();
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else if (token == XContentParser.Token.START_ARRAY) {
-
-                if (context.parseFieldMatcher().match(currentFieldName, FIELDS_FIELD)) {
-                    List<String> fieldNames = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            fieldNames.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING + "] in ["
-                                    + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
-                    }
-                    builder.fieldNames = fieldNames;
-                } else if (context.parseFieldMatcher().match(currentFieldName, FIELDDATA_FIELDS_FIELD)) {
-                    List<String> fieldDataFields = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            fieldDataFields.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING + "] in ["
-                                    + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
-                    }
-                    builder.fieldDataFields = fieldDataFields;
-                } else if (context.parseFieldMatcher().match(currentFieldName, SORT_FIELD)) {
-                    List<BytesReference> sorts = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                        sorts.add(xContentBuilder.bytes());
-                    }
-                    builder.sorts = sorts;
-                } else if (context.parseFieldMatcher().match(currentFieldName, RESCORE_FIELD)) {
-                    List<BytesReference> rescoreBuilders = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                        rescoreBuilders.add(xContentBuilder.bytes());
-                    }
-                    builder.rescoreBuilders = rescoreBuilders;
-                } else if (context.parseFieldMatcher().match(currentFieldName, STATS_FIELD)) {
-                    List<String> stats = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            stats.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING + "] in ["
-                                    + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
-                    }
-                    builder.stats = stats.toArray(new String[stats.size()]);
-                } else if (context.parseFieldMatcher().match(currentFieldName, _SOURCE_FIELD)) {
-                    FetchSourceContext fetchSourceContext = FetchSourceContext.parse(parser, context);
-                    builder.fetchSourceContext = fetchSourceContext;
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                        parser.getTokenLocation());
-            }
-        }
-        return builder;
-    }
-
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject();
@@ -905,49 +640,65 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
 
     public void innerToXContent(XContentBuilder builder, Params params) throws IOException {
         if (from != -1) {
-            builder.field(FROM_FIELD.getPreferredName(), from);
+            builder.field("from", from);
         }
         if (size != -1) {
-            builder.field(SIZE_FIELD.getPreferredName(), size);
+            builder.field("size", size);
         }
 
         if (timeoutInMillis != -1) {
-            builder.field(TIMEOUT_FIELD.getPreferredName(), timeoutInMillis);
+            builder.field("timeout", timeoutInMillis);
         }
 
         if (terminateAfter != SearchContext.DEFAULT_TERMINATE_AFTER) {
-            builder.field(TERMINATE_AFTER_FIELD.getPreferredName(), terminateAfter);
+            builder.field("terminate_after", terminateAfter);
         }
 
-        if (queryBuilder != null) {
-            builder.field(QUERY_FIELD.getPreferredName(), queryBuilder);
+        if (querySourceBuilder != null) {
+            querySourceBuilder.innerToXContent(builder, params);
         }
 
         if (postQueryBuilder != null) {
-            builder.field(POST_FILTER_FIELD.getPreferredName(), postQueryBuilder);
+            builder.field("post_filter");
+            postQueryBuilder.toXContent(builder, params);
+        }
+
+        if (filterBinary != null) {
+            if (XContentFactory.xContentType(filterBinary) == builder.contentType()) {
+                builder.rawField("filter", filterBinary);
+            } else {
+                builder.field("filter_binary", filterBinary);
+            }
         }
 
         if (minScore != null) {
-            builder.field(MIN_SCORE_FIELD.getPreferredName(), minScore);
+            builder.field("min_score", minScore);
         }
 
         if (version != null) {
-            builder.field(VERSION_FIELD.getPreferredName(), version);
+            builder.field("version", version);
         }
 
         if (explain != null) {
-            builder.field(EXPLAIN_FIELD.getPreferredName(), explain);
+            builder.field("explain", explain);
         }
 
         if (fetchSourceContext != null) {
-            builder.field(_SOURCE_FIELD.getPreferredName(), fetchSourceContext);
+            if (!fetchSourceContext.fetchSource()) {
+                builder.field("_source", false);
+            } else {
+                builder.startObject("_source");
+                builder.array("includes", fetchSourceContext.includes());
+                builder.array("excludes", fetchSourceContext.excludes());
+                builder.endObject();
+            }
         }
 
         if (fieldNames != null) {
             if (fieldNames.size() == 1) {
-                builder.field(FIELDS_FIELD.getPreferredName(), fieldNames.get(0));
+                builder.field("fields", fieldNames.get(0));
             } else {
-                builder.startArray(FIELDS_FIELD.getPreferredName());
+                builder.startArray("fields");
                 for (String fieldName : fieldNames) {
                     builder.value(fieldName);
                 }
@@ -956,15 +707,15 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         }
 
         if (fieldDataFields != null) {
-            builder.startArray(FIELDDATA_FIELDS_FIELD.getPreferredName());
-            for (String fieldDataField : fieldDataFields) {
-                builder.value(fieldDataField);
+            builder.startArray("fielddata_fields");
+            for (String fieldName : fieldDataFields) {
+                builder.value(fieldName);
             }
             builder.endArray();
         }
 
         if (scriptFields != null) {
-            builder.startObject(SCRIPT_FIELDS_FIELD.getPreferredName());
+            builder.startObject("script_fields");
             for (ScriptField scriptField : scriptFields) {
                 builder.startObject(scriptField.fieldName());
                 builder.field("script", scriptField.script());
@@ -974,21 +725,21 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         }
 
         if (sorts != null) {
-            builder.startArray(SORT_FIELD.getPreferredName());
-            for (BytesReference sort : sorts) {
-                XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(sort);
-                parser.nextToken();
-                builder.copyCurrentStructure(parser);
+            builder.startArray("sort");
+            for (SortBuilder sort : sorts) {
+                builder.startObject();
+                sort.toXContent(builder, params);
+                builder.endObject();
             }
             builder.endArray();
         }
 
         if (trackScores) {
-            builder.field(TRACK_SCORES_FIELD.getPreferredName(), true);
+            builder.field("track_scores", true);
         }
 
         if (indexBoost != null) {
-            builder.startObject(INDICES_BOOST_FIELD.getPreferredName());
+            builder.startObject("indices_boost");
             assert !indexBoost.containsKey(null);
             final Object[] keys = indexBoost.keys;
             final float[] values = indexBoost.values;
@@ -1001,64 +752,76 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         }
 
         if (aggregations != null) {
-            builder.field(AGGREGATIONS_FIELD.getPreferredName());
+            builder.field("aggregations");
             builder.startObject();
-            for (BytesReference aggregation : aggregations) {
-                XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(aggregation);
-                parser.nextToken();
-                parser.nextToken();
-                builder.copyCurrentStructure(parser);
+            for (AbstractAggregationBuilder aggregation : aggregations) {
+                aggregation.toXContent(builder, params);
             }
             builder.endObject();
         }
 
+        if (aggregationsBinary != null) {
+            if (XContentFactory.xContentType(aggregationsBinary) == builder.contentType()) {
+                builder.rawField("aggregations", aggregationsBinary);
+            } else {
+                builder.field("aggregations_binary", aggregationsBinary);
+            }
+        }
+
         if (highlightBuilder != null) {
-            builder.field(HIGHLIGHT_FIELD.getPreferredName());
-            XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(highlightBuilder);
-            parser.nextToken();
-            builder.copyCurrentStructure(parser);
+            highlightBuilder.toXContent(builder, params);
         }
 
         if (innerHitsBuilder != null) {
-            builder.field(INNER_HITS_FIELD.getPreferredName());
-            XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(innerHitsBuilder);
-            parser.nextToken();
-            builder.copyCurrentStructure(parser);
+            innerHitsBuilder.toXContent(builder, params);
         }
 
         if (suggestBuilder != null) {
-            builder.field(SUGGEST_FIELD.getPreferredName());
-            XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(suggestBuilder);
-            parser.nextToken();
-            builder.copyCurrentStructure(parser);
+            suggestBuilder.toXContent(builder, params);
         }
 
         if (rescoreBuilders != null) {
-            builder.startArray(RESCORE_FIELD.getPreferredName());
-            for (BytesReference rescoreBuilder : rescoreBuilders) {
-                XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(rescoreBuilder);
-                parser.nextToken();
-                builder.copyCurrentStructure(parser);
+            // Strip empty rescoreBuilders from the request
+            Iterator<RescoreBuilder> itr = rescoreBuilders.iterator();
+            while (itr.hasNext()) {
+                if (itr.next().isEmpty()) {
+                    itr.remove();
+                }
             }
-            builder.endArray();
-        }
 
-        if (stats != null) {
-            builder.array(STATS_FIELD.getPreferredName(), stats);
+            // Now build the request taking care to skip empty lists and only send the object form
+            // if there is just one builder.
+            if (rescoreBuilders.size() == 1) {
+                builder.startObject("rescore");
+                rescoreBuilders.get(0).toXContent(builder, params);
+                if (rescoreBuilders.get(0).windowSize() == null && defaultRescoreWindowSize != null) {
+                    builder.field("window_size", defaultRescoreWindowSize);
+                }
+                builder.endObject();
+            } else if (!rescoreBuilders.isEmpty()) {
+                builder.startArray("rescore");
+                for (RescoreBuilder rescoreBuilder : rescoreBuilders) {
+                    builder.startObject();
+                    rescoreBuilder.toXContent(builder, params);
+                    if (rescoreBuilder.windowSize() == null && defaultRescoreWindowSize != null) {
+                        builder.field("window_size", defaultRescoreWindowSize);
+                    }
+                    builder.endObject();
+                }
+                builder.endArray();
+            }
         }
 
-        if (ext != null) {
-            builder.field(EXT_FIELD.getPreferredName());
-            XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(ext);
-            parser.nextToken();
-            builder.copyCurrentStructure(parser);
+        if (stats != null) {
+            builder.startArray("stats");
+            for (String stat : stats) {
+                builder.value(stat);
+            }
+            builder.endArray();
         }
     }
 
-    public static class ScriptField implements Writeable<ScriptField>, ToXContent {
-
-        public static final ScriptField PROTOTYPE = new ScriptField(null, null);
-
+    private static class ScriptField {
         private final String fieldName;
         private final Script script;
 
@@ -1074,296 +837,5 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         public Script script() {
             return script;
         }
-
-        @Override
-        public ScriptField readFrom(StreamInput in) throws IOException {
-            return new ScriptField(in.readString(), Script.readScript(in));
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeString(fieldName);
-            script.writeTo(out);
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(fieldName);
-            builder.field(SCRIPT_FIELD.getPreferredName(), script);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(fieldName, script);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            ScriptField other = (ScriptField) obj;
-            return Objects.equals(fieldName, other.fieldName) && Objects.equals(script, other.script);
-        }
-    }
-
-    @Override
-    public SearchSourceBuilder readFrom(StreamInput in) throws IOException {
-        SearchSourceBuilder builder = new SearchSourceBuilder();
-        if (in.readBoolean()) {
-            int size = in.readVInt();
-            List<BytesReference> aggregations = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                aggregations.add(in.readBytesReference());
-            }
-            builder.aggregations = aggregations;
-        }
-        if (in.readBoolean()) {
-            builder.defaultRescoreWindowSize = in.readVInt();
-        }
-        builder.explain = in.readOptionalBoolean();
-        builder.fetchSourceContext = FetchSourceContext.optionalReadFromStream(in);
-        boolean hasFieldDataFields = in.readBoolean();
-        if (hasFieldDataFields) {
-            int size = in.readVInt();
-            List<String> fieldDataFields = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                fieldDataFields.add(in.readString());
-            }
-            builder.fieldDataFields = fieldDataFields;
-        }
-        boolean hasFieldNames = in.readBoolean();
-        if (hasFieldNames) {
-            int size = in.readVInt();
-            List<String> fieldNames = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                fieldNames.add(in.readString());
-            }
-            builder.fieldNames = fieldNames;
-        }
-        builder.from = in.readVInt();
-        if (in.readBoolean()) {
-            builder.highlightBuilder = in.readBytesReference();
-        }
-        boolean hasIndexBoost = in.readBoolean();
-        if (hasIndexBoost) {
-            int size = in.readVInt();
-            ObjectFloatHashMap<String> indexBoost = new ObjectFloatHashMap<String>(size);
-            for (int i = 0; i < size; i++) {
-                indexBoost.put(in.readString(), in.readFloat());
-            }
-            builder.indexBoost = indexBoost;
-        }
-        if (in.readBoolean()) {
-            builder.innerHitsBuilder = in.readBytesReference();
-        }
-        if (in.readBoolean()) {
-            builder.minScore = in.readFloat();
-        }
-        if (in.readBoolean()) {
-            builder.postQueryBuilder = in.readQuery();
-        }
-        if (in.readBoolean()) {
-            builder.queryBuilder = in.readQuery();
-        }
-        if (in.readBoolean()) {
-            int size = in.readVInt();
-            List<BytesReference> rescoreBuilders = new ArrayList<>();
-            for (int i = 0; i < size; i++) {
-                rescoreBuilders.add(in.readBytesReference());
-            }
-            builder.rescoreBuilders = rescoreBuilders;
-        }
-        if (in.readBoolean()) {
-            int size = in.readVInt();
-            List<ScriptField> scriptFields = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                scriptFields.add(ScriptField.PROTOTYPE.readFrom(in));
-            }
-            builder.scriptFields = scriptFields;
-        }
-        builder.size = in.readVInt();
-        if (in.readBoolean()) {
-            int size = in.readVInt();
-            List<BytesReference> sorts = new ArrayList<>();
-            for (int i = 0; i < size; i++) {
-                sorts.add(in.readBytesReference());
-            }
-            builder.sorts = sorts;
-        }
-        if (in.readBoolean()) {
-            builder.stats = in.readStringArray();
-        }
-        if (in.readBoolean()) {
-            builder.suggestBuilder = in.readBytesReference();
-        }
-        builder.terminateAfter = in.readVInt();
-        builder.timeoutInMillis = in.readLong();
-        builder.trackScores = in.readBoolean();
-        builder.version = in.readOptionalBoolean();
-        if (in.readBoolean()) {
-            builder.ext = in.readBytesReference();
-        }
-        return builder;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        boolean hasAggregations = aggregations != null;
-        out.writeBoolean(hasAggregations);
-        if (hasAggregations) {
-            out.writeVInt(aggregations.size());
-            for (BytesReference aggregation : aggregations) {
-                out.writeBytesReference(aggregation);
-            }
-        }
-        boolean hasDefaultRescoreWindowSize = defaultRescoreWindowSize != null;
-        out.writeBoolean(hasDefaultRescoreWindowSize);
-        if (hasDefaultRescoreWindowSize) {
-            out.writeVInt(defaultRescoreWindowSize);
-        }
-        out.writeOptionalBoolean(explain);
-        FetchSourceContext.optionalWriteToStream(fetchSourceContext, out);
-        boolean hasFieldDataFields = fieldDataFields != null;
-        out.writeBoolean(hasFieldDataFields);
-        if (hasFieldDataFields) {
-            out.writeVInt(fieldDataFields.size());
-            for (String field : fieldDataFields) {
-                out.writeString(field);
-            }
-        }
-        boolean hasFieldNames = fieldNames != null;
-        out.writeBoolean(hasFieldNames);
-        if (hasFieldNames) {
-            out.writeVInt(fieldNames.size());
-            for (String field : fieldNames) {
-                out.writeString(field);
-            }
-        }
-        out.writeVInt(from);
-        boolean hasHighlightBuilder = highlightBuilder != null;
-        out.writeBoolean(hasHighlightBuilder);
-        if (hasHighlightBuilder) {
-            out.writeBytesReference(highlightBuilder);
-        }
-        boolean hasIndexBoost = indexBoost != null;
-        out.writeBoolean(hasIndexBoost);
-        if (hasIndexBoost) {
-            out.writeVInt(indexBoost.size());
-            for (ObjectCursor<String> key : indexBoost.keys()) {
-                out.writeString(key.value);
-                out.writeFloat(indexBoost.get(key.value));
-            }
-        }
-        boolean hasInnerHitsBuilder = innerHitsBuilder != null;
-        out.writeBoolean(hasInnerHitsBuilder);
-        if (hasInnerHitsBuilder) {
-            out.writeBytesReference(innerHitsBuilder);
-        }
-        boolean hasMinScore = minScore != null;
-        out.writeBoolean(hasMinScore);
-        if (hasMinScore) {
-            out.writeFloat(minScore);
-        }
-        boolean hasPostQuery = postQueryBuilder != null;
-        out.writeBoolean(hasPostQuery);
-        if (hasPostQuery) {
-            out.writeQuery(postQueryBuilder);
-        }
-        boolean hasQuery = queryBuilder != null;
-        out.writeBoolean(hasQuery);
-        if (hasQuery) {
-            out.writeQuery(queryBuilder);
-        }
-        boolean hasRescoreBuilders = rescoreBuilders != null;
-        out.writeBoolean(hasRescoreBuilders);
-        if (hasRescoreBuilders) {
-            out.writeVInt(rescoreBuilders.size());
-            for (BytesReference rescoreBuilder : rescoreBuilders) {
-                out.writeBytesReference(rescoreBuilder);
-            }
-        }
-        boolean hasScriptFields = scriptFields != null;
-        out.writeBoolean(hasScriptFields);
-        if (hasScriptFields) {
-            out.writeVInt(scriptFields.size());
-            for (ScriptField scriptField : scriptFields) {
-                scriptField.writeTo(out);
-            }
-        }
-        out.writeVInt(size);
-        boolean hasSorts = sorts != null;
-        out.writeBoolean(hasSorts);
-        if (hasSorts) {
-            out.writeVInt(sorts.size());
-            for (BytesReference sort : sorts) {
-                out.writeBytesReference(sort);
-            }
-        }
-        boolean hasStats = stats != null;
-        out.writeBoolean(hasStats);
-        if (hasStats) {
-            out.writeStringArray(stats);
-        }
-        boolean hasSuggestBuilder = suggestBuilder != null;
-        out.writeBoolean(hasSuggestBuilder);
-        if (hasSuggestBuilder) {
-            out.writeBytesReference(suggestBuilder);
-        }
-        out.writeVInt(terminateAfter);
-        out.writeLong(timeoutInMillis);
-        out.writeBoolean(trackScores);
-        out.writeOptionalBoolean(version);
-        boolean hasExt = ext != null;
-        out.writeBoolean(hasExt);
-        if (hasExt) {
-            out.writeBytesReference(ext);
-        }
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(aggregations, defaultRescoreWindowSize, explain, fetchSourceContext, fieldDataFields, fieldNames, from,
-                highlightBuilder, indexBoost, innerHitsBuilder, minScore, postQueryBuilder, queryBuilder, rescoreBuilders, scriptFields,
-                size, sorts, Arrays.hashCode(stats), suggestBuilder, terminateAfter, timeoutInMillis, trackScores, version);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (obj.getClass() != getClass()) {
-            return false;
-        }
-        SearchSourceBuilder other = (SearchSourceBuilder) obj;
-        return Objects.equals(aggregations, other.aggregations)
-                && Objects.equals(defaultRescoreWindowSize, other.defaultRescoreWindowSize)
-                && Objects.equals(explain, other.explain)
-                && Objects.equals(fetchSourceContext, other.fetchSourceContext)
-                && Objects.equals(fieldDataFields, other.fieldDataFields)
-                && Objects.equals(fieldNames, other.fieldNames)
-                && Objects.equals(from, other.from)
-                && Objects.equals(highlightBuilder, other.highlightBuilder)
-                && Objects.equals(indexBoost, other.indexBoost)
-                && Objects.equals(innerHitsBuilder, other.innerHitsBuilder)
-                && Objects.equals(minScore, other.minScore)
-                && Objects.equals(postQueryBuilder, other.postQueryBuilder)
-                && Objects.equals(queryBuilder, other.queryBuilder)
-                && Objects.equals(rescoreBuilders, other.rescoreBuilders)
-                && Objects.equals(scriptFields, other.scriptFields)
-                && Objects.equals(size, other.size)
-                && Objects.equals(sorts, other.sorts)
-                && Objects.deepEquals(stats, other.stats)
-                && Objects.equals(suggestBuilder, other.suggestBuilder)
-                && Objects.equals(terminateAfter, other.terminateAfter)
-                && Objects.equals(timeoutInMillis, other.timeoutInMillis)
-                && Objects.equals(trackScores, other.trackScores)
-                && Objects.equals(version, other.version);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseContext.java b/core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseContext.java
index 981408b..237f435 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseContext.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseContext.java
@@ -33,7 +33,7 @@ public class FetchSubPhaseContext {
     /**
      * Set if this phase should be executed at all.
      */
-    public void setHitExecutionNeeded(boolean hitExecutionNeeded) {
+    void setHitExecutionNeeded(boolean hitExecutionNeeded) {
         this.hitExecutionNeeded = hitExecutionNeeded;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java
index 7941e17..a14fdfe 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.search.fetch.innerhits;
 
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
@@ -43,16 +42,12 @@ public class InnerHitsBuilder implements ToXContent {
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject("inner_hits");
-        innerXContent(builder, params);
-        return builder.endObject();
-    }
-
-    public void innerXContent(XContentBuilder builder, Params params) throws IOException {
         for (Map.Entry<String, InnerHitsHolder> entry : innerHits.entrySet()) {
             builder.startObject(entry.getKey());
             entry.getValue().toXContent(builder, params);
             builder.endObject();
         }
+        return builder.endObject();
     }
 
     /**
@@ -266,12 +261,187 @@ public class InnerHitsBuilder implements ToXContent {
             return this;
         }
 
-        public BytesReference highlighter() {
+        public HighlightBuilder highlightBuilder() {
             return sourceBuilder().highlighter();
         }
 
-        public InnerHit highlighter(HighlightBuilder highlightBuilder) {
-            sourceBuilder().highlighter(highlightBuilder);
+        /**
+         * Adds a field to be highlighted with default fragment size of 100 characters, and
+         * default number of fragments of 5.
+         *
+         * @param name The field to highlight
+         */
+        public InnerHit addHighlightedField(String name) {
+            highlightBuilder().field(name);
+            return this;
+        }
+
+
+        /**
+         * Adds a field to be highlighted with a provided fragment size (in characters), and
+         * default number of fragments of 5.
+         *
+         * @param name         The field to highlight
+         * @param fragmentSize The size of a fragment in characters
+         */
+        public InnerHit addHighlightedField(String name, int fragmentSize) {
+            highlightBuilder().field(name, fragmentSize);
+            return this;
+        }
+
+        /**
+         * Adds a field to be highlighted with a provided fragment size (in characters), and
+         * a provided (maximum) number of fragments.
+         *
+         * @param name              The field to highlight
+         * @param fragmentSize      The size of a fragment in characters
+         * @param numberOfFragments The (maximum) number of fragments
+         */
+        public InnerHit addHighlightedField(String name, int fragmentSize, int numberOfFragments) {
+            highlightBuilder().field(name, fragmentSize, numberOfFragments);
+            return this;
+        }
+
+        /**
+         * Adds a field to be highlighted with a provided fragment size (in characters),
+         * a provided (maximum) number of fragments and an offset for the highlight.
+         *
+         * @param name              The field to highlight
+         * @param fragmentSize      The size of a fragment in characters
+         * @param numberOfFragments The (maximum) number of fragments
+         */
+        public InnerHit addHighlightedField(String name, int fragmentSize, int numberOfFragments,
+                                            int fragmentOffset) {
+            highlightBuilder().field(name, fragmentSize, numberOfFragments, fragmentOffset);
+            return this;
+        }
+
+        /**
+         * Adds a highlighted field.
+         */
+        public InnerHit addHighlightedField(HighlightBuilder.Field field) {
+            highlightBuilder().field(field);
+            return this;
+        }
+
+        /**
+         * Set a tag scheme that encapsulates a built in pre and post tags. The allows schemes
+         * are <tt>styled</tt> and <tt>default</tt>.
+         *
+         * @param schemaName The tag scheme name
+         */
+        public InnerHit setHighlighterTagsSchema(String schemaName) {
+            highlightBuilder().tagsSchema(schemaName);
+            return this;
+        }
+
+        public InnerHit setHighlighterFragmentSize(Integer fragmentSize) {
+            highlightBuilder().fragmentSize(fragmentSize);
+            return this;
+        }
+
+        public InnerHit setHighlighterNumOfFragments(Integer numOfFragments) {
+            highlightBuilder().numOfFragments(numOfFragments);
+            return this;
+        }
+
+        public InnerHit setHighlighterFilter(Boolean highlightFilter) {
+            highlightBuilder().highlightFilter(highlightFilter);
+            return this;
+        }
+
+        /**
+         * The encoder to set for highlighting
+         */
+        public InnerHit setHighlighterEncoder(String encoder) {
+            highlightBuilder().encoder(encoder);
+            return this;
+        }
+
+        /**
+         * Explicitly set the pre tags that will be used for highlighting.
+         */
+        public InnerHit setHighlighterPreTags(String... preTags) {
+            highlightBuilder().preTags(preTags);
+            return this;
+        }
+
+        /**
+         * Explicitly set the post tags that will be used for highlighting.
+         */
+        public InnerHit setHighlighterPostTags(String... postTags) {
+            highlightBuilder().postTags(postTags);
+            return this;
+        }
+
+        /**
+         * The order of fragments per field. By default, ordered by the order in the
+         * highlighted text. Can be <tt>score</tt>, which then it will be ordered
+         * by score of the fragments.
+         */
+        public InnerHit setHighlighterOrder(String order) {
+            highlightBuilder().order(order);
+            return this;
+        }
+
+        public InnerHit setHighlighterRequireFieldMatch(boolean requireFieldMatch) {
+            highlightBuilder().requireFieldMatch(requireFieldMatch);
+            return this;
+        }
+
+        public InnerHit setHighlighterBoundaryMaxScan(Integer boundaryMaxScan) {
+            highlightBuilder().boundaryMaxScan(boundaryMaxScan);
+            return this;
+        }
+
+        public InnerHit setHighlighterBoundaryChars(char[] boundaryChars) {
+            highlightBuilder().boundaryChars(boundaryChars);
+            return this;
+        }
+
+        /**
+         * The highlighter type to use.
+         */
+        public InnerHit setHighlighterType(String type) {
+            highlightBuilder().highlighterType(type);
+            return this;
+        }
+
+        public InnerHit setHighlighterFragmenter(String fragmenter) {
+            highlightBuilder().fragmenter(fragmenter);
+            return this;
+        }
+
+        /**
+         * Sets a query to be used for highlighting all fields instead of the search query.
+         */
+        public InnerHit setHighlighterQuery(QueryBuilder highlightQuery) {
+            highlightBuilder().highlightQuery(highlightQuery);
+            return this;
+        }
+
+        /**
+         * Sets the size of the fragment to return from the beginning of the field if there are no matches to
+         * highlight and the field doesn't also define noMatchSize.
+         *
+         * @param noMatchSize integer to set or null to leave out of request.  default is null.
+         * @return this builder for chaining
+         */
+        public InnerHit setHighlighterNoMatchSize(Integer noMatchSize) {
+            highlightBuilder().noMatchSize(noMatchSize);
+            return this;
+        }
+
+        /**
+         * Sets the maximum number of phrases the fvh will consider if the field doesn't also define phraseLimit.
+         */
+        public InnerHit setHighlighterPhraseLimit(Integer phraseLimit) {
+            highlightBuilder().phraseLimit(phraseLimit);
+            return this;
+        }
+
+        public InnerHit setHighlighterOptions(Map<String, Object> options) {
+            highlightBuilder().options(options);
             return this;
         }
 
@@ -290,8 +460,24 @@ public class InnerHitsBuilder implements ToXContent {
             return this;
         }
 
-        public InnerHit innerHits(InnerHitsBuilder innerHitsBuilder) {
-            sourceBuilder().innerHits(innerHitsBuilder);
+
+
+
+        /**
+         * Adds a nested inner hit definition that collects inner hits for hits
+         * on this inner hit level.
+         */
+        public InnerHit addNestedInnerHits(String name, String path, InnerHit innerHit) {
+            sourceBuilder().innerHitsBuilder().addNestedInnerHits(name, path, innerHit);
+            return this;
+        }
+
+        /**
+         * Adds a nested inner hit definition that collects inner hits for hits
+         * on this inner hit level.
+         */
+        public InnerHit addParentChildInnerHits(String name, String type, InnerHit innerHit) {
+            sourceBuilder().innerHitsBuilder().addParentChildInnerHits(name, type, innerHit);
             return this;
         }
 
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java
index 8796532..2824bc1 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java
@@ -79,7 +79,7 @@ public class MatchedQueriesFetchSubPhase implements FetchSubPhase {
         hitContext.hit().matchedQueries(matchedQueries.toArray(new String[matchedQueries.size()]));
     }
 
-    private void addMatchedQueries(HitContext hitContext, ImmutableMap<String, Query> namedQueries, List<String> matchedQueries) throws IOException {
+    private void addMatchedQueries(HitContext hitContext, Map<String, Query> namedQueries, List<String> matchedQueries) throws IOException {
         for (Map.Entry<String, Query> entry : namedQueries.entrySet()) {
             String name = entry.getKey();
             Query filter = entry.getValue();
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceContext.java b/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceContext.java
index ae0a71d..9db7aea 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceContext.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceContext.java
@@ -19,30 +19,20 @@
 
 package org.elasticsearch.search.fetch.source;
 
+import org.elasticsearch.Version;
 import org.elasticsearch.common.Booleans;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.rest.RestRequest;
 
 import java.io.IOException;
-import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.List;
 
 /**
  */
-public class FetchSourceContext implements Streamable, ToXContent {
-
-    public static final ParseField INCLUDES_FIELD = new ParseField("includes", "include");
-    public static final ParseField EXCLUDES_FIELD = new ParseField("excludes", "exclude");
+public class FetchSourceContext implements Streamable {
 
     public static final FetchSourceContext FETCH_SOURCE = new FetchSourceContext(true);
     public static final FetchSourceContext DO_NOT_FETCH_SOURCE = new FetchSourceContext(false);
@@ -51,11 +41,6 @@ public class FetchSourceContext implements Streamable, ToXContent {
     private String[] includes;
     private String[] excludes;
 
-    public static FetchSourceContext parse(XContentParser parser, QueryParseContext context) throws IOException {
-        FetchSourceContext fetchSourceContext = new FetchSourceContext();
-        fetchSourceContext.fromXContent(parser, context);
-        return fetchSourceContext;
-    }
 
     FetchSourceContext() {
 
@@ -187,86 +172,6 @@ public class FetchSourceContext implements Streamable, ToXContent {
         return null;
     }
 
-    public void fromXContent(XContentParser parser, QueryParseContext context) throws IOException {
-        XContentParser.Token token = parser.currentToken();
-        boolean fetchSource = true;
-        String[] includes = Strings.EMPTY_ARRAY;
-        String[] excludes = Strings.EMPTY_ARRAY;
-        if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            fetchSource = parser.booleanValue();
-        } else if (token == XContentParser.Token.VALUE_STRING) {
-            includes = new String[]{parser.text()};
-        } else if (token == XContentParser.Token.START_ARRAY) {
-            ArrayList<String> list = new ArrayList<>();
-            while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                list.add(parser.text());
-            }
-            includes = list.toArray(new String[list.size()]);
-        } else if (token == XContentParser.Token.START_OBJECT) {
-            String currentFieldName = null;
-            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                if (token == XContentParser.Token.FIELD_NAME) {
-                    currentFieldName = parser.currentName();
-                } else if (token == XContentParser.Token.START_ARRAY) {
-                    if (context.parseFieldMatcher().match(currentFieldName, INCLUDES_FIELD)) {
-                        List<String> includesList = new ArrayList<>();
-                        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                            if (token == XContentParser.Token.VALUE_STRING) {
-                                includesList.add(parser.text());
-                            } else {
-                                throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                        parser.getTokenLocation());
-                            }
-                        }
-                        includes = includesList.toArray(new String[includesList.size()]);
-                    } else if (context.parseFieldMatcher().match(currentFieldName, EXCLUDES_FIELD)) {
-                        List<String> excludesList = new ArrayList<>();
-                        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                            if (token == XContentParser.Token.VALUE_STRING) {
-                                excludesList.add(parser.text());
-                            } else {
-                                throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                        parser.getTokenLocation());
-                            }
-                        }
-                        excludes = excludesList.toArray(new String[excludesList.size()]);
-                    } else {
-                        throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                parser.getTokenLocation());
-                    }
-                } else if (token == XContentParser.Token.VALUE_STRING) {
-                    if (context.parseFieldMatcher().match(currentFieldName, INCLUDES_FIELD)) {
-                        includes = new String[] {parser.text()};
-                    } else if (context.parseFieldMatcher().match(currentFieldName, EXCLUDES_FIELD)) {
-                        excludes = new String[] {parser.text()};
-                    }
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            }
-        } else {
-            throw new ParsingException(parser.getTokenLocation(), "Expected one of [" + XContentParser.Token.VALUE_BOOLEAN + ", "
-                    + XContentParser.Token.START_OBJECT + "] but found [" + token + "]", parser.getTokenLocation());
-        }
-        this.fetchSource = fetchSource;
-        this.includes = includes;
-        this.excludes = excludes;
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        if (fetchSource) {
-            builder.startObject();
-            builder.array(INCLUDES_FIELD.getPreferredName(), includes);
-            builder.array(EXCLUDES_FIELD.getPreferredName(), excludes);
-            builder.endObject();
-        } else {
-            builder.value(false);
-        }
-        return builder;
-    }
-
     @Override
     public void readFrom(StreamInput in) throws IOException {
         fetchSource = in.readBoolean();
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java b/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
index 7f1e19b..695598e 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
@@ -227,9 +227,9 @@ public class HighlightBuilder implements ToXContent {
     }
 
     /**
-     * Set to true to cause a field to be highlighted only if a query matches that field.
-     * Default is false meaning that terms are highlighted on all requested fields regardless
-     * if the query matches specifically on them.
+     * Set to true to cause a field to be highlighted only if a query matches that field. 
+     * Default is false meaning that terms are highlighted on all requested fields regardless 
+     * if the query matches specifically on them. 
      */
     public HighlightBuilder requireFieldMatch(boolean requireFieldMatch) {
         this.requireFieldMatch = requireFieldMatch;
@@ -237,7 +237,7 @@ public class HighlightBuilder implements ToXContent {
     }
 
     /**
-     * When using the highlighterType <tt>fast-vector-highlighter</tt> this setting
+     * When using the highlighterType <tt>fast-vector-highlighter</tt> this setting 
      * controls how far to look for boundary characters, and defaults to 20.
      */
     public HighlightBuilder boundaryMaxScan(Integer boundaryMaxScan) {
@@ -246,8 +246,8 @@ public class HighlightBuilder implements ToXContent {
     }
 
     /**
-     * When using the highlighterType <tt>fast-vector-highlighter</tt> this setting
-     * defines what constitutes a boundary for highlighting. Its a single string with
+     * When using the highlighterType <tt>fast-vector-highlighter</tt> this setting 
+     * defines what constitutes a boundary for highlighting. Its a single string with 
      * each boundary character defined in it. It defaults to .,!? \t\n
      */
     public HighlightBuilder boundaryChars(char[] boundaryChars) {
@@ -258,7 +258,7 @@ public class HighlightBuilder implements ToXContent {
     /**
      * Set type of highlighter to use. Supported types
      * are <tt>highlighter</tt>, <tt>fast-vector-highlighter</tt> and <tt>postings-highlighter</tt>.
-     * The default option selected is dependent on the mappings defined for your index.
+     * The default option selected is dependent on the mappings defined for your index. 
      * Details of the different highlighter types are covered in the reference guide.
      */
     public HighlightBuilder highlighterType(String highlighterType) {
@@ -334,13 +334,6 @@ public class HighlightBuilder implements ToXContent {
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject("highlight");
-        innerXContent(builder, params);
-        builder.endObject();
-        return builder;
-    }
-
-
-    public void innerXContent(XContentBuilder builder, Params params) throws IOException {
         if (tagsSchema != null) {
             builder.field("tags_schema", tagsSchema);
         }
@@ -472,6 +465,8 @@ public class HighlightBuilder implements ToXContent {
                 builder.endObject();
             }
         }
+        builder.endObject();
+        return builder;
     }
 
     public static class Field {
diff --git a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java
index 677f392..ca8c074 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java
@@ -31,7 +31,6 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
 
@@ -66,7 +65,9 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
     private Scroll scroll;
     private String[] types = Strings.EMPTY_ARRAY;
     private String[] filteringAliases;
-    private SearchSourceBuilder source;
+    private BytesReference source;
+    private BytesReference extraSource;
+    private BytesReference templateSource;
     private Template template;
     private Boolean requestCache;
     private long nowInMillis;
@@ -78,6 +79,8 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
                             String[] filteringAliases, long nowInMillis) {
         this(shardRouting.shardId(), numberOfShards, searchRequest.searchType(),
                 searchRequest.source(), searchRequest.types(), searchRequest.requestCache());
+        this.extraSource = searchRequest.extraSource();
+        this.templateSource = searchRequest.templateSource();
         this.template = searchRequest.template();
         this.scroll = searchRequest.scroll();
         this.filteringAliases = filteringAliases;
@@ -95,8 +98,8 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
         this.filteringAliases = filteringAliases;
     }
 
-    public ShardSearchLocalRequest(ShardId shardId, int numberOfShards, SearchType searchType, SearchSourceBuilder source, String[] types,
-            Boolean requestCache) {
+    public ShardSearchLocalRequest(ShardId shardId, int numberOfShards, SearchType searchType,
+                                   BytesReference source, String[] types, Boolean requestCache) {
         this.index = shardId.getIndex();
         this.shardId = shardId.id();
         this.numberOfShards = numberOfShards;
@@ -122,16 +125,21 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
     }
 
     @Override
-    public SearchSourceBuilder source() {
+    public BytesReference source() {
         return source;
     }
 
     @Override
-    public void source(SearchSourceBuilder source) {
+    public void source(BytesReference source) {
         this.source = source;
     }
 
     @Override
+    public BytesReference extraSource() {
+        return extraSource;
+    }
+
+    @Override
     public int numberOfShards() {
         return numberOfShards;
     }
@@ -150,12 +158,18 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
     public long nowInMillis() {
         return nowInMillis;
     }
+
     @Override
     public Template template() {
         return template;
     }
 
     @Override
+    public BytesReference templateSource() {
+        return templateSource;
+    }
+
+    @Override
     public Boolean requestCache() {
         return requestCache;
     }
@@ -174,13 +188,18 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
         if (in.readBoolean()) {
             scroll = readScroll(in);
         }
-        if (in.readBoolean()) {
-            source = SearchSourceBuilder.PROTOTYPE.readFrom(in);
-        }
+
+        source = in.readBytesReference();
+        extraSource = in.readBytesReference();
+
         types = in.readStringArray();
         filteringAliases = in.readStringArray();
         nowInMillis = in.readVLong();
-        template = in.readOptionalStreamable(new Template());
+
+        templateSource = in.readBytesReference();
+        if (in.readBoolean()) {
+            template = Template.readTemplate(in);
+        }
         requestCache = in.readOptionalBoolean();
     }
 
@@ -197,20 +216,20 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
             out.writeBoolean(true);
             scroll.writeTo(out);
         }
-        if (source == null) {
-            out.writeBoolean(false);
-        } else {
-            out.writeBoolean(true);
-            source.writeTo(out);
-
-        }
+        out.writeBytesReference(source);
+        out.writeBytesReference(extraSource);
         out.writeStringArray(types);
         out.writeStringArrayNullable(filteringAliases);
         if (!asKey) {
             out.writeVLong(nowInMillis);
         }
 
-        out.writeOptionalStreamable(template);
+        out.writeBytesReference(templateSource);
+        boolean hasTemplate = template != null;
+        out.writeBoolean(hasTemplate);
+        if (hasTemplate) {
+            template.writeTo(out);
+        }
         out.writeOptionalBoolean(requestCache);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java
index fb631b0..6d9734f 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java
@@ -20,11 +20,12 @@
 package org.elasticsearch.search.internal;
 
 import org.elasticsearch.action.search.SearchType;
+import org.elasticsearch.common.HasContext;
 import org.elasticsearch.common.HasContextAndHeaders;
+import org.elasticsearch.common.HasHeaders;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
 
@@ -41,9 +42,11 @@ public interface ShardSearchRequest extends HasContextAndHeaders {
 
     String[] types();
 
-    SearchSourceBuilder source();
+    BytesReference source();
 
-    void source(SearchSourceBuilder source);
+    void source(BytesReference source);
+
+    BytesReference extraSource();
 
     int numberOfShards();
 
@@ -55,6 +58,8 @@ public interface ShardSearchRequest extends HasContextAndHeaders {
 
     Template template();
 
+    BytesReference templateSource();
+
     Boolean requestCache();
 
     Scroll scroll();
diff --git a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java
index 279d9d6..e7b1e2f 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java
@@ -30,7 +30,6 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.transport.TransportRequest;
 
 import java.io.IOException;
@@ -88,16 +87,21 @@ public class ShardSearchTransportRequest extends TransportRequest implements Sha
     }
 
     @Override
-    public SearchSourceBuilder source() {
+    public BytesReference source() {
         return shardSearchLocalRequest.source();
     }
 
     @Override
-    public void source(SearchSourceBuilder source) {
+    public void source(BytesReference source) {
         shardSearchLocalRequest.source(source);
     }
 
     @Override
+    public BytesReference extraSource() {
+        return shardSearchLocalRequest.extraSource();
+    }
+
+    @Override
     public int numberOfShards() {
         return shardSearchLocalRequest.numberOfShards();
     }
@@ -116,12 +120,18 @@ public class ShardSearchTransportRequest extends TransportRequest implements Sha
     public long nowInMillis() {
         return shardSearchLocalRequest.nowInMillis();
     }
+
     @Override
     public Template template() {
         return shardSearchLocalRequest.template();
     }
 
     @Override
+    public BytesReference templateSource() {
+        return shardSearchLocalRequest.templateSource();
+    }
+
+    @Override
     public Boolean requestCache() {
         return shardSearchLocalRequest.requestCache();
     }
diff --git a/core/src/main/java/org/elasticsearch/search/lookup/LeafSearchLookup.java b/core/src/main/java/org/elasticsearch/search/lookup/LeafSearchLookup.java
index 6f976a5..5e41154 100644
--- a/core/src/main/java/org/elasticsearch/search/lookup/LeafSearchLookup.java
+++ b/core/src/main/java/org/elasticsearch/search/lookup/LeafSearchLookup.java
@@ -19,12 +19,13 @@
 
 package org.elasticsearch.search.lookup;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.apache.lucene.index.LeafReaderContext;
 
+import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * Per-segment version of {@link SearchLookup}.
  */
@@ -35,7 +36,7 @@ public class LeafSearchLookup {
     final SourceLookup sourceLookup;
     final LeafFieldsLookup fieldsLookup;
     final LeafIndexLookup indexLookup;
-    final ImmutableMap<String, Object> asMap;
+    final Map<String, Object> asMap;
 
     public LeafSearchLookup(LeafReaderContext ctx, LeafDocLookup docMap, SourceLookup sourceLookup,
             LeafFieldsLookup fieldsLookup, LeafIndexLookup indexLookup, Map<String, Object> topLevelMap) {
@@ -45,17 +46,17 @@ public class LeafSearchLookup {
         this.fieldsLookup = fieldsLookup;
         this.indexLookup = indexLookup;
 
-        ImmutableMap.Builder<String, Object> builder = ImmutableMap.builder();
-        builder.putAll(topLevelMap);
-        builder.put("doc", docMap);
-        builder.put("_doc", docMap);
-        builder.put("_source", sourceLookup);
-        builder.put("_fields", fieldsLookup);
-        builder.put("_index", indexLookup);
-        asMap = builder.build();
+        Map<String, Object> asMap = new HashMap<>(topLevelMap.size() + 5);
+        asMap.putAll(topLevelMap);
+        asMap.put("doc", docMap);
+        asMap.put("_doc", docMap);
+        asMap.put("_source", sourceLookup);
+        asMap.put("_fields", fieldsLookup);
+        asMap.put("_index", indexLookup);
+        this.asMap = unmodifiableMap(asMap);
     }
 
-    public ImmutableMap<String, Object> asMap() {
+    public Map<String, Object> asMap() {
         return this.asMap;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java b/core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java
index 76b2e82..6e881cb 100644
--- a/core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java
+++ b/core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.search.warmer;
 
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.action.support.ToXContentToBytes;
 import org.elasticsearch.cluster.AbstractDiffable;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.Nullable;
@@ -29,13 +27,12 @@ import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.util.ByteArray;
-import org.elasticsearch.common.xcontent.*;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.XContentType;
 
-import java.io.ByteArrayOutputStream;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -70,10 +67,10 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
     public static class Entry {
         private final String name;
         private final String[] types;
-        private final SearchSource source;
+        private final BytesReference source;
         private final Boolean requestCache;
 
-        public Entry(String name, String[] types, Boolean requestCache, SearchSource source) {
+        public Entry(String name, String[] types, Boolean requestCache, BytesReference source) {
             this.name = name;
             this.types = types == null ? Strings.EMPTY_ARRAY : types;
             this.source = source;
@@ -89,7 +86,7 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
         }
 
         @Nullable
-        public SearchSource source() {
+        public BytesReference source() {
             return this.source;
         }
 
@@ -144,9 +141,9 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
         for (int i = 0; i < entries.length; i++) {
             String name = in.readString();
             String[] types = in.readStringArray();
-            SearchSource source = null;
+            BytesReference source = null;
             if (in.readBoolean()) {
-                source = new SearchSource(in);
+                source = in.readBytesReference();
             }
             Boolean queryCache;
             queryCache = in.readOptionalBoolean();
@@ -165,7 +162,7 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
                 out.writeBoolean(false);
             } else {
                 out.writeBoolean(true);
-                entry.source.writeTo(out);
+                out.writeBytesReference(entry.source());
             }
             out.writeOptionalBoolean(entry.requestCache());
         }
@@ -197,7 +194,7 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
             } else if (token == XContentParser.Token.START_OBJECT) {
                 String name = currentFieldName;
                 List<String> types = new ArrayList<>(2);
-                SearchSource source = null;
+                BytesReference source = null;
                 Boolean queryCache = null;
                 while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                     if (token == XContentParser.Token.FIELD_NAME) {
@@ -210,15 +207,12 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
                         }
                     } else if (token == XContentParser.Token.START_OBJECT) {
                         if ("source".equals(currentFieldName)) {
-                            ByteArrayOutputStream out = new ByteArrayOutputStream();
-                            try (XContentGenerator generator = XContentType.JSON.xContent().createGenerator(out)) {
-                                generator.copyCurrentStructure(parser);
-                            }
-                            source = new SearchSource(new BytesArray(out.toByteArray()));
+                            XContentBuilder builder = XContentFactory.jsonBuilder().map(parser.mapOrdered());
+                            source = builder.bytes();
                         }
                     } else if (token == XContentParser.Token.VALUE_EMBEDDED_OBJECT) {
                         if ("source".equals(currentFieldName)) {
-                            source = new SearchSource(new BytesArray(parser.binaryValue()));
+                            source = new BytesArray(parser.binaryValue());
                         }
                     } else if (token.isValue()) {
                         if ("requestCache".equals(currentFieldName) || "request_cache".equals(currentFieldName)) {
@@ -245,12 +239,22 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
     }
 
     public static void toXContent(Entry entry, XContentBuilder builder, ToXContent.Params params) throws IOException {
+        boolean binary = params.paramAsBoolean("binary", false);
         builder.startObject(entry.name(), XContentBuilder.FieldCaseConversion.NONE);
         builder.field("types", entry.types());
         if (entry.requestCache() != null) {
             builder.field("requestCache", entry.requestCache());
         }
-        builder.field("source", entry.source());
+        builder.field("source");
+        if (binary) {
+            builder.value(entry.source());
+        } else {
+            Map<String, Object> mapping;
+            try (XContentParser parser = XContentFactory.xContent(entry.source()).createParser(entry.source())) {
+                mapping = parser.mapOrdered();
+            }
+            builder.map(mapping);
+        }
         builder.endObject();
     }
 
@@ -273,75 +277,4 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
         }
         return new IndexWarmersMetaData(entries.toArray(new Entry[entries.size()]));
     }
-
-    public static class SearchSource extends ToXContentToBytes implements Writeable<SearchSource> {
-        private final BytesReference binary;
-        private SearchSourceBuilder cached;
-
-        public SearchSource(BytesReference bytesArray) {
-            this.binary = bytesArray;
-        }
-
-        public SearchSource(StreamInput input) throws IOException {
-            this(input.readBytesReference());
-        }
-
-        public SearchSource(SearchSourceBuilder source) {
-            try (XContentBuilder builder = XContentBuilder.builder(XContentType.JSON.xContent())) {
-                source.toXContent(builder, ToXContent.EMPTY_PARAMS);
-                binary = builder.bytes();
-            } catch (IOException ex) {
-                throw new ElasticsearchException("failed to generate XContent", ex);
-            }
-        }
-
-        public SearchSourceBuilder build(QueryParseContext ctx) throws IOException {
-            if (cached == null) {
-                try (XContentParser parser = XContentFactory.xContent(binary).createParser(binary)) {
-                    ctx.reset(parser);
-                    cached = SearchSourceBuilder.PROTOTYPE.fromXContent(parser, ctx);
-                }
-            }
-            return cached;
-        }
-
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            if (binary == null) {
-                cached.toXContent(builder, params);
-            } else {
-                try (XContentParser parser = XContentFactory.xContent(binary).createParser(binary)) {
-                    builder.copyCurrentStructure(parser);
-                }
-            }
-            return builder;
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeBytesReference(binary);
-        }
-
-        @Override
-        public SearchSource readFrom(StreamInput in) throws IOException {
-            return new SearchSource(in);
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
-
-            SearchSource that = (SearchSource) o;
-
-            return binary.equals(that.binary);
-
-        }
-
-        @Override
-        public int hashCode() {
-            return binary.hashCode();
-        }
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java b/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
index 5692100..1b49da5 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
@@ -27,28 +27,11 @@ import com.google.common.collect.ImmutableSet;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.support.IndicesOptions;
-import org.elasticsearch.cluster.ClusterChangedEvent;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ClusterStateListener;
-import org.elasticsearch.cluster.ProcessedClusterStateUpdateTask;
-import org.elasticsearch.cluster.RestoreInProgress;
+import org.elasticsearch.cluster.*;
 import org.elasticsearch.cluster.RestoreInProgress.ShardRestoreStatus;
-import org.elasticsearch.cluster.TimeoutClusterStateUpdateTask;
 import org.elasticsearch.cluster.block.ClusterBlocks;
-import org.elasticsearch.cluster.metadata.AliasMetaData;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.metadata.MetaDataCreateIndexService;
-import org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService;
-import org.elasticsearch.cluster.metadata.RepositoriesMetaData;
-import org.elasticsearch.cluster.metadata.SnapshotId;
-import org.elasticsearch.cluster.routing.IndexRoutingTable;
-import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
-import org.elasticsearch.cluster.routing.RestoreSource;
-import org.elasticsearch.cluster.routing.RoutingTable;
-import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.metadata.*;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.cluster.routing.allocation.AllocationService;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
 import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
@@ -70,36 +53,15 @@ import org.elasticsearch.index.shard.StoreRecoveryService;
 import org.elasticsearch.repositories.RepositoriesService;
 import org.elasticsearch.repositories.Repository;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.EmptyTransportResponseHandler;
-import org.elasticsearch.transport.TransportChannel;
-import org.elasticsearch.transport.TransportRequest;
-import org.elasticsearch.transport.TransportRequestHandler;
-import org.elasticsearch.transport.TransportResponse;
-import org.elasticsearch.transport.TransportService;
+import org.elasticsearch.transport.*;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 import java.util.Map.Entry;
-import java.util.Set;
 import java.util.concurrent.BlockingQueue;
 import java.util.concurrent.CopyOnWriteArrayList;
 
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_AUTO_EXPAND_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_CREATION_DATE;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_INDEX_UUID;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_LEGACY_ROUTING_HASH_FUNCTION;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_LEGACY_ROUTING_USE_TYPE;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_VERSION_CREATED;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_VERSION_MINIMUM_COMPATIBLE;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_VERSION_UPGRADED;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.*;
 import static org.elasticsearch.cluster.metadata.MetaDataIndexStateService.INDEX_CLOSED_BLOCK;
 
 /**
@@ -211,7 +173,7 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
 
             // Now we can start the actual restore process by adding shards to be recovered in the cluster state
             // and updating cluster metadata (global and index) as needed
-            clusterService.submitStateUpdateTask(request.cause(), new TimeoutClusterStateUpdateTask() {
+            clusterService.submitStateUpdateTask(request.cause(), new ClusterStateUpdateTask() {
                 RestoreInfo restoreInfo = null;
 
                 @Override
@@ -525,7 +487,7 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
         logger.trace("received updated snapshot restore state [{}]", request);
         updatedSnapshotStateQueue.add(request);
 
-        clusterService.submitStateUpdateTask("update snapshot state", new ProcessedClusterStateUpdateTask() {
+        clusterService.submitStateUpdateTask("update snapshot state", new ClusterStateUpdateTask() {
             private final List<UpdateIndexShardRestoreStatusRequest> drainedRequests = new ArrayList<>();
             private Map<SnapshotId, Tuple<RestoreInfo, Map<ShardId, ShardRestoreStatus>>> batchedRestoreInfo = null;
 
diff --git a/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java b/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java
index 58a98c3..3502916 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java
@@ -91,7 +91,7 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
 
     private final Condition shutdownCondition = shutdownLock.newCondition();
 
-    private volatile ImmutableMap<SnapshotId, SnapshotShards> shardSnapshots = ImmutableMap.of();
+    private volatile Map<SnapshotId, SnapshotShards> shardSnapshots = ImmutableMap.of();
 
     private final BlockingQueue<UpdateIndexShardSnapshotStatusRequest> updatedSnapshotStateQueue = ConcurrentCollections.newBlockingQueue();
 
@@ -368,7 +368,7 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
             if (snapshot.state() == SnapshotsInProgress.State.STARTED || snapshot.state() == SnapshotsInProgress.State.ABORTED) {
                 Map<ShardId, IndexShardSnapshotStatus> localShards = currentSnapshotShards(snapshot.snapshotId());
                 if (localShards != null) {
-                    ImmutableMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> masterShards = snapshot.shards();
+                    Map<ShardId, SnapshotsInProgress.ShardSnapshotStatus> masterShards = snapshot.shards();
                     for(Map.Entry<ShardId, IndexShardSnapshotStatus> localShard : localShards.entrySet()) {
                         ShardId shardId = localShard.getKey();
                         IndexShardSnapshotStatus localShardStatus = localShard.getValue();
diff --git a/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java b/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java
index 3cb02a4..d89d260 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java
@@ -24,21 +24,10 @@ import org.apache.lucene.util.CollectionUtil;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.search.ShardSearchFailure;
 import org.elasticsearch.action.support.IndicesOptions;
-import org.elasticsearch.cluster.ClusterChangedEvent;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ClusterStateListener;
-import org.elasticsearch.cluster.ClusterStateUpdateTask;
-import org.elasticsearch.cluster.ProcessedClusterStateUpdateTask;
-import org.elasticsearch.cluster.SnapshotsInProgress;
+import org.elasticsearch.cluster.*;
 import org.elasticsearch.cluster.SnapshotsInProgress.ShardSnapshotStatus;
 import org.elasticsearch.cluster.SnapshotsInProgress.State;
-import org.elasticsearch.cluster.TimeoutClusterStateUpdateTask;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.metadata.RepositoriesMetaData;
-import org.elasticsearch.cluster.metadata.SnapshotId;
+import org.elasticsearch.cluster.metadata.*;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.IndexRoutingTable;
@@ -61,14 +50,7 @@ import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Set;
+import java.util.*;
 import java.util.concurrent.CopyOnWriteArrayList;
 
 import static org.elasticsearch.cluster.SnapshotsInProgress.completed;
@@ -182,7 +164,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
     public void createSnapshot(final SnapshotRequest request, final CreateSnapshotListener listener) {
         final SnapshotId snapshotId = new SnapshotId(request.repository(), request.name());
         validate(snapshotId);
-        clusterService.submitStateUpdateTask(request.cause(), new TimeoutClusterStateUpdateTask() {
+        clusterService.submitStateUpdateTask(request.cause(), new ClusterStateUpdateTask() {
 
             private SnapshotsInProgress.Entry newSnapshot = null;
 
@@ -303,7 +285,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                 endSnapshot(snapshot);
                 return;
             }
-            clusterService.submitStateUpdateTask("update_snapshot [" + snapshot.snapshotId().getSnapshot() + "]", new ProcessedClusterStateUpdateTask() {
+            clusterService.submitStateUpdateTask("update_snapshot [" + snapshot.snapshotId().getSnapshot() + "]", new ClusterStateUpdateTask() {
                 boolean accepted = false;
                 SnapshotsInProgress.Entry updatedSnapshot;
                 String failure = null;
@@ -614,7 +596,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                         for (final SnapshotsInProgress.Entry snapshot : snapshots.entries()) {
                             SnapshotsInProgress.Entry updatedSnapshot = snapshot;
                             if (snapshot.state() == State.STARTED) {
-                                ImmutableMap<ShardId, ShardSnapshotStatus> shards = processWaitingShards(snapshot.shards(), routingTable);
+                                Map<ShardId, ShardSnapshotStatus> shards = processWaitingShards(snapshot.shards(), routingTable);
                                 if (shards != null) {
                                     changed = true;
                                     if (!snapshot.state().completed() && completed(shards.values())) {
@@ -643,7 +625,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
         }
     }
 
-    private ImmutableMap<ShardId, ShardSnapshotStatus> processWaitingShards(ImmutableMap<ShardId, ShardSnapshotStatus> snapshotShards, RoutingTable routingTable) {
+    private Map<ShardId, ShardSnapshotStatus> processWaitingShards(Map<ShardId, ShardSnapshotStatus> snapshotShards, RoutingTable routingTable) {
         boolean snapshotChanged = false;
         ImmutableMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableMap.builder();
         for (ImmutableMap.Entry<ShardId, ShardSnapshotStatus> shardEntry : snapshotShards.entrySet()) {
@@ -734,10 +716,10 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
      * @param shards list of shard statuses
      * @return list of failed and closed indices
      */
-    private Tuple<Set<String>, Set<String>> indicesWithMissingShards(ImmutableMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards, MetaData metaData) {
+    private Tuple<Set<String>, Set<String>> indicesWithMissingShards(Map<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards, MetaData metaData) {
         Set<String> missing = new HashSet<>();
         Set<String> closed = new HashSet<>();
-        for (ImmutableMap.Entry<ShardId, SnapshotsInProgress.ShardSnapshotStatus> entry : shards.entrySet()) {
+        for (Map.Entry<ShardId, SnapshotsInProgress.ShardSnapshotStatus> entry : shards.entrySet()) {
             if (entry.getValue().state() == State.MISSING) {
                 if (metaData.hasIndex(entry.getKey().getIndex()) && metaData.index(entry.getKey().getIndex()).getState() == IndexMetaData.State.CLOSE) {
                     closed.add(entry.getKey().getIndex());
@@ -805,7 +787,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
      * @param t          exception if snapshot failed
      */
     private void removeSnapshotFromClusterState(final SnapshotId snapshotId, final SnapshotInfo snapshot, final Throwable t) {
-        clusterService.submitStateUpdateTask("remove snapshot metadata", new ProcessedClusterStateUpdateTask() {
+        clusterService.submitStateUpdateTask("remove snapshot metadata", new ClusterStateUpdateTask() {
             @Override
             public ClusterState execute(ClusterState currentState) {
                 SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);
@@ -860,7 +842,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
      */
     public void deleteSnapshot(final SnapshotId snapshotId, final DeleteSnapshotListener listener) {
         validate(snapshotId);
-        clusterService.submitStateUpdateTask("delete snapshot", new ProcessedClusterStateUpdateTask() {
+        clusterService.submitStateUpdateTask("delete snapshot", new ClusterStateUpdateTask() {
 
             boolean waitForSnapshot = false;
 
@@ -882,7 +864,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                 } else {
                     // This snapshot is currently running - stopping shards first
                     waitForSnapshot = true;
-                    ImmutableMap<ShardId, ShardSnapshotStatus> shards;
+                    Map<ShardId, ShardSnapshotStatus> shards;
                     if (snapshot.state() == State.STARTED && snapshot.shards() != null) {
                         // snapshot is currently running - stop started shards
                         ImmutableMap.Builder<ShardId, ShardSnapshotStatus> shardsBuilder = ImmutableMap.builder();
diff --git a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
index 77b86a0..85de5f6 100644
--- a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
+++ b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
@@ -91,7 +91,7 @@ public class ThreadPool extends AbstractComponent {
 
     private volatile ImmutableMap<String, ExecutorHolder> executors;
 
-    private final ImmutableMap<String, Settings> defaultExecutorTypeSettings;
+    private final Map<String, Settings> defaultExecutorTypeSettings;
 
     private final Queue<ExecutorHolder> retiredExecutors = new ConcurrentLinkedQueue<>();
 
@@ -511,8 +511,8 @@ public class ThreadPool extends AbstractComponent {
         public void run() {
             try {
                 runnable.run();
-            } catch (Exception e) {
-                logger.warn("failed to run {}", e, runnable.toString());
+            } catch (Throwable t) {
+                logger.warn("failed to run {}", t, runnable.toString());
             }
         }
 
diff --git a/core/src/main/java/org/elasticsearch/transport/TransportInfo.java b/core/src/main/java/org/elasticsearch/transport/TransportInfo.java
index ebaf3e6..4b0da47 100644
--- a/core/src/main/java/org/elasticsearch/transport/TransportInfo.java
+++ b/core/src/main/java/org/elasticsearch/transport/TransportInfo.java
@@ -58,13 +58,13 @@ public class TransportInfo implements Streamable, ToXContent {
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject(Fields.TRANSPORT);
-        builder.field(Fields.BOUND_ADDRESS, address.boundAddress().toString());
+        builder.array(Fields.BOUND_ADDRESS, (Object[]) address.boundAddresses());
         builder.field(Fields.PUBLISH_ADDRESS, address.publishAddress().toString());
         builder.startObject(Fields.PROFILES);
         if (profileAddresses != null && profileAddresses.size() > 0) {
             for (Map.Entry<String, BoundTransportAddress> entry : profileAddresses.entrySet()) {
                 builder.startObject(entry.getKey());
-                builder.field(Fields.BOUND_ADDRESS, entry.getValue().boundAddress().toString());
+                builder.array(Fields.BOUND_ADDRESS, (Object[]) entry.getValue().boundAddresses());
                 builder.field(Fields.PUBLISH_ADDRESS, entry.getValue().publishAddress().toString());
                 builder.endObject();
             }
diff --git a/core/src/main/java/org/elasticsearch/transport/TransportService.java b/core/src/main/java/org/elasticsearch/transport/TransportService.java
index 172a34e..1d24853 100644
--- a/core/src/main/java/org/elasticsearch/transport/TransportService.java
+++ b/core/src/main/java/org/elasticsearch/transport/TransportService.java
@@ -67,7 +67,7 @@ public class TransportService extends AbstractLifecycleComponent<TransportServic
     protected final Transport transport;
     protected final ThreadPool threadPool;
 
-    volatile ImmutableMap<String, RequestHandlerRegistry> requestHandlers = ImmutableMap.of();
+    volatile Map<String, RequestHandlerRegistry> requestHandlers = Collections.emptyMap();
     final Object requestHandlerMutex = new Object();
 
     final ConcurrentMapLong<RequestHolder> clientHandlers = ConcurrentCollections.newConcurrentMapLongWithAggressiveConcurrency();
@@ -171,6 +171,9 @@ public class TransportService extends AbstractLifecycleComponent<TransportServic
         transport.start();
         if (transport.boundAddress() != null && logger.isInfoEnabled()) {
             logger.info("{}", transport.boundAddress());
+            for (Map.Entry<String, BoundTransportAddress> entry : transport.profileBoundAddresses().entrySet()) {
+                logger.info("profile [{}]: {}", entry.getKey(), entry.getValue());
+            }
         }
         boolean setStarted = started.compareAndSet(false, true);
         assert setStarted : "service was already started";
diff --git a/core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java b/core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java
index 179a5db..8d2eb15 100644
--- a/core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java
+++ b/core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java
@@ -37,27 +37,11 @@ import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.common.util.concurrent.AbstractRunnable;
 import org.elasticsearch.common.util.concurrent.EsExecutors;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.ActionNotFoundTransportException;
-import org.elasticsearch.transport.ConnectTransportException;
-import org.elasticsearch.transport.NodeNotConnectedException;
-import org.elasticsearch.transport.RemoteTransportException;
-import org.elasticsearch.transport.RequestHandlerRegistry;
-import org.elasticsearch.transport.ResponseHandlerFailureTransportException;
-import org.elasticsearch.transport.Transport;
-import org.elasticsearch.transport.TransportException;
-import org.elasticsearch.transport.TransportRequest;
-import org.elasticsearch.transport.TransportRequestOptions;
-import org.elasticsearch.transport.TransportResponse;
-import org.elasticsearch.transport.TransportResponseHandler;
-import org.elasticsearch.transport.TransportSerializationException;
-import org.elasticsearch.transport.TransportServiceAdapter;
-import org.elasticsearch.transport.Transports;
+import org.elasticsearch.transport.*;
 import org.elasticsearch.transport.support.TransportStatus;
 
 import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.ThreadFactory;
 import java.util.concurrent.ThreadPoolExecutor;
@@ -81,7 +65,7 @@ public class LocalTransport extends AbstractLifecycleComponent<Transport> implem
     private final static ConcurrentMap<LocalTransportAddress, LocalTransport> transports = newConcurrentMap();
     private static final AtomicLong transportAddressIdGenerator = new AtomicLong();
     private final ConcurrentMap<DiscoveryNode, LocalTransport> connectedNodes = newConcurrentMap();
-    protected final NamedWriteableRegistry namedWriteableRegistry;
+    private final NamedWriteableRegistry namedWriteableRegistry;
 
     public static final String TRANSPORT_LOCAL_ADDRESS = "transport.local.address";
     public static final String TRANSPORT_LOCAL_WORKERS = "transport.local.workers";
@@ -121,7 +105,7 @@ public class LocalTransport extends AbstractLifecycleComponent<Transport> implem
         if (previous != null) {
             throw new ElasticsearchException("local address [" + address + "] is already bound");
         }
-        boundAddress = new BoundTransportAddress(localAddress, localAddress);
+        boundAddress = new BoundTransportAddress(new TransportAddress[] { localAddress }, localAddress);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/transport/local/LocalTransportChannel.java b/core/src/main/java/org/elasticsearch/transport/local/LocalTransportChannel.java
index dd753e6..b15add5 100644
--- a/core/src/main/java/org/elasticsearch/transport/local/LocalTransportChannel.java
+++ b/core/src/main/java/org/elasticsearch/transport/local/LocalTransportChannel.java
@@ -89,7 +89,7 @@ public class LocalTransportChannel implements TransportChannel {
     public void sendResponse(Throwable error) throws IOException {
         BytesStreamOutput stream = new BytesStreamOutput();
         writeResponseExceptionHeader(stream);
-        RemoteTransportException tx = new RemoteTransportException(targetTransport.nodeName(), targetTransport.boundAddress().boundAddress(), action, error);
+        RemoteTransportException tx = new RemoteTransportException(targetTransport.nodeName(), targetTransport.boundAddress().boundAddresses()[0], action, error);
         stream.writeThrowable(tx);
 
         final byte[] data = stream.bytes().toBytes();
diff --git a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
index 059590b..5cf9ed3 100644
--- a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
+++ b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
@@ -329,12 +329,6 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
                     createServerBootstrap(name, mergedSettings);
                     bindServerBootstrap(name, mergedSettings);
                 }
-
-                InetSocketAddress boundAddress = (InetSocketAddress) serverChannels.get(DEFAULT_PROFILE).get(0).getLocalAddress();
-                int publishPort = settings.getAsInt("transport.netty.publish_port", settings.getAsInt("transport.publish_port", boundAddress.getPort()));
-                String publishHost = settings.get("transport.netty.publish_host", settings.get("transport.publish_host", settings.get("transport.host")));
-                InetSocketAddress publishAddress = createPublishAddress(publishHost, publishPort);
-                this.boundAddress = new BoundTransportAddress(new InetSocketTransportAddress(boundAddress), new InetSocketTransportAddress(publishAddress));
             }
             success = true;
         } finally {
@@ -460,9 +454,9 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
         }
     }
         
-    private void bindServerBootstrap(final String name, final InetAddress hostAddress, Settings settings) {
+    private void bindServerBootstrap(final String name, final InetAddress hostAddress, Settings profileSettings) {
 
-        String port = settings.get("port");
+        String port = profileSettings.get("port");
         PortsRange portsRange = new PortsRange(port);
         final AtomicReference<Exception> lastException = new AtomicReference<>();
         final AtomicReference<InetSocketAddress> boundSocket = new AtomicReference<>();
@@ -478,7 +472,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
                             serverChannels.put(name, list);
                         }
                         list.add(channel);
-                        boundSocket.set((InetSocketAddress)channel.getLocalAddress());
+                        boundSocket.set((InetSocketAddress) channel.getLocalAddress());
                     }
                 } catch (Exception e) {
                     lastException.set(e);
@@ -491,16 +485,48 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
             throw new BindTransportException("Failed to bind to [" + port + "]", lastException.get());
         }
 
+        InetSocketAddress boundAddress = boundSocket.get();
+        // TODO: We can remove the special casing for the default profile and store it in the profile map to reduce the complexity here
         if (!DEFAULT_PROFILE.equals(name)) {
-            InetSocketAddress boundAddress = boundSocket.get();
-            int publishPort = settings.getAsInt("publish_port", boundAddress.getPort());
-            String publishHost = settings.get("publish_host", boundAddress.getHostString());
-            InetSocketAddress publishAddress = createPublishAddress(publishHost, publishPort);
-            // TODO: support real multihoming with publishing. Today we use putIfAbsent so only the prioritized address is published
-            profileBoundAddresses.putIfAbsent(name, new BoundTransportAddress(new InetSocketTransportAddress(boundAddress), new InetSocketTransportAddress(publishAddress)));
+            // check to see if an address is already bound for this profile
+            BoundTransportAddress boundTransportAddress = profileBoundAddresses().get(name);
+            if (boundTransportAddress == null) {
+                // no address is bound, so lets create one with the publish address information from the settings or the bound address as a fallback
+                int publishPort = profileSettings.getAsInt("publish_port", boundAddress.getPort());
+                String publishHost = profileSettings.get("publish_host", boundAddress.getHostString());
+                InetSocketAddress publishAddress = createPublishAddress(publishHost, publishPort);
+                profileBoundAddresses.put(name, new BoundTransportAddress(new TransportAddress[]{new InetSocketTransportAddress(boundAddress)}, new InetSocketTransportAddress(publishAddress)));
+            } else {
+                // TODO: support real multihoming with publishing. Today we update the bound addresses so only the prioritized address is published
+                // an address already exists. add the new bound address to the end of a new array and create a new BoundTransportAddress with the array and existing publish address
+                // the new bound address is appended in order to preserve the ordering/priority of bound addresses
+                TransportAddress[] existingBoundAddress = boundTransportAddress.boundAddresses();
+                TransportAddress[] updatedBoundAddresses = Arrays.copyOf(existingBoundAddress, existingBoundAddress.length + 1);
+                updatedBoundAddresses[updatedBoundAddresses.length - 1] = new InetSocketTransportAddress(boundAddress);
+                profileBoundAddresses.put(name, new BoundTransportAddress(updatedBoundAddresses, boundTransportAddress.publishAddress()));
+            }
+        } else {
+            if (this.boundAddress == null) {
+                // this is the first address that has been bound for the default profile so we get the publish address information and create a new BoundTransportAddress
+                // these calls are different from the profile ones due to the way the settings for a profile are created. If we want to merge the code for the default profile and
+                // other profiles together, we need to change how the profileSettings are built for the default profile...
+                int publishPort = settings.getAsInt("transport.netty.publish_port", settings.getAsInt("transport.publish_port", boundAddress.getPort()));
+                String publishHost = settings.get("transport.netty.publish_host", settings.get("transport.publish_host", settings.get("transport.host")));
+                InetSocketAddress publishAddress = createPublishAddress(publishHost, publishPort);
+                this.boundAddress = new BoundTransportAddress(new TransportAddress[]{new InetSocketTransportAddress(boundAddress)}, new InetSocketTransportAddress(publishAddress));
+            } else {
+                // the default profile is already bound to one address and has the publish address, copy the existing bound addresses as is and append the new address.
+                // the new bound address is appended in order to preserve the ordering/priority of bound addresses
+                TransportAddress[] existingBoundAddress = this.boundAddress.boundAddresses();
+                TransportAddress[] updatedBoundAddresses = Arrays.copyOf(existingBoundAddress, existingBoundAddress.length + 1);
+                updatedBoundAddresses[updatedBoundAddresses.length - 1] = new InetSocketTransportAddress(boundAddress);
+                this.boundAddress = new BoundTransportAddress(updatedBoundAddresses, this.boundAddress.publishAddress());
+            }
         }
 
-        logger.info("Bound profile [{}] to address {{}}", name, NetworkAddress.format(boundSocket.get()));
+        if (logger.isDebugEnabled()) {
+            logger.debug("Bound profile [{}] to address {{}}", name, NetworkAddress.format(boundSocket.get()));
+        }
     }
 
     private void createServerBootstrap(String name, Settings settings) {
diff --git a/core/src/main/java/org/elasticsearch/tribe/TribeService.java b/core/src/main/java/org/elasticsearch/tribe/TribeService.java
index 35617dd..36ea7fc 100644
--- a/core/src/main/java/org/elasticsearch/tribe/TribeService.java
+++ b/core/src/main/java/org/elasticsearch/tribe/TribeService.java
@@ -22,11 +22,7 @@ package org.elasticsearch.tribe;
 import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.support.master.TransportMasterNodeReadAction;
-import org.elasticsearch.cluster.ClusterChangedEvent;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ClusterStateListener;
-import org.elasticsearch.cluster.ClusterStateNonMasterUpdateTask;
+import org.elasticsearch.cluster.*;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.block.ClusterBlocks;
@@ -50,11 +46,7 @@ import org.elasticsearch.node.NodeBuilder;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.elasticsearch.rest.RestStatus;
 
-import java.util.EnumSet;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
+import java.util.*;
 import java.util.concurrent.CopyOnWriteArrayList;
 
 /**
@@ -217,7 +209,12 @@ public class TribeService extends AbstractLifecycleComponent<TribeService> {
         @Override
         public void clusterChanged(final ClusterChangedEvent event) {
             logger.debug("[{}] received cluster event, [{}]", tribeName, event.source());
-            clusterService.submitStateUpdateTask("cluster event from " + tribeName + ", " + event.source(), new ClusterStateNonMasterUpdateTask() {
+            clusterService.submitStateUpdateTask("cluster event from " + tribeName + ", " + event.source(), new ClusterStateUpdateTask() {
+                @Override
+                public boolean runOnlyOnMaster() {
+                    return false;
+                }
+
                 @Override
                 public ClusterState execute(ClusterState currentState) throws Exception {
                     ClusterState tribeState = event.state();
diff --git a/core/src/main/resources/org/elasticsearch/bootstrap/elasticsearch-start.help b/core/src/main/resources/org/elasticsearch/bootstrap/elasticsearch-start.help
index 5d20a49..9b27a8d 100644
--- a/core/src/main/resources/org/elasticsearch/bootstrap/elasticsearch-start.help
+++ b/core/src/main/resources/org/elasticsearch/bootstrap/elasticsearch-start.help
@@ -1,6 +1,6 @@
 NAME
 
-    start - start Elasticsearcion
+    start - Start Elasticsearch
 
 SYNOPSIS
 
@@ -24,3 +24,5 @@ OPTIONS
     --property=value             Configures an elasticsearch specific property, like --network.host 127.0.0.1
     --property value
 
+    NOTE: The -d, -p, and -D arguments must appear before any --property arguments.
+
diff --git a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
index 5ad6323..7831a7d 100644
--- a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
+++ b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
@@ -43,6 +43,7 @@ OFFICIAL PLUGINS
     - discovery-azure
     - discovery-ec2
     - discovery-multicast
+    - lang-expression
     - lang-javascript
     - lang-python
     - mapper-murmur3
diff --git a/core/src/test/java/org/elasticsearch/action/count/CountRequestBuilderTests.java b/core/src/test/java/org/elasticsearch/action/count/CountRequestBuilderTests.java
index 4bf5e7f..5d77247 100644
--- a/core/src/test/java/org/elasticsearch/action/count/CountRequestBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/action/count/CountRequestBuilderTests.java
@@ -28,10 +28,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.index.query.MatchAllQueryBuilder;
-import org.elasticsearch.index.query.MatchQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
@@ -78,14 +75,58 @@ public class CountRequestBuilderTests extends ESTestCase {
     @Test
     public void testStringQueryToString() {
         CountRequestBuilder countRequestBuilder = client.prepareCount();
-        countRequestBuilder.setQuery(new MatchAllQueryBuilder());
-        assertThat(countRequestBuilder.toString(), containsString("match_all"));
+        String query = "{ \"match_all\" : {} }";
+        countRequestBuilder.setQuery(new BytesArray(query));
+        assertThat(countRequestBuilder.toString(), containsString("\"query\":{ \"match_all\" : {} }"));
+    }
+
+    @Test
+    public void testXContentBuilderQueryToString() throws IOException {
+        CountRequestBuilder countRequestBuilder = client.prepareCount();
+        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));
+        xContentBuilder.startObject();
+        xContentBuilder.startObject("match_all");
+        xContentBuilder.endObject();
+        xContentBuilder.endObject();
+        countRequestBuilder.setQuery(xContentBuilder);
+        assertThat(countRequestBuilder.toString(), equalTo(new QuerySourceBuilder().setQuery(xContentBuilder.bytes()).toString()));
     }
 
     @Test
     public void testStringSourceToString() {
         CountRequestBuilder countRequestBuilder = client.prepareCount();
-        countRequestBuilder.setSource(new SearchSourceBuilder().query(new MatchAllQueryBuilder()));
-        assertThat(countRequestBuilder.toString(), containsString("match_all"));
+        String query = "{ \"query\": { \"match_all\" : {} } }";
+        countRequestBuilder.setSource(new BytesArray(query));
+        assertThat(countRequestBuilder.toString(), equalTo("{ \"query\": { \"match_all\" : {} } }"));
+    }
+
+    @Test
+    public void testXContentBuilderSourceToString() throws IOException {
+        CountRequestBuilder countRequestBuilder = client.prepareCount();
+        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));
+        xContentBuilder.startObject();
+        xContentBuilder.startObject("match_all");
+        xContentBuilder.endObject();
+        xContentBuilder.endObject();
+        countRequestBuilder.setSource(xContentBuilder.bytes());
+        assertThat(countRequestBuilder.toString(), equalTo(XContentHelper.convertToJson(xContentBuilder.bytes(), false, true)));
+    }
+
+    @Test
+    public void testThatToStringDoesntWipeSource() {
+        String source = "{\n" +
+                "            \"query\" : {\n" +
+                "            \"match\" : {\n" +
+                "                \"field\" : {\n" +
+                "                    \"query\" : \"value\"" +
+                "                }\n" +
+                "            }\n" +
+                "        }\n" +
+                "        }";
+        CountRequestBuilder countRequestBuilder = client.prepareCount().setSource(new BytesArray(source));
+        String preToString = countRequestBuilder.request().source().toUtf8();
+        assertThat(countRequestBuilder.toString(), equalTo(source));
+        String postToString = countRequestBuilder.request().source().toUtf8();
+        assertThat(preToString, equalTo(postToString));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/action/count/CountRequestTests.java b/core/src/test/java/org/elasticsearch/action/count/CountRequestTests.java
index cbf7822..407cfba 100644
--- a/core/src/test/java/org/elasticsearch/action/count/CountRequestTests.java
+++ b/core/src/test/java/org/elasticsearch/action/count/CountRequestTests.java
@@ -21,11 +21,15 @@ package org.elasticsearch.action.count;
 
 import org.elasticsearch.action.search.SearchRequest;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
+import java.util.Map;
+
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.notNullValue;
 import static org.hamcrest.CoreMatchers.nullValue;
@@ -52,9 +56,8 @@ public class CountRequestTests extends ESTestCase {
         if (randomBoolean()) {
             countRequest.preference(randomAsciiOfLengthBetween(1, 10));
         }
-        final boolean querySet;
-        if (querySet = randomBoolean()) {
-            countRequest.query(QueryBuilders.termQuery("field", "value"));
+        if (randomBoolean()) {
+            countRequest.source(new QuerySourceBuilder().setQuery(QueryBuilders.termQuery("field", "value")));
         }
         if (randomBoolean()) {
             countRequest.minScore(randomFloat());
@@ -69,19 +72,31 @@ public class CountRequestTests extends ESTestCase {
         assertThat(searchRequest.types(), equalTo(countRequest.types()));
         assertThat(searchRequest.routing(), equalTo(countRequest.routing()));
         assertThat(searchRequest.preference(), equalTo(countRequest.preference()));
-        SearchSourceBuilder source = searchRequest.source();
-        assertThat(source.size(), equalTo(0));
-        if (querySet) {
-            assertThat(source.query(), notNullValue());
+
+        if (countRequest.source() == null) {
+            assertThat(searchRequest.source(), nullValue());
         } else {
-            assertNull(source.query());
+            Map<String, Object> sourceMap = XContentHelper.convertToMap(searchRequest.source(), false).v2();
+            assertThat(sourceMap.size(), equalTo(1));
+            assertThat(sourceMap.get("query"), notNullValue());
         }
+
+        Map<String, Object> extraSourceMap = XContentHelper.convertToMap(searchRequest.extraSource(), false).v2();
+        int count = 1;
+        assertThat((Integer)extraSourceMap.get("size"), equalTo(0));
         if (countRequest.minScore() == CountRequest.DEFAULT_MIN_SCORE) {
-            assertThat(source.minScore(), nullValue());
+            assertThat(extraSourceMap.get("min_score"), nullValue());
+        } else {
+            assertThat(((Number)extraSourceMap.get("min_score")).floatValue(), equalTo(countRequest.minScore()));
+            count++;
+        }
+        if (countRequest.terminateAfter() == SearchContext.DEFAULT_TERMINATE_AFTER) {
+            assertThat(extraSourceMap.get("terminate_after"), nullValue());
         } else {
-            assertThat(source.minScore(), equalTo(countRequest.minScore()));
+            assertThat((Integer)extraSourceMap.get("terminate_after"), equalTo(countRequest.terminateAfter()));
+            count++;
         }
-        assertThat(source.terminateAfter(), equalTo(countRequest.terminateAfter()));
+        assertThat(extraSourceMap.size(), equalTo(count));
     }
 
     private static String[] randomStringArray() {
diff --git a/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java b/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java
index b07ba2f..5fd9bae 100644
--- a/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java
+++ b/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java
@@ -20,13 +20,6 @@
 package org.elasticsearch.action.search;
 
 import org.elasticsearch.action.support.IndicesOptions;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.MatchAllQueryParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.rest.action.search.RestMultiSearchAction;
-import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.test.StreamsUtils;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -35,7 +28,6 @@ import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.Collections;
 
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.nullValue;
@@ -44,9 +36,8 @@ public class MultiSearchRequestTests extends ESTestCase {
 
     @Test
     public void simpleAdd() throws Exception {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, Collections.singleton(new MatchAllQueryParser()), new NamedWriteableRegistry());
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch1.json");
-        MultiSearchRequest request = RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), false, null, null, null, null, IndicesOptions.strictExpandOpenAndForbidClosed(),true, registry);
+        MultiSearchRequest request = new MultiSearchRequest().add(data, 0, data.length, false, null, null, null);
         assertThat(request.requests().size(), equalTo(8));
         assertThat(request.requests().get(0).indices()[0], equalTo("test"));
         assertThat(request.requests().get(0).indicesOptions(), equalTo(IndicesOptions.fromOptions(true, true, true, true, IndicesOptions.strictExpandOpenAndForbidClosed())));
@@ -71,9 +62,8 @@ public class MultiSearchRequestTests extends ESTestCase {
 
     @Test
     public void simpleAdd2() throws Exception {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, Collections.singleton(new MatchAllQueryParser()), new NamedWriteableRegistry());
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch2.json");
-        MultiSearchRequest request =RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), false, null, null, null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry);
+        MultiSearchRequest request = new MultiSearchRequest().add(data, 0, data.length, false, null, null, null);
         assertThat(request.requests().size(), equalTo(5));
         assertThat(request.requests().get(0).indices()[0], equalTo("test"));
         assertThat(request.requests().get(0).types().length, equalTo(0));
@@ -90,9 +80,8 @@ public class MultiSearchRequestTests extends ESTestCase {
 
     @Test
     public void simpleAdd3() throws Exception {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, Collections.singleton(new MatchAllQueryParser()), new NamedWriteableRegistry());
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch3.json");
-        MultiSearchRequest request =RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), false, null, null, null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry);
+        MultiSearchRequest request = new MultiSearchRequest().add(data, 0, data.length, false, null, null, null);
         assertThat(request.requests().size(), equalTo(4));
         assertThat(request.requests().get(0).indices()[0], equalTo("test0"));
         assertThat(request.requests().get(0).indices()[1], equalTo("test1"));
@@ -110,9 +99,8 @@ public class MultiSearchRequestTests extends ESTestCase {
 
     @Test
     public void simpleAdd4() throws Exception {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, Collections.singleton(new MatchAllQueryParser()), new NamedWriteableRegistry());
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch4.json");
-        MultiSearchRequest request = RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), false, null, null, null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry);
+        MultiSearchRequest request = new MultiSearchRequest().add(data, 0, data.length, false, null, null, null);
         assertThat(request.requests().size(), equalTo(3));
         assertThat(request.requests().get(0).indices()[0], equalTo("test0"));
         assertThat(request.requests().get(0).indices()[1], equalTo("test1"));
@@ -132,9 +120,8 @@ public class MultiSearchRequestTests extends ESTestCase {
 
     @Test
     public void simpleAdd5() throws Exception {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, Collections.singleton(new MatchAllQueryParser()), new NamedWriteableRegistry());
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch5.json");
-        MultiSearchRequest request = RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), true, null, null, null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry);
+        MultiSearchRequest request = new MultiSearchRequest().add(data, 0, data.length, true, null, null, null);
         assertThat(request.requests().size(), equalTo(3));
         assertThat(request.requests().get(0).indices()[0], equalTo("test0"));
         assertThat(request.requests().get(0).indices()[1], equalTo("test1"));
@@ -150,18 +137,6 @@ public class MultiSearchRequestTests extends ESTestCase {
         assertThat(request.requests().get(2).types()[0], equalTo("type2"));
         assertThat(request.requests().get(2).types()[1], equalTo("type1"));
         assertThat(request.requests().get(2).routing(), equalTo("123"));
-        assertNotNull(request.requests().get(0).template());
-        assertNotNull(request.requests().get(1).template());
-        assertNotNull(request.requests().get(2).template());
-        assertEquals(ScriptService.ScriptType.INLINE, request.requests().get(0).template().getType());
-        assertEquals(ScriptService.ScriptType.INLINE, request.requests().get(1).template().getType());
-        assertEquals(ScriptService.ScriptType.INLINE, request.requests().get(2).template().getType());
-        assertEquals("{\"query\":{\"match_{{template}}\":{}}}", request.requests().get(0).template().getScript());
-        assertEquals("{\"query\":{\"match_{{template}}\":{}}}", request.requests().get(1).template().getScript());
-        assertEquals("{\"query\":{\"match_{{template}}\":{}}}", request.requests().get(2).template().getScript());
-        assertEquals(1, request.requests().get(0).template().getParams().size());
-        assertEquals(1, request.requests().get(1).template().getParams().size());
-        assertEquals(1, request.requests().get(2).template().getParams().size());
     }
 
     public void testResponseErrorToXContent() throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java b/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java
index a2f9669..1a05794 100644
--- a/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java
@@ -19,10 +19,14 @@
 
 package org.elasticsearch.action.search;
 
-import org.apache.lucene.util.LuceneTestCase.AwaitsFix;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.transport.TransportClient;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESTestCase;
@@ -30,9 +34,11 @@ import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
+import java.io.IOException;
+
+import static org.hamcrest.CoreMatchers.containsString;
 import static org.hamcrest.CoreMatchers.equalTo;
 
-@AwaitsFix(bugUrl = "fix NOCOMMITs in code below")
 public class SearchRequestBuilderTests extends ESTestCase {
 
     private static Client client;
@@ -65,4 +71,64 @@ public class SearchRequestBuilderTests extends ESTestCase {
         searchRequestBuilder.setQuery(QueryBuilders.matchAllQuery());
         assertThat(searchRequestBuilder.toString(), equalTo(new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).toString()));
     }
+
+    @Test
+    public void testXContentBuilderQueryToString() throws IOException {
+        SearchRequestBuilder searchRequestBuilder = client.prepareSearch();
+        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));
+        xContentBuilder.startObject();
+        xContentBuilder.startObject("match_all");
+        xContentBuilder.endObject();
+        xContentBuilder.endObject();
+        searchRequestBuilder.setQuery(xContentBuilder);
+        assertThat(searchRequestBuilder.toString(), equalTo(new SearchSourceBuilder().query(xContentBuilder).toString()));
+    }
+
+    @Test
+    public void testStringQueryToString() {
+        SearchRequestBuilder searchRequestBuilder = client.prepareSearch();
+        String query = "{ \"match_all\" : {} }";
+        searchRequestBuilder.setQuery(query);
+        assertThat(searchRequestBuilder.toString(), containsString("\"query\":{ \"match_all\" : {} }"));
+    }
+
+    @Test
+    public void testStringSourceToString() {
+        SearchRequestBuilder searchRequestBuilder = client.prepareSearch();
+        String source = "{ \"query\" : { \"match_all\" : {} } }";
+        searchRequestBuilder.setSource(new BytesArray(source));
+        assertThat(searchRequestBuilder.toString(), equalTo(source));
+    }
+
+    @Test
+    public void testXContentBuilderSourceToString() throws IOException {
+        SearchRequestBuilder searchRequestBuilder = client.prepareSearch();
+        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));
+        xContentBuilder.startObject();
+        xContentBuilder.startObject("query");
+        xContentBuilder.startObject("match_all");
+        xContentBuilder.endObject();
+        xContentBuilder.endObject();
+        xContentBuilder.endObject();
+        searchRequestBuilder.setSource(xContentBuilder.bytes());
+        assertThat(searchRequestBuilder.toString(), equalTo(XContentHelper.convertToJson(xContentBuilder.bytes(), false, true)));
+    }
+
+    @Test
+    public void testThatToStringDoesntWipeRequestSource() {
+        String source = "{\n" +
+                "            \"query\" : {\n" +
+                "            \"match\" : {\n" +
+                "                \"field\" : {\n" +
+                "                    \"query\" : \"value\"" +
+                "                }\n" +
+                "            }\n" +
+                "        }\n" +
+                "        }";
+        SearchRequestBuilder searchRequestBuilder = client.prepareSearch().setSource(new BytesArray(source));
+        String preToString = searchRequestBuilder.request().source().toUtf8();
+        assertThat(searchRequestBuilder.toString(), equalTo(source));
+        String postToString = searchRequestBuilder.request().source().toUtf8();
+        assertThat(preToString, equalTo(postToString));
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/benchmark/common/recycler/RecyclerBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/common/recycler/RecyclerBenchmark.java
index 97113a6..9710605 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/common/recycler/RecyclerBenchmark.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/common/recycler/RecyclerBenchmark.java
@@ -19,17 +19,22 @@
 
 package org.elasticsearch.benchmark.common.recycler;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.recycler.AbstractRecyclerC;
 import org.elasticsearch.common.recycler.Recycler;
 
+import java.util.HashMap;
 import java.util.Map;
 import java.util.Random;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicLong;
 
-import static org.elasticsearch.common.recycler.Recyclers.*;
+import static org.elasticsearch.common.recycler.Recyclers.concurrent;
+import static org.elasticsearch.common.recycler.Recyclers.concurrentDeque;
+import static org.elasticsearch.common.recycler.Recyclers.deque;
+import static org.elasticsearch.common.recycler.Recyclers.dequeFactory;
+import static org.elasticsearch.common.recycler.Recyclers.locked;
+import static org.elasticsearch.common.recycler.Recyclers.none;
 
 /** Benchmark that tries to measure the overhead of object recycling depending on concurrent access. */
 public class RecyclerBenchmark {
@@ -89,11 +94,11 @@ public class RecyclerBenchmark {
             }
         };
 
-        final ImmutableMap<String, Recycler<Object>> recyclers = ImmutableMap.<String, Recycler<Object>>builder()
-                .put("none", none(c))
-                .put("concurrent-queue", concurrentDeque(c, limit))
-                .put("locked", locked(deque(c, limit)))
-                .put("concurrent", concurrent(dequeFactory(c, limit), Runtime.getRuntime().availableProcessors())).build();
+        Map<String, Recycler<Object>> recyclers = new HashMap<>();
+        recyclers.put("none", none(c));
+        recyclers.put("concurrent-queue", concurrentDeque(c, limit));
+        recyclers.put("locked", locked(deque(c, limit)));
+        recyclers.put("concurrent", concurrent(dequeFactory(c, limit), Runtime.getRuntime().availableProcessors()));
 
         // warmup
         final long start = System.nanoTime();
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java b/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java
index 89e176f..213a522 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java
@@ -32,7 +32,6 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.node.Node;
 import org.elasticsearch.search.suggest.Suggest.Suggestion.Entry.Option;
-import org.elasticsearch.search.suggest.SuggestBuilder;
 import org.elasticsearch.search.suggest.SuggestBuilders;
 
 import java.io.IOException;
@@ -42,9 +41,7 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.prefixQuery;
+import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
 
 /**
@@ -121,9 +118,7 @@ public class SuggestSearchBenchMark {
             String term = "prefix" + startChar;
             SearchResponse response = client.prepareSearch()
                     .setQuery(prefixQuery("field", term))
-                    .suggest(
-                            new SuggestBuilder().addSuggestion(SuggestBuilders.termSuggestion("field").field("field").text(term)
-                                    .suggestMode("always")))
+                    .addSuggestion(SuggestBuilders.termSuggestion("field").field("field").text(term).suggestMode("always"))
                     .execute().actionGet();
             if (response.getHits().totalHits() == 0) {
                 System.err.println("No hits");
@@ -140,9 +135,7 @@ public class SuggestSearchBenchMark {
             String term = "prefix" + startChar;
             SearchResponse response = client.prepareSearch()
                     .setQuery(matchQuery("field", term))
-                    .suggest(
-                            new SuggestBuilder().addSuggestion(SuggestBuilders.termSuggestion("field").text(term).field("field")
-                                    .suggestMode("always")))
+                    .addSuggestion(SuggestBuilders.termSuggestion("field").text(term).field("field").suggestMode("always"))
                     .execute().actionGet();
             timeTaken += response.getTookInMillis();
             if (response.getSuggest() == null) {
diff --git a/core/src/test/java/org/elasticsearch/bootstrap/BootstrapCliParserTests.java b/core/src/test/java/org/elasticsearch/bootstrap/BootstrapCliParserTests.java
index da30874..8b68034 100644
--- a/core/src/test/java/org/elasticsearch/bootstrap/BootstrapCliParserTests.java
+++ b/core/src/test/java/org/elasticsearch/bootstrap/BootstrapCliParserTests.java
@@ -220,6 +220,16 @@ public class BootstrapCliParserTests extends CliToolTestCase {
 
     }
 
+    public void testThatHelpfulErrorMessageIsGivenWhenParametersAreOutOfOrder() throws Exception {
+        BootstrapCLIParser parser = new BootstrapCLIParser(terminal);
+        try {
+            parser.parse("start", new String[]{"--foo=bar", "-Dbaz=qux"});
+            fail("expected IllegalArgumentException for out-of-order parameters");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), containsString("must be before any parameters starting with --"));
+        }
+    }
+
     private void registerProperties(String ... systemProperties) {
         propertiesToClear.addAll(Arrays.asList(systemProperties));
     }
diff --git a/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java b/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java
index 0f9664c..3dcd8ff 100644
--- a/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java
+++ b/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java
@@ -52,7 +52,7 @@ public class BootstrapForTesting {
 
     static {
         // just like bootstrap, initialize natives, then SM
-        Bootstrap.initializeNatives(true, true);
+        Bootstrap.initializeNatives(true, true, true);
 
         // initialize probes
         Bootstrap.initializeProbes();
diff --git a/core/src/test/java/org/elasticsearch/bootstrap/SeccompTests.java b/core/src/test/java/org/elasticsearch/bootstrap/SeccompTests.java
new file mode 100644
index 0000000..3fbaf5a
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/bootstrap/SeccompTests.java
@@ -0,0 +1,73 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.bootstrap;
+
+import org.elasticsearch.test.ESTestCase;
+
+/** Simple tests seccomp filter is working. */
+public class SeccompTests extends ESTestCase {
+    
+    @Override
+    public void setUp() throws Exception {
+        super.setUp();
+        assumeTrue("requires seccomp filter installation", Natives.isSeccompInstalled());
+        // otherwise security manager will block the execution, no fun
+        assumeTrue("cannot test with security manager enabled", System.getSecurityManager() == null);
+    }
+    
+    public void testNoExecution() throws Exception {
+        try {
+            Runtime.getRuntime().exec("ls");
+            fail("should not have been able to execute!");
+        } catch (Exception expected) {
+            // we can't guarantee how its converted, currently its an IOException, like this:
+            /*
+            java.io.IOException: Cannot run program "ls": error=13, Permission denied
+                    at __randomizedtesting.SeedInfo.seed([65E6C4BED11899E:FC6E1CA6AA2DB634]:0)
+                    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
+                    at java.lang.Runtime.exec(Runtime.java:620)
+                    ...
+                  Caused by: java.io.IOException: error=13, Permission denied
+                    at java.lang.UNIXProcess.forkAndExec(Native Method)
+                    at java.lang.UNIXProcess.<init>(UNIXProcess.java:248)
+                    at java.lang.ProcessImpl.start(ProcessImpl.java:134)
+                    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
+                    ... 
+            */
+        }
+    }
+    
+    // make sure thread inherits this too (its documented that way)
+    public void testNoExecutionFromThread() throws Exception {
+        Thread t = new Thread() {
+            @Override
+            public void run() {
+                try {
+                    Runtime.getRuntime().exec("ls");
+                    fail("should not have been able to execute!");
+                } catch (Exception expected) {
+                    // ok
+                }
+            }
+        };
+        t.start();
+        t.join();
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/broadcast/BroadcastActionsIT.java b/core/src/test/java/org/elasticsearch/broadcast/BroadcastActionsIT.java
index 78ca44b..e2da702 100644
--- a/core/src/test/java/org/elasticsearch/broadcast/BroadcastActionsIT.java
+++ b/core/src/test/java/org/elasticsearch/broadcast/BroadcastActionsIT.java
@@ -68,6 +68,15 @@ public class BroadcastActionsIT extends ESIntegTestCase {
             assertThat(countResponse.getSuccessfulShards(), equalTo(numShards.numPrimaries));
             assertThat(countResponse.getFailedShards(), equalTo(0));
         }
+
+        for (int i = 0; i < 5; i++) {
+            // test failed (simply query that can't be parsed)
+            try {
+                client().count(countRequest("test").source("{ term : { _type : \"type1 } }".getBytes(StandardCharsets.UTF_8))).actionGet();
+            } catch(SearchPhaseExecutionException e) {
+                assertThat(e.shardFailures().length, equalTo(numShards.numPrimaries));
+            }
+        }
     }
 
     private XContentBuilder source(String id, String nameValue) throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java b/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java
index 72327e4..aab7e89 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java
@@ -33,6 +33,7 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.Singleton;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.discovery.zen.ZenDiscovery;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
@@ -42,12 +43,7 @@ import org.elasticsearch.test.junit.annotations.TestLogging;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.junit.Test;
 
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
+import java.util.*;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
@@ -55,11 +51,7 @@ import java.util.concurrent.atomic.AtomicBoolean;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.test.ESIntegTestCase.Scope;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.greaterThanOrEqualTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.*;
 
 /**
  *
@@ -100,7 +92,7 @@ public class ClusterServiceIT extends ESIntegTestCase {
 
         final CountDownLatch timedOut = new CountDownLatch(1);
         final AtomicBoolean executeCalled = new AtomicBoolean();
-        clusterService1.submitStateUpdateTask("test2", new TimeoutClusterStateUpdateTask() {
+        clusterService1.submitStateUpdateTask("test2", new ClusterStateUpdateTask() {
             @Override
             public TimeValue timeout() {
                 return TimeValue.timeValueMillis(2);
@@ -325,7 +317,12 @@ public class ClusterServiceIT extends ESIntegTestCase {
 
         taskFailed[0] = true;
         final CountDownLatch latch2 = new CountDownLatch(1);
-        clusterService.submitStateUpdateTask("test", new ClusterStateNonMasterUpdateTask() {
+        clusterService.submitStateUpdateTask("test", new ClusterStateUpdateTask() {
+            @Override
+            public boolean runOnlyOnMaster() {
+                return false;
+            }
+
             @Override
             public ClusterState execute(ClusterState currentState) throws Exception {
                 taskFailed[0] = false;
@@ -525,7 +522,7 @@ public class ClusterServiceIT extends ESIntegTestCase {
         invoked1.await();
         final CountDownLatch invoked2 = new CountDownLatch(9);
         for (int i = 2; i <= 10; i++) {
-            clusterService.submitStateUpdateTask(Integer.toString(i), new ProcessedClusterStateUpdateTask() {
+            clusterService.submitStateUpdateTask(Integer.toString(i), new ClusterStateUpdateTask() {
                 @Override
                 public ClusterState execute(ClusterState currentState) {
                     return currentState;
@@ -634,7 +631,7 @@ public class ClusterServiceIT extends ESIntegTestCase {
         Settings settings = settingsBuilder()
                 .put("discovery.type", "zen")
                 .put("discovery.zen.minimum_master_nodes", 1)
-                .put("discovery.zen.ping_timeout", "400ms")
+                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "400ms")
                 .put("discovery.initial_state_timeout", "500ms")
                 .build();
 
@@ -758,7 +755,7 @@ public class ClusterServiceIT extends ESIntegTestCase {
         rootLogger.addAppender(mockAppender);
         try {
             final CountDownLatch latch = new CountDownLatch(4);
-            clusterService1.submitStateUpdateTask("test1", new ProcessedClusterStateUpdateTask() {
+            clusterService1.submitStateUpdateTask("test1", new ClusterStateUpdateTask() {
                 @Override
                 public ClusterState execute(ClusterState currentState) throws Exception {
                     return currentState;
@@ -774,7 +771,7 @@ public class ClusterServiceIT extends ESIntegTestCase {
                     fail();
                 }
             });
-            clusterService1.submitStateUpdateTask("test2", new ProcessedClusterStateUpdateTask() {
+            clusterService1.submitStateUpdateTask("test2", new ClusterStateUpdateTask() {
                 @Override
                 public ClusterState execute(ClusterState currentState) {
                     throw new IllegalArgumentException("Testing handling of exceptions in the cluster state task");
@@ -790,7 +787,7 @@ public class ClusterServiceIT extends ESIntegTestCase {
                     latch.countDown();
                 }
             });
-            clusterService1.submitStateUpdateTask("test3", new ProcessedClusterStateUpdateTask() {
+            clusterService1.submitStateUpdateTask("test3", new ClusterStateUpdateTask() {
                 @Override
                 public ClusterState execute(ClusterState currentState) {
                     return ClusterState.builder(currentState).incrementVersion().build();
@@ -808,7 +805,7 @@ public class ClusterServiceIT extends ESIntegTestCase {
             });
             // Additional update task to make sure all previous logging made it to the logger
             // We don't check logging for this on since there is no guarantee that it will occur before our check
-            clusterService1.submitStateUpdateTask("test4", new ProcessedClusterStateUpdateTask() {
+            clusterService1.submitStateUpdateTask("test4", new ClusterStateUpdateTask() {
                 @Override
                 public ClusterState execute(ClusterState currentState) {
                     return currentState;
@@ -851,7 +848,7 @@ public class ClusterServiceIT extends ESIntegTestCase {
         try {
             final CountDownLatch latch = new CountDownLatch(5);
             final CountDownLatch processedFirstTask = new CountDownLatch(1);
-            clusterService1.submitStateUpdateTask("test1", new ProcessedClusterStateUpdateTask() {
+            clusterService1.submitStateUpdateTask("test1", new ClusterStateUpdateTask() {
                 @Override
                 public ClusterState execute(ClusterState currentState) throws Exception {
                     Thread.sleep(100);
@@ -874,7 +871,7 @@ public class ClusterServiceIT extends ESIntegTestCase {
             assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(settingsBuilder()
                     .put(InternalClusterService.SETTING_CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD, "10ms")));
 
-            clusterService1.submitStateUpdateTask("test2", new ProcessedClusterStateUpdateTask() {
+            clusterService1.submitStateUpdateTask("test2", new ClusterStateUpdateTask() {
                 @Override
                 public ClusterState execute(ClusterState currentState) throws Exception {
                     Thread.sleep(100);
@@ -891,7 +888,7 @@ public class ClusterServiceIT extends ESIntegTestCase {
                     latch.countDown();
                 }
             });
-            clusterService1.submitStateUpdateTask("test3", new ProcessedClusterStateUpdateTask() {
+            clusterService1.submitStateUpdateTask("test3", new ClusterStateUpdateTask() {
                 @Override
                 public ClusterState execute(ClusterState currentState) throws Exception {
                     Thread.sleep(100);
@@ -908,7 +905,7 @@ public class ClusterServiceIT extends ESIntegTestCase {
                     fail();
                 }
             });
-            clusterService1.submitStateUpdateTask("test4", new ProcessedClusterStateUpdateTask() {
+            clusterService1.submitStateUpdateTask("test4", new ClusterStateUpdateTask() {
                 @Override
                 public ClusterState execute(ClusterState currentState) throws Exception {
                     Thread.sleep(100);
@@ -927,7 +924,7 @@ public class ClusterServiceIT extends ESIntegTestCase {
             });
             // Additional update task to make sure all previous logging made it to the logger
             // We don't check logging for this on since there is no guarantee that it will occur before our check
-            clusterService1.submitStateUpdateTask("test5", new ProcessedClusterStateUpdateTask() {
+            clusterService1.submitStateUpdateTask("test5", new ClusterStateUpdateTask() {
                 @Override
                 public ClusterState execute(ClusterState currentState) {
                     return currentState;
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java b/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java
index fb60e50..1aa1602 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java
@@ -21,7 +21,6 @@ package org.elasticsearch.cluster;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlocks;
@@ -40,7 +39,6 @@ import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.gateway.GatewayService;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
@@ -533,7 +531,7 @@ public class ClusterStateDiffIT extends ESIntegTestCase {
                             randomName("warm"),
                             new String[]{randomName("type")},
                             randomBoolean(),
-                            new IndexWarmersMetaData.SearchSource(new BytesArray(randomAsciiOfLength(1000))))
+                            new BytesArray(randomAsciiOfLength(1000)))
             );
         } else {
             return new IndexWarmersMetaData();
diff --git a/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java b/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java
index ab01ea1..9ca3f17 100644
--- a/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java
@@ -69,7 +69,7 @@ public class MinimumMasterNodesIT extends ESIntegTestCase {
         Settings settings = settingsBuilder()
                 .put("discovery.type", "zen")
                 .put("discovery.zen.minimum_master_nodes", 2)
-                .put("discovery.zen.ping_timeout", "200ms")
+                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "200ms")
                 .put("discovery.initial_state_timeout", "500ms")
                 .build();
 
@@ -182,7 +182,7 @@ public class MinimumMasterNodesIT extends ESIntegTestCase {
         Settings settings = settingsBuilder()
                 .put("discovery.type", "zen")
                 .put("discovery.zen.minimum_master_nodes", 3)
-                .put("discovery.zen.ping_timeout", "1s")
+                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "1s")
                 .put("discovery.initial_state_timeout", "500ms")
                 .build();
 
@@ -258,7 +258,7 @@ public class MinimumMasterNodesIT extends ESIntegTestCase {
     public void dynamicUpdateMinimumMasterNodes() throws Exception {
         Settings settings = settingsBuilder()
                 .put("discovery.type", "zen")
-                .put("discovery.zen.ping_timeout", "400ms")
+                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "400ms")
                 .put("discovery.initial_state_timeout", "500ms")
                 .build();
 
@@ -317,7 +317,7 @@ public class MinimumMasterNodesIT extends ESIntegTestCase {
         int nodeCount = scaledRandomIntBetween(1, 5);
         Settings.Builder settings = settingsBuilder()
                 .put("discovery.type", "zen")
-                .put("discovery.zen.ping_timeout", "200ms")
+                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "200ms")
                 .put("discovery.initial_state_timeout", "500ms");
 
         // set an initial value which is at least quorum to avoid split brains during initial startup
@@ -372,7 +372,7 @@ public class MinimumMasterNodesIT extends ESIntegTestCase {
         final AtomicReference<Throwable> failure = new AtomicReference<>();
         logger.debug("--> submitting for cluster state to be rejected");
         final ClusterService masterClusterService = internalCluster().clusterService(master);
-        masterClusterService.submitStateUpdateTask("test", new ProcessedClusterStateUpdateTask() {
+        masterClusterService.submitStateUpdateTask("test", new ClusterStateUpdateTask() {
             @Override
             public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {
                 latch.countDown();
diff --git a/core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java b/core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java
index 2d3630c..fd8d6d6 100644
--- a/core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java
@@ -33,6 +33,7 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.discovery.MasterNotDiscoveredException;
+import org.elasticsearch.discovery.zen.ZenDiscovery;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService;
@@ -45,12 +46,8 @@ import java.util.HashMap;
 
 import static org.elasticsearch.action.percolate.PercolateSourceBuilder.docBuilder;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertExists;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.lessThan;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 /**
  */
@@ -68,7 +65,7 @@ public class NoMasterNodeIT extends ESIntegTestCase {
                 .put("discovery.type", "zen")
                 .put("action.auto_create_index", autoCreateIndex)
                 .put("discovery.zen.minimum_master_nodes", 2)
-                .put("discovery.zen.ping_timeout", "200ms")
+                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "200ms")
                 .put("discovery.initial_state_timeout", "500ms")
                 .put(DiscoverySettings.NO_MASTER_BLOCK, "all")
                 .build();
@@ -221,7 +218,7 @@ public class NoMasterNodeIT extends ESIntegTestCase {
                 .put("discovery.type", "zen")
                 .put("action.auto_create_index", false)
                 .put("discovery.zen.minimum_master_nodes", 2)
-                .put("discovery.zen.ping_timeout", "200ms")
+                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "200ms")
                 .put("discovery.initial_state_timeout", "500ms")
                 .put(DiscoverySettings.NO_MASTER_BLOCK, "write")
                 .build();
diff --git a/core/src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java b/core/src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java
index b7029bf..fe782f1 100644
--- a/core/src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java
@@ -41,7 +41,7 @@ import static org.hamcrest.CoreMatchers.equalTo;
 public class DiffableTests extends ESTestCase {
 
     @Test
-    public void testImmutableMapDiff() throws IOException {
+    public void testJdkMapDiff() throws IOException {
         ImmutableMap.Builder<String, TestDiffable> builder = ImmutableMap.builder();
         builder.put("foo", new TestDiffable("1"));
         builder.put("bar", new TestDiffable("2"));
@@ -57,7 +57,7 @@ public class DiffableTests extends ESTestCase {
         BytesStreamOutput out = new BytesStreamOutput();
         diff.writeTo(out);
         StreamInput in = StreamInput.wrap(out.bytes());
-        ImmutableMap<String, TestDiffable> serialized = DiffableUtils.readImmutableMapDiff(in, TestDiffable.PROTO).apply(before);
+        Map<String, TestDiffable> serialized = DiffableUtils.readJdkMapDiff(in, TestDiffable.PROTO).apply(before);
         assertThat(serialized.size(), equalTo(3));
         assertThat(serialized.get("foo").value(), equalTo("1"));
         assertThat(serialized.get("baz").value(), equalTo("4"));
diff --git a/core/src/test/java/org/elasticsearch/common/collect/CopyOnWriteHashSetTests.java b/core/src/test/java/org/elasticsearch/common/collect/CopyOnWriteHashSetTests.java
deleted file mode 100644
index 7ca4a7b..0000000
--- a/core/src/test/java/org/elasticsearch/common/collect/CopyOnWriteHashSetTests.java
+++ /dev/null
@@ -1,127 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.collect;
-
-import com.google.common.collect.ImmutableSet;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashSet;
-import java.util.Set;
-
-public class CopyOnWriteHashSetTests extends ESTestCase {
-
-    private static class O {
-
-        private final int value, hashCode;
-
-        O(int value, int hashCode) {
-            super();
-            this.value = value;
-            this.hashCode = hashCode;
-        }
-
-        @Override
-        public int hashCode() {
-            return hashCode;
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null || !(obj instanceof O)) {
-                return false;
-            }
-            return value == ((O) obj).value;
-        }
-    }
-
-    public void testDuel() {
-        final int iters = scaledRandomIntBetween(2, 5);
-        for (int iter = 0; iter < iters; ++iter) {
-            final int valueBits = randomIntBetween(1, 30);
-            final int hashBits = randomInt(valueBits);
-            // we compute the total number of ops based on the bits of the hash
-            // since the test is much heavier when few bits are used for the hash
-            final int numOps = randomInt(10 + hashBits * 100);
-
-            Set<O> ref = new HashSet<>();
-            CopyOnWriteHashSet<O> set = new CopyOnWriteHashSet<>();
-            assertEquals(ref, set);
-            final int hashBase = randomInt();
-            for (int i = 0; i < numOps; ++i) {
-                final int v = randomInt(1 << valueBits);
-                final int h = (v & ((1 << hashBits) - 1)) ^ hashBase;
-                O key = new O(v, h);
-
-                Set<O> newRef = new HashSet<>(ref);
-                final CopyOnWriteHashSet<O> newSet;
-
-                if (randomBoolean()) {
-                    // ADD
-                    newRef.add(key);
-                    newSet = set.copyAndAdd(key);
-                } else {
-                    // REMOVE
-                    final boolean modified = newRef.remove(key);
-                    newSet = set.copyAndRemove(key);
-                    if (!modified) {
-                        assertSame(set, newSet);
-                    }
-                }
-
-                assertEquals(ref, set); // make sure that the old copy has not been modified
-                assertEquals(newRef, newSet);
-                assertEquals(newSet, newRef);
-                assertEquals(ref.isEmpty(), set.isEmpty());
-                assertEquals(newRef.isEmpty(), newSet.isEmpty());
-
-                ref = newRef;
-                set = newSet;
-            }
-            assertEquals(ref, CopyOnWriteHashSet.copyOf(ref));
-            assertEquals(ImmutableSet.of(), CopyOnWriteHashSet.copyOf(ref).copyAndRemoveAll(ref));
-        }
-    }
-
-    public void testUnsupportedAPIs() {
-        try {
-            new CopyOnWriteHashSet<>().add("a");
-            fail();
-        } catch (UnsupportedOperationException e) {
-            // expected
-        }
-
-        try {
-            new CopyOnWriteHashSet<>().copyAndAdd("a").remove("a");
-            fail();
-        } catch (UnsupportedOperationException e) {
-            // expected
-        }
-    }
-
-    public void testUnsupportedValues() {
-        try {
-            new CopyOnWriteHashSet<>().copyAndAdd(null);
-            fail();
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/common/transport/BoundTransportAddressTests.java b/core/src/test/java/org/elasticsearch/common/transport/BoundTransportAddressTests.java
new file mode 100644
index 0000000..41ed95a
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/transport/BoundTransportAddressTests.java
@@ -0,0 +1,81 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.transport;
+
+import org.elasticsearch.common.io.stream.ByteBufferStreamInput;
+import org.elasticsearch.common.io.stream.BytesStreamOutput;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.test.ESTestCase;
+
+import java.net.InetAddress;
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.hamcrest.Matchers.*;
+
+/**
+ * Basic tests for the {@link BoundTransportAddress} class. These tests should not bind to any addresses but should
+ * just test things like serialization and exception handling...
+ */
+public class BoundTransportAddressTests extends ESTestCase {
+
+    public void testSerialization() throws Exception {
+        InetAddress[] inetAddresses = InetAddress.getAllByName("0.0.0.0");
+        List<InetSocketTransportAddress> transportAddressList = new ArrayList<>();
+        for (InetAddress address : inetAddresses) {
+            transportAddressList.add(new InetSocketTransportAddress(address, randomIntBetween(9200, 9299)));
+        }
+        final BoundTransportAddress transportAddress = new BoundTransportAddress(transportAddressList.toArray(new InetSocketTransportAddress[0]), transportAddressList.get(0));
+        assertThat(transportAddress.boundAddresses().length, equalTo(transportAddressList.size()));
+
+        // serialize
+        BytesStreamOutput streamOutput = new BytesStreamOutput();
+        transportAddress.writeTo(streamOutput);
+        StreamInput in = ByteBufferStreamInput.wrap(streamOutput.bytes());
+
+        BoundTransportAddress serializedAddress;
+        if (randomBoolean()) {
+            serializedAddress = BoundTransportAddress.readBoundTransportAddress(in);
+        } else {
+            serializedAddress = new BoundTransportAddress();
+            serializedAddress.readFrom(in);
+        }
+
+        assertThat(serializedAddress, not(sameInstance(transportAddress)));
+        assertThat(serializedAddress.boundAddresses().length, equalTo(transportAddress.boundAddresses().length));
+        assertThat(serializedAddress.publishAddress(), equalTo(transportAddress.publishAddress()));
+
+        TransportAddress[] serializedBoundAddresses = serializedAddress.boundAddresses();
+        TransportAddress[] boundAddresses = transportAddress.boundAddresses();
+        for (int i = 0; i < serializedBoundAddresses.length; i++) {
+            assertThat(serializedBoundAddresses[i], equalTo(boundAddresses[i]));
+        }
+    }
+
+    public void testBadBoundAddressArray() {
+        try {
+            TransportAddress[] badArray = randomBoolean() ? null : new TransportAddress[0];
+            new BoundTransportAddress(badArray, new InetSocketTransportAddress(InetAddress.getLoopbackAddress(), 80));
+            fail("expected an exception to be thrown due to no bound address");
+        } catch (IllegalArgumentException e) {
+            //expected
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java b/core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java
index 7cc315c..dc64ffd 100644
--- a/core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java
+++ b/core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java
@@ -32,8 +32,6 @@ import org.elasticsearch.action.index.IndexResponse;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.search.MultiMatchQuery;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -165,6 +163,13 @@ public class DocumentActionsIT extends ESIntegTestCase {
             assertThat(countResponse.getSuccessfulShards(), equalTo(numShards.numPrimaries));
             assertThat(countResponse.getFailedShards(), equalTo(0));
 
+            // test failed (simply query that can't be parsed)
+            try {
+                client().count(countRequest("test").source("{ term : { _type : \"type1 } }")).actionGet();
+            } catch(SearchPhaseExecutionException e) {
+                assertThat(e.shardFailures().length, equalTo(numShards.numPrimaries));
+            }
+
             // count with no query is a match all one
             countResponse = client().prepareCount("test").execute().actionGet();
             assertThat("Failures " + countResponse.getShardFailures(), countResponse.getShardFailures() == null ? 0 : countResponse.getShardFailures().length, equalTo(0));
diff --git a/core/src/test/java/org/elasticsearch/http/netty/NettyHttpServerPipeliningTests.java b/core/src/test/java/org/elasticsearch/http/netty/NettyHttpServerPipeliningTests.java
index 2070076..74e30d5 100644
--- a/core/src/test/java/org/elasticsearch/http/netty/NettyHttpServerPipeliningTests.java
+++ b/core/src/test/java/org/elasticsearch/http/netty/NettyHttpServerPipeliningTests.java
@@ -95,7 +95,7 @@ public class NettyHttpServerPipeliningTests extends ESTestCase {
         Settings settings = settingsBuilder().put("http.pipelining", true).build();
         httpServerTransport = new CustomNettyHttpServerTransport(settings);
         httpServerTransport.start();
-        InetSocketTransportAddress transportAddress = (InetSocketTransportAddress) httpServerTransport.boundAddress().boundAddress();
+        InetSocketTransportAddress transportAddress = (InetSocketTransportAddress) randomFrom(httpServerTransport.boundAddress().boundAddresses());
 
         List<String> requests = Arrays.asList("/firstfast", "/slow?sleep=500", "/secondfast", "/slow?sleep=1000", "/thirdfast");
         try (NettyHttpClient nettyHttpClient = new NettyHttpClient()) {
@@ -110,7 +110,7 @@ public class NettyHttpServerPipeliningTests extends ESTestCase {
         Settings settings = settingsBuilder().put("http.pipelining", false).build();
         httpServerTransport = new CustomNettyHttpServerTransport(settings);
         httpServerTransport.start();
-        InetSocketTransportAddress transportAddress = (InetSocketTransportAddress) httpServerTransport.boundAddress().boundAddress();
+        InetSocketTransportAddress transportAddress = (InetSocketTransportAddress) randomFrom(httpServerTransport.boundAddress().boundAddresses());
 
         List<String> requests = Arrays.asList("/slow?sleep=1000", "/firstfast", "/secondfast", "/thirdfast", "/slow?sleep=500");
         try (NettyHttpClient nettyHttpClient = new NettyHttpClient()) {
diff --git a/core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningDisabledIT.java b/core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningDisabledIT.java
index 1d968b7..964f985 100644
--- a/core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningDisabledIT.java
+++ b/core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningDisabledIT.java
@@ -56,7 +56,7 @@ public class NettyPipeliningDisabledIT extends ESIntegTestCase {
         List<String> requests = Arrays.asList("/", "/_nodes/stats", "/", "/_cluster/state", "/", "/_nodes", "/");
 
         HttpServerTransport httpServerTransport = internalCluster().getInstance(HttpServerTransport.class);
-        InetSocketTransportAddress inetSocketTransportAddress = (InetSocketTransportAddress) httpServerTransport.boundAddress().boundAddress();
+        InetSocketTransportAddress inetSocketTransportAddress = (InetSocketTransportAddress) randomFrom(httpServerTransport.boundAddress().boundAddresses());
 
         try (NettyHttpClient nettyHttpClient = new NettyHttpClient()) {
             Collection<HttpResponse> responses = nettyHttpClient.sendRequests(inetSocketTransportAddress.address(), requests.toArray(new String[]{}));
diff --git a/core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningEnabledIT.java b/core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningEnabledIT.java
index 84a0f9b..eafd242 100644
--- a/core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningEnabledIT.java
+++ b/core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningEnabledIT.java
@@ -52,7 +52,7 @@ public class NettyPipeliningEnabledIT extends ESIntegTestCase {
         List<String> requests = Arrays.asList("/", "/_nodes/stats", "/", "/_cluster/state", "/");
 
         HttpServerTransport httpServerTransport = internalCluster().getInstance(HttpServerTransport.class);
-        InetSocketTransportAddress inetSocketTransportAddress = (InetSocketTransportAddress) httpServerTransport.boundAddress().boundAddress();
+        InetSocketTransportAddress inetSocketTransportAddress = (InetSocketTransportAddress) randomFrom(httpServerTransport.boundAddress().boundAddresses());
 
         try (NettyHttpClient nettyHttpClient = new NettyHttpClient()) {
             Collection<HttpResponse> responses = nettyHttpClient.sendRequests(inetSocketTransportAddress.address(), requests.toArray(new String[]{}));
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java
index 4bc25ed..1c5a847 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java
@@ -20,7 +20,10 @@ package org.elasticsearch.index.mapper.geo;
 
 import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.Version;
+import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
+import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.DocumentMapper;
@@ -28,13 +31,18 @@ import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.MapperParsingException;
 import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.ParsedDocument;
+import org.elasticsearch.search.SearchHitField;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 import org.elasticsearch.test.VersionUtils;
 import org.junit.Test;
 
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
 
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.hamcrest.Matchers.*;
 import static org.hamcrest.Matchers.is;
 import static org.hamcrest.Matchers.nullValue;
@@ -640,4 +648,52 @@ public class GeoPointFieldMapperTests extends ESSingleNodeTestCase {
         mergeResult = stage1.merge(stage2.mapping(), false, false);
         assertThat(Arrays.toString(mergeResult.buildConflicts()), mergeResult.hasConflicts(), equalTo(false));
     }
+
+    public void testGeoHashSearch() throws Exception {
+        // create a geo_point mapping with geohash enabled and random (between 1 and 12) geohash precision
+        int precision = randomIntBetween(1, 12);
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("pin").startObject("properties").startObject("location")
+                .field("type", "geo_point").field("geohash", true).field("geohash_precision", precision).field("store", true).endObject()
+                .endObject().endObject().endObject().string();
+
+        // create index and add a test point (dr5regy6rc6z)
+        CreateIndexRequestBuilder mappingRequest = client().admin().indices().prepareCreate("test").addMapping("pin", mapping);
+        mappingRequest.execute().actionGet();
+        client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForGreenStatus().execute().actionGet();
+        client().prepareIndex("test", "pin", "1").setSource(jsonBuilder().startObject().startObject("location").field("lat", 40.7143528)
+                .field("lon", -74.0059731).endObject().endObject()).setRefresh(true).execute().actionGet();
+
+        // match all search with geohash field
+        SearchResponse searchResponse = client().prepareSearch().addField("location.geohash").setQuery(matchAllQuery()).execute().actionGet();
+        Map<String, SearchHitField> m = searchResponse.getHits().getAt(0).getFields();
+
+        // ensure single geohash was indexed
+        assertEquals("dr5regy6rc6y".substring(0, precision), m.get("location.geohash").value());
+    }
+
+    public void testGeoHashSearchWithPrefix() throws Exception {
+        // create a geo_point mapping with geohash enabled and random (between 1 and 12) geohash precision
+        int precision = randomIntBetween(1, 12);
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("pin").startObject("properties").startObject("location")
+                .field("type", "geo_point").field("geohash_prefix", true).field("geohash_precision", precision).field("store", true)
+                .endObject().endObject().endObject().endObject().string();
+
+        // create index and add a test point (dr5regy6rc6z)
+        CreateIndexRequestBuilder mappingRequest = client().admin().indices().prepareCreate("test").addMapping("pin", mapping);
+        mappingRequest.execute().actionGet();
+        client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForGreenStatus().execute().actionGet();
+        client().prepareIndex("test", "pin", "1").setSource(jsonBuilder().startObject().startObject("location").field("lat", 40.7143528)
+                .field("lon", -74.0059731).endObject().endObject()).setRefresh(true).execute().actionGet();
+
+        // match all search with geohash field (includes prefixes)
+        SearchResponse searchResponse = client().prepareSearch().addField("location.geohash").setQuery(matchAllQuery()).execute().actionGet();
+        Map<String, SearchHitField> m = searchResponse.getHits().getAt(0).getFields();
+
+        List<Object> hashes = m.get("location.geohash").values();
+
+        final int numHashes = hashes.size();
+        for(int i=0; i<numHashes; ++i) {
+            assertEquals("dr5regy6rc6y".substring(0, numHashes-i), hashes.get(i));
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
index b9c9849..da32e38 100644
--- a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
@@ -69,7 +69,8 @@ import org.elasticsearch.indices.IndicesModule;
 import org.elasticsearch.indices.analysis.IndicesAnalysisService;
 import org.elasticsearch.indices.breaker.CircuitBreakerService;
 import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.script.ScriptModule;
+import org.elasticsearch.script.*;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.TestSearchContext;
@@ -85,13 +86,11 @@ import java.io.IOException;
 import java.lang.reflect.InvocationHandler;
 import java.lang.reflect.Method;
 import java.lang.reflect.Proxy;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 import java.util.concurrent.ExecutionException;
 
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.not;
 
 public abstract class AbstractQueryTestCase<QB extends AbstractQueryBuilder<QB>> extends ESTestCase {
 
@@ -166,7 +165,38 @@ public abstract class AbstractQueryTestCase<QB extends AbstractQueryBuilder<QB>>
                         bindQueryParsersExtension();
                     }
                 },
-                new ScriptModule(settings),
+                new ScriptModule(settings) {
+                    @Override
+                    protected void configure() {
+                        Settings settings = Settings.builder()
+                                .put("path.home", createTempDir())
+                                // no file watching, so we don't need a ResourceWatcherService
+                                .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING, false)
+                                .build();
+                        MockScriptEngine mockScriptEngine = new MockScriptEngine();
+                        Multibinder<ScriptEngineService> multibinder = Multibinder.newSetBinder(binder(), ScriptEngineService.class);
+                        multibinder.addBinding().toInstance(mockScriptEngine);
+                        try {
+                            Class.forName("com.github.mustachejava.Mustache");
+                        } catch(ClassNotFoundException e) {
+                            throw new IllegalStateException("error while loading mustache", e);
+                        }
+                        MustacheScriptEngineService mustacheScriptEngineService = new MustacheScriptEngineService(settings);
+                        Set<ScriptEngineService> engines = new HashSet<>();
+                        engines.add(mockScriptEngine);
+                        engines.add(mustacheScriptEngineService);
+                        List<ScriptContext.Plugin> customContexts = new ArrayList<>();
+                        bind(ScriptContextRegistry.class).toInstance(new ScriptContextRegistry(customContexts));
+                        try {
+                            ScriptService scriptService = new ScriptService(settings, new Environment(settings), engines, null, new ScriptContextRegistry(customContexts));
+                            bind(ScriptService.class).toInstance(scriptService);
+                        } catch(IOException e) {
+                            throw new IllegalStateException("error while binding ScriptService", e);
+                        }
+
+
+                    }
+                },
                 new IndexSettingsModule(index, indexSettings),
                 new IndexCacheModule(indexSettings),
                 new AnalysisModule(indexSettings, new IndicesAnalysisService(indexSettings)),
@@ -482,7 +512,10 @@ public abstract class AbstractQueryTestCase<QB extends AbstractQueryBuilder<QB>>
      * @return a new {@link QueryParseContext} based on the base test index and queryParserService
      */
     protected static QueryParseContext createParseContext() {
-        return createShardContext().parseContext();
+        QueryParseContext queryParseContext = new QueryParseContext(queryParserService.indicesQueriesRegistry());
+        queryParseContext.reset(null);
+        queryParseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
+        return queryParseContext;
     }
 
     /**
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilderTests.java
index e94bc33..c138ad0 100644
--- a/core/src/test/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilderTests.java
@@ -73,10 +73,7 @@ public class GeoBoundingBoxQueryBuilderTests extends AbstractQueryTestCase<GeoBo
         }
 
         if (randomBoolean()) {
-            builder.coerce(randomBoolean());
-        }
-        if (randomBoolean()) {
-            builder.ignoreMalformed(randomBoolean());
+            builder.setValidationMethod(randomFrom(GeoValidationMethod.values()));
         }
 
         builder.type(randomFrom(GeoExecType.values()));
@@ -119,7 +116,7 @@ public class GeoBoundingBoxQueryBuilderTests extends AbstractQueryTestCase<GeoBo
         PointTester[] testers = { new TopTester(), new LeftTester(), new BottomTester(), new RightTester() };
 
         GeoBoundingBoxQueryBuilder builder = createTestQueryBuilder();
-        builder.coerce(false).ignoreMalformed(false);
+        builder.setValidationMethod(GeoValidationMethod.STRICT);
 
         for (PointTester tester : testers) {
             try {
@@ -136,7 +133,7 @@ public class GeoBoundingBoxQueryBuilderTests extends AbstractQueryTestCase<GeoBo
         PointTester[] testers = { new TopTester(), new LeftTester(), new BottomTester(), new RightTester() };
 
         GeoBoundingBoxQueryBuilder builder = createTestQueryBuilder();
-        builder.ignoreMalformed(true);
+        builder.setValidationMethod(GeoValidationMethod.IGNORE_MALFORMED);
 
         for (PointTester tester : testers) {
             tester.invalidateCoordinate(builder, true);
@@ -152,7 +149,7 @@ public class GeoBoundingBoxQueryBuilderTests extends AbstractQueryTestCase<GeoBo
             QueryValidationException except = null;
 
             GeoBoundingBoxQueryBuilder builder = createTestQueryBuilder();
-            tester.invalidateCoordinate(builder.coerce(true), false);
+            tester.invalidateCoordinate(builder.setValidationMethod(GeoValidationMethod.COERCE), false);
             except = builder.checkLatLon(true);
             assertNull("Inner post 2.0 validation w/ coerce should ignore invalid "
                     + tester.getClass().getName()
@@ -160,7 +157,7 @@ public class GeoBoundingBoxQueryBuilderTests extends AbstractQueryTestCase<GeoBo
                     + tester.invalidCoordinate + " ",
                     except);
 
-            tester.invalidateCoordinate(builder.coerce(true), false);
+            tester.invalidateCoordinate(builder.setValidationMethod(GeoValidationMethod.COERCE), false);
             except = builder.checkLatLon(false);
             assertNull("Inner pre 2.0 validation w/ coerce should ignore invalid coordinate: "
                     + tester.getClass().getName()
@@ -168,7 +165,7 @@ public class GeoBoundingBoxQueryBuilderTests extends AbstractQueryTestCase<GeoBo
                     + tester.invalidCoordinate + " ",
                     except);
 
-            tester.invalidateCoordinate(builder.coerce(false).ignoreMalformed(false), false);
+            tester.invalidateCoordinate(builder.setValidationMethod(GeoValidationMethod.STRICT), false);
             except = builder.checkLatLon(true);
             assertNull("Inner pre 2.0 validation w/o coerce should ignore invalid coordinate for old indexes: "
                     + tester.getClass().getName()
@@ -176,7 +173,7 @@ public class GeoBoundingBoxQueryBuilderTests extends AbstractQueryTestCase<GeoBo
                     + tester.invalidCoordinate,
                     except);
 
-            tester.invalidateCoordinate(builder.coerce(false).ignoreMalformed(false), false);
+            tester.invalidateCoordinate(builder.setValidationMethod(GeoValidationMethod.STRICT), false);
             except = builder.checkLatLon(false);
             assertNotNull("Inner post 2.0 validation w/o coerce should detect invalid coordinate: "
                     + tester.getClass().getName()
@@ -196,7 +193,7 @@ public class GeoBoundingBoxQueryBuilderTests extends AbstractQueryTestCase<GeoBo
 
         assumeTrue("top should not be equal to bottom for flip check", top != bottom);
         System.out.println("top: " + top + " bottom: " + bottom);
-        builder.coerce(false).ignoreMalformed(false).setCorners(bottom, left, top, right);
+        builder.setValidationMethod(GeoValidationMethod.STRICT).setCorners(bottom, left, top, right);
     }
 
     @Test
@@ -208,7 +205,7 @@ public class GeoBoundingBoxQueryBuilderTests extends AbstractQueryTestCase<GeoBo
         double right = builder.bottomRight().getLon();
 
         assumeTrue("top should not be equal to bottom for flip check", top != bottom);
-        builder.coerce(false).ignoreMalformed(true).setCorners(bottom, left, top, right);
+        builder.setValidationMethod(GeoValidationMethod.IGNORE_MALFORMED).setCorners(bottom, left, top, right);
     }
 
     @Test
@@ -219,8 +216,8 @@ public class GeoBoundingBoxQueryBuilderTests extends AbstractQueryTestCase<GeoBo
         double bottom = builder.bottomRight().getLat();
         double right = builder.bottomRight().getLon();
         
-        builder.ignoreMalformed(true).setCorners(top, right, bottom, left);
-        builder.ignoreMalformed(false).setCorners(top, right, bottom, left);
+        builder.setValidationMethod(GeoValidationMethod.IGNORE_MALFORMED).setCorners(top, right, bottom, left);
+        builder.setValidationMethod(GeoValidationMethod.STRICT).setCorners(top, right, bottom, left);
     }
 
     @Test
@@ -230,7 +227,7 @@ public class GeoBoundingBoxQueryBuilderTests extends AbstractQueryTestCase<GeoBo
         if (getCurrentTypes().length != 0 && "mapped_geo".equals(qb.fieldName())) {
             // only execute this test if we are running on a valid geo field
             qb.setCorners(200, 200, qb.bottomRight().getLat(), qb.bottomRight().getLon());
-            qb.coerce(true);
+            qb.setValidationMethod(GeoValidationMethod.COERCE);
             Query query = qb.toQuery(createShardContext());
             if (query instanceof ConstantScoreQuery) {
                 ConstantScoreQuery result = (ConstantScoreQuery) query;
@@ -246,6 +243,11 @@ public class GeoBoundingBoxQueryBuilderTests extends AbstractQueryTestCase<GeoBo
             }
         }
     }
+    
+    @Test
+    public void checkStrictnessDefault() {
+        assertFalse("Someone changed the default for coordinate validation - were the docs changed as well?", GeoValidationMethod.DEFAULT_LENIENT_PARSING);
+    }
 
     @Override
     protected void doAssertLuceneQuery(GeoBoundingBoxQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeoDistanceQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/GeoDistanceQueryBuilderTests.java
index 0bc0b29..f65b523 100644
--- a/core/src/test/java/org/elasticsearch/index/query/GeoDistanceQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/GeoDistanceQueryBuilderTests.java
@@ -59,11 +59,7 @@ public class GeoDistanceQueryBuilderTests extends AbstractQueryTestCase<GeoDista
         qb.point(new GeoPoint(p.getY(), p.getX()));
 
         if (randomBoolean()) {
-            qb.coerce(randomBoolean());
-        }
-
-        if (randomBoolean()) {
-            qb.ignoreMalformed(randomBoolean());
+            qb.setValidationMethod(randomFrom(GeoValidationMethod.values()));
         }
 
         if (randomBoolean()) {
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java b/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java
index fa827a0..9dd1a55 100644
--- a/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java
@@ -96,10 +96,7 @@ public class GeoDistanceRangeQueryTests extends AbstractQueryTestCase<GeoDistanc
             builder.optimizeBbox(randomFrom("none", "memory", "indexed"));
         }
         if (randomBoolean()) {
-            builder.coerce(randomBoolean());
-        }
-        if (randomBoolean()) {
-            builder.ignoreMalformed(randomBoolean());
+            builder.setValidationMethod(randomFrom(GeoValidationMethod.values()));
         }
         return builder;
     }
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeoPolygonQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/GeoPolygonQueryBuilderTests.java
index 12da995..1825398 100644
--- a/core/src/test/java/org/elasticsearch/index/query/GeoPolygonQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/GeoPolygonQueryBuilderTests.java
@@ -49,8 +49,9 @@ public class GeoPolygonQueryBuilderTests extends AbstractQueryTestCase<GeoPolygo
     protected GeoPolygonQueryBuilder doCreateTestQueryBuilder() {
         List<GeoPoint> polygon = randomPolygon(randomIntBetween(4, 50));
         GeoPolygonQueryBuilder builder = new GeoPolygonQueryBuilder(GEO_POINT_FIELD_NAME, polygon);
-        builder.coerce(randomBoolean());
-        builder.ignoreMalformed(randomBoolean());
+        if (randomBoolean()) {
+            builder.setValidationMethod(randomFrom(GeoValidationMethod.values()));
+        }
         return builder;
     }
 
@@ -62,7 +63,7 @@ public class GeoPolygonQueryBuilderTests extends AbstractQueryTestCase<GeoPolygo
         List<GeoPoint> queryBuilderPoints = queryBuilder.points();
         GeoPoint[] queryPoints = geoQuery.points();
         assertThat(queryPoints.length, equalTo(queryBuilderPoints.size()));
-        if (queryBuilder.coerce()) {
+        if (GeoValidationMethod.isCoerce(queryBuilder.getValidationMethod())) {
             for (int i = 0; i < queryBuilderPoints.size(); i++) {
                 GeoPoint queryBuilderPoint = queryBuilderPoints.get(i);
                 GeoUtils.normalizePoint(queryBuilderPoint, true, true);
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeoShapeQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/GeoShapeQueryBuilderTests.java
index 66ba602..a4ac66c 100644
--- a/core/src/test/java/org/elasticsearch/index/query/GeoShapeQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/GeoShapeQueryBuilderTests.java
@@ -166,37 +166,31 @@ public class GeoShapeQueryBuilderTests extends AbstractQueryTestCase<GeoShapeQue
         new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, "id", (String) null);
     }
 
-    @Test
+    @Test(expected=IllegalArgumentException.class)
     public void testNoRelation() throws IOException {
         ShapeBuilder shape = RandomShapeGenerator.createShapeWithin(getRandom(), null);
-        try {
-            GeoShapeQueryBuilder builder = new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, shape);
-            builder.relation(null);
-            fail("relation cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
+        GeoShapeQueryBuilder builder = new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, shape);
+        builder.relation(null);
     }
 
     @Test
-    public void testInvalidRelation() {
+    public void testInvalidRelation() throws IOException {
         ShapeBuilder shape = RandomShapeGenerator.createShapeWithin(getRandom(), null);
+        GeoShapeQueryBuilder builder = new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, shape);
         try {
-            GeoShapeQueryBuilder builder = new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, shape);
             builder.strategy(SpatialStrategy.TERM);
-            ShapeRelation relation = randomFrom(ShapeRelation.DISJOINT, ShapeRelation.WITHIN);
-            builder.relation(relation);
-            QueryValidationException exception = builder.validate();
-            assertThat(exception, notNullValue());
-            assertThat(exception.validationErrors(), notNullValue());
-            assertThat(exception.validationErrors().size(), equalTo(1));
-            assertThat(
-                    exception.validationErrors().get(0),
-                    equalTo("[" + GeoShapeQueryBuilder.NAME + "] strategy [" + SpatialStrategy.TERM.getStrategyName()
-                            + "] only supports relation [" + ShapeRelation.INTERSECTS.getRelationName() + "] found relation ["
-                            + relation.getRelationName() + "]"));
-        } catch (IOException e) {
-            throw new RuntimeException(e);
+            builder.relation(randomFrom(ShapeRelation.DISJOINT, ShapeRelation.WITHIN));
+            fail("Illegal combination of strategy and relation setting");
+        } catch (IllegalArgumentException e) {
+            // okay
+        }
+
+        try {
+            builder.relation(randomFrom(ShapeRelation.DISJOINT, ShapeRelation.WITHIN));
+            builder.strategy(SpatialStrategy.TERM);
+            fail("Illegal combination of strategy and relation setting");
+        } catch (IllegalArgumentException e) {
+            // okay
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/query/MatchQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/MatchQueryBuilderTests.java
index 3c24ec1..f9da80d 100644
--- a/core/src/test/java/org/elasticsearch/index/query/MatchQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/MatchQueryBuilderTests.java
@@ -19,11 +19,10 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.ExtendedCommonTermsQuery;
 import org.apache.lucene.search.*;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.lucene.search.MultiPhrasePrefixQuery;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.search.MatchQuery;
 import org.elasticsearch.index.search.MatchQuery.ZeroTermsQuery;
@@ -35,7 +34,6 @@ import java.util.Locale;
 import static org.hamcrest.CoreMatchers.either;
 import static org.hamcrest.CoreMatchers.instanceOf;
 import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
 import static org.hamcrest.Matchers.notNullValue;
 
 public class MatchQueryBuilderTests extends AbstractQueryTestCase<MatchQueryBuilder> {
@@ -144,7 +142,15 @@ public class MatchQueryBuilderTests extends AbstractQueryTestCase<MatchQueryBuil
         if (query instanceof BooleanQuery) {
             BooleanQuery bq = (BooleanQuery) query;
             if (queryBuilder.minimumShouldMatch() != null) {
-                assertThat(bq.getMinimumNumberShouldMatch(), greaterThan(0));
+                // calculate expected minimumShouldMatch value
+                int optionalClauses = 0;
+                for (BooleanClause c : bq.clauses()) {
+                    if (c.getOccur() == BooleanClause.Occur.SHOULD) {
+                        optionalClauses++;
+                    }
+                }
+                int msm = Queries.calculateMinShouldMatch(optionalClauses, queryBuilder.minimumShouldMatch());
+                assertThat(bq.getMinimumNumberShouldMatch(), equalTo(msm));
             }
             if (queryBuilder.analyzer() == null && queryBuilder.value().toString().length() > 0) {
                 assertEquals(bq.clauses().size(), queryBuilder.value().toString().split(" ").length);
@@ -160,7 +166,11 @@ public class MatchQueryBuilderTests extends AbstractQueryTestCase<MatchQueryBuil
         if (query instanceof FuzzyQuery) {
             assertTrue(queryBuilder.fuzziness() != null);
             FuzzyQuery fuzzyQuery = (FuzzyQuery) query;
-            assertThat(fuzzyQuery.getTerm(), equalTo(new Term(STRING_FIELD_NAME, BytesRefs.toBytesRef(queryBuilder.value()))));
+            // depending on analyzer being set or not we can have term lowercased along the way, so to simplify test we just
+            // compare lowercased terms here
+            String originalTermLc = queryBuilder.value().toString().toLowerCase(Locale.ROOT);
+            String actualTermLc = fuzzyQuery.getTerm().text().toString().toLowerCase(Locale.ROOT);
+            assertThat(actualTermLc, equalTo(originalTermLc));
             assertThat(queryBuilder.prefixLength(), equalTo(fuzzyQuery.getPrefixLength()));
             assertThat(queryBuilder.fuzzyTranspositions(), equalTo(fuzzyQuery.getTranspositions()));
         }
diff --git a/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTests.java
index 6dda3db..14c1d4f 100644
--- a/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTests.java
@@ -286,6 +286,7 @@ public class RangeQueryBuilderTests extends AbstractQueryTestCase<RangeQueryBuil
 
     @Test
     public void testDateRangeQueryTimezone() throws IOException {
+        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
         long startDate = System.currentTimeMillis();
         String query = "{\n" +
                 "    \"range\" : {\n" +
diff --git a/core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTests.java
index 534542d..87384f7 100644
--- a/core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTests.java
@@ -20,13 +20,13 @@
 package org.elasticsearch.index.query;
 
 import org.apache.lucene.search.Query;
+import org.elasticsearch.script.MockScriptEngine;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.expression.ExpressionScriptEngineService;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.HashMap;
+import java.util.Collections;
 import java.util.Map;
 
 import static org.hamcrest.Matchers.instanceOf;
@@ -35,16 +35,9 @@ public class ScriptQueryBuilderTests extends AbstractQueryTestCase<ScriptQueryBu
 
     @Override
     protected ScriptQueryBuilder doCreateTestQueryBuilder() {
-        String script;
-        Map<String, Object> params = null;
-        if (randomBoolean()) {
-            script = "5 * 2 > param";
-            params = new HashMap<>();
-            params.put("param", 1);
-        } else {
-            script = "5 * 2 > 2";
-        }
-        return new ScriptQueryBuilder(new Script(script, ScriptType.INLINE, ExpressionScriptEngineService.NAME, params));
+        String script = "5";
+        Map<String, Object> params = Collections.emptyMap();
+        return new ScriptQueryBuilder(new Script(script, ScriptType.INLINE, MockScriptEngine.NAME, params));
     }
 
     @Override
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
index 8bb1b93..3d89633 100644
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
@@ -84,8 +84,7 @@ public class TemplateQueryBuilderTests extends AbstractQueryTestCase<TemplateQue
         builder.doXContent(content, null);
         content.endObject();
         content.close();
-        assertEquals("{\"template\":{\"inline\":\"I am a $template string\",\"lang\":\"mustache\",\"params\":{\"template\":\"filled\"}}}",
-                content.string());
+        assertEquals("{\"template\":{\"inline\":\"I am a $template string\",\"params\":{\"template\":\"filled\"}}}", content.string());
     }
 
 }
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java
index adebb7f..0c9fc74 100644
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java
@@ -27,15 +27,13 @@ import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchRequest;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Before;
 import org.junit.Test;
@@ -101,14 +99,24 @@ public class TemplateQueryIT extends ESIntegTestCase {
                 "        }\n" +
                 "    }\n" +
                 "}";
-        Map<String, Object> params = new HashMap<>();
-        params.put("template", "all");
-        SearchResponse sr = client().prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().size(0).query(
-                                QueryBuilders.templateQuery(new Template("{ \"query\": { \"match_{{template}}\": {} } }",
-                                        ScriptType.INLINE, null, null, params)))).execute()
-                .actionGet();
+        SearchResponse sr = client().prepareSearch().setSource(new BytesArray(request))
+                .execute().actionGet();
+        assertNoFailures(sr);
+        assertThat(sr.getHits().hits().length, equalTo(0));
+        request = "{\n" +
+                "    \"query\": {\n" +
+                "        \"template\": {\n" +
+                "            \"query\": {\"match_{{template}}\": {}},\n" +
+                "            \"params\" : {\n" +
+                "                \"template\" : \"all\"\n" +
+                "            }\n" +
+                "        }\n" +
+                "    },\n" +
+                "    \"size\":0" +
+                "}";
+
+        sr = client().prepareSearch().setSource(new BytesArray(request))
+                .execute().actionGet();
         assertNoFailures(sr);
         assertThat(sr.getHits().hits().length, equalTo(0));
     }
@@ -136,32 +144,28 @@ public class TemplateQueryIT extends ESIntegTestCase {
         assertHitCount(sr, 2);
     }
 
-    // NORELEASE These need to be tested in TemplateQueryBuilderTests
-    // @Test
-    // public void testRawEscapedTemplate() throws IOException {
-    // String query =
-    // "{\"template\": {\"query\": \"{\\\"match_{{template}}\\\": {}}\\\"\",\"params\" : {\"template\" : \"all\"}}}";
-    //
-    // SearchResponse sr = client().prepareSearch().setQuery(query).get();
-    // assertHitCount(sr, 2);
-    // }
-    //
-    // @Test
-    // public void testRawTemplate() throws IOException {
-    // String query =
-    // "{\"template\": {\"query\": {\"match_{{template}}\": {}},\"params\" : {\"template\" : \"all\"}}}";
-    // SearchResponse sr = client().prepareSearch().setQuery(query).get();
-    // assertHitCount(sr, 2);
-    // }
-    //
-    // @Test
-    // public void testRawFSTemplate() throws IOException {
-    // String query =
-    // "{\"template\": {\"file\": \"storedTemplate\",\"params\" : {\"template\" : \"all\"}}}";
-    //
-    // SearchResponse sr = client().prepareSearch().setQuery(query).get();
-    // assertHitCount(sr, 2);
-    // }
+    @Test
+    public void testRawEscapedTemplate() throws IOException {
+        String query = "{\"template\": {\"query\": \"{\\\"match_{{template}}\\\": {}}\\\"\",\"params\" : {\"template\" : \"all\"}}}";
+
+        SearchResponse sr = client().prepareSearch().setQuery(query).get();
+        assertHitCount(sr, 2);
+    }
+
+    @Test
+    public void testRawTemplate() throws IOException {
+        String query = "{\"template\": {\"query\": {\"match_{{template}}\": {}},\"params\" : {\"template\" : \"all\"}}}";
+        SearchResponse sr = client().prepareSearch().setQuery(query).get();
+        assertHitCount(sr, 2);
+    }
+
+    @Test
+    public void testRawFSTemplate() throws IOException {
+        String query = "{\"template\": {\"file\": \"storedTemplate\",\"params\" : {\"template\" : \"all\"}}}";
+
+        SearchResponse sr = client().prepareSearch().setQuery(query).get();
+        assertHitCount(sr, 2);
+    }
 
     @Test
     public void testSearchRequestTemplateSource() throws Exception {
@@ -169,18 +173,13 @@ public class TemplateQueryIT extends ESIntegTestCase {
         searchRequest.indices("_all");
 
         String query = "{ \"template\" : { \"query\": {\"match_{{template}}\": {} } }, \"params\" : { \"template\":\"all\" } }";
-        searchRequest.template(parseTemplate(query));
+        BytesReference bytesRef = new BytesArray(query);
+        searchRequest.templateSource(bytesRef);
 
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertHitCount(searchResponse, 2);
     }
 
-    private Template parseTemplate(String template) throws IOException {
-        try (XContentParser parser = XContentFactory.xContent(template).createParser(template)) {
-            return TemplateQueryParser.parse(parser, ParseFieldMatcher.EMPTY, "params", "template");
-        }
-    }
-
     @Test
     // Releates to #6318
     public void testSearchRequestFail() throws Exception {
@@ -188,14 +187,16 @@ public class TemplateQueryIT extends ESIntegTestCase {
         searchRequest.indices("_all");
         try {
             String query = "{ \"template\" : { \"query\": {\"match_all\": {}}, \"size\" : \"{{my_size}}\"  } }";
-            searchRequest.template(parseTemplate(query));
+            BytesReference bytesRef = new BytesArray(query);
+            searchRequest.templateSource(bytesRef);
             client().search(searchRequest).get();
             fail("expected exception");
         } catch (Exception ex) {
             // expected - no params
         }
         String query = "{ \"template\" : { \"query\": {\"match_all\": {}}, \"size\" : \"{{my_size}}\"  }, \"params\" : { \"my_size\": 1 } }";
-        searchRequest.template(parseTemplate(query));
+        BytesReference bytesRef = new BytesArray(query);
+        searchRequest.templateSource(bytesRef);
 
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
@@ -233,9 +234,10 @@ public class TemplateQueryIT extends ESIntegTestCase {
     public void testSearchTemplateQueryFromFile() throws Exception {
         SearchRequest searchRequest = new SearchRequest();
         searchRequest.indices("_all");
-        String query = "{" + "  \"file\": \"full-query-template\"," + "  \"params\":{" + "    \"mySize\": 2,"
+        String templateString = "{" + "  \"file\": \"full-query-template\"," + "  \"params\":{" + "    \"mySize\": 2,"
                 + "    \"myField\": \"text\"," + "    \"myValue\": \"value1\"" + "  }" + "}";
-        searchRequest.template(parseTemplate(query));
+        BytesReference bytesRef = new BytesArray(templateString);
+        searchRequest.templateSource(bytesRef);
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
     }
@@ -247,9 +249,10 @@ public class TemplateQueryIT extends ESIntegTestCase {
     public void testTemplateQueryAsEscapedString() throws Exception {
         SearchRequest searchRequest = new SearchRequest();
         searchRequest.indices("_all");
-        String query = "{" + "  \"template\" : \"{ \\\"size\\\": \\\"{{size}}\\\", \\\"query\\\":{\\\"match_all\\\":{}}}\","
+        String templateString = "{" + "  \"template\" : \"{ \\\"size\\\": \\\"{{size}}\\\", \\\"query\\\":{\\\"match_all\\\":{}}}\","
                 + "  \"params\":{" + "    \"size\": 1" + "  }" + "}";
-        searchRequest.template(parseTemplate(query));
+        BytesReference bytesRef = new BytesArray(templateString);
+        searchRequest.templateSource(bytesRef);
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
     }
@@ -265,7 +268,8 @@ public class TemplateQueryIT extends ESIntegTestCase {
         String templateString = "{"
                 + "  \"template\" : \"{ {{#use_size}} \\\"size\\\": \\\"{{size}}\\\", {{/use_size}} \\\"query\\\":{\\\"match_all\\\":{}}}\","
                 + "  \"params\":{" + "    \"size\": 1," + "    \"use_size\": true" + "  }" + "}";
-        searchRequest.template(parseTemplate(templateString));
+        BytesReference bytesRef = new BytesArray(templateString);
+        searchRequest.templateSource(bytesRef);
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
     }
@@ -281,7 +285,8 @@ public class TemplateQueryIT extends ESIntegTestCase {
         String templateString = "{"
                 + "  \"inline\" : \"{ \\\"query\\\":{\\\"match_all\\\":{}} {{#use_size}}, \\\"size\\\": \\\"{{size}}\\\" {{/use_size}} }\","
                 + "  \"params\":{" + "    \"size\": 1," + "    \"use_size\": true" + "  }" + "}";
-        searchRequest.template(parseTemplate(templateString));
+        BytesReference bytesRef = new BytesArray(templateString);
+        searchRequest.templateSource(bytesRef);
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
     }
@@ -446,15 +451,12 @@ public class TemplateQueryIT extends ESIntegTestCase {
                 .execute().actionGet();
         assertHitCount(sr, 1);
 
-        // "{\"template\": {\"id\": \"3\",\"params\" : {\"fieldParam\" : \"foo\"}}}";
-        Map<String, Object> params = new HashMap<>();
-        params.put("fieldParam", "foo");
-        TemplateQueryBuilder templateQuery = new TemplateQueryBuilder(new Template("3", ScriptType.INDEXED, null, null, params));
-        sr = client().prepareSearch().setQuery(templateQuery).get();
+        String query = "{\"template\": {\"id\": \"3\",\"params\" : {\"fieldParam\" : \"foo\"}}}";
+        sr = client().prepareSearch().setQuery(query).get();
         assertHitCount(sr, 4);
 
-        templateQuery = new TemplateQueryBuilder(new Template("/mustache/3", ScriptType.INDEXED, null, null, params));
-        sr = client().prepareSearch().setQuery(templateQuery).get();
+        query = "{\"template\": {\"id\": \"/mustache/3\",\"params\" : {\"fieldParam\" : \"foo\"}}}";
+        sr = client().prepareSearch().setQuery(query).get();
         assertHitCount(sr, 4);
     }
 
@@ -469,7 +471,7 @@ public class TemplateQueryIT extends ESIntegTestCase {
 
         int iterations = randomIntBetween(2, 11);
         for (int i = 1; i < iterations; i++) {
-            PutIndexedScriptResponse scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "git01",
+            PutIndexedScriptResponse scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "git01", 
                     "{\"query\": {\"match\": {\"searchtext\": {\"query\": \"{{P_Keyword1}}\",\"type\": \"ooophrase_prefix\"}}}}").get();
             assertEquals(i * 2 - 1, scriptResponse.getVersion());
 
@@ -505,7 +507,7 @@ public class TemplateQueryIT extends ESIntegTestCase {
         }
     }
 
-
+    
     @Test
     public void testIndexedTemplateWithArray() throws Exception {
       createIndex(ScriptService.SCRIPT_INDEX);
diff --git a/core/src/test/java/org/elasticsearch/index/query/WrapperQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/WrapperQueryBuilderTests.java
index fff8de7..ea04b79 100644
--- a/core/src/test/java/org/elasticsearch/index/query/WrapperQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/WrapperQueryBuilderTests.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.index.query;
 
 import org.apache.lucene.search.Query;
+import org.elasticsearch.action.support.ToXContentToBytes;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentFactory;
@@ -44,9 +45,9 @@ public class WrapperQueryBuilderTests extends AbstractQueryTestCase<WrapperQuery
             case 0:
                 return new WrapperQueryBuilder(wrappedQuery.toString());
             case 1:
-                return new WrapperQueryBuilder(wrappedQuery.buildAsBytes().toBytes());
+                return new WrapperQueryBuilder(((ToXContentToBytes)wrappedQuery).buildAsBytes().toBytes());
             case 2:
-                return new WrapperQueryBuilder(wrappedQuery.buildAsBytes());
+                return new WrapperQueryBuilder(((ToXContentToBytes)wrappedQuery).buildAsBytes());
             default:
                 throw new UnsupportedOperationException();
         }
diff --git a/core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilderTests.java
index e1286c2..58193ab 100644
--- a/core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilderTests.java
@@ -35,14 +35,14 @@ import org.elasticsearch.index.query.functionscore.lin.LinearDecayFunctionBuilde
 import org.elasticsearch.index.query.functionscore.random.RandomScoreFunctionBuilder;
 import org.elasticsearch.index.query.functionscore.script.ScriptScoreFunctionBuilder;
 import org.elasticsearch.index.query.functionscore.weight.WeightBuilder;
+import org.elasticsearch.script.MockScriptEngine;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.expression.ExpressionScriptEngineService;
 import org.elasticsearch.search.MultiValueMode;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.HashMap;
+import java.util.Collections;
 import java.util.Map;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
@@ -140,16 +140,9 @@ public class FunctionScoreQueryBuilderTests extends AbstractQueryTestCase<Functi
                 functionBuilder = fieldValueFactorFunctionBuilder;
                 break;
             case 2:
-                String script;
-                Map<String, Object> params = null;
-                if (randomBoolean()) {
-                    script = "5 * 2 > param";
-                    params = new HashMap<>();
-                    params.put("param", 1);
-                } else {
-                    script = "5 * 2 > 2";
-                }
-                functionBuilder = new ScriptScoreFunctionBuilder(new Script(script, ScriptService.ScriptType.INLINE, ExpressionScriptEngineService.NAME, params));
+                String script = "5";
+                Map<String, Object> params = Collections.emptyMap();
+                functionBuilder = new ScriptScoreFunctionBuilder(new Script(script, ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, params));
                 break;
             case 3:
                 RandomScoreFunctionBuilder randomScoreFunctionBuilder = new RandomScoreFunctionBuilder();
@@ -420,7 +413,7 @@ public class FunctionScoreQueryBuilderTests extends AbstractQueryTestCase<Functi
         String functionScoreQuery = "{\n" +
                 "    \"function_score\": {\n" +
                 "      \"script_score\": {\n" +
-                "        \"script\": \"_index['text']['foo'].tf()\"\n" +
+                "        \"script\": \"5\"\n" +
                 "      },\n" +
                 "      \"weight\": 2\n" +
                 "    }\n" +
diff --git a/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java b/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
index 6038de4..432c833 100644
--- a/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
+++ b/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
@@ -27,10 +27,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.AbstractQueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.QueryParser;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.*;
 import org.elasticsearch.indices.IndicesModule;
 import org.elasticsearch.plugins.Plugin;
 
@@ -66,18 +63,18 @@ public class DummyQueryParserPlugin extends Plugin {
         }
 
         @Override
-        public String getWriteableName() {
-            return NAME;
+        protected DummyQueryBuilder doReadFrom(StreamInput in) throws IOException {
+            return null;
         }
 
         @Override
-        protected DummyQueryBuilder doReadFrom(StreamInput in) throws IOException {
-            return new DummyQueryBuilder();
+        protected void doWriteTo(StreamOutput out) throws IOException {
+
         }
 
         @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            // Do Nothing
+        protected boolean doEquals(DummyQueryBuilder other) {
+            return false;
         }
 
         @Override
@@ -86,8 +83,8 @@ public class DummyQueryParserPlugin extends Plugin {
         }
 
         @Override
-        protected boolean doEquals(DummyQueryBuilder other) {
-            return true;
+        public String getWriteableName() {
+            return NAME;
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
index 017ce5b..5176dc2 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
@@ -46,6 +46,8 @@ import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.env.Environment;
@@ -53,6 +55,7 @@ import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.env.ShardLock;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.engine.Engine;
+import org.elasticsearch.index.flush.FlushStats;
 import org.elasticsearch.index.indexing.IndexingOperationListener;
 import org.elasticsearch.index.indexing.ShardIndexingService;
 import org.elasticsearch.index.mapper.Mapping;
@@ -77,6 +80,8 @@ import java.nio.file.StandardCopyOption;
 import java.util.Arrays;
 import java.util.HashSet;
 import java.util.Set;
+import java.util.concurrent.BrokenBarrierException;
+import java.util.concurrent.CyclicBarrier;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.atomic.AtomicBoolean;
 
@@ -684,4 +689,82 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         assertTrue(postIndexWithExceptionCalled.get());
     }
 
+    public void testMaybeFlush() throws Exception {
+        createIndex("test", settingsBuilder().put(TranslogConfig.INDEX_TRANSLOG_DURABILITY, Translog.Durabilty.REQUEST).build());
+        ensureGreen();
+        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
+        IndexService test = indicesService.indexService("test");
+        IndexShard shard = test.shard(0);
+        assertFalse(shard.shouldFlush());
+        client().admin().indices().prepareUpdateSettings("test").setSettings(settingsBuilder().put(IndexShard.INDEX_TRANSLOG_FLUSH_THRESHOLD_OPS, 1).build()).get();
+        client().prepareIndex("test", "test", "0").setSource("{}").setRefresh(randomBoolean()).get();
+        assertFalse(shard.shouldFlush());
+        ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, new ParseContext.Document(), new BytesArray(new byte[]{1}), null);
+        Engine.Index index = new Engine.Index(new Term("_uid", "1"), doc);
+        shard.index(index);
+        assertTrue(shard.shouldFlush());
+        assertEquals(2, shard.engine().getTranslog().totalOperations());
+        client().prepareIndex("test", "test", "2").setSource("{}").setRefresh(randomBoolean()).get();
+        assertBusy(() -> { // this is async
+            assertFalse(shard.shouldFlush());
+        });
+        assertEquals(0, shard.engine().getTranslog().totalOperations());
+        shard.engine().getTranslog().sync();
+        long size = shard.engine().getTranslog().sizeInBytes();
+        logger.info("--> current translog size: [{}] num_ops [{}] generation [{}]", shard.engine().getTranslog().sizeInBytes(), shard.engine().getTranslog().totalOperations(), shard.engine().getTranslog().getGeneration());
+        client().admin().indices().prepareUpdateSettings("test").setSettings(settingsBuilder().put(IndexShard.INDEX_TRANSLOG_FLUSH_THRESHOLD_OPS, 1000)
+                .put(IndexShard.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE, new ByteSizeValue(size, ByteSizeUnit.BYTES))
+                .build()).get();
+        client().prepareDelete("test", "test", "2").get();
+        logger.info("--> translog size after delete: [{}] num_ops [{}] generation [{}]", shard.engine().getTranslog().sizeInBytes(), shard.engine().getTranslog().totalOperations(), shard.engine().getTranslog().getGeneration());
+        assertBusy(() -> { // this is async
+            logger.info("--> translog size on iter  : [{}] num_ops [{}] generation [{}]", shard.engine().getTranslog().sizeInBytes(), shard.engine().getTranslog().totalOperations(), shard.engine().getTranslog().getGeneration());
+            assertFalse(shard.shouldFlush());
+        });
+        assertEquals(0, shard.engine().getTranslog().totalOperations());
+    }
+
+    public void testStressMaybeFlush() throws Exception {
+        createIndex("test");
+        ensureGreen();
+        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
+        IndexService test = indicesService.indexService("test");
+        final IndexShard shard = test.shard(0);
+        assertFalse(shard.shouldFlush());
+        client().admin().indices().prepareUpdateSettings("test").setSettings(settingsBuilder().put(IndexShard.INDEX_TRANSLOG_FLUSH_THRESHOLD_OPS, 1).build()).get();
+        client().prepareIndex("test", "test", "0").setSource("{}").setRefresh(randomBoolean()).get();
+        assertFalse(shard.shouldFlush());
+        final AtomicBoolean running = new AtomicBoolean(true);
+        final int numThreads = randomIntBetween(2, 4);
+        Thread[] threads = new Thread[numThreads];
+        CyclicBarrier barrier = new CyclicBarrier(numThreads + 1);
+        for (int i = 0; i < threads.length; i++) {
+            threads[i] = new Thread() {
+                public void run() {
+                    try {
+                        barrier.await();
+                    } catch (InterruptedException | BrokenBarrierException e) {
+                        throw new RuntimeException(e);
+                    }
+                    while (running.get()) {
+                        shard.maybeFlush();
+                    }
+                }
+            };
+            threads[i].start();
+        }
+        barrier.await();
+        FlushStats flushStats = shard.flushStats();
+        long total = flushStats.getTotal();
+        client().prepareIndex("test", "test", "1").setSource("{}").get();
+        assertBusy(() -> {
+            assertEquals(total + 1, shard.flushStats().getTotal());
+        });
+        running.set(false);
+        for (int i = 0; i < threads.length; i++) {
+            threads[i].join();
+        }
+        assertEquals(total + 1, shard.flushStats().getTotal());
+    }
+
 }
diff --git a/core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java b/core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java
index 354903f..77b7c37 100644
--- a/core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java
+++ b/core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java
@@ -62,7 +62,6 @@ import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.IndexShardState;
 import org.elasticsearch.index.shard.MergePolicyConfig;
 import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.translog.TranslogService;
 import org.elasticsearch.indices.IndicesLifecycle;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.indices.recovery.RecoveryFileChunkRequest;
@@ -157,7 +156,7 @@ public class CorruptedFileIT extends ESIntegTestCase {
                         .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, "1")
                         .put(MergePolicyConfig.INDEX_MERGE_ENABLED, false)
                         .put(MockFSDirectoryService.CHECK_INDEX_ON_CLOSE, false) // no checkindex - we corrupt shards on purpose
-                        .put(TranslogService.INDEX_TRANSLOG_DISABLE_FLUSH, true) // no translog based flush - it might change the .liv / segments.N files
+                        .put(IndexShard.INDEX_TRANSLOG_DISABLE_FLUSH, true) // no translog based flush - it might change the .liv / segments.N files
                         .put("indices.recovery.concurrent_streams", 10)
         ));
         ensureGreen();
@@ -263,7 +262,7 @@ public class CorruptedFileIT extends ESIntegTestCase {
                         .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, "0")
                         .put(MergePolicyConfig.INDEX_MERGE_ENABLED, false)
                         .put(MockFSDirectoryService.CHECK_INDEX_ON_CLOSE, false) // no checkindex - we corrupt shards on purpose
-                        .put(TranslogService.INDEX_TRANSLOG_DISABLE_FLUSH, true) // no translog based flush - it might change the .liv / segments.N files
+                        .put(IndexShard.INDEX_TRANSLOG_DISABLE_FLUSH, true) // no translog based flush - it might change the .liv / segments.N files
                         .put("indices.recovery.concurrent_streams", 10)
         ));
         ensureGreen();
@@ -491,7 +490,7 @@ public class CorruptedFileIT extends ESIntegTestCase {
                         .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, "0") // no replicas for this test
                         .put(MergePolicyConfig.INDEX_MERGE_ENABLED, false)
                         .put(MockFSDirectoryService.CHECK_INDEX_ON_CLOSE, false) // no checkindex - we corrupt shards on purpose
-                        .put(TranslogService.INDEX_TRANSLOG_DISABLE_FLUSH, true) // no translog based flush - it might change the .liv / segments.N files
+                        .put(IndexShard.INDEX_TRANSLOG_DISABLE_FLUSH, true) // no translog based flush - it might change the .liv / segments.N files
                         .put("indices.recovery.concurrent_streams", 10)
         ));
         ensureGreen();
@@ -547,7 +546,7 @@ public class CorruptedFileIT extends ESIntegTestCase {
                         .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, cluster().numDataNodes() - 1)
                         .put(MergePolicyConfig.INDEX_MERGE_ENABLED, false)
                         .put(MockFSDirectoryService.CHECK_INDEX_ON_CLOSE, false) // no checkindex - we corrupt shards on purpose
-                        .put(TranslogService.INDEX_TRANSLOG_DISABLE_FLUSH, true) // no translog based flush - it might change the .liv / segments.N files
+                        .put(IndexShard.INDEX_TRANSLOG_DISABLE_FLUSH, true) // no translog based flush - it might change the .liv / segments.N files
                         .put("indices.recovery.concurrent_streams", 10)
         ));
         ensureGreen();
diff --git a/core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java b/core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java
index d7d3673..9cf2034 100644
--- a/core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java
@@ -48,10 +48,10 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.suggest.SuggestRequestBuilder;
 import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.IndexNotFoundException;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.suggest.SuggestBuilders;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -61,9 +61,7 @@ import static org.elasticsearch.action.percolate.PercolateSourceBuilder.docBuild
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.*;
 
 public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
 
@@ -510,7 +508,7 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
                 .setIndicesOptions(IndicesOptions.lenientExpandOpen())
                 .execute().actionGet();
         assertHitCount(response, 0l);
-
+        
         //you should still be able to run empty searches without things blowing up
         response  = client().prepareSearch()
                 .setIndicesOptions(IndicesOptions.lenientExpandOpen())
@@ -615,7 +613,7 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareExists("bar").get().isExists(), equalTo(false));
         assertThat(client().admin().indices().prepareExists("barbaz").get().isExists(), equalTo(false));
     }
-
+    
     @Test
     public void testPutWarmer() throws Exception {
         createIndex("foobar");
@@ -624,26 +622,26 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareGetWarmers("foobar").setWarmers("warmer1").get().getWarmers().size(), equalTo(1));
 
     }
-
+    
     @Test
     public void testPutWarmer_wildcard() throws Exception {
         createIndex("foo", "foobar", "bar", "barbaz");
         ensureYellow();
 
         verify(client().admin().indices().preparePutWarmer("warmer1").setSearchRequest(client().prepareSearch().setIndices("foo*").setQuery(QueryBuilders.matchAllQuery())), false);
-
+        
         assertThat(client().admin().indices().prepareGetWarmers("foo").setWarmers("warmer1").get().getWarmers().size(), equalTo(1));
         assertThat(client().admin().indices().prepareGetWarmers("foobar").setWarmers("warmer1").get().getWarmers().size(), equalTo(1));
         assertThat(client().admin().indices().prepareGetWarmers("bar").setWarmers("warmer1").get().getWarmers().size(), equalTo(0));
         assertThat(client().admin().indices().prepareGetWarmers("barbaz").setWarmers("warmer1").get().getWarmers().size(), equalTo(0));
 
         verify(client().admin().indices().preparePutWarmer("warmer2").setSearchRequest(client().prepareSearch().setIndices().setQuery(QueryBuilders.matchAllQuery())), false);
-
+        
         assertThat(client().admin().indices().prepareGetWarmers("foo").setWarmers("warmer2").get().getWarmers().size(), equalTo(1));
         assertThat(client().admin().indices().prepareGetWarmers("foobar").setWarmers("warmer2").get().getWarmers().size(), equalTo(1));
         assertThat(client().admin().indices().prepareGetWarmers("bar").setWarmers("warmer2").get().getWarmers().size(), equalTo(1));
         assertThat(client().admin().indices().prepareGetWarmers("barbaz").setWarmers("warmer2").get().getWarmers().size(), equalTo(1));
-
+        
     }
 
     @Test
@@ -654,7 +652,7 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareAliasesExist("foobar_alias").setIndices("foobar").get().exists(), equalTo(true));
 
     }
-
+    
     @Test
     public void testPutAlias_wildcard() throws Exception {
         createIndex("foo", "foobar", "bar", "barbaz");
@@ -671,13 +669,14 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareAliasesExist("foobar_alias").setIndices("foobar").get().exists(), equalTo(true));
         assertThat(client().admin().indices().prepareAliasesExist("foobar_alias").setIndices("bar").get().exists(), equalTo(true));
         assertThat(client().admin().indices().prepareAliasesExist("foobar_alias").setIndices("barbaz").get().exists(), equalTo(true));
-
+        
     }
-
+    
     @Test
     public void testDeleteWarmer() throws Exception {
-        SearchSourceBuilder source = new SearchSourceBuilder();
-        IndexWarmersMetaData.Entry entry = new IndexWarmersMetaData.Entry("test1", new String[] { "typ1" }, false, new IndexWarmersMetaData.SearchSource(source));
+        IndexWarmersMetaData.Entry entry = new IndexWarmersMetaData.Entry(
+                "test1", new String[]{"typ1"}, false, new BytesArray("{\"query\" : { \"match_all\" : {}}}")
+        );
         assertAcked(prepareCreate("foobar").addCustom(new IndexWarmersMetaData(entry)));
         ensureYellow();
 
@@ -691,8 +690,9 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
     public void testDeleteWarmer_wildcard() throws Exception {
         verify(client().admin().indices().prepareDeleteWarmer().setIndices("_all").setNames("test1"), true);
 
-        SearchSourceBuilder source = new SearchSourceBuilder();
-        IndexWarmersMetaData.Entry entry = new IndexWarmersMetaData.Entry("test1", new String[] { "type1" }, false, new IndexWarmersMetaData.SearchSource(source));
+        IndexWarmersMetaData.Entry entry = new IndexWarmersMetaData.Entry(
+                "test1", new String[]{"type1"}, false, new BytesArray("{\"query\" : { \"match_all\" : {}}}")
+        );
         assertAcked(prepareCreate("foo").addCustom(new IndexWarmersMetaData(entry)));
         assertAcked(prepareCreate("foobar").addCustom(new IndexWarmersMetaData(entry)));
         assertAcked(prepareCreate("bar").addCustom(new IndexWarmersMetaData(entry)));
@@ -737,7 +737,7 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareGetMappings("foobar").get().mappings().get("foobar").get("type3"), notNullValue());
         assertThat(client().admin().indices().prepareGetMappings("bar").get().mappings().get("bar").get("type3"), notNullValue());
         assertThat(client().admin().indices().prepareGetMappings("barbaz").get().mappings().get("barbaz").get("type3"), notNullValue());
-
+        
 
         verify(client().admin().indices().preparePutMapping("c*").setType("type1").setSource("field", "type=string"), true);
 
@@ -883,7 +883,7 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
     private static void verify(ActionRequestBuilder requestBuilder, boolean fail) {
         verify(requestBuilder, fail, 0);
     }
-
+    
     private static void verify(ActionRequestBuilder requestBuilder, boolean fail, long expectedCount) {
         if (fail) {
             if (requestBuilder instanceof MultiSearchRequestBuilder) {
diff --git a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java b/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java
index 36ad646..6c0f041 100644
--- a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java
@@ -24,13 +24,14 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.concurrent.EsExecutors;
 import org.elasticsearch.index.engine.EngineConfig;
-import org.elasticsearch.index.engine.SegmentsStats;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
+import java.util.concurrent.ExecutionException;
+
 
 @ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)
 public class IndexingMemoryControllerIT extends ESIntegTestCase {
@@ -77,7 +78,7 @@ public class IndexingMemoryControllerIT extends ESIntegTestCase {
     }
 
     @Test
-    public void testIndexBufferSizeUpdateInactiveShard() throws InterruptedException {
+    public void testIndexBufferSizeUpdateInactiveShard() throws InterruptedException, ExecutionException {
 
         createNode(Settings.builder().put(IndexingMemoryController.SHARD_INACTIVE_TIME_SETTING, "100ms").build());
 
@@ -86,6 +87,12 @@ public class IndexingMemoryControllerIT extends ESIntegTestCase {
         ensureGreen();
 
         final IndexShard shard1 = internalCluster().getInstance(IndicesService.class).indexService("test1").shard(0);
+
+        if (randomBoolean()) {
+            logger.info("--> indexing some pending operations");
+            indexRandom(false, client().prepareIndex("test1", "type", "0").setSource("f", "0"));
+        }
+
         boolean success = awaitBusy(() -> shard1.engine().config().getIndexingBufferSize().bytes() == EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER.bytes());
         if (!success) {
             fail("failed to update shard indexing buffer size due to inactive state. expected [" + EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER + "] got [" +
@@ -97,12 +104,15 @@ public class IndexingMemoryControllerIT extends ESIntegTestCase {
 
         success = awaitBusy(() -> shard1.engine().config().getIndexingBufferSize().bytes() > EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER.bytes());
         if (!success) {
-            fail("failed to update shard indexing buffer size due to inactive state. expected something larger then [" + EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER + "] got [" +
+            fail("failed to update shard indexing buffer size due to active state. expected something larger then [" + EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER + "] got [" +
                             shard1.engine().config().getIndexingBufferSize().bytes() + "]"
             );
         }
 
-        flush(); // clean translogs
+        if (randomBoolean()) {
+            logger.info("--> flushing translogs");
+            flush(); // clean translogs
+        }
 
         success = awaitBusy(() -> shard1.engine().config().getIndexingBufferSize().bytes() == EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER.bytes());
         if (!success) {
diff --git a/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java b/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
index d166a15..64465dc 100644
--- a/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
+++ b/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
@@ -592,6 +592,7 @@ public class PluginManagerIT extends ESIntegTestCase {
         PluginManager.checkForOfficialPlugins("analysis-stempel");
         PluginManager.checkForOfficialPlugins("cloud-gce");
         PluginManager.checkForOfficialPlugins("delete-by-query");
+        PluginManager.checkForOfficialPlugins("lang-expression");
         PluginManager.checkForOfficialPlugins("lang-javascript");
         PluginManager.checkForOfficialPlugins("lang-python");
         PluginManager.checkForOfficialPlugins("mapper-murmur3");
diff --git a/core/src/test/java/org/elasticsearch/script/CustomScriptContextIT.java b/core/src/test/java/org/elasticsearch/script/CustomScriptContextIT.java
deleted file mode 100644
index fae9e44..0000000
--- a/core/src/test/java/org/elasticsearch/script/CustomScriptContextIT.java
+++ /dev/null
@@ -1,146 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script;
-
-import com.google.common.collect.ImmutableSet;
-import org.elasticsearch.common.ContextAndHeaderHolder;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.script.expression.ExpressionScriptEngineService;
-import org.elasticsearch.script.groovy.GroovyScriptEngineService;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Test;
-
-import java.util.Collection;
-
-import static org.hamcrest.CoreMatchers.containsString;
-import static org.hamcrest.CoreMatchers.notNullValue;
-
-public class CustomScriptContextIT extends ESIntegTestCase {
-
-    private static final ImmutableSet<String> LANG_SET = ImmutableSet.of(GroovyScriptEngineService.NAME, MustacheScriptEngineService.NAME, ExpressionScriptEngineService.NAME);
-
-    private static final String PLUGIN_NAME = "testplugin";
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        return Settings.builder().put(super.nodeSettings(nodeOrdinal))
-                .put("script." + PLUGIN_NAME + "_custom_globally_disabled_op", "off")
-                .put("script.engine.expression.inline." + PLUGIN_NAME + "_custom_exp_disabled_op", "off")
-                        .build();
-    }
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(CustomScriptContextPlugin.class);
-    }
-
-    @Test
-    public void testCustomScriptContextsSettings() {
-        ContextAndHeaderHolder contextAndHeaders = new ContextAndHeaderHolder();
-
-        ScriptService scriptService = internalCluster().getInstance(ScriptService.class);
-        for (String lang : LANG_SET) {
-            for (ScriptService.ScriptType scriptType : ScriptService.ScriptType.values()) {
-                try {
-                    scriptService.compile(new Script("test", scriptType, lang, null), new ScriptContext.Plugin(PLUGIN_NAME,
-                            "custom_globally_disabled_op"), contextAndHeaders);
-                    fail("script compilation should have been rejected");
-                } catch(ScriptException e) {
-                    assertThat(e.getMessage(), containsString("scripts of type [" + scriptType + "], operation [" + PLUGIN_NAME + "_custom_globally_disabled_op] and lang [" + lang + "] are disabled"));
-                }
-            }
-        }
-
-        try {
-            scriptService.compile(new Script("1", ScriptService.ScriptType.INLINE, "expression", null), new ScriptContext.Plugin(
-                    PLUGIN_NAME, "custom_exp_disabled_op"), contextAndHeaders);
-            fail("script compilation should have been rejected");
-        } catch(ScriptException e) {
-            assertThat(e.getMessage(), containsString("scripts of type [inline], operation [" + PLUGIN_NAME + "_custom_exp_disabled_op] and lang [expression] are disabled"));
-        }
-
-        CompiledScript compiledScript = scriptService.compile(new Script("1", ScriptService.ScriptType.INLINE, "expression", null),
-                randomFrom(new ScriptContext[] { ScriptContext.Standard.AGGS, ScriptContext.Standard.SEARCH }), contextAndHeaders);
-        assertThat(compiledScript, notNullValue());
-
-        compiledScript = scriptService.compile(new Script("1", ScriptService.ScriptType.INLINE, "mustache", null),
-                new ScriptContext.Plugin(PLUGIN_NAME, "custom_exp_disabled_op"), contextAndHeaders);
-        assertThat(compiledScript, notNullValue());
-
-        for (String lang : LANG_SET) {
-            compiledScript = scriptService.compile(new Script("1", ScriptService.ScriptType.INLINE, lang, null), new ScriptContext.Plugin(
-                    PLUGIN_NAME, "custom_op"), contextAndHeaders);
-            assertThat(compiledScript, notNullValue());
-        }
-    }
-
-    @Test
-    public void testCompileNonRegisteredPluginContext() {
-        ContextAndHeaderHolder contextAndHeaders = new ContextAndHeaderHolder();
-        ScriptService scriptService = internalCluster().getInstance(ScriptService.class);
-        try {
-            scriptService.compile(
-                    new Script("test", randomFrom(ScriptService.ScriptType.values()), randomFrom(LANG_SET.toArray(new String[LANG_SET
-                            .size()])), null), new ScriptContext.Plugin("test", "unknown"), contextAndHeaders);
-            fail("script compilation should have been rejected");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("script context [test_unknown] not supported"));
-        }
-    }
-
-    @Test
-    public void testCompileNonRegisteredScriptContext() {
-        ContextAndHeaderHolder contextAndHeaders = new ContextAndHeaderHolder();
-        ScriptService scriptService = internalCluster().getInstance(ScriptService.class);
-        try {
-            scriptService.compile(
-                    new Script("test", randomFrom(ScriptService.ScriptType.values()), randomFrom(LANG_SET.toArray(new String[LANG_SET
-                            .size()])), null), new ScriptContext() {
-                @Override
-                public String getKey() {
-                    return "test";
-                }
-                    }, contextAndHeaders);
-            fail("script compilation should have been rejected");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("script context [test] not supported"));
-        }
-    }
-
-    public static class CustomScriptContextPlugin extends Plugin {
-        @Override
-        public String name() {
-            return "custom_script_context_plugin";
-        }
-
-        @Override
-        public String description() {
-            return "Custom script context plugin";
-        }
-
-        public void onModule(ScriptModule scriptModule) {
-                scriptModule.registerScriptContext(new ScriptContext.Plugin(PLUGIN_NAME, "custom_op"));
-                scriptModule.registerScriptContext(new ScriptContext.Plugin(PLUGIN_NAME, "custom_exp_disabled_op"));
-                scriptModule.registerScriptContext(new ScriptContext.Plugin(PLUGIN_NAME, "custom_globally_disabled_op"));
-            }
-        }
-}
diff --git a/core/src/test/java/org/elasticsearch/script/FileScriptTests.java b/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
new file mode 100644
index 0000000..daefc20
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
@@ -0,0 +1,84 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.script;
+
+import org.elasticsearch.common.ContextAndHeaderHolder;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Set;
+
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.hamcrest.Matchers.containsString;
+
+// TODO: these really should just be part of ScriptService tests, there is nothing special about them
+public class FileScriptTests extends ESTestCase {
+
+    ScriptService makeScriptService(Settings settings) throws Exception {
+        Path homeDir = createTempDir();
+        Path scriptsDir = homeDir.resolve("config").resolve("scripts");
+        Files.createDirectories(scriptsDir);
+        Path mockscript = scriptsDir.resolve("script1.mockscript");
+        Files.write(mockscript, "1".getBytes("UTF-8"));
+        settings = Settings.builder()
+            .put("path.home", homeDir)
+                // no file watching, so we don't need a ResourceWatcherService
+            .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING, false)
+            .put(settings)
+            .build();
+        Set<ScriptEngineService> engines = new HashSet<>(Collections.singletonList(new MockScriptEngine()));
+        return new ScriptService(settings, new Environment(settings), engines, null, new ScriptContextRegistry(Collections.emptyList()));
+    }
+
+    public void testFileScriptFound() throws Exception {
+        ContextAndHeaderHolder contextAndHeaders = new ContextAndHeaderHolder();
+        Settings settings = Settings.builder()
+            .put("script.engine." + MockScriptEngine.NAME + ".file.aggs", false).build();
+        ScriptService scriptService = makeScriptService(settings);
+        Script script = new Script("script1", ScriptService.ScriptType.FILE, MockScriptEngine.NAME, null);
+        assertNotNull(scriptService.compile(script, ScriptContext.Standard.SEARCH, contextAndHeaders));
+    }
+
+    public void testAllOpsDisabled() throws Exception {
+        ContextAndHeaderHolder contextAndHeaders = new ContextAndHeaderHolder();
+        Settings settings = Settings.builder()
+            .put("script.engine." + MockScriptEngine.NAME + ".file.aggs", false)
+            .put("script.engine." + MockScriptEngine.NAME + ".file.search", false)
+            .put("script.engine." + MockScriptEngine.NAME + ".file.mapping", false)
+            .put("script.engine." + MockScriptEngine.NAME + ".file.update", false).build();
+        ScriptService scriptService = makeScriptService(settings);
+        Script script = new Script("script1", ScriptService.ScriptType.FILE, MockScriptEngine.NAME, null);
+        for (ScriptContext context : ScriptContext.Standard.values()) {
+            try {
+                scriptService.compile(script, context, contextAndHeaders);
+                fail(context.getKey() + " script should have been rejected");
+            } catch(Exception e) {
+                assertTrue(e.getMessage(), e.getMessage().contains("scripts of type [file], operation [" + context.getKey() + "] and lang [" + MockScriptEngine.NAME + "] are disabled"));
+            }
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/script/GroovyScriptIT.java b/core/src/test/java/org/elasticsearch/script/GroovyScriptIT.java
index a855f92..320b000 100644
--- a/core/src/test/java/org/elasticsearch/script/GroovyScriptIT.java
+++ b/core/src/test/java/org/elasticsearch/script/GroovyScriptIT.java
@@ -22,26 +22,19 @@ package org.elasticsearch.script;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
-import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
 import java.util.ArrayList;
 import java.util.List;
 
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.functionScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.scriptQuery;
+import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.scriptFunction;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertOrderedSearchHits;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
 import static org.hamcrest.Matchers.equalTo;
 
 /**
@@ -54,16 +47,16 @@ public class GroovyScriptIT extends ESIntegTestCase {
         client().prepareIndex("test", "doc", "1").setSource("foo", 5).setRefresh(true).get();
 
         // Test that something that would usually be a BigDecimal is transformed into a Double
-        assertScript("def n = 1.23; assert n instanceof Double; return n;");
-        assertScript("def n = 1.23G; assert n instanceof Double; return n;");
-        assertScript("def n = BigDecimal.ONE; assert n instanceof BigDecimal; return n;");
+        assertScript("def n = 1.23; assert n instanceof Double;");
+        assertScript("def n = 1.23G; assert n instanceof Double;");
+        assertScript("def n = BigDecimal.ONE; assert n instanceof BigDecimal;");
     }
 
-    public void assertScript(String scriptString) {
-        Script script = new Script(scriptString, ScriptType.INLINE, "groovy", null);
+    public void assertScript(String script) {
         SearchResponse resp = client().prepareSearch("test")
-                .setSource(new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).sort(SortBuilders.scriptSort(script, "number")))
-                .get();
+                .setSource(new BytesArray("{\"query\": {\"match_all\": {}}," +
+                        "\"sort\":{\"_script\": {\"script\": \""+ script +
+                        "; 1\", \"type\": \"number\", \"lang\": \"groovy\"}}}")).get();
         assertNoFailures(resp);
     }
 
@@ -117,7 +110,7 @@ public class GroovyScriptIT extends ESIntegTestCase {
         assertNoFailures(resp);
         assertOrderedSearchHits(resp, "3", "2", "1");
     }
-
+    
     public void testScoreAccess() {
         client().prepareIndex("test", "doc", "1").setSource("foo", "quick brow fox jumped over the lazy dog", "bar", 1).get();
         client().prepareIndex("test", "doc", "2").setSource("foo", "fast jumping spiders", "bar", 2).get();
diff --git a/core/src/test/java/org/elasticsearch/script/GroovySecurityIT.java b/core/src/test/java/org/elasticsearch/script/GroovySecurityIT.java
index 2ca83c1..5ec89a6 100644
--- a/core/src/test/java/org/elasticsearch/script/GroovySecurityIT.java
+++ b/core/src/test/java/org/elasticsearch/script/GroovySecurityIT.java
@@ -22,12 +22,9 @@ package org.elasticsearch.script;
 import org.apache.lucene.util.Constants;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.ShardSearchFailure;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.groovy.GroovyScriptExecutionException;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -43,7 +40,7 @@ import static org.hamcrest.CoreMatchers.instanceOf;
  */
 @ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)
 public class GroovySecurityIT extends ESIntegTestCase {
-
+    
     @Override
     public void setUp() throws Exception {
         super.setUp();
@@ -73,7 +70,7 @@ public class GroovySecurityIT extends ESIntegTestCase {
         // Ranges
         assertSuccess("def range = 1..doc['foo'].value; def v = range.get(0)");
         // Maps
-        assertSuccess("def v = doc['foo'].value; def m = [:]; m.put(\"value\", v)");
+        assertSuccess("def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)");
         // Times
         assertSuccess("def t = Instant.now().getMillis()");
         // GroovyCollections
@@ -81,42 +78,40 @@ public class GroovySecurityIT extends ESIntegTestCase {
 
         // Fail cases:
         // AccessControlException[access denied ("java.io.FilePermission" "<<ALL FILES>>" "execute")]
-        assertFailure("pr = Runtime.getRuntime().exec(\"touch /tmp/gotcha\"); pr.waitFor()");
+        assertFailure("pr = Runtime.getRuntime().exec(\\\"touch /tmp/gotcha\\\"); pr.waitFor()");
 
         // AccessControlException[access denied ("java.lang.RuntimePermission" "accessClassInPackage.sun.reflect")]
-        assertFailure("d = new DateTime(); d.getClass().getDeclaredMethod(\"year\").setAccessible(true)");
-        assertFailure("d = new DateTime(); d.\"${'get' + 'Class'}\"()." +
-                        "\"${'getDeclared' + 'Method'}\"(\"year\").\"${'set' + 'Accessible'}\"(false)");
-        assertFailure("Class.forName(\"org.joda.time.DateTime\").getDeclaredMethod(\"year\").setAccessible(true)");
+        assertFailure("d = new DateTime(); d.getClass().getDeclaredMethod(\\\"year\\\").setAccessible(true)");
+        assertFailure("d = new DateTime(); d.\\\"${'get' + 'Class'}\\\"()." +
+                        "\\\"${'getDeclared' + 'Method'}\\\"(\\\"year\\\").\\\"${'set' + 'Accessible'}\\\"(false)");
+        assertFailure("Class.forName(\\\"org.joda.time.DateTime\\\").getDeclaredMethod(\\\"year\\\").setAccessible(true)");
 
         // AccessControlException[access denied ("groovy.security.GroovyCodeSourcePermission" "/groovy/shell")]
         assertFailure("Eval.me('2 + 2')");
         assertFailure("Eval.x(5, 'x + 2')");
 
         // AccessControlException[access denied ("java.lang.RuntimePermission" "accessDeclaredMembers")]
-        assertFailure("d = new Date(); java.lang.reflect.Field f = Date.class.getDeclaredField(\"fastTime\");" +
-                " f.setAccessible(true); f.get(\"fastTime\")");
+        assertFailure("d = new Date(); java.lang.reflect.Field f = Date.class.getDeclaredField(\\\"fastTime\\\");" +
+                " f.setAccessible(true); f.get(\\\"fastTime\\\")");
 
         // AccessControlException[access denied ("java.io.FilePermission" "<<ALL FILES>>" "execute")]
-        assertFailure("def methodName = 'ex'; Runtime.\"${'get' + 'Runtime'}\"().\"${methodName}ec\"(\"touch /tmp/gotcha2\")");
-
+        assertFailure("def methodName = 'ex'; Runtime.\\\"${'get' + 'Runtime'}\\\"().\\\"${methodName}ec\\\"(\\\"touch /tmp/gotcha2\\\")");
+        
         // test a directory we normally have access to, but the groovy script does not.
         Path dir = createTempDir();
         // TODO: figure out the necessary escaping for windows paths here :)
         if (!Constants.WINDOWS) {
             // access denied ("java.io.FilePermission" ".../tempDir-00N" "read")
-            assertFailure("new File(\"" + dir + "\").exists()");
+            assertFailure("new File(\\\"" + dir + "\\\").exists()");
         }
     }
 
     private void assertSuccess(String script) {
         logger.info("--> script: " + script);
-        SearchResponse resp = client()
-                .prepareSearch("test")
-                .setSource(
-                        new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).sort(
-                                SortBuilders.scriptSort(new Script(script + "; doc['foo'].value + 2", ScriptType.INLINE, "groovy", null),
-                                        "number"))).get();
+        SearchResponse resp = client().prepareSearch("test")
+                .setSource(new BytesArray("{\"query\": {\"match_all\": {}}," +
+                        "\"sort\":{\"_script\": {\"script\": \"" + script +
+                        "; doc['foo'].value + 2\", \"type\": \"number\", \"lang\": \"groovy\"}}}")).get();
         assertNoFailures(resp);
         assertEquals(1, resp.getHits().getTotalHits());
         assertThat(resp.getHits().getAt(0).getSortValues(), equalTo(new Object[]{7.0}));
@@ -124,12 +119,10 @@ public class GroovySecurityIT extends ESIntegTestCase {
 
     private void assertFailure(String script) {
         logger.info("--> script: " + script);
-        SearchResponse resp = client()
-                .prepareSearch("test")
-                .setSource(
-                        new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).sort(
-                                SortBuilders.scriptSort(new Script(script + "; doc['foo'].value + 2", ScriptType.INLINE, "groovy", null),
-                                        "number"))).get();
+        SearchResponse resp = client().prepareSearch("test")
+                 .setSource(new BytesArray("{\"query\": {\"match_all\": {}}," +
+                         "\"sort\":{\"_script\": {\"script\": \"" + script +
+                         "; doc['foo'].value + 2\", \"type\": \"number\", \"lang\": \"groovy\"}}}")).get();
         assertEquals(0, resp.getHits().getTotalHits());
         ShardSearchFailure fails[] = resp.getShardFailures();
         // TODO: GroovyScriptExecutionException needs work:
@@ -137,8 +130,8 @@ public class GroovySecurityIT extends ESIntegTestCase {
         for (ShardSearchFailure fail : fails) {
             assertThat(fail.getCause(), instanceOf(GroovyScriptExecutionException.class));
             assertTrue("unexpected exception" + fail.getCause(),
-            // different casing, depending on jvm impl...
-                    fail.getCause().toString().toLowerCase(Locale.ROOT).contains("[access denied"));
+                       // different casing, depending on jvm impl...
+                       fail.getCause().toString().toLowerCase(Locale.ROOT).contains("[access denied"));
         }
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/script/IndexedScriptIT.java b/core/src/test/java/org/elasticsearch/script/IndexedScriptIT.java
index 0b5b691..03cc4e6 100644
--- a/core/src/test/java/org/elasticsearch/script/IndexedScriptIT.java
+++ b/core/src/test/java/org/elasticsearch/script/IndexedScriptIT.java
@@ -24,23 +24,17 @@ import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.expression.ExpressionScriptEngineService;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
 import org.elasticsearch.search.SearchHit;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
 import java.util.concurrent.ExecutionException;
 
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
@@ -86,20 +80,13 @@ public class IndexedScriptIT extends ESIntegTestCase {
         builders.add(client().prepareIndex("test", "scriptTest", "5").setSource("{\"theField\":\"bar\"}"));
 
         indexRandom(true, builders);
-        Map<String, Object> script2Params = new HashMap<>();
-        script2Params.put("factor", 3);
-        SearchResponse searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).size(1)
-                                .scriptField("test1", new Script("script1", ScriptType.INDEXED, "groovy", null))
-                                .scriptField("test2", new Script("script2", ScriptType.INDEXED, "groovy", script2Params)))
-                .setIndices("test").setTypes("scriptTest").get();
+        String query = "{ \"query\" : { \"match_all\": {}} , \"script_fields\" : { \"test1\" : { \"script_id\" : \"script1\", \"lang\":\"groovy\" }, \"test2\" : { \"script_id\" : \"script2\", \"lang\":\"groovy\", \"params\":{\"factor\":3}  }}, size:1}";
+        SearchResponse searchResponse = client().prepareSearch().setSource(new BytesArray(query)).setIndices("test").setTypes("scriptTest").get();
         assertHitCount(searchResponse, 5);
         assertTrue(searchResponse.getHits().hits().length == 1);
         SearchHit sh = searchResponse.getHits().getAt(0);
-        assertThat((Integer) sh.field("test1").getValue(), equalTo(2));
-        assertThat((Integer) sh.field("test2").getValue(), equalTo(6));
+        assertThat((Integer)sh.field("test1").getValue(), equalTo(2));
+        assertThat((Integer)sh.field("test2").getValue(), equalTo(6));
     }
 
     // Relates to #10397
@@ -112,15 +99,14 @@ public class IndexedScriptIT extends ESIntegTestCase {
 
         int iterations = randomIntBetween(2, 11);
         for (int i = 1; i < iterations; i++) {
-            PutIndexedScriptResponse response =
+            PutIndexedScriptResponse response = 
                     client().preparePutIndexedScript(GroovyScriptEngineService.NAME, "script1", "{\"script\":\"" + i + "\"}").get();
             assertEquals(i, response.getVersion());
-            SearchResponse searchResponse = client()
-                    .prepareSearch()
-                    .setSource(
-                            new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).scriptField("test_field",
-                                    new Script("script1", ScriptType.INDEXED, "groovy", null))).setIndices("test_index")
-                    .setTypes("test_type").get();
+            
+            String query = "{"
+                    + " \"query\" : { \"match_all\": {}}, "
+                    + " \"script_fields\" : { \"test_field\" : { \"script_id\" : \"script1\", \"lang\":\"groovy\" } } }";    
+            SearchResponse searchResponse = client().prepareSearch().setSource(new BytesArray(query)).setIndices("test_index").setTypes("test_type").get();
             assertHitCount(searchResponse, 1);
             SearchHit sh = searchResponse.getHits().getAt(0);
             assertThat((Integer)sh.field("test_field").getValue(), equalTo(i));
@@ -156,47 +142,9 @@ public class IndexedScriptIT extends ESIntegTestCase {
         }
         client().prepareIndex("test", "scriptTest", "1").setSource("{\"theField\":\"foo\"}").get();
         refresh();
-        SearchResponse searchResponse = client()
-                .prepareSearch("test")
-                .setSource(
-                        new SearchSourceBuilder().aggregation(AggregationBuilders.terms("test").script(
-                                new Script("script1", ScriptType.INDEXED, null, null)))).get();
+        String source = "{\"aggs\": {\"test\": { \"terms\" : { \"script_id\":\"script1\" } } } }";
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(new BytesArray(source)).get();
         assertHitCount(searchResponse, 1);
         assertThat(searchResponse.getAggregations().get("test"), notNullValue());
     }
-
-    @Test
-    public void testAllOpsDisabledIndexedScripts() throws IOException {
-        if (randomBoolean()) {
-            client().preparePutIndexedScript(ExpressionScriptEngineService.NAME, "script1", "{\"script\":\"2\"}").get();
-        } else {
-            client().prepareIndex(ScriptService.SCRIPT_INDEX, ExpressionScriptEngineService.NAME, "script1").setSource("{\"script\":\"2\"}").get();
-        }
-        client().prepareIndex("test", "scriptTest", "1").setSource("{\"theField\":\"foo\"}").get();
-        try {
-            client().prepareUpdate("test", "scriptTest", "1")
-                    .setScript(new Script("script1", ScriptService.ScriptType.INDEXED, ExpressionScriptEngineService.NAME, null)).get();
-            fail("update script should have been rejected");
-        } catch(Exception e) {
-            assertThat(e.getMessage(), containsString("failed to execute script"));
-            assertThat(e.getCause().getMessage(), containsString("scripts of type [indexed], operation [update] and lang [expression] are disabled"));
-        }
-        try {
-            client().prepareSearch()
-                    .setSource(
-                            new SearchSourceBuilder().scriptField("test1", new Script("script1", ScriptType.INDEXED, "expression", null)))
-                    .setIndices("test").setTypes("scriptTest").get();
-            fail("search script should have been rejected");
-        } catch (Exception e) {
-            assertThat(e.toString(), containsString("scripts of type [indexed], operation [search] and lang [expression] are disabled"));
-        }
-        try {
-            client().prepareSearch("test")
-                    .setSource(
-                            new SearchSourceBuilder().aggregation(AggregationBuilders.terms("test").script(
-                                    new Script("script1", ScriptType.INDEXED, "expression", null)))).get();
-        } catch (Exception e) {
-            assertThat(e.toString(), containsString("scripts of type [indexed], operation [aggs] and lang [expression] are disabled"));
-        }
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/script/MockScriptEngine.java b/core/src/test/java/org/elasticsearch/script/MockScriptEngine.java
new file mode 100644
index 0000000..eb7998c
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/script/MockScriptEngine.java
@@ -0,0 +1,81 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script;
+
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.search.lookup.SearchLookup;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * A dummy script engine used for testing. Scripts must be a number. Running the script
+ */
+public class MockScriptEngine implements ScriptEngineService {
+    public static final String NAME = "mockscript";
+
+    @Override
+    public String[] types() {
+        return new String[]{ NAME };
+    }
+
+    @Override
+    public String[] extensions() {
+        return types();
+    }
+
+    @Override
+    public boolean sandboxed() {
+        return true;
+    }
+
+    @Override
+    public Object compile(String script) {
+        return Integer.parseInt(script);
+    }
+
+    @Override
+    public ExecutableScript executable(CompiledScript compiledScript, @Nullable Map<String, Object> vars) {
+        return null;
+    }
+
+    @Override
+    public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, @Nullable Map<String, Object> vars) {
+        return null;
+    }
+
+    @Override
+    public Object execute(CompiledScript compiledScript, Map<String, Object> vars) {
+        return null;
+    }
+
+    @Override
+    public Object unwrap(Object value) {
+        return null;
+    }
+
+    @Override
+    public void scriptRemoved(@Nullable CompiledScript script) {
+    }
+
+    @Override
+    public void close() throws IOException {
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/script/OnDiskScriptIT.java b/core/src/test/java/org/elasticsearch/script/OnDiskScriptIT.java
deleted file mode 100644
index 62dcd5a..0000000
--- a/core/src/test/java/org/elasticsearch/script/OnDiskScriptIT.java
+++ /dev/null
@@ -1,179 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.script;
-
-import org.elasticsearch.action.index.IndexRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.search.SearchHit;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Test;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.ExecutionException;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-//Use Suite scope so that paths get set correctly
-@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.SUITE)
-public class OnDiskScriptIT extends ESIntegTestCase {
-
-    @Override
-    public Settings nodeSettings(int nodeOrdinal) {
-        //Set path so ScriptService will pick up the test scripts
-        return settingsBuilder().put(super.nodeSettings(nodeOrdinal))
-                .put("path.conf", this.getDataPath("config"))
-                .put("script.engine.expression.file.aggs", "off")
-                .put("script.engine.mustache.file.aggs", "off")
-                .put("script.engine.mustache.file.search", "off")
-                .put("script.engine.mustache.file.mapping", "off")
-                .put("script.engine.mustache.file.update", "off").build();
-    }
-
-    @Test
-    public void testFieldOnDiskScript()  throws ExecutionException, InterruptedException {
-        List<IndexRequestBuilder> builders = new ArrayList<>();
-        builders.add(client().prepareIndex("test", "scriptTest", "1").setSource("{\"theField\":\"foo\"}"));
-        builders.add(client().prepareIndex("test", "scriptTest", "2").setSource("{\"theField\":\"foo 2\"}"));
-        builders.add(client().prepareIndex("test", "scriptTest", "3").setSource("{\"theField\":\"foo 3\"}"));
-        builders.add(client().prepareIndex("test", "scriptTest", "4").setSource("{\"theField\":\"foo 4\"}"));
-        builders.add(client().prepareIndex("test", "scriptTest", "5").setSource("{\"theField\":\"bar\"}"));
-        indexRandom(true, builders);
-
-        Map<String, Object> script2Params = new HashMap<>();
-        script2Params.put("factor", 3);
-        SearchResponse searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).size(1)
-                                .scriptField("test1", new Script("script1", ScriptType.FILE, null, null))
-                                .scriptField("test2", new Script("script2", ScriptType.FILE, null, script2Params))).setIndices("test")
-                .setTypes("scriptTest").get();
-        assertHitCount(searchResponse, 5);
-        assertTrue(searchResponse.getHits().hits().length == 1);
-        SearchHit sh = searchResponse.getHits().getAt(0);
-        assertThat((Integer) sh.field("test1").getValue(), equalTo(2));
-        assertThat((Integer) sh.field("test2").getValue(), equalTo(6));
-    }
-
-    @Test
-    public void testOnDiskScriptsSameNameDifferentLang()  throws ExecutionException, InterruptedException {
-        List<IndexRequestBuilder> builders = new ArrayList<>();
-        builders.add(client().prepareIndex("test", "scriptTest", "1").setSource("{\"theField\":\"foo\"}"));
-        builders.add(client().prepareIndex("test", "scriptTest", "2").setSource("{\"theField\":\"foo 2\"}"));
-        builders.add(client().prepareIndex("test", "scriptTest", "3").setSource("{\"theField\":\"foo 3\"}"));
-        builders.add(client().prepareIndex("test", "scriptTest", "4").setSource("{\"theField\":\"foo 4\"}"));
-        builders.add(client().prepareIndex("test", "scriptTest", "5").setSource("{\"theField\":\"bar\"}"));
-        indexRandom(true, builders);
-
-        SearchResponse searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).size(1)
-                                .scriptField("test1", new Script("script1", ScriptType.FILE, null, null))
-                                .scriptField("test2", new Script("script1", ScriptType.FILE, "expression", null))).setIndices("test")
-                .setTypes("scriptTest").get();
-        assertHitCount(searchResponse, 5);
-        assertTrue(searchResponse.getHits().hits().length == 1);
-        SearchHit sh = searchResponse.getHits().getAt(0);
-        assertThat((Integer) sh.field("test1").getValue(), equalTo(2));
-        assertThat((Double) sh.field("test2").getValue(), equalTo(10d));
-    }
-
-    @Test
-    public void testPartiallyDisabledOnDiskScripts() throws ExecutionException, InterruptedException {
-        //test that although aggs are disabled for expression, search scripts work fine
-        List<IndexRequestBuilder> builders = new ArrayList<>();
-        builders.add(client().prepareIndex("test", "scriptTest", "1").setSource("{\"theField\":\"foo\"}"));
-        builders.add(client().prepareIndex("test", "scriptTest", "2").setSource("{\"theField\":\"foo 2\"}"));
-        builders.add(client().prepareIndex("test", "scriptTest", "3").setSource("{\"theField\":\"foo 3\"}"));
-        builders.add(client().prepareIndex("test", "scriptTest", "4").setSource("{\"theField\":\"foo 4\"}"));
-        builders.add(client().prepareIndex("test", "scriptTest", "5").setSource("{\"theField\":\"bar\"}"));
-
-        indexRandom(true, builders);
-
-        try {
-            client().prepareSearch("test")
-                    .setSource(
-                            new SearchSourceBuilder().aggregation(AggregationBuilders.terms("test").script(
-                                    new Script("script1", ScriptType.FILE, "expression", null)))).get();
-            fail("aggs script should have been rejected");
-        } catch (Exception e) {
-            assertThat(e.toString(), containsString("scripts of type [file], operation [aggs] and lang [expression] are disabled"));
-        }
-
-        SearchResponse searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).size(1)
-                                .scriptField("test1", new Script("script1", ScriptType.FILE, "expression", null))).setIndices("test")
-                .setTypes("scriptTest").get();
-        assertHitCount(searchResponse, 5);
-        assertTrue(searchResponse.getHits().hits().length == 1);
-        SearchHit sh = searchResponse.getHits().getAt(0);
-        assertThat((Double) sh.field("test1").getValue(), equalTo(10d));
-    }
-
-    @Test
-    public void testAllOpsDisabledOnDiskScripts() {
-        //whether we even compile or cache the on disk scripts doesn't change the end result (the returned error)
-        client().prepareIndex("test", "scriptTest", "1").setSource("{\"theField\":\"foo\"}").get();
-        refresh();
-        try {
-            client().prepareSearch("test")
-                    .setSource(
-                            new SearchSourceBuilder().aggregation(AggregationBuilders.terms("test").script(
-                                    new Script("script1", ScriptType.FILE, MustacheScriptEngineService.NAME, null)))).get();
-            fail("aggs script should have been rejected");
-        } catch (Exception e) {
-            assertThat(e.toString(), containsString("scripts of type [file], operation [aggs] and lang [mustache] are disabled"));
-        }
-        try {
-            client().prepareSearch()
-                    .setSource(
-                            new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).size(1)
-                                    .scriptField("test1", new Script("script1", ScriptType.FILE, MustacheScriptEngineService.NAME, null)))
-                    .setIndices("test").setTypes("scriptTest").get();
-            fail("search script should have been rejected");
-        } catch (Exception e) {
-            assertThat(e.toString(), containsString("scripts of type [file], operation [search] and lang [mustache] are disabled"));
-        }
-        try {
-            client().prepareUpdate("test", "scriptTest", "1")
-                    .setScript(new Script("script1", ScriptService.ScriptType.FILE, MustacheScriptEngineService.NAME, null)).get();
-            fail("update script should have been rejected");
-        } catch (Exception e) {
-            assertThat(e.getMessage(), containsString("failed to execute script"));
-            assertThat(e.getCause().getMessage(),
-                    containsString("scripts of type [file], operation [update] and lang [mustache] are disabled"));
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptContextTests.java b/core/src/test/java/org/elasticsearch/script/ScriptContextTests.java
new file mode 100644
index 0000000..e3fba01
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/script/ScriptContextTests.java
@@ -0,0 +1,118 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script;
+
+import org.elasticsearch.common.ContextAndHeaderHolder;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+public class ScriptContextTests extends ESTestCase {
+
+    private static final String PLUGIN_NAME = "testplugin";
+
+    ScriptService makeScriptService() throws Exception {
+        Settings settings = Settings.builder()
+            .put("path.home", createTempDir())
+            // no file watching, so we don't need a ResourceWatcherService
+            .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING, false)
+            .put("script." + PLUGIN_NAME + "_custom_globally_disabled_op", false)
+            .put("script.engine." + MockScriptEngine.NAME + ".inline." + PLUGIN_NAME + "_custom_exp_disabled_op", false)
+            .build();
+        Set<ScriptEngineService> engines = new HashSet<>(Collections.singletonList(new MockScriptEngine()));
+        List<ScriptContext.Plugin> customContexts = Arrays.asList(
+            new ScriptContext.Plugin(PLUGIN_NAME, "custom_op"),
+            new ScriptContext.Plugin(PLUGIN_NAME, "custom_exp_disabled_op"),
+            new ScriptContext.Plugin(PLUGIN_NAME, "custom_globally_disabled_op"));
+        return new ScriptService(settings, new Environment(settings), engines, null, new ScriptContextRegistry(customContexts));
+    }
+
+    public void testCustomGlobalScriptContextSettings() throws Exception {
+        ContextAndHeaderHolder contextAndHeaders = new ContextAndHeaderHolder();
+        ScriptService scriptService = makeScriptService();
+        for (ScriptService.ScriptType scriptType : ScriptService.ScriptType.values()) {
+            try {
+                Script script = new Script("1", scriptType, MockScriptEngine.NAME, null);
+                scriptService.compile(script, new ScriptContext.Plugin(PLUGIN_NAME, "custom_globally_disabled_op"), contextAndHeaders);
+                fail("script compilation should have been rejected");
+            } catch (ScriptException e) {
+                assertTrue(e.getMessage(), e.getMessage().contains("scripts of type [" + scriptType + "], operation [" + PLUGIN_NAME + "_custom_globally_disabled_op] and lang [" + MockScriptEngine.NAME + "] are disabled"));
+            }
+        }
+    }
+
+    public void testCustomScriptContextSettings() throws Exception {
+        ContextAndHeaderHolder contextAndHeaders = new ContextAndHeaderHolder();
+        ScriptService scriptService = makeScriptService();
+        Script script = new Script("1", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, null);
+        try {
+            scriptService.compile(script, new ScriptContext.Plugin(PLUGIN_NAME, "custom_exp_disabled_op"), contextAndHeaders);
+            fail("script compilation should have been rejected");
+        } catch (ScriptException e) {
+            assertTrue(e.getMessage(), e.getMessage().contains("scripts of type [inline], operation [" + PLUGIN_NAME + "_custom_exp_disabled_op] and lang [" + MockScriptEngine.NAME + "] are disabled"));
+        }
+
+        // still works for other script contexts
+        assertNotNull(scriptService.compile(script, ScriptContext.Standard.AGGS, contextAndHeaders));
+        assertNotNull(scriptService.compile(script, ScriptContext.Standard.SEARCH, contextAndHeaders));
+        assertNotNull(scriptService.compile(script, new ScriptContext.Plugin(PLUGIN_NAME, "custom_op"), contextAndHeaders));
+    }
+
+    public void testUnknownPluginScriptContext() throws Exception {
+        ContextAndHeaderHolder contextAndHeaders = new ContextAndHeaderHolder();
+        ScriptService scriptService = makeScriptService();
+        for (ScriptService.ScriptType scriptType : ScriptService.ScriptType.values()) {
+            try {
+                Script script = new Script("1", scriptType, MockScriptEngine.NAME, null);
+                scriptService.compile(script, new ScriptContext.Plugin(PLUGIN_NAME, "unknown"), contextAndHeaders);
+                fail("script compilation should have been rejected");
+            } catch (IllegalArgumentException e) {
+                assertTrue(e.getMessage(), e.getMessage().contains("script context [" + PLUGIN_NAME + "_unknown] not supported"));
+            }
+        }
+    }
+
+    public void testUnknownCustomScriptContext() throws Exception {
+        ContextAndHeaderHolder contextAndHeaders = new ContextAndHeaderHolder();
+        ScriptContext context = new ScriptContext() {
+            @Override
+            public String getKey() {
+                return "test";
+            }
+        };
+        ScriptService scriptService = makeScriptService();
+        for (ScriptService.ScriptType scriptType : ScriptService.ScriptType.values()) {
+            try {
+                Script script = new Script("1", scriptType, MockScriptEngine.NAME, null);
+                scriptService.compile(script, context, contextAndHeaders);
+                fail("script compilation should have been rejected");
+            } catch (IllegalArgumentException e) {
+                assertTrue(e.getMessage(), e.getMessage().contains("script context [test] not supported"));
+            }
+        }
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java b/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
index f9db13b..cfa4eda 100644
--- a/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
+++ b/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
@@ -24,7 +24,6 @@ import com.google.common.collect.ImmutableSet;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.expression.ExpressionScriptEngineService;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
 import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.lookup.SearchLookup;
@@ -42,9 +41,10 @@ import java.util.Set;
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.notNullValue;
 
+// TODO: this needs to be a base test class, and all scripting engines extend it
 public class ScriptModesTests extends ESTestCase {
 
-    private static final Set<String> ALL_LANGS = ImmutableSet.of(GroovyScriptEngineService.NAME, MustacheScriptEngineService.NAME, ExpressionScriptEngineService.NAME, "custom", "test");
+    private static final Set<String> ALL_LANGS = ImmutableSet.of(GroovyScriptEngineService.NAME, MustacheScriptEngineService.NAME, "custom", "test");
 
     static final String[] ENABLE_VALUES = new String[]{"on", "true", "yes", "1"};
     static final String[] DISABLE_VALUES = new String[]{"off", "false", "no", "0"};
@@ -74,7 +74,6 @@ public class ScriptModesTests extends ESTestCase {
         scriptEngines = buildScriptEnginesByLangMap(ImmutableSet.of(
                 new GroovyScriptEngineService(Settings.EMPTY),
                 new MustacheScriptEngineService(Settings.EMPTY),
-                new ExpressionScriptEngineService(Settings.EMPTY),
                 //add the native engine just to make sure it gets filtered out
                 new NativeScriptEngineService(Settings.EMPTY, Collections.<String, NativeScriptFactory>emptyMap()),
                 new CustomScriptEngineService()));
@@ -95,7 +94,7 @@ public class ScriptModesTests extends ESTestCase {
         if (assertScriptModesNonNull) {
             assertThat(scriptModes, notNullValue());
             //4 is the number of engines (native excluded), custom is counted twice though as it's associated with two different names
-            int numberOfSettings = 5 * ScriptType.values().length * scriptContextRegistry.scriptContexts().size();
+            int numberOfSettings = 4 * ScriptType.values().length * scriptContextRegistry.scriptContexts().size();
             assertThat(scriptModes.scriptModes.size(), equalTo(numberOfSettings));
             if (assertAllSettingsWereChecked) {
                 assertThat(checkedSettings.size(), equalTo(numberOfSettings));
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java b/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java
index d6c9ae4..6b33de8 100644
--- a/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java
@@ -26,7 +26,6 @@ import org.elasticsearch.common.io.Streams;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.expression.ExpressionScriptEngineService;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
 import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.lookup.SearchLookup;
@@ -48,6 +47,7 @@ import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.notNullValue;
 import static org.hamcrest.Matchers.sameInstance;
 
+//TODO: this needs to be a base test class, and all scripting engines extend it
 public class ScriptServiceTests extends ESTestCase {
 
     private ResourceWatcherService resourceWatcherService;
@@ -76,7 +76,7 @@ public class ScriptServiceTests extends ESTestCase {
                 .build();
         resourceWatcherService = new ResourceWatcherService(baseSettings, null);
         scriptEngineServices = ImmutableSet.of(new TestEngineService(), new GroovyScriptEngineService(baseSettings),
-                new ExpressionScriptEngineService(baseSettings), new MustacheScriptEngineService(baseSettings));
+                                               new MustacheScriptEngineService(baseSettings));
         scriptEnginesByLangMap = ScriptModesTests.buildScriptEnginesByLangMap(scriptEngineServices);
         //randomly register custom script contexts
         int randomInt = randomIntBetween(0, 3);
@@ -159,15 +159,15 @@ public class ScriptServiceTests extends ESTestCase {
     public void testScriptsSameNameDifferentLanguage() throws IOException {
         ContextAndHeaderHolder contextAndHeaders = new ContextAndHeaderHolder();
         buildScriptService(Settings.EMPTY);
-        createFileScripts("groovy", "expression");
+        createFileScripts("groovy", "test");
         CompiledScript groovyScript = scriptService.compile(
                 new Script("file_script", ScriptType.FILE, GroovyScriptEngineService.NAME, null), randomFrom(scriptContexts),
                 contextAndHeaders);
         assertThat(groovyScript.lang(), equalTo(GroovyScriptEngineService.NAME));
-        CompiledScript expressionScript = scriptService.compile(new Script("file_script", ScriptType.FILE, ExpressionScriptEngineService.NAME,
+        CompiledScript expressionScript = scriptService.compile(new Script("file_script", ScriptType.FILE, "test",
  null), randomFrom(new ScriptContext[] { ScriptContext.Standard.AGGS,
                 ScriptContext.Standard.SEARCH }), contextAndHeaders);
-        assertThat(expressionScript.lang(), equalTo(ExpressionScriptEngineService.NAME));
+        assertThat(expressionScript.lang(), equalTo("test"));
     }
 
     @Test
@@ -219,20 +219,13 @@ public class ScriptServiceTests extends ESTestCase {
             builder.put("script.inline", ScriptMode.SANDBOX);
         }
         buildScriptService(builder.build());
-        createFileScripts("groovy", "expression", "mustache", "test");
+        createFileScripts("groovy", "mustache", "test");
 
         for (ScriptContext scriptContext : scriptContexts) {
             //groovy is not sandboxed, only file scripts are enabled by default
             assertCompileRejected(GroovyScriptEngineService.NAME, "script", ScriptType.INLINE, scriptContext, contextAndHeaders);
             assertCompileRejected(GroovyScriptEngineService.NAME, "script", ScriptType.INDEXED, scriptContext, contextAndHeaders);
             assertCompileAccepted(GroovyScriptEngineService.NAME, "file_script", ScriptType.FILE, scriptContext, contextAndHeaders);
-            //expression engine is sandboxed, all scripts are enabled by default
-            if (!scriptContext.getKey().equals(ScriptContext.Standard.MAPPING.getKey()) &&
-                    !scriptContext.getKey().equals(ScriptContext.Standard.UPDATE.getKey())) {
-                assertCompileAccepted(ExpressionScriptEngineService.NAME, "script", ScriptType.INLINE, scriptContext, contextAndHeaders);
-                assertCompileAccepted(ExpressionScriptEngineService.NAME, "script", ScriptType.INDEXED, scriptContext, contextAndHeaders);
-                assertCompileAccepted(ExpressionScriptEngineService.NAME, "file_script", ScriptType.FILE, scriptContext, contextAndHeaders);
-            }
             //mustache engine is sandboxed, all scripts are enabled by default
             assertCompileAccepted(MustacheScriptEngineService.NAME, "script", ScriptType.INLINE, scriptContext, contextAndHeaders);
             assertCompileAccepted(MustacheScriptEngineService.NAME, "script", ScriptType.INDEXED, scriptContext, contextAndHeaders);
@@ -335,12 +328,6 @@ public class ScriptServiceTests extends ESTestCase {
                 //Otherwise they are always considered file ones as they can be found in the static cache.
                 String script = scriptType == ScriptType.FILE ? "file_script" : "script";
                 for (ScriptContext scriptContext : this.scriptContexts) {
-                    // skip script contexts that aren't allowed for expressions
-                    if (scriptEngineService instanceof ExpressionScriptEngineService &&
-                            (scriptContext.getKey().equals(ScriptContext.Standard.MAPPING.getKey()) ||
-                             scriptContext.getKey().equals(ScriptContext.Standard.UPDATE.getKey()))) {
-                        continue;
-                    }
                     //fallback mechanism: 1) engine specific settings 2) op based settings 3) source based settings
                     ScriptMode scriptMode = engineSettings.get(scriptEngineService.types()[0] + "." + scriptType + "." + scriptContext.getKey());
                     if (scriptMode == null) {
diff --git a/core/src/test/java/org/elasticsearch/script/expression/ExpressionScriptIT.java b/core/src/test/java/org/elasticsearch/script/expression/ExpressionScriptIT.java
deleted file mode 100644
index 7d5b2f8..0000000
--- a/core/src/test/java/org/elasticsearch/script/expression/ExpressionScriptIT.java
+++ /dev/null
@@ -1,568 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.expression;
-
-import org.apache.lucene.expressions.Expression;
-import org.apache.lucene.expressions.js.JavascriptCompiler;
-import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.action.search.SearchPhaseExecutionException;
-import org.elasticsearch.action.search.SearchRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchType;
-import org.elasticsearch.action.update.UpdateRequestBuilder;
-import org.elasticsearch.common.lucene.search.function.CombineFunction;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders;
-import org.elasticsearch.script.CompiledScript;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptException;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.SearchHits;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
-import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
-import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.Bucket;
-import org.elasticsearch.search.aggregations.metrics.stats.Stats;
-import org.elasticsearch.search.aggregations.pipeline.SimpleValue;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.elasticsearch.test.hamcrest.ElasticsearchAssertions;
-
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.elasticsearch.search.aggregations.AggregationBuilders.histogram;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
-import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders.bucketScript;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.notNullValue;
-
-public class ExpressionScriptIT extends ESIntegTestCase {
-
-    private SearchRequestBuilder buildRequest(String script, Object... params) {
-        ensureGreen("test");
-
-        Map<String, Object> paramsMap = new HashMap<>();
-        assert (params.length % 2 == 0);
-        for (int i = 0; i < params.length; i += 2) {
-            paramsMap.put(params[i].toString(), params[i + 1]);
-        }
-
-        SearchRequestBuilder req = client().prepareSearch().setIndices("test");
-        req.setQuery(QueryBuilders.matchAllQuery())
-                .addSort(SortBuilders.fieldSort("_uid")
-                        .order(SortOrder.ASC))
-                .addScriptField("foo", new Script(script, ScriptType.INLINE, "expression", paramsMap));
-        return req;
-    }
-
-    public void testBasic() throws Exception {
-        createIndex("test");
-        ensureGreen("test");
-        client().prepareIndex("test", "doc", "1").setSource("foo", 4).setRefresh(true).get();
-        SearchResponse rsp = buildRequest("doc['foo'] + 1").get();
-        assertEquals(1, rsp.getHits().getTotalHits());
-        assertEquals(5.0, rsp.getHits().getAt(0).field("foo").getValue(), 0.0D);
-    }
-
-    public void testBasicUsingDotValue() throws Exception {
-        createIndex("test");
-        ensureGreen("test");
-        client().prepareIndex("test", "doc", "1").setSource("foo", 4).setRefresh(true).get();
-        SearchResponse rsp = buildRequest("doc['foo'].value + 1").get();
-        assertEquals(1, rsp.getHits().getTotalHits());
-        assertEquals(5.0, rsp.getHits().getAt(0).field("foo").getValue(), 0.0D);
-    }
-
-    public void testScore() throws Exception {
-        createIndex("test");
-        ensureGreen("test");
-        indexRandom(true,
-                client().prepareIndex("test", "doc", "1").setSource("text", "hello goodbye"),
-                client().prepareIndex("test", "doc", "2").setSource("text", "hello hello hello goodbye"),
-                client().prepareIndex("test", "doc", "3").setSource("text", "hello hello goodebye"));
-        ScoreFunctionBuilder score = ScoreFunctionBuilders.scriptFunction(new Script("1 / _score", ScriptType.INLINE, "expression", null));
-        SearchRequestBuilder req = client().prepareSearch().setIndices("test");
-        req.setQuery(QueryBuilders.functionScoreQuery(QueryBuilders.termQuery("text", "hello"), score).boostMode(CombineFunction.REPLACE));
-        req.setSearchType(SearchType.DFS_QUERY_THEN_FETCH); // make sure DF is consistent
-        SearchResponse rsp = req.get();
-        assertSearchResponse(rsp);
-        SearchHits hits = rsp.getHits();
-        assertEquals(3, hits.getTotalHits());
-        assertEquals("1", hits.getAt(0).getId());
-        assertEquals("3", hits.getAt(1).getId());
-        assertEquals("2", hits.getAt(2).getId());
-    }
-
-    public void testDateMethods() throws Exception {
-        ElasticsearchAssertions.assertAcked(prepareCreate("test").addMapping("doc", "date0", "type=date", "date1", "type=date"));
-        ensureGreen("test");
-        indexRandom(true,
-                client().prepareIndex("test", "doc", "1").setSource("date0", "2015-04-28T04:02:07Z", "date1", "1985-09-01T23:11:01Z"),
-                client().prepareIndex("test", "doc", "2").setSource("date0", "2013-12-25T11:56:45Z", "date1", "1983-10-13T23:15:00Z"));
-        SearchResponse rsp = buildRequest("doc['date0'].getSeconds() - doc['date0'].getMinutes()").get();
-        assertEquals(2, rsp.getHits().getTotalHits());
-        SearchHits hits = rsp.getHits();
-        assertEquals(5.0, hits.getAt(0).field("foo").getValue(), 0.0D);
-        assertEquals(-11.0, hits.getAt(1).field("foo").getValue(), 0.0D);
-        rsp = buildRequest("doc['date0'].getHourOfDay() + doc['date1'].getDayOfMonth()").get();
-        assertEquals(2, rsp.getHits().getTotalHits());
-        hits = rsp.getHits();
-        assertEquals(5.0, hits.getAt(0).field("foo").getValue(), 0.0D);
-        assertEquals(24.0, hits.getAt(1).field("foo").getValue(), 0.0D);
-        rsp = buildRequest("doc['date1'].getMonth() + 1").get();
-        assertEquals(2, rsp.getHits().getTotalHits());
-        hits = rsp.getHits();
-        assertEquals(9.0, hits.getAt(0).field("foo").getValue(), 0.0D);
-        assertEquals(10.0, hits.getAt(1).field("foo").getValue(), 0.0D);
-        rsp = buildRequest("doc['date1'].getYear()").get();
-        assertEquals(2, rsp.getHits().getTotalHits());
-        hits = rsp.getHits();
-        assertEquals(1985.0, hits.getAt(0).field("foo").getValue(), 0.0D);
-        assertEquals(1983.0, hits.getAt(1).field("foo").getValue(), 0.0D);
-    }
-
-    public void testMultiValueMethods() throws Exception {
-        ElasticsearchAssertions.assertAcked(prepareCreate("test").addMapping("doc", "double0", "type=double", "double1", "type=double"));
-        ensureGreen("test");
-        indexRandom(true,
-                client().prepareIndex("test", "doc", "1").setSource("double0", "5.0", "double0", "1.0", "double0", "1.5", "double1", "1.2", "double1", "2.4"),
-                client().prepareIndex("test", "doc", "2").setSource("double0", "5.0", "double1", "3.0"),
-                client().prepareIndex("test", "doc", "3").setSource("double0", "5.0", "double0", "1.0", "double0", "1.5", "double0", "-1.5", "double1", "4.0"));
-
-
-        SearchResponse rsp = buildRequest("doc['double0'].count() + doc['double1'].count()").get();
-        assertSearchResponse(rsp);
-        SearchHits hits = rsp.getHits();
-        assertEquals(3, hits.getTotalHits());
-        assertEquals(5.0, hits.getAt(0).field("foo").getValue(), 0.0D);
-        assertEquals(2.0, hits.getAt(1).field("foo").getValue(), 0.0D);
-        assertEquals(5.0, hits.getAt(2).field("foo").getValue(), 0.0D);
-
-        rsp = buildRequest("doc['double0'].sum()").get();
-        assertSearchResponse(rsp);
-        hits = rsp.getHits();
-        assertEquals(3, hits.getTotalHits());
-        assertEquals(7.5, hits.getAt(0).field("foo").getValue(), 0.0D);
-        assertEquals(5.0, hits.getAt(1).field("foo").getValue(), 0.0D);
-        assertEquals(6.0, hits.getAt(2).field("foo").getValue(), 0.0D);
-
-        rsp = buildRequest("doc['double0'].avg() + doc['double1'].avg()").get();
-        assertSearchResponse(rsp);
-        hits = rsp.getHits();
-        assertEquals(3, hits.getTotalHits());
-        assertEquals(4.3, hits.getAt(0).field("foo").getValue(), 0.0D);
-        assertEquals(8.0, hits.getAt(1).field("foo").getValue(), 0.0D);
-        assertEquals(5.5, hits.getAt(2).field("foo").getValue(), 0.0D);
-
-        rsp = buildRequest("doc['double0'].median()").get();
-        assertSearchResponse(rsp);
-        hits = rsp.getHits();
-        assertEquals(3, hits.getTotalHits());
-        assertEquals(1.5, hits.getAt(0).field("foo").getValue(), 0.0D);
-        assertEquals(5.0, hits.getAt(1).field("foo").getValue(), 0.0D);
-        assertEquals(1.25, hits.getAt(2).field("foo").getValue(), 0.0D);
-
-        rsp = buildRequest("doc['double0'].min()").get();
-        assertSearchResponse(rsp);
-        hits = rsp.getHits();
-        assertEquals(3, hits.getTotalHits());
-        assertEquals(1.0, hits.getAt(0).field("foo").getValue(), 0.0D);
-        assertEquals(5.0, hits.getAt(1).field("foo").getValue(), 0.0D);
-        assertEquals(-1.5, hits.getAt(2).field("foo").getValue(), 0.0D);
-
-        rsp = buildRequest("doc['double0'].max()").get();
-        assertSearchResponse(rsp);
-        hits = rsp.getHits();
-        assertEquals(3, hits.getTotalHits());
-        assertEquals(5.0, hits.getAt(0).field("foo").getValue(), 0.0D);
-        assertEquals(5.0, hits.getAt(1).field("foo").getValue(), 0.0D);
-        assertEquals(5.0, hits.getAt(2).field("foo").getValue(), 0.0D);
-
-        rsp = buildRequest("doc['double0'].sum()/doc['double0'].count()").get();
-        assertSearchResponse(rsp);
-        hits = rsp.getHits();
-        assertEquals(3, hits.getTotalHits());
-        assertEquals(2.5, hits.getAt(0).field("foo").getValue(), 0.0D);
-        assertEquals(5.0, hits.getAt(1).field("foo").getValue(), 0.0D);
-        assertEquals(1.5, hits.getAt(2).field("foo").getValue(), 0.0D);
-    }
-
-    public void testInvalidDateMethodCall() throws Exception {
-        ElasticsearchAssertions.assertAcked(prepareCreate("test").addMapping("doc", "double", "type=double"));
-        ensureGreen("test");
-        indexRandom(true, client().prepareIndex("test", "doc", "1").setSource("double", "178000000.0"));
-        try {
-            buildRequest("doc['double'].getYear()").get();
-            fail();
-        } catch (SearchPhaseExecutionException e) {
-            assertThat(e.toString() + "should have contained IllegalArgumentException",
-                    e.toString().contains("IllegalArgumentException"), equalTo(true));
-            assertThat(e.toString() + "should have contained can only be used with a date field type",
-                    e.toString().contains("can only be used with a date field type"), equalTo(true));
-        }
-    }
-
-    public void testSparseField() throws Exception {
-        ElasticsearchAssertions.assertAcked(prepareCreate("test").addMapping("doc", "x", "type=long", "y", "type=long"));
-        ensureGreen("test");
-        indexRandom(true,
-                client().prepareIndex("test", "doc", "1").setSource("x", 4),
-                client().prepareIndex("test", "doc", "2").setSource("y", 2));
-        SearchResponse rsp = buildRequest("doc['x'] + 1").get();
-        ElasticsearchAssertions.assertSearchResponse(rsp);
-        SearchHits hits = rsp.getHits();
-        assertEquals(2, rsp.getHits().getTotalHits());
-        assertEquals(5.0, hits.getAt(0).field("foo").getValue(), 0.0D);
-        assertEquals(1.0, hits.getAt(1).field("foo").getValue(), 0.0D);
-    }
-
-    public void testMissingField() throws Exception {
-        createIndex("test");
-        ensureGreen("test");
-        client().prepareIndex("test", "doc", "1").setSource("x", 4).setRefresh(true).get();
-        try {
-            buildRequest("doc['bogus']").get();
-            fail("Expected missing field to cause failure");
-        } catch (SearchPhaseExecutionException e) {
-            assertThat(e.toString() + "should have contained ExpressionScriptCompilationException",
-                    e.toString().contains("ExpressionScriptCompilationException"), equalTo(true));
-            assertThat(e.toString() + "should have contained missing field error",
-                    e.toString().contains("does not exist in mappings"), equalTo(true));
-        }
-    }
-
-    public void testParams() throws Exception {
-        createIndex("test");
-        ensureGreen("test");
-        indexRandom(true,
-                client().prepareIndex("test", "doc", "1").setSource("x", 10),
-                client().prepareIndex("test", "doc", "2").setSource("x", 3),
-                client().prepareIndex("test", "doc", "3").setSource("x", 5));
-        // a = int, b = double, c = long
-        String script = "doc['x'] * a + b + ((c + doc['x']) > 5000000009 ? 1 : 0)";
-        SearchResponse rsp = buildRequest(script, "a", 2, "b", 3.5, "c", 5000000000L).get();
-        SearchHits hits = rsp.getHits();
-        assertEquals(3, hits.getTotalHits());
-        assertEquals(24.5, hits.getAt(0).field("foo").getValue(), 0.0D);
-        assertEquals(9.5, hits.getAt(1).field("foo").getValue(), 0.0D);
-        assertEquals(13.5, hits.getAt(2).field("foo").getValue(), 0.0D);
-    }
-
-    public void testCompileFailure() {
-        client().prepareIndex("test", "doc", "1").setSource("x", 1).setRefresh(true).get();
-        try {
-            buildRequest("garbage%@#%@").get();
-            fail("Expected expression compilation failure");
-        } catch (SearchPhaseExecutionException e) {
-            assertThat(e.toString() + "should have contained ExpressionScriptCompilationException",
-                    e.toString().contains("ExpressionScriptCompilationException"), equalTo(true));
-            assertThat(e.toString() + "should have contained compilation failure",
-                    e.toString().contains("Failed to parse expression"), equalTo(true));
-        }
-    }
-
-    public void testNonNumericParam() {
-        client().prepareIndex("test", "doc", "1").setSource("x", 1).setRefresh(true).get();
-        try {
-            buildRequest("a", "a", "astring").get();
-            fail("Expected string parameter to cause failure");
-        } catch (SearchPhaseExecutionException e) {
-            assertThat(e.toString() + "should have contained ExpressionScriptCompilationException",
-                    e.toString().contains("ExpressionScriptCompilationException"), equalTo(true));
-            assertThat(e.toString() + "should have contained non-numeric parameter error",
-                    e.toString().contains("must be a numeric type"), equalTo(true));
-        }
-    }
-
-    public void testNonNumericField() {
-        client().prepareIndex("test", "doc", "1").setSource("text", "this is not a number").setRefresh(true).get();
-        try {
-            buildRequest("doc['text']").get();
-            fail("Expected text field to cause execution failure");
-        } catch (SearchPhaseExecutionException e) {
-            assertThat(e.toString() + "should have contained ExpressionScriptCompilationException",
-                    e.toString().contains("ExpressionScriptCompilationException"), equalTo(true));
-            assertThat(e.toString() + "should have contained non-numeric field error",
-                    e.toString().contains("must be numeric"), equalTo(true));
-        }
-    }
-
-    public void testInvalidGlobalVariable() {
-        client().prepareIndex("test", "doc", "1").setSource("foo", 5).setRefresh(true).get();
-        try {
-            buildRequest("bogus").get();
-            fail("Expected bogus variable to cause execution failure");
-        } catch (SearchPhaseExecutionException e) {
-            assertThat(e.toString() + "should have contained ExpressionScriptCompilationException",
-                    e.toString().contains("ExpressionScriptCompilationException"), equalTo(true));
-            assertThat(e.toString() + "should have contained unknown variable error",
-                    e.toString().contains("Unknown variable"), equalTo(true));
-        }
-    }
-
-    public void testDocWithoutField() {
-        client().prepareIndex("test", "doc", "1").setSource("foo", 5).setRefresh(true).get();
-        try {
-            buildRequest("doc").get();
-            fail("Expected doc variable without field to cause execution failure");
-        } catch (SearchPhaseExecutionException e) {
-            assertThat(e.toString() + "should have contained ExpressionScriptCompilationException",
-                    e.toString().contains("ExpressionScriptCompilationException"), equalTo(true));
-            assertThat(e.toString() + "should have contained a missing specific field error",
-                    e.toString().contains("must be used with a specific field"), equalTo(true));
-        }
-    }
-
-    public void testInvalidFieldMember() {
-        client().prepareIndex("test", "doc", "1").setSource("foo", 5).setRefresh(true).get();
-        try {
-            buildRequest("doc['foo'].bogus").get();
-            fail("Expected bogus field member to cause execution failure");
-        } catch (SearchPhaseExecutionException e) {
-            assertThat(e.toString() + "should have contained ExpressionScriptCompilationException",
-                    e.toString().contains("ExpressionScriptCompilationException"), equalTo(true));
-            assertThat(e.toString() + "should have contained member variable [value] or member methods may be accessed",
-                    e.toString().contains("member variable [value] or member methods may be accessed"), equalTo(true));
-        }
-    }
-
-    public void testSpecialValueVariable() throws Exception {
-        // i.e. _value for aggregations
-        createIndex("test");
-        ensureGreen("test");
-        indexRandom(true,
-                client().prepareIndex("test", "doc", "1").setSource("x", 5, "y", 1.2),
-                client().prepareIndex("test", "doc", "2").setSource("x", 10, "y", 1.4),
-                client().prepareIndex("test", "doc", "3").setSource("x", 13, "y", 1.8));
-
-        SearchRequestBuilder req = client().prepareSearch().setIndices("test");
-        req.setQuery(QueryBuilders.matchAllQuery())
-                .addAggregation(
-                        AggregationBuilders.stats("int_agg").field("x")
-                                .script(new Script("_value * 3", ScriptType.INLINE, ExpressionScriptEngineService.NAME, null)))
-                .addAggregation(
-                        AggregationBuilders.stats("double_agg").field("y")
-                                .script(new Script("_value - 1.1", ScriptType.INLINE, ExpressionScriptEngineService.NAME, null)));
-
-        SearchResponse rsp = req.get();
-        assertEquals(3, rsp.getHits().getTotalHits());
-
-        Stats stats = rsp.getAggregations().get("int_agg");
-        assertEquals(39.0, stats.getMax(), 0.0001);
-        assertEquals(15.0, stats.getMin(), 0.0001);
-
-        stats = rsp.getAggregations().get("double_agg");
-        assertEquals(0.7, stats.getMax(), 0.0001);
-        assertEquals(0.1, stats.getMin(), 0.0001);
-    }
-
-    public void testStringSpecialValueVariable() throws Exception {
-        // i.e. expression script for term aggregations, which is not allowed
-        createIndex("test");
-        ensureGreen("test");
-        indexRandom(true,
-                client().prepareIndex("test", "doc", "1").setSource("text", "hello"),
-                client().prepareIndex("test", "doc", "2").setSource("text", "goodbye"),
-                client().prepareIndex("test", "doc", "3").setSource("text", "hello"));
-
-        SearchRequestBuilder req = client().prepareSearch().setIndices("test");
-        req.setQuery(QueryBuilders.matchAllQuery())
-                .addAggregation(
-                        AggregationBuilders.terms("term_agg").field("text")
-                                .script(new Script("_value", ScriptType.INLINE, ExpressionScriptEngineService.NAME, null)));
-
-        String message;
-        try {
-            // shards that don't have docs with the "text" field will not fail,
-            // so we may or may not get a total failure
-            SearchResponse rsp = req.get();
-            assertThat(rsp.getShardFailures().length, greaterThan(0)); // at least the shards containing the docs should have failed
-            message = rsp.getShardFailures()[0].reason();
-        } catch (SearchPhaseExecutionException e) {
-            message = e.toString();
-        }
-        assertThat(message + "should have contained ExpressionScriptExecutionException",
-                message.contains("ExpressionScriptExecutionException"), equalTo(true));
-        assertThat(message + "should have contained text variable error",
-                message.contains("text variable"), equalTo(true));
-    }
-
-    // series of unit test for using expressions as executable scripts
-    public void testExecutableScripts() throws Exception {
-        Map<String, Object> vars = new HashMap<>();
-        vars.put("a", 2.5);
-        vars.put("b", 3);
-        vars.put("xyz", -1);
-
-        Expression expr = JavascriptCompiler.compile("a+b+xyz");
-        CompiledScript compiledScript = new CompiledScript(ScriptType.INLINE, "", "expression", expr);
-
-        ExpressionExecutableScript ees = new ExpressionExecutableScript(compiledScript, vars);
-        assertEquals((Double) ees.run(), 4.5, 0.001);
-
-        ees.setNextVar("b", -2.5);
-        assertEquals((Double) ees.run(), -1, 0.001);
-
-        ees.setNextVar("a", -2.5);
-        ees.setNextVar("b", -2.5);
-        ees.setNextVar("xyz", -2.5);
-        assertEquals((Double) ees.run(), -7.5, 0.001);
-
-        String message;
-
-        try {
-            vars = new HashMap<>();
-            vars.put("a", 1);
-            ees = new ExpressionExecutableScript(compiledScript, vars);
-            ees.run();
-            fail("An incorrect number of variables were allowed to be used in an expression.");
-        } catch (ScriptException se) {
-            message = se.getMessage();
-            assertThat(message + " should have contained number of variables", message.contains("number of variables"), equalTo(true));
-        }
-
-        try {
-            vars = new HashMap<>();
-            vars.put("a", 1);
-            vars.put("b", 3);
-            vars.put("c", -1);
-            ees = new ExpressionExecutableScript(compiledScript, vars);
-            ees.run();
-            fail("A variable was allowed to be set that does not exist in the expression.");
-        } catch (ScriptException se) {
-            message = se.getMessage();
-            assertThat(message + " should have contained does not exist in", message.contains("does not exist in"), equalTo(true));
-        }
-
-        try {
-            vars = new HashMap<>();
-            vars.put("a", 1);
-            vars.put("b", 3);
-            vars.put("xyz", "hello");
-            ees = new ExpressionExecutableScript(compiledScript, vars);
-            ees.run();
-            fail("A non-number was allowed to be use in the expression.");
-        } catch (ScriptException se) {
-            message = se.getMessage();
-            assertThat(message + " should have contained process numbers", message.contains("process numbers"), equalTo(true));
-        }
-
-    }
-
-    // test to make sure expressions are not allowed to be used as update scripts
-    public void testInvalidUpdateScript() throws Exception {
-        try {
-            createIndex("test_index");
-            ensureGreen("test_index");
-            indexRandom(true, client().prepareIndex("test_index", "doc", "1").setSource("text_field", "text"));
-            UpdateRequestBuilder urb = client().prepareUpdate().setIndex("test_index");
-            urb.setType("doc");
-            urb.setId("1");
-            urb.setScript(new Script("0", ScriptType.INLINE, ExpressionScriptEngineService.NAME, null));
-            urb.get();
-            fail("Expression scripts should not be allowed to run as update scripts.");
-        } catch (Exception e) {
-            String message = e.getMessage();
-            assertThat(message + " should have contained failed to execute", message.contains("failed to execute"), equalTo(true));
-            message = e.getCause().getMessage();
-            assertThat(message + " should have contained not supported", message.contains("not supported"), equalTo(true));
-        }
-    }
-
-    // test to make sure expressions are not allowed to be used as mapping scripts
-    public void testInvalidMappingScript() throws Exception{
-        try {
-            createIndex("test_index");
-            ensureGreen("test_index");
-            XContentBuilder builder = XContentFactory.jsonBuilder().startObject();
-            builder.startObject("transform");
-            builder.field("script", "1.0");
-            builder.field("lang", ExpressionScriptEngineService.NAME);
-            builder.endObject();
-            builder.startObject("properties");
-            builder.startObject("double_field");
-            builder.field("type", "double");
-            builder.endObject();
-            builder.endObject();
-            builder.endObject();
-            client().admin().indices().preparePutMapping("test_index").setType("trans_test").setSource(builder).get();
-            client().prepareIndex("test_index", "trans_test", "1").setSource("double_field", 0.0).get();
-            fail("Expression scripts should not be allowed to run as mapping scripts.");
-        } catch (Exception e) {
-            String message = ExceptionsHelper.detailedMessage(e);
-            assertThat(message + " should have contained failed to parse", message.contains("failed to parse"), equalTo(true));
-            assertThat(message + " should have contained not supported", message.contains("not supported"), equalTo(true));
-        }
-    }
-
-    // test to make sure expressions are allowed to be used for reduce in pipeline aggregations
-    public void testPipelineAggregationScript() throws Exception {
-        createIndex("agg_index");
-        ensureGreen("agg_index");
-        indexRandom(true,
-                client().prepareIndex("agg_index", "doc", "1").setSource("one", 1.0, "two", 2.0, "three", 3.0, "four", 4.0),
-                client().prepareIndex("agg_index", "doc", "2").setSource("one", 2.0, "two", 2.0, "three", 3.0, "four", 4.0),
-                client().prepareIndex("agg_index", "doc", "3").setSource("one", 3.0, "two", 2.0, "three", 3.0, "four", 4.0),
-                client().prepareIndex("agg_index", "doc", "4").setSource("one", 4.0, "two", 2.0, "three", 3.0, "four", 4.0),
-                client().prepareIndex("agg_index", "doc", "5").setSource("one", 5.0, "two", 2.0, "three", 3.0, "four", 4.0));
-        SearchResponse response = client()
-                .prepareSearch("agg_index")
-                .addAggregation(
-                        histogram("histogram")
-                                .field("one")
-                                .interval(2)
-                                .subAggregation(sum("twoSum").field("two"))
-                                .subAggregation(sum("threeSum").field("three"))
-                                .subAggregation(sum("fourSum").field("four"))
-                                .subAggregation(
-                                        bucketScript("totalSum").setBucketsPaths("twoSum", "threeSum", "fourSum").script(
-                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, ExpressionScriptEngineService.NAME, null)))).execute().actionGet();
-
-        InternalHistogram<Bucket> histogram = response.getAggregations().get("histogram");
-        assertThat(histogram, notNullValue());
-        assertThat(histogram.getName(), equalTo("histogram"));
-        List<Bucket> buckets = histogram.getBuckets();
-
-        for (int bucketCount = 0; bucketCount < buckets.size(); ++bucketCount) {
-            Histogram.Bucket bucket = buckets.get(bucketCount);
-            if (bucket.getDocCount() == 1) {
-                SimpleValue seriesArithmetic = bucket.getAggregations().get("totalSum");
-                assertThat(seriesArithmetic, notNullValue());
-                double seriesArithmeticValue = seriesArithmetic.value();
-                assertEquals(9.0, seriesArithmeticValue, 0.001);
-            } else if (bucket.getDocCount() == 2) {
-                SimpleValue seriesArithmetic = bucket.getAggregations().get("totalSum");
-                assertThat(seriesArithmetic, notNullValue());
-                double seriesArithmeticValue = seriesArithmetic.value();
-                assertEquals(18.0, seriesArithmeticValue, 0.001);
-            } else {
-                fail("Incorrect number of documents in a bucket in the histogram.");
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/script/expression/ExpressionScriptTests.java b/core/src/test/java/org/elasticsearch/script/expression/ExpressionScriptTests.java
deleted file mode 100644
index 4b01435..0000000
--- a/core/src/test/java/org/elasticsearch/script/expression/ExpressionScriptTests.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.expression;
-
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.IndexService;
-import org.elasticsearch.script.CompiledScript;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.SearchScript;
-import org.elasticsearch.search.lookup.SearchLookup;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-
-import java.util.Collections;
-
-public class ExpressionScriptTests extends ESSingleNodeTestCase {
-
-    public void testNeedsScores() {
-        IndexService index = createIndex("test", Settings.EMPTY, "type", "d", "type=double");
-        
-        ExpressionScriptEngineService service = new ExpressionScriptEngineService(Settings.EMPTY);
-        SearchLookup lookup = new SearchLookup(index.mapperService(), index.fieldData(), null);
-
-        Object compiled = service.compile("1.2");
-        SearchScript ss = service.search(new CompiledScript(ScriptType.INLINE, "randomName", "expression", compiled), lookup, Collections.<String, Object>emptyMap());
-        assertFalse(ss.needsScores());
-
-        compiled = service.compile("doc['d'].value");
-        ss = service.search(new CompiledScript(ScriptType.INLINE, "randomName", "expression", compiled), lookup, Collections.<String, Object>emptyMap());
-        assertFalse(ss.needsScores());
-
-        compiled = service.compile("1/_score");
-        ss = service.search(new CompiledScript(ScriptType.INLINE, "randomName", "expression", compiled), lookup, Collections.<String, Object>emptyMap());
-        assertTrue(ss.needsScores());
-
-        compiled = service.compile("doc['d'].value * _score");
-        ss = service.search(new CompiledScript(ScriptType.INLINE, "randomName", "expression", compiled), lookup, Collections.<String, Object>emptyMap());
-        assertTrue(ss.needsScores());
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java
index e5634fe..631f705 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.search.aggregations;
 
-import org.apache.lucene.util.LuceneTestCase.AwaitsFix;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.client.Requests;
@@ -42,8 +41,6 @@ import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.core.IsNull.notNullValue;
 
 @ESIntegTestCase.SuiteScopeTestCase
-@AwaitsFix(bugUrl = "needs fixing after the search request refactor. Do we need agg binary?")
-// NO RELEASE
 public class AggregationsBinaryIT extends ESIntegTestCase {
 
     private static final String STRING_FIELD_NAME = "s_value";
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/ParsingIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/ParsingIT.java
index 87307c0..64f80d6 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/ParsingIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/ParsingIT.java
@@ -31,151 +31,150 @@ import java.util.regex.Pattern;
 
 public class ParsingIT extends ESIntegTestCase {
 
-    // NORELEASE move these tests to unit tests when aggs refactoring is done
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testTwoTypes() throws Exception {
-//        createIndex("idx");
-//        ensureGreen();
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject("in_stock")
-//                    .startObject("filter")
-//                        .startObject("range")
-//                            .startObject("stock")
-//                                .field("gt", 0)
-//                            .endObject()
-//                        .endObject()
-//                    .endObject()
-//                    .startObject("terms")
-//                        .field("field", "stock")
-//                    .endObject()
-//                .endObject()
-//            .endObject()).execute().actionGet();
-//    }
-//
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testTwoAggs() throws Exception {
-//        createIndex("idx");
-//        ensureGreen();
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject("by_date")
-//                    .startObject("date_histogram")
-//                        .field("field", "timestamp")
-//                        .field("interval", "month")
-//                    .endObject()
-//                    .startObject("aggs")
-//                        .startObject("tag_count")
-//                            .startObject("cardinality")
-//                                .field("field", "tag")
-//                            .endObject()
-//                        .endObject()
-//                    .endObject()
-//                    .startObject("aggs") // 2nd "aggs": illegal
-//                        .startObject("tag_count2")
-//                            .startObject("cardinality")
-//                                .field("field", "tag")
-//                            .endObject()
-//                        .endObject()
-//                    .endObject()
-//            .endObject()).execute().actionGet();
-//    }
-//
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testInvalidAggregationName() throws Exception {
-//
-//        Matcher matcher = Pattern.compile("[^\\[\\]>]+").matcher("");
-//        String name;
-//        SecureRandom rand = new SecureRandom();
-//        int len = randomIntBetween(1, 5);
-//        char[] word = new char[len];
-//        while(true) {
-//            for (int i = 0; i < word.length; i++) {
-//                word[i] = (char) rand.nextInt(127);
-//            }
-//            name = String.valueOf(word);
-//            if (!matcher.reset(name).matches()) {
-//                break;
-//            }
-//        }
-//
-//        createIndex("idx");
-//        ensureGreen();
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject(name)
-//                    .startObject("filter")
-//                        .startObject("range")
-//                            .startObject("stock")
-//                                .field("gt", 0)
-//                            .endObject()
-//                        .endObject()
-//                    .endObject()
-//            .endObject()).execute().actionGet();
-//    }
-//
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testSameAggregationName() throws Exception {
-//        createIndex("idx");
-//        ensureGreen();
-//        final String name = RandomStrings.randomAsciiOfLength(getRandom(), 10);
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject(name)
-//                    .startObject("terms")
-//                        .field("field", "a")
-//                    .endObject()
-//                .endObject()
-//                .startObject(name)
-//                    .startObject("terms")
-//                        .field("field", "b")
-//                    .endObject()
-//                .endObject()
-//            .endObject()).execute().actionGet();
-//    }
-//
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testMissingName() throws Exception {
-//        createIndex("idx");
-//        ensureGreen();
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject("by_date")
-//                    .startObject("date_histogram")
-//                        .field("field", "timestamp")
-//                        .field("interval", "month")
-//                    .endObject()
-//                    .startObject("aggs")
-//                        // the aggregation name is missing
-//                        //.startObject("tag_count")
-//                            .startObject("cardinality")
-//                                .field("field", "tag")
-//                            .endObject()
-//                        //.endObject()
-//                    .endObject()
-//            .endObject()).execute().actionGet();
-//    }
-//
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testMissingType() throws Exception {
-//        createIndex("idx");
-//        ensureGreen();
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject("by_date")
-//                    .startObject("date_histogram")
-//                        .field("field", "timestamp")
-//                        .field("interval", "month")
-//                    .endObject()
-//                    .startObject("aggs")
-//                        .startObject("tag_count")
-//                            // the aggregation type is missing
-//                            //.startObject("cardinality")
-//                                .field("field", "tag")
-//                            //.endObject()
-//                        .endObject()
-//                    .endObject()
-//            .endObject()).execute().actionGet();
-//    }
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testTwoTypes() throws Exception {
+        createIndex("idx");
+        ensureGreen();
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject("in_stock")
+                    .startObject("filter")
+                        .startObject("range")
+                            .startObject("stock")
+                                .field("gt", 0)
+                            .endObject()
+                        .endObject()
+                    .endObject()
+                    .startObject("terms")
+                        .field("field", "stock")
+                    .endObject()
+                .endObject()
+            .endObject()).execute().actionGet();
+    }
+
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testTwoAggs() throws Exception {
+        createIndex("idx");
+        ensureGreen();
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject("by_date")
+                    .startObject("date_histogram")
+                        .field("field", "timestamp")
+                        .field("interval", "month")
+                    .endObject()
+                    .startObject("aggs")
+                        .startObject("tag_count")
+                            .startObject("cardinality")
+                                .field("field", "tag")
+                            .endObject()
+                        .endObject()
+                    .endObject()
+                    .startObject("aggs") // 2nd "aggs": illegal
+                        .startObject("tag_count2")
+                            .startObject("cardinality")
+                                .field("field", "tag")
+                            .endObject()
+                        .endObject()
+                    .endObject()
+            .endObject()).execute().actionGet();
+    }
+
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testInvalidAggregationName() throws Exception {
+
+        Matcher matcher = Pattern.compile("[^\\[\\]>]+").matcher("");
+        String name;
+        SecureRandom rand = new SecureRandom();
+        int len = randomIntBetween(1, 5);
+        char[] word = new char[len];
+        while(true) {
+            for (int i = 0; i < word.length; i++) {
+                word[i] = (char) rand.nextInt(127);
+            }
+            name = String.valueOf(word);
+            if (!matcher.reset(name).matches()) {
+                break;
+            }
+        }
+
+        createIndex("idx");
+        ensureGreen();
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject(name)
+                    .startObject("filter")
+                        .startObject("range")
+                            .startObject("stock")
+                                .field("gt", 0)
+                            .endObject()
+                        .endObject()
+                    .endObject()
+            .endObject()).execute().actionGet();
+    }
+
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testSameAggregationName() throws Exception {
+        createIndex("idx");
+        ensureGreen();
+        final String name = RandomStrings.randomAsciiOfLength(getRandom(), 10);
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject(name)
+                    .startObject("terms")
+                        .field("field", "a")
+                    .endObject()
+                .endObject()
+                .startObject(name)
+                    .startObject("terms")
+                        .field("field", "b")
+                    .endObject()
+                .endObject()
+            .endObject()).execute().actionGet();
+    }
+
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testMissingName() throws Exception {
+        createIndex("idx");
+        ensureGreen();
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject("by_date")
+                    .startObject("date_histogram")
+                        .field("field", "timestamp")
+                        .field("interval", "month")
+                    .endObject()
+                    .startObject("aggs")
+                        // the aggregation name is missing
+                        //.startObject("tag_count")
+                            .startObject("cardinality")
+                                .field("field", "tag")
+                            .endObject()
+                        //.endObject()
+                    .endObject()
+            .endObject()).execute().actionGet();
+    }
+
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testMissingType() throws Exception {
+        createIndex("idx");
+        ensureGreen();
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject("by_date")
+                    .startObject("date_histogram")
+                        .field("field", "timestamp")
+                        .field("interval", "month")
+                    .endObject()
+                    .startObject("aggs")
+                        .startObject("tag_count")
+                            // the aggregation type is missing
+                            //.startObject("cardinality")
+                                .field("field", "tag")
+                            //.endObject()
+                        .endObject()
+                    .endObject()
+            .endObject()).execute().actionGet();
+    }
 
 }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsIT.java
index 3e18f95..ffab844 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsIT.java
@@ -24,6 +24,7 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.script.Script;
@@ -526,7 +527,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .field(TERMS_AGGS_FIELD)
                                 .subAggregation(
                                         topHits("hits").setSize(1)
-                                            .highlighter(new HighlightBuilder().field("text"))
+                                            .addHighlightedField("text")
                                             .setExplain(true)
                                             .addFieldDataField("field1")
                                                 .addScriptField("script", new Script("doc['field1'].value"))
@@ -589,39 +590,38 @@ public class TopHitsIT extends ESIntegTestCase {
         }
     }
 
-    // @Test
-    // public void testFailWithSubAgg() throws Exception {
-    // String source = "{\n" +
-    // "  \"aggs\": {\n" +
-    // "    \"top-tags\": {\n" +
-    // "      \"terms\": {\n" +
-    // "        \"field\": \"tags\"\n" +
-    // "      },\n" +
-    // "      \"aggs\": {\n" +
-    // "        \"top_tags_hits\": {\n" +
-    // "          \"top_hits\": {},\n" +
-    // "          \"aggs\": {\n" +
-    // "            \"max\": {\n" +
-    // "              \"max\": {\n" +
-    // "                \"field\": \"age\"\n" +
-    // "              }\n" +
-    // "            }\n" +
-    // "          }\n" +
-    // "        }\n" +
-    // "      }\n" +
-    // "    }\n" +
-    // "  }\n" +
-    // "}";
-    // try {
-    // client().prepareSearch("idx").setTypes("type")
-    // .setSource(new BytesArray(source))
-    // .get();
-    // fail();
-    // } catch (SearchPhaseExecutionException e) {
-    // assertThat(e.toString(),
-    // containsString("Aggregator [top_tags_hits] of type [top_hits] cannot accept sub-aggregations"));
-    // }
-    // } NORELEASE this needs to be tested in a top_hits aggregations unit test
+    @Test
+    public void testFailWithSubAgg() throws Exception {
+        String source = "{\n" +
+                "  \"aggs\": {\n" +
+                "    \"top-tags\": {\n" +
+                "      \"terms\": {\n" +
+                "        \"field\": \"tags\"\n" +
+                "      },\n" +
+                "      \"aggs\": {\n" +
+                "        \"top_tags_hits\": {\n" +
+                "          \"top_hits\": {},\n" +
+                "          \"aggs\": {\n" +
+                "            \"max\": {\n" +
+                "              \"max\": {\n" +
+                "                \"field\": \"age\"\n" +
+                "              }\n" +
+                "            }\n" +
+                "          }\n" +
+                "        }\n" +
+                "      }\n" +
+                "    }\n" +
+                "  }\n" +
+                "}";
+        try {
+            client().prepareSearch("idx").setTypes("type")
+                    .setSource(new BytesArray(source))
+                            .get();
+            fail();
+        } catch (SearchPhaseExecutionException e) {
+            assertThat(e.toString(), containsString("Aggregator [top_tags_hits] of type [top_hits] cannot accept sub-aggregations"));
+        }
+    }
 
     @Test
     public void testEmptyIndex() throws Exception {
@@ -849,7 +849,7 @@ public class TopHitsIT extends ESIntegTestCase {
                 .setQuery(nestedQuery("comments", matchQuery("comments.message", "comment").queryName("test")))
                 .addAggregation(
                         nested("to-comments").path("comments").subAggregation(
-                                topHits("top-comments").setSize(1).highlighter(new HighlightBuilder().field(hlField)).setExplain(true)
+                                topHits("top-comments").setSize(1).addHighlightedField(hlField).setExplain(true)
                                                 .addFieldDataField("comments.user")
                                         .addScriptField("script", new Script("doc['comments.user'].value")).setFetchSource("message", null)
                                         .setVersion(true).addSort("comments.date", SortOrder.ASC))).get();
@@ -901,7 +901,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                         nested("to-comments")
                                                 .path("comments")
                                                 .subAggregation(topHits("comments")
-                                                        .highlighter(new HighlightBuilder().field(new HighlightBuilder.Field("comments.message").highlightQuery(matchQuery("comments.message", "text"))))
+                                                        .addHighlightedField(new HighlightBuilder.Field("comments.message").highlightQuery(matchQuery("comments.message", "text")))
                                                         .addSort("comments.id", SortOrder.ASC))
                                 )
                 )
diff --git a/core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java b/core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java
index e9bcd2c..e5cc0ee 100644
--- a/core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java
+++ b/core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java
@@ -49,66 +49,65 @@ public class TransportSearchFailuresIT extends ESIntegTestCase {
         return 1;
     }
 
-    // NORELEASE this needs to be done in a unit test
-//    @Test
-//    public void testFailedSearchWithWrongQuery() throws Exception {
-//        logger.info("Start Testing failed search with wrong query");
-//        assertAcked(prepareCreate("test", 1, settingsBuilder().put("routing.hash.type", "simple")));
-//        ensureYellow();
-//
-//        NumShards test = getNumShards("test");
-//
-//        for (int i = 0; i < 100; i++) {
-//            index(client(), Integer.toString(i), "test", i);
-//        }
-//        RefreshResponse refreshResponse = client().admin().indices().refresh(refreshRequest("test")).actionGet();
-//        assertThat(refreshResponse.getTotalShards(), equalTo(test.totalNumShards));
-//        assertThat(refreshResponse.getSuccessfulShards(), equalTo(test.numPrimaries));
-//        assertThat(refreshResponse.getFailedShards(), equalTo(0));
-//        for (int i = 0; i < 5; i++) {
-//            try {
-//                SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
-//                assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
-//                assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
-//                assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
-//                fail("search should fail");
-//            } catch (ElasticsearchException e) {
-//                assertThat(e.unwrapCause(), instanceOf(SearchPhaseExecutionException.class));
-//                // all is well
-//            }
-//        }
-//
-//        allowNodes("test", 2);
-//        assertThat(client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes(">=2").execute().actionGet().isTimedOut(), equalTo(false));
-//
-//        logger.info("Running Cluster Health");
-//        ClusterHealthResponse clusterHealth = client().admin().cluster().health(clusterHealthRequest("test")
-//                .waitForYellowStatus().waitForRelocatingShards(0).waitForActiveShards(test.totalNumShards)).actionGet();
-//        logger.info("Done Cluster Health, status " + clusterHealth.getStatus());
-//        assertThat(clusterHealth.isTimedOut(), equalTo(false));
-//        assertThat(clusterHealth.getStatus(), anyOf(equalTo(ClusterHealthStatus.YELLOW), equalTo(ClusterHealthStatus.GREEN)));
-//        assertThat(clusterHealth.getActiveShards(), equalTo(test.totalNumShards));
-//
-//        refreshResponse = client().admin().indices().refresh(refreshRequest("test")).actionGet();
-//        assertThat(refreshResponse.getTotalShards(), equalTo(test.totalNumShards));
-//        assertThat(refreshResponse.getSuccessfulShards(), equalTo(test.totalNumShards));
-//        assertThat(refreshResponse.getFailedShards(), equalTo(0));
-//
-//        for (int i = 0; i < 5; i++) {
-//            try {
-//                SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
-//                assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
-//                assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
-//                assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
-//                fail("search should fail");
-//            } catch (ElasticsearchException e) {
-//                assertThat(e.unwrapCause(), instanceOf(SearchPhaseExecutionException.class));
-//                // all is well
-//            }
-//        }
-//
-//        logger.info("Done Testing failed search");
-//    }
+    @Test
+    public void testFailedSearchWithWrongQuery() throws Exception {
+        logger.info("Start Testing failed search with wrong query");
+        assertAcked(prepareCreate("test", 1, settingsBuilder().put("routing.hash.type", "simple")));
+        ensureYellow();
+
+        NumShards test = getNumShards("test");
+
+        for (int i = 0; i < 100; i++) {
+            index(client(), Integer.toString(i), "test", i);
+        }
+        RefreshResponse refreshResponse = client().admin().indices().refresh(refreshRequest("test")).actionGet();
+        assertThat(refreshResponse.getTotalShards(), equalTo(test.totalNumShards));
+        assertThat(refreshResponse.getSuccessfulShards(), equalTo(test.numPrimaries));
+        assertThat(refreshResponse.getFailedShards(), equalTo(0));
+        for (int i = 0; i < 5; i++) {
+            try {
+                SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
+                assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
+                assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
+                assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
+                fail("search should fail");
+            } catch (ElasticsearchException e) {
+                assertThat(e.unwrapCause(), instanceOf(SearchPhaseExecutionException.class));
+                // all is well
+            }
+        }
+
+        allowNodes("test", 2);
+        assertThat(client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes(">=2").execute().actionGet().isTimedOut(), equalTo(false));
+
+        logger.info("Running Cluster Health");
+        ClusterHealthResponse clusterHealth = client().admin().cluster().health(clusterHealthRequest("test")
+                .waitForYellowStatus().waitForRelocatingShards(0).waitForActiveShards(test.totalNumShards)).actionGet();
+        logger.info("Done Cluster Health, status " + clusterHealth.getStatus());
+        assertThat(clusterHealth.isTimedOut(), equalTo(false));
+        assertThat(clusterHealth.getStatus(), anyOf(equalTo(ClusterHealthStatus.YELLOW), equalTo(ClusterHealthStatus.GREEN)));
+        assertThat(clusterHealth.getActiveShards(), equalTo(test.totalNumShards));
+
+        refreshResponse = client().admin().indices().refresh(refreshRequest("test")).actionGet();
+        assertThat(refreshResponse.getTotalShards(), equalTo(test.totalNumShards));
+        assertThat(refreshResponse.getSuccessfulShards(), equalTo(test.totalNumShards));
+        assertThat(refreshResponse.getFailedShards(), equalTo(0));
+
+        for (int i = 0; i < 5; i++) {
+            try {
+                SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
+                assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
+                assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
+                assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
+                fail("search should fail");
+            } catch (ElasticsearchException e) {
+                assertThat(e.unwrapCause(), instanceOf(SearchPhaseExecutionException.class));
+                // all is well
+            }
+        }
+
+        logger.info("Done Testing failed search");
+    }
 
     private void index(Client client, String id, String nameValue, int age) throws IOException {
         client.index(Requests.indexRequest("test").type("type1").id(id).source(source(id, nameValue, age)).consistencyLevel(WriteConsistencyLevel.ONE)).actionGet();
diff --git a/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java b/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
index 62eb717..a9b41ce 100644
--- a/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
@@ -370,25 +370,25 @@ public class TransportTwoNodesSearchIT extends ESIntegTestCase {
         assertThat(all.getDocCount(), equalTo(100l));
     }
 
-//    @Test
-//    public void testFailedSearchWithWrongQuery() throws Exception {
-//        prepareData();
-//
-//        NumShards test = getNumShards("test");
-//
-//        logger.info("Start Testing failed search with wrong query");
-//        try {
-//            SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
-//            assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
-//            assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
-//            assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
-//            fail("search should fail");
-//        } catch (ElasticsearchException e) {
-//            assertThat(e.unwrapCause(), instanceOf(SearchPhaseExecutionException.class));
-//            // all is well
-//        }
-//        logger.info("Done Testing failed search");
-//    } NORELEASE this needs to be tested in a unit test
+    @Test
+    public void testFailedSearchWithWrongQuery() throws Exception {
+        prepareData();
+
+        NumShards test = getNumShards("test");
+
+        logger.info("Start Testing failed search with wrong query");
+        try {
+            SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
+            assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
+            assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
+            assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
+            fail("search should fail");
+        } catch (ElasticsearchException e) {
+            assertThat(e.unwrapCause(), instanceOf(SearchPhaseExecutionException.class));
+            // all is well
+        }
+        logger.info("Done Testing failed search");
+    }
 
     @Test
     public void testFailedSearchWithWrongFrom() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/search/builder/NewSearchSourceBuilderTests.java b/core/src/test/java/org/elasticsearch/search/builder/NewSearchSourceBuilderTests.java
deleted file mode 100644
index 5e43a7f..0000000
--- a/core/src/test/java/org/elasticsearch/search/builder/NewSearchSourceBuilderTests.java
+++ /dev/null
@@ -1,632 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.builder;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
-import org.elasticsearch.action.get.GetRequest;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.support.PlainActionFuture;
-import org.elasticsearch.action.termvectors.MultiTermVectorsRequest;
-import org.elasticsearch.action.termvectors.MultiTermVectorsResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.compress.CompressedXContent;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.IndexNameModule;
-import org.elasticsearch.index.analysis.AnalysisModule;
-import org.elasticsearch.index.cache.IndexCacheModule;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.mapper.MapperServiceModule;
-import org.elasticsearch.index.query.AbstractQueryTestCase;
-import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.QueryShardContext;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
-import org.elasticsearch.index.settings.IndexSettingsModule;
-import org.elasticsearch.index.similarity.SimilarityModule;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.analysis.IndicesAnalysisService;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder.InnerHit;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
-import org.elasticsearch.search.highlight.HighlightBuilder;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.search.rescore.RescoreBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.search.suggest.SuggestBuilder;
-import org.elasticsearch.search.suggest.SuggestBuilders;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.TestSearchContext;
-import org.elasticsearch.test.VersionUtils;
-import org.elasticsearch.test.cluster.TestClusterService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.lang.reflect.InvocationHandler;
-import java.lang.reflect.Method;
-import java.lang.reflect.Proxy;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.TimeUnit;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class NewSearchSourceBuilderTests extends ESTestCase {
-
-    protected static final String STRING_FIELD_NAME = "mapped_string";
-    protected static final String STRING_FIELD_NAME_2 = "mapped_string_2";
-    protected static final String INT_FIELD_NAME = "mapped_int";
-    protected static final String DOUBLE_FIELD_NAME = "mapped_double";
-    protected static final String BOOLEAN_FIELD_NAME = "mapped_boolean";
-    protected static final String DATE_FIELD_NAME = "mapped_date";
-    protected static final String OBJECT_FIELD_NAME = "mapped_object";
-    protected static final String GEO_POINT_FIELD_NAME = "mapped_geo_point";
-    protected static final String GEO_SHAPE_FIELD_NAME = "mapped_geo_shape";
-    protected static final String[] MAPPED_FIELD_NAMES = new String[] { STRING_FIELD_NAME, INT_FIELD_NAME, DOUBLE_FIELD_NAME,
-            BOOLEAN_FIELD_NAME, DATE_FIELD_NAME, OBJECT_FIELD_NAME, GEO_POINT_FIELD_NAME, GEO_SHAPE_FIELD_NAME };
-    protected static final String[] MAPPED_LEAF_FIELD_NAMES = new String[] { STRING_FIELD_NAME, INT_FIELD_NAME, DOUBLE_FIELD_NAME,
-            BOOLEAN_FIELD_NAME, DATE_FIELD_NAME, GEO_POINT_FIELD_NAME };
-
-    private static Injector injector;
-    private static IndexQueryParserService queryParserService;
-
-    protected static IndexQueryParserService queryParserService() {
-        return queryParserService;
-    }
-
-    private static Index index;
-
-    protected static Index getIndex() {
-        return index;
-    }
-
-    private static String[] currentTypes;
-
-    protected static String[] getCurrentTypes() {
-        return currentTypes;
-    }
-
-    private static NamedWriteableRegistry namedWriteableRegistry;
-
-    private static String[] randomTypes;
-    private static ClientInvocationHandler clientInvocationHandler = new ClientInvocationHandler();
-
-    /**
-     * Setup for the whole base test class.
-     */
-    @BeforeClass
-    public static void init() throws IOException {
-        // we have to prefer CURRENT since with the range of versions we support it's rather unlikely to get the current actually.
-        Version version = randomBoolean() ? Version.CURRENT : VersionUtils.randomVersionBetween(random(), Version.V_2_0_0_beta1, Version.CURRENT);
-        Settings settings = Settings.settingsBuilder()
-                .put("name", AbstractQueryTestCase.class.toString())
-                .put("path.home", createTempDir())
-                .build();
-        Settings indexSettings = Settings.settingsBuilder()
-                .put(IndexMetaData.SETTING_VERSION_CREATED, version).build();
-        index = new Index(randomAsciiOfLengthBetween(1, 10));
-        final TestClusterService clusterService = new TestClusterService();
-        clusterService.setState(new ClusterState.Builder(clusterService.state()).metaData(new MetaData.Builder().put(
-                new IndexMetaData.Builder(index.name()).settings(indexSettings).numberOfShards(1).numberOfReplicas(0))));
-        final Client proxy = (Client) Proxy.newProxyInstance(Client.class.getClassLoader(), new Class[] { Client.class },
-                clientInvocationHandler);
-        injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)),
-                new SettingsModule(settings),
-                new ThreadPoolModule(new ThreadPool(settings)),
-                new MapperServiceModule(),
-                new IndicesModule(settings) {
-                    @Override
-                    public void configure() {
-                        // skip services
-                        bindQueryParsersExtension();
-                    }
-                },
-                new ScriptModule(settings),
-                new IndexSettingsModule(index, indexSettings),
-                new IndexCacheModule(indexSettings),
-                new AnalysisModule(indexSettings, new IndicesAnalysisService(indexSettings)),
-                new SimilarityModule(indexSettings),
-                new IndexNameModule(index),
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        bind(Client.class).toInstance(proxy);
-                        Multibinder.newSetBinder(binder(), ScoreFunctionParser.class);
-                        bind(ClusterService.class).toProvider(Providers.of(clusterService));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                        bind(NamedWriteableRegistry.class).asEagerSingleton();
-                    }
-                }
-        ).createInjector();
-        queryParserService = injector.getInstance(IndexQueryParserService.class);
-        MapperService mapperService = injector.getInstance(MapperService.class);
-        //create some random type with some default field, those types will stick around for all of the subclasses
-        currentTypes = new String[randomIntBetween(0, 5)];
-        for (int i = 0; i < currentTypes.length; i++) {
-            String type = randomAsciiOfLengthBetween(1, 10);
-            mapperService.merge(type, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(type,
-                    STRING_FIELD_NAME, "type=string",
-                    STRING_FIELD_NAME_2, "type=string",
-                    INT_FIELD_NAME, "type=integer",
-                    DOUBLE_FIELD_NAME, "type=double",
-                    BOOLEAN_FIELD_NAME, "type=boolean",
-                    DATE_FIELD_NAME, "type=date",
-                    OBJECT_FIELD_NAME, "type=object",
-                    GEO_POINT_FIELD_NAME, "type=geo_point,lat_lon=true,geohash=true,geohash_prefix=true",
-                    GEO_SHAPE_FIELD_NAME, "type=geo_shape"
-            ).string()), false, false);
-            // also add mappings for two inner field in the object field
-            mapperService.merge(type, new CompressedXContent("{\"properties\":{\""+OBJECT_FIELD_NAME+"\":{\"type\":\"object\","
-                    + "\"properties\":{\""+DATE_FIELD_NAME+"\":{\"type\":\"date\"},\""+INT_FIELD_NAME+"\":{\"type\":\"integer\"}}}}}"), false, false);
-            currentTypes[i] = type;
-        }
-        namedWriteableRegistry = injector.getInstance(NamedWriteableRegistry.class);
-    }
-
-    @AfterClass
-    public static void afterClass() throws Exception {
-        terminate(injector.getInstance(ThreadPool.class));
-        injector = null;
-        index = null;
-        queryParserService = null;
-        currentTypes = null;
-        namedWriteableRegistry = null;
-        randomTypes = null;
-    }
-
-    @Before
-    public void beforeTest() {
-        clientInvocationHandler.delegate = this;
-        //set some random types to be queried as part the search request, before each test
-        randomTypes = getRandomTypes();
-    }
-
-    protected void setSearchContext(String[] types) {
-        TestSearchContext testSearchContext = new TestSearchContext();
-        testSearchContext.setTypes(types);
-        SearchContext.setCurrent(testSearchContext);
-    }
-
-    @After
-    public void afterTest() {
-        clientInvocationHandler.delegate = null;
-        QueryShardContext.removeTypes();
-        SearchContext.removeCurrent();
-    }
-
-    protected final SearchSourceBuilder createSearchSourceBuilder() throws IOException {
-        SearchSourceBuilder builder = new SearchSourceBuilder();
-        if (randomBoolean()) {
-            builder.from(randomIntBetween(0, 10000));
-        }
-        if (randomBoolean()) {
-            builder.size(randomIntBetween(0, 10000));
-        }
-        if (randomBoolean()) {
-            builder.explain(randomBoolean());
-        }
-        if (randomBoolean()) {
-            builder.version(randomBoolean());
-        }
-        if (randomBoolean()) {
-            builder.trackScores(randomBoolean());
-        }
-        if (randomBoolean()) {
-            builder.minScore(randomFloat() * 1000);
-        }
-        if (randomBoolean()) {
-            builder.timeout(new TimeValue(randomIntBetween(1, 100), randomFrom(TimeUnit.values())));
-        }
-        if (randomBoolean()) {
-            builder.terminateAfter(randomIntBetween(1, 100000));
-        }
-        // if (randomBoolean()) {
-        // builder.defaultRescoreWindowSize(randomIntBetween(1, 100));
-        // }
-        if (randomBoolean()) {
-            int fieldsSize = randomInt(25);
-            List<String> fields = new ArrayList<>(fieldsSize);
-            for (int i = 0; i < fieldsSize; i++) {
-                fields.add(randomAsciiOfLengthBetween(5, 50));
-            }
-            builder.fields(fields);
-        }
-        if (randomBoolean()) {
-            int fieldDataFieldsSize = randomInt(25);
-            for (int i = 0; i < fieldDataFieldsSize; i++) {
-                builder.fieldDataField(randomAsciiOfLengthBetween(5, 50));
-            }
-        }
-        if (randomBoolean()) {
-            int scriptFieldsSize = randomInt(25);
-            for (int i = 0; i < scriptFieldsSize; i++) {
-                builder.scriptField(randomAsciiOfLengthBetween(5, 50), new Script("foo"));
-            }
-        }
-        if (randomBoolean()) {
-            FetchSourceContext fetchSourceContext;
-            int branch = randomInt(5);
-            String[] includes = new String[randomIntBetween(0, 20)];
-            for (int i = 0; i < includes.length; i++) {
-                includes[i] = randomAsciiOfLengthBetween(5, 20);
-            }
-            String[] excludes = new String[randomIntBetween(0, 20)];
-            for (int i = 0; i < excludes.length; i++) {
-                excludes[i] = randomAsciiOfLengthBetween(5, 20);
-            }
-            switch (branch) {
-            case 0:
-                fetchSourceContext = new FetchSourceContext(randomBoolean());
-                break;
-            case 1:
-                fetchSourceContext = new FetchSourceContext(includes, excludes);
-                break;
-            case 2:
-                fetchSourceContext = new FetchSourceContext(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20));
-                break;
-            case 3:
-                fetchSourceContext = new FetchSourceContext(true, includes, excludes, randomBoolean());
-                break;
-            case 4:
-                fetchSourceContext = new FetchSourceContext(includes);
-                break;
-            case 5:
-                fetchSourceContext = new FetchSourceContext(randomAsciiOfLengthBetween(5, 20));
-                break;
-            default:
-                throw new IllegalStateException();
-            }
-            builder.fetchSource(fetchSourceContext);
-        }
-        if (randomBoolean()) {
-            String[] statsGroups = new String[randomIntBetween(0, 20)];
-            for (int i = 0; i < statsGroups.length; i++) {
-                statsGroups[i] = randomAsciiOfLengthBetween(5, 20);
-            }
-            builder.stats(statsGroups);
-        }
-        if (randomBoolean()) {
-            int indexBoostSize = randomIntBetween(1, 10);
-            for (int i = 0; i < indexBoostSize; i++) {
-                builder.indexBoost(randomAsciiOfLengthBetween(5, 20), randomFloat() * 10);
-            }
-        }
-        if (randomBoolean()) {
-            // NORELEASE make RandomQueryBuilder work outside of the
-            // AbstractQueryTestCase
-            // builder.query(RandomQueryBuilder.createQuery(getRandom()));
-            builder.query(QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20)));
-        }
-        if (randomBoolean()) {
-            // NORELEASE make RandomQueryBuilder work outside of the
-            // AbstractQueryTestCase
-            // builder.postFilter(RandomQueryBuilder.createQuery(getRandom()));
-            builder.postFilter(QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20)));
-        }
-        if (randomBoolean()) {
-            int numSorts = randomIntBetween(1, 5);
-            for (int i = 0; i < numSorts; i++) {
-                int branch = randomInt(5);
-                switch (branch) {
-                case 0:
-                    builder.sort(SortBuilders.fieldSort(randomAsciiOfLengthBetween(5, 20)).order(randomFrom(SortOrder.values())));
-                    break;
-                case 1:
-                    builder.sort(SortBuilders.geoDistanceSort(randomAsciiOfLengthBetween(5, 20))
-                            .geohashes(AbstractQueryTestCase.randomGeohash(1, 12)).order(randomFrom(SortOrder.values())));
-                    break;
-                case 2:
-                    builder.sort(SortBuilders.scoreSort().order(randomFrom(SortOrder.values())));
-                    break;
-                case 3:
-                    builder.sort(SortBuilders.scriptSort(new Script("foo"), "number").order(randomFrom(SortOrder.values())));
-                    break;
-                case 4:
-                    builder.sort(randomAsciiOfLengthBetween(5, 20));
-                    break;
-                case 5:
-                    builder.sort(randomAsciiOfLengthBetween(5, 20), randomFrom(SortOrder.values()));
-                    break;
-                }
-            }
-        }
-        if (randomBoolean()) {
-            // NORELEASE need a random highlight builder method
-            builder.highlighter(new HighlightBuilder().field(randomAsciiOfLengthBetween(5, 20)));
-        }
-        if (randomBoolean()) {
-            // NORELEASE need a random suggest builder method
-            builder.suggest(new SuggestBuilder().setText(randomAsciiOfLengthBetween(1, 5)).addSuggestion(
-                    SuggestBuilders.termSuggestion(randomAsciiOfLengthBetween(1, 5))));
-        }
-        if (randomBoolean()) {
-            // NORELEASE need a random inner hits builder method
-            InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-            InnerHit innerHit = new InnerHit();
-            innerHit.field(randomAsciiOfLengthBetween(5, 20));
-            innerHitsBuilder.addNestedInnerHits(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20), innerHit);
-            builder.innerHits(innerHitsBuilder);
-        }
-        if (randomBoolean()) {
-            int numRescores = randomIntBetween(1, 5);
-            for (int i = 0; i < numRescores; i++) {
-                // NORELEASE need a random rescore builder method
-                RescoreBuilder rescoreBuilder = new RescoreBuilder();
-                rescoreBuilder.rescorer(RescoreBuilder.queryRescorer(QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20),
-                        randomAsciiOfLengthBetween(5, 20))));
-                builder.addRescorer(rescoreBuilder);
-            }
-        }
-        if (randomBoolean()) {
-            // NORELEASE need a random aggregation builder method
-            builder.aggregation(AggregationBuilders.avg(randomAsciiOfLengthBetween(5, 20)));
-        }
-        if (true) {
-            // NORELEASE need a method to randomly build content for ext
-            XContentBuilder xContentBuilder = XContentFactory.jsonBuilder();
-            xContentBuilder.startObject();
-            xContentBuilder.field("term_vectors_fetch", randomAsciiOfLengthBetween(5, 20));
-            xContentBuilder.endObject();
-            builder.ext(xContentBuilder);
-        }
-        return builder;
-    }
-
-    /**
-     * Generic test that creates new query from the test query and checks both for equality
-     * and asserts equality on the two queries.
-     */
-    @Test
-    public void testFromXContent() throws IOException {
-        SearchSourceBuilder testBuilder = createSearchSourceBuilder();
-        SearchSourceBuilder newBuilder = parseQuery(testBuilder.toString(), ParseFieldMatcher.STRICT);
-        assertNotSame(testBuilder, newBuilder);
-        assertEquals(testBuilder, newBuilder);
-        assertEquals(testBuilder.hashCode(), newBuilder.hashCode());
-    }
-
-    protected SearchSourceBuilder parseQuery(String queryAsString, ParseFieldMatcher matcher) throws IOException {
-        XContentParser parser = XContentFactory.xContent(queryAsString).createParser(queryAsString);
-        QueryParseContext context = createParseContext();
-        context.reset(parser);
-        context.parseFieldMatcher(matcher);
-        return SearchSourceBuilder.PROTOTYPE.fromXContent(parser, context);
-    }
-
-    /**
-     * Test serialization and deserialization of the test query.
-     */
-    @Test
-    public void testSerialization() throws IOException {
-        SearchSourceBuilder testBuilder = createSearchSourceBuilder();
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            testBuilder.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                SearchSourceBuilder deserializedBuilder = SearchSourceBuilder.PROTOTYPE.readFrom(in);
-                assertEquals(deserializedBuilder, testBuilder);
-                assertEquals(deserializedBuilder.hashCode(), testBuilder.hashCode());
-                assertNotSame(deserializedBuilder, testBuilder);
-            }
-        }
-    }
-
-    @Test
-    public void testEqualsAndHashcode() throws IOException {
-        SearchSourceBuilder firstBuilder = createSearchSourceBuilder();
-        assertFalse("query is equal to null", firstBuilder.equals(null));
-        assertFalse("query is equal to incompatible type", firstBuilder.equals(""));
-        assertTrue("query is not equal to self", firstBuilder.equals(firstBuilder));
-        assertThat("same query's hashcode returns different values if called multiple times", firstBuilder.hashCode(),
-                equalTo(firstBuilder.hashCode()));
-
-        SearchSourceBuilder secondBuilder = copyBuilder(firstBuilder);
-        assertTrue("query is not equal to self", secondBuilder.equals(secondBuilder));
-        assertTrue("query is not equal to its copy", firstBuilder.equals(secondBuilder));
-        assertTrue("equals is not symmetric", secondBuilder.equals(firstBuilder));
-        assertThat("query copy's hashcode is different from original hashcode", secondBuilder.hashCode(), equalTo(firstBuilder.hashCode()));
-
-        SearchSourceBuilder thirdBuilder = copyBuilder(secondBuilder);
-        assertTrue("query is not equal to self", thirdBuilder.equals(thirdBuilder));
-        assertTrue("query is not equal to its copy", secondBuilder.equals(thirdBuilder));
-        assertThat("query copy's hashcode is different from original hashcode", secondBuilder.hashCode(), equalTo(thirdBuilder.hashCode()));
-        assertTrue("equals is not transitive", firstBuilder.equals(thirdBuilder));
-        assertThat("query copy's hashcode is different from original hashcode", firstBuilder.hashCode(), equalTo(thirdBuilder.hashCode()));
-        assertTrue("equals is not symmetric", thirdBuilder.equals(secondBuilder));
-        assertTrue("equals is not symmetric", thirdBuilder.equals(firstBuilder));
-    }
-
-    //we use the streaming infra to create a copy of the query provided as argument
-    protected SearchSourceBuilder copyBuilder(SearchSourceBuilder builder) throws IOException {
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            builder.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                SearchSourceBuilder secondQuery = SearchSourceBuilder.PROTOTYPE.readFrom(in);
-                return secondQuery;
-            }
-        }
-    }
-
-    /**
-     * @return a new {@link QueryShardContext} based on the base test index and queryParserService
-     */
-    protected static QueryShardContext createShardContext() {
-        QueryShardContext queryCreationContext = new QueryShardContext(index, queryParserService);
-        queryCreationContext.reset();
-        queryCreationContext.parseFieldMatcher(ParseFieldMatcher.EMPTY);
-        return queryCreationContext;
-    }
-
-    /**
-     * @return a new {@link QueryParseContext} based on the base test index and queryParserService
-     */
-    protected static QueryParseContext createParseContext() {
-        return createShardContext().parseContext();
-    }
-
-    protected String[] getRandomTypes() {
-        String[] types;
-        if (currentTypes.length > 0 && randomBoolean()) {
-            int numberOfQueryTypes = randomIntBetween(1, currentTypes.length);
-            types = new String[numberOfQueryTypes];
-            for (int i = 0; i < numberOfQueryTypes; i++) {
-                types[i] = randomFrom(currentTypes);
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[] { MetaData.ALL };
-            } else {
-                types = new String[0];
-            }
-        }
-        return types;
-    }
-
-    private static class ClientInvocationHandler implements InvocationHandler {
-        NewSearchSourceBuilderTests delegate;
-
-        @Override
-        public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
-            if (method.equals(Client.class.getDeclaredMethod("get", GetRequest.class))) {
-                return new PlainActionFuture<GetResponse>() {
-                    @Override
-                    public GetResponse get() throws InterruptedException, ExecutionException {
-                        return delegate.executeGet((GetRequest) args[0]);
-                    }
-                };
-            } else if (method.equals(Client.class.getDeclaredMethod("multiTermVectors", MultiTermVectorsRequest.class))) {
-                return new PlainActionFuture<MultiTermVectorsResponse>() {
-                    @Override
-                    public MultiTermVectorsResponse get() throws InterruptedException, ExecutionException {
-                        return delegate.executeMultiTermVectors((MultiTermVectorsRequest) args[0]);
-                    }
-                };
-            } else if (method.equals(Object.class.getDeclaredMethod("toString"))) {
-                return "MockClient";
-            }
-            throw new UnsupportedOperationException("this test can't handle calls to: " + method);
-        }
-
-    }
-
-    /**
-     * Override this to handle {@link Client#get(GetRequest)} calls from parsers
-     * / builders
-     */
-    protected GetResponse executeGet(GetRequest getRequest) {
-        throw new UnsupportedOperationException("this test can't handle GET requests");
-    }
-
-    /**
-     * Override this to handle {@link Client#get(GetRequest)} calls from parsers
-     * / builders
-     */
-    protected MultiTermVectorsResponse executeMultiTermVectors(MultiTermVectorsRequest mtvRequest) {
-        throw new UnsupportedOperationException("this test can't handle MultiTermVector requests");
-    }
-
-    public void testParseIncludeExclude() throws IOException {
-        SearchSourceBuilder builder = new SearchSourceBuilder();
-        {
-            String restContent = " { \"_source\": { \"includes\": \"include\", \"excludes\": \"*.field2\"}}";
-            try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                SearchSourceBuilder searchSourceBuilder = builder.fromXContent(parser, new QueryParseContext(queryParserService.indicesQueriesRegistry()));
-                assertArrayEquals(new String[]{"*.field2" }, searchSourceBuilder.fetchSource().excludes());
-                assertArrayEquals(new String[]{"include" }, searchSourceBuilder.fetchSource().includes());
-            }
-        }
-        {
-            String restContent = " { \"_source\": false}";
-            try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                SearchSourceBuilder searchSourceBuilder = builder.fromXContent(parser, new QueryParseContext(queryParserService.indicesQueriesRegistry()));
-                assertArrayEquals(new String[]{}, searchSourceBuilder.fetchSource().excludes());
-                assertArrayEquals(new String[]{}, searchSourceBuilder.fetchSource().includes());
-                assertFalse(searchSourceBuilder.fetchSource().fetchSource());
-            }
-        }
-    }
-
-    public void testParseSort() throws IOException {
-        SearchSourceBuilder builder = new SearchSourceBuilder();
-        {
-            String restContent = " { \"sort\": \"foo\"}";
-            try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                SearchSourceBuilder searchSourceBuilder = builder.fromXContent(parser, new QueryParseContext(queryParserService.indicesQueriesRegistry()));
-                assertEquals(1, searchSourceBuilder.sorts().size());
-                assertEquals("{\"foo\":{}}", searchSourceBuilder.sorts().get(0).toUtf8());
-            }
-        }
-
-        {
-            String restContent = "{\"sort\" : [\n" +
-                    "        { \"post_date\" : {\"order\" : \"asc\"}},\n" +
-                    "        \"user\",\n" +
-                    "        { \"name\" : \"desc\" },\n" +
-                    "        { \"age\" : \"desc\" },\n" +
-                    "        \"_score\"\n" +
-                    "    ]}";
-            try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                SearchSourceBuilder searchSourceBuilder = builder.fromXContent(parser, new QueryParseContext(queryParserService.indicesQueriesRegistry()));
-                assertEquals(5, searchSourceBuilder.sorts().size());
-                assertEquals("{\"post_date\":{\"order\":\"asc\"}}", searchSourceBuilder.sorts().get(0).toUtf8());
-                assertEquals("\"user\"", searchSourceBuilder.sorts().get(1).toUtf8());
-                assertEquals("{\"name\":\"desc\"}", searchSourceBuilder.sorts().get(2).toUtf8());
-                assertEquals("{\"age\":\"desc\"}", searchSourceBuilder.sorts().get(3).toUtf8());
-                assertEquals("\"_score\"", searchSourceBuilder.sorts().get(4).toUtf8());
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java b/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
index 1b6017e..982a3d9 100644
--- a/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
@@ -27,6 +27,8 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
+
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery;
 import org.elasticsearch.common.settings.Settings;
@@ -43,7 +45,6 @@ import org.elasticsearch.search.aggregations.AggregationBuilders;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
 import org.elasticsearch.search.aggregations.bucket.global.Global;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.sort.SortBuilders;
 import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -53,42 +54,15 @@ import org.hamcrest.Matchers;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Set;
+import java.util.*;
 
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.hasParentQuery;
-import static org.elasticsearch.index.query.QueryBuilders.idsQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.multiMatchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.notQuery;
-import static org.elasticsearch.index.query.QueryBuilders.prefixQuery;
-import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termsQuery;
+import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.scriptFunction;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.weightFactorFunction;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThanOrEqualTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.notNullValue;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 /**
  *
@@ -1480,9 +1454,14 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
 
         SearchResponse resp;
         resp = client().prepareSearch("test")
-                .setSource(new SearchSourceBuilder().query(QueryBuilders.hasChildQuery("posts", QueryBuilders.matchQuery("field", "bar"))))
-                .get();
+                .setSource(new BytesArray("{\"query\": {\"has_child\": {\"type\": \"posts\", \"query\": {\"match\": {\"field\": \"bar\"}}}}}")).get();
+        assertHitCount(resp, 1L);
+
+        // Now reverse the order for the type after the query
+        resp = client().prepareSearch("test")
+                .setSource(new BytesArray("{\"query\": {\"has_child\": {\"query\": {\"match\": {\"field\": \"bar\"}}, \"type\": \"posts\"}}}")).get();
         assertHitCount(resp, 1L);
+
     }
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java b/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java
index 7fbe552..e07bb73 100644
--- a/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java
+++ b/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.search.fetch;
 
 import com.google.common.collect.ImmutableMap;
-
 import org.apache.lucene.index.PostingsEnum;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.util.BytesRef;
@@ -28,13 +27,13 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.termvectors.TermVectorsRequest;
 import org.elasticsearch.action.termvectors.TermVectorsResponse;
 import org.elasticsearch.common.Priority;
-import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.search.SearchHitField;
 import org.elasticsearch.search.SearchModule;
 import org.elasticsearch.search.SearchParseElement;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.internal.InternalSearchHit;
 import org.elasticsearch.search.internal.InternalSearchHitField;
 import org.elasticsearch.search.internal.SearchContext;
@@ -50,9 +49,10 @@ import java.util.HashMap;
 import java.util.Map;
 
 import static org.elasticsearch.client.Requests.indexRequest;
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.Matchers.equalTo;
 
 /**
  *
@@ -88,16 +88,14 @@ public class FetchSubPhasePluginIT extends ESIntegTestCase {
 
         client().admin().indices().prepareRefresh().execute().actionGet();
 
-        XContentBuilder extSource = jsonBuilder().startObject()
+        String searchSource = jsonBuilder().startObject()
                 .field("term_vectors_fetch", "test")
-                .endObject();
-         SearchResponse response = client().prepareSearch().setSource(new SearchSourceBuilder().ext(extSource)).get();
+                .endObject().string();
+        SearchResponse response = client().prepareSearch().setSource(new BytesArray(searchSource)).get();
         assertSearchResponse(response);
         assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("i"), equalTo(2));
-        assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("am"),
-                equalTo(2));
-        assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("sam"),
-                equalTo(1));
+        assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("am"), equalTo(2));
+        assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("sam"), equalTo(1));
     }
 
     public static class FetchTermVectorsPlugin extends Plugin {
diff --git a/core/src/test/java/org/elasticsearch/search/fields/SearchFieldsIT.java b/core/src/test/java/org/elasticsearch/search/fields/SearchFieldsIT.java
index 4559394..1075347 100644
--- a/core/src/test/java/org/elasticsearch/search/fields/SearchFieldsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/fields/SearchFieldsIT.java
@@ -21,7 +21,9 @@ package org.elasticsearch.search.fields;
 
 import com.google.common.collect.ImmutableSet;
 
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.Base64;
@@ -33,13 +35,11 @@ import org.elasticsearch.common.joda.Joda;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.internal.TimestampFieldMapper;
-import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHitField;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.joda.time.DateTime;
@@ -63,10 +63,7 @@ import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFail
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.*;
 
 /**
  *
@@ -521,22 +518,21 @@ public class SearchFieldsIT extends ESIntegTestCase {
         createIndex("test");
         indexRandom(true, client().prepareIndex("test", "type", "1").setSource("test_field", "foobar"));
         refresh();
-        SearchResponse searchResponse = client().prepareSearch("test").setTypes("type").setSource(
-                new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).fieldDataField("test_field")).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setTypes("type").setSource(new BytesArray(new BytesRef("{\"query\":{\"match_all\":{}},\"fielddata_fields\": \"test_field\"}"))).get();
         assertHitCount(searchResponse, 1);
         Map<String,SearchHitField> fields = searchResponse.getHits().getHits()[0].getFields();
         assertThat((String)fields.get("test_field").value(), equalTo("foobar"));
     }
 
-//    @Test(expected = SearchPhaseExecutionException.class)
-//    public void testInvalidFieldDataField() throws ExecutionException, InterruptedException {
-//        createIndex("test");
-//        if (randomBoolean()) {
-//            client().prepareSearch("test").setTypes("type").setSource(new BytesArray(new BytesRef("{\"query\":{\"match_all\":{}},\"fielddata_fields\": {}}"))).get();
-//        } else {
-//            client().prepareSearch("test").setTypes("type").setSource(new BytesArray(new BytesRef("{\"query\":{\"match_all\":{}},\"fielddata_fields\": 1.0}"))).get();
-//        }
-//    } NORELEASE need a unit test for this
+    @Test(expected = SearchPhaseExecutionException.class)
+    public void testInvalidFieldDataField() throws ExecutionException, InterruptedException {
+        createIndex("test");
+        if (randomBoolean()) {
+            client().prepareSearch("test").setTypes("type").setSource(new BytesArray(new BytesRef("{\"query\":{\"match_all\":{}},\"fielddata_fields\": {}}"))).get();
+        } else {
+            client().prepareSearch("test").setTypes("type").setSource(new BytesArray(new BytesRef("{\"query\":{\"match_all\":{}},\"fielddata_fields\": 1.0}"))).get();
+        }
+    }
 
     @Test
     public void testFieldsPulledFromFieldData() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java b/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
index 227d14a..c2c2782 100644
--- a/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
+++ b/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
@@ -27,8 +27,11 @@ import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilder;
+import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
 import org.elasticsearch.search.MultiValueMode;
 import org.elasticsearch.search.SearchHits;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -45,23 +48,11 @@ import java.util.concurrent.ExecutionException;
 import static org.elasticsearch.client.Requests.indexRequest;
 import static org.elasticsearch.client.Requests.searchRequest;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.functionScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.exponentialDecayFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.gaussDecayFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.linearDecayFunction;
+import static org.elasticsearch.index.query.QueryBuilders.*;
+import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.*;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertOrderedSearchHits;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.closeTo;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.isOneOf;
-import static org.hamcrest.Matchers.lessThan;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 
 public class DecayFunctionScoreIT extends ESIntegTestCase {
@@ -442,10 +433,10 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
         SearchResponse sr = response.actionGet();
         assertOrderedSearchHits(sr, "2", "1");
     }
-
+    
     @Test
     public void testParseDateMath() throws Exception {
-
+        
         assertAcked(prepareCreate("test").addMapping(
                 "type1",
                 jsonBuilder().startObject().startObject("type1").startObject("properties").startObject("test").field("type", "string")
@@ -466,7 +457,7 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
 
         assertNoFailures(sr);
         assertOrderedSearchHits(sr, "1", "2");
-
+        
         sr = client().search(
                 searchRequest().source(
                         searchSource().query(
@@ -591,9 +582,9 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
         List<IndexRequestBuilder> indexBuilders = new ArrayList<>();
 
         for (int i = 0; i < numDocs; i++) {
-            double lat = 100 + (int) (10.0 * (i) / (numDocs));
+            double lat = 100 + (int) (10.0 * (float) (i) / (float) (numDocs));
             double lon = 100;
-            int day = (int) (29.0 * (i) / (numDocs)) + 1;
+            int day = (int) (29.0 * (float) (i) / (float) (numDocs)) + 1;
             String dayString = day < 10 ? "0" + Integer.toString(day) : Integer.toString(day);
             String date = "2013-05-" + dayString;
 
@@ -781,7 +772,7 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
 
         assertThat(sh.getAt(0).getId(), equalTo("2"));
         assertThat(sh.getAt(1).getId(), equalTo("1"));
-        assertThat(1.0 - sh.getAt(0).getScore(), closeTo((1.0 - sh.getAt(1).getScore())/3.0, 1.e-6d));
+        assertThat((double)(1.0 - sh.getAt(0).getScore()), closeTo((double)((1.0 - sh.getAt(1).getScore())/3.0), 1.e-6d));
         response = client().search(
                 searchRequest().source(
                         searchSource().query(
@@ -789,57 +780,49 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
         sr = response.actionGet();
         assertSearchHits(sr, "1", "2");
         sh = sr.getHits();
-        assertThat((double) (sh.getAt(0).getScore()), closeTo((sh.getAt(1).getScore()), 1.e-6d));
+        assertThat((double) (sh.getAt(0).getScore()), closeTo((double) (sh.getAt(1).getScore()), 1.e-6d));
     }
 
-    // @Test
-    // public void errorMessageForFaultyFunctionScoreBody() throws Exception {
-    // assertAcked(prepareCreate("test").addMapping(
-    // "type",
-    // jsonBuilder().startObject().startObject("type").startObject("properties").startObject("test").field("type",
-    // "string")
-    // .endObject().startObject("num").field("type",
-    // "double").endObject().endObject().endObject().endObject()));
-    // ensureYellow();
-    // client().index(
-    // indexRequest("test").type("type").source(jsonBuilder().startObject().field("test",
-    // "value").field("num", 1.0).endObject()))
-    // .actionGet();
-    // refresh();
-    //
-    // XContentBuilder query = XContentFactory.jsonBuilder();
-    // // query that contains a single function and a functions[] array
-    // query.startObject().startObject("query").startObject("function_score").field("weight",
-    // "1").startArray("functions").startObject().startObject("script_score").field("script",
-    // "3").endObject().endObject().endArray().endObject().endObject().endObject();
-    // try {
-    // client().search(searchRequest().source(query.bytes())).actionGet();
-    // fail("Search should result in SearchPhaseExecutionException");
-    // } catch (SearchPhaseExecutionException e) {
-    // logger.info(e.shardFailures()[0].reason());
-    // assertThat(e.shardFailures()[0].reason(),
-    // containsString("already found [weight], now encountering [functions]."));
-    // }
-    //
-    // query = XContentFactory.jsonBuilder();
-    // // query that contains a single function (but not boost factor) and a
-    // functions[] array
-    // query.startObject().startObject("query").startObject("function_score").startObject("random_score").field("seed",
-    // 3).endObject().startArray("functions").startObject().startObject("random_score").field("seed",
-    // 3).endObject().endObject().endArray().endObject().endObject().endObject();
-    // try {
-    // client().search(searchRequest().source(query.bytes())).actionGet();
-    // fail("Search should result in SearchPhaseExecutionException");
-    // } catch (SearchPhaseExecutionException e) {
-    // logger.info(e.shardFailures()[0].reason());
-    // assertThat(e.shardFailures()[0].reason(),
-    // containsString("already found [random_score], now encountering [functions]"));
-    // assertThat(e.shardFailures()[0].reason(),
-    // not(containsString("did you mean [boost] instead?")));
-    //
-    // } NORELEASE this needs to be tested in a unit test
-    // (FunctionScoreQueryBuilderTests)
-    // }
+    @Test
+    public void errorMessageForFaultyFunctionScoreBody() throws Exception {
+        assertAcked(prepareCreate("test").addMapping(
+                "type",
+                jsonBuilder().startObject().startObject("type").startObject("properties").startObject("test").field("type", "string")
+                        .endObject().startObject("num").field("type", "double").endObject().endObject().endObject().endObject()));
+        ensureYellow();
+        client().index(
+                indexRequest("test").type("type").source(jsonBuilder().startObject().field("test", "value").field("num", 1.0).endObject()))
+                .actionGet();
+        refresh();
+
+        XContentBuilder query = XContentFactory.jsonBuilder();
+        // query that contains a single function and a functions[] array
+        query.startObject().startObject("function_score").field("weight", "1").startArray("functions").startObject().startObject("script_score").field("script", "3").endObject().endObject().endArray().endObject().endObject();
+        try {
+            client().search(
+                    searchRequest().source(
+                            searchSource().query(query))).actionGet();
+            fail("Search should result in SearchPhaseExecutionException");
+        } catch (SearchPhaseExecutionException e) {
+            logger.info(e.shardFailures()[0].reason());
+            assertThat(e.shardFailures()[0].reason(), containsString("already found [weight], now encountering [functions]."));
+        }
+
+        query = XContentFactory.jsonBuilder();
+        // query that contains a single function (but not boost factor) and a functions[] array
+        query.startObject().startObject("function_score").startObject("random_score").field("seed", 3).endObject().startArray("functions").startObject().startObject("random_score").field("seed", 3).endObject().endObject().endArray().endObject().endObject();
+        try {
+            client().search(
+                    searchRequest().source(
+                            searchSource().query(query))).actionGet();
+            fail("Search should result in SearchPhaseExecutionException");
+        } catch (SearchPhaseExecutionException e) {
+            logger.info(e.shardFailures()[0].reason());
+            assertThat(e.shardFailures()[0].reason(), containsString("already found [random_score], now encountering [functions]"));
+            assertThat(e.shardFailures()[0].reason(), not(containsString("did you mean [boost] instead?")));
+
+        }
+    }
 
     @Test
     public void testExplainString() throws IOException, ExecutionException, InterruptedException {
diff --git a/core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreIT.java b/core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreIT.java
index 2eda19f..4d78de1 100644
--- a/core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreIT.java
+++ b/core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreIT.java
@@ -23,20 +23,18 @@ import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.lucene.search.function.FieldValueFactorFunction;
 import org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.MatchAllQueryBuilder;
-import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilder;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders;
 import org.elasticsearch.index.query.functionscore.weight.WeightBuilder;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -266,13 +264,13 @@ public class FunctionScoreIT extends ESIntegTestCase {
         double expectedScore;
         switch(scoreMode) {
             case MULTIPLY:
-            expectedScore = 1.0;
+                expectedScore = 1.0;
                 break;
             case MAX:
-            expectedScore = Float.MAX_VALUE * -1.0;
+                expectedScore = Float.MAX_VALUE * -1.0;
                 break;
             case MIN:
-            expectedScore = Float.MAX_VALUE;
+                expectedScore = Float.MAX_VALUE;
                 break;
             default:
                 expectedScore = 0.0;
@@ -285,19 +283,19 @@ public class FunctionScoreIT extends ESIntegTestCase {
             weightSum += weights[i];
             switch(scoreMode) {
                 case AVG:
-                expectedScore += functionScore;
+                    expectedScore += functionScore;
                     break;
                 case MAX:
-                expectedScore = Math.max(functionScore, expectedScore);
+                    expectedScore = Math.max(functionScore, expectedScore);
                     break;
                 case MIN:
-                expectedScore = Math.min(functionScore, expectedScore);
+                    expectedScore = Math.min(functionScore, expectedScore);
                     break;
                 case SUM:
-                expectedScore += functionScore;
+                    expectedScore += functionScore;
                     break;
                 case MULTIPLY:
-                expectedScore *= functionScore;
+                    expectedScore *= functionScore;
                     break;
                 default:
                     throw new UnsupportedOperationException();
@@ -381,13 +379,32 @@ public class FunctionScoreIT extends ESIntegTestCase {
 
         index(INDEX, TYPE, "1", SIMPLE_DOC);
         refresh();
+        String query =jsonBuilder().startObject()
+                .startObject("query")
+                .startObject("function_score")
+                .startArray("functions")
+                .startObject()
+                .field("weight",2)
+                .endObject()
+                .endArray()
+                .endObject()
+                .endObject()
+                .endObject().string();
         SearchResponse response = client().search(
-                searchRequest().source(new SearchSourceBuilder().query(QueryBuilders.functionScoreQuery(ScoreFunctionBuilders.weightFactorFunction(2.0f))))
+                searchRequest().source(new BytesArray(query))
                 ).actionGet();
         assertSearchResponse(response);
         assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
+
+        query =jsonBuilder().startObject()
+                .startObject("query")
+                .startObject("function_score")
+                .field("weight",2)
+                .endObject()
+                .endObject()
+                .endObject().string();
         response = client().search(
-                searchRequest().source(new SearchSourceBuilder().query(QueryBuilders.functionScoreQuery(ScoreFunctionBuilders.weightFactorFunction(2.0f))))
+                searchRequest().source(new BytesArray(query))
         ).actionGet();
         assertSearchResponse(response);
         assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
@@ -401,7 +418,7 @@ public class FunctionScoreIT extends ESIntegTestCase {
         ).actionGet();
         assertSearchResponse(response);
         assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
-    } 
+    }
 
     @Test
     public void testScriptScoresNested() throws IOException {
@@ -417,7 +434,7 @@ public class FunctionScoreIT extends ESIntegTestCase {
                                                 functionScoreQuery(scriptFunction(new Script("1"))),
                                                 scriptFunction(new Script("_score.doubleValue()"))),
                                         scriptFunction(new Script("_score.doubleValue()"))
-                                        )
+                                )
                         )
                 )
         ).actionGet();
@@ -465,7 +482,7 @@ public class FunctionScoreIT extends ESIntegTestCase {
                                 new FunctionScoreQueryBuilder.FilterFunctionBuilder(scriptFunction(new Script(Float.toString(score)))),
                                 new FunctionScoreQueryBuilder.FilterFunctionBuilder(scriptFunction(new Script(Float.toString(score))))
                         }).scoreMode(FiltersFunctionScoreQuery.ScoreMode.AVG).setMinScore(minScore)))
-        ).actionGet();
+                ).actionGet();
         if (score < minScore) {
             assertThat(searchResponse.getHits().getTotalHits(), is(0l));
         } else {
@@ -527,7 +544,7 @@ public class FunctionScoreIT extends ESIntegTestCase {
         float termQueryScore = 0.19178301f;
         for (CombineFunction combineFunction : CombineFunction.values()) {
             testMinScoreApplied(combineFunction, termQueryScore);
-    }
+        }
     }
 
     protected void testMinScoreApplied(CombineFunction boostMode, float expectedScore) throws InterruptedException, ExecutionException {
diff --git a/core/src/test/java/org/elasticsearch/search/geo/GeoBoundingBoxIT.java b/core/src/test/java/org/elasticsearch/search/geo/GeoBoundingBoxIT.java
index 1f4e953..b61b389 100644
--- a/core/src/test/java/org/elasticsearch/search/geo/GeoBoundingBoxIT.java
+++ b/core/src/test/java/org/elasticsearch/search/geo/GeoBoundingBoxIT.java
@@ -22,12 +22,12 @@ package org.elasticsearch.search.geo;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.index.query.GeoValidationMethod;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
 import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
@@ -289,43 +289,43 @@ public class GeoBoundingBoxIT extends ESIntegTestCase {
 
         SearchResponse searchResponse = client().prepareSearch()
                 .setQuery(
-                        geoBoundingBoxQuery("location").coerce(true).setCorners(50, -180, -50, 180)
+                        geoBoundingBoxQuery("location").setValidationMethod(GeoValidationMethod.COERCE).setCorners(50, -180, -50, 180)
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(1l));
         searchResponse = client().prepareSearch()
                 .setQuery(
-                        geoBoundingBoxQuery("location").coerce(true).setCorners(50, -180, -50, 180).type("indexed")
+                        geoBoundingBoxQuery("location").setValidationMethod(GeoValidationMethod.COERCE).setCorners(50, -180, -50, 180).type("indexed")
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(1l));
         searchResponse = client().prepareSearch()
                 .setQuery(
-                        geoBoundingBoxQuery("location").coerce(true).setCorners(90, -180, -90, 180)
+                        geoBoundingBoxQuery("location").setValidationMethod(GeoValidationMethod.COERCE).setCorners(90, -180, -90, 180)
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(2l));
         searchResponse = client().prepareSearch()
                 .setQuery(
-                        geoBoundingBoxQuery("location").coerce(true).setCorners(90, -180, -90, 180).type("indexed")
+                        geoBoundingBoxQuery("location").setValidationMethod(GeoValidationMethod.COERCE).setCorners(90, -180, -90, 180).type("indexed")
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(2l));
 
         searchResponse = client().prepareSearch()
                 .setQuery(
-                        geoBoundingBoxQuery("location").coerce(true).setCorners(50, 0, -50, 360)
+                        geoBoundingBoxQuery("location").setValidationMethod(GeoValidationMethod.COERCE).setCorners(50, 0, -50, 360)
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(1l));
         searchResponse = client().prepareSearch()
                 .setQuery(
-                        geoBoundingBoxQuery("location").coerce(true).setCorners(50, 0, -50, 360).type("indexed")
+                        geoBoundingBoxQuery("location").setValidationMethod(GeoValidationMethod.COERCE).setCorners(50, 0, -50, 360).type("indexed")
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(1l));
         searchResponse = client().prepareSearch()
                 .setQuery(
-                        geoBoundingBoxQuery("location").coerce(true).setCorners(90, 0, -90, 360)
+                        geoBoundingBoxQuery("location").setValidationMethod(GeoValidationMethod.COERCE).setCorners(90, 0, -90, 360)
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(2l));
         searchResponse = client().prepareSearch()
                 .setQuery(
-                        geoBoundingBoxQuery("location").coerce(true).setCorners(90, 0, -90, 360).type("indexed")
+                        geoBoundingBoxQuery("location").setValidationMethod(GeoValidationMethod.COERCE).setCorners(90, 0, -90, 360).type("indexed")
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(2l));
     }
diff --git a/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java b/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java
index 4a2c8bd..4548fbd 100644
--- a/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java
+++ b/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java
@@ -536,22 +536,22 @@ public class GeoFilterIT extends ESIntegTestCase {
 
             }
         }
-        // NORELEASE these should be tested in GeohashCellQueryBuilderTests
-//        logger.info("Testing lat/lon format");
-//        String pointTest1 = "{\"geohash_cell\": {\"pin\": {\"lat\": " + point.lat() + ",\"lon\": " + point.lon() + "},\"precision\": " + precision + ",\"neighbors\": true}}";
-//        SearchResponse results3 = client().prepareSearch("locations").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(pointTest1).execute().actionGet();
-//        assertHitCount(results3, neighbors.size() + 1);
-//
-//
-//        logger.info("Testing String format");
-//        String pointTest2 = "{\"geohash_cell\": {\"pin\": \"" + point.lat() + "," + point.lon() + "\",\"precision\": " + precision + ",\"neighbors\": true}}";
-//        SearchResponse results4 = client().prepareSearch("locations").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(pointTest2).execute().actionGet();
-//        assertHitCount(results4, neighbors.size() + 1);
-//
-//        logger.info("Testing Array format");
-//        String pointTest3 = "{\"geohash_cell\": {\"pin\": [" + point.lon() + "," + point.lat() + "],\"precision\": " + precision + ",\"neighbors\": true}}";
-//        SearchResponse results5 = client().prepareSearch("locations").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(pointTest3).execute().actionGet();
-//        assertHitCount(results5, neighbors.size() + 1);
+
+        logger.info("Testing lat/lon format");
+        String pointTest1 = "{\"geohash_cell\": {\"pin\": {\"lat\": " + point.lat() + ",\"lon\": " + point.lon() + "},\"precision\": " + precision + ",\"neighbors\": true}}";
+        SearchResponse results3 = client().prepareSearch("locations").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(pointTest1).execute().actionGet();
+        assertHitCount(results3, neighbors.size() + 1);
+
+
+        logger.info("Testing String format");
+        String pointTest2 = "{\"geohash_cell\": {\"pin\": \"" + point.lat() + "," + point.lon() + "\",\"precision\": " + precision + ",\"neighbors\": true}}";
+        SearchResponse results4 = client().prepareSearch("locations").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(pointTest2).execute().actionGet();
+        assertHitCount(results4, neighbors.size() + 1);
+
+        logger.info("Testing Array format");
+        String pointTest3 = "{\"geohash_cell\": {\"pin\": [" + point.lon() + "," + point.lat() + "],\"precision\": " + precision + ",\"neighbors\": true}}";
+        SearchResponse results5 = client().prepareSearch("locations").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(pointTest3).execute().actionGet();
+        assertHitCount(results5, neighbors.size() + 1);
     }
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java b/core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java
index 64b42a0..670d317 100644
--- a/core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java
@@ -225,44 +225,43 @@ public class GeoShapeIntegrationIT extends ESIntegTestCase {
         assertThat(before, equalTo(after));
     }
 
-    // NORELEASE  these should be tested in GeoShapeQueryBuilderTests
-//    @Test
-//    public void testParsingMultipleShapes() throws Exception {
-//        String mapping = XContentFactory.jsonBuilder()
-//                .startObject()
-//                .startObject("type1")
-//                .startObject("properties")
-//                .startObject("location1")
-//                .field("type", "geo_shape")
-//                .endObject()
-//                .startObject("location2")
-//                .field("type", "geo_shape")
-//                .endObject()
-//                .endObject()
-//                .endObject()
-//                .endObject()
-//                .string();
-//
-//        assertAcked(prepareCreate("test").addMapping("type1", mapping));
-//        ensureYellow();
-//
-//        String p1 = "\"location1\" : {\"type\":\"polygon\", \"coordinates\":[[[-10,-10],[10,-10],[10,10],[-10,10],[-10,-10]]]}";
-//        String p2 = "\"location2\" : {\"type\":\"polygon\", \"coordinates\":[[[-20,-20],[20,-20],[20,20],[-20,20],[-20,-20]]]}";
-//        String o1 = "{" + p1 + ", " + p2 + "}";
-//
-//        indexRandom(true, client().prepareIndex("test", "type1", "1").setSource(o1));
-//
-//        String filter = "{\"geo_shape\": {\"location2\": {\"indexed_shape\": {"
-//                + "\"id\": \"1\","
-//                + "\"type\": \"type1\","
-//                + "\"index\": \"test\","
-//                + "\"path\": \"location2\""
-//                + "}}}}";
-//
-//        SearchResponse result = client().prepareSearch("test").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(filter).execute().actionGet();
-//        assertSearchResponse(result);
-//        assertHitCount(result, 1);
-//    }
+    @Test
+    public void testParsingMultipleShapes() throws Exception {
+        String mapping = XContentFactory.jsonBuilder()
+                .startObject()
+                .startObject("type1")
+                .startObject("properties")
+                .startObject("location1")
+                .field("type", "geo_shape")
+                .endObject()
+                .startObject("location2")
+                .field("type", "geo_shape")
+                .endObject()
+                .endObject()
+                .endObject()
+                .endObject()
+                .string();
+
+        assertAcked(prepareCreate("test").addMapping("type1", mapping));
+        ensureYellow();
+
+        String p1 = "\"location1\" : {\"type\":\"polygon\", \"coordinates\":[[[-10,-10],[10,-10],[10,10],[-10,10],[-10,-10]]]}";
+        String p2 = "\"location2\" : {\"type\":\"polygon\", \"coordinates\":[[[-20,-20],[20,-20],[20,20],[-20,20],[-20,-20]]]}";
+        String o1 = "{" + p1 + ", " + p2 + "}";
+
+        indexRandom(true, client().prepareIndex("test", "type1", "1").setSource(o1));
+
+        String filter = "{\"geo_shape\": {\"location2\": {\"indexed_shape\": {"
+                + "\"id\": \"1\","
+                + "\"type\": \"type1\","
+                + "\"index\": \"test\","
+                + "\"path\": \"location2\""
+                + "}}}}";
+
+        SearchResponse result = client().prepareSearch("test").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(filter).execute().actionGet();
+        assertSearchResponse(result);
+        assertHitCount(result, 1);
+    }
 
     @Test
     public void testShapeFetchingPath() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterSearchIT.java b/core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterSearchIT.java
index 5f5ecfc..7c1f163 100644
--- a/core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterSearchIT.java
@@ -60,7 +60,7 @@ public class CustomHighlighterSearchIT extends ESIntegTestCase {
     public void testThatCustomHighlightersAreSupported() throws IOException {
         SearchResponse searchResponse = client().prepareSearch("test").setTypes("test")
                 .setQuery(QueryBuilders.matchAllQuery())
-                .highlighter(new HighlightBuilder().field("name").highlighterType("test-custom"))
+                .addHighlightedField("name").setHighlighterType("test-custom")
                 .execute().actionGet();
         assertHighlight(searchResponse, 0, "name", 0, equalTo("standard response for name at position 1"));
     }
@@ -75,7 +75,7 @@ public class CustomHighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse searchResponse = client().prepareSearch("test").setTypes("test")
                 .setQuery(QueryBuilders.matchAllQuery())
-                .highlighter(new HighlightBuilder().field(highlightConfig))
+                .addHighlightedField(highlightConfig)
                 .execute().actionGet();
 
         assertHighlight(searchResponse, 0, "name", 0, equalTo("standard response for name at position 1"));
@@ -87,8 +87,11 @@ public class CustomHighlighterSearchIT extends ESIntegTestCase {
         Map<String, Object> options = new HashMap<>();
         options.put("myGlobalOption", "someValue");
 
-        SearchResponse searchResponse = client().prepareSearch("test").setTypes("test").setQuery(QueryBuilders.matchAllQuery())
-                .highlighter(new HighlightBuilder().field("name").highlighterType("test-custom").options(options))
+        SearchResponse searchResponse = client().prepareSearch("test").setTypes("test")
+                .setQuery(QueryBuilders.matchAllQuery())
+                .setHighlighterOptions(options)
+                .setHighlighterType("test-custom")
+                .addHighlightedField("name")
                 .execute().actionGet();
 
         assertHighlight(searchResponse, 0, "name", 0, equalTo("standard response for name at position 1"));
@@ -100,9 +103,11 @@ public class CustomHighlighterSearchIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("test").setTypes("test")
                 .setQuery(QueryBuilders.boolQuery().must(QueryBuilders.matchAllQuery()).should(QueryBuilders
                         .termQuery("name", "arbitrary")))
-                .highlighter(
-                        new HighlightBuilder().highlighterType("test-custom").field("name").field("other_name").field("other_other_name")
-                                .useExplicitFieldOrder(true))
+                .setHighlighterType("test-custom")
+                .addHighlightedField("name")
+                .addHighlightedField("other_name")
+                .addHighlightedField("other_other_name")
+                .setHighlighterExplicitFieldOrder(true)
                 .get();
 
         assertHighlight(searchResponse, 0, "name", 0, equalTo("standard response for name at position 1"));
diff --git a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
index 93fd7eb..4134c4f 100644
--- a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
@@ -19,22 +19,20 @@
 package org.elasticsearch.search.highlight;
 
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.settings.Settings.Builder;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.AbstractQueryBuilder;
+import org.elasticsearch.index.query.*;
 import org.elasticsearch.index.query.IdsQueryBuilder;
 import org.elasticsearch.index.query.MatchQueryBuilder;
+import org.elasticsearch.index.search.MatchQuery.Type;
+import org.elasticsearch.index.search.MatchQuery;
 import org.elasticsearch.index.query.MultiMatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.search.MatchQuery;
-import org.elasticsearch.index.search.MatchQuery.Type;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
@@ -51,38 +49,12 @@ import java.util.Map;
 import static org.elasticsearch.client.Requests.searchRequest;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.boostingQuery;
-import static org.elasticsearch.index.query.QueryBuilders.commonTermsQuery;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.fuzzyQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchPhrasePrefixQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchPhraseQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.missingQuery;
-import static org.elasticsearch.index.query.QueryBuilders.multiMatchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.prefixQuery;
-import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
-import static org.elasticsearch.index.query.QueryBuilders.rangeQuery;
-import static org.elasticsearch.index.query.QueryBuilders.regexpQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.index.query.QueryBuilders.typeQuery;
-import static org.elasticsearch.index.query.QueryBuilders.wildcardQuery;
+import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.highlight;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHighlight;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNotHighlighted;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
 import static org.elasticsearch.test.hamcrest.RegexMatcher.matches;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.hasKey;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.startsWith;
+import static org.hamcrest.Matchers.*;
 
 public class HighlighterSearchIT extends ESIntegTestCase {
 
@@ -110,8 +82,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .get();
         refresh();
         String highlighter = randomFrom(new String[]{"plain", "postings", "fvh"});
-        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text")))
-                .highlighter(new HighlightBuilder().field(new Field("*").highlighterType(highlighter))).get();
+        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text"))).addHighlightedField(new Field("*").highlighterType(highlighter)).get();
         assertHighlight(search, 0, "text", 0, equalTo("<em>text</em>"));
     }
 
@@ -150,17 +121,14 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .setSource(jsonBuilder().startObject().field("long_text", builder.toString()).field("text", "text").endObject())
                 .get();
         refresh();
-        String highlighter = randomFrom(new String[] { "plain", "postings", "fvh" });
-        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text")))
-                .highlighter(new HighlightBuilder().field(new Field("*").highlighterType(highlighter))).get();
+        String highlighter = randomFrom(new String[]{"plain", "postings", "fvh"});
+        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text"))).addHighlightedField(new Field("*").highlighterType(highlighter)).get();
         assertHighlight(search, 0, "text", 0, equalTo("<em>text</em>"));
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text")))
-                .highlighter(new HighlightBuilder().field(new Field("long_text").highlighterType(highlighter))).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text"))).addHighlightedField(new Field("long_text").highlighterType(highlighter)).get();
         assertNoFailures(search);
         assertThat(search.getHits().getAt(0).getHighlightFields().size(), equalTo(0));
 
-        search = client().prepareSearch().setQuery(prefixQuery("text", "te"))
-                .highlighter(new HighlightBuilder().field(new Field("long_text").highlighterType(highlighter))).get();
+        search = client().prepareSearch().setQuery(prefixQuery("text", "te")).addHighlightedField(new Field("long_text").highlighterType(highlighter)).get();
         assertNoFailures(search);
         assertThat(search.getHits().getAt(0).getHighlightFields().size(), equalTo(0));
     }
@@ -196,12 +164,10 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .setSource(jsonBuilder().startObject().field("unstored_text", "text").field("text", "text").endObject())
                 .get();
         refresh();
-        String highlighter = randomFrom(new String[] { "plain", "postings", "fvh" });
-        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text")))
-                .highlighter(new HighlightBuilder().field(new Field("*").highlighterType(highlighter))).get();
+        String highlighter = randomFrom(new String[]{"plain", "postings", "fvh"});
+        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text"))).addHighlightedField(new Field("*").highlighterType(highlighter)).get();
         assertHighlight(search, 0, "text", 0, equalTo("<em>text</em>"));
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text")))
-                .highlighter(new HighlightBuilder().field(new Field("unstored_text"))).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text"))).addHighlightedField(new Field("unstored_text")).get();
         assertNoFailures(search);
         assertThat(search.getHits().getAt(0).getHighlightFields().size(), equalTo(0));
     }
@@ -221,8 +187,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
             .setSource("name", builder.toString())
             .get();
         refresh();
-        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "abc")))
-                .highlighter(new HighlightBuilder().field("name")).get();
+        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "abc"))).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, startsWith("<em>abc</em> <em>abc</em> <em>abc</em> <em>abc</em>"));
     }
 
@@ -278,9 +243,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         client().prepareIndex("test", "test", "1")
             .setSource("name", "ARCOTEL Hotels Deutschland").get();
         refresh();
-        SearchResponse search = client().prepareSearch("test").setTypes("test")
-                .setQuery(matchQuery("name.autocomplete", "deut tel").operator(Operator.OR))
-                .highlighter(new HighlightBuilder().field("name.autocomplete")).execute().actionGet();
+        SearchResponse search = client().prepareSearch("test").setTypes("test").setQuery(matchQuery("name.autocomplete", "deut tel").operator(Operator.OR)).addHighlightedField("name.autocomplete").execute().actionGet();
         assertHighlight(search, 0, "name.autocomplete", 0, equalTo("ARCO<em>TEL</em> Ho<em>tel</em>s <em>Deut</em>schland"));
     }
 
@@ -310,22 +273,10 @@ public class HighlighterSearchIT extends ESIntegTestCase {
             .setSource("body", "Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature")
             .get();
         refresh();
-        SearchResponse search = client().prepareSearch().setQuery(matchQuery("body", "Test: http://www.facebook.com ").type(Type.PHRASE))
-                .highlighter(new HighlightBuilder().field("body")).execute().actionGet();
+        SearchResponse search = client().prepareSearch().setQuery(matchQuery("body", "Test: http://www.facebook.com ").type(Type.PHRASE)).addHighlightedField("body").execute().actionGet();
         assertHighlight(search, 0, "body", 0, startsWith("<em>Test: http://www.facebook.com</em>"));
-        search = client()
-                .prepareSearch()
-                .setQuery(
-                        matchQuery(
-                                "body",
-                                "Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature")
-                                .type(Type.PHRASE)).highlighter(new HighlightBuilder().field("body")).execute().actionGet();
-        assertHighlight(
-                search,
-                0,
-                "body",
-                0,
-                equalTo("<em>Test</em>: <em>http://www.facebook.com</em> <em>http://elasticsearch.org</em> <em>http://xing.com</em> <em>http://cnn.com</em> http://quora.com"));
+        search = client().prepareSearch().setQuery(matchQuery("body", "Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature").type(Type.PHRASE)).addHighlightedField("body").execute().actionGet();
+        assertHighlight(search, 0, "body", 0, equalTo("<em>Test</em>: <em>http://www.facebook.com</em> <em>http://elasticsearch.org</em> <em>http://xing.com</em> <em>http://cnn.com</em> http://quora.com"));
     }
 
     @Test
@@ -359,43 +310,37 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                     "name2", "avinci, unilog avinci, logicacmg, logica").get();
         refresh();
 
-        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica m")))
-                .highlighter(new HighlightBuilder().field("name")).get();
+        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica m"))).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, anyOf(equalTo("<em>logica</em>c<em>m</em>g ehe<em>m</em>als avinci - the know how co<em>m</em>pany"),
                 equalTo("avinci, unilog avinci, <em>logica</em>c<em>m</em>g, <em>logica</em>")));
         assertHighlight(search, 1, "name", 0, anyOf(equalTo("<em>logica</em>c<em>m</em>g ehe<em>m</em>als avinci - the know how co<em>m</em>pany"),
                 equalTo("avinci, unilog avinci, <em>logica</em>c<em>m</em>g, <em>logica</em>")));
 
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica ma")))
-                .highlighter(new HighlightBuilder().field("name")).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica ma"))).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, anyOf(equalTo("<em>logica</em>cmg ehe<em>ma</em>ls avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
         assertHighlight(search, 1, "name", 0, anyOf(equalTo("<em>logica</em>cmg ehe<em>ma</em>ls avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
 
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica")))
-                .highlighter(new HighlightBuilder().field("name")).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica"))).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, anyOf(equalTo("<em>logica</em>cmg ehemals avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
         assertHighlight(search, 0, "name", 0, anyOf(equalTo("<em>logica</em>cmg ehemals avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
 
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica m")))
-                .highlighter(new HighlightBuilder().field("name2")).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica m"))).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, anyOf(equalTo("<em>logica</em>c<em>m</em>g ehe<em>m</em>als avinci - the know how co<em>m</em>pany"),
                 equalTo("avinci, unilog avinci, <em>logica</em>c<em>m</em>g, <em>logica</em>")));
         assertHighlight(search, 1, "name2", 0, anyOf(equalTo("<em>logica</em>c<em>m</em>g ehe<em>m</em>als avinci - the know how co<em>m</em>pany"),
                 equalTo("avinci, unilog avinci, <em>logica</em>c<em>m</em>g, <em>logica</em>")));
 
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica ma")))
-                .highlighter(new HighlightBuilder().field("name2")).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica ma"))).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, anyOf(equalTo("<em>logica</em>cmg ehe<em>ma</em>ls avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
         assertHighlight(search, 1, "name2", 0, anyOf(equalTo("<em>logica</em>cmg ehe<em>ma</em>ls avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
 
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica")))
-                .highlighter(new HighlightBuilder().field("name2")).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica"))).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, anyOf(equalTo("<em>logica</em>cmg ehemals avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
         assertHighlight(search, 1, "name2", 0, anyOf(equalTo("<em>logica</em>cmg ehemals avinci - the know how company"),
@@ -426,25 +371,22 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                        "name2", "logicacmg ehemals avinci - the know how company").get();
         refresh();
         ensureGreen();
-        SearchResponse search = client().prepareSearch().setQuery(matchQuery("name", "logica m"))
-                .highlighter(new HighlightBuilder().field("name")).get();
+        SearchResponse search = client().prepareSearch().setQuery(matchQuery("name", "logica m")).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, equalTo("<em>logica</em>c<em>m</em>g ehe<em>m</em>als avinci - the know how co<em>m</em>pany"));
 
-        search = client().prepareSearch().setQuery(matchQuery("name", "logica ma")).highlighter(new HighlightBuilder().field("name")).get();
+        search = client().prepareSearch().setQuery(matchQuery("name", "logica ma")).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, equalTo("<em>logica</em>cmg ehe<em>ma</em>ls avinci - the know how company"));
 
-        search = client().prepareSearch().setQuery(matchQuery("name", "logica")).highlighter(new HighlightBuilder().field("name")).get();
+        search = client().prepareSearch().setQuery(matchQuery("name", "logica")).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, equalTo("<em>logica</em>cmg ehemals avinci - the know how company"));
 
-        search = client().prepareSearch().setQuery(matchQuery("name2", "logica m")).highlighter(new HighlightBuilder().field("name2"))
-                .get();
+        search = client().prepareSearch().setQuery(matchQuery("name2", "logica m")).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, equalTo("<em>logicacmg</em> <em>ehemals</em> avinci - the know how <em>company</em>"));
 
-        search = client().prepareSearch().setQuery(matchQuery("name2", "logica ma")).highlighter(new HighlightBuilder().field("name2"))
-                .get();
+        search = client().prepareSearch().setQuery(matchQuery("name2", "logica ma")).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, equalTo("<em>logicacmg</em> <em>ehemals</em> avinci - the know how company"));
 
-        search = client().prepareSearch().setQuery(matchQuery("name2", "logica")).highlighter(new HighlightBuilder().field("name2")).get();
+        search = client().prepareSearch().setQuery(matchQuery("name2", "logica")).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, equalTo("<em>logicacmg</em> ehemals avinci - the know how company"));
     }
 
@@ -464,19 +406,19 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("long_term", "thisisaverylongwordandmakessurethisfails foo highlighed"))
-                .highlighter(new HighlightBuilder().field("long_term", 18, 1))
+                .addHighlightedField("long_term", 18, 1)
                 .get();
         assertHighlight(search, 0, "long_term", 0, 1, equalTo("<em>thisisaverylongwordandmakessurethisfails</em>"));
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("no_long_term", "test foo highlighed").type(Type.PHRASE).slop(3))
-                .highlighter(new HighlightBuilder().field("no_long_term", 18, 1).postTags("</b>").preTags("<b>"))
+                .addHighlightedField("no_long_term", 18, 1).setHighlighterPostTags("</b>").setHighlighterPreTags("<b>")
                 .get();
         assertNotHighlighted(search, 0, "no_long_term");
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("no_long_term", "test foo highlighed").type(Type.PHRASE).slop(3))
-                .highlighter(new HighlightBuilder().field("no_long_term", 30, 1).postTags("</b>").preTags("<b>"))
+                .addHighlightedField("no_long_term", 30, 1).setHighlighterPostTags("</b>").setHighlighterPreTags("<b>")
                 .get();
 
         assertHighlight(search, 0, "no_long_term", 0, 1, equalTo("a <b>test</b> where <b>foo</b> is <b>highlighed</b> and"));
@@ -504,7 +446,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
-                .highlighter(new HighlightBuilder().field("title", -1, 0))
+                .addHighlightedField("title", -1, 0)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -513,7 +455,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("attachments.body", "attachment"))
-                .highlighter(new HighlightBuilder().field("attachments.body", -1, 0))
+                .addHighlightedField("attachments.body", -1, 0)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -544,7 +486,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
-                .highlighter(new HighlightBuilder().field("title", -1, 0))
+                .addHighlightedField("title", -1, 0)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -553,7 +495,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("attachments.body", "attachment"))
-                .highlighter(new HighlightBuilder().field("attachments.body", -1, 2))
+                .addHighlightedField("attachments.body", -1, 2)
                 .execute().get();
 
         for (int i = 0; i < 5; i++) {
@@ -586,7 +528,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
                 //asking for the whole field to be highlighted
-                .highlighter(new HighlightBuilder().field("title", -1, 0)).get();
+                .addHighlightedField("title", -1, 0).get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
             assertHighlight(search, i, "title", 0, equalTo("This is a test on the highlighting <em>bug</em> present in elasticsearch. Hopefully it works."));
@@ -596,7 +538,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
                 //sentences will be generated out of each value
-                .highlighter(new HighlightBuilder().field("title")).get();
+                .addHighlightedField("title").get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
             assertHighlight(search, i, "title", 0, equalTo("This is a test on the highlighting <em>bug</em> present in elasticsearch."));
@@ -605,7 +547,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("attachments.body", "attachment"))
-                .highlighter(new HighlightBuilder().field("attachments.body", -1, 2))
+                .addHighlightedField("attachments.body", -1, 2)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -629,7 +571,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
-                .highlighter(new HighlightBuilder().field("title", -1, 2).field("titleTV", -1, 2).requireFieldMatch(false))
+                .addHighlightedField("title", -1, 2)
+                .addHighlightedField("titleTV", -1, 2).setHighlighterRequireFieldMatch(false)
                 .get();
 
         assertHighlight(search, 0, "title", 0, equalTo("This is a test on the highlighting <em>bug</em> present in elasticsearch"));
@@ -639,7 +582,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("titleTV", "highlight"))
-                .highlighter(new HighlightBuilder().field("titleTV", -1, 2))
+                .addHighlightedField("titleTV", -1, 2)
                 .get();
 
         assertHighlight(search, 0, "titleTV", 0, equalTo("some text to <em>highlight</em>"));
@@ -659,7 +602,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1 and field2 produces different tags");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "test"))
-                .highlighter(highlight().order("score").preTags("<global>").postTags("</global>").fragmentSize(1).numOfFragments(1)
+                .highlight(highlight().order("score").preTags("<global>").postTags("</global>").fragmentSize(1).numOfFragments(1)
                         .field(new HighlightBuilder.Field("field1").numOfFragments(2))
                         .field(new HighlightBuilder.Field("field2").preTags("<field2>").postTags("</field2>").fragmentSize(50).requireFieldMatch(false)));
 
@@ -689,7 +632,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         SearchSourceBuilder source = searchSource()
                 //postings hl doesn't support require_field_match, its field needs to be queried directly
                 .query(termQuery("field-postings", "test"))
-                .highlighter(highlight().field("field*").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field*").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -718,42 +661,36 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         //works using stored field
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(termQuery("field1", "quick"))
-                .highlighter(new HighlightBuilder().field(new Field("field1").preTags("<xxx>").postTags("</xxx>")))
+                .addHighlightedField(new Field("field1").preTags("<xxx>").postTags("</xxx>"))
                 .get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <xxx>quick</xxx> brown fox jumps over the lazy dog"));
 
         assertFailures(client().prepareSearch("test")
                         .setQuery(termQuery("field1", "quick"))
-                        .highlighter(
-                                new HighlightBuilder().field(new Field("field1").preTags("<xxx>").postTags("</xxx>")
-                                        .highlighterType("plain").forceSource(true))),
+                        .addHighlightedField(new Field("field1").preTags("<xxx>").postTags("</xxx>").highlighterType("plain").forceSource(true)),
                 RestStatus.BAD_REQUEST,
                 containsString("source is forced for fields [field1] but type [type1] has disabled _source"));
 
         assertFailures(client().prepareSearch("test")
                         .setQuery(termQuery("field1", "quick"))
-                        .highlighter(
-                                new HighlightBuilder().field(new Field("field1").preTags("<xxx>").postTags("</xxx>").highlighterType("fvh")
-                                        .forceSource(true))),
+                        .addHighlightedField(new Field("field1").preTags("<xxx>").postTags("</xxx>").highlighterType("fvh").forceSource(true)),
                 RestStatus.BAD_REQUEST,
                 containsString("source is forced for fields [field1] but type [type1] has disabled _source"));
 
         assertFailures(client().prepareSearch("test")
                 .setQuery(termQuery("field1", "quick"))
-                        .highlighter(
-                                new HighlightBuilder().field(new Field("field1").preTags("<xxx>").postTags("</xxx>")
-                                        .highlighterType("postings").forceSource(true))),
+                .addHighlightedField(new Field("field1").preTags("<xxx>").postTags("</xxx>").highlighterType("postings").forceSource(true)),
                 RestStatus.BAD_REQUEST,
                 containsString("source is forced for fields [field1] but type [type1] has disabled _source"));
 
         SearchSourceBuilder searchSource = SearchSourceBuilder.searchSource().query(termQuery("field1", "quick"))
-                .highlighter(highlight().forceSource(true).field("field1"));
+                .highlight(highlight().forceSource(true).field("field1"));
         assertFailures(client().prepareSearch("test").setSource(searchSource),
                 RestStatus.BAD_REQUEST,
                 containsString("source is forced for fields [field1] but type [type1] has disabled _source"));
 
         searchSource = SearchSourceBuilder.searchSource().query(termQuery("field1", "quick"))
-                .highlighter(highlight().forceSource(true).field("field*"));
+                .highlight(highlight().forceSource(true).field("field*"));
         assertFailures(client().prepareSearch("test").setSource(searchSource),
                 RestStatus.BAD_REQUEST,
                 matches("source is forced for fields \\[field\\d, field\\d\\] but type \\[type1\\] has disabled _source"));
@@ -771,7 +708,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "test"))
-                .highlighter(highlight().field("field1").order("score").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field1").order("score").preTags("<xxx>").postTags("</xxx>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -780,7 +717,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field1");
         source = searchSource()
                 .query(termQuery("_all", "test"))
-                .highlighter(highlight().field("field1").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field1").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -789,7 +726,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field2");
         source = searchSource()
                 .query(termQuery("_all", "quick"))
-                .highlighter(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -798,7 +735,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field2");
         source = searchSource()
                 .query(prefixQuery("_all", "qui"))
-                .highlighter(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -807,7 +744,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all with constant score, highlighting on field2");
         source = searchSource()
                 .query(constantScoreQuery(prefixQuery("_all", "qui")))
-                .highlighter(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -816,7 +753,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all with constant score, highlighting on field2");
         source = searchSource()
                 .query(boolQuery().should(constantScoreQuery(prefixQuery("_all", "qui"))))
-                .highlighter(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <xxx>quick</xxx> brown fox jumps over the lazy dog"));
@@ -834,7 +771,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "test"))
-                .highlighter(highlight().field("field1", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field1", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -843,7 +780,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field1");
         source = searchSource()
                 .query(termQuery("_all", "test"))
-                .highlighter(highlight().field("field1", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field1", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -853,7 +790,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field2");
         source = searchSource()
                 .query(termQuery("_all", "quick"))
-                .highlighter(highlight().field("field2", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -863,7 +800,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field2");
         source = searchSource()
                 .query(prefixQuery("_all", "qui"))
-                .highlighter(highlight().field("field2", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -889,7 +826,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "t"))
-                .highlighter(highlight().highlighterType("fvh").field("field1", 20, 1).order("score").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().highlighterType("fvh").field("field1", 20, 1).order("score").preTags("<xxx>").postTags("</xxx>"));
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
         assertHighlight(searchResponse, 0, "field1", 0, 1, containsString("<xxx>t</xxx>"));
         logger.info("--> done");
@@ -957,7 +894,9 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         Field fooField = new Field("foo").numOfFragments(1).order("score").fragmentSize(25)
                 .highlighterType("fvh").requireFieldMatch(requireFieldMatch);
-        SearchRequestBuilder req = client().prepareSearch("test").highlighter(new HighlightBuilder().field(fooField));
+        Field barField = new Field("bar").numOfFragments(1).order("score").fragmentSize(25)
+                .highlighterType("fvh").requireFieldMatch(requireFieldMatch);
+        SearchRequestBuilder req = client().prepareSearch("test").addHighlightedField(fooField);
 
         // First check highlighting without any matched fields set
         SearchResponse resp = req.setQuery(queryStringQuery("running scissors").field("foo")).get();
@@ -969,31 +908,21 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         // Add the subfield to the list of matched fields but don't match it.  Everything should still work
         // like before we added it.
-        fooField = new Field("foo").numOfFragments(1).order("score").fragmentSize(25).highlighterType("fvh")
-                .requireFieldMatch(requireFieldMatch);
         fooField.matchedFields("foo", "foo.plain");
-        req = client().prepareSearch("test").highlighter(new HighlightBuilder().field(fooField));
         resp = req.setQuery(queryStringQuery("running scissors").field("foo")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with <em>scissors</em>"));
 
-
         // Now make half the matches come from the stored field and half from just a matched field.
         resp = req.setQuery(queryStringQuery("foo.plain:running scissors").field("foo")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with <em>scissors</em>"));
 
         // Now remove the stored field from the matched field list.  That should work too.
-        fooField = new Field("foo").numOfFragments(1).order("score").fragmentSize(25).highlighterType("fvh")
-                .requireFieldMatch(requireFieldMatch);
         fooField.matchedFields("foo.plain");
-        req = client().prepareSearch("test").highlighter(new HighlightBuilder().field(fooField));
         resp = req.setQuery(queryStringQuery("foo.plain:running scissors").field("foo")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with scissors"));
 
         // Now make sure boosted fields don't blow up when matched fields is both the subfield and stored field.
-        fooField = new Field("foo").numOfFragments(1).order("score").fragmentSize(25).highlighterType("fvh")
-                .requireFieldMatch(requireFieldMatch);
         fooField.matchedFields("foo", "foo.plain");
-        req = client().prepareSearch("test").highlighter(new HighlightBuilder().field(fooField));
         resp = req.setQuery(queryStringQuery("foo.plain:running^5 scissors").field("foo")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with <em>scissors</em>"));
 
@@ -1020,46 +949,41 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // Speaking of two fields, you can have two fields, only one of which has matchedFields enabled
         QueryBuilder twoFieldsQuery = queryStringQuery("cats").field("foo").field("foo.plain", 5)
                 .field("bar").field("bar.plain", 5);
-        Field barField = new Field("bar").numOfFragments(1).order("score").fragmentSize(25).highlighterType("fvh")
-                .requireFieldMatch(requireFieldMatch);
-        resp = req.setQuery(twoFieldsQuery).highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.setQuery(twoFieldsQuery).addHighlightedField(barField).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("junk junk <em>cats</em> junk junk"));
         assertHighlight(resp, 0, "bar", 0, equalTo("<em>cat</em> <em>cat</em> junk junk junk junk"));
+
         // And you can enable matchedField highlighting on both
         barField.matchedFields("bar", "bar.plain");
-        resp = req.setQuery(twoFieldsQuery).highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.get();
         assertHighlight(resp, 0, "foo", 0, equalTo("junk junk <em>cats</em> junk junk"));
         assertHighlight(resp, 0, "bar", 0, equalTo("junk junk <em>cats</em> junk junk"));
 
         // Setting a matchedField that isn't searched/doesn't exist is simply ignored.
         barField.matchedFields("bar", "candy");
-        resp = req.setQuery(twoFieldsQuery).highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.get();
         assertHighlight(resp, 0, "foo", 0, equalTo("junk junk <em>cats</em> junk junk"));
         assertHighlight(resp, 0, "bar", 0, equalTo("<em>cat</em> <em>cat</em> junk junk junk junk"));
 
         // If the stored field doesn't have a value it doesn't matter what you match, you get nothing.
         barField.matchedFields("bar", "foo.plain");
-        resp = req.setQuery(queryStringQuery("running scissors").field("foo.plain").field("bar"))
-                .highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.setQuery(queryStringQuery("running scissors").field("foo.plain").field("bar")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with <em>scissors</em>"));
         assertThat(resp.getHits().getAt(0).getHighlightFields(), not(hasKey("bar")));
 
         // If the stored field is found but the matched field isn't then you don't get a result either.
         fooField.matchedFields("bar.plain");
-        resp = req.setQuery(queryStringQuery("running scissors").field("foo").field("foo.plain").field("bar").field("bar.plain"))
-                .highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.setQuery(queryStringQuery("running scissors").field("foo").field("foo.plain").field("bar").field("bar.plain")).get();
         assertThat(resp.getHits().getAt(0).getHighlightFields(), not(hasKey("foo")));
 
         // But if you add the stored field to the list of matched fields then you'll get a result again
         fooField.matchedFields("foo", "bar.plain");
-        resp = req.setQuery(queryStringQuery("running scissors").field("foo").field("foo.plain").field("bar").field("bar.plain"))
-                .highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.setQuery(queryStringQuery("running scissors").field("foo").field("foo.plain").field("bar").field("bar.plain")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with <em>scissors</em>"));
         assertThat(resp.getHits().getAt(0).getHighlightFields(), not(hasKey("bar")));
 
         // You _can_ highlight fields that aren't subfields of one another.
-        resp = req.setQuery(queryStringQuery("weird").field("foo").field("foo.plain").field("bar").field("bar.plain"))
-                .highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.setQuery(queryStringQuery("weird").field("foo").field("foo.plain").field("bar").field("bar.plain")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>weird</em>"));
         assertHighlight(resp, 0, "bar", 0, equalTo("<em>resul</em>t"));
 
@@ -1084,7 +1008,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch()
                 .setSize(COUNT)
                 .setQuery(termQuery("field1", "test"))
-                .highlighter(new HighlightBuilder().field("field1", 100, 0))
+                .addHighlightedField("field1", 100, 0)
                 .get();
         for (int i = 0; i < COUNT; i++) {
             SearchHit hit = searchResponse.getHits().getHits()[i];
@@ -1096,7 +1020,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         searchResponse = client().prepareSearch()
                 .setSize(COUNT)
                 .setQuery(termQuery("_all", "test"))
-                .highlighter(new HighlightBuilder().field("_all", 100, 0))
+                .addHighlightedField("_all", 100, 0)
                 .get();
         for (int i = 0; i < COUNT; i++) {
             SearchHit hit = searchResponse.getHits().getHits()[i];
@@ -1129,7 +1053,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
-                .highlighter(new HighlightBuilder().field("title", -1, 0))
+                .addHighlightedField("title", -1, 0)
                 .get();
 
         for (int i = 0; i < 5; i++) {
@@ -1152,7 +1076,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
-                .highlighter(new HighlightBuilder().field("title", 30, 1, 10))
+                .addHighlightedField("title", 30, 1, 10)
                 .get();
 
         for (int i = 0; i < 5; i++) {
@@ -1176,7 +1100,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 50, 1, 10))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 50, 1, 10)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -1199,7 +1124,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 30, 1, 10))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 30, 1, 10)
                 .get();
 
         for (int i = 0; i < 5; i++) {
@@ -1222,7 +1148,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // simple search on body with standard analyzer with a simple field query
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title", 0, 1, equalTo("this is a <em>test</em>"));
@@ -1230,7 +1157,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title
         search = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title.key", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title.key", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title.key", 0, 1, equalTo("<em>this</em> <em>is</em> <em>a</em> <em>test</em>"));
@@ -1253,7 +1181,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // simple search on body with standard analyzer with a simple field query
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title", 0, 1, equalTo("this is a <em>test</em>"));
@@ -1261,7 +1190,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title.key
         search = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title.key", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title.key", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title.key", 0, 1, equalTo("<em>this</em> <em>is</em> <em>a</em> <em>test</em>"));
@@ -1284,7 +1214,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // simple search on body with standard analyzer with a simple field query
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title", 0, 1, equalTo("this is a <em>test</em>"));
@@ -1292,7 +1223,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title
         search = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title.key", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title.key", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title.key", 0, 1, equalTo("<em>this</em> <em>is</em> <em>a</em> <em>test</em>"));
@@ -1314,7 +1246,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // simple search on body with standard analyzer with a simple field query
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title", 0, 1, equalTo("this is a <em>test</em>"));
@@ -1322,7 +1255,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title.key
         search = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title.key", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title.key", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title.key", 0, 1, equalTo("<em>this</em> <em>is</em> <em>a</em> <em>test</em>"));
@@ -1343,20 +1277,22 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().field("title", 50, 1, 10))
+                .addHighlightedField("title", 50, 1, 10)
                 .get();
         assertNoFailures(search);
 
         assertFailures(client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "this is a test"))
-                        .highlighter(new HighlightBuilder().field("title", 50, 1, 10).highlighterType("fast-vector-highlighter")),
+                .addHighlightedField("title", 50, 1, 10)
+                .setHighlighterType("fast-vector-highlighter"),
                 RestStatus.BAD_REQUEST,
                 containsString("the field [title] should be indexed with term vector with position offsets to be used with fast vector highlighter"));
 
         //should not fail if there is a wildcard
         assertNoFailures(client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().field("tit*", 50, 1, 10).highlighterType("fast-vector-highlighter")).get());
+                .addHighlightedField("tit*", 50, 1, 10)
+                .setHighlighterType("fast-vector-highlighter").get());
     }
 
     @Test
@@ -1374,7 +1310,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "test for the workaround"))
-                .highlighter(new HighlightBuilder().field("title", 50, 1, 10))
+                .addHighlightedField("title", 50, 1, 10)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -1385,7 +1321,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // Using plain highlighter instead of FVH
         search = client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "test for the workaround"))
-                .highlighter(new HighlightBuilder().field("title", 50, 1, 10).highlighterType("highlighter"))
+                .addHighlightedField("title", 50, 1, 10)
+                .setHighlighterType("highlighter")
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -1395,9 +1332,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // Using plain highlighter instead of FVH on the field level
         search = client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "test for the workaround"))
-                .highlighter(
-                        new HighlightBuilder().field(new HighlightBuilder.Field("title").highlighterType("highlighter")).highlighterType(
-                                "highlighter"))
+                .addHighlightedField(new HighlightBuilder.Field("title").highlighterType("highlighter"))
+                .setHighlighterType("highlighter")
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -1418,7 +1354,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("tags", "tag"))
-                .highlighter(new HighlightBuilder().field("tags", -1, 0)).get();
+                .addHighlightedField("tags", -1, 0).get();
 
         assertHighlight(response, 0, "tags", 0, equalTo("this is a really long <em>tag</em> i would like to highlight"));
         assertHighlight(response, 0, "tags", 1, 2, equalTo("here is another one that is very long and has the <em>tag</em> token near the end"));
@@ -1435,7 +1371,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
-                .highlighter(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -1454,7 +1390,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
-                .highlighter(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -1474,7 +1410,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(commonTermsQuery("field2", "quick brown").cutoffFrequency(100))
-                .highlighter(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <x>quick</x> <x>brown</x> fox jumps over the lazy dog"));
@@ -1489,7 +1425,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(commonTermsQuery("field2", "quick brown").cutoffFrequency(100))
-                .highlighter(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -1519,7 +1455,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field0");
         SearchSourceBuilder source = searchSource()
                 .query(matchPhrasePrefixQuery("field0", "quick bro"))
-                .highlighter(highlight().field("field0").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field0").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -1528,7 +1464,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         source = searchSource()
                 .query(matchPhrasePrefixQuery("field1", "quick bro"))
-                .highlighter(highlight().field("field1").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field1").order("score").preTags("<x>").postTags("</x>"));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -1545,7 +1481,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
 
         source = searchSource().postFilter(typeQuery("type2")).query(matchPhrasePrefixQuery("field3", "fast bro"))
-                .highlighter(highlight().field("field3").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field3").order("score").preTags("<x>").postTags("</x>"));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -1553,7 +1489,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field4");
         source = searchSource().postFilter(typeQuery("type2")).query(matchPhrasePrefixQuery("field4", "the fast bro"))
-                .highlighter(highlight().field("field4").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field4").order("score").preTags("<x>").postTags("</x>"));
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
         assertHighlight(searchResponse, 0, "field4", 0, 1, anyOf(equalTo("<x>The quick browse</x> button is a fancy thing, right bro?"), equalTo("<x>The quick brown</x> fox jumps over the lazy dog")));
@@ -1561,7 +1497,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field4");
         source = searchSource().postFilter(typeQuery("type2")).query(matchPhrasePrefixQuery("field4", "a fast quick blue ca"))
-                .highlighter(highlight().field("field4").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field4").order("score").preTags("<x>").postTags("</x>"));
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
         assertHighlight(searchResponse, 0, "field4", 0, 1, equalTo("<x>a quick fast blue car</x>"));
@@ -1580,27 +1516,24 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("tags", "long tag").type(MatchQuery.Type.PHRASE))
-                .highlighter(
-                        new HighlightBuilder().field(new HighlightBuilder.Field("tags").fragmentSize(-1).numOfFragments(2)
-                                .fragmenter("simple"))).get();
+                .addHighlightedField(new HighlightBuilder.Field("tags")
+                        .fragmentSize(-1).numOfFragments(2).fragmenter("simple")).get();
 
         assertHighlight(response, 0, "tags", 0, equalTo("this is a really <em>long</em> <em>tag</em> i would like to highlight"));
         assertHighlight(response, 0, "tags", 1, 2, equalTo("here is another one that is very <em>long</em> <em>tag</em> and has the <em>tag</em> token near the end"));
 
         response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("tags", "long tag").type(MatchQuery.Type.PHRASE))
-                .highlighter(
-                        new HighlightBuilder().field(new HighlightBuilder.Field("tags").fragmentSize(-1).numOfFragments(2)
-                                .fragmenter("span"))).get();
+                .addHighlightedField(new HighlightBuilder.Field("tags")
+                        .fragmentSize(-1).numOfFragments(2).fragmenter("span")).get();
 
         assertHighlight(response, 0, "tags", 0, equalTo("this is a really <em>long</em> <em>tag</em> i would like to highlight"));
         assertHighlight(response, 0, "tags", 1, 2, equalTo("here is another one that is very <em>long</em> <em>tag</em> and has the <em>tag</em> token near the end"));
 
         assertFailures(client().prepareSearch("test")
                         .setQuery(QueryBuilders.matchQuery("tags", "long tag").type(MatchQuery.Type.PHRASE))
-                        .highlighter(
-                                new HighlightBuilder().field(new HighlightBuilder.Field("tags").fragmentSize(-1).numOfFragments(2)
-                                        .fragmenter("invalid"))),
+                        .addHighlightedField(new HighlightBuilder.Field("tags")
+                                .fragmentSize(-1).numOfFragments(2).fragmenter("invalid")),
                 RestStatus.BAD_REQUEST,
                 containsString("unknown fragmenter option [invalid] for the field [tags]"));
     }
@@ -1615,10 +1548,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("field1", "fox"))
-                .highlighter(
-                        new HighlightBuilder().field(
-                                new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>").requireFieldMatch(true)).field(
-                                new HighlightBuilder.Field("field2").preTags("<2>").postTags("</2>").requireFieldMatch(false)))
+                .addHighlightedField(new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>").requireFieldMatch(true))
+                .addHighlightedField(new HighlightBuilder.Field("field2").preTags("<2>").postTags("</2>").requireFieldMatch(false))
                 .get();
         assertHighlight(response, 0, "field1", 0, 1, equalTo("The <b>quick<b> brown <1>fox</1>"));
         assertHighlight(response, 0, "field2", 0, 1, equalTo("The <b>slow<b> brown <2>fox</2>"));
@@ -1635,10 +1566,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("field1", "fox"))
-                .highlighter(
-                        new HighlightBuilder().field(
-                                new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>").requireFieldMatch(true)).field(
-                                new HighlightBuilder.Field("field2").preTags("<2>").postTags("</2>").requireFieldMatch(false)))
+                .addHighlightedField(new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>").requireFieldMatch(true))
+                .addHighlightedField(new HighlightBuilder.Field("field2").preTags("<2>").postTags("</2>").requireFieldMatch(false))
                 .get();
         assertHighlight(response, 0, "field1", 0, 1, equalTo("The <b>quick<b> brown <1>fox</1>"));
         assertHighlight(response, 0, "field2", 0, 1, equalTo("The <b>slow<b> brown <2>fox</2>"));
@@ -1658,9 +1587,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // This query used to fail when the field to highlight was absent
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("field", "highlight").type(MatchQuery.Type.BOOLEAN))
-                .highlighter(
-                        new HighlightBuilder().field(new HighlightBuilder.Field("highlight_field").fragmentSize(-1).numOfFragments(1)
-                                .fragmenter("simple"))).get();
+                .addHighlightedField(new HighlightBuilder.Field("highlight_field")
+                        .fragmentSize(-1).numOfFragments(1).fragmenter("simple")).get();
         assertThat(response.getHits().hits()[0].highlightFields().isEmpty(), equalTo(true));
     }
 
@@ -1679,9 +1607,13 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("text", "test").type(MatchQuery.Type.BOOLEAN))
-                .highlighter(
-                        new HighlightBuilder().field("text").field("byte").field("short").field("int").field("long").field("float")
-                                .field("double"))
+                .addHighlightedField("text")
+                .addHighlightedField("byte")
+                .addHighlightedField("short")
+                .addHighlightedField("int")
+                .addHighlightedField("long")
+                .addHighlightedField("float")
+                .addHighlightedField("double")
                 .get();
         // Highlighting of numeric fields is not supported, but it should not raise errors
         // (this behavior is consistent with version 0.20)
@@ -1705,7 +1637,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("text", "test").type(MatchQuery.Type.BOOLEAN))
-                .highlighter(new HighlightBuilder().field("text")).execute().actionGet();
+                .addHighlightedField("text").execute().actionGet();
         // PatternAnalyzer will throw an exception if it is resetted twice
         assertHitCount(response, 1l);
     }
@@ -1721,9 +1653,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         HighlightBuilder.Field field = new HighlightBuilder.Field("text");
 
-        HighlightBuilder highlightBuilder = new HighlightBuilder().field(field);
         SearchRequestBuilder search = client().prepareSearch("test").setQuery(QueryBuilders.matchQuery("text", "testing"))
-                .highlighter(highlightBuilder);
+                .addHighlightedField(field);
         Matcher<String> searchQueryMatcher = equalTo("<em>Testing</em> the highlight query feature");
 
         field.highlighterType("plain");
@@ -1736,12 +1667,9 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         response = search.get();
         assertHighlight(response, 0, "text", 0, searchQueryMatcher);
 
-        field = new HighlightBuilder.Field("text");
 
         Matcher<String> hlQueryMatcher = equalTo("Testing the highlight <em>query</em> feature");
         field.highlightQuery(matchQuery("text", "query"));
-        highlightBuilder = new HighlightBuilder().field(field);
-        search = client().prepareSearch("test").setQuery(QueryBuilders.matchQuery("text", "testing")).highlighter(highlightBuilder);
 
         field.highlighterType("fvh");
         response = search.get();
@@ -1756,7 +1684,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         assertHighlight(response, 0, "text", 0, hlQueryMatcher);
 
         // Make sure the the highlightQuery is taken into account when it is set on the highlight context instead of the field
-        highlightBuilder.highlightQuery(matchQuery("text", "query"));
+        search.setHighlighterQuery(matchQuery("text", "query"));
         field.highlighterType("fvh").highlightQuery(null);
         response = search.get();
         assertHighlight(response, 0, "text", 0, hlQueryMatcher);
@@ -1792,97 +1720,97 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .fragmentSize(21)
                 .numOfFragments(1)
                 .highlighterType("plain");
-        SearchResponse response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        SearchResponse response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         // When noMatchSize is set to 0 you also shouldn't get any
         field.highlighterType("plain").noMatchSize(0);
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         // When noMatchSize is between 0 and the size of the string
         field.highlighterType("plain").noMatchSize(21);
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so"));
 
         // The FVH also works but the fragment is longer than the plain highlighter because of boundary_max_scan
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some"));
 
         // Postings hl also works but the fragment is the whole first sentence (size ignored)
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some of me should get cut off."));
 
         // We can also ask for a fragment longer than the input string and get the whole string
         field.highlighterType("plain").noMatchSize(text.length() * 2);
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo(text));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo(text));
 
         //no difference using postings hl as the noMatchSize is ignored (just needs to be greater than 0)
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some of me should get cut off."));
 
         // We can also ask for a fragment exactly the size of the input field and get the whole field
         field.highlighterType("plain").noMatchSize(text.length());
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo(text));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo(text));
 
         //no difference using postings hl as the noMatchSize is ignored (just needs to be greater than 0)
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some of me should get cut off."));
 
         // You can set noMatchSize globally in the highlighter as well
         field.highlighterType("plain").noMatchSize(null);
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field).noMatchSize(21)).get();
+        response = client().prepareSearch("test").setHighlighterNoMatchSize(21).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so"));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field).noMatchSize(21)).get();
+        response = client().prepareSearch("test").setHighlighterNoMatchSize(21).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some"));
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field).noMatchSize(21)).get();
+        response = client().prepareSearch("test").setHighlighterNoMatchSize(21).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some of me should get cut off."));
 
         // We don't break if noMatchSize is less than zero though
         field.highlighterType("plain").noMatchSize(randomIntBetween(Integer.MIN_VALUE, -1));
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
     }
 
@@ -1903,16 +1831,16 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .numOfFragments(1)
                 .highlighterType("plain")
                 .noMatchSize(21);
-        SearchResponse response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        SearchResponse response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so"));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some"));
 
         // Postings hl also works but the fragment is the whole first sentence (size ignored)
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some of me should get cut off."));
 
         // And noMatchSize returns nothing when the first entry is empty string!
@@ -1923,19 +1851,19 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         field.highlighterType("plain");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         // But if the field was actually empty then you should get no highlighting field
@@ -1945,19 +1873,19 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         field.highlighterType("plain");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         // Same for if the field doesn't even exist on the document
@@ -1968,34 +1896,34 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         field.highlighterType("plain");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "postings");
 
         // Again same if the field isn't mapped
         field = new HighlightBuilder.Field("unmapped")
                 .highlighterType("plain")
                 .noMatchSize(21);
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
     }
 
@@ -2017,32 +1945,32 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .numOfFragments(0)
                 .highlighterType("plain")
                 .noMatchSize(20);
-        SearchResponse response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        SearchResponse response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("This is the first"));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("This is the first sentence"));
 
         // Postings hl also works but the fragment is the whole first sentence (size ignored)
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("This is the first sentence."));
 
         //if there's a match we only return the values with matches (whole value as number_of_fragments == 0)
         MatchQueryBuilder queryBuilder = QueryBuilders.matchQuery("text", "third fifth");
         field.highlighterType("plain");
-        response = client().prepareSearch("test").setQuery(queryBuilder).highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").setQuery(queryBuilder).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 2, equalTo("This is the <em>third</em> sentence. This is the fourth sentence."));
         assertHighlight(response, 0, "text", 1, 2, equalTo("This is the <em>fifth</em> sentence"));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").setQuery(queryBuilder).highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").setQuery(queryBuilder).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 2, equalTo("This is the <em>third</em> sentence. This is the fourth sentence."));
         assertHighlight(response, 0, "text", 1, 2, equalTo("This is the <em>fifth</em> sentence"));
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").setQuery(queryBuilder).highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").setQuery(queryBuilder).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 2, equalTo("This is the <em>third</em> sentence. This is the fourth sentence."));
         assertHighlight(response, 0, "text", 1, 2, equalTo("This is the <em>fifth</em> sentence"));
     }
@@ -2059,7 +1987,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "test"))
-                .highlighter(highlight().field("field1").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field1").preTags("<xxx>").postTags("</xxx>"));
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("this is a <xxx>test</xxx>"));
@@ -2067,7 +1995,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on field1, highlighting on field1");
         source = searchSource()
                 .query(termQuery("field1", "test"))
-                .highlighter(highlight().field("field1").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field1").preTags("<xxx>").postTags("</xxx>"));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2076,7 +2004,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on field2, highlighting on field2");
         source = searchSource()
                 .query(termQuery("field2", "quick"))
-                .highlighter(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>"));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2085,7 +2013,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on field2, highlighting on field2");
         source = searchSource()
                 .query(matchPhraseQuery("field2", "quick brown"))
-                .highlighter(highlight().field("field2").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field2").preTags("<xxx>").postTags("</xxx>"));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2096,7 +2024,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on field2, highlighting on field2, falling back to the plain highlighter");
         source = searchSource()
                 .query(matchPhraseQuery("_all", "quick brown"))
-                .highlighter(highlight().field("field2").preTags("<xxx>").postTags("</xxx>").highlighterType("highlighter").requireFieldMatch(false));
+                .highlight(highlight().field("field2").preTags("<xxx>").postTags("</xxx>").highlighterType("highlighter").requireFieldMatch(false));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2113,9 +2041,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("field1", "fox"))
-                .highlighter(
-                        new HighlightBuilder().field(new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>")
-                                .requireFieldMatch(true)))
+                .addHighlightedField(new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>").requireFieldMatch(true))
                 .get();
         assertHighlight(response, 0, "field1", 0, 1, equalTo("The <b>quick<b> brown <1>fox</1>."));
     }
@@ -2133,7 +2059,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "fox"))
-                .highlighter(highlight()
+                .highlight(highlight()
                         .field(new HighlightBuilder.Field("field1").numOfFragments(5).preTags("<field1>").postTags("</field1>")));
 
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
@@ -2148,7 +2074,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         source = searchSource()
                 .query(termQuery("field1", "fox"))
-                .highlighter(highlight()
+                .highlight(highlight()
                         .field(new HighlightBuilder.Field("field1").numOfFragments(0).preTags("<field1>").postTags("</field1>")));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
@@ -2198,7 +2124,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
             SearchSourceBuilder source = searchSource()
                     .query(multiMatchQueryBuilder)
-                    .highlighter(highlight().highlightQuery(randomBoolean() ? multiMatchQueryBuilder : null).highlighterType(highlighterType)
+                    .highlight(highlight().highlightQuery(randomBoolean() ? multiMatchQueryBuilder : null).highlighterType(highlighterType)
                             .field(new Field("field1").requireFieldMatch(true).preTags("<field1>").postTags("</field1>")));
             logger.info("Running multi-match type: [" + matchQueryType + "] highlight with type: [" + highlighterType + "]");
             SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
@@ -2222,7 +2148,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "sentence"))
-                .highlighter(highlight().field("field1").order("score"));
+                .highlight(highlight().field("field1").order("score"));
 
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2252,7 +2178,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse searchResponse = client().prepareSearch()
                 .setQuery(matchQuery("title", "test"))
-                .highlighter(new HighlightBuilder().field("title").encoder("html")).get();
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title").get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
             assertHighlight(searchResponse, i, "title", 0, 1, equalTo("This is a html escaping highlighting <em>test</em> for *&amp;?"));
@@ -2276,7 +2203,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch()
                 //lets make sure we analyze the query and we highlight the resulting terms
                 .setQuery(matchQuery("title", "This is a Test"))
-.highlighter(new HighlightBuilder().field("title")).get();
+                .addHighlightedField("title").get();
 
         assertHitCount(searchResponse, 1l);
         SearchHit hit = searchResponse.getHits().getAt(0);
@@ -2286,7 +2213,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title
         searchResponse = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().field("title.key")).get();
+                .addHighlightedField("title.key").get();
         assertHitCount(searchResponse, 1l);
 
         //stopwords are now highlighted since we used only whitespace analyzer here
@@ -2310,7 +2237,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // simple search on body with standard analyzer with a simple field query
         SearchResponse searchResponse = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().field("title"))
+                .addHighlightedField("title")
                 .get();
 
         assertHighlight(searchResponse, 0, "title", 0, 1, equalTo("this is a <em>test</em>"));
@@ -2318,7 +2245,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title.key
         searchResponse = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().field("title.key")).get();
+                .addHighlightedField("title.key").get();
 
         assertHighlight(searchResponse, 0, "title.key", 0, 1, equalTo("<em>this</em> <em>is</em> <em>a</em> <em>test</em>"));
     }
@@ -2340,27 +2267,30 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().field("title"))
+                .addHighlightedField("title")
                 .get();
         assertNoFailures(search);
 
         assertFailures(client().prepareSearch()
                         .setQuery(matchQuery("title", "this is a test"))
-                        .highlighter(new HighlightBuilder().field("title").highlighterType("postings-highlighter")),
+                        .addHighlightedField("title")
+                        .setHighlighterType("postings-highlighter"),
                 RestStatus.BAD_REQUEST,
                 containsString("the field [title] should be indexed with positions and offsets in the postings list to be used with postings highlighter"));
 
 
         assertFailures(client().prepareSearch()
                         .setQuery(matchQuery("title", "this is a test"))
-                        .highlighter(new HighlightBuilder().field("title").highlighterType("postings")),
+                        .addHighlightedField("title")
+                        .setHighlighterType("postings"),
                 RestStatus.BAD_REQUEST,
                 containsString("the field [title] should be indexed with positions and offsets in the postings list to be used with postings highlighter"));
 
         //should not fail if there is a wildcard
         assertNoFailures(client().prepareSearch()
                         .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().field("tit*").highlighterType("postings")).get());
+                        .addHighlightedField("tit*")
+                        .setHighlighterType("postings").get());
     }
 
     @Test
@@ -2374,7 +2304,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
-                .highlighter(highlight().field("field2").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").preTags("<x>").postTags("</x>"));
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The quick <x>brown</x> fox jumps over the lazy dog!"));
@@ -2389,7 +2319,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(commonTermsQuery("field2", "quick brown").cutoffFrequency(100))
-                .highlighter(highlight().field("field2").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").preTags("<x>").postTags("</x>"));
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
         assertHitCount(searchResponse, 1l);
 
@@ -2415,7 +2345,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field2");
 
         SearchSourceBuilder source = searchSource().query(prefixQuery("field2", "qui"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
 
@@ -2430,7 +2360,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(fuzzyQuery("field2", "quck"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
@@ -2445,7 +2375,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(regexpQuery("field2", "qu[a-l]+k"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
@@ -2460,13 +2390,13 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(wildcardQuery("field2", "qui*"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
 
         source = searchSource().query(wildcardQuery("field2", "qu*k"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHitCount(searchResponse, 1l);
 
@@ -2482,7 +2412,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(rangeQuery("field2").gte("aaaa").lt("zzzz"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("<em>aaab</em>"));
@@ -2497,7 +2427,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(queryStringQuery("qui*").defaultField("field2"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
     }
@@ -2513,7 +2443,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(constantScoreQuery(regexpQuery("field1", "pho[a-z]+")))
-                .highlighter(highlight().field("field1"));
+                .highlight(highlight().field("field1"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
     }
@@ -2532,7 +2462,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .should(constantScoreQuery(QueryBuilders.missingQuery("field1")))
                 .should(matchQuery("field1", "test"))
                 .should(constantScoreQuery(queryStringQuery("field1:photo*"))))
-                .highlighter(highlight().field("field1"));
+                .highlight(highlight().field("field1"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
     }
@@ -2548,7 +2478,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(boolQuery().must(prefixQuery("field1", "photo")).should(matchQuery("field1", "test").minimumShouldMatch("0")))
-                .highlighter(highlight().field("field1"));
+                .highlight(highlight().field("field1"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
     }
@@ -2564,7 +2494,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(boolQuery().must(queryStringQuery("field1:photo*")).filter(missingQuery("field_null")))
-                .highlighter(highlight().field("field1"));
+                .highlight(highlight().field("field1"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
     }
@@ -2593,10 +2523,10 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         SearchRequestBuilder searchRequestBuilder = client().prepareSearch()
                 .setSize(COUNT)
                 .setQuery(termQuery("field1", "test"))
-                .highlighter(new HighlightBuilder().field("field1"));
+                .addHighlightedField("field1");
         SearchResponse searchResponse =
                 searchRequestBuilder.get();
-        assertHitCount(searchResponse, COUNT);
+        assertHitCount(searchResponse, (long)COUNT);
         assertThat(searchResponse.getHits().hits().length, equalTo(COUNT));
         for (SearchHit hit : searchResponse.getHits()) {
             String prefix = prefixes.get(hit.id());
@@ -2666,8 +2596,9 @@ public class HighlighterSearchIT extends ESIntegTestCase {
             phraseBoostTestCaseForClauses(String highlighterType, float boost, QueryBuilder terms, P phrase) {
         Matcher<String> highlightedMatcher = Matchers.either(containsString("<em>highlight words together</em>")).or(
                 containsString("<em>highlight</em> <em>words</em> <em>together</em>"));
-        SearchRequestBuilder search = client().prepareSearch("test").highlighter(
-                new HighlightBuilder().field("field1", 100, 1).order("score").highlighterType(highlighterType).requireFieldMatch(true));
+        SearchRequestBuilder search = client().prepareSearch("test").setHighlighterRequireFieldMatch(true)
+                .setHighlighterOrder("score").setHighlighterType(highlighterType)
+                .addHighlightedField("field1", 100, 1);
 
         // Try with a bool query
         phrase.boost(boost);
diff --git a/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java b/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
index 417b6da..16c54c4 100644
--- a/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
@@ -33,7 +33,6 @@ import org.elasticsearch.script.Script;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHits;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
-import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.sort.SortBuilders;
 import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -44,24 +43,9 @@ import java.util.List;
 import java.util.Locale;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.hasChildQuery;
-import static org.elasticsearch.index.query.QueryBuilders.hasParentQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.nestedQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAllSuccessful;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.elasticsearch.index.query.QueryBuilders.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 /**
  */
@@ -102,14 +86,11 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .endObject()));
         indexRandom(true, requests);
 
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addNestedInnerHits("comment", "comments",
-                new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.message", "fox")));
         // Inner hits can be defined in two ways: 1) with the query 2) as seperate inner_hit definition
         SearchRequest[] searchRequests = new SearchRequest[]{
                 client().prepareSearch("articles").setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits("comment", null))).request(),
                 client().prepareSearch("articles").setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")))
-                        .innerHits(innerHitsBuilder).request()
+                        .addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.message", "fox"))).request()
         };
         for (SearchRequest searchRequest : searchRequests) {
             SearchResponse response = client().search(searchRequest).actionGet();
@@ -128,15 +109,10 @@ public class InnerHitsIT extends ESIntegTestCase {
             assertThat(innerHits.getAt(1).getNestedIdentity().getOffset(), equalTo(1));
         }
 
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addNestedInnerHits("comment", "comments",
-                new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.message", "elephant")));
-        // Inner hits can be defined in two ways: 1) with the query 2) as
-        // seperate inner_hit definition
         searchRequests = new SearchRequest[] {
                 client().prepareSearch("articles")
                         .setQuery(nestedQuery("comments", matchQuery("comments.message", "elephant")))
-                        .innerHits(innerHitsBuilder).request(),
+                        .addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.message", "elephant"))).request(),
                 client().prepareSearch("articles")
                         .setQuery(nestedQuery("comments", matchQuery("comments.message", "elephant")).innerHit(new QueryInnerHits("comment", null))).request(),
                 client().prepareSearch("articles")
@@ -163,23 +139,21 @@ public class InnerHitsIT extends ESIntegTestCase {
             assertThat(innerHits.getAt(2).getNestedIdentity().getOffset(), equalTo(2));
         }
         InnerHitsBuilder.InnerHit innerHit = new InnerHitsBuilder.InnerHit();
-        innerHit.highlighter(new HighlightBuilder().field("comments.message"));
+        innerHit.highlightBuilder().field("comments.message");
         innerHit.setExplain(true);
         innerHit.addFieldDataField("comments.message");
         innerHit.addScriptField("script", new Script("doc['comments.message'].value"));
         innerHit.setSize(1);
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addNestedInnerHits("comments", "comments", new InnerHitsBuilder.InnerHit()
-                            .setQuery(matchQuery("comments.message", "fox"))
-                            .highlighter(new HighlightBuilder().field("comments.message"))
-                            .setExplain(true)
-                            .addFieldDataField("comments.message")
-                            .addScriptField("script", new Script("doc['comments.message'].value"))
-                            .setSize(1));
         searchRequests = new SearchRequest[] {
                 client().prepareSearch("articles")
                         .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")))
-                        .innerHits(innerHitsBuilder).request(),
+                        .addNestedInnerHits("comments", "comments", new InnerHitsBuilder.InnerHit()
+                                .setQuery(matchQuery("comments.message", "fox"))
+                                .addHighlightedField("comments.message")
+                                .setExplain(true)
+                                .addFieldDataField("comments.message")
+                                .addScriptField("script", new Script("doc['comments.message'].value"))
+                                .setSize(1)).request(),
                 client().prepareSearch("articles")
                         .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits(null, innerHit))).request()
         };
@@ -224,13 +198,11 @@ public class InnerHitsIT extends ESIntegTestCase {
         int size = randomIntBetween(0, numDocs);
         SearchResponse searchResponse;
         if (randomBoolean()) {
-            InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-            innerHitsBuilder.addNestedInnerHits("a", "field1", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size)); // Sort order is DESC, because we reverse the inner objects during indexing!
-            innerHitsBuilder.addNestedInnerHits("b", "field2", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size));
             searchResponse = client().prepareSearch("idx")
                     .setSize(numDocs)
                     .addSort("_uid", SortOrder.ASC)
-                    .innerHits(innerHitsBuilder)
+                    .addNestedInnerHits("a", "field1", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size)) // Sort order is DESC, because we reverse the inner objects during indexing!
+                    .addNestedInnerHits("b", "field2", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size))
                     .get();
         } else {
             BoolQueryBuilder boolQuery = new BoolQueryBuilder();
@@ -292,12 +264,10 @@ public class InnerHitsIT extends ESIntegTestCase {
         requests.add(client().prepareIndex("articles", "comment", "6").setParent("2").setSource("message", "elephant scared by mice x y"));
         indexRandom(true, requests);
 
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "fox")));
         SearchRequest[] searchRequests = new SearchRequest[]{
                 client().prepareSearch("articles")
                         .setQuery(hasChildQuery("comment", matchQuery("message", "fox")))
-                        .innerHits(innerHitsBuilder)
+                        .addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "fox")))
                         .request(),
                 client().prepareSearch("articles")
                         .setQuery(hasChildQuery("comment", matchQuery("message", "fox")).innerHit(new QueryInnerHits("comment", null)))
@@ -320,12 +290,10 @@ public class InnerHitsIT extends ESIntegTestCase {
             assertThat(innerHits.getAt(1).type(), equalTo("comment"));
         }
 
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "elephant")));
         searchRequests = new SearchRequest[] {
                 client().prepareSearch("articles")
                         .setQuery(hasChildQuery("comment", matchQuery("message", "elephant")))
-                        .innerHits(innerHitsBuilder)
+                        .addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "elephant")))
                         .request(),
                 client().prepareSearch("articles")
                         .setQuery(hasChildQuery("comment", matchQuery("message", "elephant")).innerHit(new QueryInnerHits()))
@@ -349,24 +317,22 @@ public class InnerHitsIT extends ESIntegTestCase {
             assertThat(innerHits.getAt(2).type(), equalTo("comment"));
         }
         InnerHitsBuilder.InnerHit innerHit = new InnerHitsBuilder.InnerHit();
-        innerHit.highlighter(new HighlightBuilder().field("message"));
+        innerHit.highlightBuilder().field("message");
         innerHit.setExplain(true);
         innerHit.addFieldDataField("message");
         innerHit.addScriptField("script", new Script("doc['message'].value"));
         innerHit.setSize(1);
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit()
-                            .setQuery(matchQuery("message", "fox"))
-                            .highlighter(new HighlightBuilder().field("message"))
-                            .setExplain(true)
-                            .addFieldDataField("message")
-                            .addScriptField("script", new Script("doc['message'].value"))
-                            .setSize(1));
         searchRequests = new SearchRequest[] {
                 client().prepareSearch("articles")
                         .setQuery(hasChildQuery("comment", matchQuery("message", "fox")))
-                        .innerHits(innerHitsBuilder)
-                        .request(),
+                        .addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit()
+                                        .setQuery(matchQuery("message", "fox"))
+                                        .addHighlightedField("message")
+                                        .setExplain(true)
+                                        .addFieldDataField("message")
+                                        .addScriptField("script", new Script("doc['message'].value"))
+                                        .setSize(1)
+                        ).request(),
 
                 client().prepareSearch("articles")
                         .setQuery(
@@ -417,16 +383,14 @@ public class InnerHitsIT extends ESIntegTestCase {
         indexRandom(true, requestBuilders);
 
         int size = randomIntBetween(0, numDocs);
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("a", "child1", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size));
-        innerHitsBuilder.addParentChildInnerHits("b", "child2", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size));
         SearchResponse searchResponse;
         if (randomBoolean()) {
             searchResponse = client().prepareSearch("idx")
                     .setSize(numDocs)
                     .setTypes("parent")
                     .addSort("_uid", SortOrder.ASC)
-                    .innerHits(innerHitsBuilder)
+                    .addParentChildInnerHits("a", "child1", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size))
+                    .addParentChildInnerHits("b", "child2", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size))
                     .get();
         } else {
             BoolQueryBuilder boolQuery = new BoolQueryBuilder();
@@ -482,15 +446,12 @@ public class InnerHitsIT extends ESIntegTestCase {
     }
 
     @Test
-    @AwaitsFix(bugUrl = "need validation of type or path defined in InnerHitsBuilder")
     public void testPathOrTypeMustBeDefined() {
         createIndex("articles");
         ensureGreen("articles");
         try {
-            InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-            innerHitsBuilder.addParentChildInnerHits("comment", null, new InnerHitsBuilder.InnerHit());
             client().prepareSearch("articles")
-                    .innerHits(innerHitsBuilder)
+                    .addParentChildInnerHits("comment", null, new InnerHitsBuilder.InnerHit())
                     .get();
         } catch (Exception e) {
             assertThat(e.getMessage(), containsString("Failed to build"));
@@ -554,15 +515,13 @@ public class InnerHitsIT extends ESIntegTestCase {
         requests.add(client().prepareIndex("articles", "remark", "2").setParent("2").setRouting("2").setSource("message", "bad"));
         indexRandom(true, requests);
 
-        InnerHitsBuilder innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addParentChildInnerHits("remark", "remark", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "good")));
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit()
-                            .setQuery(hasChildQuery("remark", matchQuery("message", "good")))
-                            .innerHits(innerInnerHitsBuilder));
         SearchResponse response = client().prepareSearch("articles")
                 .setQuery(hasChildQuery("comment", hasChildQuery("remark", matchQuery("message", "good"))))
-                .innerHits(innerHitsBuilder)
+                .addParentChildInnerHits("comment", "comment",
+                        new InnerHitsBuilder.InnerHit()
+                                .setQuery(hasChildQuery("remark", matchQuery("message", "good")))
+                                .addParentChildInnerHits("remark", "remark", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "good")))
+                )
                 .get();
 
         assertNoFailures(response);
@@ -580,15 +539,13 @@ public class InnerHitsIT extends ESIntegTestCase {
         assertThat(innerHits.getAt(0).getId(), equalTo("1"));
         assertThat(innerHits.getAt(0).type(), equalTo("remark"));
 
-        innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addParentChildInnerHits("remark", "remark", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "bad")));
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit()
-                .setQuery(hasChildQuery("remark", matchQuery("message", "bad")))
-                .innerHits(innerInnerHitsBuilder));
         response = client().prepareSearch("articles")
                 .setQuery(hasChildQuery("comment", hasChildQuery("remark", matchQuery("message", "bad"))))
-                .innerHits(innerHitsBuilder)
+                .addParentChildInnerHits("comment", "comment",
+                        new InnerHitsBuilder.InnerHit()
+                                .setQuery(hasChildQuery("remark", matchQuery("message", "bad")))
+                                .addParentChildInnerHits("remark", "remark", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "bad")))
+                )
                 .get();
 
         assertNoFailures(response);
@@ -650,16 +607,12 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .endObject()));
         indexRandom(true, requests);
 
-        InnerHitsBuilder innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addNestedInnerHits("remark", "comments.remarks", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.remarks.message", "good")));
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit()
-                .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "good")))
-                .innerHits(innerInnerHitsBuilder)
-        );
         SearchResponse response = client().prepareSearch("articles")
                 .setQuery(nestedQuery("comments", nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "good"))))
-                .innerHits(innerHitsBuilder).get();
+                .addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit()
+                                .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "good")))
+                                .addNestedInnerHits("remark", "comments.remarks", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.remarks.message", "good")))
+                ).get();
         assertNoFailures(response);
         assertHitCount(response, 1);
         assertSearchHit(response, 1, hasId("1"));
@@ -696,15 +649,11 @@ public class InnerHitsIT extends ESIntegTestCase {
         assertThat(innerHits.getAt(0).getNestedIdentity().getChild().getField().string(), equalTo("remarks"));
         assertThat(innerHits.getAt(0).getNestedIdentity().getChild().getOffset(), equalTo(0));
 
-        innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addNestedInnerHits("remark", "comments.remarks", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.remarks.message", "bad")));
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit()
-                            .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "bad")))
-                            .innerHits(innerInnerHitsBuilder));
         response = client().prepareSearch("articles")
                 .setQuery(nestedQuery("comments", nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "bad"))))
-                .innerHits(innerHitsBuilder)
+                .addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit()
+                        .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "bad")))
+                        .addNestedInnerHits("remark", "comments.remarks", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.remarks.message", "bad"))))
                 .get();
         assertNoFailures(response);
         assertHitCount(response, 1);
@@ -752,8 +701,6 @@ public class InnerHitsIT extends ESIntegTestCase {
     }
 
     @Test
-    @AwaitsFix(bugUrl = "needs fixing following search request refactoring")
-    // NORELEASE
     public void testNestedInnerHitsWithStoredFieldsAndNoSourceBackcompat() throws Exception {
         assertAcked(prepareCreate("articles")
                 .setSettings(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_4_2.id)
@@ -789,7 +736,7 @@ public class InnerHitsIT extends ESIntegTestCase {
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getField().string(), equalTo("comments"));
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getOffset(), equalTo(0));
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getChild(), nullValue());
-        assertThat(String.valueOf(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).fields().get("comments.message").getValue()), equalTo("fox eat quick"));
+        assertThat(String.valueOf((Object)response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).fields().get("comments.message").getValue()), equalTo("fox eat quick"));
     }
 
     @Test
@@ -817,7 +764,7 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .endObject()));
         indexRandom(true, requests);
         InnerHitsBuilder.InnerHit builder = new InnerHitsBuilder.InnerHit();
-        builder.highlighter(new HighlightBuilder().field("comments.message"));
+        builder.highlightBuilder().field("comments.message");
         SearchResponse response = client().prepareSearch("articles")
                 .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits(null, builder)))
                 .get();
@@ -833,8 +780,6 @@ public class InnerHitsIT extends ESIntegTestCase {
     }
 
     @Test
-    @AwaitsFix(bugUrl = "needs fixing following search request refactoring")
-    // NORELEASE
     public void testNestedInnerHitsWithExcludeSourceBackcompat() throws Exception {
         assertAcked(prepareCreate("articles").setSettings(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_4_2.id)
                         .addMapping("article", jsonBuilder().startObject()
@@ -871,12 +816,10 @@ public class InnerHitsIT extends ESIntegTestCase {
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getField().string(), equalTo("comments"));
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getOffset(), equalTo(0));
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getChild(), nullValue());
-        assertThat(String.valueOf(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).fields().get("comments.message").getValue()), equalTo("fox eat quick"));
+        assertThat(String.valueOf((Object)response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).fields().get("comments.message").getValue()), equalTo("fox eat quick"));
     }
 
     @Test
-    @AwaitsFix(bugUrl = "needs fixing following search request refactoring")
-    // NORELEASE
     public void testNestedInnerHitsHiglightWithExcludeSourceBackcompat() throws Exception {
         assertAcked(prepareCreate("articles").setSettings(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_4_2.id)
                         .addMapping("article", jsonBuilder().startObject()
@@ -900,7 +843,7 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .endObject()));
         indexRandom(true, requests);
         InnerHitsBuilder.InnerHit builder = new InnerHitsBuilder.InnerHit();
-        builder.highlighter(new HighlightBuilder().field("comments.message"));
+        builder.highlightBuilder().field("comments.message");
         SearchResponse response = client().prepareSearch("articles")
                 .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits(null, builder)))
                         .get();
@@ -1012,23 +955,17 @@ public class InnerHitsIT extends ESIntegTestCase {
         requests.add(client().prepareIndex("royals", "baron", "baron4").setParent("earl4").setRouting("king").setSource("{}"));
         indexRandom(true, requests);
 
-        InnerHitsBuilder innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addParentChildInnerHits("barons", "baron", new InnerHitsBuilder.InnerHit());
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("earls", "earl", new InnerHitsBuilder.InnerHit()
-                .addSort(SortBuilders.fieldSort("_uid").order(SortOrder.ASC))
-                .setSize(4)
-                .innerHits(innerInnerHitsBuilder)
-        );
-        innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addParentChildInnerHits("kings", "king", new InnerHitsBuilder.InnerHit());
-        innerHitsBuilder.addParentChildInnerHits("princes", "prince",
-        new InnerHitsBuilder.InnerHit()
-            .innerHits(innerInnerHitsBuilder)
-        );
         SearchResponse response = client().prepareSearch("royals")
                 .setTypes("duke")
-                .innerHits(innerHitsBuilder)
+                .addParentChildInnerHits("earls", "earl", new InnerHitsBuilder.InnerHit()
+                                .addSort(SortBuilders.fieldSort("_uid").order(SortOrder.ASC))
+                                .setSize(4)
+                                .addParentChildInnerHits("barons", "baron", new InnerHitsBuilder.InnerHit())
+                )
+                .addParentChildInnerHits("princes", "prince",
+                        new InnerHitsBuilder.InnerHit()
+                        .addParentChildInnerHits("kings", "king", new InnerHitsBuilder.InnerHit())
+                )
                 .get();
         assertHitCount(response, 1);
         assertThat(response.getHits().getAt(0).getId(), equalTo("duke"));
diff --git a/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java b/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
index ac364a8..83a8008 100644
--- a/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
@@ -145,15 +145,15 @@ public class SearchQueryIT extends ESIntegTestCase {
                 client().prepareSearch().setQuery(matchAllQuery()).setPostFilter(notQuery(termQuery("field1", "value3"))).get(),
                 2l);
     }
-// NORELEASE  This should be tested in SearchSourceBuilderTests
-//    @Test
-//    public void passQueryAsStringTest() throws Exception {
-//        createIndex("test");
-//        client().prepareIndex("test", "type1", "1").setSource("field1", "value1_1", "field2", "value2_1").setRefresh(true).get();
-//
-//        SearchResponse searchResponse = client().prepareSearch().setQuery("{ \"term\" : { \"field1\" : \"value1_1\" }}").get();
-//        assertHitCount(searchResponse, 1l);
-//    }
+
+    @Test
+    public void passQueryAsStringTest() throws Exception {
+        createIndex("test");
+        client().prepareIndex("test", "type1", "1").setSource("field1", "value1_1", "field2", "value2_1").setRefresh(true).get();
+
+        SearchResponse searchResponse = client().prepareSearch().setQuery("{ \"term\" : { \"field1\" : \"value1_1\" }}").get();
+        assertHitCount(searchResponse, 1l);
+    }
 
     @Test
     public void testIndexOptions() throws Exception {
@@ -310,10 +310,9 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("2"));
 
-     // NORELEASE  This should be tested in SearchSourceBuilderTests
-//        searchResponse = client().prepareSearch().setQuery("{ \"common\" : { \"field1\" : { \"query\" : \"the lazy fox brown\", \"cutoff_frequency\" : 1, \"minimum_should_match\" : { \"high_freq\" : 4 } } } }").get();
-//        assertHitCount(searchResponse, 1l);
-//        assertFirstHit(searchResponse, hasId("2"));
+        searchResponse = client().prepareSearch().setQuery("{ \"common\" : { \"field1\" : { \"query\" : \"the lazy fox brown\", \"cutoff_frequency\" : 1, \"minimum_should_match\" : { \"high_freq\" : 4 } } } }").get();
+        assertHitCount(searchResponse, 1l);
+        assertFirstHit(searchResponse, hasId("2"));
 
         // Default
         searchResponse = client().prepareSearch().setQuery(commonTermsQuery("field1", "the lazy fox brown").cutoffFrequency(1)).get();
@@ -403,10 +402,9 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("2"));
 
-     // NORELEASE  This should be tested in SearchSourceBuilderTests
-//        searchResponse = client().prepareSearch().setQuery("{ \"common\" : { \"field1\" : { \"query\" : \"the fast lazy fox brown\", \"cutoff_frequency\" : 1, \"minimum_should_match\" : { \"high_freq\" : 6 } } } }").get();
-//        assertHitCount(searchResponse, 1l);
-//        assertFirstHit(searchResponse, hasId("2"));
+        searchResponse = client().prepareSearch().setQuery("{ \"common\" : { \"field1\" : { \"query\" : \"the fast lazy fox brown\", \"cutoff_frequency\" : 1, \"minimum_should_match\" : { \"high_freq\" : 6 } } } }").get();
+        assertHitCount(searchResponse, 1l);
+        assertFirstHit(searchResponse, hasId("2"));
 
         // Default
         searchResponse = client().prepareSearch().setQuery(commonTermsQuery("field1", "the fast lazy fox brown").cutoffFrequency(1)).get();
@@ -1508,14 +1506,13 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertHitCount(searchResponse, 2l);
     }
 
- // NORELEASE  This should be tested in SearchSourceBuilderTests
-//    @Test
-//    public void testEmptyTopLevelFilter() {
-//        client().prepareIndex("test", "type", "1").setSource("field", "value").setRefresh(true).get();
-//
-//        SearchResponse searchResponse = client().prepareSearch().setPostFilter("{}").get();
-//        assertHitCount(searchResponse, 1l);
-//    }
+    @Test
+    public void testEmptyTopLevelFilter() {
+        client().prepareIndex("test", "type", "1").setSource("field", "value").setRefresh(true).get();
+
+        SearchResponse searchResponse = client().prepareSearch().setPostFilter("{}").get();
+        assertHitCount(searchResponse, 1l);
+    }
 
     @Test // see #2926
     public void testMustNot() throws IOException, ExecutionException, InterruptedException {
@@ -2238,26 +2235,25 @@ functionScoreQuery(scriptFunction(new Script("_doc['score'].value")))).setMinSco
         }
     }
 
- // NORELEASE  This should be tested in SearchSourceBuilderTests
-//    @Test // see #7686.
-//    public void testIdsQueryWithInvalidValues() throws Exception {
-//        createIndex("test");
-//        indexRandom(true, false, client().prepareIndex("test", "type", "1").setSource("body", "foo"));
-//
-//        try {
-//            client().prepareSearch("test")
-//                    .setTypes("type")
-//                    .setQuery("{\n" +
-//                            "  \"ids\": {\n" +
-//                            "    \"values\": [[\"1\"]]\n" +
-//                            "  }\n" +
-//                            "}")
-//                    .get();
-//            fail("query is invalid and should have produced a parse exception");
-//        } catch (Exception e) {
-//            assertThat("query could not be parsed due to bad format: " + e.toString(),
-//                    e.toString().contains("Illegal value for id, expecting a string or number, got: START_ARRAY"),
-//                    equalTo(true));
-//        }
-//    }
+    @Test // see #7686.
+    public void testIdsQueryWithInvalidValues() throws Exception {
+        createIndex("test");
+        indexRandom(true, false, client().prepareIndex("test", "type", "1").setSource("body", "foo"));
+
+        try {
+            client().prepareSearch("test")
+                    .setTypes("type")
+                    .setQuery("{\n" +
+                            "  \"ids\": {\n" +
+                            "    \"values\": [[\"1\"]]\n" +
+                            "  }\n" +
+                            "}")
+                    .get();
+            fail("query is invalid and should have produced a parse exception");
+        } catch (Exception e) {
+            assertThat("query could not be parsed due to bad format: " + e.toString(),
+                    e.toString().contains("Illegal value for id, expecting a string or number, got: START_ARRAY"),
+                    equalTo(true));
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java b/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
index fe44034..bf3e458 100644
--- a/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
@@ -21,12 +21,11 @@ package org.elasticsearch.search.query;
 
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.BoolQueryBuilder;
 import org.elasticsearch.index.query.Operator;
-import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.SimpleQueryStringFlag;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -35,17 +34,8 @@ import java.util.Locale;
 import java.util.concurrent.ExecutionException;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
-import static org.elasticsearch.index.query.QueryBuilders.simpleQueryStringQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFirstHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
+import static org.elasticsearch.index.query.QueryBuilders.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
 import static org.hamcrest.Matchers.equalTo;
 
 /**
@@ -252,12 +242,10 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
         assertHitCount(searchResponse, 3l);
         assertSearchHits(searchResponse, "1", "2", "3");
 
-
-        // NORELEASE  This should be tested in SimpleQueryStringQueryBuilderTests
-//        // Sending a negative 'flags' value is the same as SimpleQueryStringFlag.ALL
-//        searchResponse = client().prepareSearch().setQuery("{\"simple_query_string\": {\"query\": \"foo bar\", \"flags\": -1}}").get();
-//        assertHitCount(searchResponse, 3l);
-//        assertSearchHits(searchResponse, "1", "2", "3");
+        // Sending a negative 'flags' value is the same as SimpleQueryStringFlag.ALL
+        searchResponse = client().prepareSearch().setQuery("{\"simple_query_string\": {\"query\": \"foo bar\", \"flags\": -1}}").get();
+        assertHitCount(searchResponse, 3l);
+        assertSearchHits(searchResponse, "1", "2", "3");
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("foo | bar")
@@ -279,18 +267,21 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
                         .flags(SimpleQueryStringFlag.NONE)).get();
         assertHitCount(searchResponse, 0l);
 
-        searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().query(QueryBuilders.simpleQueryStringQuery("foo|bar").defaultOperator(Operator.AND)
-                                .flags(SimpleQueryStringFlag.NONE))).get();
+        searchResponse = client().prepareSearch().setSource(new BytesArray("{\n" +
+                "  \"query\": {\n" +
+                "    \"simple_query_string\": {\n" +
+                "      \"query\": \"foo|bar\",\n" +
+                "      \"default_operator\": \"AND\"," +
+                "      \"flags\": \"NONE\"\n" +
+                "    }\n" +
+                "  }\n" +
+                "}")).get();
         assertHitCount(searchResponse, 1l);
 
-        searchResponse = client()
-                .prepareSearch()
-                .setQuery(
-                        simpleQueryStringQuery("baz | egg*").defaultOperator(Operator.AND).flags(SimpleQueryStringFlag.WHITESPACE,
-                                SimpleQueryStringFlag.PREFIX)).get();
+        searchResponse = client().prepareSearch().setQuery(
+                simpleQueryStringQuery("baz | egg*")
+                        .defaultOperator(Operator.AND)
+                        .flags(SimpleQueryStringFlag.WHITESPACE, SimpleQueryStringFlag.PREFIX)).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("4"));
     }
diff --git a/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java b/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java
index ee27e37..5a22afe 100644
--- a/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java
+++ b/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java
@@ -57,8 +57,6 @@ import static org.hamcrest.Matchers.*;
 public class QueryRescorerIT extends ESIntegTestCase {
 
     @Test
-    @AwaitsFix(bugUrl = "Need to fix default window size for rescorers so that they are applied")
-    // NORELEASE
     public void testEnforceWindowSize() {
         createIndex("test");
         // this
@@ -217,8 +215,6 @@ public class QueryRescorerIT extends ESIntegTestCase {
 
     // Tests a rescore window smaller than number of hits:
     @Test
-    @AwaitsFix(bugUrl = "Need to fix default window size for rescorers so that they are applied")
-    // NORELEASE
     public void testSmallRescoreWindow() throws Exception {
         Builder builder = Settings.builder();
         builder.put("index.analysis.analyzer.synonym.tokenizer", "whitespace");
@@ -290,8 +286,6 @@ public class QueryRescorerIT extends ESIntegTestCase {
 
     // Tests a rescorer that penalizes the scores:
     @Test
-    @AwaitsFix(bugUrl = "Need to fix default window size for rescorers so that they are applied")
-    // NORELEASE
     public void testRescorerMadeScoresWorse() throws Exception {
         Builder builder = Settings.builder();
         builder.put("index.analysis.analyzer.synonym.tokenizer", "whitespace");
@@ -619,10 +613,10 @@ public class QueryRescorerIT extends ESIntegTestCase {
                                         ScoreFunctionBuilders.scriptFunction(new Script("5.0f"))).boostMode(CombineFunction.REPLACE))
                                 .should(QueryBuilders.functionScoreQuery(QueryBuilders.termQuery("field1", intToEnglish[3]),
                                         ScoreFunctionBuilders.scriptFunction(new Script("0.2f"))).boostMode(CombineFunction.REPLACE)))
-                        .setFrom(0)
-                        .setSize(10)
-                        .setRescorer(rescoreQuery)
-                        .setRescoreWindow(50).execute().actionGet();
+                                .setFrom(0)
+                                .setSize(10)
+                                .setRescorer(rescoreQuery)
+                                .setRescoreWindow(50).execute().actionGet();
 
                 assertHitCount(rescored, 4);
 
@@ -672,8 +666,6 @@ public class QueryRescorerIT extends ESIntegTestCase {
     }
 
     @Test
-    @AwaitsFix(bugUrl = "Need to fix default window size for rescorers so that they are applied")
-    // NORELEASE
     public void testMultipleRescores() throws Exception {
         int numDocs = indexRandomNumbers("keyword", 1, true);
         QueryRescorer eightIsGreat = RescoreBuilder.queryRescorer(
diff --git a/core/src/test/java/org/elasticsearch/search/sort/SimpleSortIT.java b/core/src/test/java/org/elasticsearch/search/sort/SimpleSortIT.java
index 315592b..e487819 100644
--- a/core/src/test/java/org/elasticsearch/search/sort/SimpleSortIT.java
+++ b/core/src/test/java/org/elasticsearch/search/sort/SimpleSortIT.java
@@ -30,6 +30,7 @@ import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.ShardSearchFailure;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.geo.GeoDistance;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.text.StringAndBytesText;
@@ -43,7 +44,6 @@ import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHitField;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.junit.annotations.TestLogging;
 import org.hamcrest.Matchers;
@@ -1868,6 +1868,50 @@ public class SimpleSortIT extends ESIntegTestCase {
         assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(3.25, 4, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
         assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(5.25, 4, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
 
+        //test all the different formats in one
+        createQPoints(qHashes, qPoints);
+        XContentBuilder searchSourceBuilder = jsonBuilder();
+        searchSourceBuilder.startObject().startArray("sort").startObject().startObject("_geo_distance").startArray("location");
+
+        for (int i = 0; i < 4; i++) {
+            int at = randomInt(qPoints.size() - 1);
+            int format = randomInt(3);
+            switch (format) {
+                case 0: {
+                    searchSourceBuilder.value(qHashes.get(at));
+                    break;
+                }
+                case 1: {
+                    searchSourceBuilder.value(qPoints.get(at).lat() + "," + qPoints.get(at).lon());
+                    break;
+                }
+                case 2: {
+                    searchSourceBuilder.value(qPoints.get(at));
+                    break;
+                }
+                case 3: {
+                    searchSourceBuilder.startArray().value(qPoints.get(at).lon()).value(qPoints.get(at).lat()).endArray();
+                    break;
+                }
+            }
+            qHashes.remove(at);
+            qPoints.remove(at);
+        }
+
+        searchSourceBuilder.endArray();
+        searchSourceBuilder.field("order", "asc");
+        searchSourceBuilder.field("unit", "km");
+        searchSourceBuilder.field("sort_mode", "min");
+        searchSourceBuilder.field("distance_type", "plane");
+        searchSourceBuilder.endObject();
+        searchSourceBuilder.endObject();
+        searchSourceBuilder.endArray();
+        searchSourceBuilder.endObject();
+
+        searchResponse = client().prepareSearch().setSource(searchSourceBuilder.bytes()).execute().actionGet();
+        assertOrderedSearchHits(searchResponse, "d1", "d2");
+        assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(2.5, 1, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
+        assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(4.5, 1, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
     }
 
     public void testSinglePointGeoDistanceSort() throws ExecutionException, InterruptedException, IOException {
@@ -1906,25 +1950,40 @@ public class SimpleSortIT extends ESIntegTestCase {
                 .execute().actionGet();
         checkCorrectSortOrderForGeoSort(searchResponse);
 
-        searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().sort(SortBuilders.geoDistanceSort("location").point(2.0, 2.0)
-                                .unit(DistanceUnit.KILOMETERS).geoDistance(GeoDistance.PLANE))).execute().actionGet();
+        String geoSortRequest = jsonBuilder().startObject().startArray("sort").startObject()
+                .startObject("_geo_distance")
+                .startArray("location").value(2f).value(2f).endArray()
+                .field("unit", "km")
+                .field("distance_type", "plane")
+                .endObject()
+                .endObject().endArray().string();
+        searchResponse = client().prepareSearch().setSource(new BytesArray(geoSortRequest))
+                .execute().actionGet();
         checkCorrectSortOrderForGeoSort(searchResponse);
 
-        searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().sort(SortBuilders.geoDistanceSort("location").geohashes("s037ms06g7h0")
-                                .unit(DistanceUnit.KILOMETERS).geoDistance(GeoDistance.PLANE))).execute().actionGet();
+        geoSortRequest = jsonBuilder().startObject().startArray("sort").startObject()
+                .startObject("_geo_distance")
+                .field("location", "s037ms06g7h0")
+                .field("unit", "km")
+                .field("distance_type", "plane")
+                .endObject()
+                .endObject().endArray().string();
+        searchResponse = client().prepareSearch().setSource(new BytesArray(geoSortRequest))
+                .execute().actionGet();
         checkCorrectSortOrderForGeoSort(searchResponse);
 
-        searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().sort(SortBuilders.geoDistanceSort("location").point(2.0, 2.0)
-                                .unit(DistanceUnit.KILOMETERS).geoDistance(GeoDistance.PLANE))).execute().actionGet();
+        geoSortRequest = jsonBuilder().startObject().startArray("sort").startObject()
+                .startObject("_geo_distance")
+                .startObject("location")
+                .field("lat", 2)
+                .field("lon", 2)
+                .endObject()
+                .field("unit", "km")
+                .field("distance_type", "plane")
+                .endObject()
+                .endObject().endArray().string();
+        searchResponse = client().prepareSearch().setSource(new BytesArray(geoSortRequest))
+                .execute().actionGet();
         checkCorrectSortOrderForGeoSort(searchResponse);
     }
 
diff --git a/core/src/test/java/org/elasticsearch/search/stats/SearchStatsIT.java b/core/src/test/java/org/elasticsearch/search/stats/SearchStatsIT.java
index c1e54f9..be9bfde 100644
--- a/core/src/test/java/org/elasticsearch/search/stats/SearchStatsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/stats/SearchStatsIT.java
@@ -32,7 +32,6 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.search.stats.SearchStats.Stats;
 import org.elasticsearch.script.Script;
-import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -101,7 +100,7 @@ public class SearchStatsIT extends ESIntegTestCase {
         for (int i = 0; i < iters; i++) {
             SearchResponse searchResponse = internalCluster().clientNodeClient().prepareSearch()
                     .setQuery(QueryBuilders.termQuery("field", "value")).setStats("group1", "group2")
-                    .highlighter(new HighlightBuilder().field("field"))
+                    .addHighlightedField("field")
                     .addScriptField("scrip1", new Script("_source.field"))
                     .setSize(100)
                     .execute().actionGet();
@@ -138,9 +137,9 @@ public class SearchStatsIT extends ESIntegTestCase {
                 assertThat(total.getQueryTimeInMillis(), equalTo(0l));
             }
         }
-
+        
         assertThat(num, greaterThan(0));
-
+     
     }
 
     private Set<String> nodeIdsWithIndex(String... indices) {
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java b/core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java
index df04d43..9b97afc 100644
--- a/core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java
@@ -20,12 +20,13 @@ package org.elasticsearch.search.suggest;
 
 import org.elasticsearch.action.search.SearchRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.CollectionUtils;
+import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
-import org.elasticsearch.test.ESIntegTestCase.Scope;
 import org.junit.Test;
 
 import java.io.IOException;
@@ -34,6 +35,7 @@ import java.util.List;
 import java.util.Locale;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.test.ESIntegTestCase.Scope;
 import static org.hamcrest.Matchers.hasSize;
 import static org.hamcrest.Matchers.is;
 
@@ -57,12 +59,11 @@ public class CustomSuggesterSearchIT extends ESIntegTestCase {
                 .endObject())
                 .setRefresh(true).execute().actionGet();
         ensureYellow();
-
+        
         String randomText = randomAsciiOfLength(10);
         String randomField = randomAsciiOfLength(10);
         String randomSuffix = randomAsciiOfLength(10);
-        SuggestBuilder suggestBuilder = new SuggestBuilder();
-        suggestBuilder.addSuggestion(
+        SearchRequestBuilder searchRequestBuilder = client().prepareSearch("test").setTypes("test").setFrom(0).setSize(1).addSuggestion(
                 new SuggestBuilder.SuggestionBuilder<SuggestBuilder.SuggestionBuilder>("someName", "custom") {
                     @Override
                     protected XContentBuilder innerToXContent(XContentBuilder builder, Params params) throws IOException {
@@ -72,8 +73,6 @@ public class CustomSuggesterSearchIT extends ESIntegTestCase {
                     }
                 }.text(randomText)
         );
-        SearchRequestBuilder searchRequestBuilder = client().prepareSearch("test").setTypes("test").setFrom(0).setSize(1)
-                .suggest(suggestBuilder);
 
         SearchResponse searchResponse = searchRequestBuilder.execute().actionGet();
 
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java b/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
index 5e03a22..e55a736 100644
--- a/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
@@ -19,16 +19,12 @@
 
 package org.elasticsearch.search.suggest;
 
+import java.nio.charset.StandardCharsets;
 import com.google.common.io.Resources;
-
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.index.IndexRequestBuilder;
-import org.elasticsearch.action.search.ReduceSearchPhaseException;
-import org.elasticsearch.action.search.SearchPhaseExecutionException;
-import org.elasticsearch.action.search.SearchRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.ShardSearchFailure;
+import org.elasticsearch.action.search.*;
 import org.elasticsearch.action.suggest.SuggestRequestBuilder;
 import org.elasticsearch.action.suggest.SuggestResponse;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -42,13 +38,7 @@ import org.elasticsearch.test.hamcrest.ElasticsearchAssertions;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 import java.util.concurrent.ExecutionException;
 
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
@@ -58,17 +48,8 @@ import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
 import static org.elasticsearch.search.suggest.SuggestBuilders.phraseSuggestion;
 import static org.elasticsearch.search.suggest.SuggestBuilders.termSuggestion;
 import static org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder.candidateGenerator;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestion;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestionPhraseCollateMatchExists;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestionSize;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.endsWith;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.nullValue;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 /**
  * Integration tests for term and phrase suggestions.  Many of these tests many requests that vary only slightly from one another.  Where
@@ -178,7 +159,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
                 .put("index.analysis.filter.shingler.type", "shingle")
                 .put("index.analysis.filter.shingler.min_shingle_size", 2)
                 .put("index.analysis.filter.shingler.max_shingle_size", 3));
-
+        
         XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
                 .startObject("properties")
                 .startObject("name")
@@ -198,7 +179,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
                 .endObject().endObject();
         assertAcked(builder.addMapping("type1", mapping));
         ensureGreen();
-
+        
 
         index("test", "type1", "1", "name", "I like iced tea");
         index("test", "type1", "2", "name", "I like tea.");
@@ -216,7 +197,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
         searchSuggest = searchSuggest( "ice tea", phraseSuggestion);
         assertSuggestionSize(searchSuggest, 0, 0, "did_you_mean");
     }
-
+    
     @Test // see #2729
     public void testSizeOneShard() throws Exception {
         prepareCreate("test").setSettings(
@@ -231,7 +212,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch().setQuery(matchQuery("text", "spellchecker")).get();
         assertThat("didn't ask for suggestions but got some", search.getSuggest(), nullValue());
-
+        
         TermSuggestionBuilder termSuggestion = termSuggestion("test")
                 .suggestMode("always") // Always, otherwise the results can vary between requests.
                 .text("abcd")
@@ -244,7 +225,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
         suggest = searchSuggest( termSuggestion);
         assertSuggestion(suggest, 0, "test", 5, "abc0");
     }
-
+    
     @Test
     public void testUnmappedField() throws IOException, InterruptedException, ExecutionException {
         CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
@@ -287,14 +268,16 @@ public class SuggestSearchIT extends ESIntegTestCase {
 
         phraseSuggestion.field("nosuchField");
         {
-            SearchRequestBuilder searchBuilder = client().prepareSearch().setSize(0);
-            searchBuilder.suggest(new SuggestBuilder().setText("tetsting sugestion").addSuggestion(phraseSuggestion));
-            assertThrows(searchBuilder, SearchPhaseExecutionException.class);
+            SearchRequestBuilder suggestBuilder = client().prepareSearch().setSize(0);
+            suggestBuilder.setSuggestText("tetsting sugestion");
+            suggestBuilder.addSuggestion(phraseSuggestion);
+            assertThrows(suggestBuilder, SearchPhaseExecutionException.class);
         }
         {
-            SearchRequestBuilder searchBuilder = client().prepareSearch().setSize(0);
-            searchBuilder.suggest(new SuggestBuilder().setText("tetsting sugestion").addSuggestion(phraseSuggestion));
-            assertThrows(searchBuilder, SearchPhaseExecutionException.class);
+            SearchRequestBuilder suggestBuilder = client().prepareSearch().setSize(0);
+            suggestBuilder.setSuggestText("tetsting sugestion");
+            suggestBuilder.addSuggestion(phraseSuggestion);
+            assertThrows(suggestBuilder, SearchPhaseExecutionException.class);
         }
     }
 
@@ -308,10 +291,10 @@ public class SuggestSearchIT extends ESIntegTestCase {
         index("test", "type1", "3", "text", "abbd");
         index("test", "type1", "4", "text", "abcc");
         refresh();
-
+        
         SearchResponse search = client().prepareSearch().setQuery(matchQuery("text", "spellcecker")).get();
         assertThat("didn't ask for suggestions but got some", search.getSuggest(), nullValue());
-
+        
         TermSuggestionBuilder termSuggest = termSuggestion("test")
                 .suggestMode("always") // Always, otherwise the results can vary between requests.
                 .text("abcd")
@@ -423,7 +406,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
         // assertThat(suggest.get(3).getSuggestedWords().get("prefix_abcd").get(4).getTerm(), equalTo("prefix_abcc"));
         // assertThat(suggest.get(3).getSuggestedWords().get("prefix_abcd").get(4).getTerm(), equalTo("prefix_accd"));
     }
-
+    
     @Test // see #2817
     public void testStopwordsOnlyPhraseSuggest() throws IOException {
         assertAcked(prepareCreate("test").addMapping("typ1", "body", "type=string,analyzer=stopwd").setSettings(
@@ -441,7 +424,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
                         .size(1));
         assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
     }
-
+    
     @Test
     public void testPrefixLength() throws IOException {  // Stopped here
         CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
@@ -477,14 +460,14 @@ public class SuggestSearchIT extends ESIntegTestCase {
                         .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").prefixLength(4).minWordLength(1).suggestMode("always"))
                         .size(1).confidence(1.0f));
         assertSuggestion(searchSuggest, 0, "simple_phrase", "hello words");
-
+        
         searchSuggest = searchSuggest( "hello word",
                 phraseSuggestion("simple_phrase").field("body")
                         .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").prefixLength(2).minWordLength(1).suggestMode("always"))
                         .size(1).confidence(1.0f));
         assertSuggestion(searchSuggest, 0, "simple_phrase", "hello world");
     }
-
+    
     @Test
     @Nightly
     public void testMarvelHerosPhraseSuggest() throws IOException {
@@ -589,7 +572,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
 
         searchSuggest = searchSuggest( "american ame", phraseSuggest);
         assertSuggestion(searchSuggest, 0, "simple_phrase", "american ace");
-
+        
         // try all smoothing methods
         phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(0.4,0.4,0.2));
         searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
@@ -614,7 +597,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
         // Check the name this time because we're repeating it which is funky
         assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getText().string(), equalTo("Xor the Got-Jewel Xor the Got-Jewel Xor the Got-Jewel"));
     }
-
+    
     @Test
     public void testSizePararm() throws IOException {
         CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
@@ -629,7 +612,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
                 .put("index.analysis.filter.my_shingle.output_unigrams", false)
                 .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
                 .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
-
+        
         XContentBuilder mapping = XContentFactory.jsonBuilder()
                 .startObject()
                     .startObject("type1")
@@ -702,7 +685,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
                 .put("index.analysis.filter.my_shingle2.output_unigrams", true)
                 .put("index.analysis.filter.my_shingle2.min_shingle_size", 2)
                 .put("index.analysis.filter.my_shingle2.max_shingle_size", 2));
-
+        
         XContentBuilder mapping = XContentFactory.jsonBuilder()
                     .startObject().startObject("type1")
                     .startObject("_all").field("store", "yes").field("termVector", "with_positions_offsets").endObject()
@@ -828,16 +811,14 @@ public class SuggestSearchIT extends ESIntegTestCase {
 
         // When searching on a shard with a non existing mapping, we should fail
         SearchRequestBuilder request = client().prepareSearch().setSize(0)
-                .suggest(
-                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
-                                phraseSuggestion("did_you_mean").field("fielddoesnotexist").maxErrors(5.0f)));
+            .setSuggestText("tetsting sugestion")
+            .addSuggestion(phraseSuggestion("did_you_mean").field("fielddoesnotexist").maxErrors(5.0f));
         assertThrows(request, SearchPhaseExecutionException.class);
 
         // When searching on a shard which does not hold yet any document of an existing type, we should not fail
         SearchResponse searchResponse = client().prepareSearch().setSize(0)
-                .suggest(
-                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
-                                phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f)))
+            .setSuggestText("tetsting sugestion")
+            .addSuggestion(phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f))
             .get();
         ElasticsearchAssertions.assertNoFailures(searchResponse);
         ElasticsearchAssertions.assertSuggestion(searchResponse.getSuggest(), 0, 0, "did_you_mean", "testing suggestions");
@@ -879,9 +860,8 @@ public class SuggestSearchIT extends ESIntegTestCase {
 
         SearchResponse searchResponse = client().prepareSearch()
                 .setSize(0)
-                .suggest(
-                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
-                                phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f)))
+                .setSuggestText("tetsting sugestion")
+                .addSuggestion(phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f))
                 .get();
 
         assertNoFailures(searchResponse);
@@ -1272,14 +1252,12 @@ public class SuggestSearchIT extends ESIntegTestCase {
     protected Suggest searchSuggest(String suggestText, int expectShardsFailed, SuggestionBuilder<?>... suggestions) {
         if (randomBoolean()) {
             SearchRequestBuilder builder = client().prepareSearch().setSize(0);
-            SuggestBuilder suggestBuilder = new SuggestBuilder();
             if (suggestText != null) {
-                suggestBuilder.setText(suggestText);
+                builder.setSuggestText(suggestText);
             }
             for (SuggestionBuilder<?> suggestion : suggestions) {
-                suggestBuilder.addSuggestion(suggestion);
+                builder.addSuggestion(suggestion);
             }
-            builder.suggest(suggestBuilder);
             SearchResponse actionGet = builder.execute().actionGet();
             assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(expectShardsFailed));
             return actionGet.getSuggest();
diff --git a/core/src/test/java/org/elasticsearch/search/timeout/SearchTimeoutIT.java b/core/src/test/java/org/elasticsearch/search/timeout/SearchTimeoutIT.java
index 5a16d74..6197224 100644
--- a/core/src/test/java/org/elasticsearch/search/timeout/SearchTimeoutIT.java
+++ b/core/src/test/java/org/elasticsearch/search/timeout/SearchTimeoutIT.java
@@ -21,13 +21,11 @@ package org.elasticsearch.search.timeout;
 
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
-import java.util.concurrent.TimeUnit;
-
+import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.index.query.QueryBuilders.scriptQuery;
 import static org.hamcrest.Matchers.equalTo;
 
@@ -46,7 +44,7 @@ public class SearchTimeoutIT extends ESIntegTestCase {
         client().prepareIndex("test", "type", "1").setSource("field", "value").setRefresh(true).execute().actionGet();
 
         SearchResponse searchResponse = client().prepareSearch("test")
-                .setTimeout(new TimeValue(10, TimeUnit.MILLISECONDS))
+                .setTimeout("10ms")
                 .setQuery(scriptQuery(new Script("Thread.sleep(500); return true;")))
                 .execute().actionGet();
         assertThat(searchResponse.isTimedOut(), equalTo(true));
diff --git a/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java b/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java
index 6249232..60aaa64 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java
@@ -35,7 +35,7 @@ import org.elasticsearch.client.Client;
 import org.elasticsearch.cluster.AbstractDiffable;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ProcessedClusterStateUpdateTask;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.metadata.MetaData.Custom;
 import org.elasticsearch.cluster.metadata.MetaDataIndexStateService;
@@ -50,6 +50,7 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.discovery.zen.ZenDiscovery;
 import org.elasticsearch.discovery.zen.elect.ElectMasterService;
 import org.elasticsearch.index.store.IndexStore;
 import org.elasticsearch.indices.recovery.RecoveryState;
@@ -81,18 +82,8 @@ import java.util.concurrent.atomic.AtomicReference;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.test.ESIntegTestCase.ClusterScope;
 import static org.elasticsearch.test.ESIntegTestCase.Scope;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertBlocked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
-import static org.hamcrest.Matchers.allOf;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.greaterThanOrEqualTo;
-import static org.hamcrest.Matchers.lessThan;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 /**
  */
@@ -110,7 +101,7 @@ public class DedicatedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTest
         logger.info("--> start 2 nodes");
         Settings nodeSettings = settingsBuilder()
                 .put("discovery.type", "zen")
-                .put("discovery.zen.ping_timeout", "200ms")
+                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "200ms")
                 .put("discovery.initial_state_timeout", "500ms")
                 .build();
         internalCluster().startNode(nodeSettings);
@@ -272,7 +263,7 @@ public class DedicatedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTest
     private void updateClusterState(final ClusterStateUpdater updater) throws InterruptedException {
         final CountDownLatch countDownLatch = new CountDownLatch(1);
         final ClusterService clusterService = internalCluster().getInstance(ClusterService.class);
-        clusterService.submitStateUpdateTask("test", new ProcessedClusterStateUpdateTask() {
+        clusterService.submitStateUpdateTask("test", new ClusterStateUpdateTask() {
             @Override
             public ClusterState execute(ClusterState currentState) throws Exception {
                 return updater.execute(currentState);
diff --git a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
index c085e60..206dfee 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
@@ -29,11 +29,7 @@ import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotRes
 import org.elasticsearch.action.admin.cluster.snapshots.delete.DeleteSnapshotResponse;
 import org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsResponse;
 import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;
-import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotIndexShardStage;
-import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotIndexShardStatus;
-import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotIndexStatus;
-import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotStatus;
-import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotsStatusResponse;
+import org.elasticsearch.action.admin.cluster.snapshots.status.*;
 import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;
 import org.elasticsearch.action.admin.indices.flush.FlushResponse;
 import org.elasticsearch.action.admin.indices.settings.get.GetSettingsResponse;
@@ -43,7 +39,7 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ProcessedClusterStateUpdateTask;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.cluster.SnapshotsInProgress;
 import org.elasticsearch.cluster.SnapshotsInProgress.Entry;
 import org.elasticsearch.cluster.SnapshotsInProgress.ShardSnapshotStatus;
@@ -83,24 +79,8 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
 import static org.elasticsearch.index.shard.IndexShard.INDEX_REFRESH_INTERVAL;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAliasesExist;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAliasesMissing;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAllSuccessful;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertBlocked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertIndexTemplateExists;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertIndexTemplateMissing;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
-import static org.hamcrest.Matchers.allOf;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.lessThan;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
-import static org.hamcrest.Matchers.startsWith;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 public class SharedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTestCase {
 
@@ -1839,7 +1819,7 @@ public class SharedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTestCas
 
         logger.info("--> emulate an orphan snapshot");
 
-        clusterService.submitStateUpdateTask("orphan snapshot test", new ProcessedClusterStateUpdateTask() {
+        clusterService.submitStateUpdateTask("orphan snapshot test", new ClusterStateUpdateTask() {
 
             @Override
             public ClusterState execute(ClusterState currentState) {
diff --git a/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java b/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
index 40b9025..14b6526 100644
--- a/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
@@ -101,11 +101,11 @@ import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MappedFieldType.Loading;
 import org.elasticsearch.index.mapper.internal.TimestampFieldMapper;
+import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.MergePolicyConfig;
 import org.elasticsearch.index.shard.MergeSchedulerConfig;
 import org.elasticsearch.index.translog.Translog;
 import org.elasticsearch.index.translog.TranslogConfig;
-import org.elasticsearch.index.translog.TranslogService;
 import org.elasticsearch.index.translog.TranslogWriter;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.indices.cache.request.IndicesRequestCache;
@@ -496,19 +496,13 @@ public abstract class ESIntegTestCase extends ESTestCase {
 
     private static Settings.Builder setRandomIndexTranslogSettings(Random random, Settings.Builder builder) {
         if (random.nextBoolean()) {
-            builder.put(TranslogService.INDEX_TRANSLOG_FLUSH_THRESHOLD_OPS, RandomInts.randomIntBetween(random, 1, 10000));
+            builder.put(IndexShard.INDEX_TRANSLOG_FLUSH_THRESHOLD_OPS, RandomInts.randomIntBetween(random, 1, 10000));
         }
         if (random.nextBoolean()) {
-            builder.put(TranslogService.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE, new ByteSizeValue(RandomInts.randomIntBetween(random, 1, 300), ByteSizeUnit.MB));
+            builder.put(IndexShard.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE, new ByteSizeValue(RandomInts.randomIntBetween(random, 1, 300), ByteSizeUnit.MB));
         }
         if (random.nextBoolean()) {
-            builder.put(TranslogService.INDEX_TRANSLOG_FLUSH_THRESHOLD_PERIOD, TimeValue.timeValueMinutes(RandomInts.randomIntBetween(random, 1, 60)));
-        }
-        if (random.nextBoolean()) {
-            builder.put(TranslogService.INDEX_TRANSLOG_FLUSH_INTERVAL, TimeValue.timeValueMillis(RandomInts.randomIntBetween(random, 1, 10000)));
-        }
-        if (random.nextBoolean()) {
-            builder.put(TranslogService.INDEX_TRANSLOG_DISABLE_FLUSH, random.nextBoolean());
+            builder.put(IndexShard.INDEX_TRANSLOG_DISABLE_FLUSH, random.nextBoolean());
         }
         if (random.nextBoolean()) {
             builder.put(TranslogConfig.INDEX_TRANSLOG_DURABILITY, RandomPicks.randomFrom(random, Translog.Durabilty.values()));
@@ -1444,13 +1438,13 @@ public abstract class ESIntegTestCase extends ESTestCase {
 
     /** Disables translog flushing for the specified index */
     public static void disableTranslogFlush(String index) {
-        Settings settings = Settings.builder().put(TranslogService.INDEX_TRANSLOG_DISABLE_FLUSH, true).build();
+        Settings settings = Settings.builder().put(IndexShard.INDEX_TRANSLOG_DISABLE_FLUSH, true).build();
         client().admin().indices().prepareUpdateSettings(index).setSettings(settings).get();
     }
 
     /** Enables translog flushing for the specified index */
     public static void enableTranslogFlush(String index) {
-        Settings settings = Settings.builder().put(TranslogService.INDEX_TRANSLOG_DISABLE_FLUSH, false).build();
+        Settings settings = Settings.builder().put(IndexShard.INDEX_TRANSLOG_DISABLE_FLUSH, false).build();
         client().admin().indices().prepareUpdateSettings(index).setSettings(settings).get();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/test/cluster/TestClusterService.java b/core/src/test/java/org/elasticsearch/test/cluster/TestClusterService.java
index 162dff7..fa62dd4 100644
--- a/core/src/test/java/org/elasticsearch/test/cluster/TestClusterService.java
+++ b/core/src/test/java/org/elasticsearch/test/cluster/TestClusterService.java
@@ -32,7 +32,6 @@ import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.component.Lifecycle;
 import org.elasticsearch.common.component.LifecycleListener;
 import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.ESLoggerFactory;
 import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.DummyTransportAddress;
@@ -200,8 +199,8 @@ public class TestClusterService implements ClusterService {
             return;
         }
         setStateAndNotifyListeners(newState);
-        if (updateTask instanceof ProcessedClusterStateUpdateTask) {
-            ((ProcessedClusterStateUpdateTask) updateTask).clusterStateProcessed(source, previousClusterState, newState);
+        if (updateTask instanceof ClusterStateUpdateTask) {
+            ((ClusterStateUpdateTask) updateTask).clusterStateProcessed(source, previousClusterState, newState);
         }
         logger.debug("finished [{}]", source);
     }
diff --git a/core/src/test/java/org/elasticsearch/test/disruption/BlockClusterStateProcessing.java b/core/src/test/java/org/elasticsearch/test/disruption/BlockClusterStateProcessing.java
index c8a0820..8154abf 100644
--- a/core/src/test/java/org/elasticsearch/test/disruption/BlockClusterStateProcessing.java
+++ b/core/src/test/java/org/elasticsearch/test/disruption/BlockClusterStateProcessing.java
@@ -20,7 +20,7 @@ package org.elasticsearch.test.disruption;
 
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ClusterStateNonMasterUpdateTask;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.test.InternalTestCluster;
@@ -58,7 +58,12 @@ public class BlockClusterStateProcessing extends SingleNodeDisruption {
         boolean success = disruptionLatch.compareAndSet(null, new CountDownLatch(1));
         assert success : "startDisrupting called without waiting on stopDistrupting to complete";
         final CountDownLatch started = new CountDownLatch(1);
-        clusterService.submitStateUpdateTask("service_disruption_block", Priority.IMMEDIATE, new ClusterStateNonMasterUpdateTask() {
+        clusterService.submitStateUpdateTask("service_disruption_block", Priority.IMMEDIATE, new ClusterStateUpdateTask() {
+
+            @Override
+            public boolean runOnlyOnMaster() {
+                return false;
+            }
 
             @Override
             public ClusterState execute(ClusterState currentState) throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/test/disruption/SlowClusterStateProcessing.java b/core/src/test/java/org/elasticsearch/test/disruption/SlowClusterStateProcessing.java
index 430332a..3c56f83 100644
--- a/core/src/test/java/org/elasticsearch/test/disruption/SlowClusterStateProcessing.java
+++ b/core/src/test/java/org/elasticsearch/test/disruption/SlowClusterStateProcessing.java
@@ -20,7 +20,7 @@ package org.elasticsearch.test.disruption;
 
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ClusterStateNonMasterUpdateTask;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.test.InternalTestCluster;
@@ -102,7 +102,12 @@ public class SlowClusterStateProcessing extends SingleNodeDisruption {
             return false;
         }
         final AtomicBoolean stopped = new AtomicBoolean(false);
-        clusterService.submitStateUpdateTask("service_disruption_delay", Priority.IMMEDIATE, new ClusterStateNonMasterUpdateTask() {
+        clusterService.submitStateUpdateTask("service_disruption_delay", Priority.IMMEDIATE, new ClusterStateUpdateTask() {
+
+            @Override
+            public boolean runOnlyOnMaster() {
+                return false;
+            }
 
             @Override
             public ClusterState execute(ClusterState currentState) throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java b/core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
index 9c24a11..5772543 100644
--- a/core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
+++ b/core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
@@ -53,8 +53,6 @@ import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
@@ -84,7 +82,13 @@ import java.util.function.Function;
 import java.util.function.Predicate;
 import java.util.stream.Collectors;
 
-import static org.apache.lucene.util.LuceneTestCase.random;
+import static org.elasticsearch.test.ESTestCase.assertArrayEquals;
+import static org.elasticsearch.test.ESTestCase.assertEquals;
+import static org.elasticsearch.test.ESTestCase.assertFalse;
+import static org.elasticsearch.test.ESTestCase.assertNotNull;
+import static org.elasticsearch.test.ESTestCase.assertTrue;
+import static org.elasticsearch.test.ESTestCase.fail;
+import static org.elasticsearch.test.ESTestCase.random;
 import static org.elasticsearch.test.VersionUtils.randomVersion;
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.is;
@@ -98,12 +102,6 @@ import static org.hamcrest.Matchers.instanceOf;
 import static org.hamcrest.Matchers.not;
 import static org.hamcrest.Matchers.notNullValue;
 import static org.hamcrest.Matchers.nullValue;
-import static org.junit.Assert.assertArrayEquals;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertTrue;
-import static org.junit.Assert.fail;
 
 /**
  *
@@ -665,10 +663,6 @@ public class ElasticsearchAssertions {
     }
 
     public static void assertVersionSerializable(Version version, Streamable streamable) {
-        assertVersionSerializable(version, streamable, null);
-    }
-
-    public static void assertVersionSerializable(Version version, Streamable streamable, NamedWriteableRegistry namedWriteableRegistry) {
         try {
             Streamable newInstance = tryCreateNewInstance(streamable);
             if (newInstance == null) {
@@ -680,15 +674,10 @@ public class ElasticsearchAssertions {
             }
             BytesReference orig = serialize(version, streamable);
             StreamInput input = StreamInput.wrap(orig);
-            if (namedWriteableRegistry != null) {
-                input = new NamedWriteableAwareStreamInput(input, namedWriteableRegistry);
-            }
             input.setVersion(version);
             newInstance.readFrom(input);
-            assertThat("Stream should be fully read with version [" + version + "] for streamable [" + streamable + "]", input.available(),
-                    equalTo(0));
-            assertThat("Serialization failed with version [" + version + "] bytes should be equal for streamable [" + streamable + "]",
-                    serialize(version, streamable), equalTo(orig));
+            assertThat("Stream should be fully read with version [" + version + "] for streamable [" + streamable + "]", input.available(), equalTo(0));
+            assertThat("Serialization failed with version [" + version + "] bytes should be equal for streamable [" + streamable + "]", serialize(version, streamable), equalTo(orig));
         } catch (Throwable ex) {
             throw new RuntimeException("failed to check serialization - version [" + version + "] for streamable [" + streamable + "]", ex);
         }
diff --git a/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java b/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
index 64cc401..c253a75 100644
--- a/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
+++ b/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
@@ -77,15 +77,13 @@ public class AssertingLocalTransport extends LocalTransport {
 
     @Override
     protected void handleParsedResponse(final TransportResponse response, final TransportResponseHandler handler) {
-        ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), response,
-                namedWriteableRegistry);
+        ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), response);
         super.handleParsedResponse(response, handler);
     }
 
     @Override
     public void sendRequest(final DiscoveryNode node, final long requestId, final String action, final TransportRequest request, TransportRequestOptions options) throws IOException, TransportException {
-        ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), request,
-                namedWriteableRegistry);
+        ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), request);
         super.sendRequest(node, requestId, action, request, options);
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java b/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
index 6beb5c4..abd0a06 100644
--- a/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
+++ b/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
@@ -45,7 +45,6 @@ import org.elasticsearch.client.FilterClient;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.Module;
-import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
@@ -56,7 +55,6 @@ import org.elasticsearch.index.query.MoreLikeThisQueryBuilder;
 import org.elasticsearch.index.query.MoreLikeThisQueryBuilder.Item;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.TermsQueryBuilder;
-import org.elasticsearch.index.query.functionscore.script.ScriptScoreFunctionBuilder;
 import org.elasticsearch.indices.cache.query.terms.TermsLookup;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.rest.RestController;
@@ -68,7 +66,6 @@ import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.aggregations.AggregationBuilders;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders;
 import org.elasticsearch.search.suggest.Suggest;
-import org.elasticsearch.search.suggest.SuggestBuilder;
 import org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
@@ -276,12 +273,14 @@ public class ContextAndHeaderTransportIT extends ESIntegTestCase {
                 .get();
         transportClient().admin().indices().prepareRefresh(queryIndex).get();
 
+        // custom content, not sure how to specify "script_id" otherwise in the API
+        XContentBuilder builder = jsonBuilder().startObject().startObject("function_score").field("boost_mode", "replace").startArray("functions")
+                .startObject().startObject("script_score").field("script_id", "my_script").field("lang", "groovy").endObject().endObject().endArray().endObject().endObject();
+
         SearchResponse searchResponse = transportClient()
                 .prepareSearch(queryIndex)
-                .setQuery(
-                        QueryBuilders.functionScoreQuery(
-                                new ScriptScoreFunctionBuilder(new Script("my_script", ScriptType.INDEXED, "groovy", null))).boostMode(
-                                CombineFunction.REPLACE)).get();
+                .setQuery(builder)
+                .get();
         assertNoFailures(searchResponse);
         assertHitCount(searchResponse, 1);
         assertThat(searchResponse.getHits().getMaxScore(), is(10.0f));
@@ -443,13 +442,11 @@ public class ContextAndHeaderTransportIT extends ESIntegTestCase {
                 MustacheScriptEngineService.NAME, null, null));
 
         SearchRequestBuilder searchRequestBuilder = transportClient().prepareSearch("test").setSize(0);
-        SuggestBuilder suggestBuilder = new SuggestBuilder();
         String suggestText = "united states house of representatives elections in washington 2006";
         if (suggestText != null) {
-            suggestBuilder.setText(suggestText);
+            searchRequestBuilder.setSuggestText(suggestText);
         }
-        suggestBuilder.addSuggestion(filteredFilterSuggest);
-        searchRequestBuilder.suggest(suggestBuilder);
+        searchRequestBuilder.addSuggestion(filteredFilterSuggest);
         SearchResponse actionGet = searchRequestBuilder.execute().actionGet();
         assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(0));
         Suggest searchSuggest = actionGet.getSuggest();
diff --git a/core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java b/core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java
index 3003a7b..44039ce 100644
--- a/core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java
+++ b/core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java
@@ -70,7 +70,7 @@ public class NettySizeHeaderFrameDecoderTests extends ESTestCase {
         TransportService transportService = new TransportService(nettyTransport, threadPool);
         nettyTransport.transportServiceAdapter(transportService.createAdapter());
 
-        InetSocketTransportAddress transportAddress = (InetSocketTransportAddress) nettyTransport.boundAddress().boundAddress();
+        InetSocketTransportAddress transportAddress = (InetSocketTransportAddress) randomFrom(nettyTransport.boundAddress().boundAddresses());
         port = transportAddress.address().getPort();
         host = transportAddress.address().getAddress();
 
diff --git a/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortIntegrationIT.java b/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortIntegrationIT.java
index 48c0215..9a64861 100644
--- a/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortIntegrationIT.java
@@ -26,10 +26,9 @@ import org.elasticsearch.client.transport.TransportClient;
 import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.InetSocketTransportAddress;
+import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.junit.annotations.Network;
-import org.elasticsearch.test.transport.AssertingLocalTransport;
-import org.elasticsearch.test.transport.MockTransportService;
 import org.elasticsearch.transport.TransportModule;
 import org.junit.Test;
 
@@ -86,11 +85,15 @@ public class NettyTransportMultiPortIntegrationIT extends ESIntegTestCase {
         for (NodeInfo nodeInfo : response.getNodes()) {
             assertThat(nodeInfo.getTransport().getProfileAddresses().keySet(), hasSize(1));
             assertThat(nodeInfo.getTransport().getProfileAddresses(), hasKey("client1"));
-            assertThat(nodeInfo.getTransport().getProfileAddresses().get("client1").boundAddress(), instanceOf(InetSocketTransportAddress.class));
+            for (TransportAddress transportAddress : nodeInfo.getTransport().getProfileAddresses().get("client1").boundAddresses()) {
+                assertThat(transportAddress, instanceOf(InetSocketTransportAddress.class));
+            }
 
-            // bound address
-            InetSocketTransportAddress inetSocketTransportAddress = (InetSocketTransportAddress) nodeInfo.getTransport().getProfileAddresses().get("client1").boundAddress();
-            assertThat(inetSocketTransportAddress.address().getPort(), is(allOf(greaterThanOrEqualTo(randomPort), lessThanOrEqualTo(randomPort + 10))));
+            // bound addresses
+            for (TransportAddress transportAddress : nodeInfo.getTransport().getProfileAddresses().get("client1").boundAddresses()) {
+                assertThat(transportAddress, instanceOf(InetSocketTransportAddress.class));
+                assertThat(((InetSocketTransportAddress) transportAddress).address().getPort(), is(allOf(greaterThanOrEqualTo(randomPort), lessThanOrEqualTo(randomPort + 10))));
+            }
 
             // publish address
             assertThat(nodeInfo.getTransport().getProfileAddresses().get("client1").publishAddress(), instanceOf(InetSocketTransportAddress.class));
diff --git a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch1.json b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch1.json
index eefec53..3d98f37 100644
--- a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch1.json
+++ b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch1.json
@@ -1,16 +1,16 @@
 {"index":"test", "ignore_unavailable" : true, "expand_wildcards" : "open,closed"}}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {"index" : "test", "type" : "type1", "expand_wildcards" : ["open", "closed"]}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {"index":"test", "ignore_unavailable" : false, "expand_wildcards" : ["open"]}}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {"index":"test", "ignore_unavailable" : true, "allow_no_indices": true, "expand_wildcards" : ["open", "closed"]}}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {"index":"test", "ignore_unavailable" : true, "allow_no_indices": false, "expand_wildcards" : ["closed"]}}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {"search_type" : "dfs_query_then_fetch"}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
diff --git a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch2.json b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch2.json
index 79330d8..e2e06d9 100644
--- a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch2.json
+++ b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch2.json
@@ -1,10 +1,10 @@
 {"index":"test"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"index" : "test", "type" : "type1"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"search_type" : "dfs_query_then_fetch"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
diff --git a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch3.json b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch3.json
index a6b52fd..6416720 100644
--- a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch3.json
+++ b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch3.json
@@ -1,8 +1,8 @@
 {"index":["test0", "test1"]}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"index" : "test2,test3", "type" : "type1"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"index" : ["test4", "test1"], "type" :  [ "type2", "type1" ]}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"search_type" : "dfs_query_then_fetch"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
diff --git a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch4.json b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch4.json
index 844d8be..b98e24b 100644
--- a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch4.json
+++ b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch4.json
@@ -1,6 +1,6 @@
 {"index":["test0", "test1"], "request_cache": true}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"index" : "test2,test3", "type" : "type1", "preference": "_local"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"index" : ["test4", "test1"], "type" :  [ "type2", "type1" ], "routing": "123"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
diff --git a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch5.json b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch5.json
index b337eae..5f08919 100644
--- a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch5.json
+++ b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch5.json
@@ -1,6 +1,6 @@
 {"index":["test0", "test1"], "request_cache": true}
-{"template": {"query" : {"match_{{template}}" :{}}}, "params": {"template": "all" } } }
+{"template": {"query" : {"match_{{template}}" {}}}, "params": {"template": "all" } } }
 {"index" : "test2,test3", "type" : "type1", "preference": "_local"}
-{"template": {"query" : {"match_{{template}}" :{}}}, "params": {"template": "all" } } }
+{"template": {"query" : {"match_{{template}}" {}}}, "params": {"template": "all" } } }
 {"index" : ["test4", "test1"], "type" :  [ "type2", "type1" ], "routing": "123"}
-{"template": {"query" : {"match_{{template}}" :{}}}, "params": {"template": "all" } } }
+{"template": {"query" : {"match_{{template}}" {}}}, "params": {"template": "all" } } }
diff --git a/dev-tools/smoke_test_rc.py b/dev-tools/smoke_test_rc.py
index 8e7cf3a..c0bd3cf 100644
--- a/dev-tools/smoke_test_rc.py
+++ b/dev-tools/smoke_test_rc.py
@@ -67,6 +67,7 @@ DEFAULT_PLUGINS = ["analysis-icu",
                    "discovery-azure",
                    "discovery-ec2",
                    "discovery-multicast",
+                   "lang-expression",
                    "lang-javascript",
                    "lang-python",
                    "mapper-murmur3",
diff --git a/dev-tools/src/main/resources/forbidden/all-signatures.txt b/dev-tools/src/main/resources/forbidden/all-signatures.txt
index 8b976b5..60a07c0 100644
--- a/dev-tools/src/main/resources/forbidden/all-signatures.txt
+++ b/dev-tools/src/main/resources/forbidden/all-signatures.txt
@@ -42,8 +42,8 @@ org.apache.lucene.search.TermRangeFilter
 org.apache.lucene.search.NumericRangeFilter
 org.apache.lucene.search.PrefixFilter
 
-java.nio.file.Paths @ Use PathUtils.get instead.
-java.nio.file.FileSystems#getDefault() @ use PathUtils.getDefault instead.
+java.nio.file.Paths @ Use org.elasticsearch.common.io.PathUtils.get() instead.
+java.nio.file.FileSystems#getDefault() @ use org.elasticsearch.common.io.PathUtils.getDefaultFileSystem() instead.
 
 @defaultMessage Specify a location for the temp file/directory instead.
 java.nio.file.Files#createTempDirectory(java.lang.String,java.nio.file.attribute.FileAttribute[])
@@ -57,8 +57,8 @@ java.io.ObjectInput
 
 java.nio.file.Files#isHidden(java.nio.file.Path) @ Dependent on the operating system, use FileSystemUtils.isHidden instead
 
-java.nio.file.Files#getFileStore(java.nio.file.Path) @ Use Environment.getFileStore() instead, impacted by JDK-8034057
-java.nio.file.Files#isWritable(java.nio.file.Path) @ Use Environment.isWritable() instead, impacted by JDK-8034057
+java.nio.file.Files#getFileStore(java.nio.file.Path) @ Use org.elasticsearch.env.Environment.getFileStore() instead, impacted by JDK-8034057
+java.nio.file.Files#isWritable(java.nio.file.Path) @ Use org.elasticsearch.env.Environment.isWritable() instead, impacted by JDK-8034057
 
 @defaultMessage Resolve hosts explicitly to the address(es) you want with InetAddress.
 java.net.InetSocketAddress#<init>(java.lang.String,int)
@@ -96,6 +96,7 @@ com.google.common.base.Predicates
 com.google.common.base.Strings
 com.google.common.base.Throwables
 com.google.common.collect.Maps
+com.google.common.collect.ForwardingSet
 com.google.common.collect.Sets
 com.google.common.base.Preconditions
 com.google.common.collect.ImmutableSortedSet
@@ -126,6 +127,7 @@ com.google.common.collect.HashMultimap
 com.google.common.collect.FluentIterable
 com.google.common.io.Files
 com.google.common.primitives.Ints
+com.google.common.collect.ImmutableMap#entrySet()
 
 @defaultMessage Do not violate java's access system
 java.lang.reflect.AccessibleObject#setAccessible(boolean)
diff --git a/dev-tools/update_lucene.sh b/dev-tools/update_lucene.sh
index 4918c12..a3f8b08 100644
--- a/dev-tools/update_lucene.sh
+++ b/dev-tools/update_lucene.sh
@@ -12,3 +12,5 @@ perl dev-tools/src/main/resources/license-check/check_license_and_sha.pl \
      --update plugins/analysis-smartcn/licenses/ plugins/analysis-smartcn/target/releases/analysis-smartcn-3.0.0-SNAPSHOT.zip analysis-smartcn-3.0.0-SNAPSHOT
 perl dev-tools/src/main/resources/license-check/check_license_and_sha.pl \
      --update plugins/analysis-stempel/licenses/ plugins/analysis-stempel/target/releases/analysis-stempel-3.0.0-SNAPSHOT.zip analysis-stempel-3.0.0-SNAPSHOT
+perl dev-tools/src/main/resources/license-check/check_license_and_sha.pl \
+     --update plugins/lang-expression/licenses/ plugins/lang-expression/target/releases/lang-expression-3.0.0-SNAPSHOT.zip lang-expression-3.0.0-SNAPSHOT
diff --git a/distribution/deb/src/main/packaging/init.d/elasticsearch b/distribution/deb/src/main/packaging/init.d/elasticsearch
index 7bbb69d..9ea2beb 100755
--- a/distribution/deb/src/main/packaging/init.d/elasticsearch
+++ b/distribution/deb/src/main/packaging/init.d/elasticsearch
@@ -43,17 +43,6 @@ fi
 ES_USER=elasticsearch
 ES_GROUP=elasticsearch
 
-# The first existing directory is used for JAVA_HOME (if JAVA_HOME is not defined in $DEFAULT)
-JDK_DIRS="/usr/lib/jvm/java-8-oracle/ /usr/lib/jvm/j2sdk1.8-oracle/ /usr/lib/jvm/jdk-7-oracle-x64 /usr/lib/jvm/java-7-oracle /usr/lib/jvm/j2sdk1.7-oracle/ /usr/lib/jvm/java-7-openjdk /usr/lib/jvm/java-7-openjdk-amd64/ /usr/lib/jvm/java-7-openjdk-armhf /usr/lib/jvm/java-7-openjdk-i386/ /usr/lib/jvm/default-java"
-
-# Look for the right JVM to use
-for jdir in $JDK_DIRS; do
-	if [ -r "$jdir/bin/java" -a -z "${JAVA_HOME}" ]; then
-		JAVA_HOME="$jdir"
-	fi
-done
-export JAVA_HOME
-
 # Directory where the Elasticsearch binary distribution resides
 ES_HOME=/usr/share/$NAME
 
@@ -114,6 +103,7 @@ export ES_HEAP_NEWSIZE
 export ES_DIRECT_SIZE
 export ES_JAVA_OPTS
 export ES_GC_LOG_FILE
+export JAVA_HOME
 
 # Check DAEMON exists
 test -x $DAEMON || exit 0
diff --git a/distribution/licenses/antlr4-runtime-4.5.jar.sha1 b/distribution/licenses/antlr4-runtime-4.5.jar.sha1
deleted file mode 100644
index 5299c19..0000000
--- a/distribution/licenses/antlr4-runtime-4.5.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-29e48af049f17dd89153b83a7ad5d01b3b4bcdda
diff --git a/distribution/licenses/antlr4-runtime-LICENSE.txt b/distribution/licenses/antlr4-runtime-LICENSE.txt
deleted file mode 100644
index 95d0a25..0000000
--- a/distribution/licenses/antlr4-runtime-LICENSE.txt
+++ /dev/null
@@ -1,26 +0,0 @@
-[The "BSD license"]
-Copyright (c) 2015 Terence Parr, Sam Harwell
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions
-are met:
-
- 1. Redistributions of source code must retain the above copyright
-    notice, this list of conditions and the following disclaimer.
- 2. Redistributions in binary form must reproduce the above copyright
-    notice, this list of conditions and the following disclaimer in the
-    documentation and/or other materials provided with the distribution.
- 3. The name of the author may not be used to endorse or promote products
-    derived from this software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
-IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
-IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
-INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
-NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
-DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
-THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
-THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
diff --git a/distribution/licenses/antlr4-runtime-NOTICE.txt b/distribution/licenses/antlr4-runtime-NOTICE.txt
deleted file mode 100644
index e69de29..0000000
diff --git a/distribution/licenses/asm-5.0.4.jar.sha1 b/distribution/licenses/asm-5.0.4.jar.sha1
deleted file mode 100644
index 9223dba..0000000
--- a/distribution/licenses/asm-5.0.4.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-0da08b8cce7bbf903602a25a3a163ae252435795
diff --git a/distribution/licenses/asm-LICENSE.txt b/distribution/licenses/asm-LICENSE.txt
deleted file mode 100644
index afb064f..0000000
--- a/distribution/licenses/asm-LICENSE.txt
+++ /dev/null
@@ -1,26 +0,0 @@
-Copyright (c) 2012 France Tlcom
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions
-are met:
-1. Redistributions of source code must retain the above copyright
-   notice, this list of conditions and the following disclaimer.
-2. Redistributions in binary form must reproduce the above copyright
-   notice, this list of conditions and the following disclaimer in the
-   documentation and/or other materials provided with the distribution.
-3. Neither the name of the copyright holders nor the names of its
-   contributors may be used to endorse or promote products derived from
-   this software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
-AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
-IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
-ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
-LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
-CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
-SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
-INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
-CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
-ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
-THE POSSIBILITY OF SUCH DAMAGE.
diff --git a/distribution/licenses/asm-NOTICE.txt b/distribution/licenses/asm-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/distribution/licenses/asm-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/distribution/licenses/asm-commons-5.0.4.jar.sha1 b/distribution/licenses/asm-commons-5.0.4.jar.sha1
deleted file mode 100644
index 94fe0cd..0000000
--- a/distribution/licenses/asm-commons-5.0.4.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-5a556786086c23cd689a0328f8519db93821c04c
diff --git a/distribution/licenses/asm-commons-LICENSE.txt b/distribution/licenses/asm-commons-LICENSE.txt
deleted file mode 100644
index afb064f..0000000
--- a/distribution/licenses/asm-commons-LICENSE.txt
+++ /dev/null
@@ -1,26 +0,0 @@
-Copyright (c) 2012 France Tlcom
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions
-are met:
-1. Redistributions of source code must retain the above copyright
-   notice, this list of conditions and the following disclaimer.
-2. Redistributions in binary form must reproduce the above copyright
-   notice, this list of conditions and the following disclaimer in the
-   documentation and/or other materials provided with the distribution.
-3. Neither the name of the copyright holders nor the names of its
-   contributors may be used to endorse or promote products derived from
-   this software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
-AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
-IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
-ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
-LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
-CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
-SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
-INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
-CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
-ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
-THE POSSIBILITY OF SUCH DAMAGE.
diff --git a/distribution/licenses/asm-commons-NOTICE.txt b/distribution/licenses/asm-commons-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/distribution/licenses/asm-commons-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/distribution/licenses/lucene-expressions-5.4.0-snapshot-1702855.jar.sha1 b/distribution/licenses/lucene-expressions-5.4.0-snapshot-1702855.jar.sha1
deleted file mode 100644
index d04bca5..0000000
--- a/distribution/licenses/lucene-expressions-5.4.0-snapshot-1702855.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-ecfb9a923b19fac61b4e9a79275f6bd242b1f091
diff --git a/distribution/pom.xml b/distribution/pom.xml
index 44c59b8..48b4e40 100644
--- a/distribution/pom.xml
+++ b/distribution/pom.xml
@@ -77,11 +77,6 @@
         </dependency>
 
         <dependency>
-            <groupId>org.apache.lucene</groupId>
-            <artifactId>lucene-expressions</artifactId>
-        </dependency>
-
-        <dependency>
             <groupId>com.spatial4j</groupId>
             <artifactId>spatial4j</artifactId>
         </dependency>
diff --git a/docs/plugins/cloud-gce.asciidoc b/docs/plugins/cloud-gce.asciidoc
index 2461888..6d712ad 100644
--- a/docs/plugins/cloud-gce.asciidoc
+++ b/docs/plugins/cloud-gce.asciidoc
@@ -143,7 +143,7 @@ sudo apt-get update
 wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-2.0.0.deb
 
 # Prepare Java installation
-sudo apt-get install java7-runtime-headless
+sudo apt-get install java8-runtime-headless
 
 # Prepare Elasticsearch installation
 sudo dpkg -i elasticsearch-2.0.0.deb
@@ -412,7 +412,7 @@ Set the service account scopes when creating the machine:
 --------------------------------------------------
 knife google server create www1 \
     -m n1-standard-1 \
-    -I debian-7-wheezy-v20131120 \
+    -I debian-8 \
     -Z us-central1-a \
     -i ~/.ssh/id_rsa \
     -x jdoe \
diff --git a/docs/plugins/discovery-azure.asciidoc b/docs/plugins/discovery-azure.asciidoc
index 05b195a..cedfca7 100644
--- a/docs/plugins/discovery-azure.asciidoc
+++ b/docs/plugins/discovery-azure.asciidoc
@@ -353,14 +353,14 @@ Once connected, install Elasticsearch:
 [source,sh]
 ----
 # Install Latest Java version
-# Read http://www.webupd8.org/2012/01/install-oracle-java-jdk-7-in-ubuntu-via.html for details
+# Read http://www.webupd8.org/2012/09/install-oracle-java-8-in-ubuntu-via-ppa.html for details
 sudo add-apt-repository ppa:webupd8team/java
 sudo apt-get update
-sudo apt-get install oracle-java7-installer
+sudo apt-get install oracle-java8-installer
 
 # If you want to install OpenJDK instead
 # sudo apt-get update
-# sudo apt-get install openjdk-7-jre-headless
+# sudo apt-get install openjdk-8-jre-headless
 
 # Download Elasticsearch
 curl -s https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-2.0.0.deb -o elasticsearch-2.0.0.deb
diff --git a/docs/plugins/discovery-ec2.asciidoc b/docs/plugins/discovery-ec2.asciidoc
index d870283..fa1ac07 100644
--- a/docs/plugins/discovery-ec2.asciidoc
+++ b/docs/plugins/discovery-ec2.asciidoc
@@ -128,8 +128,10 @@ discovery:
     type: ec2
 ----
 
+You must also set `cloud.aws.region` if you are not using default AWS region. See <<discovery-ec2-usage-region>> for details.
+
 The ec2 discovery is using the same credentials as the rest of the AWS services provided by this plugin (`repositories`).
-See <<discovery-ec2-usage>> for details.
+See <<discovery-ec2-usage>> for details. 
 
 The following are a list of settings (prefixed with `discovery.ec2`) that can further control the discovery:
 
diff --git a/docs/plugins/plugin-script.asciidoc b/docs/plugins/plugin-script.asciidoc
index fc1c91c..bae6613 100644
--- a/docs/plugins/plugin-script.asciidoc
+++ b/docs/plugins/plugin-script.asciidoc
@@ -13,6 +13,15 @@ Run the following command to get usage instructions:
 sudo bin/plugin -h
 -----------------------------------
 
+[IMPORTANT]
+.Running as root
+=====================
+If Elasticsearch was installed using the deb or rpm package then run
+`bin/plugin` as `root` so it can write to the appropriate files on disk.
+Otherwise run `bin/plugin` as the user that owns all of the Elasticsearch
+files.
+=====================
+
 [[installation]]
 === Installing Plugins
 
@@ -222,4 +231,3 @@ plugin.mandatory: mapper-attachments,lang-groovy
 --------------------------------------------------
 
 For safety reasons, a node will not start if it is missing a mandatory plugin.
-
diff --git a/docs/reference/getting-started.asciidoc b/docs/reference/getting-started.asciidoc
index 31f386c..2731fca 100755
--- a/docs/reference/getting-started.asciidoc
+++ b/docs/reference/getting-started.asciidoc
@@ -101,7 +101,7 @@ With that out of the way, let's get started with the fun part...
 
 == Installation
 
-Elasticsearch requires at least Java 7. Specifically as of this writing, it is recommended that you use the Oracle JDK version {jdk}. Java installation varies from platform to platform so we won't go into those details here. Oracle's recommended installation documentation can be found on http://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html[Oracle's website]. Suffice to say, before you install Elasticsearch, please check your Java version first by running (and then install/upgrade accordingly if needed):
+Elasticsearch requires at least Java 8. Specifically as of this writing, it is recommended that you use the Oracle JDK version {jdk}. Java installation varies from platform to platform so we won't go into those details here. Oracle's recommended installation documentation can be found on http://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html[Oracle's website]. Suffice to say, before you install Elasticsearch, please check your Java version first by running (and then install/upgrade accordingly if needed):
 
 [source,sh]
 --------------------------------------------------
diff --git a/docs/reference/index-modules/translog.asciidoc b/docs/reference/index-modules/translog.asciidoc
index 31cc4f7..5cb2e4b 100644
--- a/docs/reference/index-modules/translog.asciidoc
+++ b/docs/reference/index-modules/translog.asciidoc
@@ -35,16 +35,6 @@ Once the translog hits this size, a flush will happen. Defaults to `512mb`.
 
 After how many operations to flush. Defaults to `unlimited`.
 
-`index.translog.flush_threshold_period`::
-
-How long to wait before triggering a flush regardless of translog size. Defaults to `30m`.
-
-`index.translog.interval`::
-
-How often to check if a flush is needed, randomized between the interval value
-and 2x the interval value. Defaults to `5s`.
-
-
 [float]
 === Translog settings
 
diff --git a/docs/reference/mapping/params/position-increment-gap.asciidoc b/docs/reference/mapping/params/position-increment-gap.asciidoc
index 16a963c..962e217 100644
--- a/docs/reference/mapping/params/position-increment-gap.asciidoc
+++ b/docs/reference/mapping/params/position-increment-gap.asciidoc
@@ -22,7 +22,9 @@ GET /my_index/groups/_search
 {
     "query": {
         "match_phrase": {
-            "names": "Abraham Lincoln" <1>
+            "names": {
+                "query": "Abraham Lincoln" <1>
+            }
         }
     }
 }
@@ -31,8 +33,10 @@ GET /my_index/groups/_search
 {
     "query": {
         "match_phrase": {
-            "names": "Abraham Lincoln",
-            "slop": 101 <2>
+            "names": {
+                "query": "Abraham Lincoln",
+                "slop": 101 <2>
+            }
         }
     }
 }
@@ -50,7 +54,7 @@ The `position_increment_gap` can be specified in the mapping.  For instance:
 PUT my_index
 {
   "mappings": {
-    "my_type": {
+    "groups": {
       "properties": {
         "names": {
           "type": "string",
diff --git a/docs/reference/migration/migrate_2_0/settings.asciidoc b/docs/reference/migration/migrate_2_0/settings.asciidoc
index f5898a4..17d36c5 100644
--- a/docs/reference/migration/migrate_2_0/settings.asciidoc
+++ b/docs/reference/migration/migrate_2_0/settings.asciidoc
@@ -1,5 +1,24 @@
 === Setting changes
 
+==== Command line flags
+
+Command line flags using single dash notation must be now specified as the first arguments.
+For example if previously using: 
+
+[source,sh]
+---------------
+./elasticsearch --node.name=test_node -Des.path.conf=/opt/elasticsearch/conf/test_node
+---------------
+
+This will now need to be changed to:
+
+[source,sh]
+---------------
+./elasticsearch -Des.path.conf=/opt/elasticsearch/conf/test_node --node.name=test_node 
+---------------
+
+for the flag to take effect.
+
 [[migration-script-settings]]
 ==== Scripting settings
 
diff --git a/docs/reference/migration/migrate_3_0.asciidoc b/docs/reference/migration/migrate_3_0.asciidoc
index 1d61fca..5654d8d 100644
--- a/docs/reference/migration/migrate_3_0.asciidoc
+++ b/docs/reference/migration/migrate_3_0.asciidoc
@@ -87,4 +87,13 @@ previous versions.
 
 When `max_children` was set to `0` on the `has_child` query then there was no upper limit on how many children documents
 are allowed to match. This has changed and `0` now really means to zero child documents are allowed. If no upper limit
-is needed then the `max_children` option shouldn't be defined at all on the `has_child` query.
\ No newline at end of file
+is needed then the `max_children` option shouldn't be defined at all on the `has_child` query.
+
+=== Settings changes ===
+
+==== Ping timeout settings
+
+Previously, there were three settings for the ping timeout: `discovery.zen.initial_ping_timeout`,
+`discovery.zen.ping.timeout` and `discovery.zen.ping_timeout`. The former two have been removed and
+the only setting key for the ping timeout is now `discovery.zen.ping_timeout`. The default value for
+ping timeouts remains at three seconds.
diff --git a/docs/reference/search/suggesters/phrase-suggest.asciidoc b/docs/reference/search/suggesters/phrase-suggest.asciidoc
index 9f33d04..bc2f016 100644
--- a/docs/reference/search/suggesters/phrase-suggest.asciidoc
+++ b/docs/reference/search/suggesters/phrase-suggest.asciidoc
@@ -122,7 +122,7 @@ can contain misspellings (See parameter descriptions below).
     the maximum percentage of the terms that at most
     considered to be misspellings in order to form a correction. This method
     accepts a float value in the range `[0..1)` as a fraction of the actual
-    query terms a number `>=1` as an absolute number of query terms. The
+    query terms or a number `>=1` as an absolute number of query terms. The
     default is set to `1.0` which corresponds to that only corrections with
     at most 1 misspelled term are returned.  Note that setting this too high
     can negatively impact performance. Low values like `1` or `2` are recommended
diff --git a/docs/reference/setup.asciidoc b/docs/reference/setup.asciidoc
index 38812b1..9d14e62 100644
--- a/docs/reference/setup.asciidoc
+++ b/docs/reference/setup.asciidoc
@@ -74,14 +74,12 @@ $ bin/elasticsearch -Des.index.refresh_interval=5s --node.name=my-node
 == Java (JVM) version
 
 Elasticsearch is built using Java, and requires at least
-http://www.oracle.com/technetwork/java/javase/downloads/index.html[Java 7] in
+http://www.oracle.com/technetwork/java/javase/downloads/index.html[Java 8] in
 order to run. Only Oracle's Java and the OpenJDK are supported. The same JVM
 version should be used on all Elasticsearch nodes and clients.
 
-We recommend installing the *Java 8 update 20 or later*, or *Java 7 update 55
-or later*.  Previous versions of Java 7 are known to have bugs that can cause
-index corruption and data loss.  Elasticsearch will refuse to start if a
-known-bad version of Java is used.
+We recommend installing the *Java 8 update 20 or later*.
+Elasticsearch will refuse to start if a known-bad version of Java is used.
 
 The version of Java to use can be configured by setting the `JAVA_HOME`
 environment variable.
diff --git a/docs/reference/setup/as-a-service-win.asciidoc b/docs/reference/setup/as-a-service-win.asciidoc
index 11016aa..157f20c 100644
--- a/docs/reference/setup/as-a-service-win.asciidoc
+++ b/docs/reference/setup/as-a-service-win.asciidoc
@@ -42,7 +42,7 @@ service will be installed. This information is made available during install:
 --------------------------------------------------
 c:\elasticsearch-{version}\bin>service install
 Installing service      :  "elasticsearch-service-x64"
-Using JAVA_HOME (64-bit):  "c:\jvm\jdk1.7"
+Using JAVA_HOME (64-bit):  "c:\jvm\jdk1.8"
 The service 'elasticsearch-service-x64' has been installed.
 --------------------------------------------------
 
diff --git a/docs/reference/setup/as-a-service.asciidoc b/docs/reference/setup/as-a-service.asciidoc
index f603a13..01bbd2d 100644
--- a/docs/reference/setup/as-a-service.asciidoc
+++ b/docs/reference/setup/as-a-service.asciidoc
@@ -43,17 +43,17 @@ sudo /etc/init.d/elasticsearch start
 [float]
 ===== Installing the oracle JDK
 
-The usual recommendation is to run the Oracle JDK with elasticsearch. However Ubuntu and Debian only ship the OpenJDK due to license issues. You can easily install the oracle installer package though. In case you are missing the `add-apt-repository` command under Debian GNU/Linux, make sure have at least Debian Wheezy and the package `python-software-properties` installed
+The usual recommendation is to run the Oracle JDK with elasticsearch. However Ubuntu and Debian only ship the OpenJDK due to license issues. You can easily install the oracle installer package though. In case you are missing the `add-apt-repository` command under Debian GNU/Linux, make sure have at least Debian Jessie and the package `python-software-properties` installed
 
 [source,sh]
 --------------------------------------------------
 sudo add-apt-repository ppa:webupd8team/java
 sudo apt-get update
-sudo apt-get install oracle-java7-installer
+sudo apt-get install oracle-java8-installer
 java -version
 --------------------------------------------------
 
-The last command should verify a successful installation of the Oracle JDK. If you want to install java8, you can call `apt-get install oracle-java8-installer`
+The last command should verify a successful installation of the Oracle JDK.
 
 
 [float]
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java
index 86d224b..4c29e7c 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java
@@ -19,20 +19,27 @@
 
 package org.elasticsearch.action.deletebyquery;
 
+import org.elasticsearch.ElasticsearchGenerationException;
 import org.elasticsearch.action.ActionRequest;
 import org.elasticsearch.action.ActionRequestValidationException;
 import org.elasticsearch.action.IndicesRequest;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
+import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.Scroll;
 
 import java.io.IOException;
 import java.util.Arrays;
+import java.util.Map;
 
 import static org.elasticsearch.action.ValidateActions.addValidationError;
 import static org.elasticsearch.search.Scroll.readScroll;
@@ -70,7 +77,7 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
 
     private String[] types = Strings.EMPTY_ARRAY;
 
-    private QueryBuilder<?> query;
+    private BytesReference source;
 
     private String routing;
 
@@ -94,7 +101,7 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
     @Override
     public ActionRequestValidationException validate() {
         ActionRequestValidationException validationException = null;
-        if (query == null) {
+        if (source == null) {
             validationException = addValidationError("source is missing", validationException);
         }
         return validationException;
@@ -133,12 +140,45 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
         return this;
     }
 
-    public QueryBuilder<?> query() {
-        return query;
+    public BytesReference source() {
+        return source;
     }
 
-    public DeleteByQueryRequest query(QueryBuilder<?> queryBuilder) {
-        this.query = queryBuilder;
+    public DeleteByQueryRequest source(QuerySourceBuilder sourceBuilder) {
+        this.source = sourceBuilder.buildAsBytes(Requests.CONTENT_TYPE);
+        return this;
+    }
+
+    public DeleteByQueryRequest source(Map<String,?> querySource) {
+        try {
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(querySource);
+            return source(builder);
+        } catch (IOException e) {
+            throw new ElasticsearchGenerationException("Failed to generate [" + querySource + "]", e);
+        }
+    }
+
+    public DeleteByQueryRequest source(XContentBuilder builder) {
+        this.source = builder.bytes();
+        return this;
+    }
+
+    public DeleteByQueryRequest source(String querySource) {
+        this.source = new BytesArray(querySource);
+        return this;
+    }
+
+    public DeleteByQueryRequest source(byte[] querySource) {
+        return source(querySource, 0, querySource.length);
+    }
+
+    public DeleteByQueryRequest source(byte[] querySource, int offset, int length) {
+        return source(new BytesArray(querySource, offset, length));
+    }
+
+    public DeleteByQueryRequest source(BytesReference querySource) {
+        this.source = querySource;
         return this;
     }
 
@@ -209,7 +249,7 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
         indices = in.readStringArray();
         indicesOptions = IndicesOptions.readIndicesOptions(in);
         types = in.readStringArray();
-        query = in.readQuery();
+        source = in.readBytesReference();
         routing = in.readOptionalString();
         size = in.readVInt();
         if (in.readBoolean()) {
@@ -226,7 +266,7 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
         out.writeStringArray(indices);
         indicesOptions.writeIndicesOptions(out);
         out.writeStringArray(types);
-        out.writeQuery(query);
+        out.writeBytesReference(source);
         out.writeOptionalString(routing);
         out.writeVInt(size);
         out.writeOptionalStreamable(scroll);
@@ -237,7 +277,7 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
     public String toString() {
         String sSource = "_na_";
         try {
-            sSource = XContentHelper.toString(query);
+            sSource = XContentHelper.convertToJson(source, false);
         } catch (Exception e) {
             // ignore
         }
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java
index 7560e1e..d30cfaa 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java
@@ -20,17 +20,25 @@
 package org.elasticsearch.action.deletebyquery;
 
 import org.elasticsearch.action.ActionRequestBuilder;
+import org.elasticsearch.action.ListenableActionFuture;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.client.ElasticsearchClient;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
 
+import java.util.Map;
+
 /**
  * Creates a new {@link DeleteByQueryRequestBuilder}
  * @see DeleteByQueryRequest
  */
 public class DeleteByQueryRequestBuilder extends ActionRequestBuilder<DeleteByQueryRequest, DeleteByQueryResponse, DeleteByQueryRequestBuilder> {
 
+    private QuerySourceBuilder sourceBuilder;
+
     public DeleteByQueryRequestBuilder(ElasticsearchClient client, DeleteByQueryAction action) {
         super(client, action, new DeleteByQueryRequest());
     }
@@ -56,11 +64,26 @@ public class DeleteByQueryRequestBuilder extends ActionRequestBuilder<DeleteByQu
      * @see org.elasticsearch.index.query.QueryBuilders
      */
     public DeleteByQueryRequestBuilder setQuery(QueryBuilder<?> queryBuilder) {
-        request.query(queryBuilder);
+        sourceBuilder().setQuery(queryBuilder);
+        return this;
+    }
+
+    /**
+     * The query binary used to delete documents.
+     */
+    public DeleteByQueryRequestBuilder setQuery(BytesReference queryBinary) {
+        sourceBuilder().setQuery(queryBinary);
         return this;
     }
 
     /**
+     * Constructs a new builder with a raw search query.
+     */
+    public DeleteByQueryRequestBuilder setQuery(XContentBuilder query) {
+        return setQuery(query.bytes());
+    }
+
+    /**
      * A comma separated list of routing values to control the shards the action will be executed on.
      */
     public DeleteByQueryRequestBuilder setRouting(String routing) {
@@ -77,6 +100,47 @@ public class DeleteByQueryRequestBuilder extends ActionRequestBuilder<DeleteByQu
     }
 
     /**
+     * The source to execute. It is preferable to use either {@link #setSource(byte[])}
+     * or {@link #setQuery(QueryBuilder)}.
+     */
+    public DeleteByQueryRequestBuilder setSource(String source) {
+        request().source(source);
+        return this;
+    }
+
+    /**
+     * The source to execute in the form of a map.
+     */
+    public DeleteByQueryRequestBuilder setSource(Map<String, Object> source) {
+        request().source(source);
+        return this;
+    }
+
+    /**
+     * The source to execute in the form of a builder.
+     */
+    public DeleteByQueryRequestBuilder setSource(XContentBuilder builder) {
+        request().source(builder);
+        return this;
+    }
+
+    /**
+     * The source to execute.
+     */
+    public DeleteByQueryRequestBuilder setSource(byte[] source) {
+        request().source(source);
+        return this;
+    }
+
+    /**
+     * The source to execute.
+     */
+    public DeleteByQueryRequestBuilder setSource(BytesReference source) {
+        request().source(source);
+        return this;
+    }
+
+    /**
      * An optional timeout to control how long the delete by query is allowed to take.
      */
     public DeleteByQueryRequestBuilder setTimeout(TimeValue timeout) {
@@ -100,4 +164,19 @@ public class DeleteByQueryRequestBuilder extends ActionRequestBuilder<DeleteByQu
         return this;
     }
 
+    @Override
+    public ListenableActionFuture<DeleteByQueryResponse> execute() {
+        if (sourceBuilder != null) {
+            request.source(sourceBuilder);
+        }
+        return super.execute();
+    }
+
+    private QuerySourceBuilder sourceBuilder() {
+        if (sourceBuilder == null) {
+            sourceBuilder = new QuerySourceBuilder();
+        }
+        return sourceBuilder;
+    }
+
 }
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java
index 83a3015..252befd 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java
@@ -27,13 +27,7 @@ import org.elasticsearch.action.bulk.BulkRequest;
 import org.elasticsearch.action.bulk.BulkResponse;
 import org.elasticsearch.action.delete.DeleteRequest;
 import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.search.ClearScrollResponse;
-import org.elasticsearch.action.search.SearchRequest;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchScrollRequest;
-import org.elasticsearch.action.search.ShardSearchFailure;
-import org.elasticsearch.action.search.TransportSearchAction;
-import org.elasticsearch.action.search.TransportSearchScrollAction;
+import org.elasticsearch.action.search.*;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.HandledTransportAction;
 import org.elasticsearch.client.Client;
@@ -48,9 +42,7 @@ import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
-import java.util.ArrayList;
 import java.util.HashMap;
-import java.util.List;
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicLong;
@@ -115,11 +107,9 @@ public class TransportDeleteByQueryAction extends HandledTransportAction<DeleteB
                     scanRequest.routing(request.routing());
                 }
 
-                List<String> fields = new ArrayList<>();
-                fields.add("_routing");
-                fields.add("_parent");
                 SearchSourceBuilder source = new SearchSourceBuilder()
-.query(request.query()).fields(fields)
+                        .query(request.source())
+                        .fields("_routing", "_parent")
                         .sort("_doc") // important for performance
                         .fetchSource(false)
                         .version(true);
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java
index 19443e5..251953d 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java
@@ -22,15 +22,11 @@ package org.elasticsearch.rest.action.deletebyquery;
 import org.elasticsearch.action.deletebyquery.DeleteByQueryRequest;
 import org.elasticsearch.action.deletebyquery.DeleteByQueryResponse;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.rest.BaseRestHandler;
 import org.elasticsearch.rest.RestChannel;
 import org.elasticsearch.rest.RestController;
@@ -38,8 +34,6 @@ import org.elasticsearch.rest.RestRequest;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestToXContentListener;
 
-import java.io.IOException;
-
 import static org.elasticsearch.action.deletebyquery.DeleteByQueryAction.INSTANCE;
 import static org.elasticsearch.rest.RestRequest.Method.DELETE;
 
@@ -48,19 +42,15 @@ import static org.elasticsearch.rest.RestRequest.Method.DELETE;
  */
 public class RestDeleteByQueryAction extends BaseRestHandler {
 
-    private IndicesQueriesRegistry indicesQueriesRegistry;
-
     @Inject
-    public RestDeleteByQueryAction(Settings settings, RestController controller, Client client,
-            IndicesQueriesRegistry indicesQueriesRegistry) {
+    public RestDeleteByQueryAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
-        this.indicesQueriesRegistry = indicesQueriesRegistry;
         controller.registerHandler(DELETE, "/{index}/_query", this);
         controller.registerHandler(DELETE, "/{index}/{type}/_query", this);
     }
 
     @Override
-    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) throws IOException {
+    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) {
         DeleteByQueryRequest delete = new DeleteByQueryRequest(Strings.splitStringByCommaToArray(request.param("index")));
         delete.indicesOptions(IndicesOptions.fromRequest(request, delete.indicesOptions()));
         delete.routing(request.param("routing"));
@@ -68,23 +58,15 @@ public class RestDeleteByQueryAction extends BaseRestHandler {
             delete.timeout(request.paramAsTime("timeout", null));
         }
         if (request.hasContent()) {
-            XContentParser requestParser = XContentFactory.xContent(request.content()).createParser(request.content());
-            QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
-            context.reset(requestParser);
-            final QueryBuilder<?> builder = context.parseInnerQueryBuilder();
-            delete.query(builder);
+            delete.source(request.content());
         } else {
             String source = request.param("source");
             if (source != null) {
-                XContentParser requestParser = XContentFactory.xContent(source).createParser(source);
-                QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
-                context.reset(requestParser);
-                final QueryBuilder<?> builder = context.parseInnerQueryBuilder();
-                delete.query(builder);
+                delete.source(source);
             } else {
-                QueryBuilder<?> queryBuilder = RestActions.parseQuerySource(request);
-                if (queryBuilder != null) {
-                    delete.query(queryBuilder);
+                QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+                if (querySourceBuilder != null) {
+                    delete.source(querySourceBuilder);
                 }
             }
         }
diff --git a/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java b/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java
index d5d2e79..c9d3f44 100644
--- a/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java
+++ b/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java
@@ -58,20 +58,19 @@ public class TransportDeleteByQueryActionTests extends ESSingleNodeTestCase {
         assertSearchContextsClosed();
     }
 
-    // NORELEASE re-implement this parsing test as a unit test
-//    @Test
-//    public void testExecuteScanFailsOnMalformedQuery() {
-//        createIndex("test");
-//
-//        DeleteByQueryRequest delete = new DeleteByQueryRequest().indices(new String[]{"test"}).query("{...}");
-//        TestActionListener listener = new TestActionListener();
-//
-//        newAsyncAction(delete, listener).executeScan();
-//        waitForCompletion("scan request should fail on malformed query", listener);
-//
-//        assertFailure(listener, "all shards failed");
-//        assertSearchContextsClosed();
-//    }
+    @Test
+    public void testExecuteScanFailsOnMalformedQuery() {
+        createIndex("test");
+
+        DeleteByQueryRequest delete = new DeleteByQueryRequest().indices(new String[]{"test"}).source("{...}");
+        TestActionListener listener = new TestActionListener();
+
+        newAsyncAction(delete, listener).executeScan();
+        waitForCompletion("scan request should fail on malformed query", listener);
+
+        assertFailure(listener, "all shards failed");
+        assertSearchContextsClosed();
+    }
 
     @Test
     public void testExecuteScan() {
@@ -84,7 +83,7 @@ public class TransportDeleteByQueryActionTests extends ESSingleNodeTestCase {
         assertHitCount(client().prepareCount("test").get(), numDocs);
 
         final long limit = randomIntBetween(0, numDocs);
-        DeleteByQueryRequest delete = new DeleteByQueryRequest().indices(new String[]{"test"}).query(boolQuery().must(rangeQuery("num").lte(limit)));
+        DeleteByQueryRequest delete = new DeleteByQueryRequest().indices(new String[]{"test"}).source(boolQuery().must(rangeQuery("num").lte(limit)).buildAsBytes());
         TestActionListener listener = new TestActionListener();
 
         newAsyncAction(delete, listener).executeScan();
@@ -220,7 +219,7 @@ public class TransportDeleteByQueryActionTests extends ESSingleNodeTestCase {
         assertTrue(Strings.hasText(scrollId));
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(limit));
 
-        DeleteByQueryRequest delete = new DeleteByQueryRequest().indices(new String[]{"test"}).size(100).query(boolQuery().must(rangeQuery("num").lte(limit)));
+        DeleteByQueryRequest delete = new DeleteByQueryRequest().indices(new String[]{"test"}).size(100).source(boolQuery().must(rangeQuery("num").lte(limit)).buildAsBytes());
         TestActionListener listener = new TestActionListener();
 
         newAsyncAction(delete, listener).executeScroll(searchResponse.getScrollId());
diff --git a/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java b/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java
index 8465136..75ae011 100644
--- a/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java
+++ b/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java
@@ -23,6 +23,7 @@ import org.elasticsearch.cloud.azure.AbstractAzureComputeServiceTestCase;
 import org.elasticsearch.cloud.azure.AzureComputeServiceTwoNodesMock;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.discovery.MasterNotDiscoveredException;
+import org.elasticsearch.discovery.zen.ZenDiscovery;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 import org.apache.lucene.util.LuceneTestCase.AwaitsFix;
@@ -53,8 +54,8 @@ public class AzureMinimumMasterNodesTests extends AbstractAzureComputeServiceTes
                 .put(super.nodeSettings(nodeOrdinal))
                 .put("discovery.zen.minimum_master_nodes", 2)
                 // Make the test run faster
-                .put("discovery.zen.join.timeout", "50ms")
-                .put("discovery.zen.ping.timeout", "10ms")
+                .put(ZenDiscovery.SETTING_JOIN_TIMEOUT, "50ms")
+                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "10ms")
                 .put("discovery.initial_state_timeout", "100ms");
         return builder.build();
     }
diff --git a/plugins/lang-expression/licenses/antlr4-runtime-4.5.jar.sha1 b/plugins/lang-expression/licenses/antlr4-runtime-4.5.jar.sha1
new file mode 100644
index 0000000..5299c19
--- /dev/null
+++ b/plugins/lang-expression/licenses/antlr4-runtime-4.5.jar.sha1
@@ -0,0 +1 @@
+29e48af049f17dd89153b83a7ad5d01b3b4bcdda
diff --git a/plugins/lang-expression/licenses/antlr4-runtime-LICENSE.txt b/plugins/lang-expression/licenses/antlr4-runtime-LICENSE.txt
new file mode 100644
index 0000000..95d0a25
--- /dev/null
+++ b/plugins/lang-expression/licenses/antlr4-runtime-LICENSE.txt
@@ -0,0 +1,26 @@
+[The "BSD license"]
+Copyright (c) 2015 Terence Parr, Sam Harwell
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+
+ 1. Redistributions of source code must retain the above copyright
+    notice, this list of conditions and the following disclaimer.
+ 2. Redistributions in binary form must reproduce the above copyright
+    notice, this list of conditions and the following disclaimer in the
+    documentation and/or other materials provided with the distribution.
+ 3. The name of the author may not be used to endorse or promote products
+    derived from this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
diff --git a/plugins/lang-expression/licenses/antlr4-runtime-NOTICE.txt b/plugins/lang-expression/licenses/antlr4-runtime-NOTICE.txt
new file mode 100644
index 0000000..e69de29
diff --git a/plugins/lang-expression/licenses/asm-5.0.4.jar.sha1 b/plugins/lang-expression/licenses/asm-5.0.4.jar.sha1
new file mode 100644
index 0000000..9223dba
--- /dev/null
+++ b/plugins/lang-expression/licenses/asm-5.0.4.jar.sha1
@@ -0,0 +1 @@
+0da08b8cce7bbf903602a25a3a163ae252435795
diff --git a/plugins/lang-expression/licenses/asm-LICENSE.txt b/plugins/lang-expression/licenses/asm-LICENSE.txt
new file mode 100644
index 0000000..afb064f
--- /dev/null
+++ b/plugins/lang-expression/licenses/asm-LICENSE.txt
@@ -0,0 +1,26 @@
+Copyright (c) 2012 France Tlcom
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+1. Redistributions of source code must retain the above copyright
+   notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+   notice, this list of conditions and the following disclaimer in the
+   documentation and/or other materials provided with the distribution.
+3. Neither the name of the copyright holders nor the names of its
+   contributors may be used to endorse or promote products derived from
+   this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
diff --git a/plugins/lang-expression/licenses/asm-NOTICE.txt b/plugins/lang-expression/licenses/asm-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/lang-expression/licenses/asm-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/lang-expression/licenses/asm-commons-5.0.4.jar.sha1 b/plugins/lang-expression/licenses/asm-commons-5.0.4.jar.sha1
new file mode 100644
index 0000000..94fe0cd
--- /dev/null
+++ b/plugins/lang-expression/licenses/asm-commons-5.0.4.jar.sha1
@@ -0,0 +1 @@
+5a556786086c23cd689a0328f8519db93821c04c
diff --git a/plugins/lang-expression/licenses/asm-commons-LICENSE.txt b/plugins/lang-expression/licenses/asm-commons-LICENSE.txt
new file mode 100644
index 0000000..afb064f
--- /dev/null
+++ b/plugins/lang-expression/licenses/asm-commons-LICENSE.txt
@@ -0,0 +1,26 @@
+Copyright (c) 2012 France Tlcom
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+1. Redistributions of source code must retain the above copyright
+   notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+   notice, this list of conditions and the following disclaimer in the
+   documentation and/or other materials provided with the distribution.
+3. Neither the name of the copyright holders nor the names of its
+   contributors may be used to endorse or promote products derived from
+   this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
diff --git a/plugins/lang-expression/licenses/asm-commons-NOTICE.txt b/plugins/lang-expression/licenses/asm-commons-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/lang-expression/licenses/asm-commons-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/lang-expression/licenses/lucene-LICENSE.txt b/plugins/lang-expression/licenses/lucene-LICENSE.txt
new file mode 100644
index 0000000..28b134f
--- /dev/null
+++ b/plugins/lang-expression/licenses/lucene-LICENSE.txt
@@ -0,0 +1,475 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
+
+
+Some code in core/src/java/org/apache/lucene/util/UnicodeUtil.java was
+derived from unicode conversion examples available at
+http://www.unicode.org/Public/PROGRAMS/CVTUTF.  Here is the copyright
+from those sources:
+
+/*
+ * Copyright 2001-2004 Unicode, Inc.
+ * 
+ * Disclaimer
+ * 
+ * This source code is provided as is by Unicode, Inc. No claims are
+ * made as to fitness for any particular purpose. No warranties of any
+ * kind are expressed or implied. The recipient agrees to determine
+ * applicability of information provided. If this file has been
+ * purchased on magnetic or optical media from Unicode, Inc., the
+ * sole remedy for any claim will be exchange of defective media
+ * within 90 days of receipt.
+ * 
+ * Limitations on Rights to Redistribute This Code
+ * 
+ * Unicode, Inc. hereby grants the right to freely use the information
+ * supplied in this file in the creation of products supporting the
+ * Unicode Standard, and to make copies of this file in any form
+ * for internal or external distribution as long as this notice
+ * remains attached.
+ */
+
+
+Some code in core/src/java/org/apache/lucene/util/ArrayUtil.java was
+derived from Python 2.4.2 sources available at
+http://www.python.org. Full license is here:
+
+  http://www.python.org/download/releases/2.4.2/license/
+
+Some code in core/src/java/org/apache/lucene/util/UnicodeUtil.java was
+derived from Python 3.1.2 sources available at
+http://www.python.org. Full license is here:
+
+  http://www.python.org/download/releases/3.1.2/license/
+
+Some code in core/src/java/org/apache/lucene/util/automaton was
+derived from Brics automaton sources available at
+www.brics.dk/automaton/. Here is the copyright from those sources:
+
+/*
+ * Copyright (c) 2001-2009 Anders Moeller
+ * All rights reserved.
+ * 
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ * 
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+ 
+The levenshtein automata tables in core/src/java/org/apache/lucene/util/automaton 
+were automatically generated with the moman/finenight FSA package.
+Here is the copyright for those sources:
+
+# Copyright (c) 2010, Jean-Philippe Barrette-LaPierre, <jpb@rrette.com>
+#
+# Permission is hereby granted, free of charge, to any person
+# obtaining a copy of this software and associated documentation
+# files (the "Software"), to deal in the Software without
+# restriction, including without limitation the rights to use,
+# copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the
+# Software is furnished to do so, subject to the following
+# conditions:
+#
+# The above copyright notice and this permission notice shall be
+# included in all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+# OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+# OTHER DEALINGS IN THE SOFTWARE.
+
+Some code in core/src/java/org/apache/lucene/util/UnicodeUtil.java was
+derived from ICU (http://www.icu-project.org)
+The full license is available here: 
+  http://source.icu-project.org/repos/icu/icu/trunk/license.html
+
+/*
+ * Copyright (C) 1999-2010, International Business Machines
+ * Corporation and others.  All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy 
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights 
+ * to use, copy, modify, merge, publish, distribute, and/or sell copies of the 
+ * Software, and to permit persons to whom the Software is furnished to do so, 
+ * provided that the above copyright notice(s) and this permission notice appear 
+ * in all copies of the Software and that both the above copyright notice(s) and
+ * this permission notice appear in supporting documentation.
+ * 
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR 
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, 
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS. 
+ * IN NO EVENT SHALL THE COPYRIGHT HOLDER OR HOLDERS INCLUDED IN THIS NOTICE BE 
+ * LIABLE FOR ANY CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR 
+ * ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER 
+ * IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT 
+ * OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
+ *
+ * Except as contained in this notice, the name of a copyright holder shall not 
+ * be used in advertising or otherwise to promote the sale, use or other 
+ * dealings in this Software without prior written authorization of the 
+ * copyright holder.
+ */
+ 
+The following license applies to the Snowball stemmers:
+
+Copyright (c) 2001, Dr Martin Porter
+Copyright (c) 2002, Richard Boulton
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+    * Redistributions of source code must retain the above copyright notice,
+    * this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+    * notice, this list of conditions and the following disclaimer in the
+    * documentation and/or other materials provided with the distribution.
+    * Neither the name of the copyright holders nor the names of its contributors
+    * may be used to endorse or promote products derived from this software
+    * without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+The following license applies to the KStemmer:
+
+Copyright  2003,
+Center for Intelligent Information Retrieval,
+University of Massachusetts, Amherst.
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without modification,
+are permitted provided that the following conditions are met:
+
+1. Redistributions of source code must retain the above copyright notice, this
+list of conditions and the following disclaimer.
+
+2. Redistributions in binary form must reproduce the above copyright notice,
+this list of conditions and the following disclaimer in the documentation
+and/or other materials provided with the distribution.
+
+3. The names "Center for Intelligent Information Retrieval" and
+"University of Massachusetts" must not be used to endorse or promote products
+derived from this software without prior written permission. To obtain
+permission, contact info@ciir.cs.umass.edu.
+
+THIS SOFTWARE IS PROVIDED BY UNIVERSITY OF MASSACHUSETTS AND OTHER CONTRIBUTORS
+"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
+THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE
+LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE
+GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+SUCH DAMAGE.
+
+The following license applies to the Morfologik project:
+
+Copyright (c) 2006 Dawid Weiss
+Copyright (c) 2007-2011 Dawid Weiss, Marcin Mikowski
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without modification, 
+are permitted provided that the following conditions are met:
+
+    * Redistributions of source code must retain the above copyright notice, 
+    this list of conditions and the following disclaimer.
+    
+    * Redistributions in binary form must reproduce the above copyright notice, 
+    this list of conditions and the following disclaimer in the documentation 
+    and/or other materials provided with the distribution.
+    
+    * Neither the name of Morfologik nor the names of its contributors 
+    may be used to endorse or promote products derived from this software 
+    without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND 
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED 
+WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE 
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR 
+ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES 
+(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; 
+LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON 
+ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT 
+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS 
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+---
+
+The dictionary comes from Morfologik project. Morfologik uses data from 
+Polish ispell/myspell dictionary hosted at http://www.sjp.pl/slownik/en/ and 
+is licenced on the terms of (inter alia) LGPL and Creative Commons 
+ShareAlike. The part-of-speech tags were added in Morfologik project and
+are not found in the data from sjp.pl. The tagset is similar to IPI PAN
+tagset.
+
+---
+
+The following license applies to the Morfeusz project,
+used by org.apache.lucene.analysis.morfologik.
+
+BSD-licensed dictionary of Polish (SGJP)
+http://sgjp.pl/morfeusz/
+
+Copyright  2011 Zygmunt Saloni, Wodzimierz Gruszczyski, 
+             Marcin Woliski, Robert Woosz
+
+All rights reserved.
+
+Redistribution and  use in  source and binary  forms, with  or without
+modification, are permitted provided that the following conditions are
+met:
+
+1. Redistributions of source code must retain the above copyright
+   notice, this list of conditions and the following disclaimer.
+
+2. Redistributions in binary form must reproduce the above copyright
+   notice, this list of conditions and the following disclaimer in the
+   documentation and/or other materials provided with the
+   distribution.
+
+THIS SOFTWARE IS PROVIDED BY COPYRIGHT HOLDERS AS IS AND ANY EXPRESS
+OR  IMPLIED WARRANTIES,  INCLUDING, BUT  NOT LIMITED  TO,  THE IMPLIED
+WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED.  IN NO EVENT  SHALL COPYRIGHT  HOLDERS OR  CONTRIBUTORS BE
+LIABLE FOR  ANY DIRECT,  INDIRECT, INCIDENTAL, SPECIAL,  EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES  (INCLUDING, BUT NOT LIMITED  TO, PROCUREMENT OF
+SUBSTITUTE  GOODS OR  SERVICES;  LOSS  OF USE,  DATA,  OR PROFITS;  OR
+BUSINESS INTERRUPTION) HOWEVER CAUSED  AND ON ANY THEORY OF LIABILITY,
+WHETHER IN  CONTRACT, STRICT LIABILITY, OR  TORT (INCLUDING NEGLIGENCE
+OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
diff --git a/plugins/lang-expression/licenses/lucene-NOTICE.txt b/plugins/lang-expression/licenses/lucene-NOTICE.txt
new file mode 100644
index 0000000..ecf0820
--- /dev/null
+++ b/plugins/lang-expression/licenses/lucene-NOTICE.txt
@@ -0,0 +1,191 @@
+Apache Lucene
+Copyright 2014 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
+
+Includes software from other Apache Software Foundation projects,
+including, but not limited to:
+ - Apache Ant
+ - Apache Jakarta Regexp
+ - Apache Commons
+ - Apache Xerces
+
+ICU4J, (under analysis/icu) is licensed under an MIT styles license
+and Copyright (c) 1995-2008 International Business Machines Corporation and others
+
+Some data files (under analysis/icu/src/data) are derived from Unicode data such
+as the Unicode Character Database. See http://unicode.org/copyright.html for more
+details.
+
+Brics Automaton (under core/src/java/org/apache/lucene/util/automaton) is 
+BSD-licensed, created by Anders Mller. See http://www.brics.dk/automaton/
+
+The levenshtein automata tables (under core/src/java/org/apache/lucene/util/automaton) were
+automatically generated with the moman/finenight FSA library, created by
+Jean-Philippe Barrette-LaPierre. This library is available under an MIT license,
+see http://sites.google.com/site/rrettesite/moman and 
+http://bitbucket.org/jpbarrette/moman/overview/
+
+The class org.apache.lucene.util.WeakIdentityMap was derived from
+the Apache CXF project and is Apache License 2.0.
+
+The Google Code Prettify is Apache License 2.0.
+See http://code.google.com/p/google-code-prettify/
+
+JUnit (junit-4.10) is licensed under the Common Public License v. 1.0
+See http://junit.sourceforge.net/cpl-v10.html
+
+This product includes code (JaspellTernarySearchTrie) from Java Spelling Checkin
+g Package (jaspell): http://jaspell.sourceforge.net/
+License: The BSD License (http://www.opensource.org/licenses/bsd-license.php)
+
+The snowball stemmers in
+  analysis/common/src/java/net/sf/snowball
+were developed by Martin Porter and Richard Boulton.
+The snowball stopword lists in
+  analysis/common/src/resources/org/apache/lucene/analysis/snowball
+were developed by Martin Porter and Richard Boulton.
+The full snowball package is available from
+  http://snowball.tartarus.org/
+
+The KStem stemmer in
+  analysis/common/src/org/apache/lucene/analysis/en
+was developed by Bob Krovetz and Sergio Guzman-Lara (CIIR-UMass Amherst)
+under the BSD-license.
+
+The Arabic,Persian,Romanian,Bulgarian, and Hindi analyzers (common) come with a default
+stopword list that is BSD-licensed created by Jacques Savoy.  These files reside in:
+analysis/common/src/resources/org/apache/lucene/analysis/ar/stopwords.txt,
+analysis/common/src/resources/org/apache/lucene/analysis/fa/stopwords.txt,
+analysis/common/src/resources/org/apache/lucene/analysis/ro/stopwords.txt,
+analysis/common/src/resources/org/apache/lucene/analysis/bg/stopwords.txt,
+analysis/common/src/resources/org/apache/lucene/analysis/hi/stopwords.txt
+See http://members.unine.ch/jacques.savoy/clef/index.html.
+
+The German,Spanish,Finnish,French,Hungarian,Italian,Portuguese,Russian and Swedish light stemmers
+(common) are based on BSD-licensed reference implementations created by Jacques Savoy and
+Ljiljana Dolamic. These files reside in:
+analysis/common/src/java/org/apache/lucene/analysis/de/GermanLightStemmer.java
+analysis/common/src/java/org/apache/lucene/analysis/de/GermanMinimalStemmer.java
+analysis/common/src/java/org/apache/lucene/analysis/es/SpanishLightStemmer.java
+analysis/common/src/java/org/apache/lucene/analysis/fi/FinnishLightStemmer.java
+analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchLightStemmer.java
+analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchMinimalStemmer.java
+analysis/common/src/java/org/apache/lucene/analysis/hu/HungarianLightStemmer.java
+analysis/common/src/java/org/apache/lucene/analysis/it/ItalianLightStemmer.java
+analysis/common/src/java/org/apache/lucene/analysis/pt/PortugueseLightStemmer.java
+analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLightStemmer.java
+analysis/common/src/java/org/apache/lucene/analysis/sv/SwedishLightStemmer.java
+
+The Stempel analyzer (stempel) includes BSD-licensed software developed 
+by the Egothor project http://egothor.sf.net/, created by Leo Galambos, Martin Kvapil,
+and Edmond Nolan.
+
+The Polish analyzer (stempel) comes with a default
+stopword list that is BSD-licensed created by the Carrot2 project. The file resides
+in stempel/src/resources/org/apache/lucene/analysis/pl/stopwords.txt.
+See http://project.carrot2.org/license.html.
+
+The SmartChineseAnalyzer source code (smartcn) was
+provided by Xiaoping Gao and copyright 2009 by www.imdict.net.
+
+WordBreakTestUnicode_*.java (under modules/analysis/common/src/test/) 
+is derived from Unicode data such as the Unicode Character Database. 
+See http://unicode.org/copyright.html for more details.
+
+The Morfologik analyzer (morfologik) includes BSD-licensed software
+developed by Dawid Weiss and Marcin Mikowski (http://morfologik.blogspot.com/).
+
+Morfologik uses data from Polish ispell/myspell dictionary
+(http://www.sjp.pl/slownik/en/) licenced on the terms of (inter alia)
+LGPL and Creative Commons ShareAlike.
+
+Morfologic includes data from BSD-licensed dictionary of Polish (SGJP)
+(http://sgjp.pl/morfeusz/)
+
+Servlet-api.jar and javax.servlet-*.jar are under the CDDL license, the original
+source code for this can be found at http://www.eclipse.org/jetty/downloads.php
+
+===========================================================================
+Kuromoji Japanese Morphological Analyzer - Apache Lucene Integration
+===========================================================================
+
+This software includes a binary and/or source version of data from
+
+  mecab-ipadic-2.7.0-20070801
+
+which can be obtained from
+
+  http://atilika.com/releases/mecab-ipadic/mecab-ipadic-2.7.0-20070801.tar.gz
+
+or
+
+  http://jaist.dl.sourceforge.net/project/mecab/mecab-ipadic/2.7.0-20070801/mecab-ipadic-2.7.0-20070801.tar.gz
+
+===========================================================================
+mecab-ipadic-2.7.0-20070801 Notice
+===========================================================================
+
+Nara Institute of Science and Technology (NAIST),
+the copyright holders, disclaims all warranties with regard to this
+software, including all implied warranties of merchantability and
+fitness, in no event shall NAIST be liable for
+any special, indirect or consequential damages or any damages
+whatsoever resulting from loss of use, data or profits, whether in an
+action of contract, negligence or other tortuous action, arising out
+of or in connection with the use or performance of this software.
+
+A large portion of the dictionary entries
+originate from ICOT Free Software.  The following conditions for ICOT
+Free Software applies to the current dictionary as well.
+
+Each User may also freely distribute the Program, whether in its
+original form or modified, to any third party or parties, PROVIDED
+that the provisions of Section 3 ("NO WARRANTY") will ALWAYS appear
+on, or be attached to, the Program, which is distributed substantially
+in the same form as set out herein and that such intended
+distribution, if actually made, will neither violate or otherwise
+contravene any of the laws and regulations of the countries having
+jurisdiction over the User or the intended distribution itself.
+
+NO WARRANTY
+
+The program was produced on an experimental basis in the course of the
+research and development conducted during the project and is provided
+to users as so produced on an experimental basis.  Accordingly, the
+program is provided without any warranty whatsoever, whether express,
+implied, statutory or otherwise.  The term "warranty" used herein
+includes, but is not limited to, any warranty of the quality,
+performance, merchantability and fitness for a particular purpose of
+the program and the nonexistence of any infringement or violation of
+any right of any third party.
+
+Each user of the program will agree and understand, and be deemed to
+have agreed and understood, that there is no warranty whatsoever for
+the program and, accordingly, the entire risk arising from or
+otherwise connected with the program is assumed by the user.
+
+Therefore, neither ICOT, the copyright holder, or any other
+organization that participated in or was otherwise related to the
+development of the program and their respective officials, directors,
+officers and other employees shall be held liable for any and all
+damages, including, without limitation, general, special, incidental
+and consequential damages, arising out of or otherwise in connection
+with the use or inability to use the program or any product, material
+or result produced or otherwise obtained by using the program,
+regardless of whether they have been advised of, or otherwise had
+knowledge of, the possibility of such damages at any time during the
+project or thereafter.  Each user will be deemed to have agreed to the
+foregoing by his or her commencement of use of the program.  The term
+"use" as used herein includes, but is not limited to, the use,
+modification, copying and distribution of the program and the
+production of secondary products from the program.
+
+In the case where the program, whether in its original form or
+modified, was distributed or delivered to or received by a user from
+any person, organization or entity other than ICOT, unless it makes or
+grants independently of ICOT any specific warranty to the user in
+writing, such person, organization or entity, will also be exempted
+from and not be held liable to the user for any such damages as noted
+above as far as the program is concerned.
diff --git a/plugins/lang-expression/licenses/lucene-expressions-5.4.0-snapshot-1702855.jar.sha1 b/plugins/lang-expression/licenses/lucene-expressions-5.4.0-snapshot-1702855.jar.sha1
new file mode 100644
index 0000000..d04bca5
--- /dev/null
+++ b/plugins/lang-expression/licenses/lucene-expressions-5.4.0-snapshot-1702855.jar.sha1
@@ -0,0 +1 @@
+ecfb9a923b19fac61b4e9a79275f6bd242b1f091
diff --git a/plugins/lang-expression/pom.xml b/plugins/lang-expression/pom.xml
new file mode 100644
index 0000000..cf503d4
--- /dev/null
+++ b/plugins/lang-expression/pom.xml
@@ -0,0 +1,41 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.elasticsearch.plugin</groupId>
+        <artifactId>plugins</artifactId>
+        <version>3.0.0-SNAPSHOT</version>
+    </parent>
+
+    <artifactId>lang-expression</artifactId>
+    <name>Plugin: Language: Expression</name>
+    <description>Lucene expressions integration for Elasticsearch</description>
+
+    <properties>
+        <elasticsearch.plugin.classname>org.elasticsearch.script.expression.ExpressionPlugin</elasticsearch.plugin.classname>
+        <tests.rest.suite>lang_expression</tests.rest.suite>
+        <tests.rest.load_packaged>false</tests.rest.load_packaged>
+        <xlint.options>-Xlint:-rawtypes</xlint.options>
+    </properties>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.lucene</groupId>
+            <artifactId>lucene-expressions</artifactId>
+            <version>${lucene.maven.version}</version>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
diff --git a/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/CountMethodFunctionValues.java b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/CountMethodFunctionValues.java
new file mode 100644
index 0000000..818404e
--- /dev/null
+++ b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/CountMethodFunctionValues.java
@@ -0,0 +1,44 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
+import org.elasticsearch.index.fielddata.AtomicNumericFieldData;
+import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
+
+/**
+ * FunctionValues to get the count of the number of values in a field for a document.
+ */
+public class CountMethodFunctionValues extends DoubleDocValues {
+    SortedNumericDoubleValues values;
+
+    CountMethodFunctionValues(ValueSource parent, AtomicNumericFieldData fieldData) {
+        super(parent);
+
+        values = fieldData.getDoubleValues();
+    }
+
+    @Override
+    public double doubleVal(int doc) {
+        values.setDocument(doc);
+        return values.count();
+    }
+}
diff --git a/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/CountMethodValueSource.java b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/CountMethodValueSource.java
new file mode 100644
index 0000000..0fa83d9
--- /dev/null
+++ b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/CountMethodValueSource.java
@@ -0,0 +1,73 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.elasticsearch.index.fielddata.AtomicFieldData;
+import org.elasticsearch.index.fielddata.AtomicNumericFieldData;
+import org.elasticsearch.index.fielddata.IndexFieldData;
+import org.elasticsearch.search.MultiValueMode;
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.Objects;
+
+/**
+ * A ValueSource to create FunctionValues to get the count of the number of values in a field for a document.
+ */
+public class CountMethodValueSource extends ValueSource {
+    protected IndexFieldData<?> fieldData;
+
+    protected CountMethodValueSource(IndexFieldData<?> fieldData) {
+        Objects.requireNonNull(fieldData);
+
+        this.fieldData = fieldData;
+    }
+
+    @Override
+    public FunctionValues getValues(Map context, LeafReaderContext leaf) throws IOException {
+        AtomicFieldData leafData = fieldData.load(leaf);
+        assert(leafData instanceof AtomicNumericFieldData);
+
+        return new CountMethodFunctionValues(this, (AtomicNumericFieldData)leafData);
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) return true;
+        if (o == null || getClass() != o.getClass()) return false;
+
+        FieldDataValueSource that = (FieldDataValueSource) o;
+
+        return fieldData.equals(that.fieldData);
+    }
+
+    @Override
+    public int hashCode() {
+        return fieldData.hashCode();
+    }
+
+    @Override
+    public String description() {
+        return "count: field(" + fieldData.getFieldNames().toString() + ")";
+    }
+}
diff --git a/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/DateMethodFunctionValues.java b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/DateMethodFunctionValues.java
new file mode 100644
index 0000000..f7198fc
--- /dev/null
+++ b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/DateMethodFunctionValues.java
@@ -0,0 +1,47 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import java.util.Calendar;
+import java.util.Locale;
+import java.util.TimeZone;
+
+import org.apache.lucene.queries.function.ValueSource;
+import org.elasticsearch.index.fielddata.AtomicNumericFieldData;
+import org.elasticsearch.search.MultiValueMode;
+
+class DateMethodFunctionValues extends FieldDataFunctionValues {
+    private final int calendarType;
+    private final Calendar calendar;
+
+    DateMethodFunctionValues(ValueSource parent, MultiValueMode multiValueMode,  AtomicNumericFieldData data, int calendarType) {
+        super(parent, multiValueMode, data);
+
+        this.calendarType = calendarType;
+        calendar = Calendar.getInstance(TimeZone.getTimeZone("UTC"), Locale.ROOT);
+    }
+
+    @Override
+    public double doubleVal(int docId) {
+        long millis = (long)dataAccessor.get(docId);
+        calendar.setTimeInMillis(millis);
+        return calendar.get(calendarType);
+    }
+}
diff --git a/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/DateMethodValueSource.java b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/DateMethodValueSource.java
new file mode 100644
index 0000000..522b546
--- /dev/null
+++ b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/DateMethodValueSource.java
@@ -0,0 +1,81 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.Objects;
+
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.queries.function.FunctionValues;
+
+import org.elasticsearch.index.fielddata.AtomicFieldData;
+import org.elasticsearch.index.fielddata.AtomicNumericFieldData;
+import org.elasticsearch.index.fielddata.IndexFieldData;
+import org.elasticsearch.search.MultiValueMode;
+
+class DateMethodValueSource extends FieldDataValueSource {
+
+    protected final String methodName;
+    protected final int calendarType;
+
+    DateMethodValueSource(IndexFieldData<?> indexFieldData, MultiValueMode multiValueMode, String methodName, int calendarType) {
+        super(indexFieldData, multiValueMode);
+
+        Objects.requireNonNull(methodName);
+
+        this.methodName = methodName;
+        this.calendarType = calendarType;
+    }
+
+    @Override
+    public FunctionValues getValues(Map context, LeafReaderContext leaf) throws IOException {
+        AtomicFieldData leafData = fieldData.load(leaf);
+        assert(leafData instanceof AtomicNumericFieldData);
+
+        return new DateMethodFunctionValues(this, multiValueMode, (AtomicNumericFieldData)leafData, calendarType);
+    }
+
+    @Override
+    public String description() {
+        return methodName + ": field(" + fieldData.getFieldNames().toString() + ")";
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) return true;
+        if (o == null || getClass() != o.getClass()) return false;
+        if (!super.equals(o)) return false;
+
+        DateMethodValueSource that = (DateMethodValueSource) o;
+
+        if (calendarType != that.calendarType) return false;
+        return methodName.equals(that.methodName);
+
+    }
+
+    @Override
+    public int hashCode() {
+        int result = super.hashCode();
+        result = 31 * result + methodName.hashCode();
+        result = 31 * result + calendarType;
+        return result;
+    }
+}
diff --git a/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionExecutableScript.java b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionExecutableScript.java
new file mode 100644
index 0000000..d4ae9e1
--- /dev/null
+++ b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionExecutableScript.java
@@ -0,0 +1,98 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import org.apache.lucene.expressions.Expression;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.ScriptException;
+
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ * A bridge to evaluate an {@link Expression} against a map of variables in the context
+ * of an {@link ExecutableScript}.
+ */
+public class ExpressionExecutableScript implements ExecutableScript {
+
+    private final int NO_DOCUMENT = -1;
+
+    public final CompiledScript compiledScript;
+    public final Map<String, ReplaceableConstFunctionValues> functionValuesMap;
+    public final ReplaceableConstFunctionValues[] functionValuesArray;
+
+    public ExpressionExecutableScript(CompiledScript compiledScript, Map<String, Object> vars) {
+        this.compiledScript = compiledScript;
+        Expression expression = (Expression)this.compiledScript.compiled();
+        int functionValuesLength = expression.variables.length;
+
+        if (vars.size() != functionValuesLength) {
+            throw new ScriptException("Error using " + compiledScript + ". " +
+                    "The number of variables in an executable expression script [" +
+                    functionValuesLength + "] must match the number of variables in the variable map" +
+                    " [" + vars.size() + "].");
+        }
+
+        functionValuesArray = new ReplaceableConstFunctionValues[functionValuesLength];
+        functionValuesMap = new HashMap<>();
+
+        for (int functionValuesIndex = 0; functionValuesIndex < functionValuesLength; ++functionValuesIndex) {
+            String variableName = expression.variables[functionValuesIndex];
+            functionValuesArray[functionValuesIndex] = new ReplaceableConstFunctionValues();
+            functionValuesMap.put(variableName, functionValuesArray[functionValuesIndex]);
+        }
+
+        for (String varsName : vars.keySet()) {
+            setNextVar(varsName, vars.get(varsName));
+        }
+    }
+
+    @Override
+    public void setNextVar(String name, Object value) {
+        if (functionValuesMap.containsKey(name)) {
+            if (value instanceof Number) {
+                double doubleValue = ((Number)value).doubleValue();
+                functionValuesMap.get(name).setValue(doubleValue);
+            } else {
+                throw new ScriptException("Error using " + compiledScript + ". " +
+                        "Executable expressions scripts can only process numbers." +
+                        "  The variable [" + name + "] is not a number.");
+            }
+        } else {
+            throw new ScriptException("Error using " + compiledScript + ". " +
+                    "The variable [" + name + "] does not exist in the executable expressions script.");
+        }
+    }
+
+    @Override
+    public Object run() {
+        try {
+            return ((Expression) compiledScript.compiled()).evaluate(NO_DOCUMENT, functionValuesArray);
+        } catch (Exception exception) {
+            throw new ScriptException("Error evaluating " + compiledScript, exception);
+        }
+    }
+
+    @Override
+    public Object unwrap(Object value) {
+        return value;
+    }
+}
diff --git a/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionPlugin.java b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionPlugin.java
new file mode 100644
index 0000000..c1e2ed4
--- /dev/null
+++ b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionPlugin.java
@@ -0,0 +1,40 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.ScriptModule;
+
+public class ExpressionPlugin extends Plugin {
+
+    @Override
+    public String name() {
+        return "lang-expression";
+    }
+
+    @Override
+    public String description() {
+        return "Lucene expressions integration for Elasticsearch";
+    }
+
+    public void onModule(ScriptModule module) {
+        module.addScriptEngine(ExpressionScriptEngineService.class);
+    }
+}
diff --git a/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java
new file mode 100644
index 0000000..3762ffe
--- /dev/null
+++ b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java
@@ -0,0 +1,244 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import org.apache.lucene.expressions.Expression;
+import org.apache.lucene.expressions.SimpleBindings;
+import org.apache.lucene.expressions.js.JavascriptCompiler;
+import org.apache.lucene.expressions.js.VariableContext;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.valuesource.DoubleConstValueSource;
+import org.apache.lucene.search.SortField;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.fielddata.IndexFieldData;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.MapperService;
+import org.elasticsearch.index.mapper.core.DateFieldMapper;
+import org.elasticsearch.index.mapper.core.NumberFieldMapper;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.ScriptEngineService;
+import org.elasticsearch.script.ScriptException;
+import org.elasticsearch.script.SearchScript;
+import org.elasticsearch.search.MultiValueMode;
+import org.elasticsearch.search.lookup.SearchLookup;
+
+import java.text.ParseException;
+import java.util.Calendar;
+import java.util.Map;
+
+/**
+ * Provides the infrastructure for Lucene expressions as a scripting language for Elasticsearch.  Only
+ * {@link SearchScript}s are supported.
+ */
+public class ExpressionScriptEngineService extends AbstractComponent implements ScriptEngineService {
+
+    public static final String NAME = "expression";
+
+    protected static final String GET_YEAR_METHOD         = "getYear";
+    protected static final String GET_MONTH_METHOD        = "getMonth";
+    protected static final String GET_DAY_OF_MONTH_METHOD = "getDayOfMonth";
+    protected static final String GET_HOUR_OF_DAY_METHOD  = "getHourOfDay";
+    protected static final String GET_MINUTES_METHOD      = "getMinutes";
+    protected static final String GET_SECONDS_METHOD      = "getSeconds";
+
+    protected static final String MINIMUM_METHOD          = "min";
+    protected static final String MAXIMUM_METHOD          = "max";
+    protected static final String AVERAGE_METHOD          = "avg";
+    protected static final String MEDIAN_METHOD           = "median";
+    protected static final String SUM_METHOD              = "sum";
+    protected static final String COUNT_METHOD            = "count";
+
+    @Inject
+    public ExpressionScriptEngineService(Settings settings) {
+        super(settings);
+    }
+
+    @Override
+    public String[] types() {
+        return new String[]{NAME};
+    }
+
+    @Override
+    public String[] extensions() {
+        return new String[]{NAME};
+    }
+
+    @Override
+    public boolean sandboxed() {
+        return true;
+    }
+
+    @Override
+    public Object compile(String script) {
+        try {
+            // NOTE: validation is delayed to allow runtime vars, and we don't have access to per index stuff here
+            return JavascriptCompiler.compile(script);
+        } catch (ParseException e) {
+            throw new ScriptException("Failed to parse expression: " + script, e);
+        }
+    }
+
+    @Override
+    public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, @Nullable Map<String, Object> vars) {
+        try {
+            Expression expr = (Expression)compiledScript.compiled();
+            MapperService mapper = lookup.doc().mapperService();
+            // NOTE: if we need to do anything complicated with bindings in the future, we can just extend Bindings,
+            // instead of complicating SimpleBindings (which should stay simple)
+            SimpleBindings bindings = new SimpleBindings();
+            ReplaceableConstValueSource specialValue = null;
+
+            for (String variable : expr.variables) {
+                if (variable.equals("_score")) {
+                    bindings.add(new SortField("_score", SortField.Type.SCORE));
+                } else if (variable.equals("_value")) {
+                    specialValue = new ReplaceableConstValueSource();
+                    bindings.add("_value", specialValue);
+                    // noop: _value is special for aggregations, and is handled in ExpressionScriptBindings
+                    // TODO: if some uses it in a scoring expression, they will get a nasty failure when evaluating...need a
+                    // way to know this is for aggregations and so _value is ok to have...
+
+                } else if (vars != null && vars.containsKey(variable)) {
+                    // TODO: document and/or error if vars contains _score?
+                    // NOTE: by checking for the variable in vars first, it allows masking document fields with a global constant,
+                    // but if we were to reverse it, we could provide a way to supply dynamic defaults for documents missing the field?
+                    Object value = vars.get(variable);
+                    if (value instanceof Number) {
+                        bindings.add(variable, new DoubleConstValueSource(((Number) value).doubleValue()));
+                    } else {
+                        throw new ScriptException("Parameter [" + variable + "] must be a numeric type");
+                    }
+
+                } else {
+                    String fieldname = null;
+                    String methodname = null;
+                    VariableContext[] parts = VariableContext.parse(variable);
+                    if (parts[0].text.equals("doc") == false) {
+                        throw new ScriptException("Unknown variable [" + parts[0].text + "] in expression");
+                    }
+                    if (parts.length < 2 || parts[1].type != VariableContext.Type.STR_INDEX) {
+                        throw new ScriptException("Variable 'doc' in expression must be used with a specific field like: doc['myfield']");
+                    } else {
+                        fieldname = parts[1].text;
+                    }
+                    if (parts.length == 3) {
+                        if (parts[2].type == VariableContext.Type.METHOD) {
+                            methodname = parts[2].text;
+                        } else if (parts[2].type != VariableContext.Type.MEMBER || !"value".equals(parts[2].text)) {
+                            throw new ScriptException("Only the member variable [value] or member methods may be accessed on a field when not accessing the field directly");
+                        }
+                    }
+                    if (parts.length > 3) {
+                        throw new ScriptException("Variable [" + variable + "] does not follow an allowed format of either doc['field'] or doc['field'].method()");
+                    }
+
+                    MappedFieldType fieldType = mapper.smartNameFieldType(fieldname);
+
+                    if (fieldType == null) {
+                        throw new ScriptException("Field [" + fieldname + "] used in expression does not exist in mappings");
+                    }
+                    if (fieldType.isNumeric() == false) {
+                        // TODO: more context (which expression?)
+                        throw new ScriptException("Field [" + fieldname + "] used in expression must be numeric");
+                    }
+
+                    IndexFieldData<?> fieldData = lookup.doc().fieldDataService().getForField((NumberFieldMapper.NumberFieldType) fieldType);
+                    if (methodname == null) {
+                        bindings.add(variable, new FieldDataValueSource(fieldData, MultiValueMode.MIN));
+                    } else {
+                        bindings.add(variable, getMethodValueSource(fieldType, fieldData, fieldname, methodname));
+                    }
+                }
+            }
+
+            final boolean needsScores = expr.getSortField(bindings, false).needsScores();
+            return new ExpressionSearchScript(compiledScript, bindings, specialValue, needsScores);
+        } catch (Exception exception) {
+            throw new ScriptException("Error during search with " + compiledScript, exception);
+        }
+    }
+
+    protected ValueSource getMethodValueSource(MappedFieldType fieldType, IndexFieldData<?> fieldData, String fieldName, String methodName) {
+        switch (methodName) {
+            case GET_YEAR_METHOD:
+                return getDateMethodValueSource(fieldType, fieldData, fieldName, methodName, Calendar.YEAR);
+            case GET_MONTH_METHOD:
+                return getDateMethodValueSource(fieldType, fieldData, fieldName, methodName, Calendar.MONTH);
+            case GET_DAY_OF_MONTH_METHOD:
+                return getDateMethodValueSource(fieldType, fieldData, fieldName, methodName, Calendar.DAY_OF_MONTH);
+            case GET_HOUR_OF_DAY_METHOD:
+                return getDateMethodValueSource(fieldType, fieldData, fieldName, methodName, Calendar.HOUR_OF_DAY);
+            case GET_MINUTES_METHOD:
+                return getDateMethodValueSource(fieldType, fieldData, fieldName, methodName, Calendar.MINUTE);
+            case GET_SECONDS_METHOD:
+                return getDateMethodValueSource(fieldType, fieldData, fieldName, methodName, Calendar.SECOND);
+            case MINIMUM_METHOD:
+                return new FieldDataValueSource(fieldData, MultiValueMode.MIN);
+            case MAXIMUM_METHOD:
+                return new FieldDataValueSource(fieldData, MultiValueMode.MAX);
+            case AVERAGE_METHOD:
+                return new FieldDataValueSource(fieldData, MultiValueMode.AVG);
+            case MEDIAN_METHOD:
+                return new FieldDataValueSource(fieldData, MultiValueMode.MEDIAN);
+            case SUM_METHOD:
+                return new FieldDataValueSource(fieldData, MultiValueMode.SUM);
+            case COUNT_METHOD:
+                return new CountMethodValueSource(fieldData);
+            default:
+                throw new IllegalArgumentException("Member method [" + methodName + "] does not exist.");
+        }
+    }
+
+    protected ValueSource getDateMethodValueSource(MappedFieldType fieldType, IndexFieldData<?> fieldData, String fieldName, String methodName, int calendarType) {
+        if (!(fieldType instanceof DateFieldMapper.DateFieldType)) {
+            throw new IllegalArgumentException("Member method [" + methodName + "] can only be used with a date field type, not the field [" + fieldName + "].");
+        }
+
+        return new DateMethodValueSource(fieldData, MultiValueMode.MIN, methodName, calendarType);
+    }
+
+    @Override
+    public ExecutableScript executable(CompiledScript compiledScript, Map<String, Object> vars) {
+        return new ExpressionExecutableScript(compiledScript, vars);
+    }
+
+    @Override
+    public Object execute(CompiledScript compiledScript, Map<String, Object> vars) {
+        ExpressionExecutableScript expressionExecutableScript = new ExpressionExecutableScript(compiledScript, vars);
+        return expressionExecutableScript.run();
+    }
+
+    @Override
+    public Object unwrap(Object value) {
+        return value;
+    }
+
+    @Override
+    public void close() {}
+
+    @Override
+    public void scriptRemoved(CompiledScript script) {
+        // Nothing to do
+    }
+}
diff --git a/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionSearchScript.java b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionSearchScript.java
new file mode 100644
index 0000000..4926ceb
--- /dev/null
+++ b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionSearchScript.java
@@ -0,0 +1,131 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import org.apache.lucene.expressions.Bindings;
+import org.apache.lucene.expressions.Expression;
+import org.apache.lucene.expressions.SimpleBindings;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.search.Scorer;
+import org.elasticsearch.common.lucene.Lucene;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.LeafSearchScript;
+import org.elasticsearch.script.ScriptException;
+import org.elasticsearch.script.SearchScript;
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Map;
+
+/**
+ * A bridge to evaluate an {@link Expression} against {@link Bindings} in the context
+ * of a {@link SearchScript}.
+ */
+class ExpressionSearchScript implements SearchScript {
+
+    final CompiledScript compiledScript;
+    final SimpleBindings bindings;
+    final ValueSource source;
+    final ReplaceableConstValueSource specialValue; // _value
+    final boolean needsScores;
+    Scorer scorer;
+    int docid;
+
+    ExpressionSearchScript(CompiledScript c, SimpleBindings b, ReplaceableConstValueSource v, boolean needsScores) {
+        compiledScript = c;
+        bindings = b;
+        source = ((Expression)compiledScript.compiled()).getValueSource(bindings);
+        specialValue = v;
+        this.needsScores = needsScores;
+    }
+
+    @Override
+    public boolean needsScores() {
+        return needsScores;
+    }
+
+    @Override
+    public LeafSearchScript getLeafSearchScript(final LeafReaderContext leaf) throws IOException {
+        return new LeafSearchScript() {
+
+            FunctionValues values = source.getValues(Collections.singletonMap("scorer", Lucene.illegalScorer("Scores are not available in the current context")), leaf);
+
+            double evaluate() {
+                try {
+                    return values.doubleVal(docid);
+                } catch (Exception exception) {
+                    throw new ScriptException("Error evaluating " + compiledScript, exception);
+                }
+            }
+
+            @Override
+            public Object run() { return new Double(evaluate()); }
+
+            @Override
+            public float runAsFloat() { return (float)evaluate();}
+
+            @Override
+            public long runAsLong() { return (long)evaluate(); }
+
+            @Override
+            public double runAsDouble() { return evaluate(); }
+
+            @Override
+            public Object unwrap(Object value) { return value; }
+
+            @Override
+            public void setDocument(int d) {
+                docid = d;
+            }
+
+            @Override
+            public void setScorer(Scorer s) {
+                scorer = s;
+                try {
+                    // We have a new binding for the scorer so we need to reset the values
+                    values = source.getValues(Collections.singletonMap("scorer", scorer), leaf);
+                } catch (IOException e) {
+                    throw new IllegalStateException("Can't get values using " + compiledScript, e);
+                }
+            }
+
+            @Override
+            public void setSource(Map<String, Object> source) {
+                // noop: expressions don't use source data
+            }
+
+            @Override
+            public void setNextVar(String name, Object value) {
+                assert(specialValue != null);
+                // this should only be used for the special "_value" variable used in aggregations
+                assert(name.equals("_value"));
+
+                if (value instanceof Number) {
+                    specialValue.setValue(((Number)value).doubleValue());
+                } else {
+                    throw new ScriptException("Cannot use expression with text variable using " + compiledScript);
+                }
+            }
+        };
+    }
+
+}
diff --git a/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/FieldDataFunctionValues.java b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/FieldDataFunctionValues.java
new file mode 100644
index 0000000..b3e06d6
--- /dev/null
+++ b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/FieldDataFunctionValues.java
@@ -0,0 +1,43 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
+import org.elasticsearch.index.fielddata.AtomicNumericFieldData;
+import org.elasticsearch.index.fielddata.NumericDoubleValues;
+import org.elasticsearch.search.MultiValueMode;
+
+/**
+ * A {@link org.apache.lucene.queries.function.FunctionValues} which wrap field data.
+ */
+class FieldDataFunctionValues extends DoubleDocValues {
+    NumericDoubleValues dataAccessor;
+
+    FieldDataFunctionValues(ValueSource parent, MultiValueMode m, AtomicNumericFieldData d) {
+        super(parent);
+        dataAccessor = m.select(d.getDoubleValues(), 0d);
+    }
+
+    @Override
+    public double doubleVal(int i) {
+        return dataAccessor.get(i);
+    }
+}
diff --git a/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/FieldDataValueSource.java b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/FieldDataValueSource.java
new file mode 100644
index 0000000..39386ee
--- /dev/null
+++ b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/FieldDataValueSource.java
@@ -0,0 +1,80 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.elasticsearch.index.fielddata.AtomicFieldData;
+import org.elasticsearch.index.fielddata.AtomicNumericFieldData;
+import org.elasticsearch.index.fielddata.IndexFieldData;
+import org.elasticsearch.search.MultiValueMode;
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.Objects;
+
+/**
+ * A {@link ValueSource} wrapper for field data.
+ */
+class FieldDataValueSource extends ValueSource {
+
+    protected IndexFieldData<?> fieldData;
+    protected MultiValueMode multiValueMode;
+
+    protected FieldDataValueSource(IndexFieldData<?> d, MultiValueMode m) {
+        Objects.requireNonNull(d);
+        Objects.requireNonNull(m);
+
+        fieldData = d;
+        multiValueMode = m;
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) return true;
+        if (o == null || getClass() != o.getClass()) return false;
+
+        FieldDataValueSource that = (FieldDataValueSource) o;
+
+        if (!fieldData.equals(that.fieldData)) return false;
+        return multiValueMode == that.multiValueMode;
+
+    }
+
+    @Override
+    public int hashCode() {
+        int result = fieldData.hashCode();
+        result = 31 * result + multiValueMode.hashCode();
+        return result;
+    }
+
+    @Override
+    public FunctionValues getValues(Map context, LeafReaderContext leaf) throws IOException {
+        AtomicFieldData leafData = fieldData.load(leaf);
+        assert(leafData instanceof AtomicNumericFieldData);
+        return new FieldDataFunctionValues(this, multiValueMode, (AtomicNumericFieldData)leafData);
+    }
+
+    @Override
+    public String description() {
+        return "field(" + fieldData.getFieldNames().toString() + ")";
+    }
+}
diff --git a/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ReplaceableConstFunctionValues.java b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ReplaceableConstFunctionValues.java
new file mode 100644
index 0000000..b3ebfab
--- /dev/null
+++ b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ReplaceableConstFunctionValues.java
@@ -0,0 +1,44 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import org.apache.lucene.queries.function.FunctionValues;
+
+/**
+ * A support class for an executable expression script that allows the double returned
+ * by a {@link FunctionValues} to be modified.
+ */
+public class ReplaceableConstFunctionValues extends FunctionValues {
+    private double value = 0;
+
+    public void setValue(double value) {
+        this.value = value;
+    }
+
+    @Override
+    public double doubleVal(int doc) {
+        return value;
+    }
+
+    @Override
+    public String toString(int i) {
+        return "ReplaceableConstFunctionValues: " + value;
+    }
+}
diff --git a/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ReplaceableConstValueSource.java b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ReplaceableConstValueSource.java
new file mode 100644
index 0000000..c9bcc23
--- /dev/null
+++ b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ReplaceableConstValueSource.java
@@ -0,0 +1,62 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * A {@link ValueSource} which has a stub {@link FunctionValues} that holds a dynamically replaceable constant double.
+ */
+class ReplaceableConstValueSource extends ValueSource {
+    final ReplaceableConstFunctionValues fv;
+
+    public ReplaceableConstValueSource() {
+        fv = new ReplaceableConstFunctionValues();
+    }
+
+    @Override
+    public FunctionValues getValues(Map map, LeafReaderContext atomicReaderContext) throws IOException {
+        return fv;
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        return o == this;
+    }
+
+    @Override
+    public int hashCode() {
+        return System.identityHashCode(this);
+    }
+
+    @Override
+    public String description() {
+        return "replaceableConstDouble";
+    }
+
+    public void setValue(double v) {
+        fv.setValue(v);
+    }
+}
diff --git a/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/ExpressionRestIT.java b/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/ExpressionRestIT.java
new file mode 100644
index 0000000..8dcefce
--- /dev/null
+++ b/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/ExpressionRestIT.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+import java.util.Collection;
+
+public class ExpressionRestIT extends ESRestTestCase {
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(ExpressionPlugin.class);
+    }
+
+    public ExpressionRestIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+}
+
diff --git a/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/ExpressionTests.java b/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/ExpressionTests.java
new file mode 100644
index 0000000..b05b963
--- /dev/null
+++ b/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/ExpressionTests.java
@@ -0,0 +1,57 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.IndexService;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ScriptService.ScriptType;
+import org.elasticsearch.script.SearchScript;
+import org.elasticsearch.search.lookup.SearchLookup;
+import org.elasticsearch.test.ESSingleNodeTestCase;
+
+import java.util.Collections;
+
+public class ExpressionTests extends ESSingleNodeTestCase {
+
+    public void testNeedsScores() {
+        IndexService index = createIndex("test", Settings.EMPTY, "type", "d", "type=double");
+        
+        ExpressionScriptEngineService service = new ExpressionScriptEngineService(Settings.EMPTY);
+        SearchLookup lookup = new SearchLookup(index.mapperService(), index.fieldData(), null);
+
+        Object compiled = service.compile("1.2");
+        SearchScript ss = service.search(new CompiledScript(ScriptType.INLINE, "randomName", "expression", compiled), lookup, Collections.<String, Object>emptyMap());
+        assertFalse(ss.needsScores());
+
+        compiled = service.compile("doc['d'].value");
+        ss = service.search(new CompiledScript(ScriptType.INLINE, "randomName", "expression", compiled), lookup, Collections.<String, Object>emptyMap());
+        assertFalse(ss.needsScores());
+
+        compiled = service.compile("1/_score");
+        ss = service.search(new CompiledScript(ScriptType.INLINE, "randomName", "expression", compiled), lookup, Collections.<String, Object>emptyMap());
+        assertTrue(ss.needsScores());
+
+        compiled = service.compile("doc['d'].value * _score");
+        ss = service.search(new CompiledScript(ScriptType.INLINE, "randomName", "expression", compiled), lookup, Collections.<String, Object>emptyMap());
+        assertTrue(ss.needsScores());
+    }
+
+}
diff --git a/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/IndexedExpressionTests.java b/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/IndexedExpressionTests.java
new file mode 100644
index 0000000..b91450f
--- /dev/null
+++ b/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/IndexedExpressionTests.java
@@ -0,0 +1,84 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Collections;
+
+import static org.hamcrest.Matchers.containsString;
+
+//TODO: please convert to unit tests!
+public class IndexedExpressionTests extends ESIntegTestCase {
+    
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        Settings.Builder builder = Settings.builder().put(super.nodeSettings(nodeOrdinal));
+        builder.put("script.engine.expression.indexed.update", "off");
+        builder.put("script.engine.expression.indexed.search", "off");
+        builder.put("script.engine.expression.indexed.aggs", "off");
+        builder.put("script.engine.expression.indexed.mapping", "off");
+        return builder.build();
+    }
+    
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return Collections.singleton(ExpressionPlugin.class);
+    }
+
+    @Test
+    public void testAllOpsDisabledIndexedScripts() throws IOException {
+        if (randomBoolean()) {
+            client().preparePutIndexedScript(ExpressionScriptEngineService.NAME, "script1", "{\"script\":\"2\"}").get();
+        } else {
+            client().prepareIndex(ScriptService.SCRIPT_INDEX, ExpressionScriptEngineService.NAME, "script1").setSource("{\"script\":\"2\"}").get();
+        }
+        client().prepareIndex("test", "scriptTest", "1").setSource("{\"theField\":\"foo\"}").get();
+        try {
+            client().prepareUpdate("test", "scriptTest", "1")
+                    .setScript(new Script("script1", ScriptService.ScriptType.INDEXED, ExpressionScriptEngineService.NAME, null)).get();
+            fail("update script should have been rejected");
+        } catch(Exception e) {
+            assertThat(e.getMessage(), containsString("failed to execute script"));
+            assertThat(e.getCause().getMessage(), containsString("scripts of type [indexed], operation [update] and lang [expression] are disabled"));
+        }
+        try {
+            String query = "{ \"script_fields\" : { \"test1\" : { \"script_id\" : \"script1\", \"lang\":\"expression\" }}}";
+            client().prepareSearch().setSource(new BytesArray(query)).setIndices("test").setTypes("scriptTest").get();
+            fail("search script should have been rejected");
+        } catch(Exception e) {
+            assertThat(e.toString(), containsString("scripts of type [indexed], operation [search] and lang [expression] are disabled"));
+        }
+        try {
+            String source = "{\"aggs\": {\"test\": { \"terms\" : { \"script_id\":\"script1\", \"script_lang\":\"expression\" } } } }";
+            client().prepareSearch("test").setSource(new BytesArray(source)).get();
+        } catch(Exception e) {
+            assertThat(e.toString(), containsString("scripts of type [indexed], operation [aggs] and lang [expression] are disabled"));
+        }
+    }
+}
diff --git a/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java b/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java
new file mode 100644
index 0000000..5966c1d
--- /dev/null
+++ b/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java
@@ -0,0 +1,577 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import org.apache.lucene.expressions.Expression;
+import org.apache.lucene.expressions.js.JavascriptCompiler;
+import org.elasticsearch.ExceptionsHelper;
+import org.elasticsearch.action.search.SearchPhaseExecutionException;
+import org.elasticsearch.action.search.SearchRequestBuilder;
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.action.search.SearchType;
+import org.elasticsearch.action.update.UpdateRequestBuilder;
+import org.elasticsearch.common.lucene.search.function.CombineFunction;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.index.query.QueryBuilders;
+import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
+import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.ScriptException;
+import org.elasticsearch.script.ScriptService.ScriptType;
+import org.elasticsearch.search.SearchHits;
+import org.elasticsearch.search.aggregations.AggregationBuilders;
+import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
+import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
+import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.Bucket;
+import org.elasticsearch.search.aggregations.metrics.stats.Stats;
+import org.elasticsearch.search.aggregations.pipeline.SimpleValue;
+import org.elasticsearch.search.sort.SortBuilders;
+import org.elasticsearch.search.sort.SortOrder;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.test.hamcrest.ElasticsearchAssertions;
+
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.elasticsearch.search.aggregations.AggregationBuilders.histogram;
+import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders.bucketScript;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.notNullValue;
+
+// TODO: please convert to unit tests!
+public class MoreExpressionTests extends ESIntegTestCase {
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return Collections.singleton(ExpressionPlugin.class);
+    }
+
+    private SearchRequestBuilder buildRequest(String script, Object... params) {
+        ensureGreen("test");
+
+        Map<String, Object> paramsMap = new HashMap<>();
+        assert (params.length % 2 == 0);
+        for (int i = 0; i < params.length; i += 2) {
+            paramsMap.put(params[i].toString(), params[i + 1]);
+        }
+
+        SearchRequestBuilder req = client().prepareSearch().setIndices("test");
+        req.setQuery(QueryBuilders.matchAllQuery())
+                .addSort(SortBuilders.fieldSort("_uid")
+                        .order(SortOrder.ASC))
+                .addScriptField("foo", new Script(script, ScriptType.INLINE, "expression", paramsMap));
+        return req;
+    }
+
+    public void testBasic() throws Exception {
+        createIndex("test");
+        ensureGreen("test");
+        client().prepareIndex("test", "doc", "1").setSource("foo", 4).setRefresh(true).get();
+        SearchResponse rsp = buildRequest("doc['foo'] + 1").get();
+        assertEquals(1, rsp.getHits().getTotalHits());
+        assertEquals(5.0, rsp.getHits().getAt(0).field("foo").getValue(), 0.0D);
+    }
+
+    public void testBasicUsingDotValue() throws Exception {
+        createIndex("test");
+        ensureGreen("test");
+        client().prepareIndex("test", "doc", "1").setSource("foo", 4).setRefresh(true).get();
+        SearchResponse rsp = buildRequest("doc['foo'].value + 1").get();
+        assertEquals(1, rsp.getHits().getTotalHits());
+        assertEquals(5.0, rsp.getHits().getAt(0).field("foo").getValue(), 0.0D);
+    }
+
+    public void testScore() throws Exception {
+        createIndex("test");
+        ensureGreen("test");
+        indexRandom(true,
+                client().prepareIndex("test", "doc", "1").setSource("text", "hello goodbye"),
+                client().prepareIndex("test", "doc", "2").setSource("text", "hello hello hello goodbye"),
+                client().prepareIndex("test", "doc", "3").setSource("text", "hello hello goodebye"));
+        ScoreFunctionBuilder score = ScoreFunctionBuilders.scriptFunction(new Script("1 / _score", ScriptType.INLINE, "expression", null));
+        SearchRequestBuilder req = client().prepareSearch().setIndices("test");
+        req.setQuery(QueryBuilders.functionScoreQuery(QueryBuilders.termQuery("text", "hello"), score).boostMode(CombineFunction.REPLACE));
+        req.setSearchType(SearchType.DFS_QUERY_THEN_FETCH); // make sure DF is consistent
+        SearchResponse rsp = req.get();
+        assertSearchResponse(rsp);
+        SearchHits hits = rsp.getHits();
+        assertEquals(3, hits.getTotalHits());
+        assertEquals("1", hits.getAt(0).getId());
+        assertEquals("3", hits.getAt(1).getId());
+        assertEquals("2", hits.getAt(2).getId());
+    }
+
+    public void testDateMethods() throws Exception {
+        ElasticsearchAssertions.assertAcked(prepareCreate("test").addMapping("doc", "date0", "type=date", "date1", "type=date"));
+        ensureGreen("test");
+        indexRandom(true,
+                client().prepareIndex("test", "doc", "1").setSource("date0", "2015-04-28T04:02:07Z", "date1", "1985-09-01T23:11:01Z"),
+                client().prepareIndex("test", "doc", "2").setSource("date0", "2013-12-25T11:56:45Z", "date1", "1983-10-13T23:15:00Z"));
+        SearchResponse rsp = buildRequest("doc['date0'].getSeconds() - doc['date0'].getMinutes()").get();
+        assertEquals(2, rsp.getHits().getTotalHits());
+        SearchHits hits = rsp.getHits();
+        assertEquals(5.0, hits.getAt(0).field("foo").getValue(), 0.0D);
+        assertEquals(-11.0, hits.getAt(1).field("foo").getValue(), 0.0D);
+        rsp = buildRequest("doc['date0'].getHourOfDay() + doc['date1'].getDayOfMonth()").get();
+        assertEquals(2, rsp.getHits().getTotalHits());
+        hits = rsp.getHits();
+        assertEquals(5.0, hits.getAt(0).field("foo").getValue(), 0.0D);
+        assertEquals(24.0, hits.getAt(1).field("foo").getValue(), 0.0D);
+        rsp = buildRequest("doc['date1'].getMonth() + 1").get();
+        assertEquals(2, rsp.getHits().getTotalHits());
+        hits = rsp.getHits();
+        assertEquals(9.0, hits.getAt(0).field("foo").getValue(), 0.0D);
+        assertEquals(10.0, hits.getAt(1).field("foo").getValue(), 0.0D);
+        rsp = buildRequest("doc['date1'].getYear()").get();
+        assertEquals(2, rsp.getHits().getTotalHits());
+        hits = rsp.getHits();
+        assertEquals(1985.0, hits.getAt(0).field("foo").getValue(), 0.0D);
+        assertEquals(1983.0, hits.getAt(1).field("foo").getValue(), 0.0D);
+    }
+
+    public void testMultiValueMethods() throws Exception {
+        ElasticsearchAssertions.assertAcked(prepareCreate("test").addMapping("doc", "double0", "type=double", "double1", "type=double"));
+        ensureGreen("test");
+        indexRandom(true,
+                client().prepareIndex("test", "doc", "1").setSource("double0", "5.0", "double0", "1.0", "double0", "1.5", "double1", "1.2", "double1", "2.4"),
+                client().prepareIndex("test", "doc", "2").setSource("double0", "5.0", "double1", "3.0"),
+                client().prepareIndex("test", "doc", "3").setSource("double0", "5.0", "double0", "1.0", "double0", "1.5", "double0", "-1.5", "double1", "4.0"));
+
+
+        SearchResponse rsp = buildRequest("doc['double0'].count() + doc['double1'].count()").get();
+        assertSearchResponse(rsp);
+        SearchHits hits = rsp.getHits();
+        assertEquals(3, hits.getTotalHits());
+        assertEquals(5.0, hits.getAt(0).field("foo").getValue(), 0.0D);
+        assertEquals(2.0, hits.getAt(1).field("foo").getValue(), 0.0D);
+        assertEquals(5.0, hits.getAt(2).field("foo").getValue(), 0.0D);
+
+        rsp = buildRequest("doc['double0'].sum()").get();
+        assertSearchResponse(rsp);
+        hits = rsp.getHits();
+        assertEquals(3, hits.getTotalHits());
+        assertEquals(7.5, hits.getAt(0).field("foo").getValue(), 0.0D);
+        assertEquals(5.0, hits.getAt(1).field("foo").getValue(), 0.0D);
+        assertEquals(6.0, hits.getAt(2).field("foo").getValue(), 0.0D);
+
+        rsp = buildRequest("doc['double0'].avg() + doc['double1'].avg()").get();
+        assertSearchResponse(rsp);
+        hits = rsp.getHits();
+        assertEquals(3, hits.getTotalHits());
+        assertEquals(4.3, hits.getAt(0).field("foo").getValue(), 0.0D);
+        assertEquals(8.0, hits.getAt(1).field("foo").getValue(), 0.0D);
+        assertEquals(5.5, hits.getAt(2).field("foo").getValue(), 0.0D);
+
+        rsp = buildRequest("doc['double0'].median()").get();
+        assertSearchResponse(rsp);
+        hits = rsp.getHits();
+        assertEquals(3, hits.getTotalHits());
+        assertEquals(1.5, hits.getAt(0).field("foo").getValue(), 0.0D);
+        assertEquals(5.0, hits.getAt(1).field("foo").getValue(), 0.0D);
+        assertEquals(1.25, hits.getAt(2).field("foo").getValue(), 0.0D);
+
+        rsp = buildRequest("doc['double0'].min()").get();
+        assertSearchResponse(rsp);
+        hits = rsp.getHits();
+        assertEquals(3, hits.getTotalHits());
+        assertEquals(1.0, hits.getAt(0).field("foo").getValue(), 0.0D);
+        assertEquals(5.0, hits.getAt(1).field("foo").getValue(), 0.0D);
+        assertEquals(-1.5, hits.getAt(2).field("foo").getValue(), 0.0D);
+
+        rsp = buildRequest("doc['double0'].max()").get();
+        assertSearchResponse(rsp);
+        hits = rsp.getHits();
+        assertEquals(3, hits.getTotalHits());
+        assertEquals(5.0, hits.getAt(0).field("foo").getValue(), 0.0D);
+        assertEquals(5.0, hits.getAt(1).field("foo").getValue(), 0.0D);
+        assertEquals(5.0, hits.getAt(2).field("foo").getValue(), 0.0D);
+
+        rsp = buildRequest("doc['double0'].sum()/doc['double0'].count()").get();
+        assertSearchResponse(rsp);
+        hits = rsp.getHits();
+        assertEquals(3, hits.getTotalHits());
+        assertEquals(2.5, hits.getAt(0).field("foo").getValue(), 0.0D);
+        assertEquals(5.0, hits.getAt(1).field("foo").getValue(), 0.0D);
+        assertEquals(1.5, hits.getAt(2).field("foo").getValue(), 0.0D);
+    }
+
+    public void testInvalidDateMethodCall() throws Exception {
+        ElasticsearchAssertions.assertAcked(prepareCreate("test").addMapping("doc", "double", "type=double"));
+        ensureGreen("test");
+        indexRandom(true, client().prepareIndex("test", "doc", "1").setSource("double", "178000000.0"));
+        try {
+            buildRequest("doc['double'].getYear()").get();
+            fail();
+        } catch (SearchPhaseExecutionException e) {
+            assertThat(e.toString() + "should have contained IllegalArgumentException",
+                    e.toString().contains("IllegalArgumentException"), equalTo(true));
+            assertThat(e.toString() + "should have contained can only be used with a date field type",
+                    e.toString().contains("can only be used with a date field type"), equalTo(true));
+        }
+    }
+
+    public void testSparseField() throws Exception {
+        ElasticsearchAssertions.assertAcked(prepareCreate("test").addMapping("doc", "x", "type=long", "y", "type=long"));
+        ensureGreen("test");
+        indexRandom(true,
+                client().prepareIndex("test", "doc", "1").setSource("x", 4),
+                client().prepareIndex("test", "doc", "2").setSource("y", 2));
+        SearchResponse rsp = buildRequest("doc['x'] + 1").get();
+        ElasticsearchAssertions.assertSearchResponse(rsp);
+        SearchHits hits = rsp.getHits();
+        assertEquals(2, rsp.getHits().getTotalHits());
+        assertEquals(5.0, hits.getAt(0).field("foo").getValue(), 0.0D);
+        assertEquals(1.0, hits.getAt(1).field("foo").getValue(), 0.0D);
+    }
+
+    public void testMissingField() throws Exception {
+        createIndex("test");
+        ensureGreen("test");
+        client().prepareIndex("test", "doc", "1").setSource("x", 4).setRefresh(true).get();
+        try {
+            buildRequest("doc['bogus']").get();
+            fail("Expected missing field to cause failure");
+        } catch (SearchPhaseExecutionException e) {
+            assertThat(e.toString() + "should have contained ScriptException",
+                    e.toString().contains("ScriptException"), equalTo(true));
+            assertThat(e.toString() + "should have contained missing field error",
+                    e.toString().contains("does not exist in mappings"), equalTo(true));
+        }
+    }
+
+    public void testParams() throws Exception {
+        createIndex("test");
+        ensureGreen("test");
+        indexRandom(true,
+                client().prepareIndex("test", "doc", "1").setSource("x", 10),
+                client().prepareIndex("test", "doc", "2").setSource("x", 3),
+                client().prepareIndex("test", "doc", "3").setSource("x", 5));
+        // a = int, b = double, c = long
+        String script = "doc['x'] * a + b + ((c + doc['x']) > 5000000009 ? 1 : 0)";
+        SearchResponse rsp = buildRequest(script, "a", 2, "b", 3.5, "c", 5000000000L).get();
+        SearchHits hits = rsp.getHits();
+        assertEquals(3, hits.getTotalHits());
+        assertEquals(24.5, hits.getAt(0).field("foo").getValue(), 0.0D);
+        assertEquals(9.5, hits.getAt(1).field("foo").getValue(), 0.0D);
+        assertEquals(13.5, hits.getAt(2).field("foo").getValue(), 0.0D);
+    }
+
+    public void testCompileFailure() {
+        client().prepareIndex("test", "doc", "1").setSource("x", 1).setRefresh(true).get();
+        try {
+            buildRequest("garbage%@#%@").get();
+            fail("Expected expression compilation failure");
+        } catch (SearchPhaseExecutionException e) {
+            assertThat(e.toString() + "should have contained ScriptException",
+                    e.toString().contains("ScriptException"), equalTo(true));
+            assertThat(e.toString() + "should have contained compilation failure",
+                    e.toString().contains("Failed to parse expression"), equalTo(true));
+        }
+    }
+
+    public void testNonNumericParam() {
+        client().prepareIndex("test", "doc", "1").setSource("x", 1).setRefresh(true).get();
+        try {
+            buildRequest("a", "a", "astring").get();
+            fail("Expected string parameter to cause failure");
+        } catch (SearchPhaseExecutionException e) {
+            assertThat(e.toString() + "should have contained ScriptException",
+                    e.toString().contains("ScriptException"), equalTo(true));
+            assertThat(e.toString() + "should have contained non-numeric parameter error",
+                    e.toString().contains("must be a numeric type"), equalTo(true));
+        }
+    }
+
+    public void testNonNumericField() {
+        client().prepareIndex("test", "doc", "1").setSource("text", "this is not a number").setRefresh(true).get();
+        try {
+            buildRequest("doc['text']").get();
+            fail("Expected text field to cause execution failure");
+        } catch (SearchPhaseExecutionException e) {
+            assertThat(e.toString() + "should have contained ScriptException",
+                    e.toString().contains("ScriptException"), equalTo(true));
+            assertThat(e.toString() + "should have contained non-numeric field error",
+                    e.toString().contains("must be numeric"), equalTo(true));
+        }
+    }
+
+    public void testInvalidGlobalVariable() {
+        client().prepareIndex("test", "doc", "1").setSource("foo", 5).setRefresh(true).get();
+        try {
+            buildRequest("bogus").get();
+            fail("Expected bogus variable to cause execution failure");
+        } catch (SearchPhaseExecutionException e) {
+            assertThat(e.toString() + "should have contained ScriptException",
+                    e.toString().contains("ScriptException"), equalTo(true));
+            assertThat(e.toString() + "should have contained unknown variable error",
+                    e.toString().contains("Unknown variable"), equalTo(true));
+        }
+    }
+
+    public void testDocWithoutField() {
+        client().prepareIndex("test", "doc", "1").setSource("foo", 5).setRefresh(true).get();
+        try {
+            buildRequest("doc").get();
+            fail("Expected doc variable without field to cause execution failure");
+        } catch (SearchPhaseExecutionException e) {
+            assertThat(e.toString() + "should have contained ScriptException",
+                    e.toString().contains("ScriptException"), equalTo(true));
+            assertThat(e.toString() + "should have contained a missing specific field error",
+                    e.toString().contains("must be used with a specific field"), equalTo(true));
+        }
+    }
+
+    public void testInvalidFieldMember() {
+        client().prepareIndex("test", "doc", "1").setSource("foo", 5).setRefresh(true).get();
+        try {
+            buildRequest("doc['foo'].bogus").get();
+            fail("Expected bogus field member to cause execution failure");
+        } catch (SearchPhaseExecutionException e) {
+            assertThat(e.toString() + "should have contained ScriptException",
+                    e.toString().contains("ScriptException"), equalTo(true));
+            assertThat(e.toString() + "should have contained member variable [value] or member methods may be accessed",
+                    e.toString().contains("member variable [value] or member methods may be accessed"), equalTo(true));
+        }
+    }
+
+    public void testSpecialValueVariable() throws Exception {
+        // i.e. _value for aggregations
+        createIndex("test");
+        ensureGreen("test");
+        indexRandom(true,
+                client().prepareIndex("test", "doc", "1").setSource("x", 5, "y", 1.2),
+                client().prepareIndex("test", "doc", "2").setSource("x", 10, "y", 1.4),
+                client().prepareIndex("test", "doc", "3").setSource("x", 13, "y", 1.8));
+
+        SearchRequestBuilder req = client().prepareSearch().setIndices("test");
+        req.setQuery(QueryBuilders.matchAllQuery())
+                .addAggregation(
+                        AggregationBuilders.stats("int_agg").field("x")
+                                .script(new Script("_value * 3", ScriptType.INLINE, ExpressionScriptEngineService.NAME, null)))
+                .addAggregation(
+                        AggregationBuilders.stats("double_agg").field("y")
+                                .script(new Script("_value - 1.1", ScriptType.INLINE, ExpressionScriptEngineService.NAME, null)));
+
+        SearchResponse rsp = req.get();
+        assertEquals(3, rsp.getHits().getTotalHits());
+
+        Stats stats = rsp.getAggregations().get("int_agg");
+        assertEquals(39.0, stats.getMax(), 0.0001);
+        assertEquals(15.0, stats.getMin(), 0.0001);
+
+        stats = rsp.getAggregations().get("double_agg");
+        assertEquals(0.7, stats.getMax(), 0.0001);
+        assertEquals(0.1, stats.getMin(), 0.0001);
+    }
+
+    public void testStringSpecialValueVariable() throws Exception {
+        // i.e. expression script for term aggregations, which is not allowed
+        createIndex("test");
+        ensureGreen("test");
+        indexRandom(true,
+                client().prepareIndex("test", "doc", "1").setSource("text", "hello"),
+                client().prepareIndex("test", "doc", "2").setSource("text", "goodbye"),
+                client().prepareIndex("test", "doc", "3").setSource("text", "hello"));
+
+        SearchRequestBuilder req = client().prepareSearch().setIndices("test");
+        req.setQuery(QueryBuilders.matchAllQuery())
+                .addAggregation(
+                        AggregationBuilders.terms("term_agg").field("text")
+                                .script(new Script("_value", ScriptType.INLINE, ExpressionScriptEngineService.NAME, null)));
+
+        String message;
+        try {
+            // shards that don't have docs with the "text" field will not fail,
+            // so we may or may not get a total failure
+            SearchResponse rsp = req.get();
+            assertThat(rsp.getShardFailures().length, greaterThan(0)); // at least the shards containing the docs should have failed
+            message = rsp.getShardFailures()[0].reason();
+        } catch (SearchPhaseExecutionException e) {
+            message = e.toString();
+        }
+        assertThat(message + "should have contained ScriptException",
+                message.contains("ScriptException"), equalTo(true));
+        assertThat(message + "should have contained text variable error",
+                message.contains("text variable"), equalTo(true));
+    }
+
+    // series of unit test for using expressions as executable scripts
+    public void testExecutableScripts() throws Exception {
+        Map<String, Object> vars = new HashMap<>();
+        vars.put("a", 2.5);
+        vars.put("b", 3);
+        vars.put("xyz", -1);
+
+        Expression expr = JavascriptCompiler.compile("a+b+xyz");
+        CompiledScript compiledScript = new CompiledScript(ScriptType.INLINE, "", "expression", expr);
+
+        ExpressionExecutableScript ees = new ExpressionExecutableScript(compiledScript, vars);
+        assertEquals((Double) ees.run(), 4.5, 0.001);
+
+        ees.setNextVar("b", -2.5);
+        assertEquals((Double) ees.run(), -1, 0.001);
+
+        ees.setNextVar("a", -2.5);
+        ees.setNextVar("b", -2.5);
+        ees.setNextVar("xyz", -2.5);
+        assertEquals((Double) ees.run(), -7.5, 0.001);
+
+        String message;
+
+        try {
+            vars = new HashMap<>();
+            vars.put("a", 1);
+            ees = new ExpressionExecutableScript(compiledScript, vars);
+            ees.run();
+            fail("An incorrect number of variables were allowed to be used in an expression.");
+        } catch (ScriptException se) {
+            message = se.getMessage();
+            assertThat(message + " should have contained number of variables", message.contains("number of variables"), equalTo(true));
+        }
+
+        try {
+            vars = new HashMap<>();
+            vars.put("a", 1);
+            vars.put("b", 3);
+            vars.put("c", -1);
+            ees = new ExpressionExecutableScript(compiledScript, vars);
+            ees.run();
+            fail("A variable was allowed to be set that does not exist in the expression.");
+        } catch (ScriptException se) {
+            message = se.getMessage();
+            assertThat(message + " should have contained does not exist in", message.contains("does not exist in"), equalTo(true));
+        }
+
+        try {
+            vars = new HashMap<>();
+            vars.put("a", 1);
+            vars.put("b", 3);
+            vars.put("xyz", "hello");
+            ees = new ExpressionExecutableScript(compiledScript, vars);
+            ees.run();
+            fail("A non-number was allowed to be use in the expression.");
+        } catch (ScriptException se) {
+            message = se.getMessage();
+            assertThat(message + " should have contained process numbers", message.contains("process numbers"), equalTo(true));
+        }
+
+    }
+
+    // test to make sure expressions are not allowed to be used as update scripts
+    public void testInvalidUpdateScript() throws Exception {
+        try {
+            createIndex("test_index");
+            ensureGreen("test_index");
+            indexRandom(true, client().prepareIndex("test_index", "doc", "1").setSource("text_field", "text"));
+            UpdateRequestBuilder urb = client().prepareUpdate().setIndex("test_index");
+            urb.setType("doc");
+            urb.setId("1");
+            urb.setScript(new Script("0", ScriptType.INLINE, ExpressionScriptEngineService.NAME, null));
+            urb.get();
+            fail("Expression scripts should not be allowed to run as update scripts.");
+        } catch (Exception e) {
+            String message = e.getMessage();
+            assertThat(message + " should have contained failed to execute", message.contains("failed to execute"), equalTo(true));
+            message = e.getCause().getMessage();
+            assertThat(message + " should have contained not supported", message.contains("not supported"), equalTo(true));
+        }
+    }
+
+    // test to make sure expressions are not allowed to be used as mapping scripts
+    public void testInvalidMappingScript() throws Exception{
+        try {
+            createIndex("test_index");
+            ensureGreen("test_index");
+            XContentBuilder builder = XContentFactory.jsonBuilder().startObject();
+            builder.startObject("transform");
+            builder.field("script", "1.0");
+            builder.field("lang", ExpressionScriptEngineService.NAME);
+            builder.endObject();
+            builder.startObject("properties");
+            builder.startObject("double_field");
+            builder.field("type", "double");
+            builder.endObject();
+            builder.endObject();
+            builder.endObject();
+            client().admin().indices().preparePutMapping("test_index").setType("trans_test").setSource(builder).get();
+            client().prepareIndex("test_index", "trans_test", "1").setSource("double_field", 0.0).get();
+            fail("Expression scripts should not be allowed to run as mapping scripts.");
+        } catch (Exception e) {
+            String message = ExceptionsHelper.detailedMessage(e);
+            assertThat(message + " should have contained failed to parse", message.contains("failed to parse"), equalTo(true));
+            assertThat(message + " should have contained not supported", message.contains("not supported"), equalTo(true));
+        }
+    }
+
+    // test to make sure expressions are allowed to be used for reduce in pipeline aggregations
+    public void testPipelineAggregationScript() throws Exception {
+        createIndex("agg_index");
+        ensureGreen("agg_index");
+        indexRandom(true,
+                client().prepareIndex("agg_index", "doc", "1").setSource("one", 1.0, "two", 2.0, "three", 3.0, "four", 4.0),
+                client().prepareIndex("agg_index", "doc", "2").setSource("one", 2.0, "two", 2.0, "three", 3.0, "four", 4.0),
+                client().prepareIndex("agg_index", "doc", "3").setSource("one", 3.0, "two", 2.0, "three", 3.0, "four", 4.0),
+                client().prepareIndex("agg_index", "doc", "4").setSource("one", 4.0, "two", 2.0, "three", 3.0, "four", 4.0),
+                client().prepareIndex("agg_index", "doc", "5").setSource("one", 5.0, "two", 2.0, "three", 3.0, "four", 4.0));
+        SearchResponse response = client()
+                .prepareSearch("agg_index")
+                .addAggregation(
+                        histogram("histogram")
+                                .field("one")
+                                .interval(2)
+                                .subAggregation(sum("twoSum").field("two"))
+                                .subAggregation(sum("threeSum").field("three"))
+                                .subAggregation(sum("fourSum").field("four"))
+                                .subAggregation(
+                                        bucketScript("totalSum").setBucketsPaths("twoSum", "threeSum", "fourSum").script(
+                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, ExpressionScriptEngineService.NAME, null)))).execute().actionGet();
+
+        InternalHistogram<Bucket> histogram = response.getAggregations().get("histogram");
+        assertThat(histogram, notNullValue());
+        assertThat(histogram.getName(), equalTo("histogram"));
+        List<Bucket> buckets = histogram.getBuckets();
+
+        for (int bucketCount = 0; bucketCount < buckets.size(); ++bucketCount) {
+            Histogram.Bucket bucket = buckets.get(bucketCount);
+            if (bucket.getDocCount() == 1) {
+                SimpleValue seriesArithmetic = bucket.getAggregations().get("totalSum");
+                assertThat(seriesArithmetic, notNullValue());
+                double seriesArithmeticValue = seriesArithmetic.value();
+                assertEquals(9.0, seriesArithmeticValue, 0.001);
+            } else if (bucket.getDocCount() == 2) {
+                SimpleValue seriesArithmetic = bucket.getAggregations().get("totalSum");
+                assertThat(seriesArithmetic, notNullValue());
+                double seriesArithmeticValue = seriesArithmetic.value();
+                assertEquals(18.0, seriesArithmeticValue, 0.001);
+            } else {
+                fail("Incorrect number of documents in a bucket in the histogram.");
+            }
+        }
+    }
+}
diff --git a/plugins/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/10_basic.yaml b/plugins/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/10_basic.yaml
new file mode 100644
index 0000000..9c7819d
--- /dev/null
+++ b/plugins/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/10_basic.yaml
@@ -0,0 +1,14 @@
+# Integration tests for Expression scripts
+#
+"Expression loaded":
+    - do:
+        cluster.state: {}
+
+    # Get master node id
+    - set: { master_node: master }
+
+    - do:
+        nodes.info: {}
+
+    - match:  { nodes.$master.plugins.0.name: lang-expression  }
+    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/20_search.yaml b/plugins/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/20_search.yaml
new file mode 100644
index 0000000..a0953a25
--- /dev/null
+++ b/plugins/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/20_search.yaml
@@ -0,0 +1,27 @@
+---
+setup:
+  - do:
+      indices.create:
+        index: test123
+        body:
+          mappings:
+            test:
+              properties:
+                age:
+                  type: long
+  - do:
+      index:
+        index:  test123
+        type:   test
+        id:     1
+        body:   { age: 23 }
+
+  - do:
+      indices.refresh: {}
+
+---
+"Expressions scripting test":
+
+  - do: { search: { body: { script_fields : { my_field : { lang: expression, script: 'doc["age"].value + 19' } } } } }
+  - match:  { hits.hits.0.fields.my_field.0: 42.0 }
+
diff --git a/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/10_basic.yaml b/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/10_basic.yaml
index 6d1625a..ee77a84 100644
--- a/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/10_basic.yaml
+++ b/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/10_basic.yaml
@@ -18,10 +18,9 @@ setup:
             body:
                 script_fields:
                     bar:
-                        script: 
-                            inline: "doc['foo'].value + x"
-                            lang: javascript
-                            params:
-                                x: "bbb"
+                        lang: javascript
+                        script: "doc['foo'].value + x"
+                        params:
+                            x: "bbb"
 
     - match: { hits.hits.0.fields.bar.0: "aaabbb"}
diff --git a/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/20_search.yaml b/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/20_search.yaml
index 742c8f0..24a6c8b 100644
--- a/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/20_search.yaml
+++ b/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/20_search.yaml
@@ -33,9 +33,8 @@
                             lang: js
                 script_fields:
                     sNum1:
-                        script: 
-                            inline: "doc['num1'].value"
-                            lang: js
+                        lang: js
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -58,9 +57,8 @@
 
                 script_fields:
                     sNum1:
-                        script:
-                            inline: "doc['num1'].value"
-                            lang: js
+                        lang: js
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -83,9 +81,8 @@
 
                 script_fields:
                     sNum1:
-                        script: 
-                            inline: "doc['num1'].value"
-                            lang: js
+                        lang: js
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -121,21 +118,17 @@
             body:
                 script_fields:
                     s_obj1:
-                        script: 
-                            inline: "_source.obj1"
-                            lang: js
+                        lang: js
+                        script: "_source.obj1"
                     s_obj1_test:
-                        script: 
-                            inline: "_source.obj1.test"
-                            lang: js
+                        lang: js
+                        script: "_source.obj1.test"
                     s_obj2:
-                        script: 
-                            inline: "_source.obj2"
-                            lang: js
+                        lang: js
+                        script: "_source.obj2"
                     s_obj2_arr2:
-                        script: 
-                            inline: "_source.obj2.arr2"
-                            lang: js
+                        lang: js
+                        script: "_source.obj2.arr2"
 
     - match: { hits.total: 1 }
     - match: { hits.hits.0.fields.s_obj1.0.test: something }
@@ -406,9 +399,8 @@
             body:
                 script_fields:
                     foobar:
-                        script: 
-                            inline: "doc['f'].values.length"
-                            lang: js
+                        lang: js
+                        script: "doc['f'].values.length"
 
 
     - match: { hits.total: 1 }
diff --git a/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/10_basic.yaml b/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/10_basic.yaml
index 4a811d1..ba7b733 100644
--- a/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/10_basic.yaml
+++ b/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/10_basic.yaml
@@ -18,10 +18,9 @@ setup:
             body:
                 script_fields:
                     bar:
-                        script: 
-                            inline: "doc['foo'].value + x"
-                            lang: python
-                            params:
-                                x: "bbb"
+                        lang: python
+                        script: "doc['foo'].value + x"
+                        params:
+                            x: "bbb"
 
     - match: { hits.hits.0.fields.bar.0: "aaabbb"}
diff --git a/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/20_search.yaml b/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/20_search.yaml
index b0f18e1..d19561a 100644
--- a/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/20_search.yaml
+++ b/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/20_search.yaml
@@ -33,9 +33,8 @@
                             lang: python
                 script_fields:
                     sNum1:
-                        script: 
-                            inline: "doc['num1'].value"
-                            lang: python
+                        lang: python
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -58,9 +57,8 @@
 
                 script_fields:
                     sNum1:
-                        script: 
-                            inline: "doc['num1'].value"
-                            lang: python
+                        lang: python
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -83,9 +81,8 @@
 
                 script_fields:
                     sNum1:
-                        script: 
-                            inline: "doc['num1'].value"
-                            lang: python
+                        lang: python
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -121,21 +118,17 @@
             body:
                 script_fields:
                     s_obj1:
-                        script: 
-                            inline: "_source['obj1']"
-                            lang: python
+                        lang: python
+                        script: "_source['obj1']"
                     s_obj1_test:
-                        script: 
-                            inline: "_source['obj1']['test']"
-                            lang: python
+                        lang: python
+                        script: "_source['obj1']['test']"
                     s_obj2:
-                        script: 
-                            inline: "_source['obj2']"
-                            lang: python
+                        lang: python
+                        script: "_source['obj2']"
                     s_obj2_arr2:
-                        script: 
-                            inline: "_source['obj2']['arr2']"
-                            lang: python
+                        lang: python
+                        script: "_source['obj2']['arr2']"
 
     - match: { hits.total: 1 }
     - match: { hits.hits.0.fields.s_obj1.0.test: something }
diff --git a/plugins/pom.xml b/plugins/pom.xml
index 3462485..2e95806 100644
--- a/plugins/pom.xml
+++ b/plugins/pom.xml
@@ -105,11 +105,6 @@
             <scope>provided</scope>
         </dependency>
         <dependency>
-            <groupId>org.apache.lucene</groupId>
-            <artifactId>lucene-expressions</artifactId>
-            <scope>provided</scope>
-        </dependency>
-        <dependency>
             <groupId>com.spatial4j</groupId>
             <artifactId>spatial4j</artifactId>
             <scope>provided</scope>
@@ -401,6 +396,7 @@
         <module>discovery-azure</module>
         <module>discovery-ec2</module>
         <module>discovery-multicast</module>
+        <module>lang-expression</module>
         <module>lang-javascript</module>
         <module>lang-python</module>
         <module>mapper-murmur3</module>
diff --git a/pom.xml b/pom.xml
index 54014ee..dd9e539 100644
--- a/pom.xml
+++ b/pom.xml
@@ -48,12 +48,7 @@
         <lucene.version>5.4.0</lucene.version>
         <lucene.snapshot.revision>1702855</lucene.snapshot.revision>
         <lucene.maven.version>5.4.0-snapshot-${lucene.snapshot.revision}</lucene.maven.version>
-        <!-- we are using a snapshot of randomizedtesting with this PR:
-             https://github.com/randomizedtesting/randomizedtesting/pull/202
-             This allows us to only give the test-framework jars dangerous permissions and not
-             all of our code/test code. It is temporary while Dawid is on vacation. -->
-        <testframework.snapshot.revision>pr202</testframework.snapshot.revision>
-        <testframework.version>2.2.0-snapshot-${testframework.snapshot.revision}</testframework.version>
+        <testframework.version>2.1.17</testframework.version>
         <jackson.version>2.5.3</jackson.version>
         <slf4j.version>1.6.2</slf4j.version>
         <log4j.version>1.2.17</log4j.version>
@@ -151,19 +146,8 @@
             <id>Lucene snapshots</id>
             <url>https://download.elasticsearch.org/lucenesnapshots/${lucene.snapshot.revision}</url>
         </repository>
-        <repository>
-            <id>Randomized Runner snapshot</id>
-            <url>https://download.elasticsearch.org/lucenesnapshots/rr-${testframework.snapshot.revision}</url>
-        </repository>
     </repositories>
 
-    <pluginRepositories>
-        <pluginRepository>
-            <id>Randomized Runner snapshot</id>
-            <url>https://download.elasticsearch.org/lucenesnapshots/rr-${testframework.snapshot.revision}</url>
-        </pluginRepository>
-    </pluginRepositories>
-
     <dependencyManagement>
         <dependencies>
             <dependency>
@@ -299,12 +283,6 @@
                 <version>${lucene.maven.version}</version>
             </dependency>
             <dependency>
-                <groupId>org.apache.lucene</groupId>
-                <artifactId>lucene-expressions</artifactId>
-                <version>${lucene.maven.version}</version>
-                <optional>true</optional>
-            </dependency>
-            <dependency>
                 <groupId>com.spatial4j</groupId>
                 <artifactId>spatial4j</artifactId>
                 <version>0.4.1</version>
diff --git a/qa/smoke-test-multinode/pom.xml b/qa/smoke-test-multinode/pom.xml
index a2aa7b0..879c319 100644
--- a/qa/smoke-test-multinode/pom.xml
+++ b/qa/smoke-test-multinode/pom.xml
@@ -91,11 +91,6 @@
       <scope>provided</scope>
     </dependency>
     <dependency>
-      <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-expressions</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
       <groupId>com.spatial4j</groupId>
       <artifactId>spatial4j</artifactId>
       <scope>provided</scope>
diff --git a/qa/smoke-test-plugins/pom.xml b/qa/smoke-test-plugins/pom.xml
index 9aa983c..59a2cbf 100644
--- a/qa/smoke-test-plugins/pom.xml
+++ b/qa/smoke-test-plugins/pom.xml
@@ -96,11 +96,6 @@
       <scope>provided</scope>
     </dependency>
     <dependency>
-      <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-expressions</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
       <groupId>com.spatial4j</groupId>
       <artifactId>spatial4j</artifactId>
       <scope>provided</scope>
@@ -322,6 +317,14 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
+                   <artifactId>lang-expression</artifactId>
+                   <version>${elasticsearch.version}</version>
+                   <type>zip</type>
+                   <overWrite>true</overWrite>
+                 </artifactItem>
+
+                 <artifactItem>
+                   <groupId>org.elasticsearch.plugin</groupId>
                    <artifactId>lang-javascript</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
diff --git a/qa/vagrant/pom.xml b/qa/vagrant/pom.xml
index 7e53968..203ad88 100644
--- a/qa/vagrant/pom.xml
+++ b/qa/vagrant/pom.xml
@@ -21,7 +21,7 @@
       <testCommand>sudo bats $BATS/${testScripts}</testCommand>
 
       <allDebBoxes>precise, trusty, vivid, jessie</allDebBoxes>
-      <allRpmBoxes>centos-6, centos-7, oel-7, fedora-22, opensuse-13</allRpmBoxes>
+      <allRpmBoxes>centos-6, centos-7, oel-7, fedora-22, opensuse-13, sles-12</allRpmBoxes>
 
       <defaultDebBoxes>trusty</defaultDebBoxes>
       <defaultRpmBoxes>centos-7</defaultRpmBoxes>
@@ -143,37 +143,37 @@
                                 </artifactItem>
                                 <artifactItem>
                                     <groupId>org.elasticsearch.plugin</groupId>
-                                    <artifactId>discovery-azure</artifactId>
+                                    <artifactId>cloud-gce</artifactId>
                                     <version>${elasticsearch.version}</version>
                                     <type>zip</type>
                                 </artifactItem>
                                 <artifactItem>
                                     <groupId>org.elasticsearch.plugin</groupId>
-                                    <artifactId>discovery-ec2</artifactId>
+                                    <artifactId>delete-by-query</artifactId>
                                     <version>${elasticsearch.version}</version>
                                     <type>zip</type>
                                 </artifactItem>
                                 <artifactItem>
                                     <groupId>org.elasticsearch.plugin</groupId>
-                                    <artifactId>cloud-gce</artifactId>
+                                    <artifactId>discovery-azure</artifactId>
                                     <version>${elasticsearch.version}</version>
                                     <type>zip</type>
                                 </artifactItem>
                                 <artifactItem>
                                     <groupId>org.elasticsearch.plugin</groupId>
-                                    <artifactId>delete-by-query</artifactId>
+                                    <artifactId>discovery-ec2</artifactId>
                                     <version>${elasticsearch.version}</version>
                                     <type>zip</type>
                                 </artifactItem>
                                 <artifactItem>
                                     <groupId>org.elasticsearch.plugin</groupId>
-                                    <artifactId>discovery-ec2</artifactId>
+                                    <artifactId>discovery-multicast</artifactId>
                                     <version>${elasticsearch.version}</version>
                                     <type>zip</type>
                                 </artifactItem>
                                 <artifactItem>
                                     <groupId>org.elasticsearch.plugin</groupId>
-                                    <artifactId>discovery-multicast</artifactId>
+                                    <artifactId>lang-expression</artifactId>
                                     <version>${elasticsearch.version}</version>
                                     <type>zip</type>
                                 </artifactItem>
@@ -203,12 +203,6 @@
                                 </artifactItem>
                                 <artifactItem>
                                     <groupId>org.elasticsearch.plugin</groupId>
-                                    <artifactId>repository-s3</artifactId>
-                                    <version>${elasticsearch.version}</version>
-                                    <type>zip</type>
-                                </artifactItem>
-                                <artifactItem>
-                                    <groupId>org.elasticsearch.plugin</groupId>
                                     <artifactId>repository-azure</artifactId>
                                     <version>${elasticsearch.version}</version>
                                     <type>zip</type>
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash b/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
index 82775ae..7f24b8a 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
@@ -316,15 +316,11 @@ wait_for_elasticsearch_status() {
     echo "Making sure elasticsearch is up..."
     wget -O - --retry-connrefused --waitretry=1 --timeout=60 --tries 60 http://localhost:9200 || {
           echo "Looks like elasticsearch never started. Here is its log:"
-          if [ -r "/tmp/elasticsearch/elasticsearch.pid" ]; then
-              cat /tmp/elasticsearch/logs/elasticsearch.log
+          if [ -e "$ESLOG/elasticsearch.log" ]; then
+              cat "$ESLOG/elasticsearch.log"
           else
-              if [ -e '/var/log/elasticsearch/elasticsearch.log' ]; then
-                  cat /var/log/elasticsearch/elasticsearch.log
-              else
-                  echo "The elasticsearch log doesn't exist. Maybe /vag/log/messages has something:"
-                  tail -n20 /var/log/messages
-              fi
+              echo "The elasticsearch log doesn't exist. Maybe /vag/log/messages has something:"
+              tail -n20 /var/log/messages
           fi
           false
     }
@@ -398,6 +394,7 @@ move_config() {
     mv "$oldConfig"/* "$ESCONFIG"
     chown -R elasticsearch:elasticsearch "$ESCONFIG"
     assert_file_exist "$ESCONFIG/elasticsearch.yml"
+    assert_file_exist "$ESCONFIG/logging.yml"
 }
 
 # Copies a script into the Elasticsearch install.
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash b/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
index aa9f784..332f788 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
@@ -67,6 +67,7 @@ if [[ "$BATS_TEST_FILENAME" =~ 25_tar_plugins.bats$ ]]; then
     }
     export ESHOME=/tmp/elasticsearch
     export_elasticsearch_paths
+    export ESPLUGIN_COMMAND_USER=elasticsearch
 else
     load os_package
     if is_rpm; then
@@ -75,6 +76,7 @@ else
         GROUP='DEB PLUGINS'
     fi
     export_elasticsearch_paths
+    export ESPLUGIN_COMMAND_USER=root
     install() {
         install_package
         verify_package_installation
@@ -94,7 +96,7 @@ fi
 
     install_jvm_example
     start_elasticsearch_service
-    # check that configuration was actually picked up    
+    # check that configuration was actually picked up
     curl -s localhost:9200/_cat/configured_example | sed 's/ *$//' > /tmp/installed
     echo "foo" > /tmp/expected
     diff /tmp/installed /tmp/expected
@@ -169,10 +171,6 @@ fi
     install_and_check_plugin analysis stempel
 }
 
-@test "[$GROUP] install azure plugin" {
-    install_and_check_plugin cloud azure azure-core-*.jar
-}
-
 @test "[$GROUP] install gce plugin" {
     install_and_check_plugin cloud gce google-api-client-*.jar
 }
@@ -181,7 +179,11 @@ fi
     install_and_check_plugin - delete-by-query
 }
 
-@test "[$GROUP] install ec2 discovery plugin" {
+@test "[$GROUP] install discovery-azure plugin" {
+    install_and_check_plugin discovery azure azure-core-*.jar
+}
+
+@test "[$GROUP] install discovery-ec2 plugin" {
     install_and_check_plugin discovery ec2 aws-java-sdk-core-*.jar
 }
 
@@ -189,6 +191,10 @@ fi
     install_and_check_plugin discovery multicast
 }
 
+@test "[$GROUP] install lang-expression plugin" {
+    install_and_check_plugin lang expression
+}
+
 @test "[$GROUP] install javascript plugin" {
     install_and_check_plugin lang javascript rhino-*.jar
 }
@@ -205,7 +211,11 @@ fi
     install_and_check_plugin mapper size
 }
 
-@test "[$GROUP] install s3 repository plugin" {
+@test "[$GROUP] install repository-azure plugin" {
+    install_and_check_plugin repository azure azure-storage-*.jar
+}
+
+@test "[$GROUP] install repository-s3 plugin" {
     install_and_check_plugin repository s3 aws-java-sdk-core-*.jar
 }
 
@@ -215,6 +225,10 @@ fi
     assert_file_exist "$ESHOME/plugins/site-example/_site/index.html"
 }
 
+@test "[$GROUP] install store-smb plugin" {
+    install_and_check_plugin store smb
+}
+
 @test "[$GROUP] check the installed plugins can be listed with 'plugins list' and result matches the list of plugins in plugins pom" {
     "$ESHOME/bin/plugin" list | tail -n +2 | sed 's/^......//' > /tmp/installed
     compare_plugins_list "/tmp/installed" "'plugins list'"
@@ -257,14 +271,6 @@ fi
     remove_plugin analysis-stempel
 }
 
-@test "[$GROUP] remove aws plugin" {
-    remove_plugin cloud-aws
-}
-
-@test "[$GROUP] remove azure plugin" {
-    remove_plugin cloud-azure
-}
-
 @test "[$GROUP] remove gce plugin" {
     remove_plugin cloud-gce
 }
@@ -273,7 +279,11 @@ fi
     remove_plugin delete-by-query
 }
 
-@test "[$GROUP] remove ec2 discovery plugin" {
+@test "[$GROUP] remove discovery-azure plugin" {
+    remove_plugin discovery-azure
+}
+
+@test "[$GROUP] remove discovery-ec2 plugin" {
     remove_plugin discovery-ec2
 }
 
@@ -281,6 +291,10 @@ fi
     remove_plugin discovery-multicast
 }
 
+@test "[$GROUP] remove lang-expression plugin" {
+    remove_plugin lang-expression
+}
+
 @test "[$GROUP] remove javascript plugin" {
     remove_plugin lang-javascript
 }
@@ -297,7 +311,11 @@ fi
     remove_plugin mapper-size
 }
 
-@test "[$GROUP] remove s3 repository plugin" {
+@test "[$GROUP] remove repository-azure plugin" {
+    remove_plugin repository-azure
+}
+
+@test "[$GROUP] remove repository-s3 plugin" {
     remove_plugin repository-s3
 }
 
@@ -305,6 +323,10 @@ fi
     remove_plugin site-example
 }
 
+@test "[$GROUP] remove store-smb plugin" {
+    remove_plugin store-smb
+}
+
 @test "[$GROUP] start elasticsearch with all plugins removed" {
     start_elasticsearch_service
 }
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/plugins.bash b/qa/vagrant/src/test/resources/packaging/scripts/plugins.bash
index 65faa12..2af60c8 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/plugins.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/plugins.bash
@@ -32,10 +32,18 @@ install_plugin() {
 
     assert_file_exist "$path"
 
-    "$ESHOME/bin/plugin" install "file://$path"
+    sudo -E -u $ESPLUGIN_COMMAND_USER "$ESHOME/bin/plugin" install "file://$path"
 
     assert_file_exist "$ESPLUGINS/$name"
     assert_file_exist "$ESPLUGINS/$name/plugin-descriptor.properties"
+
+    # At some point installing or removing plugins caused elasticsearch's logs
+    # to be owned by root. This is bad so we want to make sure it doesn't
+    # happen.
+    if [ -e "$ESLOG" ] && [ $(stat "$ESLOG" --format "%U") == "root" ]; then
+        echo "$ESLOG is now owned by root! That'll break logging when elasticsearch tries to start."
+        false
+    fi
 }
 
 install_jvm_plugin() {
@@ -50,9 +58,17 @@ remove_plugin() {
     local name=$1
 
     echo "Removing $name...."
-    "$ESHOME/bin/plugin" remove $name
+    sudo -E -u $ESPLUGIN_COMMAND_USER "$ESHOME/bin/plugin" remove $name
 
     assert_file_not_exist "$ESPLUGINS/$name"
+
+    # At some point installing or removing plugins caused elasticsearch's logs
+    # to be owned by root. This is bad so we want to make sure it doesn't
+    # happen.
+    if [ -e "$ESLOG" ] && [ $(stat "$ESLOG" --format "%U") == "root" ]; then
+        echo "$ESLOG is now owned by root! That'll break logging when elasticsearch tries to start."
+        false
+    fi
 }
 
 # Install the jvm-example plugin which fully excercises the special case file
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/tar.bash b/qa/vagrant/src/test/resources/packaging/scripts/tar.bash
index 0940a26..d447b1a 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/tar.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/tar.bash
@@ -73,7 +73,6 @@ export_elasticsearch_paths() {
     export ESSCRIPTS="$ESCONFIG/scripts"
     export ESDATA="$ESHOME/data"
     export ESLOG="$ESHOME/logs"
-
 }
 
 # Checks that all directories & files are correctly installed
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/10_basic.yaml
index 7e4c574..44313aa 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/10_basic.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/10_basic.yaml
@@ -35,7 +35,7 @@ setup:
         index: test_index
         name: test_warmer
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
 
   - do:
       indices.delete_warmer:
@@ -55,8 +55,8 @@ setup:
   - do:
       indices.get_warmer: {}
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 
 ---
@@ -67,8 +67,8 @@ setup:
         index: '*'
         name: '*'
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 ---
 "Getting warmers for several indices should work using _all":
@@ -78,8 +78,8 @@ setup:
         index: _all
         name: _all
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 ---
 "Getting all warmers without specifying index should work":
@@ -88,8 +88,8 @@ setup:
       indices.get_warmer:
         name: _all
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 ---
 "Getting warmers for several indices should work using prefix*":
@@ -99,8 +99,8 @@ setup:
         index: test_i*
         name: test_w*
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 ---
 "Getting warmers for several indices should work using comma-separated lists":
@@ -110,8 +110,8 @@ setup:
         index: test_index,test_idx
         name: test_warmer,test_warmer2
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 ---
 "Getting a non-existent warmer on an existing index should return an empty body":
@@ -131,7 +131,7 @@ setup:
         index: test_index
         name: test_warmer,non-existent
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
   - is_false: test_index.warmers.non-existent
 
 --- 
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/20_aliases.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/20_aliases.yaml
index b8a2fa6..96d7344 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/20_aliases.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/20_aliases.yaml
@@ -26,5 +26,5 @@
       indices.get_warmer:
           index: test_alias
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
 
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/all_path_options.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/all_path_options.yaml
index ffad427..b9c64f7 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/all_path_options.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/all_path_options.yaml
@@ -38,8 +38,8 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
   - is_false: foo
 
 ---
@@ -54,9 +54,9 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {foo.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
+  - match: {foo.warmers.warmer.source.query.match_all: {}}
 
 ---
 "put warmer in * index":
@@ -70,9 +70,9 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {foo.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
+  - match: {foo.warmers.warmer.source.query.match_all: {}}
 
 ---
 "put warmer prefix* index":
@@ -86,8 +86,8 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
   - is_false: foo
 
 ---
@@ -102,8 +102,8 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
   - is_false: foo
 
 ---
@@ -117,9 +117,9 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {foo.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
+  - match: {foo.warmers.warmer.source.query.match_all: {}}
 
 ---
 "put warmer with missing name":
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/script/30_expressions.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/script/30_expressions.yaml
deleted file mode 100644
index 36ff7f5..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/script/30_expressions.yaml
+++ /dev/null
@@ -1,27 +0,0 @@
----
-setup:
-  - do:
-      indices.create:
-        index: test123
-        body:
-          mappings:
-            test:
-              properties:
-                age:
-                  type: long
-  - do:
-      index:
-        index:  test123
-        type:   test
-        id:     1
-        body:   { age: 23 }
-
-  - do:
-      indices.refresh: {}
-
----
-"Expressions scripting test":
-
-  - do: { search: { body: { script_fields : { my_field : { script: { lang: expression, inline: 'doc["age"].value + 19' } } } } } }
-  - match:  { hits.hits.0.fields.my_field.0: 42.0 }
-
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/search/10_source_filtering.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/search/10_source_filtering.yaml
index b49d659..a78a5a2 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/search/10_source_filtering.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/search/10_source_filtering.yaml
@@ -14,12 +14,12 @@
   - do:
       search:
       # stringified for boolean value
-        body: { _source: true, query: { match_all: {} } }
+        body: "{ _source: true, query: { match_all: {} } }"
 
   - length:   { hits.hits: 1  }
   - match: { hits.hits.0._source.count: 1 }
 
-  - do: { search: { body: { _source: false, query: { match_all: {} } } } }
+  - do: { search: { body: "{ _source: false, query: { match_all: {} } }" } }
   - length:   { hits.hits: 1  }
   - is_false: hits.hits.0._source
 
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml
index 4da748a..5153f6c 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml
@@ -28,11 +28,16 @@
 
   - do:
       search_template:
+        body: { "template": { "id" : "1" }, "params" : { "my_value" : "value1_foo", "my_size" : 1 } }
+  - match: { hits.total: 1 }
+
+  - do:
+      search_template:
         body: {  "id" : "1", "params" : { "my_value" : "value1_foo", "my_size" : 1 } }
   - match: { hits.total: 1 }
 
   - do:
       catch: /Unable.to.find.on.disk.file.script.\[simple1\].using.lang.\[mustache\]/
       search_template:
-        body: { "file" : "simple1"}
+        body: { "template" : "simple1" }
 
