diff --git a/buildSrc/src/main/resources/forbidden/all-signatures.txt b/buildSrc/src/main/resources/forbidden/all-signatures.txt
index c1e65cb..f72de51 100644
--- a/buildSrc/src/main/resources/forbidden/all-signatures.txt
+++ b/buildSrc/src/main/resources/forbidden/all-signatures.txt
@@ -45,6 +45,7 @@ org.apache.lucene.search.NumericRangeFilter
 org.apache.lucene.search.PrefixFilter
 org.apache.lucene.search.QueryWrapperFilter
 org.apache.lucene.search.join.BitDocIdSetCachingWrapperFilter
+org.apache.lucene.index.IndexWriter#isLocked(org.apache.lucene.store.Directory)
 
 java.nio.file.Paths @ Use org.elasticsearch.common.io.PathUtils.get() instead.
 java.nio.file.FileSystems#getDefault() @ use org.elasticsearch.common.io.PathUtils.getDefaultFileSystem() instead.
@@ -125,4 +126,6 @@ java.util.Collections#EMPTY_MAP
 java.util.Collections#EMPTY_SET
 
 java.util.Collections#shuffle(java.util.List) @ Use java.util.Collections#shuffle(java.util.List, java.util.Random) with a reproducible source of randomness
-java.util.Random#<init>() @ Use org.elasticsearch.common.random.Randomness#create for reproducible sources of randomness
+@defaultMessage Use org.elasticsearch.common.Randomness#get for reproducible sources of randomness
+java.util.Random#<init>()
+java.util.concurrent.ThreadLocalRandom
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/action/ActionModule.java b/core/src/main/java/org/elasticsearch/action/ActionModule.java
index 51a5498..5f1a181 100644
--- a/core/src/main/java/org/elasticsearch/action/ActionModule.java
+++ b/core/src/main/java/org/elasticsearch/action/ActionModule.java
@@ -149,16 +149,6 @@ import org.elasticsearch.action.indexedscripts.get.GetIndexedScriptAction;
 import org.elasticsearch.action.indexedscripts.get.TransportGetIndexedScriptAction;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptAction;
 import org.elasticsearch.action.indexedscripts.put.TransportPutIndexedScriptAction;
-import org.elasticsearch.action.ingest.IngestActionFilter;
-import org.elasticsearch.action.ingest.IngestDisabledActionFilter;
-import org.elasticsearch.action.ingest.DeletePipelineAction;
-import org.elasticsearch.action.ingest.DeletePipelineTransportAction;
-import org.elasticsearch.action.ingest.GetPipelineAction;
-import org.elasticsearch.action.ingest.GetPipelineTransportAction;
-import org.elasticsearch.action.ingest.PutPipelineAction;
-import org.elasticsearch.action.ingest.PutPipelineTransportAction;
-import org.elasticsearch.action.ingest.SimulatePipelineAction;
-import org.elasticsearch.action.ingest.SimulatePipelineTransportAction;
 import org.elasticsearch.action.percolate.MultiPercolateAction;
 import org.elasticsearch.action.percolate.PercolateAction;
 import org.elasticsearch.action.percolate.TransportMultiPercolateAction;
@@ -196,8 +186,6 @@ import org.elasticsearch.action.update.UpdateAction;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.multibindings.MapBinder;
 import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.IngestModule;
 
 import java.util.ArrayList;
 import java.util.HashMap;
@@ -222,13 +210,13 @@ public class ActionModule extends AbstractModule {
             this.transportAction = transportAction;
             this.supportTransportActions = supportTransportActions;
         }
+
+
     }
 
-    private final boolean ingestEnabled;
     private final boolean proxy;
 
-    public ActionModule(Settings settings, boolean proxy) {
-        this.ingestEnabled = IngestModule.isIngestEnabled(settings);
+    public ActionModule(boolean proxy) {
         this.proxy = proxy;
     }
 
@@ -252,13 +240,6 @@ public class ActionModule extends AbstractModule {
 
     @Override
     protected void configure() {
-        if (proxy == false) {
-            if (ingestEnabled) {
-                registerFilter(IngestActionFilter.class);
-            } else {
-                registerFilter(IngestDisabledActionFilter.class);
-            }
-        }
 
         Multibinder<ActionFilter> actionFilterMultibinder = Multibinder.newSetBinder(binder(), ActionFilter.class);
         for (Class<? extends ActionFilter> actionFilter : actionFilters) {
@@ -359,11 +340,6 @@ public class ActionModule extends AbstractModule {
 
         registerAction(FieldStatsAction.INSTANCE, TransportFieldStatsTransportAction.class);
 
-        registerAction(PutPipelineAction.INSTANCE, PutPipelineTransportAction.class);
-        registerAction(GetPipelineAction.INSTANCE, GetPipelineTransportAction.class);
-        registerAction(DeletePipelineAction.INSTANCE, DeletePipelineTransportAction.class);
-        registerAction(SimulatePipelineAction.INSTANCE, SimulatePipelineTransportAction.class);
-
         // register Name -> GenericAction Map that can be injected to instances.
         MapBinder<String, GenericAction> actionsBinder
                 = MapBinder.newMapBinder(binder(), String.class, GenericAction.class);
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java
index ebdc8b7..85644e8 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java
@@ -32,7 +32,7 @@ import org.elasticsearch.index.engine.SegmentsStats;
 import org.elasticsearch.index.fielddata.FieldDataStats;
 import org.elasticsearch.index.flush.FlushStats;
 import org.elasticsearch.index.get.GetStats;
-import org.elasticsearch.index.indexing.IndexingStats;
+import org.elasticsearch.index.shard.IndexingStats;
 import org.elasticsearch.index.merge.MergeStats;
 import org.elasticsearch.index.percolator.PercolateStats;
 import org.elasticsearch.index.recovery.RecoveryStats;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
index 326dbc0..225ee32 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
@@ -37,6 +37,7 @@ import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.routing.GroupShardsIterator;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.Randomness;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.BigArrays;
@@ -59,7 +60,6 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.concurrent.ThreadLocalRandom;
 import java.util.concurrent.atomic.AtomicReferenceArray;
 
 /**
@@ -108,7 +108,7 @@ public class TransportValidateQueryAction extends TransportBroadcastAction<Valid
     @Override
     protected GroupShardsIterator shards(ClusterState clusterState, ValidateQueryRequest request, String[] concreteIndices) {
         // Hard-code routing to limit request to a single shard, but still, randomize it...
-        Map<String, Set<String>> routingMap = indexNameExpressionResolver.resolveSearchRouting(clusterState, Integer.toString(ThreadLocalRandom.current().nextInt(1000)), request.indices());
+        Map<String, Set<String>> routingMap = indexNameExpressionResolver.resolveSearchRouting(clusterState, Integer.toString(Randomness.get().nextInt(1000)), request.indices());
         return clusterService.operationRouting().searchShards(clusterState, concreteIndices, routingMap, "_local");
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java
index 8ae9164..78a0c76 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java
@@ -289,11 +289,11 @@ public class BulkProcessor implements Closeable {
     }
 
     public BulkProcessor add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType) throws Exception {
-        return add(data, defaultIndex, defaultType, null, null);
+        return add(data, defaultIndex, defaultType, null);
     }
 
-    public synchronized BulkProcessor add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType, @Nullable String defaultPipeline, @Nullable Object payload) throws Exception {
-        bulkRequest.add(data, defaultIndex, defaultType, null, null, defaultPipeline, payload, true);
+    public synchronized BulkProcessor add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType, @Nullable Object payload) throws Exception {
+        bulkRequest.add(data, defaultIndex, defaultType, null, null, payload, true);
         executeIfNeeded();
         return this;
     }
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java
index 2e93546..02e0ea4 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java
@@ -254,17 +254,17 @@ public class BulkRequest extends ActionRequest<BulkRequest> implements Composite
      * Adds a framed data in binary format
      */
     public BulkRequest add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType) throws Exception {
-        return add(data, defaultIndex, defaultType, null, null, null, null, true);
+        return add(data, defaultIndex, defaultType, null, null, null, true);
     }
 
     /**
      * Adds a framed data in binary format
      */
     public BulkRequest add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType, boolean allowExplicitIndex) throws Exception {
-        return add(data, defaultIndex, defaultType, null, null, null, null, allowExplicitIndex);
+        return add(data, defaultIndex, defaultType, null, null, null, allowExplicitIndex);
     }
 
-    public BulkRequest add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType, @Nullable String defaultRouting, @Nullable String[] defaultFields, @Nullable String defaultPipeline, @Nullable Object payload, boolean allowExplicitIndex) throws Exception {
+    public BulkRequest add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType, @Nullable String defaultRouting, @Nullable String[] defaultFields, @Nullable Object payload, boolean allowExplicitIndex) throws Exception {
         XContent xContent = XContentFactory.xContent(data);
         int line = 0;
         int from = 0;
@@ -305,7 +305,6 @@ public class BulkRequest extends ActionRequest<BulkRequest> implements Composite
                 long version = Versions.MATCH_ANY;
                 VersionType versionType = VersionType.INTERNAL;
                 int retryOnConflict = 0;
-                String pipeline = defaultPipeline;
 
                 // at this stage, next token can either be END_OBJECT (and use default index and type, with auto generated id)
                 // or START_OBJECT which will have another set of parameters
@@ -346,8 +345,6 @@ public class BulkRequest extends ActionRequest<BulkRequest> implements Composite
                                 versionType = VersionType.fromString(parser.text());
                             } else if ("_retry_on_conflict".equals(currentFieldName) || "_retryOnConflict".equals(currentFieldName)) {
                                 retryOnConflict = parser.intValue();
-                            } else if ("pipeline".equals(currentFieldName)) {
-                                pipeline = parser.text();
                             } else if ("fields".equals(currentFieldName)) {
                                 throw new IllegalArgumentException("Action/metadata line [" + line + "] contains a simple value for parameter [fields] while a list is expected");
                             } else {
@@ -384,15 +381,15 @@ public class BulkRequest extends ActionRequest<BulkRequest> implements Composite
                     if ("index".equals(action)) {
                         if (opType == null) {
                             internalAdd(new IndexRequest(index, type, id).routing(routing).parent(parent).timestamp(timestamp).ttl(ttl).version(version).versionType(versionType)
-                                    .pipeline(pipeline).source(data.slice(from, nextMarker - from)), payload);
+                                    .source(data.slice(from, nextMarker - from)), payload);
                         } else {
                             internalAdd(new IndexRequest(index, type, id).routing(routing).parent(parent).timestamp(timestamp).ttl(ttl).version(version).versionType(versionType)
-                                    .create("create".equals(opType)).pipeline(pipeline)
+                                    .create("create".equals(opType))
                                     .source(data.slice(from, nextMarker - from)), payload);
                         }
                     } else if ("create".equals(action)) {
                         internalAdd(new IndexRequest(index, type, id).routing(routing).parent(parent).timestamp(timestamp).ttl(ttl).version(version).versionType(versionType)
-                                .create(true).pipeline(pipeline)
+                                .create(true)
                                 .source(data.slice(from, nextMarker - from)), payload);
                     } else if ("update".equals(action)) {
                         UpdateRequest updateRequest = new UpdateRequest(index, type, id).routing(routing).parent(parent).retryOnConflict(retryOnConflict)
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java b/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
index 2597695..d7d4042 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
@@ -410,7 +410,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                 }
             case NONE:
                 UpdateResponse updateResponse = translate.action();
-                indexShard.indexingService().noopUpdate(updateRequest.type());
+                indexShard.noopUpdate(updateRequest.type());
                 return new UpdateResult(translate, updateResponse);
             default:
                 throw new IllegalStateException("Illegal update operation " + translate.operation());
diff --git a/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java b/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
index 5c9f5ae..9899a54 100644
--- a/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
@@ -155,8 +155,6 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
 
     private XContentType contentType = Requests.INDEX_CONTENT_TYPE;
 
-    private String pipeline;
-
     public IndexRequest() {
     }
 
@@ -366,21 +364,6 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
     }
 
     /**
-     * Sets the ingest pipeline to be executed before indexing the document
-     */
-    public IndexRequest pipeline(String pipeline) {
-        this.pipeline = pipeline;
-        return this;
-    }
-
-    /**
-     * Returns the ingest pipeline to be executed before indexing the document
-     */
-    public String pipeline() {
-        return this.pipeline;
-    }
-
-    /**
      * The source of the document to index, recopied to a new array if it is unsage.
      */
     public BytesReference source() {
@@ -675,7 +658,6 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
         refresh = in.readBoolean();
         version = in.readLong();
         versionType = VersionType.fromValue(in.readByte());
-        pipeline = in.readOptionalString();
     }
 
     @Override
@@ -697,7 +679,6 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
         out.writeBoolean(refresh);
         out.writeLong(version);
         out.writeByte(versionType.getValue());
-        out.writeOptionalString(pipeline);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java
index a355d68..f7134d8 100644
--- a/core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java
@@ -278,12 +278,4 @@ public class IndexRequestBuilder extends ReplicationRequestBuilder<IndexRequest,
         request.ttl(ttl);
         return this;
     }
-
-    /**
-     * Sets the ingest pipeline to be executed before indexing the document
-     */
-    public IndexRequestBuilder setPipeline(String pipeline) {
-        request.pipeline(pipeline);
-        return this;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineAction.java b/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineAction.java
deleted file mode 100644
index 8456d7e..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineAction.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.Action;
-import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class DeletePipelineAction extends Action<DeletePipelineRequest, DeleteResponse, DeletePipelineRequestBuilder> {
-
-    public static final DeletePipelineAction INSTANCE = new DeletePipelineAction();
-    public static final String NAME = "cluster:admin/ingest/pipeline/delete";
-
-    public DeletePipelineAction() {
-        super(NAME);
-    }
-
-    @Override
-    public DeletePipelineRequestBuilder newRequestBuilder(ElasticsearchClient client) {
-        return new DeletePipelineRequestBuilder(client, this);
-    }
-
-    @Override
-    public DeleteResponse newResponse() {
-        return new DeleteResponse();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequest.java b/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequest.java
deleted file mode 100644
index 3d958f8..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequest.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionRequestValidationException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-
-import java.io.IOException;
-
-import static org.elasticsearch.action.ValidateActions.addValidationError;
-
-public class DeletePipelineRequest extends ActionRequest {
-
-    private String id;
-
-    public void id(String id) {
-        this.id = id;
-    }
-
-    public String id() {
-        return id;
-    }
-
-    @Override
-    public ActionRequestValidationException validate() {
-        ActionRequestValidationException validationException = null;
-        if (id == null) {
-            validationException = addValidationError("id is missing", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        id = in.readString();
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeString(id);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequestBuilder.java
deleted file mode 100644
index 29563fa..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequestBuilder.java
+++ /dev/null
@@ -1,37 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionRequestBuilder;
-import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class DeletePipelineRequestBuilder extends ActionRequestBuilder<DeletePipelineRequest, DeleteResponse, DeletePipelineRequestBuilder> {
-
-    public DeletePipelineRequestBuilder(ElasticsearchClient client, DeletePipelineAction action) {
-        super(client, action, new DeletePipelineRequest());
-    }
-
-    public DeletePipelineRequestBuilder setId(String id) {
-        request.id(id);
-        return this;
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineTransportAction.java b/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineTransportAction.java
deleted file mode 100644
index 4f25a9d..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineTransportAction.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.support.ActionFilters;
-import org.elasticsearch.action.support.HandledTransportAction;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.IngestBootstrapper;
-import org.elasticsearch.ingest.PipelineStore;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-public class DeletePipelineTransportAction extends HandledTransportAction<DeletePipelineRequest, DeleteResponse> {
-
-    private final PipelineStore pipelineStore;
-
-    @Inject
-    public DeletePipelineTransportAction(Settings settings, ThreadPool threadPool, TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver, IngestBootstrapper bootstrapper) {
-        super(settings, DeletePipelineAction.NAME, threadPool, transportService, actionFilters, indexNameExpressionResolver, DeletePipelineRequest::new);
-        this.pipelineStore = bootstrapper.getPipelineStore();
-    }
-
-    @Override
-    protected void doExecute(DeletePipelineRequest request, ActionListener<DeleteResponse> listener) {
-        pipelineStore.delete(request, listener);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineAction.java b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineAction.java
deleted file mode 100644
index f6bc3d9..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineAction.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.Action;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class GetPipelineAction extends Action<GetPipelineRequest, GetPipelineResponse, GetPipelineRequestBuilder> {
-
-    public static final GetPipelineAction INSTANCE = new GetPipelineAction();
-    public static final String NAME = "cluster:admin/ingest/pipeline/get";
-
-    public GetPipelineAction() {
-        super(NAME);
-    }
-
-    @Override
-    public GetPipelineRequestBuilder newRequestBuilder(ElasticsearchClient client) {
-        return new GetPipelineRequestBuilder(client, this);
-    }
-
-    @Override
-    public GetPipelineResponse newResponse() {
-        return new GetPipelineResponse();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequest.java b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequest.java
deleted file mode 100644
index e0bfca6..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequest.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionRequestValidationException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-
-import java.io.IOException;
-
-import static org.elasticsearch.action.ValidateActions.addValidationError;
-
-public class GetPipelineRequest extends ActionRequest {
-
-    private String[] ids;
-
-    public void ids(String... ids) {
-        this.ids = ids;
-    }
-
-    public String[] ids() {
-        return ids;
-    }
-
-    @Override
-    public ActionRequestValidationException validate() {
-        ActionRequestValidationException validationException = null;
-        if (ids == null || ids.length == 0) {
-            validationException = addValidationError("ids is missing", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        ids = in.readStringArray();
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeStringArray(ids);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequestBuilder.java
deleted file mode 100644
index c339603..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequestBuilder.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionRequestBuilder;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class GetPipelineRequestBuilder extends ActionRequestBuilder<GetPipelineRequest, GetPipelineResponse, GetPipelineRequestBuilder> {
-
-    public GetPipelineRequestBuilder(ElasticsearchClient client, GetPipelineAction action) {
-        super(client, action, new GetPipelineRequest());
-    }
-
-    public GetPipelineRequestBuilder setIds(String... ids) {
-        request.ids(ids);
-        return this;
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineResponse.java b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineResponse.java
deleted file mode 100644
index 9a12f4b..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineResponse.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.StatusToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.ingest.PipelineDefinition;
-import org.elasticsearch.rest.RestStatus;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-public class GetPipelineResponse extends ActionResponse implements StatusToXContent {
-
-    private List<PipelineDefinition> pipelines;
-
-    public GetPipelineResponse() {
-    }
-
-    public GetPipelineResponse(List<PipelineDefinition> pipelines) {
-        this.pipelines = pipelines;
-    }
-
-    public List<PipelineDefinition> pipelines() {
-        return pipelines;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        int size = in.readVInt();
-        pipelines = new ArrayList<>(size);
-        for (int i = 0; i < size; i++) {
-            pipelines.add(PipelineDefinition.readPipelineDefinitionFrom(in));
-        }
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeVInt(pipelines.size());
-        for (PipelineDefinition pipeline : pipelines) {
-            pipeline.writeTo(out);
-        }
-    }
-
-    public boolean isFound() {
-        return !pipelines.isEmpty();
-    }
-
-    @Override
-    public RestStatus status() {
-        return isFound() ? RestStatus.OK : RestStatus.NOT_FOUND;
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        for (PipelineDefinition definition : pipelines) {
-            definition.toXContent(builder, params);
-        }
-        return builder;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineTransportAction.java b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineTransportAction.java
deleted file mode 100644
index 471238e..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineTransportAction.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.support.ActionFilters;
-import org.elasticsearch.action.support.HandledTransportAction;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.IngestBootstrapper;
-import org.elasticsearch.ingest.PipelineDefinition;
-import org.elasticsearch.ingest.PipelineStore;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-import java.util.List;
-
-public class GetPipelineTransportAction extends HandledTransportAction<GetPipelineRequest, GetPipelineResponse> {
-
-    private final PipelineStore pipelineStore;
-
-    @Inject
-    public GetPipelineTransportAction(Settings settings, ThreadPool threadPool, TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver, IngestBootstrapper bootstrapper) {
-        super(settings, GetPipelineAction.NAME, threadPool, transportService, actionFilters, indexNameExpressionResolver, GetPipelineRequest::new);
-        this.pipelineStore = bootstrapper.getPipelineStore();
-    }
-
-    @Override
-    protected void doExecute(GetPipelineRequest request, ActionListener<GetPipelineResponse> listener) {
-        List<PipelineDefinition> references = pipelineStore.getReference(request.ids());
-        listener.onResponse(new GetPipelineResponse(references));
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/IngestActionFilter.java b/core/src/main/java/org/elasticsearch/action/ingest/IngestActionFilter.java
deleted file mode 100644
index c9467b0..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/IngestActionFilter.java
+++ /dev/null
@@ -1,229 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.action.bulk.BulkAction;
-import org.elasticsearch.action.bulk.BulkItemResponse;
-import org.elasticsearch.action.bulk.BulkRequest;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.index.IndexAction;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.support.ActionFilter;
-import org.elasticsearch.action.support.ActionFilterChain;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.IngestBootstrapper;
-import org.elasticsearch.ingest.PipelineExecutionService;
-import org.elasticsearch.tasks.Task;
-
-import java.util.ArrayList;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Set;
-
-public final class IngestActionFilter extends AbstractComponent implements ActionFilter {
-
-    private final PipelineExecutionService executionService;
-
-    @Inject
-    public IngestActionFilter(Settings settings, IngestBootstrapper bootstrapper) {
-        super(settings);
-        this.executionService = bootstrapper.getPipelineExecutionService();
-    }
-
-    @Override
-    public void apply(Task task, String action, ActionRequest request, ActionListener listener, ActionFilterChain chain) {
-        if (IndexAction.NAME.equals(action)) {
-            assert request instanceof IndexRequest;
-            IndexRequest indexRequest = (IndexRequest) request;
-            if (Strings.hasText(indexRequest.pipeline())) {
-                processIndexRequest(task, action, listener, chain, (IndexRequest) request);
-                return;
-            }
-        }
-        if (BulkAction.NAME.equals(action)) {
-            assert request instanceof BulkRequest;
-            BulkRequest bulkRequest = (BulkRequest) request;
-            boolean isIngestRequest = false;
-            for (ActionRequest actionRequest : bulkRequest.requests()) {
-                if (actionRequest instanceof IndexRequest) {
-                    IndexRequest indexRequest = (IndexRequest) actionRequest;
-                    if (Strings.hasText(indexRequest.pipeline())) {
-                        isIngestRequest = true;
-                        break;
-                    }
-                }
-            }
-            if (isIngestRequest) {
-                @SuppressWarnings("unchecked")
-                ActionListener<BulkResponse> actionListener = (ActionListener<BulkResponse>) listener;
-                processBulkIndexRequest(task, bulkRequest, action, chain, actionListener);
-                return;
-            }
-        }
-
-        chain.proceed(task, action, request, listener);
-    }
-
-    @Override
-    public void apply(String action, ActionResponse response, ActionListener listener, ActionFilterChain chain) {
-        chain.proceed(action, response, listener);
-    }
-
-    void processIndexRequest(Task task, String action, ActionListener listener, ActionFilterChain chain, IndexRequest indexRequest) {
-
-        executionService.execute(indexRequest, t -> {
-            logger.error("failed to execute pipeline [{}]", t, indexRequest.pipeline());
-            listener.onFailure(t);
-        }, success -> {
-            // TransportIndexAction uses IndexRequest and same action name on the node that receives the request and the node that
-            // processes the primary action. This could lead to a pipeline being executed twice for the same
-            // index request, hence we set the pipeline to null once its execution completed.
-            indexRequest.pipeline(null);
-            chain.proceed(task, action, indexRequest, listener);
-        });
-    }
-
-    void processBulkIndexRequest(Task task, BulkRequest original, String action, ActionFilterChain chain, ActionListener<BulkResponse> listener) {
-        BulkRequestModifier bulkRequestModifier = new BulkRequestModifier(original);
-        executionService.execute(() -> bulkRequestModifier, tuple -> {
-            IndexRequest indexRequest = tuple.v1();
-            Throwable throwable = tuple.v2();
-            logger.debug("failed to execute pipeline [{}] for document [{}/{}/{}]", indexRequest.pipeline(), indexRequest.index(), indexRequest.type(), indexRequest.id(), throwable);
-            bulkRequestModifier.markCurrentItemAsFailed(throwable);
-        }, (success) -> {
-            BulkRequest bulkRequest = bulkRequestModifier.getBulkRequest();
-            ActionListener<BulkResponse> actionListener = bulkRequestModifier.wrapActionListenerIfNeeded(listener);
-            if (bulkRequest.requests().isEmpty()) {
-                // at this stage, the transport bulk action can't deal with a bulk request with no requests,
-                // so we stop and send an empty response back to the client.
-                // (this will happen if pre-processing all items in the bulk failed)
-                actionListener.onResponse(new BulkResponse(new BulkItemResponse[0], 0));
-            } else {
-                chain.proceed(task, action, bulkRequest, actionListener);
-            }
-        });
-    }
-
-    @Override
-    public int order() {
-        return Integer.MAX_VALUE;
-    }
-
-    final static class BulkRequestModifier implements Iterator<ActionRequest> {
-
-        final BulkRequest bulkRequest;
-        final Set<Integer> failedSlots;
-        final List<BulkItemResponse> itemResponses;
-
-        int currentSlot = -1;
-        int[] originalSlots;
-
-        BulkRequestModifier(BulkRequest bulkRequest) {
-            this.bulkRequest = bulkRequest;
-            this.failedSlots = new HashSet<>();
-            this.itemResponses = new ArrayList<>(bulkRequest.requests().size());
-        }
-
-        @Override
-        public ActionRequest next() {
-            return bulkRequest.requests().get(++currentSlot);
-        }
-
-        @Override
-        public boolean hasNext() {
-            return (currentSlot + 1) < bulkRequest.requests().size();
-        }
-
-        BulkRequest getBulkRequest() {
-            if (itemResponses.isEmpty()) {
-                return bulkRequest;
-            } else {
-                BulkRequest modifiedBulkRequest = new BulkRequest(bulkRequest);
-                modifiedBulkRequest.refresh(bulkRequest.refresh());
-                modifiedBulkRequest.consistencyLevel(bulkRequest.consistencyLevel());
-                modifiedBulkRequest.timeout(bulkRequest.timeout());
-
-                int slot = 0;
-                originalSlots = new int[bulkRequest.requests().size() - failedSlots.size()];
-                for (int i = 0; i < bulkRequest.requests().size(); i++) {
-                    ActionRequest request = bulkRequest.requests().get(i);
-                    if (failedSlots.contains(i) == false) {
-                        modifiedBulkRequest.add(request);
-                        originalSlots[slot++] = i;
-                    }
-                }
-                return modifiedBulkRequest;
-            }
-        }
-
-        ActionListener<BulkResponse> wrapActionListenerIfNeeded(ActionListener<BulkResponse> actionListener) {
-            if (itemResponses.isEmpty()) {
-                return actionListener;
-            } else {
-                return new IngestBulkResponseListener(originalSlots, itemResponses, actionListener);
-            }
-        }
-
-        void markCurrentItemAsFailed(Throwable e) {
-            IndexRequest indexRequest = (IndexRequest) bulkRequest.requests().get(currentSlot);
-            // We hit a error during preprocessing a request, so we:
-            // 1) Remember the request item slot from the bulk, so that we're done processing all requests we know what failed
-            // 2) Add a bulk item failure for this request
-            // 3) Continue with the next request in the bulk.
-            failedSlots.add(currentSlot);
-            BulkItemResponse.Failure failure = new BulkItemResponse.Failure(indexRequest.index(), indexRequest.type(), indexRequest.id(), e);
-            itemResponses.add(new BulkItemResponse(currentSlot, indexRequest.opType().lowercase(), failure));
-        }
-
-    }
-
-    private final static class IngestBulkResponseListener implements ActionListener<BulkResponse> {
-
-        private final int[] originalSlots;
-        private final List<BulkItemResponse> itemResponses;
-        private final ActionListener<BulkResponse> actionListener;
-
-        IngestBulkResponseListener(int[] originalSlots, List<BulkItemResponse> itemResponses, ActionListener<BulkResponse> actionListener) {
-            this.itemResponses = itemResponses;
-            this.actionListener = actionListener;
-            this.originalSlots = originalSlots;
-        }
-
-        @Override
-        public void onResponse(BulkResponse bulkItemResponses) {
-            for (int i = 0; i < bulkItemResponses.getItems().length; i++) {
-                itemResponses.add(originalSlots[i], bulkItemResponses.getItems()[i]);
-            }
-            actionListener.onResponse(new BulkResponse(itemResponses.toArray(new BulkItemResponse[itemResponses.size()]), bulkItemResponses.getTookInMillis()));
-        }
-
-        @Override
-        public void onFailure(Throwable e) {
-            actionListener.onFailure(e);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/IngestDisabledActionFilter.java b/core/src/main/java/org/elasticsearch/action/ingest/IngestDisabledActionFilter.java
deleted file mode 100644
index 14abf4e..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/IngestDisabledActionFilter.java
+++ /dev/null
@@ -1,70 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.action.bulk.BulkAction;
-import org.elasticsearch.action.bulk.BulkRequest;
-import org.elasticsearch.action.index.IndexAction;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.support.ActionFilter;
-import org.elasticsearch.action.support.ActionFilterChain;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.tasks.Task;
-
-public final class IngestDisabledActionFilter implements ActionFilter {
-
-    @Override
-    public void apply(Task task, String action, ActionRequest request, ActionListener listener, ActionFilterChain chain) {
-        boolean isIngestRequest = false;
-        if (IndexAction.NAME.equals(action)) {
-            assert request instanceof IndexRequest;
-            IndexRequest indexRequest = (IndexRequest) request;
-            isIngestRequest = Strings.hasText(indexRequest.pipeline());
-        } else if (BulkAction.NAME.equals(action)) {
-            assert request instanceof BulkRequest;
-            BulkRequest bulkRequest = (BulkRequest) request;
-            for (ActionRequest actionRequest : bulkRequest.requests()) {
-                if (actionRequest instanceof IndexRequest) {
-                    IndexRequest indexRequest = (IndexRequest) actionRequest;
-                    if (Strings.hasText(indexRequest.pipeline())) {
-                        isIngestRequest = true;
-                        break;
-                    }
-                }
-            }
-        }
-        if (isIngestRequest) {
-            throw new IllegalArgumentException("node.ingest is set to false, cannot execute pipeline");
-        }
-        chain.proceed(task, action, request, listener);
-    }
-
-    @Override
-    public void apply(String action, ActionResponse response, ActionListener listener, ActionFilterChain chain) {
-        chain.proceed(action, response, listener);
-    }
-
-    @Override
-    public int order() {
-        return Integer.MAX_VALUE;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineAction.java b/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineAction.java
deleted file mode 100644
index 7f37009..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineAction.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.Action;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class PutPipelineAction extends Action<PutPipelineRequest, IndexResponse, PutPipelineRequestBuilder> {
-
-    public static final PutPipelineAction INSTANCE = new PutPipelineAction();
-    public static final String NAME = "cluster:admin/ingest/pipeline/put";
-
-    public PutPipelineAction() {
-        super(NAME);
-    }
-
-    @Override
-    public PutPipelineRequestBuilder newRequestBuilder(ElasticsearchClient client) {
-        return new PutPipelineRequestBuilder(client, this);
-    }
-
-    @Override
-    public IndexResponse newResponse() {
-        return new IndexResponse();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequest.java b/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequest.java
deleted file mode 100644
index 3ee46a0..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequest.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionRequestValidationException;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-
-import java.io.IOException;
-
-import static org.elasticsearch.action.ValidateActions.addValidationError;
-
-public class PutPipelineRequest extends ActionRequest {
-
-    private String id;
-    private BytesReference source;
-
-    @Override
-    public ActionRequestValidationException validate() {
-        ActionRequestValidationException validationException = null;
-        if (id == null) {
-            validationException = addValidationError("id is missing", validationException);
-        }
-        if (source == null) {
-            validationException = addValidationError("source is missing", validationException);
-        }
-        return validationException;
-    }
-
-    public String id() {
-        return id;
-    }
-
-    public void id(String id) {
-        this.id = id;
-    }
-
-    public BytesReference source() {
-        return source;
-    }
-
-    public void source(BytesReference source) {
-        this.source = source;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        id = in.readString();
-        source = in.readBytesReference();
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeString(id);
-        out.writeBytesReference(source);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequestBuilder.java
deleted file mode 100644
index f2b5a8d..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequestBuilder.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionRequestBuilder;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.client.ElasticsearchClient;
-import org.elasticsearch.common.bytes.BytesReference;
-
-public class PutPipelineRequestBuilder extends ActionRequestBuilder<PutPipelineRequest, IndexResponse, PutPipelineRequestBuilder> {
-
-    public PutPipelineRequestBuilder(ElasticsearchClient client, PutPipelineAction action) {
-        super(client, action, new PutPipelineRequest());
-    }
-
-    public PutPipelineRequestBuilder setId(String id) {
-        request.id(id);
-        return this;
-    }
-
-    public PutPipelineRequestBuilder setSource(BytesReference source) {
-        request.source(source);
-        return this;
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineTransportAction.java b/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineTransportAction.java
deleted file mode 100644
index 8f7da7e..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineTransportAction.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.action.support.ActionFilters;
-import org.elasticsearch.action.support.HandledTransportAction;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.IngestBootstrapper;
-import org.elasticsearch.ingest.PipelineStore;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-public class PutPipelineTransportAction extends HandledTransportAction<PutPipelineRequest, IndexResponse> {
-
-    private final PipelineStore pipelineStore;
-
-    @Inject
-    public PutPipelineTransportAction(Settings settings, ThreadPool threadPool, TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver, IngestBootstrapper bootstrapper) {
-        super(settings, PutPipelineAction.NAME, threadPool, transportService, actionFilters, indexNameExpressionResolver, PutPipelineRequest::new);
-        this.pipelineStore = bootstrapper.getPipelineStore();
-    }
-
-    @Override
-    protected void doExecute(PutPipelineRequest request, ActionListener<IndexResponse> listener) {
-        pipelineStore.put(request, listener);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/ReloadPipelinesAction.java b/core/src/main/java/org/elasticsearch/action/ingest/ReloadPipelinesAction.java
deleted file mode 100644
index 452f3a3..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/ReloadPipelinesAction.java
+++ /dev/null
@@ -1,117 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.PipelineStore;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportChannel;
-import org.elasticsearch.transport.TransportException;
-import org.elasticsearch.transport.TransportRequest;
-import org.elasticsearch.transport.TransportRequestHandler;
-import org.elasticsearch.transport.TransportResponse;
-import org.elasticsearch.transport.TransportResponseHandler;
-import org.elasticsearch.transport.TransportService;
-
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.function.Consumer;
-
-/**
- * An internal api that refreshes the in-memory representation of all the pipelines on all ingest nodes.
- */
-public class ReloadPipelinesAction extends AbstractComponent implements TransportRequestHandler<ReloadPipelinesAction.ReloadPipelinesRequest> {
-
-    public static final String ACTION_NAME = "internal:admin/ingest/reload/pipelines";
-
-    private final ClusterService clusterService;
-    private final TransportService transportService;
-    private final PipelineStore pipelineStore;
-
-    public ReloadPipelinesAction(Settings settings, PipelineStore pipelineStore, ClusterService clusterService, TransportService transportService) {
-        super(settings);
-        this.pipelineStore = pipelineStore;
-        this.clusterService = clusterService;
-        this.transportService = transportService;
-        transportService.registerRequestHandler(ACTION_NAME, ReloadPipelinesRequest::new, ThreadPool.Names.MANAGEMENT, this);
-    }
-
-    public void reloadPipelinesOnAllNodes(Consumer<Boolean> listener) {
-        AtomicBoolean failed = new AtomicBoolean();
-        DiscoveryNodes nodes = clusterService.state().getNodes();
-        AtomicInteger expectedResponses = new AtomicInteger(nodes.size());
-        for (DiscoveryNode node : nodes) {
-            ReloadPipelinesRequest nodeRequest = new ReloadPipelinesRequest();
-            transportService.sendRequest(node, ACTION_NAME, nodeRequest, new TransportResponseHandler<ReloadPipelinesResponse>() {
-                @Override
-                public ReloadPipelinesResponse newInstance() {
-                    return new ReloadPipelinesResponse();
-                }
-
-                @Override
-                public void handleResponse(ReloadPipelinesResponse response) {
-                    decrementAndReturn();
-                }
-
-                @Override
-                public void handleException(TransportException exp) {
-                    logger.warn("failed to update pipelines on remote node [{}]", exp, node);
-                    failed.set(true);
-                    decrementAndReturn();
-                }
-
-                void decrementAndReturn() {
-                    if (expectedResponses.decrementAndGet() == 0) {
-                        listener.accept(!failed.get());
-                    }
-                }
-
-                @Override
-                public String executor() {
-                    return ThreadPool.Names.SAME;
-                }
-            });
-        }
-    }
-
-    @Override
-    public void messageReceived(ReloadPipelinesRequest request, TransportChannel channel) throws Exception {
-        try {
-            pipelineStore.updatePipelines();
-            channel.sendResponse(new ReloadPipelinesResponse());
-        } catch (Throwable e) {
-            logger.warn("failed to update pipelines", e);
-            channel.sendResponse(e);
-        }
-    }
-
-    final static class ReloadPipelinesRequest extends TransportRequest {
-
-    }
-
-    final static class ReloadPipelinesResponse extends TransportResponse {
-
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentResult.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentResult.java
deleted file mode 100644
index 7e7682b..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentResult.java
+++ /dev/null
@@ -1,26 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-
-public interface SimulateDocumentResult<T extends SimulateDocumentResult> extends Writeable<T>, ToXContent {
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentSimpleResult.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentSimpleResult.java
deleted file mode 100644
index 3249775..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentSimpleResult.java
+++ /dev/null
@@ -1,95 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.ingest.core.IngestDocument;
-
-import java.io.IOException;
-import java.util.Collections;
-
-public class SimulateDocumentSimpleResult implements SimulateDocumentResult<SimulateDocumentSimpleResult> {
-
-    private static final SimulateDocumentSimpleResult PROTOTYPE = new SimulateDocumentSimpleResult(new WriteableIngestDocument(new IngestDocument(Collections.emptyMap(), Collections.emptyMap())));
-
-    private WriteableIngestDocument ingestDocument;
-    private Exception failure;
-
-    public SimulateDocumentSimpleResult(IngestDocument ingestDocument) {
-        this.ingestDocument = new WriteableIngestDocument(ingestDocument);
-    }
-
-    private SimulateDocumentSimpleResult(WriteableIngestDocument ingestDocument) {
-        this.ingestDocument = ingestDocument;
-    }
-
-    public SimulateDocumentSimpleResult(Exception failure) {
-        this.failure = failure;
-    }
-
-    public IngestDocument getIngestDocument() {
-        if (ingestDocument == null) {
-            return null;
-        }
-        return ingestDocument.getIngestDocument();
-    }
-
-    public Exception getFailure() {
-        return failure;
-    }
-
-    public static SimulateDocumentSimpleResult readSimulateDocumentSimpleResult(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public SimulateDocumentSimpleResult readFrom(StreamInput in) throws IOException {
-        if (in.readBoolean()) {
-            Exception exception = in.readThrowable();
-            return new SimulateDocumentSimpleResult(exception);
-        }
-        return new SimulateDocumentSimpleResult(WriteableIngestDocument.readWriteableIngestDocumentFrom(in));
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        if (failure == null) {
-            out.writeBoolean(false);
-            ingestDocument.writeTo(out);
-        } else {
-            out.writeBoolean(true);
-            out.writeThrowable(failure);
-        }
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        if (failure == null) {
-            ingestDocument.toXContent(builder, params);
-        } else {
-            ElasticsearchException.renderThrowable(builder, params, failure);
-        }
-        builder.endObject();
-        return builder;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentVerboseResult.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentVerboseResult.java
deleted file mode 100644
index 2b119af..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentVerboseResult.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-
-public class SimulateDocumentVerboseResult implements SimulateDocumentResult<SimulateDocumentVerboseResult> {
-
-    private static final SimulateDocumentVerboseResult PROTOTYPE = new SimulateDocumentVerboseResult(Collections.emptyList());
-
-    private final List<SimulateProcessorResult> processorResults;
-
-    public SimulateDocumentVerboseResult(List<SimulateProcessorResult> processorResults) {
-        this.processorResults = processorResults;
-    }
-
-    public List<SimulateProcessorResult> getProcessorResults() {
-        return processorResults;
-    }
-
-    public static SimulateDocumentVerboseResult readSimulateDocumentVerboseResultFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public SimulateDocumentVerboseResult readFrom(StreamInput in) throws IOException {
-        int size = in.readVInt();
-        List<SimulateProcessorResult> processorResults = new ArrayList<>();
-        for (int i = 0; i < size; i++) {
-            processorResults.add(SimulateProcessorResult.readSimulateProcessorResultFrom(in));
-        }
-        return new SimulateDocumentVerboseResult(processorResults);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(processorResults.size());
-        for (SimulateProcessorResult result : processorResults) {
-            result.writeTo(out);
-        }
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        builder.startArray(Fields.PROCESSOR_RESULTS);
-        for (SimulateProcessorResult processorResult : processorResults) {
-            processorResult.toXContent(builder, params);
-        }
-        builder.endArray();
-        builder.endObject();
-        return builder;
-    }
-
-    static final class Fields {
-        static final XContentBuilderString PROCESSOR_RESULTS = new XContentBuilderString("processor_results");
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulateExecutionService.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulateExecutionService.java
deleted file mode 100644
index ccfb652..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulateExecutionService.java
+++ /dev/null
@@ -1,76 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.threadpool.ThreadPool;
-
-import java.util.ArrayList;
-import java.util.List;
-
-class SimulateExecutionService {
-
-    private static final String THREAD_POOL_NAME = ThreadPool.Names.MANAGEMENT;
-
-    private final ThreadPool threadPool;
-
-    SimulateExecutionService(ThreadPool threadPool) {
-        this.threadPool = threadPool;
-    }
-
-    SimulateDocumentResult executeDocument(Pipeline pipeline, IngestDocument ingestDocument, boolean verbose) {
-        if (verbose) {
-            List<SimulateProcessorResult> processorResultList = new ArrayList<>();
-            IngestDocument currentIngestDocument = new IngestDocument(ingestDocument);
-            for (int i = 0; i < pipeline.getProcessors().size(); i++) {
-                Processor processor = pipeline.getProcessors().get(i);
-                String processorId = "processor[" + processor.getType() + "]-" + i;
-                try {
-                    processor.execute(currentIngestDocument);
-                    processorResultList.add(new SimulateProcessorResult(processorId, currentIngestDocument));
-                } catch (Exception e) {
-                    processorResultList.add(new SimulateProcessorResult(processorId, e));
-                }
-                currentIngestDocument = new IngestDocument(currentIngestDocument);
-            }
-            return new SimulateDocumentVerboseResult(processorResultList);
-        } else {
-            try {
-                pipeline.execute(ingestDocument);
-                return new SimulateDocumentSimpleResult(ingestDocument);
-            } catch (Exception e) {
-                return new SimulateDocumentSimpleResult(e);
-            }
-        }
-    }
-
-    public void execute(SimulatePipelineRequest.Parsed request, ActionListener<SimulatePipelineResponse> listener) {
-        threadPool.executor(THREAD_POOL_NAME).execute(() -> {
-            List<SimulateDocumentResult> responses = new ArrayList<>();
-            for (IngestDocument ingestDocument : request.getDocuments()) {
-                responses.add(executeDocument(request.getPipeline(), ingestDocument, request.isVerbose()));
-            }
-            listener.onResponse(new SimulatePipelineResponse(request.getPipeline().getId(), request.isVerbose(), responses));
-        });
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineAction.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineAction.java
deleted file mode 100644
index c1d219a..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineAction.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.Action;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class SimulatePipelineAction extends Action<SimulatePipelineRequest, SimulatePipelineResponse, SimulatePipelineRequestBuilder> {
-
-    public static final SimulatePipelineAction INSTANCE = new SimulatePipelineAction();
-    public static final String NAME = "cluster:admin/ingest/pipeline/simulate";
-
-    public SimulatePipelineAction() {
-        super(NAME);
-    }
-
-    @Override
-    public SimulatePipelineRequestBuilder newRequestBuilder(ElasticsearchClient client) {
-        return new SimulatePipelineRequestBuilder(client, this);
-    }
-
-    @Override
-    public SimulatePipelineResponse newResponse() {
-        return new SimulatePipelineResponse();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequest.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequest.java
deleted file mode 100644
index ccc51e7..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequest.java
+++ /dev/null
@@ -1,162 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionRequestValidationException;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.ingest.PipelineStore;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-
-import static org.elasticsearch.action.ValidateActions.addValidationError;
-import static org.elasticsearch.ingest.core.IngestDocument.MetaData;
-
-public class SimulatePipelineRequest extends ActionRequest {
-
-    private String id;
-    private boolean verbose;
-    private BytesReference source;
-
-    @Override
-    public ActionRequestValidationException validate() {
-        ActionRequestValidationException validationException = null;
-        if (source == null) {
-            validationException = addValidationError("source is missing", validationException);
-        }
-        return validationException;
-    }
-
-    public String getId() {
-        return id;
-    }
-
-    public void setId(String id) {
-        this.id = id;
-    }
-
-    public boolean isVerbose() {
-        return verbose;
-    }
-
-    public void setVerbose(boolean verbose) {
-        this.verbose = verbose;
-    }
-
-    public BytesReference getSource() {
-        return source;
-    }
-
-    public void setSource(BytesReference source) {
-        this.source = source;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        id = in.readString();
-        verbose = in.readBoolean();
-        source = in.readBytesReference();
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeString(id);
-        out.writeBoolean(verbose);
-        out.writeBytesReference(source);
-    }
-
-    public static final class Fields {
-        static final String PIPELINE = "pipeline";
-        static final String DOCS = "docs";
-        static final String SOURCE = "_source";
-    }
-
-    static class Parsed {
-        private final List<IngestDocument> documents;
-        private final Pipeline pipeline;
-        private final boolean verbose;
-
-        Parsed(Pipeline pipeline, List<IngestDocument> documents, boolean verbose) {
-            this.pipeline = pipeline;
-            this.documents = Collections.unmodifiableList(documents);
-            this.verbose = verbose;
-        }
-
-        public Pipeline getPipeline() {
-            return pipeline;
-        }
-
-        public List<IngestDocument> getDocuments() {
-            return documents;
-        }
-
-        public boolean isVerbose() {
-            return verbose;
-        }
-    }
-
-    private static final Pipeline.Factory PIPELINE_FACTORY = new Pipeline.Factory();
-    static final String SIMULATED_PIPELINE_ID = "_simulate_pipeline";
-
-    static Parsed parseWithPipelineId(String pipelineId, Map<String, Object> config, boolean verbose, PipelineStore pipelineStore) {
-        if (pipelineId == null) {
-            throw new IllegalArgumentException("param [pipeline] is null");
-        }
-        Pipeline pipeline = pipelineStore.get(pipelineId);
-        List<IngestDocument> ingestDocumentList = parseDocs(config);
-        return new Parsed(pipeline, ingestDocumentList, verbose);
-    }
-
-    static Parsed parse(Map<String, Object> config, boolean verbose, PipelineStore pipelineStore) throws Exception {
-        Map<String, Object> pipelineConfig = ConfigurationUtils.readMap(config, Fields.PIPELINE);
-        Pipeline pipeline = PIPELINE_FACTORY.create(SIMULATED_PIPELINE_ID, pipelineConfig, pipelineStore.getProcessorFactoryRegistry());
-        List<IngestDocument> ingestDocumentList = parseDocs(config);
-        return new Parsed(pipeline, ingestDocumentList, verbose);
-    }
-
-    private static List<IngestDocument> parseDocs(Map<String, Object> config) {
-        List<Map<String, Object>> docs = ConfigurationUtils.readList(config, Fields.DOCS);
-        List<IngestDocument> ingestDocumentList = new ArrayList<>();
-        for (Map<String, Object> dataMap : docs) {
-            Map<String, Object> document = ConfigurationUtils.readMap(dataMap, Fields.SOURCE);
-            IngestDocument ingestDocument = new IngestDocument(ConfigurationUtils.readStringProperty(dataMap, MetaData.INDEX.getFieldName(), "_index"),
-                    ConfigurationUtils.readStringProperty(dataMap, MetaData.TYPE.getFieldName(), "_type"),
-                    ConfigurationUtils.readStringProperty(dataMap, MetaData.ID.getFieldName(), "_id"),
-                    ConfigurationUtils.readOptionalStringProperty(dataMap, MetaData.ROUTING.getFieldName()),
-                    ConfigurationUtils.readOptionalStringProperty(dataMap, MetaData.PARENT.getFieldName()),
-                    ConfigurationUtils.readOptionalStringProperty(dataMap, MetaData.TIMESTAMP.getFieldName()),
-                    ConfigurationUtils.readOptionalStringProperty(dataMap, MetaData.TTL.getFieldName()),
-                    document);
-            ingestDocumentList.add(ingestDocument);
-        }
-        return ingestDocumentList;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequestBuilder.java
deleted file mode 100644
index d2e259f..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequestBuilder.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionRequestBuilder;
-import org.elasticsearch.client.ElasticsearchClient;
-import org.elasticsearch.common.bytes.BytesReference;
-
-public class SimulatePipelineRequestBuilder extends ActionRequestBuilder<SimulatePipelineRequest, SimulatePipelineResponse, SimulatePipelineRequestBuilder> {
-
-    public SimulatePipelineRequestBuilder(ElasticsearchClient client, SimulatePipelineAction action) {
-        super(client, action, new SimulatePipelineRequest());
-    }
-
-    public SimulatePipelineRequestBuilder setId(String id) {
-        request.setId(id);
-        return this;
-    }
-
-    public SimulatePipelineRequestBuilder setVerbose(boolean verbose) {
-        request.setVerbose(verbose);
-        return this;
-    }
-
-    public SimulatePipelineRequestBuilder setSource(BytesReference source) {
-        request.setSource(source);
-        return this;
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineResponse.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineResponse.java
deleted file mode 100644
index 7a9ab0b..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineResponse.java
+++ /dev/null
@@ -1,103 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-
-public class SimulatePipelineResponse extends ActionResponse implements ToXContent {
-    private String pipelineId;
-    private boolean verbose;
-    private List<SimulateDocumentResult> results;
-
-    public SimulatePipelineResponse() {
-
-    }
-
-    public SimulatePipelineResponse(String pipelineId, boolean verbose, List<SimulateDocumentResult> responses) {
-        this.pipelineId = pipelineId;
-        this.verbose = verbose;
-        this.results = Collections.unmodifiableList(responses);
-    }
-
-    public String getPipelineId() {
-        return pipelineId;
-    }
-
-    public List<SimulateDocumentResult> getResults() {
-        return results;
-    }
-
-    public boolean isVerbose() {
-        return verbose;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeString(pipelineId);
-        out.writeBoolean(verbose);
-        out.writeVInt(results.size());
-        for (SimulateDocumentResult response : results) {
-            response.writeTo(out);
-        }
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        this.pipelineId = in.readString();
-        boolean verbose = in.readBoolean();
-        int responsesLength = in.readVInt();
-        results = new ArrayList<>();
-        for (int i = 0; i < responsesLength; i++) {
-            SimulateDocumentResult<?> simulateDocumentResult;
-            if (verbose) {
-                simulateDocumentResult = SimulateDocumentVerboseResult.readSimulateDocumentVerboseResultFrom(in);
-            } else {
-                simulateDocumentResult = SimulateDocumentSimpleResult.readSimulateDocumentSimpleResult(in);
-            }
-            results.add(simulateDocumentResult);
-        }
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startArray(Fields.DOCUMENTS);
-        for (SimulateDocumentResult response : results) {
-            response.toXContent(builder, params);
-        }
-        builder.endArray();
-        return builder;
-    }
-
-    static final class Fields {
-        static final XContentBuilderString DOCUMENTS = new XContentBuilderString("docs");
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineTransportAction.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineTransportAction.java
deleted file mode 100644
index 3d5e02a..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineTransportAction.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.support.ActionFilters;
-import org.elasticsearch.action.support.HandledTransportAction;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.ingest.IngestBootstrapper;
-import org.elasticsearch.ingest.PipelineStore;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-import java.util.Map;
-
-public class SimulatePipelineTransportAction extends HandledTransportAction<SimulatePipelineRequest, SimulatePipelineResponse> {
-
-    private final PipelineStore pipelineStore;
-    private final SimulateExecutionService executionService;
-
-    @Inject
-    public SimulatePipelineTransportAction(Settings settings, ThreadPool threadPool, TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver, IngestBootstrapper bootstrapper) {
-        super(settings, SimulatePipelineAction.NAME, threadPool, transportService, actionFilters, indexNameExpressionResolver, SimulatePipelineRequest::new);
-        this.pipelineStore = bootstrapper.getPipelineStore();
-        this.executionService = new SimulateExecutionService(threadPool);
-    }
-
-    @Override
-    protected void doExecute(SimulatePipelineRequest request, ActionListener<SimulatePipelineResponse> listener) {
-        Map<String, Object> source = XContentHelper.convertToMap(request.getSource(), false).v2();
-
-        SimulatePipelineRequest.Parsed simulateRequest;
-        try {
-            if (request.getId() != null) {
-                simulateRequest = SimulatePipelineRequest.parseWithPipelineId(request.getId(), source, request.isVerbose(), pipelineStore);
-            } else {
-                simulateRequest = SimulatePipelineRequest.parse(source, request.isVerbose(), pipelineStore);
-            }
-        } catch (Exception e) {
-            listener.onFailure(e);
-            return;
-        }
-
-        executionService.execute(simulateRequest, listener);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java
deleted file mode 100644
index afa85b4..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-import org.elasticsearch.ingest.core.IngestDocument;
-
-import java.io.IOException;
-import java.util.Collections;
-
-public class SimulateProcessorResult implements Writeable<SimulateProcessorResult>, ToXContent {
-
-    private static final SimulateProcessorResult PROTOTYPE = new SimulateProcessorResult("_na", new WriteableIngestDocument(new IngestDocument(Collections.emptyMap(), Collections.emptyMap())));
-
-    private String processorId;
-    private WriteableIngestDocument ingestDocument;
-    private Exception failure;
-
-    public SimulateProcessorResult(String processorId, IngestDocument ingestDocument) {
-        this.processorId = processorId;
-        this.ingestDocument = new WriteableIngestDocument(ingestDocument);
-    }
-
-    private SimulateProcessorResult(String processorId, WriteableIngestDocument ingestDocument) {
-        this.processorId = processorId;
-        this.ingestDocument = ingestDocument;
-    }
-
-    public SimulateProcessorResult(String processorId, Exception failure) {
-        this.processorId = processorId;
-        this.failure = failure;
-    }
-
-    public IngestDocument getIngestDocument() {
-        if (ingestDocument == null) {
-            return null;
-        }
-        return ingestDocument.getIngestDocument();
-    }
-
-    public String getProcessorId() {
-        return processorId;
-    }
-
-    public Exception getFailure() {
-        return failure;
-    }
-
-    public static SimulateProcessorResult readSimulateProcessorResultFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public SimulateProcessorResult readFrom(StreamInput in) throws IOException {
-        String processorId = in.readString();
-        if (in.readBoolean()) {
-            Exception exception = in.readThrowable();
-            return new SimulateProcessorResult(processorId, exception);
-        }
-        return new SimulateProcessorResult(processorId, WriteableIngestDocument.readWriteableIngestDocumentFrom(in));
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(processorId);
-        if (failure == null) {
-            out.writeBoolean(false);
-            ingestDocument.writeTo(out);
-        } else {
-            out.writeBoolean(true);
-            out.writeThrowable(failure);
-        }
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        builder.field(Fields.PROCESSOR_ID, processorId);
-        if (failure == null) {
-            ingestDocument.toXContent(builder, params);
-        } else {
-            ElasticsearchException.renderThrowable(builder, params, failure);
-        }
-        builder.endObject();
-        return builder;
-    }
-
-    static final class Fields {
-        static final XContentBuilderString PROCESSOR_ID = new XContentBuilderString("processor_id");
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java b/core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java
deleted file mode 100644
index 0f33f00..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java
+++ /dev/null
@@ -1,112 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-import org.elasticsearch.ingest.core.IngestDocument;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Map;
-import java.util.Objects;
-
-final class WriteableIngestDocument implements Writeable<WriteableIngestDocument>, ToXContent {
-
-    private static final WriteableIngestDocument PROTOTYPE = new WriteableIngestDocument(new IngestDocument(Collections.emptyMap(), Collections.emptyMap()));
-
-    private final IngestDocument ingestDocument;
-
-    WriteableIngestDocument(IngestDocument ingestDocument) {
-        assert ingestDocument != null;
-        this.ingestDocument = ingestDocument;
-    }
-
-    IngestDocument getIngestDocument() {
-        return ingestDocument;
-    }
-
-    static WriteableIngestDocument readWriteableIngestDocumentFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public WriteableIngestDocument readFrom(StreamInput in) throws IOException {
-        Map<String, Object> sourceAndMetadata = in.readMap();
-        @SuppressWarnings("unchecked")
-        Map<String, String> ingestMetadata = (Map<String, String>) in.readGenericValue();
-        return new WriteableIngestDocument(new IngestDocument(sourceAndMetadata, ingestMetadata));
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeMap(ingestDocument.getSourceAndMetadata());
-        out.writeGenericValue(ingestDocument.getIngestMetadata());
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(Fields.DOCUMENT);
-        Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
-        for (Map.Entry<IngestDocument.MetaData, String> metadata : metadataMap.entrySet()) {
-            builder.field(metadata.getKey().getFieldName(), metadata.getValue());
-        }
-        builder.field(Fields.SOURCE, ingestDocument.getSourceAndMetadata());
-        builder.startObject(Fields.INGEST);
-        for (Map.Entry<String, String> ingestMetadata : ingestDocument.getIngestMetadata().entrySet()) {
-            builder.field(ingestMetadata.getKey(), ingestMetadata.getValue());
-        }
-        builder.endObject();
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public boolean equals(Object o) {
-        if (this == o) {
-            return true;
-        }
-        if (o == null || getClass() != o.getClass()) {
-            return false;
-        }
-        WriteableIngestDocument that = (WriteableIngestDocument) o;
-        return Objects.equals(ingestDocument, that.ingestDocument);
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(ingestDocument);
-    }
-
-    @Override
-    public String toString() {
-        return ingestDocument.toString();
-    }
-
-    static final class Fields {
-        static final XContentBuilderString DOCUMENT = new XContentBuilderString("doc");
-        static final XContentBuilderString SOURCE = new XContentBuilderString("_source");
-        static final XContentBuilderString INGEST = new XContentBuilderString("_ingest");
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java b/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
index ad1ea75..9ba1f2d 100644
--- a/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
+++ b/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
@@ -269,7 +269,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
                 if (indexServiceOrNull !=  null) {
                     IndexShard shard = indexService.getShardOrNull(request.shardId());
                     if (shard != null) {
-                        shard.indexingService().noopUpdate(request.type());
+                        shard.noopUpdate(request.type());
                     }
                 }
                 listener.onResponse(update);
diff --git a/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java b/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
index daed566..3b8be66 100644
--- a/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
+++ b/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
@@ -147,7 +147,7 @@ public class TransportClient extends AbstractClient {
                         // noop
                     }
                 });
-                modules.add(new ActionModule(this.settings, true));
+                modules.add(new ActionModule(true));
                 modules.add(new CircuitBreakerModule(this.settings));
 
                 pluginsService.processModules(modules);
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java b/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java
index c037024..5dce6d5 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java
@@ -67,7 +67,7 @@ import org.elasticsearch.gateway.GatewayAllocator;
 import org.elasticsearch.gateway.PrimaryShardAllocator;
 import org.elasticsearch.index.IndexSettings;
 import org.elasticsearch.index.engine.EngineConfig;
-import org.elasticsearch.index.indexing.IndexingSlowLog;
+import org.elasticsearch.index.IndexingSlowLog;
 import org.elasticsearch.index.search.stats.SearchSlowLog;
 import org.elasticsearch.index.settings.IndexDynamicSettings;
 import org.elasticsearch.index.shard.IndexShard;
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
index d7b70b8..c1f1007 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
@@ -859,10 +859,16 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
             if (parser.currentToken() == XContentParser.Token.START_OBJECT) {  // on a start object move to next token
                 parser.nextToken();
             }
+            if (parser.currentToken() != XContentParser.Token.FIELD_NAME) {
+                throw new IllegalArgumentException("expected field name but got a " + parser.currentToken());
+            }
             Builder builder = new Builder(parser.currentName());
 
             String currentFieldName = null;
             XContentParser.Token token = parser.nextToken();
+            if (token != XContentParser.Token.START_OBJECT) {
+                throw new IllegalArgumentException("expected object but got a " + token);
+            }
             while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                 if (token == XContentParser.Token.FIELD_NAME) {
                     currentFieldName = parser.currentName();
@@ -877,6 +883,8 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
                                 String mappingType = currentFieldName;
                                 Map<String, Object> mappingSource = MapBuilder.<String, Object>newMapBuilder().put(mappingType, parser.mapOrdered()).map();
                                 builder.putMapping(new MappingMetaData(mappingType, mappingSource));
+                            } else {
+                                throw new IllegalArgumentException("Unexpected token: " + token);
                             }
                         }
                     } else if ("aliases".equals(currentFieldName)) {
@@ -896,6 +904,8 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
                                     }
                                 }
                                 builder.putActiveAllocationIds(Integer.valueOf(shardId), allocationIds);
+                            } else {
+                                throw new IllegalArgumentException("Unexpected token: " + token);
                             }
                         }
                     } else if ("warmers".equals(currentFieldName)) {
@@ -904,6 +914,7 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
                         // ignore: warmers have been removed in 3.0 and are
                         // simply ignored when upgrading from 2.x
                         assert Version.CURRENT.major <= 3;
+                        parser.skipChildren();
                     } else {
                         // check if its a custom index metadata
                         Custom proto = lookupPrototype(currentFieldName);
@@ -928,13 +939,19 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
                                 }
                             }
                         }
+                    } else {
+                        throw new IllegalArgumentException("Unexpected field for an array " + currentFieldName);
                     }
                 } else if (token.isValue()) {
                     if ("state".equals(currentFieldName)) {
                         builder.state(State.fromString(parser.text()));
                     } else if ("version".equals(currentFieldName)) {
                         builder.version(parser.longValue());
+                    } else {
+                        throw new IllegalArgumentException("Unexpected field [" + currentFieldName + "]");
                     }
+                } else {
+                    throw new IllegalArgumentException("Unexpected token " + token);
                 }
             }
             return builder.build();
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java
index 4b1514d..d2f3a47 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java
@@ -237,7 +237,7 @@ public class IndexNameExpressionResolver extends AbstractComponent {
     public String[] filteringAliases(ClusterState state, String index, String... expressions) {
         // expand the aliases wildcard
         List<String> resolvedExpressions = expressions != null ? Arrays.asList(expressions) : Collections.<String>emptyList();
-        Context context = new Context(state, IndicesOptions.lenientExpandOpen());
+        Context context = new Context(state, IndicesOptions.lenientExpandOpen(), true);
         for (ExpressionResolver expressionResolver : expressionResolvers) {
             resolvedExpressions = expressionResolver.resolve(context, resolvedExpressions);
         }
@@ -459,17 +459,25 @@ public class IndexNameExpressionResolver extends AbstractComponent {
         private final ClusterState state;
         private final IndicesOptions options;
         private final long startTime;
+        private final boolean preserveAliases;
 
         Context(ClusterState state, IndicesOptions options) {
-            this.state = state;
-            this.options = options;
-            startTime = System.currentTimeMillis();
+            this(state, options, System.currentTimeMillis());
+        }
+
+        Context(ClusterState state, IndicesOptions options, boolean preserveAliases) {
+            this(state, options, System.currentTimeMillis(), preserveAliases);
         }
 
         public Context(ClusterState state, IndicesOptions options, long startTime) {
+           this(state, options, startTime, false);
+        }
+
+        public Context(ClusterState state, IndicesOptions options, long startTime, boolean preserveAliases) {
             this.state = state;
             this.options = options;
             this.startTime = startTime;
+            this.preserveAliases = preserveAliases;
         }
 
         public ClusterState getState() {
@@ -483,6 +491,15 @@ public class IndexNameExpressionResolver extends AbstractComponent {
         public long getStartTime() {
             return startTime;
         }
+
+        /**
+         * This is used to prevent resolving aliases to concrete indices but this also means
+         * that we might return aliases that point to a closed index. This is currently only used
+         * by {@link #filteringAliases(ClusterState, String, String...)} since it's the only one that needs aliases
+         */
+        boolean isPreserveAliases() {
+            return preserveAliases;
+        }
     }
 
     private interface ExpressionResolver {
@@ -531,6 +548,9 @@ public class IndexNameExpressionResolver extends AbstractComponent {
                     }
                     continue;
                 }
+                if (Strings.isEmpty(expression)) {
+                    throw new IndexNotFoundException(expression);
+                }
                 boolean add = true;
                 if (expression.charAt(0) == '+') {
                     // if its the first, add empty result set
@@ -612,21 +632,24 @@ public class IndexNameExpressionResolver extends AbstractComponent {
                             .filter(e -> Regex.simpleMatch(pattern, e.getKey()))
                             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
                 }
+                Set<String> expand = new HashSet<>();
                 for (Map.Entry<String, AliasOrIndex> entry : matches.entrySet()) {
                     AliasOrIndex aliasOrIndex = entry.getValue();
-                    if (aliasOrIndex.isAlias() == false) {
-                        AliasOrIndex.Index index = (AliasOrIndex.Index) aliasOrIndex;
-                        if (excludeState != null && index.getIndex().getState() == excludeState) {
-                            continue;
-                        }
-                    }
-
-                    if (add) {
-                        result.add(entry.getKey());
+                    if (context.isPreserveAliases() && aliasOrIndex.isAlias()) {
+                        expand.add(entry.getKey());
                     } else {
-                        result.remove(entry.getKey());
+                        for (IndexMetaData meta : aliasOrIndex.getIndices()) {
+                            if (excludeState == null || meta.getState() != excludeState) {
+                                expand.add(meta.getIndex());
+                            }
+                        }
                     }
                 }
+                if (add) {
+                    result.addAll(expand);
+                } else {
+                    result.removeAll(expand);
+                }
 
                 if (matches.isEmpty() && options.allowNoIndices() == false) {
                     IndexNotFoundException infe = new IndexNotFoundException(expression);
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
index d96e1f1..002d1a5 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
@@ -1074,14 +1074,20 @@ public class MetaData implements Iterable<IndexMetaData>, Diffable<MetaData>, Fr
                 if (token == XContentParser.Token.START_OBJECT) {
                     // move to the field name (meta-data)
                     token = parser.nextToken();
+                    if (token != XContentParser.Token.FIELD_NAME) {
+                        throw new IllegalArgumentException("Expected a field name but got " + token);
+                    }
                     // move to the next object
                     token = parser.nextToken();
                 }
                 currentFieldName = parser.currentName();
-                if (token == null) {
-                    // no data...
-                    return builder.build();
-                }
+            }
+
+            if (!"meta-data".equals(parser.currentName())) {
+                throw new IllegalArgumentException("Expected [meta-data] as a field name but got " + currentFieldName);
+            }
+            if (token != XContentParser.Token.START_OBJECT) {
+                throw new IllegalArgumentException("Expected a START_OBJECT but got " + token);
             }
 
             while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -1114,7 +1120,11 @@ public class MetaData implements Iterable<IndexMetaData>, Diffable<MetaData>, Fr
                         builder.version = parser.longValue();
                     } else if ("cluster_uuid".equals(currentFieldName) || "uuid".equals(currentFieldName)) {
                         builder.clusterUUID = parser.text();
+                    } else {
+                        throw new IllegalArgumentException("Unexpected field [" + currentFieldName + "]");
                     }
+                } else {
+                    throw new IllegalArgumentException("Unexpected token " + token);
                 }
             }
             return builder.build();
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java b/core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java
index bcf489c..bb186a6 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java
@@ -26,6 +26,7 @@ import org.apache.lucene.util.CollectionUtil;
 import org.elasticsearch.cluster.AbstractDiffable;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.common.Randomness;
 import org.elasticsearch.common.collect.ImmutableOpenIntMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -39,7 +40,6 @@ import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Set;
-import java.util.concurrent.ThreadLocalRandom;
 
 /**
  * The {@link IndexRoutingTable} represents routing information for a single
@@ -71,7 +71,7 @@ public class IndexRoutingTable extends AbstractDiffable<IndexRoutingTable> imple
 
     IndexRoutingTable(String index, ImmutableOpenIntMap<IndexShardRoutingTable> shards) {
         this.index = index;
-        this.shuffler = new RotationShardShuffler(ThreadLocalRandom.current().nextInt());
+        this.shuffler = new RotationShardShuffler(Randomness.get().nextInt());
         this.shards = shards;
         List<ShardRouting> allActiveShards = new ArrayList<>();
         for (IntObjectCursor<IndexShardRoutingTable> cursor : shards) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java b/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java
index d425b63..bcdb7a4 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java
@@ -21,6 +21,7 @@ package org.elasticsearch.cluster.routing;
 
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
+import org.elasticsearch.common.Randomness;
 import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -36,7 +37,6 @@ import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.concurrent.ThreadLocalRandom;
 
 import static java.util.Collections.emptyMap;
 
@@ -73,7 +73,7 @@ public class IndexShardRoutingTable implements Iterable<ShardRouting> {
 
     IndexShardRoutingTable(ShardId shardId, List<ShardRouting> shards) {
         this.shardId = shardId;
-        this.shuffler = new RotationShardShuffler(ThreadLocalRandom.current().nextInt());
+        this.shuffler = new RotationShardShuffler(Randomness.get().nextInt());
         this.shards = Collections.unmodifiableList(shards);
 
         ShardRouting primary = null;
diff --git a/core/src/main/java/org/elasticsearch/common/Randomness.java b/core/src/main/java/org/elasticsearch/common/Randomness.java
index dbfa803..7f71afc 100644
--- a/core/src/main/java/org/elasticsearch/common/Randomness.java
+++ b/core/src/main/java/org/elasticsearch/common/Randomness.java
@@ -109,6 +109,7 @@ public final class Randomness {
         }
     }
 
+    @SuppressForbidden(reason = "ThreadLocalRandom is okay when not running tests")
     private static Random getWithoutSeed() {
         assert currentMethod == null && getRandomMethod == null : "running under tests but tried to create non-reproducible random";
         return ThreadLocalRandom.current();
diff --git a/core/src/main/java/org/elasticsearch/common/SearchScrollIterator.java b/core/src/main/java/org/elasticsearch/common/SearchScrollIterator.java
deleted file mode 100644
index 18535d1..0000000
--- a/core/src/main/java/org/elasticsearch/common/SearchScrollIterator.java
+++ /dev/null
@@ -1,93 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common;
-
-import org.elasticsearch.action.search.SearchRequest;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchScrollRequest;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.search.SearchHit;
-
-import java.util.Collections;
-import java.util.Iterator;
-
-/**
- * An iterator that easily helps to consume all hits from a scroll search.
- */
-public final class SearchScrollIterator implements Iterator<SearchHit> {
-
-    /**
-     * Creates an iterator that returns all matching hits of a scroll search via an iterator.
-     * The iterator will return all hits per scroll search and execute additional scroll searches
-     * to get more hits until all hits have been returned by the scroll search on the ES side.
-     */
-    public static Iterable<SearchHit> createIterator(Client client, TimeValue scrollTimeout, SearchRequest searchRequest) {
-        searchRequest.scroll(scrollTimeout);
-        SearchResponse searchResponse = client.search(searchRequest).actionGet(scrollTimeout);
-        if (searchResponse.getHits().getTotalHits() == 0) {
-            return Collections.emptyList();
-        } else {
-            return () -> new SearchScrollIterator(client, scrollTimeout, searchResponse);
-        }
-    }
-
-    private final Client client;
-    private final TimeValue scrollTimeout;
-
-    private int currentIndex;
-    private SearchHit[] currentHits;
-    private SearchResponse searchResponse;
-
-    private SearchScrollIterator(Client client, TimeValue scrollTimeout, SearchResponse searchResponse) {
-        this.client = client;
-        this.scrollTimeout = scrollTimeout;
-        this.searchResponse = searchResponse;
-        this.currentHits = searchResponse.getHits().getHits();
-    }
-
-    @Override
-    public boolean hasNext() {
-        if (currentIndex < currentHits.length) {
-            return true;
-        } else {
-            if (searchResponse == null) {
-                return false;
-            }
-
-            SearchScrollRequest request = new SearchScrollRequest(searchResponse.getScrollId());
-            request.scroll(scrollTimeout);
-            searchResponse = client.searchScroll(request).actionGet(scrollTimeout);
-            if (searchResponse.getHits().getHits().length == 0) {
-                searchResponse = null;
-                return false;
-            } else {
-                currentHits = searchResponse.getHits().getHits();
-                currentIndex = 0;
-                return true;
-            }
-        }
-    }
-
-    @Override
-    public SearchHit next() {
-        return currentHits[currentIndex++];
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/BytesStreamOutput.java b/core/src/main/java/org/elasticsearch/common/io/stream/BytesStreamOutput.java
index 155a8ca..2d2719a 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/BytesStreamOutput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/BytesStreamOutput.java
@@ -39,10 +39,12 @@ public class BytesStreamOutput extends StreamOutput implements BytesStream {
     protected int count;
 
     /**
-     * Create a non recycling {@link BytesStreamOutput} with 1 initial page acquired.
+     * Create a non recycling {@link BytesStreamOutput} with an initial capacity of 0.
      */
     public BytesStreamOutput() {
-        this(BigArrays.PAGE_SIZE_IN_BYTES);
+        // since this impl is not recycling anyway, don't bother aligning to
+        // the page size, this will even save memory
+        this(0);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/common/math/UnboxedMathUtils.java b/core/src/main/java/org/elasticsearch/common/math/UnboxedMathUtils.java
deleted file mode 100644
index 6c8e0b4..0000000
--- a/core/src/main/java/org/elasticsearch/common/math/UnboxedMathUtils.java
+++ /dev/null
@@ -1,593 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.math;
-
-import org.elasticsearch.common.SuppressForbidden;
-
-import java.util.concurrent.ThreadLocalRandom;
-
-/**
- *
- */
-public class UnboxedMathUtils {
-
-    public static double sin(Short a) {
-        return Math.sin(a.doubleValue());
-    }
-
-    public static double sin(Integer a) {
-        return Math.sin(a.doubleValue());
-    }
-
-    public static double sin(Float a) {
-        return Math.sin(a.doubleValue());
-    }
-
-    public static double sin(Long a) {
-        return Math.sin(a.doubleValue());
-    }
-
-    public static double sin(Double a) {
-        return Math.sin(a);
-    }
-
-    public static double cos(Short a) {
-        return Math.cos(a.doubleValue());
-    }
-
-    public static double cos(Integer a) {
-        return Math.cos(a.doubleValue());
-    }
-
-    public static double cos(Float a) {
-        return Math.cos(a.doubleValue());
-    }
-
-    public static double cos(Long a) {
-        return Math.cos(a.doubleValue());
-    }
-
-    public static double cos(Double a) {
-        return Math.cos(a);
-    }
-
-    public static double tan(Short a) {
-        return Math.tan(a.doubleValue());
-    }
-
-    public static double tan(Integer a) {
-        return Math.tan(a.doubleValue());
-    }
-
-    public static double tan(Float a) {
-        return Math.tan(a.doubleValue());
-    }
-
-    public static double tan(Long a) {
-        return Math.tan(a.doubleValue());
-    }
-
-    public static double tan(Double a) {
-        return Math.tan(a);
-    }
-
-    public static double asin(Short a) {
-        return Math.asin(a.doubleValue());
-    }
-
-    public static double asin(Integer a) {
-        return Math.asin(a.doubleValue());
-    }
-
-    public static double asin(Float a) {
-        return Math.asin(a.doubleValue());
-    }
-
-    public static double asin(Long a) {
-        return Math.asin(a.doubleValue());
-    }
-
-    public static double asin(Double a) {
-        return Math.asin(a);
-    }
-
-    public static double acos(Short a) {
-        return Math.acos(a.doubleValue());
-    }
-
-
-    public static double acos(Integer a) {
-        return Math.acos(a.doubleValue());
-    }
-
-
-    public static double acos(Float a) {
-        return Math.acos(a.doubleValue());
-    }
-
-    public static double acos(Long a) {
-        return Math.acos(a.doubleValue());
-    }
-
-    public static double acos(Double a) {
-        return Math.acos(a);
-    }
-
-    public static double atan(Short a) {
-        return Math.atan(a.doubleValue());
-    }
-
-    public static double atan(Integer a) {
-        return Math.atan(a.doubleValue());
-    }
-
-    public static double atan(Float a) {
-        return Math.atan(a.doubleValue());
-    }
-
-    public static double atan(Long a) {
-        return Math.atan(a.doubleValue());
-    }
-
-    public static double atan(Double a) {
-        return Math.atan(a);
-    }
-
-    public static double toRadians(Short angdeg) {
-        return Math.toRadians(angdeg.doubleValue());
-    }
-
-    public static double toRadians(Integer angdeg) {
-        return Math.toRadians(angdeg.doubleValue());
-    }
-
-    public static double toRadians(Float angdeg) {
-        return Math.toRadians(angdeg.doubleValue());
-    }
-
-    public static double toRadians(Long angdeg) {
-        return Math.toRadians(angdeg.doubleValue());
-    }
-
-    public static double toRadians(Double angdeg) {
-        return Math.toRadians(angdeg);
-    }
-
-    public static double toDegrees(Short angrad) {
-        return Math.toDegrees(angrad.doubleValue());
-    }
-
-    public static double toDegrees(Integer angrad) {
-        return Math.toDegrees(angrad.doubleValue());
-    }
-
-    public static double toDegrees(Float angrad) {
-        return Math.toDegrees(angrad.doubleValue());
-    }
-
-    public static double toDegrees(Long angrad) {
-        return Math.toDegrees(angrad.doubleValue());
-    }
-
-    public static double toDegrees(Double angrad) {
-        return Math.toDegrees(angrad);
-    }
-
-    public static double exp(Short a) {
-        return Math.exp(a.doubleValue());
-    }
-
-    public static double exp(Integer a) {
-        return Math.exp(a.doubleValue());
-    }
-
-    public static double exp(Float a) {
-        return Math.exp(a.doubleValue());
-    }
-
-    public static double exp(Long a) {
-        return Math.exp(a.doubleValue());
-    }
-
-    public static double exp(Double a) {
-        return Math.exp(a);
-    }
-
-    public static double log(Short a) {
-        return Math.log(a.doubleValue());
-    }
-
-    public static double log(Integer a) {
-        return Math.log(a.doubleValue());
-    }
-
-    public static double log(Float a) {
-        return Math.log(a.doubleValue());
-    }
-
-    public static double log(Long a) {
-        return Math.log(a.doubleValue());
-    }
-
-    public static double log(Double a) {
-        return Math.log(a);
-    }
-
-    public static double log10(Short a) {
-        return Math.log10(a.doubleValue());
-    }
-
-    public static double log10(Integer a) {
-        return Math.log10(a.doubleValue());
-    }
-
-    public static double log10(Float a) {
-        return Math.log10(a.doubleValue());
-    }
-
-    public static double log10(Long a) {
-        return Math.log10(a.doubleValue());
-    }
-
-    public static double log10(Double a) {
-        return Math.log10(a);
-    }
-
-    public static double sqrt(Short a) {
-        return Math.sqrt(a.doubleValue());
-    }
-
-    public static double sqrt(Integer a) {
-        return Math.sqrt(a.doubleValue());
-    }
-
-    public static double sqrt(Float a) {
-        return Math.sqrt(a.doubleValue());
-    }
-
-    public static double sqrt(Long a) {
-        return Math.sqrt(a.doubleValue());
-    }
-
-    public static double sqrt(Double a) {
-        return Math.sqrt(a);
-    }
-
-    public static double cbrt(Short a) {
-        return Math.cbrt(a.doubleValue());
-    }
-
-    public static double cbrt(Integer a) {
-        return Math.cbrt(a.doubleValue());
-    }
-
-    public static double cbrt(Float a) {
-        return Math.cbrt(a.doubleValue());
-    }
-
-    public static double cbrt(Long a) {
-        return Math.cbrt(a.doubleValue());
-    }
-
-    public static double cbrt(Double a) {
-        return Math.cbrt(a);
-    }
-
-    public static double IEEEremainder(Short f1, Short f2) {
-        return Math.IEEEremainder(f1.doubleValue(), f2.doubleValue());
-    }
-
-    public static double IEEEremainder(Integer f1, Integer f2) {
-        return Math.IEEEremainder(f1.doubleValue(), f2.doubleValue());
-    }
-
-    public static double IEEEremainder(Float f1, Float f2) {
-        return Math.IEEEremainder(f1.doubleValue(), f2.doubleValue());
-    }
-
-    public static double IEEEremainder(Long f1, Long f2) {
-        return Math.IEEEremainder(f1.doubleValue(), f2.doubleValue());
-    }
-
-    public static double IEEEremainder(Double f1, Double f2) {
-        return Math.IEEEremainder(f1, f2);
-    }
-
-    public static double ceil(Short a) {
-        return Math.ceil(a.doubleValue());
-    }
-
-    public static double ceil(Integer a) {
-        return Math.ceil(a.doubleValue());
-    }
-
-    public static double ceil(Float a) {
-        return Math.ceil(a.doubleValue());
-    }
-
-    public static double ceil(Long a) {
-        return Math.ceil(a.doubleValue());
-    }
-
-    public static double ceil(Double a) {
-        return Math.ceil(a);
-    }
-
-    public static double floor(Short a) {
-        return Math.floor(a.doubleValue());
-    }
-
-    public static double floor(Integer a) {
-        return Math.floor(a.doubleValue());
-    }
-
-    public static double floor(Float a) {
-        return Math.floor(a.doubleValue());
-    }
-
-    public static double floor(Long a) {
-        return Math.floor(a.doubleValue());
-    }
-
-    public static double floor(Double a) {
-        return Math.floor(a);
-    }
-
-    public static double rint(Short a) {
-        return Math.rint(a.doubleValue());
-    }
-
-    public static double rint(Integer a) {
-        return Math.rint(a.doubleValue());
-    }
-
-    public static double rint(Float a) {
-        return Math.rint(a.doubleValue());
-    }
-
-    public static double rint(Long a) {
-        return Math.rint(a.doubleValue());
-    }
-
-    public static double rint(Double a) {
-        return Math.rint(a);
-    }
-
-    public static double atan2(Short y, Short x) {
-        return Math.atan2(y.doubleValue(), x.doubleValue());
-    }
-
-    public static double atan2(Integer y, Integer x) {
-        return Math.atan2(y.doubleValue(), x.doubleValue());
-    }
-
-    public static double atan2(Float y, Float x) {
-        return Math.atan2(y.doubleValue(), x.doubleValue());
-    }
-
-    public static double atan2(Long y, Long x) {
-        return Math.atan2(y.doubleValue(), x.doubleValue());
-    }
-
-    public static double atan2(Double y, Double x) {
-        return Math.atan2(y, x);
-    }
-
-    public static double pow(Short a, Short b) {
-        return Math.pow(a.doubleValue(), b.doubleValue());
-    }
-
-    public static double pow(Integer a, Integer b) {
-        return Math.pow(a.doubleValue(), b.doubleValue());
-    }
-
-    public static double pow(Float a, Float b) {
-        return Math.pow(a.doubleValue(), b.doubleValue());
-    }
-
-    public static double pow(Long a, Long b) {
-        return Math.pow(a.doubleValue(), b.doubleValue());
-    }
-
-    public static double pow(Double a, Double b) {
-        return Math.pow(a, b);
-    }
-
-    public static int round(Short a) {
-        return Math.round(a.floatValue());
-    }
-
-    public static int round(Integer a) {
-        return Math.round(a.floatValue());
-    }
-
-    public static int round(Float a) {
-        return Math.round(a);
-    }
-
-    public static long round(Long a) {
-        return Math.round(a.doubleValue());
-    }
-
-    public static long round(Double a) {
-        return Math.round(a);
-    }
-
-    public static double random() {
-        return ThreadLocalRandom.current().nextDouble();
-    }
-
-    public static double randomDouble() {
-        return ThreadLocalRandom.current().nextDouble();
-    }
-
-    public static double randomFloat() {
-        return ThreadLocalRandom.current().nextFloat();
-    }
-
-    public static double randomInt() {
-        return ThreadLocalRandom.current().nextInt();
-    }
-
-    public static double randomInt(Integer i) {
-        return ThreadLocalRandom.current().nextInt(i);
-    }
-
-    public static double randomLong() {
-        return ThreadLocalRandom.current().nextLong();
-    }
-
-    public static double randomLong(Long l) {
-        return ThreadLocalRandom.current().nextLong(l);
-    }
-
-    @SuppressForbidden(reason = "Math#abs is trappy")
-    public static int abs(Integer a) {
-        return Math.abs(a);
-    }
-
-    @SuppressForbidden(reason = "Math#abs is trappy")
-    public static long abs(Long a) {
-        return Math.abs(a);
-    }
-
-    @SuppressForbidden(reason = "Math#abs is trappy")
-    public static float abs(Float a) {
-        return Math.abs(a);
-    }
-
-    @SuppressForbidden(reason = "Math#abs is trappy")
-    public static double abs(Double a) {
-        return Math.abs(a);
-    }
-
-    public static int max(Integer a, Integer b) {
-        return Math.max(a, b);
-    }
-
-    public static long max(Long a, Long b) {
-        return Math.max(a, b);
-    }
-
-    public static float max(Float a, Float b) {
-        return Math.max(a, b);
-    }
-
-    public static double max(Double a, Double b) {
-        return Math.max(a, b);
-    }
-
-    public static int min(Integer a, Integer b) {
-        return Math.min(a, b);
-    }
-
-    public static long min(Long a, Long b) {
-        return Math.min(a, b);
-    }
-
-    public static float min(Float a, Float b) {
-        return Math.min(a, b);
-    }
-
-    public static double min(Double a, Double b) {
-        return Math.min(a, b);
-    }
-
-    public static double ulp(Double d) {
-        return Math.ulp(d);
-    }
-
-    public static float ulp(Float f) {
-        return Math.ulp(f);
-    }
-
-    public static double signum(Double d) {
-        return Math.signum(d);
-    }
-
-    public static float signum(Float f) {
-        return Math.signum(f);
-    }
-
-    public static double sinh(Double x) {
-        return Math.sinh(x);
-    }
-
-    public static double cosh(Double x) {
-        return Math.cosh(x);
-    }
-
-    public static double tanh(Double x) {
-        return Math.tanh(x);
-    }
-
-    public static double hypot(Double x, Double y) {
-        return Math.hypot(x, y);
-    }
-
-    public static double expm1(Double x) {
-        return Math.expm1(x);
-    }
-
-    public static double log1p(Double x) {
-        return Math.log1p(x);
-    }
-
-    public static double copySign(Double magnitude, Double sign) {
-        return Math.copySign(magnitude, sign);
-    }
-
-    public static float copySign(Float magnitude, Float sign) {
-        return Math.copySign(magnitude, sign);
-    }
-
-    public static int getExponent(Float f) {
-        return Math.getExponent(f);
-    }
-
-    public static int getExponent(Double d) {
-        return Math.getExponent(d);
-    }
-
-    public static double nextAfter(Double start, Double direction) {
-        return Math.nextAfter(start, direction);
-    }
-
-    public static float nextAfter(Float start, Double direction) {
-        return Math.nextAfter(start, direction);
-    }
-
-    public static double nextUp(Double d) {
-        return Math.nextUp(d);
-    }
-
-    public static float nextUp(Float f) {
-        return Math.nextUp(f);
-    }
-
-
-    public static double scalb(Double d, Integer scaleFactor) {
-        return Math.scalb(d, scaleFactor);
-    }
-
-    public static float scalb(Float f, Integer scaleFactor) {
-        return Math.scalb(f, scaleFactor);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/common/network/NetworkModule.java b/core/src/main/java/org/elasticsearch/common/network/NetworkModule.java
index 984c2d6..b3abed6 100644
--- a/core/src/main/java/org/elasticsearch/common/network/NetworkModule.java
+++ b/core/src/main/java/org/elasticsearch/common/network/NetworkModule.java
@@ -113,10 +113,6 @@ import org.elasticsearch.rest.action.get.RestGetSourceAction;
 import org.elasticsearch.rest.action.get.RestHeadAction;
 import org.elasticsearch.rest.action.get.RestMultiGetAction;
 import org.elasticsearch.rest.action.index.RestIndexAction;
-import org.elasticsearch.rest.action.ingest.RestDeletePipelineAction;
-import org.elasticsearch.rest.action.ingest.RestGetPipelineAction;
-import org.elasticsearch.rest.action.ingest.RestPutPipelineAction;
-import org.elasticsearch.rest.action.ingest.RestSimulatePipelineAction;
 import org.elasticsearch.rest.action.main.RestMainAction;
 import org.elasticsearch.rest.action.percolate.RestMultiPercolateAction;
 import org.elasticsearch.rest.action.percolate.RestPercolateAction;
@@ -260,13 +256,7 @@ public class NetworkModule extends AbstractModule {
         RestCatAction.class,
 
         // Tasks API
-        RestListTasksAction.class,
-
-        // Ingest API
-        RestPutPipelineAction.class,
-        RestGetPipelineAction.class,
-        RestDeletePipelineAction.class,
-        RestSimulatePipelineAction.class
+        RestListTasksAction.class
     );
 
     private static final List<Class<? extends AbstractCatAction>> builtinCatHandlers = Arrays.asList(
diff --git a/core/src/main/java/org/elasticsearch/index/IndexService.java b/core/src/main/java/org/elasticsearch/index/IndexService.java
index 853a123..3d1a9f8 100644
--- a/core/src/main/java/org/elasticsearch/index/IndexService.java
+++ b/core/src/main/java/org/elasticsearch/index/IndexService.java
@@ -45,7 +45,6 @@ import org.elasticsearch.index.engine.EngineFactory;
 import org.elasticsearch.index.fielddata.FieldDataType;
 import org.elasticsearch.index.fielddata.IndexFieldDataCache;
 import org.elasticsearch.index.fielddata.IndexFieldDataService;
-import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.index.query.QueryShardContext;
@@ -68,6 +67,7 @@ import org.elasticsearch.threadpool.ThreadPool;
 import java.io.Closeable;
 import java.io.IOException;
 import java.nio.file.Path;
+import java.util.Arrays;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.Map;
@@ -101,6 +101,7 @@ public final class IndexService extends AbstractIndexComponent implements IndexC
     private final AtomicBoolean closed = new AtomicBoolean(false);
     private final AtomicBoolean deleted = new AtomicBoolean(false);
     private final IndexSettings indexSettings;
+    private final IndexingSlowLog slowLog;
 
     public IndexService(IndexSettings indexSettings, NodeEnvironment nodeEnv,
                         SimilarityService similarityService,
@@ -130,6 +131,7 @@ public final class IndexService extends AbstractIndexComponent implements IndexC
         this.engineFactory = engineFactory;
         // initialize this last -- otherwise if the wrapper requires any other member to be non-null we fail with an NPE
         this.searcherWrapper = wrapperFactory.newWrapper(this);
+        this.slowLog = new IndexingSlowLog(indexSettings.getSettings());
     }
 
     public int numberOfShards() {
@@ -292,9 +294,9 @@ public final class IndexService extends AbstractIndexComponent implements IndexC
                 (primary && IndexMetaData.isOnSharedFilesystem(indexSettings));
             store = new Store(shardId, this.indexSettings, indexStore.newDirectoryService(path), lock, new StoreCloseListener(shardId, canDeleteShardContent, () -> nodeServicesProvider.getIndicesQueryCache().onClose(shardId)));
             if (useShadowEngine(primary, indexSettings)) {
-                indexShard = new ShadowIndexShard(shardId, this.indexSettings, path, store, indexCache, mapperService, similarityService, indexFieldData, engineFactory, eventListener, searcherWrapper, nodeServicesProvider);
+                indexShard = new ShadowIndexShard(shardId, this.indexSettings, path, store, indexCache, mapperService, similarityService, indexFieldData, engineFactory, eventListener, searcherWrapper, nodeServicesProvider); // no indexing listeners - shadow  engines don't index
             } else {
-                indexShard = new IndexShard(shardId, this.indexSettings, path, store, indexCache, mapperService, similarityService, indexFieldData, engineFactory, eventListener, searcherWrapper, nodeServicesProvider);
+                indexShard = new IndexShard(shardId, this.indexSettings, path, store, indexCache, mapperService, similarityService, indexFieldData, engineFactory, eventListener, searcherWrapper, nodeServicesProvider, slowLog);
             }
 
             eventListener.indexShardStateChanged(indexShard, null, indexShard.state(), "shard created");
@@ -552,6 +554,11 @@ public final class IndexService extends AbstractIndexComponent implements IndexC
             } catch (Exception e) {
                 logger.warn("failed to refresh index store settings", e);
             }
+            try {
+                slowLog.onRefreshSettings(settings); // this will be refactored soon anyway so duplication is ok here
+            } catch (Exception e) {
+                logger.warn("failed to refresh slowlog settings", e);
+            }
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/IndexingSlowLog.java b/core/src/main/java/org/elasticsearch/index/IndexingSlowLog.java
new file mode 100644
index 0000000..5cd3685
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/index/IndexingSlowLog.java
@@ -0,0 +1,197 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index;
+
+import org.elasticsearch.common.Booleans;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.index.engine.Engine;
+import org.elasticsearch.index.mapper.ParsedDocument;
+import org.elasticsearch.index.shard.IndexingOperationListener;
+
+import java.io.IOException;
+import java.util.Locale;
+import java.util.concurrent.TimeUnit;
+
+/**
+ */
+public final class IndexingSlowLog implements IndexingOperationListener {
+
+    private boolean reformat;
+
+    private long indexWarnThreshold;
+    private long indexInfoThreshold;
+    private long indexDebugThreshold;
+    private long indexTraceThreshold;
+    /**
+     * How much of the source to log in the slowlog - 0 means log none and
+     * anything greater than 0 means log at least that many <em>characters</em>
+     * of the source.
+     */
+    private int maxSourceCharsToLog;
+
+    private String level;
+
+    private final ESLogger indexLogger;
+    private final ESLogger deleteLogger;
+
+    private static final String INDEX_INDEXING_SLOWLOG_PREFIX = "index.indexing.slowlog";
+    public static final String INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_WARN = INDEX_INDEXING_SLOWLOG_PREFIX +".threshold.index.warn";
+    public static final String INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_INFO = INDEX_INDEXING_SLOWLOG_PREFIX +".threshold.index.info";
+    public static final String INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_DEBUG = INDEX_INDEXING_SLOWLOG_PREFIX +".threshold.index.debug";
+    public static final String INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_TRACE = INDEX_INDEXING_SLOWLOG_PREFIX +".threshold.index.trace";
+    public static final String INDEX_INDEXING_SLOWLOG_REFORMAT = INDEX_INDEXING_SLOWLOG_PREFIX +".reformat";
+    public static final String INDEX_INDEXING_SLOWLOG_LEVEL = INDEX_INDEXING_SLOWLOG_PREFIX +".level";
+    public static final String INDEX_INDEXING_SLOWLOG_MAX_SOURCE_CHARS_TO_LOG = INDEX_INDEXING_SLOWLOG_PREFIX + ".source";
+
+    IndexingSlowLog(Settings indexSettings) {
+        this(indexSettings, Loggers.getLogger(INDEX_INDEXING_SLOWLOG_PREFIX + ".index"),
+                Loggers.getLogger(INDEX_INDEXING_SLOWLOG_PREFIX + ".delete"));
+    }
+
+    /**
+     * Build with the specified loggers. Only used to testing.
+     */
+    IndexingSlowLog(Settings indexSettings, ESLogger indexLogger, ESLogger deleteLogger) {
+        this.indexLogger = indexLogger;
+        this.deleteLogger = deleteLogger;
+
+        this.reformat = indexSettings.getAsBoolean(INDEX_INDEXING_SLOWLOG_REFORMAT, true);
+        this.indexWarnThreshold = indexSettings.getAsTime(INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_WARN, TimeValue.timeValueNanos(-1)).nanos();
+        this.indexInfoThreshold = indexSettings.getAsTime(INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_INFO, TimeValue.timeValueNanos(-1)).nanos();
+        this.indexDebugThreshold = indexSettings.getAsTime(INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_DEBUG, TimeValue.timeValueNanos(-1)).nanos();
+        this.indexTraceThreshold = indexSettings.getAsTime(INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_TRACE, TimeValue.timeValueNanos(-1)).nanos();
+        this.level = indexSettings.get(INDEX_INDEXING_SLOWLOG_LEVEL, "TRACE").toUpperCase(Locale.ROOT);
+        this.maxSourceCharsToLog = readSourceToLog(indexSettings);
+
+        indexLogger.setLevel(level);
+        deleteLogger.setLevel(level);
+    }
+
+    synchronized void onRefreshSettings(Settings settings) {
+        long indexWarnThreshold = settings.getAsTime(INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_WARN, TimeValue.timeValueNanos(this.indexWarnThreshold)).nanos();
+        if (indexWarnThreshold != this.indexWarnThreshold) {
+            this.indexWarnThreshold = indexWarnThreshold;
+        }
+        long indexInfoThreshold = settings.getAsTime(INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_INFO, TimeValue.timeValueNanos(this.indexInfoThreshold)).nanos();
+        if (indexInfoThreshold != this.indexInfoThreshold) {
+            this.indexInfoThreshold = indexInfoThreshold;
+        }
+        long indexDebugThreshold = settings.getAsTime(INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_DEBUG, TimeValue.timeValueNanos(this.indexDebugThreshold)).nanos();
+        if (indexDebugThreshold != this.indexDebugThreshold) {
+            this.indexDebugThreshold = indexDebugThreshold;
+        }
+        long indexTraceThreshold = settings.getAsTime(INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_TRACE, TimeValue.timeValueNanos(this.indexTraceThreshold)).nanos();
+        if (indexTraceThreshold != this.indexTraceThreshold) {
+            this.indexTraceThreshold = indexTraceThreshold;
+        }
+
+        String level = settings.get(INDEX_INDEXING_SLOWLOG_LEVEL, this.level);
+        if (!level.equals(this.level)) {
+            this.indexLogger.setLevel(level.toUpperCase(Locale.ROOT));
+            this.deleteLogger.setLevel(level.toUpperCase(Locale.ROOT));
+            this.level = level;
+        }
+
+        boolean reformat = settings.getAsBoolean(INDEX_INDEXING_SLOWLOG_REFORMAT, this.reformat);
+        if (reformat != this.reformat) {
+            this.reformat = reformat;
+        }
+
+        int maxSourceCharsToLog = readSourceToLog(settings);
+        if (maxSourceCharsToLog != this.maxSourceCharsToLog) {
+            this.maxSourceCharsToLog = maxSourceCharsToLog;
+        }
+    }
+
+    public void postIndex(Engine.Index index) {
+        final long took = index.endTime() - index.startTime();
+        postIndexing(index.parsedDoc(), took);
+    }
+
+    /**
+     * Reads how much of the source to log. The user can specify any value they
+     * like and numbers are interpreted the maximum number of characters to log
+     * and everything else is interpreted as Elasticsearch interprets booleans
+     * which is then converted to 0 for false and Integer.MAX_VALUE for true.
+     */
+    private int readSourceToLog(Settings settings) {
+        String sourceToLog = settings.get(INDEX_INDEXING_SLOWLOG_MAX_SOURCE_CHARS_TO_LOG, "1000");
+        try {
+            return Integer.parseInt(sourceToLog, 10);
+        } catch (NumberFormatException e) {
+            return Booleans.parseBoolean(sourceToLog, true) ? Integer.MAX_VALUE : 0;
+        }
+    }
+
+    private void postIndexing(ParsedDocument doc, long tookInNanos) {
+        if (indexWarnThreshold >= 0 && tookInNanos > indexWarnThreshold) {
+            indexLogger.warn("{}", new SlowLogParsedDocumentPrinter(doc, tookInNanos, reformat, maxSourceCharsToLog));
+        } else if (indexInfoThreshold >= 0 && tookInNanos > indexInfoThreshold) {
+            indexLogger.info("{}", new SlowLogParsedDocumentPrinter(doc, tookInNanos, reformat, maxSourceCharsToLog));
+        } else if (indexDebugThreshold >= 0 && tookInNanos > indexDebugThreshold) {
+            indexLogger.debug("{}", new SlowLogParsedDocumentPrinter(doc, tookInNanos, reformat, maxSourceCharsToLog));
+        } else if (indexTraceThreshold >= 0 && tookInNanos > indexTraceThreshold) {
+            indexLogger.trace("{}", new SlowLogParsedDocumentPrinter(doc, tookInNanos, reformat, maxSourceCharsToLog));
+        }
+    }
+
+    static final class SlowLogParsedDocumentPrinter {
+        private final ParsedDocument doc;
+        private final long tookInNanos;
+        private final boolean reformat;
+        private final int maxSourceCharsToLog;
+
+        SlowLogParsedDocumentPrinter(ParsedDocument doc, long tookInNanos, boolean reformat, int maxSourceCharsToLog) {
+            this.doc = doc;
+            this.tookInNanos = tookInNanos;
+            this.reformat = reformat;
+            this.maxSourceCharsToLog = maxSourceCharsToLog;
+        }
+
+        @Override
+        public String toString() {
+            StringBuilder sb = new StringBuilder();
+            sb.append("took[").append(TimeValue.timeValueNanos(tookInNanos)).append("], took_millis[").append(TimeUnit.NANOSECONDS.toMillis(tookInNanos)).append("], ");
+            sb.append("type[").append(doc.type()).append("], ");
+            sb.append("id[").append(doc.id()).append("], ");
+            if (doc.routing() == null) {
+                sb.append("routing[] ");
+            } else {
+                sb.append("routing[").append(doc.routing()).append("] ");
+            }
+
+            if (maxSourceCharsToLog == 0 || doc.source() == null || doc.source().length() == 0) {
+                return sb.toString();
+            }
+            try {
+                String source = XContentHelper.convertToJson(doc.source(), reformat);
+                sb.append(", source[").append(Strings.cleanTruncate(source, maxSourceCharsToLog)).append("]");
+            } catch (IOException e) {
+                sb.append(", source[_failed_to_convert_]");
+            }
+            return sb.toString();
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java b/core/src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java
index 84da434..424aa04 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java
@@ -20,9 +20,7 @@
 package org.elasticsearch.index.analysis;
 
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.ngram.Lucene43NGramTokenizer;
 import org.apache.lucene.analysis.ngram.NGramTokenizer;
-import org.apache.lucene.util.Version;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.IndexSettings;
@@ -43,7 +41,6 @@ public class NGramTokenizerFactory extends AbstractTokenizerFactory {
     private final int minGram;
     private final int maxGram;
     private final CharMatcher matcher;
-    private org.elasticsearch.Version esVersion;
 
     static final Map<String, CharMatcher> MATCHERS;
 
@@ -92,30 +89,19 @@ public class NGramTokenizerFactory extends AbstractTokenizerFactory {
         this.minGram = settings.getAsInt("min_gram", NGramTokenizer.DEFAULT_MIN_NGRAM_SIZE);
         this.maxGram = settings.getAsInt("max_gram", NGramTokenizer.DEFAULT_MAX_NGRAM_SIZE);
         this.matcher = parseTokenChars(settings.getAsArray("token_chars"));
-        this.esVersion = indexSettings.getIndexVersionCreated();
     }
 
-    @SuppressWarnings("deprecation")
     @Override
     public Tokenizer create() {
-        if (version.onOrAfter(Version.LUCENE_4_3) && esVersion.onOrAfter(org.elasticsearch.Version.V_0_90_2)) {
-            /*
-             * We added this in 0.90.2 but 0.90.1 used LUCENE_43 already so we can not rely on the lucene version.
-             * Yet if somebody uses 0.90.2 or higher with a prev. lucene version we should also use the deprecated version.
-             */
-            final Version version = this.version == Version.LUCENE_4_3 ? Version.LUCENE_4_4 : this.version; // always use 4.4 or higher
-            if (matcher == null) {
-                return new NGramTokenizer(minGram, maxGram);
-            } else {
-                return new NGramTokenizer(minGram, maxGram) {
-                    @Override
-                    protected boolean isTokenChar(int chr) {
-                        return matcher.isTokenChar(chr);
-                    }
-                };
-            }
+        if (matcher == null) {
+            return new NGramTokenizer(minGram, maxGram);
         } else {
-            return new Lucene43NGramTokenizer(minGram, maxGram);
+            return new NGramTokenizer(minGram, maxGram) {
+                @Override
+                protected boolean isTokenChar(int chr) {
+                    return matcher.isTokenChar(chr);
+                }
+            };
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java b/core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java
index 4e9ecf5..1de139a 100644
--- a/core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java
+++ b/core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java
@@ -118,6 +118,12 @@ public final class BitsetFilterCache extends AbstractIndexComponent implements L
     private BitSet getAndLoadIfNotPresent(final Query query, final LeafReaderContext context) throws IOException, ExecutionException {
         final Object coreCacheReader = context.reader().getCoreCacheKey();
         final ShardId shardId = ShardUtils.extractShardId(context.reader());
+        if (shardId != null // can't require it because of the percolator
+                && indexSettings.getIndex().getName().equals(shardId.getIndex()) == false) {
+            // insanity
+            throw new IllegalStateException("Trying to load bit set for index [" + shardId.getIndex()
+                    + "] with cache of index [" + indexSettings.getIndex().getName() + "]");
+        }
         Cache<Query, Value> filterToFbs = loadedFilters.computeIfAbsent(coreCacheReader, key -> {
             context.reader().addCoreClosedListener(BitsetFilterCache.this);
             return CacheBuilder.<Query, Value>builder().build();
@@ -208,6 +214,11 @@ public final class BitsetFilterCache extends AbstractIndexComponent implements L
 
         @Override
         public IndicesWarmer.TerminationHandle warmNewReaders(final IndexShard indexShard, final Engine.Searcher searcher) {
+            if (indexSettings.getIndex().equals(indexShard.getIndexSettings().getIndex()) == false) {
+                // this is from a different index
+                return TerminationHandle.NO_WAIT;
+            }
+
             if (!loadRandomAccessFiltersEagerly) {
                 return TerminationHandle.NO_WAIT;
             }
diff --git a/core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java b/core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java
index 35f1e06..913c498 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java
@@ -30,13 +30,12 @@ import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.index.IndexSettings;
 import org.elasticsearch.index.codec.CodecService;
-import org.elasticsearch.index.indexing.ShardIndexingService;
 import org.elasticsearch.index.shard.MergeSchedulerConfig;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.index.shard.TranslogRecoveryPerformer;
 import org.elasticsearch.index.store.Store;
 import org.elasticsearch.index.translog.TranslogConfig;
-import org.elasticsearch.indices.memory.IndexingMemoryController;
+import org.elasticsearch.indices.IndexingMemoryController;
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.util.concurrent.TimeUnit;
@@ -193,7 +192,7 @@ public final class EngineConfig {
     }
 
     /**
-     * Returns the initial index buffer size. This setting is only read on startup and otherwise controlled by {@link org.elasticsearch.indices.memory.IndexingMemoryController}
+     * Returns the initial index buffer size. This setting is only read on startup and otherwise controlled by {@link IndexingMemoryController}
      */
     public ByteSizeValue getIndexingBufferSize() {
         return indexingBufferSize;
diff --git a/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java b/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
index 1fcbbee..b2d81f0 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
@@ -55,12 +55,10 @@ import org.elasticsearch.common.lucene.index.ElasticsearchDirectoryReader;
 import org.elasticsearch.common.lucene.index.ElasticsearchLeafReader;
 import org.elasticsearch.common.lucene.uid.Versions;
 import org.elasticsearch.common.math.MathUtils;
-import org.elasticsearch.common.metrics.CounterMetric;
 import org.elasticsearch.common.util.concurrent.AbstractRunnable;
 import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;
 import org.elasticsearch.common.util.concurrent.ReleasableLock;
 import org.elasticsearch.index.IndexSettings;
-import org.elasticsearch.index.indexing.ShardIndexingService;
 import org.elasticsearch.index.mapper.Uid;
 import org.elasticsearch.index.merge.MergeStats;
 import org.elasticsearch.index.merge.OnGoingMerge;
@@ -961,8 +959,7 @@ public class InternalEngine extends Engine {
             });
             return new IndexWriter(store.directory(), iwc);
         } catch (LockObtainFailedException ex) {
-            boolean isLocked = IndexWriter.isLocked(store.directory());
-            logger.warn("Could not lock IndexWriter isLocked [{}]", ex, isLocked);
+            logger.warn("could not lock IndexWriter", ex);
             throw ex;
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java b/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
deleted file mode 100644
index 39f3dd6..0000000
--- a/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
+++ /dev/null
@@ -1,70 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.indexing;
-
-import org.elasticsearch.index.engine.Engine;
-
-/**
- * An indexing listener for indexing, delete, events.
- */
-public abstract class IndexingOperationListener {
-
-    /**
-     * Called before the indexing occurs.
-     */
-    public Engine.Index preIndex(Engine.Index operation) {
-        return operation;
-    }
-
-    /**
-     * Called after the indexing operation occurred.
-     */
-    public void postIndex(Engine.Index index) {
-
-    }
-
-    /**
-     * Called after the indexing operation occurred with exception.
-     */
-    public void postIndex(Engine.Index index, Throwable ex) {
-
-    }
-
-    /**
-     * Called before the delete occurs.
-     */
-    public Engine.Delete preDelete(Engine.Delete delete) {
-        return delete;
-    }
-
-
-    /**
-     * Called after the delete operation occurred.
-     */
-    public void postDelete(Engine.Delete delete) {
-
-    }
-
-    /**
-     * Called after the delete operation occurred with exception.
-     */
-    public void postDelete(Engine.Delete delete, Throwable ex) {
-
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java b/core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java
deleted file mode 100644
index 292c2a1..0000000
--- a/core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java
+++ /dev/null
@@ -1,195 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.indexing;
-
-import org.elasticsearch.common.Booleans;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.index.engine.Engine;
-import org.elasticsearch.index.mapper.ParsedDocument;
-
-import java.io.IOException;
-import java.util.Locale;
-import java.util.concurrent.TimeUnit;
-
-/**
- */
-public final class IndexingSlowLog {
-
-    private boolean reformat;
-
-    private long indexWarnThreshold;
-    private long indexInfoThreshold;
-    private long indexDebugThreshold;
-    private long indexTraceThreshold;
-    /**
-     * How much of the source to log in the slowlog - 0 means log none and
-     * anything greater than 0 means log at least that many <em>characters</em>
-     * of the source.
-     */
-    private int maxSourceCharsToLog;
-
-    private String level;
-
-    private final ESLogger indexLogger;
-    private final ESLogger deleteLogger;
-
-    private static final String INDEX_INDEXING_SLOWLOG_PREFIX = "index.indexing.slowlog";
-    public static final String INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_WARN = INDEX_INDEXING_SLOWLOG_PREFIX +".threshold.index.warn";
-    public static final String INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_INFO = INDEX_INDEXING_SLOWLOG_PREFIX +".threshold.index.info";
-    public static final String INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_DEBUG = INDEX_INDEXING_SLOWLOG_PREFIX +".threshold.index.debug";
-    public static final String INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_TRACE = INDEX_INDEXING_SLOWLOG_PREFIX +".threshold.index.trace";
-    public static final String INDEX_INDEXING_SLOWLOG_REFORMAT = INDEX_INDEXING_SLOWLOG_PREFIX +".reformat";
-    public static final String INDEX_INDEXING_SLOWLOG_LEVEL = INDEX_INDEXING_SLOWLOG_PREFIX +".level";
-    public static final String INDEX_INDEXING_SLOWLOG_MAX_SOURCE_CHARS_TO_LOG = INDEX_INDEXING_SLOWLOG_PREFIX + ".source";
-
-    IndexingSlowLog(Settings indexSettings) {
-        this(indexSettings, Loggers.getLogger(INDEX_INDEXING_SLOWLOG_PREFIX + ".index"),
-                Loggers.getLogger(INDEX_INDEXING_SLOWLOG_PREFIX + ".delete"));
-    }
-
-    /**
-     * Build with the specified loggers. Only used to testing.
-     */
-    IndexingSlowLog(Settings indexSettings, ESLogger indexLogger, ESLogger deleteLogger) {
-        this.indexLogger = indexLogger;
-        this.deleteLogger = deleteLogger;
-
-        this.reformat = indexSettings.getAsBoolean(INDEX_INDEXING_SLOWLOG_REFORMAT, true);
-        this.indexWarnThreshold = indexSettings.getAsTime(INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_WARN, TimeValue.timeValueNanos(-1)).nanos();
-        this.indexInfoThreshold = indexSettings.getAsTime(INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_INFO, TimeValue.timeValueNanos(-1)).nanos();
-        this.indexDebugThreshold = indexSettings.getAsTime(INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_DEBUG, TimeValue.timeValueNanos(-1)).nanos();
-        this.indexTraceThreshold = indexSettings.getAsTime(INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_TRACE, TimeValue.timeValueNanos(-1)).nanos();
-        this.level = indexSettings.get(INDEX_INDEXING_SLOWLOG_LEVEL, "TRACE").toUpperCase(Locale.ROOT);
-        this.maxSourceCharsToLog = readSourceToLog(indexSettings);
-
-        indexLogger.setLevel(level);
-        deleteLogger.setLevel(level);
-    }
-
-    synchronized void onRefreshSettings(Settings settings) {
-        long indexWarnThreshold = settings.getAsTime(INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_WARN, TimeValue.timeValueNanos(this.indexWarnThreshold)).nanos();
-        if (indexWarnThreshold != this.indexWarnThreshold) {
-            this.indexWarnThreshold = indexWarnThreshold;
-        }
-        long indexInfoThreshold = settings.getAsTime(INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_INFO, TimeValue.timeValueNanos(this.indexInfoThreshold)).nanos();
-        if (indexInfoThreshold != this.indexInfoThreshold) {
-            this.indexInfoThreshold = indexInfoThreshold;
-        }
-        long indexDebugThreshold = settings.getAsTime(INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_DEBUG, TimeValue.timeValueNanos(this.indexDebugThreshold)).nanos();
-        if (indexDebugThreshold != this.indexDebugThreshold) {
-            this.indexDebugThreshold = indexDebugThreshold;
-        }
-        long indexTraceThreshold = settings.getAsTime(INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_TRACE, TimeValue.timeValueNanos(this.indexTraceThreshold)).nanos();
-        if (indexTraceThreshold != this.indexTraceThreshold) {
-            this.indexTraceThreshold = indexTraceThreshold;
-        }
-
-        String level = settings.get(INDEX_INDEXING_SLOWLOG_LEVEL, this.level);
-        if (!level.equals(this.level)) {
-            this.indexLogger.setLevel(level.toUpperCase(Locale.ROOT));
-            this.deleteLogger.setLevel(level.toUpperCase(Locale.ROOT));
-            this.level = level;
-        }
-
-        boolean reformat = settings.getAsBoolean(INDEX_INDEXING_SLOWLOG_REFORMAT, this.reformat);
-        if (reformat != this.reformat) {
-            this.reformat = reformat;
-        }
-
-        int maxSourceCharsToLog = readSourceToLog(settings);
-        if (maxSourceCharsToLog != this.maxSourceCharsToLog) {
-            this.maxSourceCharsToLog = maxSourceCharsToLog;
-        }
-    }
-
-    void postIndex(Engine.Index index, long tookInNanos) {
-        postIndexing(index.parsedDoc(), tookInNanos);
-    }
-
-    /**
-     * Reads how much of the source to log. The user can specify any value they
-     * like and numbers are interpreted the maximum number of characters to log
-     * and everything else is interpreted as Elasticsearch interprets booleans
-     * which is then converted to 0 for false and Integer.MAX_VALUE for true.
-     */
-    private int readSourceToLog(Settings settings) {
-        String sourceToLog = settings.get(INDEX_INDEXING_SLOWLOG_MAX_SOURCE_CHARS_TO_LOG, "1000");
-        try {
-            return Integer.parseInt(sourceToLog, 10);
-        } catch (NumberFormatException e) {
-            return Booleans.parseBoolean(sourceToLog, true) ? Integer.MAX_VALUE : 0;
-        }
-    }
-
-    private void postIndexing(ParsedDocument doc, long tookInNanos) {
-        if (indexWarnThreshold >= 0 && tookInNanos > indexWarnThreshold) {
-            indexLogger.warn("{}", new SlowLogParsedDocumentPrinter(doc, tookInNanos, reformat, maxSourceCharsToLog));
-        } else if (indexInfoThreshold >= 0 && tookInNanos > indexInfoThreshold) {
-            indexLogger.info("{}", new SlowLogParsedDocumentPrinter(doc, tookInNanos, reformat, maxSourceCharsToLog));
-        } else if (indexDebugThreshold >= 0 && tookInNanos > indexDebugThreshold) {
-            indexLogger.debug("{}", new SlowLogParsedDocumentPrinter(doc, tookInNanos, reformat, maxSourceCharsToLog));
-        } else if (indexTraceThreshold >= 0 && tookInNanos > indexTraceThreshold) {
-            indexLogger.trace("{}", new SlowLogParsedDocumentPrinter(doc, tookInNanos, reformat, maxSourceCharsToLog));
-        }
-    }
-
-    static final class SlowLogParsedDocumentPrinter {
-        private final ParsedDocument doc;
-        private final long tookInNanos;
-        private final boolean reformat;
-        private final int maxSourceCharsToLog;
-
-        SlowLogParsedDocumentPrinter(ParsedDocument doc, long tookInNanos, boolean reformat, int maxSourceCharsToLog) {
-            this.doc = doc;
-            this.tookInNanos = tookInNanos;
-            this.reformat = reformat;
-            this.maxSourceCharsToLog = maxSourceCharsToLog;
-        }
-
-        @Override
-        public String toString() {
-            StringBuilder sb = new StringBuilder();
-            sb.append("took[").append(TimeValue.timeValueNanos(tookInNanos)).append("], took_millis[").append(TimeUnit.NANOSECONDS.toMillis(tookInNanos)).append("], ");
-            sb.append("type[").append(doc.type()).append("], ");
-            sb.append("id[").append(doc.id()).append("], ");
-            if (doc.routing() == null) {
-                sb.append("routing[] ");
-            } else {
-                sb.append("routing[").append(doc.routing()).append("] ");
-            }
-
-            if (maxSourceCharsToLog == 0 || doc.source() == null || doc.source().length() == 0) {
-                return sb.toString();
-            }
-            try {
-                String source = XContentHelper.convertToJson(doc.source(), reformat);
-                sb.append(", source[").append(Strings.cleanTruncate(source, maxSourceCharsToLog)).append("]");
-            } catch (IOException e) {
-                sb.append(", source[_failed_to_convert_]");
-            }
-            return sb.toString();
-        }
-    }
-}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/IndexingStats.java b/core/src/main/java/org/elasticsearch/index/indexing/IndexingStats.java
deleted file mode 100644
index 07ca8af..0000000
--- a/core/src/main/java/org/elasticsearch/index/indexing/IndexingStats.java
+++ /dev/null
@@ -1,321 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.indexing;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Streamable;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- */
-public class IndexingStats implements Streamable, ToXContent {
-
-    public static class Stats implements Streamable, ToXContent {
-
-        private long indexCount;
-        private long indexTimeInMillis;
-        private long indexCurrent;
-        private long indexFailedCount;
-        private long deleteCount;
-        private long deleteTimeInMillis;
-        private long deleteCurrent;
-        private long noopUpdateCount;
-        private long throttleTimeInMillis;
-        private boolean isThrottled;
-
-        Stats() {}
-
-        public Stats(long indexCount, long indexTimeInMillis, long indexCurrent, long indexFailedCount, long deleteCount, long deleteTimeInMillis, long deleteCurrent, long noopUpdateCount, boolean isThrottled, long throttleTimeInMillis) {
-            this.indexCount = indexCount;
-            this.indexTimeInMillis = indexTimeInMillis;
-            this.indexCurrent = indexCurrent;
-            this.indexFailedCount = indexFailedCount;
-            this.deleteCount = deleteCount;
-            this.deleteTimeInMillis = deleteTimeInMillis;
-            this.deleteCurrent = deleteCurrent;
-            this.noopUpdateCount = noopUpdateCount;
-            this.isThrottled = isThrottled;
-            this.throttleTimeInMillis = throttleTimeInMillis;
-        }
-
-        public void add(Stats stats) {
-            indexCount += stats.indexCount;
-            indexTimeInMillis += stats.indexTimeInMillis;
-            indexCurrent += stats.indexCurrent;
-            indexFailedCount += stats.indexFailedCount;
-
-            deleteCount += stats.deleteCount;
-            deleteTimeInMillis += stats.deleteTimeInMillis;
-            deleteCurrent += stats.deleteCurrent;
-
-            noopUpdateCount += stats.noopUpdateCount;
-            throttleTimeInMillis += stats.throttleTimeInMillis;
-            if (isThrottled != stats.isThrottled) {
-                isThrottled = true; //When combining if one is throttled set result to throttled.
-            }
-        }
-
-        /**
-         * The total number of indexing operations
-         */
-        public long getIndexCount() { return indexCount; }
-
-        /**
-         * The number of failed indexing operations
-         */
-        public long getIndexFailedCount() { return indexFailedCount; }
-
-        /**
-         * The total amount of time spend on executing index operations.
-         */
-        public TimeValue getIndexTime() { return new TimeValue(indexTimeInMillis); }
-
-        /**
-         * Returns the currently in-flight indexing operations.
-         */
-        public long getIndexCurrent() { return indexCurrent;}
-
-        /**
-         * Returns the number of delete operation executed
-         */
-        public long getDeleteCount() {
-            return deleteCount;
-        }
-
-        /**
-         * Returns if the index is under merge throttling control
-         */
-        public boolean isThrottled() { return isThrottled; }
-
-        /**
-         * Gets the amount of time in a TimeValue that the index has been under merge throttling control
-         */
-        public TimeValue getThrottleTime() { return new TimeValue(throttleTimeInMillis); }
-
-        /**
-         * The total amount of time spend on executing delete operations.
-         */
-        public TimeValue getDeleteTime() { return new TimeValue(deleteTimeInMillis); }
-
-        /**
-         * Returns the currently in-flight delete operations
-         */
-        public long getDeleteCurrent() {
-            return deleteCurrent;
-        }
-
-        public long getNoopUpdateCount() {
-            return noopUpdateCount;
-        }
-
-        public static Stats readStats(StreamInput in) throws IOException {
-            Stats stats = new Stats();
-            stats.readFrom(in);
-            return stats;
-        }
-
-        @Override
-        public void readFrom(StreamInput in) throws IOException {
-            indexCount = in.readVLong();
-            indexTimeInMillis = in.readVLong();
-            indexCurrent = in.readVLong();
-
-            if(in.getVersion().onOrAfter(Version.V_2_1_0)){
-                indexFailedCount = in.readVLong();
-            }
-
-            deleteCount = in.readVLong();
-            deleteTimeInMillis = in.readVLong();
-            deleteCurrent = in.readVLong();
-            noopUpdateCount = in.readVLong();
-            isThrottled = in.readBoolean();
-            throttleTimeInMillis = in.readLong();
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVLong(indexCount);
-            out.writeVLong(indexTimeInMillis);
-            out.writeVLong(indexCurrent);
-
-            if(out.getVersion().onOrAfter(Version.V_2_1_0)) {
-                out.writeVLong(indexFailedCount);
-            }
-
-            out.writeVLong(deleteCount);
-            out.writeVLong(deleteTimeInMillis);
-            out.writeVLong(deleteCurrent);
-            out.writeVLong(noopUpdateCount);
-            out.writeBoolean(isThrottled);
-            out.writeLong(throttleTimeInMillis);
-
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(Fields.INDEX_TOTAL, indexCount);
-            builder.timeValueField(Fields.INDEX_TIME_IN_MILLIS, Fields.INDEX_TIME, indexTimeInMillis);
-            builder.field(Fields.INDEX_CURRENT, indexCurrent);
-            builder.field(Fields.INDEX_FAILED, indexFailedCount);
-
-            builder.field(Fields.DELETE_TOTAL, deleteCount);
-            builder.timeValueField(Fields.DELETE_TIME_IN_MILLIS, Fields.DELETE_TIME, deleteTimeInMillis);
-            builder.field(Fields.DELETE_CURRENT, deleteCurrent);
-
-            builder.field(Fields.NOOP_UPDATE_TOTAL, noopUpdateCount);
-
-            builder.field(Fields.IS_THROTTLED, isThrottled);
-            builder.timeValueField(Fields.THROTTLED_TIME_IN_MILLIS, Fields.THROTTLED_TIME, throttleTimeInMillis);
-            return builder;
-        }
-    }
-
-    private Stats totalStats;
-
-    @Nullable
-    private Map<String, Stats> typeStats;
-
-    public IndexingStats() {
-        totalStats = new Stats();
-    }
-
-    public IndexingStats(Stats totalStats, @Nullable Map<String, Stats> typeStats) {
-        this.totalStats = totalStats;
-        this.typeStats = typeStats;
-    }
-
-    public void add(IndexingStats indexingStats) {
-        add(indexingStats, true);
-    }
-
-    public void add(IndexingStats indexingStats, boolean includeTypes) {
-        if (indexingStats == null) {
-            return;
-        }
-        addTotals(indexingStats);
-        if (includeTypes && indexingStats.typeStats != null && !indexingStats.typeStats.isEmpty()) {
-            if (typeStats == null) {
-                typeStats = new HashMap<>(indexingStats.typeStats.size());
-            }
-            for (Map.Entry<String, Stats> entry : indexingStats.typeStats.entrySet()) {
-                Stats stats = typeStats.get(entry.getKey());
-                if (stats == null) {
-                    typeStats.put(entry.getKey(), entry.getValue());
-                } else {
-                    stats.add(entry.getValue());
-                }
-            }
-        }
-    }
-
-    public void addTotals(IndexingStats indexingStats) {
-        if (indexingStats == null) {
-            return;
-        }
-        totalStats.add(indexingStats.totalStats);
-    }
-
-    public Stats getTotal() {
-        return this.totalStats;
-    }
-
-    @Nullable
-    public Map<String, Stats> getTypeStats() {
-        return this.typeStats;
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, ToXContent.Params params) throws IOException {
-        builder.startObject(Fields.INDEXING);
-        totalStats.toXContent(builder, params);
-        if (typeStats != null && !typeStats.isEmpty()) {
-            builder.startObject(Fields.TYPES);
-            for (Map.Entry<String, Stats> entry : typeStats.entrySet()) {
-                builder.startObject(entry.getKey(), XContentBuilder.FieldCaseConversion.NONE);
-                entry.getValue().toXContent(builder, params);
-                builder.endObject();
-            }
-            builder.endObject();
-        }
-        builder.endObject();
-        return builder;
-    }
-
-    static final class Fields {
-        static final XContentBuilderString INDEXING = new XContentBuilderString("indexing");
-        static final XContentBuilderString TYPES = new XContentBuilderString("types");
-        static final XContentBuilderString INDEX_TOTAL = new XContentBuilderString("index_total");
-        static final XContentBuilderString INDEX_TIME = new XContentBuilderString("index_time");
-        static final XContentBuilderString INDEX_TIME_IN_MILLIS = new XContentBuilderString("index_time_in_millis");
-        static final XContentBuilderString INDEX_CURRENT = new XContentBuilderString("index_current");
-        static final XContentBuilderString INDEX_FAILED = new XContentBuilderString("index_failed");
-        static final XContentBuilderString DELETE_TOTAL = new XContentBuilderString("delete_total");
-        static final XContentBuilderString DELETE_TIME = new XContentBuilderString("delete_time");
-        static final XContentBuilderString DELETE_TIME_IN_MILLIS = new XContentBuilderString("delete_time_in_millis");
-        static final XContentBuilderString DELETE_CURRENT = new XContentBuilderString("delete_current");
-        static final XContentBuilderString NOOP_UPDATE_TOTAL = new XContentBuilderString("noop_update_total");
-        static final XContentBuilderString IS_THROTTLED = new XContentBuilderString("is_throttled");
-        static final XContentBuilderString THROTTLED_TIME_IN_MILLIS = new XContentBuilderString("throttle_time_in_millis");
-        static final XContentBuilderString THROTTLED_TIME = new XContentBuilderString("throttle_time");
-    }
-
-    public static IndexingStats readIndexingStats(StreamInput in) throws IOException {
-        IndexingStats indexingStats = new IndexingStats();
-        indexingStats.readFrom(in);
-        return indexingStats;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        totalStats = Stats.readStats(in);
-        if (in.readBoolean()) {
-            int size = in.readVInt();
-            typeStats = new HashMap<>(size);
-            for (int i = 0; i < size; i++) {
-                typeStats.put(in.readString(), Stats.readStats(in));
-            }
-        }
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        totalStats.writeTo(out);
-        if (typeStats == null || typeStats.isEmpty()) {
-            out.writeBoolean(false);
-        } else {
-            out.writeBoolean(true);
-            out.writeVInt(typeStats.size());
-            for (Map.Entry<String, Stats> entry : typeStats.entrySet()) {
-                out.writeString(entry.getKey());
-                entry.getValue().writeTo(out);
-            }
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java b/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
deleted file mode 100644
index f7175c0..0000000
--- a/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
+++ /dev/null
@@ -1,233 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.indexing;
-
-import org.elasticsearch.common.collect.MapBuilder;
-import org.elasticsearch.common.metrics.CounterMetric;
-import org.elasticsearch.common.metrics.MeanMetric;
-import org.elasticsearch.common.regex.Regex;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.index.IndexSettings;
-import org.elasticsearch.index.engine.Engine;
-import org.elasticsearch.index.shard.AbstractIndexShardComponent;
-import org.elasticsearch.index.shard.ShardId;
-
-import java.util.HashMap;
-import java.util.Map;
-import java.util.concurrent.CopyOnWriteArrayList;
-import java.util.concurrent.TimeUnit;
-
-import static java.util.Collections.emptyMap;
-
-/**
- */
-public class ShardIndexingService extends AbstractIndexShardComponent {
-
-    private final IndexingSlowLog slowLog;
-
-    private final StatsHolder totalStats = new StatsHolder();
-
-    private final CopyOnWriteArrayList<IndexingOperationListener> listeners = new CopyOnWriteArrayList<>();
-
-    private volatile Map<String, StatsHolder> typesStats = emptyMap();
-
-    public ShardIndexingService(ShardId shardId, IndexSettings indexSettings) {
-        super(shardId, indexSettings);
-        this.slowLog = new IndexingSlowLog(this.indexSettings.getSettings());
-    }
-
-    /**
-     * Returns the stats, including type specific stats. If the types are null/0 length, then nothing
-     * is returned for them. If they are set, then only types provided will be returned, or
-     * <tt>_all</tt> for all types.
-     */
-    public IndexingStats stats(boolean isThrottled, long currentThrottleInMillis, String... types) {
-        IndexingStats.Stats total = totalStats.stats(isThrottled, currentThrottleInMillis);
-        Map<String, IndexingStats.Stats> typesSt = null;
-        if (types != null && types.length > 0) {
-            typesSt = new HashMap<>(typesStats.size());
-            if (types.length == 1 && types[0].equals("_all")) {
-                for (Map.Entry<String, StatsHolder> entry : typesStats.entrySet()) {
-                    typesSt.put(entry.getKey(), entry.getValue().stats(isThrottled, currentThrottleInMillis));
-                }
-            } else {
-                for (Map.Entry<String, StatsHolder> entry : typesStats.entrySet()) {
-                    if (Regex.simpleMatch(types, entry.getKey())) {
-                        typesSt.put(entry.getKey(), entry.getValue().stats(isThrottled, currentThrottleInMillis));
-                    }
-                }
-            }
-        }
-        return new IndexingStats(total, typesSt);
-    }
-
-    public void addListener(IndexingOperationListener listener) {
-        listeners.add(listener);
-    }
-
-    public void removeListener(IndexingOperationListener listener) {
-        listeners.remove(listener);
-    }
-
-    public Engine.Index preIndex(Engine.Index operation) {
-        totalStats.indexCurrent.inc();
-        typeStats(operation.type()).indexCurrent.inc();
-        for (IndexingOperationListener listener : listeners) {
-            operation = listener.preIndex(operation);
-        }
-        return operation;
-    }
-
-    public void postIndex(Engine.Index index) {
-        long took = index.endTime() - index.startTime();
-        totalStats.indexMetric.inc(took);
-        totalStats.indexCurrent.dec();
-        StatsHolder typeStats = typeStats(index.type());
-        typeStats.indexMetric.inc(took);
-        typeStats.indexCurrent.dec();
-        slowLog.postIndex(index, took);
-        for (IndexingOperationListener listener : listeners) {
-            try {
-                listener.postIndex(index);
-            } catch (Exception e) {
-                logger.warn("postIndex listener [{}] failed", e, listener);
-            }
-        }
-    }
-
-    public void postIndex(Engine.Index index, Throwable ex) {
-        totalStats.indexCurrent.dec();
-        typeStats(index.type()).indexCurrent.dec();
-        totalStats.indexFailed.inc();
-        typeStats(index.type()).indexFailed.inc();
-        for (IndexingOperationListener listener : listeners) {
-            try {
-                listener.postIndex(index, ex);
-            } catch (Throwable t) {
-                logger.warn("postIndex listener [{}] failed", t, listener);
-            }
-        }
-    }
-
-    public Engine.Delete preDelete(Engine.Delete delete) {
-        totalStats.deleteCurrent.inc();
-        typeStats(delete.type()).deleteCurrent.inc();
-        for (IndexingOperationListener listener : listeners) {
-            delete = listener.preDelete(delete);
-        }
-        return delete;
-    }
-
-
-    public void postDelete(Engine.Delete delete) {
-        long took = delete.endTime() - delete.startTime();
-        totalStats.deleteMetric.inc(took);
-        totalStats.deleteCurrent.dec();
-        StatsHolder typeStats = typeStats(delete.type());
-        typeStats.deleteMetric.inc(took);
-        typeStats.deleteCurrent.dec();
-        for (IndexingOperationListener listener : listeners) {
-            try {
-                listener.postDelete(delete);
-            } catch (Exception e) {
-                logger.warn("postDelete listener [{}] failed", e, listener);
-            }
-        }
-    }
-
-    public void postDelete(Engine.Delete delete, Throwable ex) {
-        totalStats.deleteCurrent.dec();
-        typeStats(delete.type()).deleteCurrent.dec();
-        for (IndexingOperationListener listener : listeners) {
-            try {
-                listener. postDelete(delete, ex);
-            } catch (Throwable t) {
-                logger.warn("postDelete listener [{}] failed", t, listener);
-            }
-        }
-    }
-
-    public void noopUpdate(String type) {
-        totalStats.noopUpdates.inc();
-        typeStats(type).noopUpdates.inc();
-    }
-
-    public void clear() {
-        totalStats.clear();
-        synchronized (this) {
-            if (!typesStats.isEmpty()) {
-                MapBuilder<String, StatsHolder> typesStatsBuilder = MapBuilder.newMapBuilder();
-                for (Map.Entry<String, StatsHolder> typeStats : typesStats.entrySet()) {
-                    if (typeStats.getValue().totalCurrent() > 0) {
-                        typeStats.getValue().clear();
-                        typesStatsBuilder.put(typeStats.getKey(), typeStats.getValue());
-                    }
-                }
-                typesStats = typesStatsBuilder.immutableMap();
-            }
-        }
-    }
-
-    private StatsHolder typeStats(String type) {
-        StatsHolder stats = typesStats.get(type);
-        if (stats == null) {
-            synchronized (this) {
-                stats = typesStats.get(type);
-                if (stats == null) {
-                    stats = new StatsHolder();
-                    typesStats = MapBuilder.newMapBuilder(typesStats).put(type, stats).immutableMap();
-                }
-            }
-        }
-        return stats;
-    }
-
-    public void onRefreshSettings(Settings settings) {
-        slowLog.onRefreshSettings(settings);
-    }
-
-    static class StatsHolder {
-        public final MeanMetric indexMetric = new MeanMetric();
-        public final MeanMetric deleteMetric = new MeanMetric();
-        public final CounterMetric indexCurrent = new CounterMetric();
-        public final CounterMetric indexFailed = new CounterMetric();
-        public final CounterMetric deleteCurrent = new CounterMetric();
-        public final CounterMetric noopUpdates = new CounterMetric();
-
-        public IndexingStats.Stats stats(boolean isThrottled, long currentThrottleMillis) {
-            return new IndexingStats.Stats(
-                    indexMetric.count(), TimeUnit.NANOSECONDS.toMillis(indexMetric.sum()), indexCurrent.count(), indexFailed.count(),
-                    deleteMetric.count(), TimeUnit.NANOSECONDS.toMillis(deleteMetric.sum()), deleteCurrent.count(),
-                    noopUpdates.count(), isThrottled, TimeUnit.MILLISECONDS.toMillis(currentThrottleMillis));
-        }
-
-        public long totalCurrent() {
-            return indexCurrent.count() + deleteMetric.count();
-        }
-
-        public void clear() {
-            indexMetric.clear();
-            deleteMetric.clear();
-        }
-
-
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java b/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
index 3fb0967..6e9c86b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
@@ -604,14 +604,14 @@ public abstract class QueryBuilders {
      * Facilitates creating template query requests using an inline script
      */
     public static TemplateQueryBuilder templateQuery(String template, Map<String, Object> vars) {
-        return new TemplateQueryBuilder(template, vars);
+        return new TemplateQueryBuilder(new Template(template, ScriptService.ScriptType.INLINE, null, null, vars));
     }
 
     /**
      * Facilitates creating template query requests
      */
     public static TemplateQueryBuilder templateQuery(String template, ScriptService.ScriptType templateType, Map<String, Object> vars) {
-        return new TemplateQueryBuilder(template, templateType, vars);
+        return new TemplateQueryBuilder(new Template(template, templateType, null, null, vars));
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
index 54bd1cd..e4f656d 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -81,8 +81,6 @@ import org.elasticsearch.index.fielddata.ShardFieldData;
 import org.elasticsearch.index.flush.FlushStats;
 import org.elasticsearch.index.get.GetStats;
 import org.elasticsearch.index.get.ShardGetService;
-import org.elasticsearch.index.indexing.IndexingStats;
-import org.elasticsearch.index.indexing.ShardIndexingService;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperForType;
 import org.elasticsearch.index.mapper.MapperService;
@@ -113,7 +111,7 @@ import org.elasticsearch.index.warmer.ShardIndexWarmerService;
 import org.elasticsearch.index.warmer.WarmerStats;
 import org.elasticsearch.indices.IndicesWarmer;
 import org.elasticsearch.indices.cache.query.IndicesQueryCache;
-import org.elasticsearch.indices.memory.IndexingMemoryController;
+import org.elasticsearch.indices.IndexingMemoryController;
 import org.elasticsearch.indices.recovery.RecoveryFailedException;
 import org.elasticsearch.indices.recovery.RecoveryState;
 import org.elasticsearch.percolator.PercolatorService;
@@ -125,6 +123,8 @@ import java.io.IOException;
 import java.io.PrintStream;
 import java.nio.channels.ClosedByInterruptException;
 import java.nio.charset.StandardCharsets;
+import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.EnumSet;
 import java.util.List;
 import java.util.Map;
@@ -143,7 +143,7 @@ public class IndexShard extends AbstractIndexShardComponent {
     private final IndexCache indexCache;
     private final Store store;
     private final MergeSchedulerConfig mergeSchedulerConfig;
-    private final ShardIndexingService indexingService;
+    private final InternalIndexingStats internalIndexingStats;
     private final ShardSearchStats searchService;
     private final ShardGetService getService;
     private final ShardIndexWarmerService shardWarmerService;
@@ -167,7 +167,6 @@ public class IndexShard extends AbstractIndexShardComponent {
     private final IndexEventListener indexEventListener;
     private final IndexSettings idxSettings;
     private final NodeServicesProvider provider;
-
     private TimeValue refreshInterval;
 
     private volatile ScheduledFuture<?> refreshScheduledFuture;
@@ -176,6 +175,8 @@ public class IndexShard extends AbstractIndexShardComponent {
     protected final AtomicReference<Engine> currentEngineReference = new AtomicReference<>();
     protected final EngineFactory engineFactory;
 
+    private final IndexingOperationListener indexingOperationListeners;
+
     @Nullable
     private RecoveryState recoveryState;
 
@@ -215,7 +216,7 @@ public class IndexShard extends AbstractIndexShardComponent {
     public IndexShard(ShardId shardId, IndexSettings indexSettings, ShardPath path, Store store, IndexCache indexCache,
                       MapperService mapperService, SimilarityService similarityService, IndexFieldDataService indexFieldDataService,
                       @Nullable EngineFactory engineFactory,
-                      IndexEventListener indexEventListener, IndexSearcherWrapper indexSearcherWrapper, NodeServicesProvider provider) {
+                      IndexEventListener indexEventListener, IndexSearcherWrapper indexSearcherWrapper, NodeServicesProvider provider, IndexingOperationListener... listeners) {
         super(shardId, indexSettings);
         final Settings settings = indexSettings.getSettings();
         this.inactiveTime = settings.getAsTime(INDEX_SHARD_INACTIVE_TIME_SETTING, settings.getAsTime(INDICES_INACTIVE_TIME_SETTING, TimeValue.timeValueMinutes(5)));
@@ -232,7 +233,10 @@ public class IndexShard extends AbstractIndexShardComponent {
         this.threadPool = provider.getThreadPool();
         this.mapperService = mapperService;
         this.indexCache = indexCache;
-        this.indexingService = new ShardIndexingService(shardId, indexSettings);
+        this.internalIndexingStats = new InternalIndexingStats();
+        final List<IndexingOperationListener> listenersList = new ArrayList<>(Arrays.asList(listeners));
+        listenersList.add(internalIndexingStats);
+        this.indexingOperationListeners = new IndexingOperationListener.CompositeListener(listenersList, logger);
         this.getService = new ShardGetService(indexSettings, this, mapperService);
         this.termVectorsService = provider.getTermVectorsService();
         this.searchService = new ShardSearchStats(settings);
@@ -285,10 +289,6 @@ public class IndexShard extends AbstractIndexShardComponent {
         return true;
     }
 
-    public ShardIndexingService indexingService() {
-        return this.indexingService;
-    }
-
     public ShardGetService getService() {
         return this.getService;
     }
@@ -489,7 +489,7 @@ public class IndexShard extends AbstractIndexShardComponent {
     public boolean index(Engine.Index index) {
         ensureWriteAllowed(index);
         markLastWrite();
-        index = indexingService.preIndex(index);
+        index = indexingOperationListeners.preIndex(index);
         final boolean created;
         try {
             if (logger.isTraceEnabled()) {
@@ -503,10 +503,10 @@ public class IndexShard extends AbstractIndexShardComponent {
             }
             index.endTime(System.nanoTime());
         } catch (Throwable ex) {
-            indexingService.postIndex(index, ex);
+            indexingOperationListeners.postIndex(index, ex);
             throw ex;
         }
-        indexingService.postIndex(index);
+        indexingOperationListeners.postIndex(index);
         return created;
     }
 
@@ -532,7 +532,7 @@ public class IndexShard extends AbstractIndexShardComponent {
     public void delete(Engine.Delete delete) {
         ensureWriteAllowed(delete);
         markLastWrite();
-        delete = indexingService.preDelete(delete);
+        delete = indexingOperationListeners.preDelete(delete);
         try {
             if (logger.isTraceEnabled()) {
                 logger.trace("delete [{}]", delete.uid().text());
@@ -545,10 +545,10 @@ public class IndexShard extends AbstractIndexShardComponent {
             }
             delete.endTime(System.nanoTime());
         } catch (Throwable ex) {
-            indexingService.postDelete(delete, ex);
+            indexingOperationListeners.postDelete(delete, ex);
             throw ex;
         }
-        indexingService.postDelete(delete);
+        indexingOperationListeners.postDelete(delete);
     }
 
     public Engine.GetResult get(Engine.Get get) {
@@ -600,7 +600,7 @@ public class IndexShard extends AbstractIndexShardComponent {
             throttled = engine.isThrottled();
             throttleTimeInMillis = engine.getIndexThrottleTimeInMillis();
         }
-        return indexingService.stats(throttled, throttleTimeInMillis, types);
+        return internalIndexingStats.stats(throttled, throttleTimeInMillis, types);
     }
 
     public SearchStats searchStats(String... groups) {
@@ -1222,7 +1222,6 @@ public class IndexShard extends AbstractIndexShardComponent {
         }
         mergePolicyConfig.onRefreshSettings(settings);
         searchService.onRefreshSettings(settings);
-        indexingService.onRefreshSettings(settings);
         if (change) {
             getEngine().onSettingsChanged();
         }
@@ -1258,6 +1257,14 @@ public class IndexShard extends AbstractIndexShardComponent {
         return inactiveTime;
     }
 
+    /**
+     * Should be called for each no-op update operation to increment relevant statistics.
+     * @param type the doc type of the update
+     */
+    public void noopUpdate(String type) {
+        internalIndexingStats.noopUpdate(type);
+    }
+
     class EngineRefresher implements Runnable {
         @Override
         public void run() {
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexingOperationListener.java b/core/src/main/java/org/elasticsearch/index/shard/IndexingOperationListener.java
new file mode 100644
index 0000000..e5d3574
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexingOperationListener.java
@@ -0,0 +1,152 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.shard;
+
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.index.engine.Engine;
+
+import java.util.List;
+
+/**
+ * An indexing listener for indexing, delete, events.
+ */
+public interface IndexingOperationListener {
+
+    /**
+     * Called before the indexing occurs.
+     */
+    default Engine.Index preIndex(Engine.Index operation) {
+        return operation;
+    }
+
+    /**
+     * Called after the indexing operation occurred.
+     */
+    default void postIndex(Engine.Index index) {}
+
+    /**
+     * Called after the indexing operation occurred with exception.
+     */
+    default void postIndex(Engine.Index index, Throwable ex) {}
+
+    /**
+     * Called before the delete occurs.
+     */
+    default Engine.Delete preDelete(Engine.Delete delete) {
+        return delete;
+    }
+
+
+    /**
+     * Called after the delete operation occurred.
+     */
+    default void postDelete(Engine.Delete delete) {}
+
+    /**
+     * Called after the delete operation occurred with exception.
+     */
+    default void postDelete(Engine.Delete delete, Throwable ex) {}
+
+    /**
+     * A Composite listener that multiplexes calls to each of the listeners methods.
+     */
+    final class CompositeListener implements IndexingOperationListener{
+        private final List<IndexingOperationListener> listeners;
+        private final ESLogger logger;
+
+        public CompositeListener(List<IndexingOperationListener> listeners, ESLogger logger) {
+            this.listeners = listeners;
+            this.logger = logger;
+        }
+
+        @Override
+        public Engine.Index preIndex(Engine.Index operation) {
+            assert operation != null;
+            for (IndexingOperationListener listener : listeners) {
+                try {
+                    listener.preIndex(operation);
+                } catch (Throwable t) {
+                    logger.warn("preIndex listener [{}] failed", t, listener);
+                }
+            }
+            return operation;
+        }
+
+        @Override
+        public void postIndex(Engine.Index index) {
+            assert index != null;
+            for (IndexingOperationListener listener : listeners) {
+                try {
+                    listener.postIndex(index);
+                } catch (Throwable t) {
+                    logger.warn("postIndex listener [{}] failed", t, listener);
+                }
+            }
+        }
+
+        @Override
+        public void postIndex(Engine.Index index, Throwable ex) {
+            assert index != null && ex != null;
+            for (IndexingOperationListener listener : listeners) {
+                try {
+                    listener.postIndex(index, ex);
+                } catch (Throwable t) {
+                    logger.warn("postIndex listener [{}] failed", t, listener);
+                }
+            }
+        }
+
+        @Override
+        public Engine.Delete preDelete(Engine.Delete delete) {
+            assert delete != null;
+            for (IndexingOperationListener listener : listeners) {
+                try {
+                    listener.preDelete(delete);
+                } catch (Throwable t) {
+                    logger.warn("preDelete listener [{}] failed", t, listener);
+                }
+            }
+            return delete;
+        }
+
+        @Override
+        public void postDelete(Engine.Delete delete) {
+            assert delete != null;
+            for (IndexingOperationListener listener : listeners) {
+                try {
+                    listener.postDelete(delete);
+                } catch (Throwable t) {
+                    logger.warn("postDelete listener [{}] failed", t, listener);
+                }
+            }
+        }
+
+        @Override
+        public void postDelete(Engine.Delete delete, Throwable ex) {
+            assert delete != null && ex != null;
+            for (IndexingOperationListener listener : listeners) {
+                try {
+                    listener.postDelete(delete, ex);
+                } catch (Throwable t) {
+                    logger.warn("postDelete listener [{}] failed", t, listener);
+                }
+            }
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexingStats.java b/core/src/main/java/org/elasticsearch/index/shard/IndexingStats.java
new file mode 100644
index 0000000..27cda2c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexingStats.java
@@ -0,0 +1,321 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.shard;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.io.stream.Streamable;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ */
+public class IndexingStats implements Streamable, ToXContent {
+
+    public static class Stats implements Streamable, ToXContent {
+
+        private long indexCount;
+        private long indexTimeInMillis;
+        private long indexCurrent;
+        private long indexFailedCount;
+        private long deleteCount;
+        private long deleteTimeInMillis;
+        private long deleteCurrent;
+        private long noopUpdateCount;
+        private long throttleTimeInMillis;
+        private boolean isThrottled;
+
+        Stats() {}
+
+        public Stats(long indexCount, long indexTimeInMillis, long indexCurrent, long indexFailedCount, long deleteCount, long deleteTimeInMillis, long deleteCurrent, long noopUpdateCount, boolean isThrottled, long throttleTimeInMillis) {
+            this.indexCount = indexCount;
+            this.indexTimeInMillis = indexTimeInMillis;
+            this.indexCurrent = indexCurrent;
+            this.indexFailedCount = indexFailedCount;
+            this.deleteCount = deleteCount;
+            this.deleteTimeInMillis = deleteTimeInMillis;
+            this.deleteCurrent = deleteCurrent;
+            this.noopUpdateCount = noopUpdateCount;
+            this.isThrottled = isThrottled;
+            this.throttleTimeInMillis = throttleTimeInMillis;
+        }
+
+        public void add(Stats stats) {
+            indexCount += stats.indexCount;
+            indexTimeInMillis += stats.indexTimeInMillis;
+            indexCurrent += stats.indexCurrent;
+            indexFailedCount += stats.indexFailedCount;
+
+            deleteCount += stats.deleteCount;
+            deleteTimeInMillis += stats.deleteTimeInMillis;
+            deleteCurrent += stats.deleteCurrent;
+
+            noopUpdateCount += stats.noopUpdateCount;
+            throttleTimeInMillis += stats.throttleTimeInMillis;
+            if (isThrottled != stats.isThrottled) {
+                isThrottled = true; //When combining if one is throttled set result to throttled.
+            }
+        }
+
+        /**
+         * The total number of indexing operations
+         */
+        public long getIndexCount() { return indexCount; }
+
+        /**
+         * The number of failed indexing operations
+         */
+        public long getIndexFailedCount() { return indexFailedCount; }
+
+        /**
+         * The total amount of time spend on executing index operations.
+         */
+        public TimeValue getIndexTime() { return new TimeValue(indexTimeInMillis); }
+
+        /**
+         * Returns the currently in-flight indexing operations.
+         */
+        public long getIndexCurrent() { return indexCurrent;}
+
+        /**
+         * Returns the number of delete operation executed
+         */
+        public long getDeleteCount() {
+            return deleteCount;
+        }
+
+        /**
+         * Returns if the index is under merge throttling control
+         */
+        public boolean isThrottled() { return isThrottled; }
+
+        /**
+         * Gets the amount of time in a TimeValue that the index has been under merge throttling control
+         */
+        public TimeValue getThrottleTime() { return new TimeValue(throttleTimeInMillis); }
+
+        /**
+         * The total amount of time spend on executing delete operations.
+         */
+        public TimeValue getDeleteTime() { return new TimeValue(deleteTimeInMillis); }
+
+        /**
+         * Returns the currently in-flight delete operations
+         */
+        public long getDeleteCurrent() {
+            return deleteCurrent;
+        }
+
+        public long getNoopUpdateCount() {
+            return noopUpdateCount;
+        }
+
+        public static Stats readStats(StreamInput in) throws IOException {
+            Stats stats = new Stats();
+            stats.readFrom(in);
+            return stats;
+        }
+
+        @Override
+        public void readFrom(StreamInput in) throws IOException {
+            indexCount = in.readVLong();
+            indexTimeInMillis = in.readVLong();
+            indexCurrent = in.readVLong();
+
+            if(in.getVersion().onOrAfter(Version.V_2_1_0)){
+                indexFailedCount = in.readVLong();
+            }
+
+            deleteCount = in.readVLong();
+            deleteTimeInMillis = in.readVLong();
+            deleteCurrent = in.readVLong();
+            noopUpdateCount = in.readVLong();
+            isThrottled = in.readBoolean();
+            throttleTimeInMillis = in.readLong();
+        }
+
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            out.writeVLong(indexCount);
+            out.writeVLong(indexTimeInMillis);
+            out.writeVLong(indexCurrent);
+
+            if(out.getVersion().onOrAfter(Version.V_2_1_0)) {
+                out.writeVLong(indexFailedCount);
+            }
+
+            out.writeVLong(deleteCount);
+            out.writeVLong(deleteTimeInMillis);
+            out.writeVLong(deleteCurrent);
+            out.writeVLong(noopUpdateCount);
+            out.writeBoolean(isThrottled);
+            out.writeLong(throttleTimeInMillis);
+
+        }
+
+        @Override
+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+            builder.field(Fields.INDEX_TOTAL, indexCount);
+            builder.timeValueField(Fields.INDEX_TIME_IN_MILLIS, Fields.INDEX_TIME, indexTimeInMillis);
+            builder.field(Fields.INDEX_CURRENT, indexCurrent);
+            builder.field(Fields.INDEX_FAILED, indexFailedCount);
+
+            builder.field(Fields.DELETE_TOTAL, deleteCount);
+            builder.timeValueField(Fields.DELETE_TIME_IN_MILLIS, Fields.DELETE_TIME, deleteTimeInMillis);
+            builder.field(Fields.DELETE_CURRENT, deleteCurrent);
+
+            builder.field(Fields.NOOP_UPDATE_TOTAL, noopUpdateCount);
+
+            builder.field(Fields.IS_THROTTLED, isThrottled);
+            builder.timeValueField(Fields.THROTTLED_TIME_IN_MILLIS, Fields.THROTTLED_TIME, throttleTimeInMillis);
+            return builder;
+        }
+    }
+
+    private Stats totalStats;
+
+    @Nullable
+    private Map<String, Stats> typeStats;
+
+    public IndexingStats() {
+        totalStats = new Stats();
+    }
+
+    public IndexingStats(Stats totalStats, @Nullable Map<String, Stats> typeStats) {
+        this.totalStats = totalStats;
+        this.typeStats = typeStats;
+    }
+
+    public void add(IndexingStats indexingStats) {
+        add(indexingStats, true);
+    }
+
+    public void add(IndexingStats indexingStats, boolean includeTypes) {
+        if (indexingStats == null) {
+            return;
+        }
+        addTotals(indexingStats);
+        if (includeTypes && indexingStats.typeStats != null && !indexingStats.typeStats.isEmpty()) {
+            if (typeStats == null) {
+                typeStats = new HashMap<>(indexingStats.typeStats.size());
+            }
+            for (Map.Entry<String, Stats> entry : indexingStats.typeStats.entrySet()) {
+                Stats stats = typeStats.get(entry.getKey());
+                if (stats == null) {
+                    typeStats.put(entry.getKey(), entry.getValue());
+                } else {
+                    stats.add(entry.getValue());
+                }
+            }
+        }
+    }
+
+    public void addTotals(IndexingStats indexingStats) {
+        if (indexingStats == null) {
+            return;
+        }
+        totalStats.add(indexingStats.totalStats);
+    }
+
+    public Stats getTotal() {
+        return this.totalStats;
+    }
+
+    @Nullable
+    public Map<String, Stats> getTypeStats() {
+        return this.typeStats;
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, ToXContent.Params params) throws IOException {
+        builder.startObject(Fields.INDEXING);
+        totalStats.toXContent(builder, params);
+        if (typeStats != null && !typeStats.isEmpty()) {
+            builder.startObject(Fields.TYPES);
+            for (Map.Entry<String, Stats> entry : typeStats.entrySet()) {
+                builder.startObject(entry.getKey(), XContentBuilder.FieldCaseConversion.NONE);
+                entry.getValue().toXContent(builder, params);
+                builder.endObject();
+            }
+            builder.endObject();
+        }
+        builder.endObject();
+        return builder;
+    }
+
+    static final class Fields {
+        static final XContentBuilderString INDEXING = new XContentBuilderString("indexing");
+        static final XContentBuilderString TYPES = new XContentBuilderString("types");
+        static final XContentBuilderString INDEX_TOTAL = new XContentBuilderString("index_total");
+        static final XContentBuilderString INDEX_TIME = new XContentBuilderString("index_time");
+        static final XContentBuilderString INDEX_TIME_IN_MILLIS = new XContentBuilderString("index_time_in_millis");
+        static final XContentBuilderString INDEX_CURRENT = new XContentBuilderString("index_current");
+        static final XContentBuilderString INDEX_FAILED = new XContentBuilderString("index_failed");
+        static final XContentBuilderString DELETE_TOTAL = new XContentBuilderString("delete_total");
+        static final XContentBuilderString DELETE_TIME = new XContentBuilderString("delete_time");
+        static final XContentBuilderString DELETE_TIME_IN_MILLIS = new XContentBuilderString("delete_time_in_millis");
+        static final XContentBuilderString DELETE_CURRENT = new XContentBuilderString("delete_current");
+        static final XContentBuilderString NOOP_UPDATE_TOTAL = new XContentBuilderString("noop_update_total");
+        static final XContentBuilderString IS_THROTTLED = new XContentBuilderString("is_throttled");
+        static final XContentBuilderString THROTTLED_TIME_IN_MILLIS = new XContentBuilderString("throttle_time_in_millis");
+        static final XContentBuilderString THROTTLED_TIME = new XContentBuilderString("throttle_time");
+    }
+
+    public static IndexingStats readIndexingStats(StreamInput in) throws IOException {
+        IndexingStats indexingStats = new IndexingStats();
+        indexingStats.readFrom(in);
+        return indexingStats;
+    }
+
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        totalStats = Stats.readStats(in);
+        if (in.readBoolean()) {
+            int size = in.readVInt();
+            typeStats = new HashMap<>(size);
+            for (int i = 0; i < size; i++) {
+                typeStats.put(in.readString(), Stats.readStats(in));
+            }
+        }
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        totalStats.writeTo(out);
+        if (typeStats == null || typeStats.isEmpty()) {
+            out.writeBoolean(false);
+        } else {
+            out.writeBoolean(true);
+            out.writeVInt(typeStats.size());
+            for (Map.Entry<String, Stats> entry : typeStats.entrySet()) {
+                out.writeString(entry.getKey());
+                entry.getValue().writeTo(out);
+            }
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/index/shard/InternalIndexingStats.java b/core/src/main/java/org/elasticsearch/index/shard/InternalIndexingStats.java
new file mode 100644
index 0000000..ce8c814
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/index/shard/InternalIndexingStats.java
@@ -0,0 +1,154 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.shard;
+
+import org.elasticsearch.common.collect.MapBuilder;
+import org.elasticsearch.common.metrics.CounterMetric;
+import org.elasticsearch.common.metrics.MeanMetric;
+import org.elasticsearch.common.regex.Regex;
+import org.elasticsearch.index.engine.Engine;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+import static java.util.Collections.emptyMap;
+
+/**
+ * Internal class that maintains relevant indexing statistics / metrics.
+ * @see IndexShard
+ */
+final class InternalIndexingStats implements IndexingOperationListener {
+    private final StatsHolder totalStats = new StatsHolder();
+    private volatile Map<String, StatsHolder> typesStats = emptyMap();
+
+    /**
+     * Returns the stats, including type specific stats. If the types are null/0 length, then nothing
+     * is returned for them. If they are set, then only types provided will be returned, or
+     * <tt>_all</tt> for all types.
+     */
+    IndexingStats stats(boolean isThrottled, long currentThrottleInMillis, String... types) {
+        IndexingStats.Stats total = totalStats.stats(isThrottled, currentThrottleInMillis);
+        Map<String, IndexingStats.Stats> typesSt = null;
+        if (types != null && types.length > 0) {
+            typesSt = new HashMap<>(typesStats.size());
+            if (types.length == 1 && types[0].equals("_all")) {
+                for (Map.Entry<String, StatsHolder> entry : typesStats.entrySet()) {
+                    typesSt.put(entry.getKey(), entry.getValue().stats(isThrottled, currentThrottleInMillis));
+                }
+            } else {
+                for (Map.Entry<String, StatsHolder> entry : typesStats.entrySet()) {
+                    if (Regex.simpleMatch(types, entry.getKey())) {
+                        typesSt.put(entry.getKey(), entry.getValue().stats(isThrottled, currentThrottleInMillis));
+                    }
+                }
+            }
+        }
+        return new IndexingStats(total, typesSt);
+    }
+
+    @Override
+    public Engine.Index preIndex(Engine.Index operation) {
+        totalStats.indexCurrent.inc();
+        typeStats(operation.type()).indexCurrent.inc();
+        return operation;
+    }
+
+    @Override
+    public void postIndex(Engine.Index index) {
+        long took = index.endTime() - index.startTime();
+        totalStats.indexMetric.inc(took);
+        totalStats.indexCurrent.dec();
+        StatsHolder typeStats = typeStats(index.type());
+        typeStats.indexMetric.inc(took);
+        typeStats.indexCurrent.dec();
+    }
+
+    @Override
+    public void postIndex(Engine.Index index, Throwable ex) {
+        totalStats.indexCurrent.dec();
+        typeStats(index.type()).indexCurrent.dec();
+        totalStats.indexFailed.inc();
+        typeStats(index.type()).indexFailed.inc();
+    }
+
+    @Override
+    public Engine.Delete preDelete(Engine.Delete delete) {
+        totalStats.deleteCurrent.inc();
+        typeStats(delete.type()).deleteCurrent.inc();
+        return delete;
+    }
+
+    @Override
+    public void postDelete(Engine.Delete delete) {
+        long took = delete.endTime() - delete.startTime();
+        totalStats.deleteMetric.inc(took);
+        totalStats.deleteCurrent.dec();
+        StatsHolder typeStats = typeStats(delete.type());
+        typeStats.deleteMetric.inc(took);
+        typeStats.deleteCurrent.dec();
+    }
+
+    @Override
+    public void postDelete(Engine.Delete delete, Throwable ex) {
+        totalStats.deleteCurrent.dec();
+        typeStats(delete.type()).deleteCurrent.dec();
+    }
+
+    public void noopUpdate(String type) {
+        totalStats.noopUpdates.inc();
+        typeStats(type).noopUpdates.inc();
+    }
+
+    private StatsHolder typeStats(String type) {
+        StatsHolder stats = typesStats.get(type);
+        if (stats == null) {
+            synchronized (this) {
+                stats = typesStats.get(type);
+                if (stats == null) {
+                    stats = new StatsHolder();
+                    typesStats = MapBuilder.newMapBuilder(typesStats).put(type, stats).immutableMap();
+                }
+            }
+        }
+        return stats;
+    }
+
+    static class StatsHolder {
+        private final MeanMetric indexMetric = new MeanMetric();
+        private final MeanMetric deleteMetric = new MeanMetric();
+        private final CounterMetric indexCurrent = new CounterMetric();
+        private final CounterMetric indexFailed = new CounterMetric();
+        private final CounterMetric deleteCurrent = new CounterMetric();
+        private final CounterMetric noopUpdates = new CounterMetric();
+
+        IndexingStats.Stats stats(boolean isThrottled, long currentThrottleMillis) {
+            return new IndexingStats.Stats(
+                indexMetric.count(), TimeUnit.NANOSECONDS.toMillis(indexMetric.sum()), indexCurrent.count(), indexFailed.count(),
+                deleteMetric.count(), TimeUnit.NANOSECONDS.toMillis(deleteMetric.sum()), deleteCurrent.count(),
+                noopUpdates.count(), isThrottled, TimeUnit.MILLISECONDS.toMillis(currentThrottleMillis));
+        }
+
+        void clear() {
+            indexMetric.clear();
+            deleteMetric.clear();
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java b/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
index 88df4ee..4811ff1 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
@@ -70,6 +70,7 @@ public class TranslogRecoveryPerformer {
                 performRecoveryOperation(engine, operation, false);
                 numOps++;
             }
+            engine.getTranslog().sync();
         } catch (Throwable t) {
             throw new BatchOperationException(shardId, "failed to apply batch translog operation", numOps, t);
         }
diff --git a/core/src/main/java/org/elasticsearch/index/store/Store.java b/core/src/main/java/org/elasticsearch/index/store/Store.java
index c47770d..3b3074e 100644
--- a/core/src/main/java/org/elasticsearch/index/store/Store.java
+++ b/core/src/main/java/org/elasticsearch/index/store/Store.java
@@ -118,6 +118,7 @@ import static java.util.Collections.unmodifiableMap;
  * </pre>
  */
 public class Store extends AbstractIndexShardComponent implements Closeable, RefCounted {
+    private static final Version FIRST_LUCENE_CHECKSUM_VERSION = Version.LUCENE_4_8_0;
 
     static final String CODEC = "store";
     static final int VERSION_WRITE_THROWABLE= 2; // we write throwable since 2.0
@@ -466,7 +467,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
                 output = new LegacyVerification.LengthVerifyingIndexOutput(output, metadata.length());
             } else {
                 assert metadata.writtenBy() != null;
-                assert metadata.writtenBy().onOrAfter(Version.LUCENE_4_8);
+                assert metadata.writtenBy().onOrAfter(FIRST_LUCENE_CHECKSUM_VERSION);
                 output = new LuceneVerifyingIndexOutput(metadata, output);
             }
             success = true;
@@ -490,7 +491,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
             return directory().openInput(filename, context);
         }
         assert metadata.writtenBy() != null;
-        assert metadata.writtenBy().onOrAfter(Version.LUCENE_4_8_0);
+        assert metadata.writtenBy().onOrAfter(FIRST_LUCENE_CHECKSUM_VERSION);
         return new VerifyingIndexInput(directory().openInput(filename, context));
     }
 
@@ -518,7 +519,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
             if (input.length() != md.length()) { // first check the length no matter how old this file is
                 throw new CorruptIndexException("expected length=" + md.length() + " != actual length: " + input.length() + " : file truncated?", input);
             }
-            if (md.writtenBy() != null && md.writtenBy().onOrAfter(Version.LUCENE_4_8_0)) {
+            if (md.writtenBy() != null && md.writtenBy().onOrAfter(FIRST_LUCENE_CHECKSUM_VERSION)) {
                 // throw exception if the file is corrupt
                 String checksum = Store.digestToString(CodecUtil.checksumEntireFile(input));
                 // throw exception if metadata is inconsistent
@@ -766,7 +767,6 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
      */
     public final static class MetadataSnapshot implements Iterable<StoreFileMetaData>, Writeable<MetadataSnapshot> {
         private static final ESLogger logger = Loggers.getLogger(MetadataSnapshot.class);
-        private static final Version FIRST_LUCENE_CHECKSUM_VERSION = Version.LUCENE_4_8;
 
         private final Map<String, StoreFileMetaData> metadata;
 
@@ -843,6 +843,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
                 final SegmentInfos segmentCommitInfos = Store.readSegmentsInfo(commit, directory);
                 numDocs = Lucene.getNumDocs(segmentCommitInfos);
                 commitUserDataBuilder.putAll(segmentCommitInfos.getUserData());
+                @SuppressWarnings("deprecation")
                 Version maxVersion = Version.LUCENE_4_0; // we don't know which version was used to write so we take the max version.
                 for (SegmentCommitInfo info : segmentCommitInfos) {
                     final Version version = info.info.getVersion();
@@ -907,6 +908,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
          * @param directory the directory to read checksums from
          * @return a map of file checksums and the checksum file version
          */
+        @SuppressWarnings("deprecation") // Legacy checksum needs legacy methods
         static Tuple<Map<String, String>, Long> readLegacyChecksums(Directory directory) throws IOException {
             synchronized (directory) {
                 long lastFound = -1;
@@ -922,10 +924,10 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
                 if (lastFound > -1) {
                     try (IndexInput indexInput = directory.openInput(CHECKSUMS_PREFIX + lastFound, IOContext.READONCE)) {
                         indexInput.readInt(); // version
-                        return new Tuple(indexInput.readStringStringMap(), lastFound);
+                        return new Tuple<>(indexInput.readStringStringMap(), lastFound);
                     }
                 }
-                return new Tuple(new HashMap<>(), -1l);
+                return new Tuple<>(new HashMap<>(), -1l);
             }
         }
 
@@ -1243,6 +1245,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
             }
         }
 
+        @SuppressWarnings("deprecation") // Legacy checksum uses legacy methods
         synchronized void writeChecksums(Directory directory, Map<String, String> checksums, long lastVersion) throws IOException {
             // Make sure if clock goes backwards we still move version forwards:
             long nextVersion = Math.max(lastVersion+1, System.currentTimeMillis());
diff --git a/core/src/main/java/org/elasticsearch/indices/IndexingMemoryController.java b/core/src/main/java/org/elasticsearch/indices/IndexingMemoryController.java
new file mode 100644
index 0000000..d84c3b0
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/indices/IndexingMemoryController.java
@@ -0,0 +1,242 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.indices;
+
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.util.concurrent.FutureUtils;
+import org.elasticsearch.index.IndexService;
+import org.elasticsearch.index.engine.EngineClosedException;
+import org.elasticsearch.index.engine.FlushNotAllowedEngineException;
+import org.elasticsearch.index.shard.IndexEventListener;
+import org.elasticsearch.index.shard.IndexShard;
+import org.elasticsearch.index.shard.IndexShardState;
+import org.elasticsearch.monitor.jvm.JvmInfo;
+import org.elasticsearch.threadpool.ThreadPool;
+
+import java.io.Closeable;
+import java.util.ArrayList;
+import java.util.EnumSet;
+import java.util.List;
+import java.util.concurrent.ScheduledFuture;
+
+public class IndexingMemoryController extends AbstractComponent implements IndexEventListener, Closeable {
+
+    /** How much heap (% or bytes) we will share across all actively indexing shards on this node (default: 10%). */
+    public static final String INDEX_BUFFER_SIZE_SETTING = "indices.memory.index_buffer_size";
+
+    /** Only applies when <code>indices.memory.index_buffer_size</code> is a %, to set a floor on the actual size in bytes (default: 48 MB). */
+    public static final String MIN_INDEX_BUFFER_SIZE_SETTING = "indices.memory.min_index_buffer_size";
+
+    /** Only applies when <code>indices.memory.index_buffer_size</code> is a %, to set a ceiling on the actual size in bytes (default: not set). */
+    public static final String MAX_INDEX_BUFFER_SIZE_SETTING = "indices.memory.max_index_buffer_size";
+
+    /** Sets a floor on the per-shard index buffer size (default: 4 MB). */
+    public static final String MIN_SHARD_INDEX_BUFFER_SIZE_SETTING = "indices.memory.min_shard_index_buffer_size";
+
+    /** Sets a ceiling on the per-shard index buffer size (default: 512 MB). */
+    public static final String MAX_SHARD_INDEX_BUFFER_SIZE_SETTING = "indices.memory.max_shard_index_buffer_size";
+
+    /** Sets a floor on the per-shard translog buffer size (default: 2 KB). */
+    public static final String MIN_SHARD_TRANSLOG_BUFFER_SIZE_SETTING = "indices.memory.min_shard_translog_buffer_size";
+
+    /** Sets a ceiling on the per-shard translog buffer size (default: 64 KB). */
+    public static final String MAX_SHARD_TRANSLOG_BUFFER_SIZE_SETTING = "indices.memory.max_shard_translog_buffer_size";
+
+    /** How frequently we check shards to find inactive ones (default: 30 seconds). */
+    public static final String SHARD_INACTIVE_INTERVAL_TIME_SETTING = "indices.memory.interval";
+
+    /** Once a shard becomes inactive, we reduce the {@code IndexWriter} buffer to this value (500 KB) to let active shards use the heap instead. */
+    public static final ByteSizeValue INACTIVE_SHARD_INDEXING_BUFFER = ByteSizeValue.parseBytesSizeValue("500kb", "INACTIVE_SHARD_INDEXING_BUFFER");
+
+    private final IndicesService indicesService;
+
+    private final ByteSizeValue indexingBuffer;
+    private final ByteSizeValue minShardIndexBufferSize;
+    private final ByteSizeValue maxShardIndexBufferSize;
+    private final TimeValue interval;
+
+    private final ScheduledFuture scheduler;
+
+    private static final EnumSet<IndexShardState> CAN_UPDATE_INDEX_BUFFER_STATES = EnumSet.of(
+            IndexShardState.RECOVERING, IndexShardState.POST_RECOVERY, IndexShardState.STARTED, IndexShardState.RELOCATED);
+
+    private final ShardsIndicesStatusChecker statusChecker;
+
+    IndexingMemoryController(Settings settings, ThreadPool threadPool, IndicesService indicesService) {
+        this(settings, threadPool, indicesService, JvmInfo.jvmInfo().getMem().getHeapMax().bytes());
+    }
+
+    // for testing
+    IndexingMemoryController(Settings settings, ThreadPool threadPool, IndicesService indicesService, long jvmMemoryInBytes) {
+        super(settings);
+        this.indicesService = indicesService;
+
+        ByteSizeValue indexingBuffer;
+        String indexingBufferSetting = this.settings.get(INDEX_BUFFER_SIZE_SETTING, "10%");
+        if (indexingBufferSetting.endsWith("%")) {
+            double percent = Double.parseDouble(indexingBufferSetting.substring(0, indexingBufferSetting.length() - 1));
+            indexingBuffer = new ByteSizeValue((long) (((double) jvmMemoryInBytes) * (percent / 100)));
+            ByteSizeValue minIndexingBuffer = this.settings.getAsBytesSize(MIN_INDEX_BUFFER_SIZE_SETTING, new ByteSizeValue(48, ByteSizeUnit.MB));
+            ByteSizeValue maxIndexingBuffer = this.settings.getAsBytesSize(MAX_INDEX_BUFFER_SIZE_SETTING, null);
+
+            if (indexingBuffer.bytes() < minIndexingBuffer.bytes()) {
+                indexingBuffer = minIndexingBuffer;
+            }
+            if (maxIndexingBuffer != null && indexingBuffer.bytes() > maxIndexingBuffer.bytes()) {
+                indexingBuffer = maxIndexingBuffer;
+            }
+        } else {
+            indexingBuffer = ByteSizeValue.parseBytesSizeValue(indexingBufferSetting, INDEX_BUFFER_SIZE_SETTING);
+        }
+        this.indexingBuffer = indexingBuffer;
+        this.minShardIndexBufferSize = this.settings.getAsBytesSize(MIN_SHARD_INDEX_BUFFER_SIZE_SETTING, new ByteSizeValue(4, ByteSizeUnit.MB));
+        // LUCENE MONITOR: Based on this thread, currently (based on Mike), having a large buffer does not make a lot of sense: https://issues.apache.org/jira/browse/LUCENE-2324?focusedCommentId=13005155&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13005155
+        this.maxShardIndexBufferSize = this.settings.getAsBytesSize(MAX_SHARD_INDEX_BUFFER_SIZE_SETTING, new ByteSizeValue(512, ByteSizeUnit.MB));
+
+        // we need to have this relatively small to move a shard from inactive to active fast (enough)
+        this.interval = this.settings.getAsTime(SHARD_INACTIVE_INTERVAL_TIME_SETTING, TimeValue.timeValueSeconds(30));
+
+        this.statusChecker = new ShardsIndicesStatusChecker();
+
+        logger.debug("using indexing buffer size [{}], with {} [{}], {} [{}], {} [{}]",
+                this.indexingBuffer,
+                MIN_SHARD_INDEX_BUFFER_SIZE_SETTING, this.minShardIndexBufferSize,
+                MAX_SHARD_INDEX_BUFFER_SIZE_SETTING, this.maxShardIndexBufferSize,
+                SHARD_INACTIVE_INTERVAL_TIME_SETTING, this.interval);
+        this.scheduler = scheduleTask(threadPool);
+    }
+
+    protected ScheduledFuture<?> scheduleTask(ThreadPool threadPool) {
+        // it's fine to run it on the scheduler thread, no busy work
+        return threadPool.scheduleWithFixedDelay(statusChecker, interval);
+    }
+
+    @Override
+    public void close() {
+        FutureUtils.cancel(scheduler);
+    }
+
+    /**
+     * returns the current budget for the total amount of indexing buffers of
+     * active shards on this node
+     */
+    ByteSizeValue indexingBufferSize() {
+        return indexingBuffer;
+    }
+
+    protected List<IndexShard> availableShards() {
+        List<IndexShard> availableShards = new ArrayList<>();
+
+        for (IndexService indexService : indicesService) {
+            for (IndexShard shard : indexService) {
+                if (shardAvailable(shard)) {
+                    availableShards.add(shard);
+                }
+            }
+        }
+        return availableShards;
+    }
+
+    /** returns true if shard exists and is availabe for updates */
+    protected boolean shardAvailable(IndexShard shard) {
+        // shadow replica doesn't have an indexing buffer
+        return shard.canIndex() && CAN_UPDATE_INDEX_BUFFER_STATES.contains(shard.state());
+    }
+
+    /** set new indexing and translog buffers on this shard.  this may cause the shard to refresh to free up heap. */
+    protected void updateShardBuffers(IndexShard shard, ByteSizeValue shardIndexingBufferSize) {
+        try {
+            shard.updateBufferSize(shardIndexingBufferSize);
+        } catch (EngineClosedException | FlushNotAllowedEngineException e) {
+            // ignore
+        } catch (Exception e) {
+            logger.warn("failed to set shard {} index buffer to [{}]", e, shard.shardId(), shardIndexingBufferSize);
+        }
+    }
+
+    /** check if any shards active status changed, now. */
+    void forceCheck() {
+        statusChecker.run();
+    }
+
+    class ShardsIndicesStatusChecker implements Runnable {
+        @Override
+        public synchronized void run() {
+            List<IndexShard> availableShards = availableShards();
+            List<IndexShard> activeShards = new ArrayList<>();
+            for (IndexShard shard : availableShards) {
+                if (!checkIdle(shard)) {
+                    activeShards.add(shard);
+                }
+            }
+            int activeShardCount = activeShards.size();
+
+            // TODO: we could be smarter here by taking into account how RAM the IndexWriter on each shard
+            // is actually using (using IW.ramBytesUsed), so that small indices (e.g. Marvel) would not
+            // get the same indexing buffer as large indices.  But it quickly gets tricky...
+            if (activeShardCount == 0) {
+                return;
+            }
+
+            ByteSizeValue shardIndexingBufferSize = new ByteSizeValue(indexingBuffer.bytes() / activeShardCount);
+            if (shardIndexingBufferSize.bytes() < minShardIndexBufferSize.bytes()) {
+                shardIndexingBufferSize = minShardIndexBufferSize;
+            }
+            if (shardIndexingBufferSize.bytes() > maxShardIndexBufferSize.bytes()) {
+                shardIndexingBufferSize = maxShardIndexBufferSize;
+            }
+
+            logger.debug("recalculating shard indexing buffer, total is [{}] with [{}] active shards, each shard set to indexing=[{}]", indexingBuffer, activeShardCount, shardIndexingBufferSize);
+
+            for (IndexShard shard : activeShards) {
+                updateShardBuffers(shard, shardIndexingBufferSize);
+            }
+        }
+    }
+
+    protected long currentTimeInNanos() {
+        return System.nanoTime();
+    }
+
+    /**
+     * ask this shard to check now whether it is inactive, and reduces its indexing and translog buffers if so.
+     * return false if the shard is not idle, otherwise true
+     */
+    protected boolean checkIdle(IndexShard shard) {
+        try {
+            return shard.checkIdle();
+        } catch (EngineClosedException | FlushNotAllowedEngineException e) {
+            logger.trace("ignore [{}] while marking shard {} as inactive", e.getClass().getSimpleName(), shard.shardId());
+            return true;
+        }
+    }
+
+    @Override
+    public void onShardActive(IndexShard indexShard) {
+        // At least one shard used to be inactive ie. a new write operation just showed up.
+        // We try to fix the shards indexing buffer immediately. We could do this async instead, but cost should
+        // be low, and it's rare this happens.
+        forceCheck();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/indices/IndicesModule.java b/core/src/main/java/org/elasticsearch/indices/IndicesModule.java
index e5beb4d..faf7f73 100644
--- a/core/src/main/java/org/elasticsearch/indices/IndicesModule.java
+++ b/core/src/main/java/org/elasticsearch/indices/IndicesModule.java
@@ -111,7 +111,6 @@ import org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache;
 import org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCacheListener;
 import org.elasticsearch.indices.flush.SyncedFlushService;
 import org.elasticsearch.indices.mapper.MapperRegistry;
-import org.elasticsearch.indices.memory.IndexingMemoryController;
 import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.indices.recovery.RecoverySettings;
 import org.elasticsearch.indices.recovery.RecoverySource;
@@ -273,7 +272,6 @@ public class IndicesModule extends AbstractModule {
         bind(RecoverySource.class).asEagerSingleton();
         bind(IndicesStore.class).asEagerSingleton();
         bind(IndicesClusterStateService.class).asEagerSingleton();
-        bind(IndexingMemoryController.class).asEagerSingleton();
         bind(SyncedFlushService.class).asEagerSingleton();
         bind(IndicesQueryCache.class).asEagerSingleton();
         bind(IndicesRequestCache.class).asEagerSingleton();
diff --git a/core/src/main/java/org/elasticsearch/indices/IndicesService.java b/core/src/main/java/org/elasticsearch/indices/IndicesService.java
index 36ed70a..d751b24 100644
--- a/core/src/main/java/org/elasticsearch/indices/IndicesService.java
+++ b/core/src/main/java/org/elasticsearch/indices/IndicesService.java
@@ -52,7 +52,7 @@ import org.elasticsearch.index.NodeServicesProvider;
 import org.elasticsearch.index.analysis.AnalysisRegistry;
 import org.elasticsearch.index.flush.FlushStats;
 import org.elasticsearch.index.get.GetStats;
-import org.elasticsearch.index.indexing.IndexingStats;
+import org.elasticsearch.index.shard.IndexingStats;
 import org.elasticsearch.index.merge.MergeStats;
 import org.elasticsearch.index.recovery.RecoveryStats;
 import org.elasticsearch.index.refresh.RefreshStats;
@@ -65,6 +65,7 @@ import org.elasticsearch.index.store.IndexStoreConfig;
 import org.elasticsearch.indices.mapper.MapperRegistry;
 import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.plugins.PluginsService;
+import org.elasticsearch.threadpool.ThreadPool;
 
 import java.io.IOException;
 import java.nio.file.Files;
@@ -105,6 +106,7 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
     private final OldShardsStats oldShardsStats = new OldShardsStats();
     private final IndexStoreConfig indexStoreConfig;
     private final MapperRegistry mapperRegistry;
+    private final IndexingMemoryController indexingMemoryController;
 
     @Override
     protected void doStart() {
@@ -114,7 +116,7 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
     public IndicesService(Settings settings, PluginsService pluginsService, NodeEnvironment nodeEnv,
                           ClusterSettings clusterSettings, AnalysisRegistry analysisRegistry,
                           IndicesQueriesRegistry indicesQueriesRegistry, IndexNameExpressionResolver indexNameExpressionResolver,
-                          ClusterService clusterService, MapperRegistry mapperRegistry) {
+                          ClusterService clusterService, MapperRegistry mapperRegistry, ThreadPool threadPool) {
         super(settings);
         this.pluginsService = pluginsService;
         this.nodeEnv = nodeEnv;
@@ -127,7 +129,7 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
         this.mapperRegistry = mapperRegistry;
         clusterSettings.addSettingsUpdateConsumer(IndexStoreConfig.INDICES_STORE_THROTTLE_TYPE_SETTING, indexStoreConfig::setRateLimitingType);
         clusterSettings.addSettingsUpdateConsumer(IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING, indexStoreConfig::setRateLimitingThrottle);
-
+        indexingMemoryController = new IndexingMemoryController(settings, threadPool, this);
     }
 
     @Override
@@ -161,7 +163,7 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
 
     @Override
     protected void doClose() {
-        IOUtils.closeWhileHandlingException(analysisRegistry);
+        IOUtils.closeWhileHandlingException(analysisRegistry, indexingMemoryController);
     }
 
     /**
@@ -291,6 +293,7 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
 
         final IndexModule indexModule = new IndexModule(idxSettings, indexStoreConfig, analysisRegistry);
         pluginsService.onIndexModule(indexModule);
+        indexModule.addIndexEventListener(indexingMemoryController);
         for (IndexEventListener listener : builtInListeners) {
             indexModule.addIndexEventListener(listener);
         }
diff --git a/core/src/main/java/org/elasticsearch/indices/NodeIndicesStats.java b/core/src/main/java/org/elasticsearch/indices/NodeIndicesStats.java
index c8142f3..0a036cb 100644
--- a/core/src/main/java/org/elasticsearch/indices/NodeIndicesStats.java
+++ b/core/src/main/java/org/elasticsearch/indices/NodeIndicesStats.java
@@ -36,7 +36,7 @@ import org.elasticsearch.index.engine.SegmentsStats;
 import org.elasticsearch.index.fielddata.FieldDataStats;
 import org.elasticsearch.index.flush.FlushStats;
 import org.elasticsearch.index.get.GetStats;
-import org.elasticsearch.index.indexing.IndexingStats;
+import org.elasticsearch.index.shard.IndexingStats;
 import org.elasticsearch.index.merge.MergeStats;
 import org.elasticsearch.index.percolator.PercolateStats;
 import org.elasticsearch.index.recovery.RecoveryStats;
diff --git a/core/src/main/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzers.java b/core/src/main/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzers.java
index 36795c6..6d5c3a8 100644
--- a/core/src/main/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzers.java
+++ b/core/src/main/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzers.java
@@ -73,15 +73,10 @@ import java.util.Locale;
  */
 public enum PreBuiltAnalyzers {
 
-    STANDARD(CachingStrategy.ELASTICSEARCH) { // we don't do stopwords anymore from 1.0Beta on
+    STANDARD(CachingStrategy.ELASTICSEARCH) {
         @Override
         protected Analyzer create(Version version) {
-            final Analyzer a;
-            if (version.onOrAfter(Version.V_1_0_0_Beta1)) {
-                a = new StandardAnalyzer(CharArraySet.EMPTY_SET);
-            } else {
-                a = new StandardAnalyzer();
-            }
+            final Analyzer a = new StandardAnalyzer(CharArraySet.EMPTY_SET);
             a.setVersion(version.luceneVersion);
             return a;
         }
@@ -151,22 +146,14 @@ public enum PreBuiltAnalyzers {
     PATTERN(CachingStrategy.ELASTICSEARCH) {
         @Override
         protected Analyzer create(Version version) {
-            if (version.onOrAfter(Version.V_1_0_0_RC1)) {
-                return new PatternAnalyzer(Regex.compile("\\W+" /*PatternAnalyzer.NON_WORD_PATTERN*/, null), true, CharArraySet.EMPTY_SET);
-            }
-            return new PatternAnalyzer(Regex.compile("\\W+" /*PatternAnalyzer.NON_WORD_PATTERN*/, null), true, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
+            return new PatternAnalyzer(Regex.compile("\\W+" /*PatternAnalyzer.NON_WORD_PATTERN*/, null), true, CharArraySet.EMPTY_SET);
         }
     },
 
     STANDARD_HTML_STRIP(CachingStrategy.ELASTICSEARCH) {
         @Override
         protected Analyzer create(Version version) {
-            final Analyzer analyzer;
-            if (version.onOrAfter(Version.V_1_0_0_RC1)) {
-                analyzer = new StandardHtmlStripAnalyzer(CharArraySet.EMPTY_SET);
-            } else {
-                analyzer = new StandardHtmlStripAnalyzer();
-            }
+            final Analyzer analyzer = new StandardHtmlStripAnalyzer(CharArraySet.EMPTY_SET);
             analyzer.setVersion(version.luceneVersion);
             return analyzer;
         }
diff --git a/core/src/main/java/org/elasticsearch/indices/analysis/PreBuiltTokenFilters.java b/core/src/main/java/org/elasticsearch/indices/analysis/PreBuiltTokenFilters.java
index 631d8ba..027c750 100644
--- a/core/src/main/java/org/elasticsearch/indices/analysis/PreBuiltTokenFilters.java
+++ b/core/src/main/java/org/elasticsearch/indices/analysis/PreBuiltTokenFilters.java
@@ -28,7 +28,6 @@ import org.apache.lucene.analysis.ckb.SoraniNormalizationFilter;
 import org.apache.lucene.analysis.commongrams.CommonGramsFilter;
 import org.apache.lucene.analysis.core.DecimalDigitFilter;
 import org.apache.lucene.analysis.core.LowerCaseFilter;
-import org.apache.lucene.analysis.core.Lucene43StopFilter;
 import org.apache.lucene.analysis.core.StopAnalyzer;
 import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.core.UpperCaseFilter;
@@ -45,9 +44,6 @@ import org.apache.lucene.analysis.miscellaneous.ASCIIFoldingFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordRepeatFilter;
 import org.apache.lucene.analysis.miscellaneous.LengthFilter;
 import org.apache.lucene.analysis.miscellaneous.LimitTokenCountFilter;
-import org.apache.lucene.analysis.miscellaneous.Lucene43LengthFilter;
-import org.apache.lucene.analysis.miscellaneous.Lucene43TrimFilter;
-import org.apache.lucene.analysis.miscellaneous.Lucene47WordDelimiterFilter;
 import org.apache.lucene.analysis.miscellaneous.ScandinavianFoldingFilter;
 import org.apache.lucene.analysis.miscellaneous.ScandinavianNormalizationFilter;
 import org.apache.lucene.analysis.miscellaneous.TrimFilter;
@@ -55,8 +51,6 @@ import org.apache.lucene.analysis.miscellaneous.TruncateTokenFilter;
 import org.apache.lucene.analysis.miscellaneous.UniqueTokenFilter;
 import org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter;
 import org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter;
-import org.apache.lucene.analysis.ngram.Lucene43EdgeNGramTokenFilter;
-import org.apache.lucene.analysis.ngram.Lucene43NGramTokenFilter;
 import org.apache.lucene.analysis.ngram.NGramTokenFilter;
 import org.apache.lucene.analysis.payloads.DelimitedPayloadTokenFilter;
 import org.apache.lucene.analysis.payloads.TypeAsPayloadTokenFilter;
@@ -86,49 +80,26 @@ public enum PreBuiltTokenFilters {
     WORD_DELIMITER(CachingStrategy.ONE) {
         @Override
         public TokenStream create(TokenStream tokenStream, Version version) {
-            if (version.luceneVersion.onOrAfter(org.apache.lucene.util.Version.LUCENE_4_8)) {
-                return new WordDelimiterFilter(tokenStream,
-                           WordDelimiterFilter.GENERATE_WORD_PARTS |
-                           WordDelimiterFilter.GENERATE_NUMBER_PARTS |
-                           WordDelimiterFilter.SPLIT_ON_CASE_CHANGE |
-                           WordDelimiterFilter.SPLIT_ON_NUMERICS |
-                           WordDelimiterFilter.STEM_ENGLISH_POSSESSIVE, null);
-            } else {
-                return new Lucene47WordDelimiterFilter(tokenStream,
-                           WordDelimiterFilter.GENERATE_WORD_PARTS |
-                           WordDelimiterFilter.GENERATE_NUMBER_PARTS |
-                           WordDelimiterFilter.SPLIT_ON_CASE_CHANGE |
-                           WordDelimiterFilter.SPLIT_ON_NUMERICS |
-                           WordDelimiterFilter.STEM_ENGLISH_POSSESSIVE, null);
-            }
+            return new WordDelimiterFilter(tokenStream,
+                       WordDelimiterFilter.GENERATE_WORD_PARTS |
+                       WordDelimiterFilter.GENERATE_NUMBER_PARTS |
+                       WordDelimiterFilter.SPLIT_ON_CASE_CHANGE |
+                       WordDelimiterFilter.SPLIT_ON_NUMERICS |
+                       WordDelimiterFilter.STEM_ENGLISH_POSSESSIVE, null);
         }
-
-
     },
 
     STOP(CachingStrategy.LUCENE) {
         @Override
         public TokenStream create(TokenStream tokenStream, Version version) {
-            if (version.luceneVersion.onOrAfter(org.apache.lucene.util.Version.LUCENE_4_4_0)) {
-                return new StopFilter(tokenStream, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
-            } else {
-                @SuppressWarnings("deprecation")
-                final TokenStream filter = new Lucene43StopFilter(true, tokenStream, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
-                return filter;
-            }
+            return new StopFilter(tokenStream, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
         }
     },
 
     TRIM(CachingStrategy.LUCENE) {
         @Override
         public TokenStream create(TokenStream tokenStream, Version version) {
-            if (version.luceneVersion.onOrAfter(org.apache.lucene.util.Version.LUCENE_4_4_0)) {
-                return new TrimFilter(tokenStream);
-            } else {
-                @SuppressWarnings("deprecation")
-                final TokenStream filter = new Lucene43TrimFilter(tokenStream, true);
-                return filter;
-            }
+            return new TrimFilter(tokenStream);
         }
     },
 
@@ -149,13 +120,7 @@ public enum PreBuiltTokenFilters {
     LENGTH(CachingStrategy.LUCENE) {
         @Override
         public TokenStream create(TokenStream tokenStream, Version version) {
-            if (version.luceneVersion.onOrAfter(org.apache.lucene.util.Version.LUCENE_4_4_0)) {
-                return new LengthFilter(tokenStream, 0, Integer.MAX_VALUE);
-            } else {
-                @SuppressWarnings("deprecation")
-                final TokenStream filter = new Lucene43LengthFilter(true, tokenStream, 0, Integer.MAX_VALUE);
-                return filter;
-            }
+            return new LengthFilter(tokenStream, 0, Integer.MAX_VALUE);
         }
     },
 
@@ -211,26 +176,14 @@ public enum PreBuiltTokenFilters {
     NGRAM(CachingStrategy.LUCENE) {
         @Override
         public TokenStream create(TokenStream tokenStream, Version version) {
-            if (version.luceneVersion.onOrAfter(org.apache.lucene.util.Version.LUCENE_4_4_0)) {
-                return new NGramTokenFilter(tokenStream);
-            } else {
-                @SuppressWarnings("deprecation")
-                final TokenStream filter = new Lucene43NGramTokenFilter(tokenStream);
-                return filter;
-            }
+            return new NGramTokenFilter(tokenStream);
         }
     },
 
     EDGE_NGRAM(CachingStrategy.LUCENE) {
         @Override
         public TokenStream create(TokenStream tokenStream, Version version) {
-            if (version.luceneVersion.onOrAfter(org.apache.lucene.util.Version.LUCENE_4_4_0)) {
-                return new EdgeNGramTokenFilter(tokenStream, EdgeNGramTokenFilter.DEFAULT_MIN_GRAM_SIZE, EdgeNGramTokenFilter.DEFAULT_MAX_GRAM_SIZE);
-            } else {
-                @SuppressWarnings("deprecation")
-                final TokenStream filter = new Lucene43EdgeNGramTokenFilter(tokenStream, EdgeNGramTokenFilter.DEFAULT_MIN_GRAM_SIZE, EdgeNGramTokenFilter.DEFAULT_MAX_GRAM_SIZE);
-                return filter;
-            }
+            return new EdgeNGramTokenFilter(tokenStream, EdgeNGramTokenFilter.DEFAULT_MIN_GRAM_SIZE, EdgeNGramTokenFilter.DEFAULT_MAX_GRAM_SIZE);
         }
     },
 
diff --git a/core/src/main/java/org/elasticsearch/indices/analysis/PreBuiltTokenizers.java b/core/src/main/java/org/elasticsearch/indices/analysis/PreBuiltTokenizers.java
index 7b15633..c6a8255 100644
--- a/core/src/main/java/org/elasticsearch/indices/analysis/PreBuiltTokenizers.java
+++ b/core/src/main/java/org/elasticsearch/indices/analysis/PreBuiltTokenizers.java
@@ -24,16 +24,12 @@ import org.apache.lucene.analysis.core.LetterTokenizer;
 import org.apache.lucene.analysis.core.LowerCaseTokenizer;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.ngram.EdgeNGramTokenizer;
-import org.apache.lucene.analysis.ngram.Lucene43EdgeNGramTokenizer;
-import org.apache.lucene.analysis.ngram.Lucene43NGramTokenizer;
 import org.apache.lucene.analysis.ngram.NGramTokenizer;
 import org.apache.lucene.analysis.path.PathHierarchyTokenizer;
 import org.apache.lucene.analysis.pattern.PatternTokenizer;
 import org.apache.lucene.analysis.standard.ClassicTokenizer;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.standard.UAX29URLEmailTokenizer;
-import org.apache.lucene.analysis.standard.std40.StandardTokenizer40;
-import org.apache.lucene.analysis.standard.std40.UAX29URLEmailTokenizer40;
 import org.apache.lucene.analysis.th.ThaiTokenizer;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.regex.Regex;
@@ -50,11 +46,7 @@ public enum PreBuiltTokenizers {
     STANDARD(CachingStrategy.LUCENE) {
         @Override
         protected Tokenizer create(Version version) {
-            if (version.luceneVersion.onOrAfter(org.apache.lucene.util.Version.LUCENE_4_7_0)) {
-                return new StandardTokenizer();
-            } else {
-                return new StandardTokenizer40();
-            }
+            return new StandardTokenizer();
         }
     },
 
@@ -68,11 +60,7 @@ public enum PreBuiltTokenizers {
     UAX_URL_EMAIL(CachingStrategy.LUCENE) {
         @Override
         protected Tokenizer create(Version version) {
-            if (version.luceneVersion.onOrAfter(org.apache.lucene.util.Version.LUCENE_4_7_0)) {
-                return new UAX29URLEmailTokenizer();
-            } else {
-                return new UAX29URLEmailTokenizer40();
-            }
+            return new UAX29URLEmailTokenizer();
         }
     },
 
@@ -114,28 +102,14 @@ public enum PreBuiltTokenizers {
     NGRAM(CachingStrategy.LUCENE) {
         @Override
         protected Tokenizer create(Version version) {
-            // see NGramTokenizerFactory for an explanation of this logic:
-            // 4.4 patch was used before 4.4 was released
-            if (version.onOrAfter(org.elasticsearch.Version.V_0_90_2) &&
-                  version.luceneVersion.onOrAfter(org.apache.lucene.util.Version.LUCENE_4_3)) {
-                return new NGramTokenizer();
-            } else {
-                return new Lucene43NGramTokenizer();
-            }
+            return new NGramTokenizer();
         }
     },
 
     EDGE_NGRAM(CachingStrategy.LUCENE) {
         @Override
         protected Tokenizer create(Version version) {
-            // see EdgeNGramTokenizerFactory for an explanation of this logic:
-            // 4.4 patch was used before 4.4 was released
-            if (version.onOrAfter(org.elasticsearch.Version.V_0_90_2) &&
-                  version.luceneVersion.onOrAfter(org.apache.lucene.util.Version.LUCENE_4_3)) {
-                return new EdgeNGramTokenizer(EdgeNGramTokenizer.DEFAULT_MIN_GRAM_SIZE, EdgeNGramTokenizer.DEFAULT_MAX_GRAM_SIZE);
-            } else {
-                return new Lucene43EdgeNGramTokenizer(EdgeNGramTokenizer.DEFAULT_MIN_GRAM_SIZE, EdgeNGramTokenizer.DEFAULT_MAX_GRAM_SIZE);
-            }
+            return new EdgeNGramTokenizer(EdgeNGramTokenizer.DEFAULT_MIN_GRAM_SIZE, EdgeNGramTokenizer.DEFAULT_MAX_GRAM_SIZE);
         }
     },
 
diff --git a/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java b/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
index 8a21389..9357de7 100644
--- a/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
+++ b/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
@@ -63,7 +63,6 @@ import org.elasticsearch.index.shard.ShardNotFoundException;
 import org.elasticsearch.index.snapshots.IndexShardRepository;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.indices.flush.SyncedFlushService;
-import org.elasticsearch.indices.memory.IndexingMemoryController;
 import org.elasticsearch.indices.recovery.RecoveryFailedException;
 import org.elasticsearch.indices.recovery.RecoverySource;
 import org.elasticsearch.indices.recovery.RecoveryState;
@@ -130,9 +129,9 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
                                       NodeMappingRefreshAction nodeMappingRefreshAction,
                                       RepositoriesService repositoriesService, RestoreService restoreService,
                                       SearchService searchService, SyncedFlushService syncedFlushService,
-                                      RecoverySource recoverySource, NodeServicesProvider nodeServicesProvider, IndexingMemoryController indexingMemoryController) {
+                                      RecoverySource recoverySource, NodeServicesProvider nodeServicesProvider) {
         super(settings);
-        this.buildInIndexListener = Arrays.asList(recoverySource, recoveryTarget, searchService, syncedFlushService, indexingMemoryController);
+        this.buildInIndexListener = Arrays.asList(recoverySource, recoveryTarget, searchService, syncedFlushService);
         this.indicesService = indicesService;
         this.clusterService = clusterService;
         this.threadPool = threadPool;
diff --git a/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java b/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java
deleted file mode 100644
index a72c115..0000000
--- a/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java
+++ /dev/null
@@ -1,254 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.indices.memory;
-
-import org.elasticsearch.common.component.AbstractLifecycleComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.util.concurrent.FutureUtils;
-import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.engine.EngineClosedException;
-import org.elasticsearch.index.engine.FlushNotAllowedEngineException;
-import org.elasticsearch.index.shard.IndexEventListener;
-import org.elasticsearch.index.shard.IndexShard;
-import org.elasticsearch.index.shard.IndexShardState;
-import org.elasticsearch.indices.IndicesService;
-import org.elasticsearch.monitor.jvm.JvmInfo;
-import org.elasticsearch.threadpool.ThreadPool;
-
-import java.util.ArrayList;
-import java.util.EnumSet;
-import java.util.List;
-import java.util.concurrent.ScheduledFuture;
-
-public class IndexingMemoryController extends AbstractLifecycleComponent<IndexingMemoryController> implements IndexEventListener {
-
-    /** How much heap (% or bytes) we will share across all actively indexing shards on this node (default: 10%). */
-    public static final String INDEX_BUFFER_SIZE_SETTING = "indices.memory.index_buffer_size";
-
-    /** Only applies when <code>indices.memory.index_buffer_size</code> is a %, to set a floor on the actual size in bytes (default: 48 MB). */
-    public static final String MIN_INDEX_BUFFER_SIZE_SETTING = "indices.memory.min_index_buffer_size";
-
-    /** Only applies when <code>indices.memory.index_buffer_size</code> is a %, to set a ceiling on the actual size in bytes (default: not set). */
-    public static final String MAX_INDEX_BUFFER_SIZE_SETTING = "indices.memory.max_index_buffer_size";
-
-    /** Sets a floor on the per-shard index buffer size (default: 4 MB). */
-    public static final String MIN_SHARD_INDEX_BUFFER_SIZE_SETTING = "indices.memory.min_shard_index_buffer_size";
-
-    /** Sets a ceiling on the per-shard index buffer size (default: 512 MB). */
-    public static final String MAX_SHARD_INDEX_BUFFER_SIZE_SETTING = "indices.memory.max_shard_index_buffer_size";
-
-    /** Sets a floor on the per-shard translog buffer size (default: 2 KB). */
-    public static final String MIN_SHARD_TRANSLOG_BUFFER_SIZE_SETTING = "indices.memory.min_shard_translog_buffer_size";
-
-    /** Sets a ceiling on the per-shard translog buffer size (default: 64 KB). */
-    public static final String MAX_SHARD_TRANSLOG_BUFFER_SIZE_SETTING = "indices.memory.max_shard_translog_buffer_size";
-
-    /** How frequently we check shards to find inactive ones (default: 30 seconds). */
-    public static final String SHARD_INACTIVE_INTERVAL_TIME_SETTING = "indices.memory.interval";
-
-    /** Once a shard becomes inactive, we reduce the {@code IndexWriter} buffer to this value (500 KB) to let active shards use the heap instead. */
-    public static final ByteSizeValue INACTIVE_SHARD_INDEXING_BUFFER = ByteSizeValue.parseBytesSizeValue("500kb", "INACTIVE_SHARD_INDEXING_BUFFER");
-
-    /** Once a shard becomes inactive, we reduce the {@code Translog} buffer to this value (1 KB) to let active shards use the heap instead. */
-    public static final ByteSizeValue INACTIVE_SHARD_TRANSLOG_BUFFER = ByteSizeValue.parseBytesSizeValue("1kb", "INACTIVE_SHARD_TRANSLOG_BUFFER");
-
-    private final ThreadPool threadPool;
-    private final IndicesService indicesService;
-
-    private final ByteSizeValue indexingBuffer;
-    private final ByteSizeValue minShardIndexBufferSize;
-    private final ByteSizeValue maxShardIndexBufferSize;
-    private final TimeValue interval;
-
-    private volatile ScheduledFuture scheduler;
-
-    private static final EnumSet<IndexShardState> CAN_UPDATE_INDEX_BUFFER_STATES = EnumSet.of(
-            IndexShardState.RECOVERING, IndexShardState.POST_RECOVERY, IndexShardState.STARTED, IndexShardState.RELOCATED);
-
-    private final ShardsIndicesStatusChecker statusChecker;
-
-    @Inject
-    public IndexingMemoryController(Settings settings, ThreadPool threadPool, IndicesService indicesService) {
-        this(settings, threadPool, indicesService, JvmInfo.jvmInfo().getMem().getHeapMax().bytes());
-    }
-
-    // for testing
-    protected IndexingMemoryController(Settings settings, ThreadPool threadPool, IndicesService indicesService, long jvmMemoryInBytes) {
-        super(settings);
-        this.threadPool = threadPool;
-        this.indicesService = indicesService;
-
-        ByteSizeValue indexingBuffer;
-        String indexingBufferSetting = this.settings.get(INDEX_BUFFER_SIZE_SETTING, "10%");
-        if (indexingBufferSetting.endsWith("%")) {
-            double percent = Double.parseDouble(indexingBufferSetting.substring(0, indexingBufferSetting.length() - 1));
-            indexingBuffer = new ByteSizeValue((long) (((double) jvmMemoryInBytes) * (percent / 100)));
-            ByteSizeValue minIndexingBuffer = this.settings.getAsBytesSize(MIN_INDEX_BUFFER_SIZE_SETTING, new ByteSizeValue(48, ByteSizeUnit.MB));
-            ByteSizeValue maxIndexingBuffer = this.settings.getAsBytesSize(MAX_INDEX_BUFFER_SIZE_SETTING, null);
-
-            if (indexingBuffer.bytes() < minIndexingBuffer.bytes()) {
-                indexingBuffer = minIndexingBuffer;
-            }
-            if (maxIndexingBuffer != null && indexingBuffer.bytes() > maxIndexingBuffer.bytes()) {
-                indexingBuffer = maxIndexingBuffer;
-            }
-        } else {
-            indexingBuffer = ByteSizeValue.parseBytesSizeValue(indexingBufferSetting, INDEX_BUFFER_SIZE_SETTING);
-        }
-        this.indexingBuffer = indexingBuffer;
-        this.minShardIndexBufferSize = this.settings.getAsBytesSize(MIN_SHARD_INDEX_BUFFER_SIZE_SETTING, new ByteSizeValue(4, ByteSizeUnit.MB));
-        // LUCENE MONITOR: Based on this thread, currently (based on Mike), having a large buffer does not make a lot of sense: https://issues.apache.org/jira/browse/LUCENE-2324?focusedCommentId=13005155&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13005155
-        this.maxShardIndexBufferSize = this.settings.getAsBytesSize(MAX_SHARD_INDEX_BUFFER_SIZE_SETTING, new ByteSizeValue(512, ByteSizeUnit.MB));
-
-        // we need to have this relatively small to move a shard from inactive to active fast (enough)
-        this.interval = this.settings.getAsTime(SHARD_INACTIVE_INTERVAL_TIME_SETTING, TimeValue.timeValueSeconds(30));
-
-        this.statusChecker = new ShardsIndicesStatusChecker();
-
-        logger.debug("using indexing buffer size [{}], with {} [{}], {} [{}], {} [{}]",
-                this.indexingBuffer,
-                MIN_SHARD_INDEX_BUFFER_SIZE_SETTING, this.minShardIndexBufferSize,
-                MAX_SHARD_INDEX_BUFFER_SIZE_SETTING, this.maxShardIndexBufferSize,
-                SHARD_INACTIVE_INTERVAL_TIME_SETTING, this.interval);
-    }
-
-    @Override
-    protected void doStart() {
-        // it's fine to run it on the scheduler thread, no busy work
-        this.scheduler = threadPool.scheduleWithFixedDelay(statusChecker, interval);
-    }
-
-    @Override
-    protected void doStop() {
-        FutureUtils.cancel(scheduler);
-        scheduler = null;
-    }
-
-    @Override
-    protected void doClose() {
-    }
-
-    /**
-     * returns the current budget for the total amount of indexing buffers of
-     * active shards on this node
-     */
-    public ByteSizeValue indexingBufferSize() {
-        return indexingBuffer;
-    }
-
-    protected List<IndexShard> availableShards() {
-        List<IndexShard> availableShards = new ArrayList<>();
-
-        for (IndexService indexService : indicesService) {
-            for (IndexShard shard : indexService) {
-                if (shardAvailable(shard)) {
-                    availableShards.add(shard);
-                }
-            }
-        }
-        return availableShards;
-    }
-
-    /** returns true if shard exists and is availabe for updates */
-    protected boolean shardAvailable(IndexShard shard) {
-        // shadow replica doesn't have an indexing buffer
-        return shard.canIndex() && CAN_UPDATE_INDEX_BUFFER_STATES.contains(shard.state());
-    }
-
-    /** set new indexing and translog buffers on this shard.  this may cause the shard to refresh to free up heap. */
-    protected void updateShardBuffers(IndexShard shard, ByteSizeValue shardIndexingBufferSize) {
-        try {
-            shard.updateBufferSize(shardIndexingBufferSize);
-        } catch (EngineClosedException | FlushNotAllowedEngineException e) {
-            // ignore
-        } catch (Exception e) {
-            logger.warn("failed to set shard {} index buffer to [{}]", e, shard.shardId(), shardIndexingBufferSize);
-        }
-    }
-
-    /** check if any shards active status changed, now. */
-    public void forceCheck() {
-        statusChecker.run();
-    }
-
-    class ShardsIndicesStatusChecker implements Runnable {
-        @Override
-        public synchronized void run() {
-            List<IndexShard> availableShards = availableShards();
-            List<IndexShard> activeShards = new ArrayList<>();
-            for (IndexShard shard : availableShards) {
-                if (!checkIdle(shard)) {
-                    activeShards.add(shard);
-                }
-            }
-            int activeShardCount = activeShards.size();
-
-            // TODO: we could be smarter here by taking into account how RAM the IndexWriter on each shard
-            // is actually using (using IW.ramBytesUsed), so that small indices (e.g. Marvel) would not
-            // get the same indexing buffer as large indices.  But it quickly gets tricky...
-            if (activeShardCount == 0) {
-                return;
-            }
-
-            ByteSizeValue shardIndexingBufferSize = new ByteSizeValue(indexingBuffer.bytes() / activeShardCount);
-            if (shardIndexingBufferSize.bytes() < minShardIndexBufferSize.bytes()) {
-                shardIndexingBufferSize = minShardIndexBufferSize;
-            }
-            if (shardIndexingBufferSize.bytes() > maxShardIndexBufferSize.bytes()) {
-                shardIndexingBufferSize = maxShardIndexBufferSize;
-            }
-
-            logger.debug("recalculating shard indexing buffer, total is [{}] with [{}] active shards, each shard set to indexing=[{}]", indexingBuffer, activeShardCount, shardIndexingBufferSize);
-
-            for (IndexShard shard : activeShards) {
-                updateShardBuffers(shard, shardIndexingBufferSize);
-            }
-        }
-    }
-
-    protected long currentTimeInNanos() {
-        return System.nanoTime();
-    }
-
-    /**
-     * ask this shard to check now whether it is inactive, and reduces its indexing and translog buffers if so.
-     * return false if the shard is not idle, otherwise true
-     */
-    protected boolean checkIdle(IndexShard shard) {
-        try {
-            return shard.checkIdle();
-        } catch (EngineClosedException | FlushNotAllowedEngineException e) {
-            logger.trace("ignore [{}] while marking shard {} as inactive", e.getClass().getSimpleName(), shard.shardId());
-            return true;
-        }
-    }
-
-    @Override
-    public void onShardActive(IndexShard indexShard) {
-        // At least one shard used to be inactive ie. a new write operation just showed up.
-        // We try to fix the shards indexing buffer immediately. We could do this async instead, but cost should
-        // be low, and it's rare this happens.
-        forceCheck();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/IngestBootstrapper.java b/core/src/main/java/org/elasticsearch/ingest/IngestBootstrapper.java
deleted file mode 100644
index ad062da..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/IngestBootstrapper.java
+++ /dev/null
@@ -1,187 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateAction;
-import org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequest;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.ClusterChangedEvent;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ClusterStateListener;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.routing.IndexRoutingTable;
-import org.elasticsearch.common.component.AbstractLifecycleComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.io.Streams;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;
-import org.elasticsearch.discovery.DiscoverySettings;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.gateway.GatewayService;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-import java.io.IOException;
-import java.io.InputStream;
-
-/**
- * Instantiates and wires all the services that the ingest plugin will be needing.
- * Also the bootstrapper is in charge of starting and stopping the ingest plugin based on the cluster state.
- */
-public class IngestBootstrapper extends AbstractLifecycleComponent implements ClusterStateListener {
-
-    static final String INGEST_INDEX_TEMPLATE_NAME = "ingest-template";
-
-    private final ThreadPool threadPool;
-    private final Environment environment;
-    private final PipelineStore pipelineStore;
-    private final PipelineExecutionService pipelineExecutionService;
-    private final ProcessorsRegistry processorsRegistry;
-
-    @Inject
-    public IngestBootstrapper(Settings settings, ThreadPool threadPool, Environment environment,
-                              ClusterService clusterService, TransportService transportService,
-                              ProcessorsRegistry processorsRegistry) {
-        super(settings);
-        this.threadPool = threadPool;
-        this.environment = environment;
-        this.processorsRegistry = processorsRegistry;
-        this.pipelineStore = new PipelineStore(settings, clusterService, transportService);
-        this.pipelineExecutionService = new PipelineExecutionService(pipelineStore, threadPool);
-
-        boolean isNoTribeNode = settings.getByPrefix("tribe.").getAsMap().isEmpty();
-        if (isNoTribeNode) {
-            clusterService.add(this);
-        }
-    }
-
-    // for testing:
-    IngestBootstrapper(Settings settings, ThreadPool threadPool, ClusterService clusterService,
-                       PipelineStore pipelineStore, PipelineExecutionService pipelineExecutionService) {
-        super(settings);
-        this.threadPool = threadPool;
-        this.environment = null;
-        clusterService.add(this);
-        this.pipelineStore = pipelineStore;
-        this.pipelineExecutionService = pipelineExecutionService;
-        this.processorsRegistry = null;
-    }
-
-    public PipelineStore getPipelineStore() {
-        return pipelineStore;
-    }
-
-    public PipelineExecutionService getPipelineExecutionService() {
-        return pipelineExecutionService;
-    }
-
-    @Inject
-    public void setClient(Client client) {
-        pipelineStore.setClient(client);
-    }
-
-    @Inject
-    public void setScriptService(ScriptService scriptService) {
-        pipelineStore.buildProcessorFactoryRegistry(processorsRegistry, environment, scriptService);
-    }
-
-    @Override
-    public void clusterChanged(ClusterChangedEvent event) {
-        ClusterState state = event.state();
-        if (state.blocks().hasGlobalBlock(GatewayService.STATE_NOT_RECOVERED_BLOCK)) {
-            return;
-        }
-
-        if (pipelineStore.isStarted()) {
-            if (validClusterState(state) == false) {
-                stopPipelineStore("cluster state invalid [" + state + "]");
-            }
-        } else {
-            if (validClusterState(state)) {
-                startPipelineStore(state.metaData());
-            }
-        }
-    }
-
-    boolean validClusterState(ClusterState state) {
-        if (state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_WRITES) ||
-            state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ALL)) {
-            return false;
-        }
-
-        if (state.getMetaData().hasConcreteIndex(PipelineStore.INDEX)) {
-            IndexRoutingTable routingTable = state.getRoutingTable().index(PipelineStore.INDEX);
-            return routingTable.allPrimaryShardsActive();
-        } else {
-            // it will be ready when auto create index kicks in before the first pipeline doc gets added
-            return true;
-        }
-    }
-
-    @Override
-    protected void doStart() {
-    }
-
-    @Override
-    protected void doStop() {
-    }
-
-    @Override
-    protected void doClose() {
-        try {
-            pipelineStore.close();
-        } catch (IOException e) {
-            throw new RuntimeException(e);
-        }
-    }
-
-    void startPipelineStore(MetaData metaData) {
-        try {
-            threadPool.executor(ThreadPool.Names.GENERIC).execute(() -> {
-                try {
-                    pipelineStore.start();
-                } catch (Exception e1) {
-                    logger.warn("pipeline store failed to start, retrying...", e1);
-                    startPipelineStore(metaData);
-                }
-            });
-        } catch (EsRejectedExecutionException e) {
-            logger.debug("async pipeline store start failed", e);
-        }
-    }
-
-    void stopPipelineStore(String reason) {
-        try {
-            threadPool.executor(ThreadPool.Names.GENERIC).execute(() -> {
-                try {
-                    pipelineStore.stop(reason);
-                } catch (Exception e) {
-                    logger.error("pipeline store stop failure", e);
-                }
-            });
-        } catch (EsRejectedExecutionException e) {
-            logger.debug("async pipeline store stop failed", e);
-        }
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/IngestModule.java b/core/src/main/java/org/elasticsearch/ingest/IngestModule.java
deleted file mode 100644
index 9dddbf6..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/IngestModule.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.ingest.processor.AppendProcessor;
-import org.elasticsearch.ingest.processor.ConvertProcessor;
-import org.elasticsearch.ingest.processor.DateProcessor;
-import org.elasticsearch.ingest.processor.FailProcessor;
-import org.elasticsearch.ingest.processor.GsubProcessor;
-import org.elasticsearch.ingest.processor.JoinProcessor;
-import org.elasticsearch.ingest.processor.LowercaseProcessor;
-import org.elasticsearch.ingest.processor.RemoveProcessor;
-import org.elasticsearch.ingest.processor.RenameProcessor;
-import org.elasticsearch.ingest.processor.SetProcessor;
-import org.elasticsearch.ingest.processor.SplitProcessor;
-import org.elasticsearch.ingest.processor.TrimProcessor;
-import org.elasticsearch.ingest.processor.UppercaseProcessor;
-
-import java.util.function.BiFunction;
-
-/**
- * Registry for processor factories
- * @see Processor.Factory
- */
-public class IngestModule extends AbstractModule {
-
-    private final ProcessorsRegistry processorsRegistry;
-
-    public IngestModule() {
-        this.processorsRegistry = new ProcessorsRegistry();
-        registerProcessor(DateProcessor.TYPE, (environment, templateService) -> new DateProcessor.Factory());
-        registerProcessor(SetProcessor.TYPE, (environment, templateService) -> new SetProcessor.Factory(templateService));
-        registerProcessor(AppendProcessor.TYPE, (environment, templateService) -> new AppendProcessor.Factory(templateService));
-        registerProcessor(RenameProcessor.TYPE, (environment, templateService) -> new RenameProcessor.Factory());
-        registerProcessor(RemoveProcessor.TYPE, (environment, templateService) -> new RemoveProcessor.Factory(templateService));
-        registerProcessor(SplitProcessor.TYPE, (environment, templateService) -> new SplitProcessor.Factory());
-        registerProcessor(JoinProcessor.TYPE, (environment, templateService) -> new JoinProcessor.Factory());
-        registerProcessor(UppercaseProcessor.TYPE, (environment, templateService) -> new UppercaseProcessor.Factory());
-        registerProcessor(LowercaseProcessor.TYPE, (environment, templateService) -> new LowercaseProcessor.Factory());
-        registerProcessor(TrimProcessor.TYPE, (environment, templateService) -> new TrimProcessor.Factory());
-        registerProcessor(ConvertProcessor.TYPE, (environment, templateService) -> new ConvertProcessor.Factory());
-        registerProcessor(GsubProcessor.TYPE, (environment, templateService) -> new GsubProcessor.Factory());
-        registerProcessor(FailProcessor.TYPE, (environment, templateService) -> new FailProcessor.Factory(templateService));
-    }
-
-    @Override
-    protected void configure() {
-        bind(ProcessorsRegistry.class).toInstance(processorsRegistry);
-        binder().bind(IngestBootstrapper.class).asEagerSingleton();
-    }
-
-    /**
-     * Adds a processor factory under a specific type name.
-     */
-    public void registerProcessor(String type, BiFunction<Environment, TemplateService, Processor.Factory<?>> processorFactoryProvider) {
-        processorsRegistry.registerProcessor(type, processorFactoryProvider);
-    }
-
-    public static boolean isIngestEnabled(Settings settings) {
-        return settings.getAsBoolean("node.ingest", true);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/InternalTemplateService.java b/core/src/main/java/org/elasticsearch/ingest/InternalTemplateService.java
deleted file mode 100644
index b4b5ce8..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/InternalTemplateService.java
+++ /dev/null
@@ -1,92 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.script.CompiledScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.ScriptService;
-
-import java.util.Collections;
-import java.util.Map;
-
-public class InternalTemplateService implements TemplateService {
-
-    private final ScriptService scriptService;
-
-    InternalTemplateService(ScriptService scriptService) {
-        this.scriptService = scriptService;
-    }
-
-    @Override
-    public Template compile(String template) {
-        int mustacheStart = template.indexOf("{{");
-        int mustacheEnd = template.indexOf("}}");
-        if (mustacheStart != -1 && mustacheEnd != -1 && mustacheStart < mustacheEnd) {
-            Script script = new Script(template, ScriptService.ScriptType.INLINE, "mustache", Collections.emptyMap());
-            CompiledScript compiledScript = scriptService.compile(
-                script,
-                ScriptContext.Standard.INGEST,
-                null /* we can supply null here, because ingest doesn't use indexed scripts */,
-                Collections.emptyMap()
-            );
-            return new Template() {
-                @Override
-                public String execute(Map<String, Object> model) {
-                    ExecutableScript executableScript = scriptService.executable(compiledScript, model);
-                    Object result = executableScript.run();
-                    if (result instanceof BytesReference) {
-                        return ((BytesReference) result).toUtf8();
-                    }
-                    return String.valueOf(result);
-                }
-
-                @Override
-                public String getKey() {
-                    return template;
-                }
-            };
-        } else {
-            return new StringTemplate(template);
-        }
-    }
-
-    class StringTemplate implements Template {
-
-        private final String value;
-
-        public StringTemplate(String value) {
-            this.value = value;
-        }
-
-        @Override
-        public String execute(Map<String, Object> model) {
-            return value;
-        }
-
-        @Override
-        public String getKey() {
-            return value;
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/PipelineDefinition.java b/core/src/main/java/org/elasticsearch/ingest/PipelineDefinition.java
deleted file mode 100644
index 94c584a..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/PipelineDefinition.java
+++ /dev/null
@@ -1,114 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.ingest.core.Pipeline;
-
-import java.io.IOException;
-
-public class PipelineDefinition implements Writeable<PipelineDefinition>, ToXContent {
-
-    private static final PipelineDefinition PROTOTYPE = new PipelineDefinition((String) null, -1, null);
-
-    public static PipelineDefinition readPipelineDefinitionFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    private final String id;
-    private final long version;
-    private final BytesReference source;
-
-    private final Pipeline pipeline;
-
-    PipelineDefinition(Pipeline pipeline, long version, BytesReference source) {
-        this.id = pipeline.getId();
-        this.version = version;
-        this.source = source;
-        this.pipeline = pipeline;
-    }
-
-    PipelineDefinition(String id, long version, BytesReference source) {
-        this.id = id;
-        this.version = version;
-        this.source = source;
-        this.pipeline = null;
-    }
-
-    public String getId() {
-        return id;
-    }
-
-    public long getVersion() {
-        return version;
-    }
-
-    public BytesReference getSource() {
-        return source;
-    }
-
-    Pipeline getPipeline() {
-        return pipeline;
-    }
-
-    @Override
-    public boolean equals(Object o) {
-        if (this == o) return true;
-        if (o == null || getClass() != o.getClass()) return false;
-
-        PipelineDefinition holder = (PipelineDefinition) o;
-        return source.equals(holder.source);
-    }
-
-    @Override
-    public int hashCode() {
-        return source.hashCode();
-    }
-
-    @Override
-    public PipelineDefinition readFrom(StreamInput in) throws IOException {
-        String id = in.readString();
-        long version = in.readLong();
-        BytesReference source = in.readBytesReference();
-        return new PipelineDefinition(id, version, source);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(id);
-        out.writeLong(version);
-        out.writeBytesReference(source);
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(id);
-        XContentHelper.writeRawField("_source", source, builder, params);
-        builder.field("_version", version);
-        builder.endObject();
-        return builder;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/PipelineExecutionService.java b/core/src/main/java/org/elasticsearch/ingest/PipelineExecutionService.java
deleted file mode 100644
index 5553374..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/PipelineExecutionService.java
+++ /dev/null
@@ -1,108 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.collect.Tuple;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.threadpool.ThreadPool;
-
-import java.util.Map;
-import java.util.function.Consumer;
-
-public class PipelineExecutionService {
-
-    private final PipelineStore store;
-    private final ThreadPool threadPool;
-
-    public PipelineExecutionService(PipelineStore store, ThreadPool threadPool) {
-        this.store = store;
-        this.threadPool = threadPool;
-    }
-
-    public void execute(IndexRequest request, Consumer<Throwable> failureHandler, Consumer<Boolean> completionHandler) {
-        Pipeline pipeline = getPipeline(request.pipeline());
-        threadPool.executor(ThreadPool.Names.INGEST).execute(() -> {
-            try {
-                innerExecute(request, pipeline);
-                completionHandler.accept(true);
-            } catch (Exception e) {
-                failureHandler.accept(e);
-            }
-        });
-    }
-
-    public void execute(Iterable<ActionRequest> actionRequests,
-                        Consumer<Tuple<IndexRequest, Throwable>> itemFailureHandler, Consumer<Boolean> completionHandler) {
-        threadPool.executor(ThreadPool.Names.INGEST).execute(() -> {
-            for (ActionRequest actionRequest : actionRequests) {
-                if ((actionRequest instanceof IndexRequest)) {
-                    IndexRequest indexRequest = (IndexRequest) actionRequest;
-                    if (Strings.hasText(indexRequest.pipeline())) {
-                        try {
-                            innerExecute(indexRequest, getPipeline(indexRequest.pipeline()));
-                            //this shouldn't be needed here but we do it for consistency with index api which requires it to prevent double execution
-                            indexRequest.pipeline(null);
-                        } catch (Throwable e) {
-                            itemFailureHandler.accept(new Tuple<>(indexRequest, e));
-                        }
-                    }
-                }
-            }
-            completionHandler.accept(true);
-        });
-    }
-
-    private void innerExecute(IndexRequest indexRequest, Pipeline pipeline) throws Exception {
-        String index = indexRequest.index();
-        String type = indexRequest.type();
-        String id = indexRequest.id();
-        String routing = indexRequest.routing();
-        String parent = indexRequest.parent();
-        String timestamp = indexRequest.timestamp();
-        String ttl = indexRequest.ttl() == null ? null : indexRequest.ttl().toString();
-        Map<String, Object> sourceAsMap = indexRequest.sourceAsMap();
-        IngestDocument ingestDocument = new IngestDocument(index, type, id, routing, parent, timestamp, ttl, sourceAsMap);
-        pipeline.execute(ingestDocument);
-
-        Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
-        //it's fine to set all metadata fields all the time, as ingest document holds their starting values
-        //before ingestion, which might also get modified during ingestion.
-        indexRequest.index(metadataMap.get(IngestDocument.MetaData.INDEX));
-        indexRequest.type(metadataMap.get(IngestDocument.MetaData.TYPE));
-        indexRequest.id(metadataMap.get(IngestDocument.MetaData.ID));
-        indexRequest.routing(metadataMap.get(IngestDocument.MetaData.ROUTING));
-        indexRequest.parent(metadataMap.get(IngestDocument.MetaData.PARENT));
-        indexRequest.timestamp(metadataMap.get(IngestDocument.MetaData.TIMESTAMP));
-        indexRequest.ttl(metadataMap.get(IngestDocument.MetaData.TTL));
-        indexRequest.source(ingestDocument.getSourceAndMetadata());
-    }
-
-    private Pipeline getPipeline(String pipelineId) {
-        Pipeline pipeline = store.get(pipelineId);
-        if (pipeline == null) {
-            throw new IllegalArgumentException("pipeline with id [" + pipelineId + "] does not exist");
-        }
-        return pipeline;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/PipelineStore.java b/core/src/main/java/org/elasticsearch/ingest/PipelineStore.java
deleted file mode 100644
index 785eb58..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/PipelineStore.java
+++ /dev/null
@@ -1,452 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.admin.indices.create.CreateIndexRequest;
-import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;
-import org.elasticsearch.action.delete.DeleteRequest;
-import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.get.GetRequest;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.action.search.SearchRequest;
-import org.elasticsearch.action.support.IndicesOptions;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.Booleans;
-import org.elasticsearch.common.SearchScrollIterator;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.regex.Regex;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.common.xcontent.support.XContentMapValues;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.index.IndexNotFoundException;
-import org.elasticsearch.action.ingest.DeletePipelineRequest;
-import org.elasticsearch.action.ingest.PutPipelineRequest;
-import org.elasticsearch.action.ingest.ReloadPipelinesAction;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.search.SearchHit;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.transport.TransportService;
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.function.BiFunction;
-
-public class PipelineStore extends AbstractComponent implements Closeable {
-
-    public final static String INDEX = ".ingest";
-    public final static String TYPE = "pipeline";
-
-    final static Settings INGEST_INDEX_SETTING = Settings.builder()
-        .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
-        .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1)
-        .put("index.mapper.dynamic", false)
-        .build();
-
-    final static String PIPELINE_MAPPING;
-
-    static {
-        try {
-            PIPELINE_MAPPING = XContentFactory.jsonBuilder().startObject()
-                .field("dynamic", "strict")
-                .startObject("_all")
-                    .field("enabled", false)
-                .endObject()
-                .startObject("properties")
-                    .startObject("processors")
-                        .field("type", "object")
-                        .field("enabled", false)
-                        .field("dynamic", "true")
-                    .endObject()
-                    .startObject("on_failure")
-                        .field("type", "object")
-                        .field("enabled", false)
-                        .field("dynamic", "true")
-                    .endObject()
-                    .startObject("description")
-                        .field("type", "string")
-                    .endObject()
-                .endObject()
-                .endObject().string();
-        } catch (IOException e) {
-            throw new RuntimeException(e);
-        }
-    }
-
-    private Client client;
-    private final TimeValue scrollTimeout;
-    private final ClusterService clusterService;
-    private final ReloadPipelinesAction reloadPipelinesAction;
-    private final Pipeline.Factory factory = new Pipeline.Factory();
-    private Map<String, Processor.Factory> processorFactoryRegistry;
-
-    private volatile boolean started = false;
-    private volatile Map<String, PipelineDefinition> pipelines = new HashMap<>();
-
-    public PipelineStore(Settings settings, ClusterService clusterService, TransportService transportService) {
-        super(settings);
-        this.clusterService = clusterService;
-        this.scrollTimeout = settings.getAsTime("ingest.pipeline.store.scroll.timeout", TimeValue.timeValueSeconds(30));
-        this.reloadPipelinesAction = new ReloadPipelinesAction(settings, this, clusterService, transportService);
-    }
-
-    public void setClient(Client client) {
-        this.client = client;
-    }
-
-    public void buildProcessorFactoryRegistry(ProcessorsRegistry processorsRegistry, Environment environment, ScriptService scriptService) {
-        Map<String, Processor.Factory> processorFactories = new HashMap<>();
-        TemplateService templateService = new InternalTemplateService(scriptService);
-        for (Map.Entry<String, BiFunction<Environment, TemplateService, Processor.Factory<?>>> entry : processorsRegistry.entrySet()) {
-            Processor.Factory processorFactory = entry.getValue().apply(environment, templateService);
-            processorFactories.put(entry.getKey(), processorFactory);
-        }
-        this.processorFactoryRegistry = Collections.unmodifiableMap(processorFactories);
-    }
-
-    @Override
-    public void close() throws IOException {
-        stop("closing");
-        // TODO: When org.elasticsearch.node.Node can close Closable instances we should try to remove this code,
-        // since any wired closable should be able to close itself
-        List<Closeable> closeables = new ArrayList<>();
-        for (Processor.Factory factory : processorFactoryRegistry.values()) {
-            if (factory instanceof Closeable) {
-                closeables.add((Closeable) factory);
-            }
-        }
-        IOUtils.close(closeables);
-    }
-
-    /**
-     * Deletes the pipeline specified by id in the request.
-     */
-    public void delete(DeletePipelineRequest request, ActionListener<DeleteResponse> listener) {
-        ensureReady();
-
-        DeleteRequest deleteRequest = new DeleteRequest(request);
-        deleteRequest.index(PipelineStore.INDEX);
-        deleteRequest.type(PipelineStore.TYPE);
-        deleteRequest.id(request.id());
-        deleteRequest.refresh(true);
-        client.delete(deleteRequest, handleWriteResponseAndReloadPipelines(listener));
-    }
-
-    /**
-     * Stores the specified pipeline definition in the request.
-     *
-     * @throws IllegalArgumentException If the pipeline holds incorrect configuration
-     */
-    public void put(PutPipelineRequest request, ActionListener<IndexResponse> listener) throws IllegalArgumentException {
-        ensureReady();
-
-        try {
-            // validates the pipeline and processor configuration:
-            Map<String, Object> pipelineConfig = XContentHelper.convertToMap(request.source(), false).v2();
-            constructPipeline(request.id(), pipelineConfig);
-        } catch (Exception e) {
-            throw new IllegalArgumentException("Invalid pipeline configuration", e);
-        }
-
-        ClusterState state = clusterService.state();
-        if (isIngestIndexPresent(state)) {
-            innerPut(request, listener);
-        } else {
-            CreateIndexRequest createIndexRequest = new CreateIndexRequest(INDEX);
-            createIndexRequest.settings(INGEST_INDEX_SETTING);
-            createIndexRequest.mapping(TYPE, PIPELINE_MAPPING);
-            client.admin().indices().create(createIndexRequest, new ActionListener<CreateIndexResponse>() {
-                @Override
-                public void onResponse(CreateIndexResponse createIndexResponse) {
-                    innerPut(request, listener);
-                }
-
-                @Override
-                public void onFailure(Throwable e) {
-                    listener.onFailure(e);
-                }
-            });
-        }
-    }
-
-    private void innerPut(PutPipelineRequest request, ActionListener<IndexResponse> listener) {
-        IndexRequest indexRequest = new IndexRequest(request);
-        indexRequest.index(PipelineStore.INDEX);
-        indexRequest.type(PipelineStore.TYPE);
-        indexRequest.id(request.id());
-        indexRequest.source(request.source());
-        indexRequest.refresh(true);
-        client.index(indexRequest, handleWriteResponseAndReloadPipelines(listener));
-    }
-
-    /**
-     * Returns the pipeline by the specified id
-     */
-    public Pipeline get(String id) {
-        ensureReady();
-
-        PipelineDefinition ref = pipelines.get(id);
-        if (ref != null) {
-            return ref.getPipeline();
-        } else {
-            return null;
-        }
-    }
-
-    public Map<String, Processor.Factory> getProcessorFactoryRegistry() {
-        return processorFactoryRegistry;
-    }
-
-    public List<PipelineDefinition> getReference(String... ids) {
-        ensureReady();
-
-        List<PipelineDefinition> result = new ArrayList<>(ids.length);
-        for (String id : ids) {
-            if (Regex.isSimpleMatchPattern(id)) {
-                for (Map.Entry<String, PipelineDefinition> entry : pipelines.entrySet()) {
-                    if (Regex.simpleMatch(id, entry.getKey())) {
-                        result.add(entry.getValue());
-                    }
-                }
-            } else {
-                PipelineDefinition reference = pipelines.get(id);
-                if (reference != null) {
-                    result.add(reference);
-                }
-            }
-        }
-        return result;
-    }
-
-    public synchronized void updatePipelines() throws Exception {
-        // note: this process isn't fast or smart, but the idea is that there will not be many pipelines,
-        // so for that reason the goal is to keep the update logic simple.
-
-        int changed = 0;
-        Map<String, PipelineDefinition> newPipelines = new HashMap<>(pipelines);
-        for (SearchHit hit : readAllPipelines()) {
-            String pipelineId = hit.getId();
-            BytesReference pipelineSource = hit.getSourceRef();
-            PipelineDefinition current = newPipelines.get(pipelineId);
-            if (current != null) {
-                // If we first read from a primary shard copy and then from a replica copy,
-                // and a write did not yet make it into the replica shard
-                // then the source is not equal but we don't update because the current pipeline is the latest:
-                if (current.getVersion() > hit.getVersion()) {
-                    continue;
-                }
-                if (current.getSource().equals(pipelineSource)) {
-                    continue;
-                }
-            }
-
-            changed++;
-            Pipeline pipeline = constructPipeline(hit.getId(), hit.sourceAsMap());
-            newPipelines.put(pipelineId, new PipelineDefinition(pipeline, hit.getVersion(), pipelineSource));
-        }
-
-        int removed = 0;
-        for (String existingPipelineId : pipelines.keySet()) {
-            if (pipelineExists(existingPipelineId) == false) {
-                newPipelines.remove(existingPipelineId);
-                removed++;
-            }
-        }
-
-        if (changed != 0 || removed != 0) {
-            logger.debug("adding or updating [{}] pipelines and [{}] pipelines removed", changed, removed);
-            pipelines = newPipelines;
-        } else {
-            logger.debug("no pipelines changes detected");
-        }
-    }
-
-    private Pipeline constructPipeline(String id, Map<String, Object> config) throws Exception {
-        return factory.create(id, config, processorFactoryRegistry);
-    }
-
-    boolean pipelineExists(String pipelineId) {
-        GetRequest request = new GetRequest(PipelineStore.INDEX, PipelineStore.TYPE, pipelineId);
-        try {
-            GetResponse response = client.get(request).actionGet();
-            return response.isExists();
-        } catch (IndexNotFoundException e) {
-            // the ingest index doesn't exist, so the pipeline doesn't either:
-            return false;
-        }
-    }
-
-    /**
-     * @param clusterState The cluster just to check whether the ingest index exists and the state of the ingest index
-     * @throws IllegalStateException If the ingest template exists, but is in an invalid state
-     * @return <code>true</code> when the ingest index exists and has the expected settings and mappings or returns
-     * <code>false</code> when the ingest index doesn't exists and needs to be created.
-     */
-    boolean isIngestIndexPresent(ClusterState clusterState) throws IllegalStateException {
-        if (clusterState.getMetaData().hasIndex(INDEX)) {
-            IndexMetaData indexMetaData = clusterState.getMetaData().index(INDEX);
-            Settings indexSettings = indexMetaData.getSettings();
-            int numberOfShards = indexSettings.getAsInt(IndexMetaData.SETTING_NUMBER_OF_SHARDS, -1);
-            if (numberOfShards != 1) {
-                throw new IllegalStateException("illegal ingest index setting, [" + IndexMetaData.SETTING_NUMBER_OF_SHARDS + "] setting is [" + numberOfShards + "] while [1] is expected");
-            }
-            int numberOfReplicas = indexSettings.getAsInt(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, -1);
-            if (numberOfReplicas != 1) {
-                throw new IllegalStateException("illegal ingest index setting, [" + IndexMetaData.SETTING_NUMBER_OF_REPLICAS + "] setting is [" + numberOfReplicas + "] while [1] is expected");
-            }
-            boolean dynamicMappings = indexSettings.getAsBoolean("index.mapper.dynamic", true);
-            if (dynamicMappings != false) {
-                throw new IllegalStateException("illegal ingest index setting, [index.mapper.dynamic] setting is [" + dynamicMappings + "] while [false] is expected");
-            }
-
-            if (indexMetaData.getMappings().size() != 1 && indexMetaData.getMappings().containsKey(TYPE) == false) {
-                throw new IllegalStateException("illegal ingest mappings, only [" + TYPE + "] mapping is allowed to exist in the " + INDEX +" index");
-            }
-
-            try {
-                Map<String, Object> pipelineMapping = indexMetaData.getMappings().get(TYPE).getSourceAsMap();
-                String dynamicMapping = (String) XContentMapValues.extractValue("dynamic", pipelineMapping);
-                if ("strict".equals(dynamicMapping) == false) {
-                    throw new IllegalStateException("illegal ingest mapping, pipeline mapping must be strict");
-                }
-                Boolean allEnabled = (Boolean) XContentMapValues.extractValue("_all.enabled", pipelineMapping);
-                if (Boolean.FALSE.equals(allEnabled) == false) {
-                    throw new IllegalStateException("illegal ingest mapping, _all field is enabled");
-                }
-
-                String processorsType = (String) XContentMapValues.extractValue("properties.processors.type", pipelineMapping);
-                if ("object".equals(processorsType) == false) {
-                    throw new IllegalStateException("illegal ingest mapping, processors field's type is [" + processorsType + "] while [object] is expected");
-                }
-
-                Boolean processorsEnabled = (Boolean) XContentMapValues.extractValue("properties.processors.enabled", pipelineMapping);
-                if (Boolean.FALSE.equals(processorsEnabled) == false) {
-                    throw new IllegalStateException("illegal ingest mapping, processors field enabled option is [true] while [false] is expected");
-                }
-
-                String processorsDynamic = (String) XContentMapValues.extractValue("properties.processors.dynamic", pipelineMapping);
-                if ("true".equals(processorsDynamic) == false) {
-                    throw new IllegalStateException("illegal ingest mapping, processors field dynamic option is [false] while [true] is expected");
-                }
-
-                String onFailureType = (String) XContentMapValues.extractValue("properties.on_failure.type", pipelineMapping);
-                if ("object".equals(onFailureType) == false) {
-                    throw new IllegalStateException("illegal ingest mapping, on_failure field type option is [" + onFailureType + "] while [object] is expected");
-                }
-
-                Boolean onFailureEnabled = (Boolean) XContentMapValues.extractValue("properties.on_failure.enabled", pipelineMapping);
-                if (Boolean.FALSE.equals(onFailureEnabled) == false) {
-                    throw new IllegalStateException("illegal ingest mapping, on_failure field enabled option is [true] while [false] is expected");
-                }
-
-                String onFailureDynamic = (String) XContentMapValues.extractValue("properties.on_failure.dynamic", pipelineMapping);
-                if ("true".equals(onFailureDynamic) == false) {
-                    throw new IllegalStateException("illegal ingest mapping, on_failure field dynamic option is [false] while [true] is expected");
-                }
-            } catch (IOException e) {
-                throw new RuntimeException(e);
-            }
-            return true;
-        } else {
-            return false;
-        }
-    }
-
-
-    synchronized void start() throws Exception {
-        if (started) {
-            logger.debug("Pipeline already started");
-        } else {
-            updatePipelines();
-            started = true;
-            logger.debug("Pipeline store started with [{}] pipelines", pipelines.size());
-        }
-    }
-
-    synchronized void stop(String reason) {
-        if (started) {
-            started = false;
-            pipelines = new HashMap<>();
-            logger.debug("Pipeline store stopped, reason [{}]", reason);
-        } else {
-            logger.debug("Pipeline alreadt stopped");
-        }
-    }
-
-    public boolean isStarted() {
-        return started;
-    }
-
-    private Iterable<SearchHit> readAllPipelines() {
-        // TODO: the search should be replaced with an ingest API when it is available
-        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
-        sourceBuilder.version(true);
-        sourceBuilder.sort("_doc", SortOrder.ASC);
-        SearchRequest searchRequest = new SearchRequest(PipelineStore.INDEX);
-        searchRequest.source(sourceBuilder);
-        searchRequest.indicesOptions(IndicesOptions.lenientExpandOpen());
-        return SearchScrollIterator.createIterator(client, scrollTimeout, searchRequest);
-    }
-
-    private void ensureReady() {
-        if (started == false) {
-            throw new IllegalStateException("pipeline store isn't ready yet");
-        }
-    }
-
-    @SuppressWarnings("unchecked")
-    private <T> ActionListener<T> handleWriteResponseAndReloadPipelines(ActionListener<T> listener) {
-        return new ActionListener<T>() {
-            @Override
-            public void onResponse(T result) {
-                try {
-                    reloadPipelinesAction.reloadPipelinesOnAllNodes(reloadResult -> listener.onResponse(result));
-                } catch (Throwable e) {
-                    listener.onFailure(e);
-                }
-            }
-
-            @Override
-            public void onFailure(Throwable e) {
-                listener.onFailure(e);
-            }
-        };
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/ProcessorsRegistry.java b/core/src/main/java/org/elasticsearch/ingest/ProcessorsRegistry.java
deleted file mode 100644
index 3561d80..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/ProcessorsRegistry.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.core.TemplateService;
-
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Set;
-import java.util.function.BiFunction;
-
-public class ProcessorsRegistry {
-
-    private final Map<String, BiFunction<Environment, TemplateService, Processor.Factory<?>>> processorFactoryProviders = new HashMap<>();
-
-    /**
-     * Adds a processor factory under a specific name.
-     */
-    public void registerProcessor(String name, BiFunction<Environment, TemplateService, Processor.Factory<?>> processorFactoryProvider) {
-        BiFunction<Environment, TemplateService, Processor.Factory<?>> provider = processorFactoryProviders.putIfAbsent(name, processorFactoryProvider);
-        if (provider != null) {
-            throw new IllegalArgumentException("Processor factory already registered for name [" + name + "]");
-        }
-    }
-
-    public Set<Map.Entry<String, BiFunction<Environment, TemplateService, Processor.Factory<?>>>> entrySet() {
-        return processorFactoryProviders.entrySet();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/CompoundProcessor.java b/core/src/main/java/org/elasticsearch/ingest/core/CompoundProcessor.java
deleted file mode 100644
index 28cfc95..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/CompoundProcessor.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-
-package org.elasticsearch.ingest.core;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-import java.util.stream.Collectors;
-
-/**
- * A Processor that executes a list of other "processors". It executes a separate list of
- * "onFailureProcessors" when any of the processors throw an {@link Exception}.
- */
-public class CompoundProcessor implements Processor {
-    static final String ON_FAILURE_MESSAGE_FIELD = "on_failure_message";
-    static final String ON_FAILURE_PROCESSOR_FIELD = "on_failure_processor";
-
-    private final List<Processor> processors;
-    private final List<Processor> onFailureProcessors;
-
-    public CompoundProcessor(Processor... processor) {
-        this(Arrays.asList(processor), Collections.emptyList());
-    }
-
-    public CompoundProcessor(List<Processor> processors, List<Processor> onFailureProcessors) {
-        this.processors = processors;
-        this.onFailureProcessors = onFailureProcessors;
-    }
-
-    public List<Processor> getOnFailureProcessors() {
-        return onFailureProcessors;
-    }
-
-    public List<Processor> getProcessors() {
-        return processors;
-    }
-
-    @Override
-    public String getType() {
-        return "compound[" + processors.stream().map(Processor::getType).collect(Collectors.joining(",")) + "]";
-    }
-
-    @Override
-    public void execute(IngestDocument ingestDocument) throws Exception {
-        for (Processor processor : processors) {
-            try {
-                processor.execute(ingestDocument);
-            } catch (Exception e) {
-                if (onFailureProcessors.isEmpty()) {
-                    throw e;
-                } else {
-                    executeOnFailure(ingestDocument, e, processor.getType());
-                }
-                return;
-            }
-        }
-    }
-
-    void executeOnFailure(IngestDocument ingestDocument, Exception cause, String failedProcessorType) throws Exception {
-        Map<String, String> ingestMetadata = ingestDocument.getIngestMetadata();
-        try {
-            ingestMetadata.put(ON_FAILURE_MESSAGE_FIELD, cause.getMessage());
-            ingestMetadata.put(ON_FAILURE_PROCESSOR_FIELD, failedProcessorType);
-            for (Processor processor : onFailureProcessors) {
-                processor.execute(ingestDocument);
-            }
-        } finally {
-            ingestMetadata.remove(ON_FAILURE_MESSAGE_FIELD);
-            ingestMetadata.remove(ON_FAILURE_PROCESSOR_FIELD);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/ConfigurationUtils.java b/core/src/main/java/org/elasticsearch/ingest/core/ConfigurationUtils.java
deleted file mode 100644
index c620416..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/ConfigurationUtils.java
+++ /dev/null
@@ -1,163 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-import java.util.List;
-import java.util.Map;
-
-public final class ConfigurationUtils {
-
-    private ConfigurationUtils() {
-    }
-
-    /**
-     * Returns and removes the specified optional property from the specified configuration map.
-     *
-     * If the property value isn't of type string a {@link IllegalArgumentException} is thrown.
-     */
-    public static String readOptionalStringProperty(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        return readString(propertyName, value);
-    }
-
-    /**
-     * Returns and removes the specified property from the specified configuration map.
-     *
-     * If the property value isn't of type string an {@link IllegalArgumentException} is thrown.
-     * If the property is missing an {@link IllegalArgumentException} is thrown
-     */
-    public static String readStringProperty(Map<String, Object> configuration, String propertyName) {
-        return readStringProperty(configuration, propertyName, null);
-    }
-
-    /**
-     * Returns and removes the specified property from the specified configuration map.
-     *
-     * If the property value isn't of type string a {@link IllegalArgumentException} is thrown.
-     * If the property is missing and no default value has been specified a {@link IllegalArgumentException} is thrown
-     */
-    public static String readStringProperty(Map<String, Object> configuration, String propertyName, String defaultValue) {
-        Object value = configuration.remove(propertyName);
-        if (value == null && defaultValue != null) {
-            return defaultValue;
-        } else if (value == null) {
-            throw new IllegalArgumentException("required property [" + propertyName + "] is missing");
-        }
-        return readString(propertyName, value);
-    }
-
-    private static String readString(String propertyName, Object value) {
-        if (value == null) {
-            return null;
-        }
-        if (value instanceof String) {
-            return (String) value;
-        }
-        throw new IllegalArgumentException("property [" + propertyName + "] isn't a string, but of type [" + value.getClass().getName() + "]");
-    }
-
-    /**
-     * Returns and removes the specified property of type list from the specified configuration map.
-     *
-     * If the property value isn't of type list an {@link IllegalArgumentException} is thrown.
-     */
-    public static <T> List<T> readOptionalList(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        if (value == null) {
-            return null;
-        }
-        return readList(propertyName, value);
-    }
-
-    /**
-     * Returns and removes the specified property of type list from the specified configuration map.
-     *
-     * If the property value isn't of type list an {@link IllegalArgumentException} is thrown.
-     * If the property is missing an {@link IllegalArgumentException} is thrown
-     */
-    public static <T> List<T> readList(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        if (value == null) {
-            throw new IllegalArgumentException("required property [" + propertyName + "] is missing");
-        }
-
-        return readList(propertyName, value);
-    }
-
-    private static <T> List<T> readList(String propertyName, Object value) {
-        if (value instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<T> stringList = (List<T>) value;
-            return stringList;
-        } else {
-            throw new IllegalArgumentException("property [" + propertyName + "] isn't a list, but of type [" + value.getClass().getName() + "]");
-        }
-    }
-
-    /**
-     * Returns and removes the specified property of type map from the specified configuration map.
-     *
-     * If the property value isn't of type map an {@link IllegalArgumentException} is thrown.
-     * If the property is missing an {@link IllegalArgumentException} is thrown
-     */
-    public static <T> Map<String, T> readMap(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        if (value == null) {
-            throw new IllegalArgumentException("required property [" + propertyName + "] is missing");
-        }
-
-        return readMap(propertyName, value);
-    }
-
-    /**
-     * Returns and removes the specified property of type map from the specified configuration map.
-     *
-     * If the property value isn't of type map an {@link IllegalArgumentException} is thrown.
-     */
-    public static <T> Map<String, T> readOptionalMap(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        if (value == null) {
-            return null;
-        }
-
-        return readMap(propertyName, value);
-    }
-
-    private static <T> Map<String, T> readMap(String propertyName, Object value) {
-        if (value instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, T> map = (Map<String, T>) value;
-            return map;
-        } else {
-            throw new IllegalArgumentException("property [" + propertyName + "] isn't a map, but of type [" + value.getClass().getName() + "]");
-        }
-    }
-
-    /**
-     * Returns and removes the specified property as an {@link Object} from the specified configuration map.
-     */
-    public static Object readObject(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        if (value == null) {
-            throw new IllegalArgumentException("required property [" + propertyName + "] is missing");
-        }
-        return value;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/IngestDocument.java b/core/src/main/java/org/elasticsearch/ingest/core/IngestDocument.java
deleted file mode 100644
index b5c40e1..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/IngestDocument.java
+++ /dev/null
@@ -1,544 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.index.mapper.internal.IdFieldMapper;
-import org.elasticsearch.index.mapper.internal.IndexFieldMapper;
-import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
-import org.elasticsearch.index.mapper.internal.RoutingFieldMapper;
-import org.elasticsearch.index.mapper.internal.SourceFieldMapper;
-import org.elasticsearch.index.mapper.internal.TTLFieldMapper;
-import org.elasticsearch.index.mapper.internal.TimestampFieldMapper;
-import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
-
-import java.text.DateFormat;
-import java.text.SimpleDateFormat;
-import java.util.ArrayList;
-import java.util.Date;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Objects;
-import java.util.TimeZone;
-
-/**
- * Represents a single document being captured before indexing and holds the source and metadata (like id, type and index).
- */
-public final class IngestDocument {
-
-    public final static String INGEST_KEY = "_ingest";
-
-    static final String TIMESTAMP = "timestamp";
-
-    private final Map<String, Object> sourceAndMetadata;
-    private final Map<String, String> ingestMetadata;
-
-    public IngestDocument(String index, String type, String id, String routing, String parent, String timestamp, String ttl, Map<String, Object> source) {
-        this.sourceAndMetadata = new HashMap<>();
-        this.sourceAndMetadata.putAll(source);
-        this.sourceAndMetadata.put(MetaData.INDEX.getFieldName(), index);
-        this.sourceAndMetadata.put(MetaData.TYPE.getFieldName(), type);
-        this.sourceAndMetadata.put(MetaData.ID.getFieldName(), id);
-        if (routing != null) {
-            this.sourceAndMetadata.put(MetaData.ROUTING.getFieldName(), routing);
-        }
-        if (parent != null) {
-            this.sourceAndMetadata.put(MetaData.PARENT.getFieldName(), parent);
-        }
-        if (timestamp != null) {
-            this.sourceAndMetadata.put(MetaData.TIMESTAMP.getFieldName(), timestamp);
-        }
-        if (ttl != null) {
-            this.sourceAndMetadata.put(MetaData.TTL.getFieldName(), ttl);
-        }
-
-        this.ingestMetadata = new HashMap<>();
-        DateFormat df = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSSZZ", Locale.ROOT);
-        df.setTimeZone(TimeZone.getTimeZone("UTC"));
-        this.ingestMetadata.put(TIMESTAMP, df.format(new Date()));
-    }
-
-    /**
-     * Copy constructor that creates a new {@link IngestDocument} which has exactly the same properties as the one provided as argument
-     */
-    public IngestDocument(IngestDocument other) {
-        this(new HashMap<>(other.sourceAndMetadata), new HashMap<>(other.ingestMetadata));
-    }
-
-    /**
-     * Constructor needed for testing that allows to create a new {@link IngestDocument} given the provided elasticsearch metadata,
-     * source and ingest metadata. This is needed because the ingest metadata will be initialized with the current timestamp at
-     * init time, which makes equality comparisons impossible in tests.
-     */
-    public IngestDocument(Map<String, Object> sourceAndMetadata, Map<String, String> ingestMetadata) {
-        this.sourceAndMetadata = sourceAndMetadata;
-        this.ingestMetadata = ingestMetadata;
-    }
-
-    /**
-     * Returns the value contained in the document for the provided path
-     * @param path The path within the document in dot-notation
-     * @param clazz The expected class of the field value
-     * @return the value for the provided path if existing, null otherwise
-     * @throws IllegalArgumentException if the path is null, empty, invalid, if the field doesn't exist
-     * or if the field that is found at the provided path is not of the expected type.
-     */
-    public <T> T getFieldValue(String path, Class<T> clazz) {
-        FieldPath fieldPath = new FieldPath(path);
-        Object context = fieldPath.initialContext;
-        for (String pathElement : fieldPath.pathElements) {
-            context = resolve(pathElement, path, context);
-        }
-        return cast(path, context, clazz);
-    }
-
-    /**
-     * Checks whether the document contains a value for the provided path
-     * @param path The path within the document in dot-notation
-     * @return true if the document contains a value for the field, false otherwise
-     * @throws IllegalArgumentException if the path is null, empty or invalid.
-     */
-    public boolean hasField(String path) {
-        FieldPath fieldPath = new FieldPath(path);
-        Object context = fieldPath.initialContext;
-        for (int i = 0; i < fieldPath.pathElements.length - 1; i++) {
-            String pathElement = fieldPath.pathElements[i];
-            if (context == null) {
-                return false;
-            }
-            if (context instanceof Map) {
-                @SuppressWarnings("unchecked")
-                Map<String, Object> map = (Map<String, Object>) context;
-                context = map.get(pathElement);
-            } else if (context instanceof List) {
-                @SuppressWarnings("unchecked")
-                List<Object> list = (List<Object>) context;
-                try {
-                    int index = Integer.parseInt(pathElement);
-                    if (index < 0 || index >= list.size()) {
-                        return false;
-                    }
-                    context = list.get(index);
-                } catch (NumberFormatException e) {
-                    return false;
-                }
-
-            } else {
-                return false;
-            }
-        }
-
-        String leafKey = fieldPath.pathElements[fieldPath.pathElements.length - 1];
-        if (context instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> map = (Map<String, Object>) context;
-            return map.containsKey(leafKey);
-        }
-        if (context instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> list = (List<Object>) context;
-            try {
-                int index = Integer.parseInt(leafKey);
-                return index >= 0 && index < list.size();
-            } catch (NumberFormatException e) {
-                return false;
-            }
-        }
-        return false;
-    }
-
-    /**
-     * Removes the field identified by the provided path.
-     * @param fieldPathTemplate Resolves to the path with dot-notation within the document
-     * @throws IllegalArgumentException if the path is null, empty, invalid or if the field doesn't exist.
-     */
-    public void removeField(TemplateService.Template fieldPathTemplate) {
-        removeField(renderTemplate(fieldPathTemplate));
-    }
-
-    /**
-     * Removes the field identified by the provided path.
-     * @param path the path of the field to be removed
-     * @throws IllegalArgumentException if the path is null, empty, invalid or if the field doesn't exist.
-     */
-    public void removeField(String path) {
-        FieldPath fieldPath = new FieldPath(path);
-        Object context = fieldPath.initialContext;
-        for (int i = 0; i < fieldPath.pathElements.length - 1; i++) {
-            context = resolve(fieldPath.pathElements[i], path, context);
-        }
-
-        String leafKey = fieldPath.pathElements[fieldPath.pathElements.length - 1];
-        if (context instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> map = (Map<String, Object>) context;
-            if (map.containsKey(leafKey)) {
-                map.remove(leafKey);
-                return;
-            }
-            throw new IllegalArgumentException("field [" + leafKey + "] not present as part of path [" + path + "]");
-        }
-        if (context instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> list = (List<Object>) context;
-            int index;
-            try {
-                index = Integer.parseInt(leafKey);
-            } catch (NumberFormatException e) {
-                throw new IllegalArgumentException("[" + leafKey + "] is not an integer, cannot be used as an index as part of path [" + path + "]", e);
-            }
-            if (index < 0 || index >= list.size()) {
-                throw new IllegalArgumentException("[" + index + "] is out of bounds for array with length [" + list.size() + "] as part of path [" + path + "]");
-            }
-            list.remove(index);
-            return;
-        }
-
-        if (context == null) {
-            throw new IllegalArgumentException("cannot remove [" + leafKey + "] from null as part of path [" + path + "]");
-        }
-        throw new IllegalArgumentException("cannot remove [" + leafKey + "] from object of type [" + context.getClass().getName() + "] as part of path [" + path + "]");
-    }
-
-    private static Object resolve(String pathElement, String fullPath, Object context) {
-        if (context == null) {
-            throw new IllegalArgumentException("cannot resolve [" + pathElement + "] from null as part of path [" + fullPath + "]");
-        }
-        if (context instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> map = (Map<String, Object>) context;
-            if (map.containsKey(pathElement)) {
-                return map.get(pathElement);
-            }
-            throw new IllegalArgumentException("field [" + pathElement + "] not present as part of path [" + fullPath + "]");
-        }
-        if (context instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> list = (List<Object>) context;
-            int index;
-            try {
-                index = Integer.parseInt(pathElement);
-            } catch (NumberFormatException e) {
-                throw new IllegalArgumentException("[" + pathElement + "] is not an integer, cannot be used as an index as part of path [" + fullPath + "]", e);
-            }
-            if (index < 0 || index >= list.size()) {
-                throw new IllegalArgumentException("[" + index + "] is out of bounds for array with length [" + list.size() + "] as part of path [" + fullPath + "]");
-            }
-            return list.get(index);
-        }
-        throw new IllegalArgumentException("cannot resolve [" + pathElement + "] from object of type [" + context.getClass().getName() + "] as part of path [" + fullPath + "]");
-    }
-
-    /**
-     * Appends the provided value to the provided path in the document.
-     * Any non existing path element will be created.
-     * If the path identifies a list, the value will be appended to the existing list.
-     * If the path identifies a scalar, the scalar will be converted to a list and
-     * the provided value will be added to the newly created list.
-     * Supports multiple values too provided in forms of list, in that case all the values will be appeneded to the
-     * existing (or newly created) list.
-     * @param path The path within the document in dot-notation
-     * @param value The value or values to append to the existing ones
-     * @throws IllegalArgumentException if the path is null, empty or invalid.
-     */
-    public void appendFieldValue(String path, Object value) {
-        setFieldValue(path, value, true);
-    }
-
-    /**
-     * Appends the provided value to the provided path in the document.
-     * Any non existing path element will be created.
-     * If the path identifies a list, the value will be appended to the existing list.
-     * If the path identifies a scalar, the scalar will be converted to a list and
-     * the provided value will be added to the newly created list.
-     * Supports multiple values too provided in forms of list, in that case all the values will be appeneded to the
-     * existing (or newly created) list.
-     * @param fieldPathTemplate Resolves to the path with dot-notation within the document
-     * @param valueSource The value source that will produce the value or values to append to the existing ones
-     * @throws IllegalArgumentException if the path is null, empty or invalid.
-     */
-    public void appendFieldValue(TemplateService.Template fieldPathTemplate, ValueSource valueSource) {
-        Map<String, Object> model = createTemplateModel();
-        appendFieldValue(fieldPathTemplate.execute(model), valueSource.copyAndResolve(model));
-    }
-
-    /**
-     * Sets the provided value to the provided path in the document.
-     * Any non existing path element will be created.
-     * If the last item in the path is a list, the value will replace the existing list as a whole.
-     * Use {@link #appendFieldValue(String, Object)} to append values to lists instead.
-     * @param path The path within the document in dot-notation
-     * @param value The value to put in for the path key
-     * @throws IllegalArgumentException if the path is null, empty, invalid or if the value cannot be set to the
-     * item identified by the provided path.
-     */
-    public void setFieldValue(String path, Object value) {
-        setFieldValue(path, value, false);
-    }
-
-    /**
-     * Sets the provided value to the provided path in the document.
-     * Any non existing path element will be created. If the last element is a list,
-     * the value will replace the existing list.
-     * @param fieldPathTemplate Resolves to the path with dot-notation within the document
-     * @param valueSource The value source that will produce the value to put in for the path key
-     * @throws IllegalArgumentException if the path is null, empty, invalid or if the value cannot be set to the
-     * item identified by the provided path.
-     */
-    public void setFieldValue(TemplateService.Template fieldPathTemplate, ValueSource valueSource) {
-        Map<String, Object> model = createTemplateModel();
-        setFieldValue(fieldPathTemplate.execute(model), valueSource.copyAndResolve(model), false);
-    }
-
-    private void setFieldValue(String path, Object value, boolean append) {
-        FieldPath fieldPath = new FieldPath(path);
-        Object context = fieldPath.initialContext;
-        for (int i = 0; i < fieldPath.pathElements.length - 1; i++) {
-            String pathElement = fieldPath.pathElements[i];
-            if (context == null) {
-                throw new IllegalArgumentException("cannot resolve [" + pathElement + "] from null as part of path [" + path + "]");
-            }
-            if (context instanceof Map) {
-                @SuppressWarnings("unchecked")
-                Map<String, Object> map = (Map<String, Object>) context;
-                if (map.containsKey(pathElement)) {
-                    context = map.get(pathElement);
-                } else {
-                    HashMap<Object, Object> newMap = new HashMap<>();
-                    map.put(pathElement, newMap);
-                    context = newMap;
-                }
-            } else if (context instanceof List) {
-                @SuppressWarnings("unchecked")
-                List<Object> list = (List<Object>) context;
-                int index;
-                try {
-                    index = Integer.parseInt(pathElement);
-                } catch (NumberFormatException e) {
-                    throw new IllegalArgumentException("[" + pathElement + "] is not an integer, cannot be used as an index as part of path [" + path + "]", e);
-                }
-                if (index < 0 || index >= list.size()) {
-                    throw new IllegalArgumentException("[" + index + "] is out of bounds for array with length [" + list.size() + "] as part of path [" + path + "]");
-                }
-                context = list.get(index);
-            } else {
-                throw new IllegalArgumentException("cannot resolve [" + pathElement + "] from object of type [" + context.getClass().getName() + "] as part of path [" + path + "]");
-            }
-        }
-
-        String leafKey = fieldPath.pathElements[fieldPath.pathElements.length - 1];
-        if (context == null) {
-            throw new IllegalArgumentException("cannot set [" + leafKey + "] with null parent as part of path [" + path + "]");
-        }
-        if (context instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> map = (Map<String, Object>) context;
-            if (append) {
-                if (map.containsKey(leafKey)) {
-                    Object object = map.get(leafKey);
-                    List<Object> list = appendValues(object, value);
-                    if (list != object) {
-                        map.put(leafKey, list);
-                    }
-                } else {
-                    List<Object> list = new ArrayList<>();
-                    appendValues(list, value);
-                    map.put(leafKey, list);
-                }
-                return;
-            }
-            map.put(leafKey, value);
-        } else if (context instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> list = (List<Object>) context;
-            int index;
-            try {
-                index = Integer.parseInt(leafKey);
-            } catch (NumberFormatException e) {
-                throw new IllegalArgumentException("[" + leafKey + "] is not an integer, cannot be used as an index as part of path [" + path + "]", e);
-            }
-            if (index < 0 || index >= list.size()) {
-                throw new IllegalArgumentException("[" + index + "] is out of bounds for array with length [" + list.size() + "] as part of path [" + path + "]");
-            }
-            if (append) {
-                Object object = list.get(index);
-                List<Object> newList = appendValues(object, value);
-                if (newList != object) {
-                    list.set(index, newList);
-                }
-                return;
-            }
-            list.set(index, value);
-        } else {
-            throw new IllegalArgumentException("cannot set [" + leafKey + "] with parent object of type [" + context.getClass().getName() + "] as part of path [" + path + "]");
-        }
-    }
-
-    @SuppressWarnings("unchecked")
-    private static List<Object> appendValues(Object maybeList, Object value) {
-        List<Object> list;
-        if (maybeList instanceof List) {
-            //maybeList is already a list, we append the provided values to it
-            list = (List<Object>) maybeList;
-        } else {
-            //maybeList is a scalar, we convert it to a list and append the provided values to it
-            list = new ArrayList<>();
-            list.add(maybeList);
-        }
-        appendValues(list, value);
-        return list;
-    }
-
-    private static void appendValues(List<Object> list, Object value) {
-        if (value instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<?> valueList = (List<?>) value;
-            valueList.stream().forEach(list::add);
-        } else {
-            list.add(value);
-        }
-    }
-
-    private static <T> T cast(String path, Object object, Class<T> clazz) {
-        if (object == null) {
-            return null;
-        }
-        if (clazz.isInstance(object)) {
-            return clazz.cast(object);
-        }
-        throw new IllegalArgumentException("field [" + path + "] of type [" + object.getClass().getName() + "] cannot be cast to [" + clazz.getName() + "]");
-    }
-
-    public String renderTemplate(TemplateService.Template template) {
-        return template.execute(createTemplateModel());
-    }
-
-    private Map<String, Object> createTemplateModel() {
-        Map<String, Object> model = new HashMap<>(sourceAndMetadata);
-        model.put(SourceFieldMapper.NAME, sourceAndMetadata);
-        // If there is a field in the source with the name '_ingest' it gets overwritten here,
-        // if access to that field is required then it get accessed via '_source._ingest'
-        model.put(INGEST_KEY, ingestMetadata);
-        return model;
-    }
-
-    /**
-     * one time operation that extracts the metadata fields from the ingest document and returns them.
-     * Metadata fields that used to be accessible as ordinary top level fields will be removed as part of this call.
-     */
-    public Map<MetaData, String> extractMetadata() {
-        Map<MetaData, String> metadataMap = new HashMap<>();
-        for (MetaData metaData : MetaData.values()) {
-            metadataMap.put(metaData, cast(metaData.getFieldName(), sourceAndMetadata.remove(metaData.getFieldName()), String.class));
-        }
-        return metadataMap;
-    }
-
-    /**
-     * Returns the available ingest metadata fields, by default only timestamp, but it is possible to set additional ones.
-     * Use only for reading values, modify them instead using {@link #setFieldValue(String, Object)} and {@link #removeField(String)}
-     */
-    public Map<String, String> getIngestMetadata() {
-        return this.ingestMetadata;
-    }
-
-    /**
-     * Returns the document including its metadata fields, unless {@link #extractMetadata()} has been called, in which case the
-     * metadata fields will not be present anymore. Should be used only for reading.
-     * Modify the document instead using {@link #setFieldValue(String, Object)} and {@link #removeField(String)}
-     */
-    public Map<String, Object> getSourceAndMetadata() {
-        return this.sourceAndMetadata;
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == this) { return true; }
-        if (obj == null || getClass() != obj.getClass()) {
-            return false;
-        }
-
-        IngestDocument other = (IngestDocument) obj;
-        return Objects.equals(sourceAndMetadata, other.sourceAndMetadata) &&
-                Objects.equals(ingestMetadata, other.ingestMetadata);
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(sourceAndMetadata, ingestMetadata);
-    }
-
-    @Override
-    public String toString() {
-        return "IngestDocument{" +
-                " sourceAndMetadata=" + sourceAndMetadata +
-                ", ingestMetadata=" + ingestMetadata +
-                '}';
-    }
-
-    public enum MetaData {
-        INDEX(IndexFieldMapper.NAME),
-        TYPE(TypeFieldMapper.NAME),
-        ID(IdFieldMapper.NAME),
-        ROUTING(RoutingFieldMapper.NAME),
-        PARENT(ParentFieldMapper.NAME),
-        TIMESTAMP(TimestampFieldMapper.NAME),
-        TTL(TTLFieldMapper.NAME);
-
-        private final String fieldName;
-
-        MetaData(String fieldName) {
-            this.fieldName = fieldName;
-        }
-
-        public String getFieldName() {
-            return fieldName;
-        }
-    }
-
-    private class FieldPath {
-        private final String[] pathElements;
-        private final Object initialContext;
-
-        private FieldPath(String path) {
-            if (Strings.isEmpty(path)) {
-                throw new IllegalArgumentException("path cannot be null nor empty");
-            }
-            String newPath;
-            if (path.startsWith(INGEST_KEY + ".")) {
-                initialContext = ingestMetadata;
-                newPath = path.substring(8, path.length());
-            } else {
-                initialContext = sourceAndMetadata;
-                if (path.startsWith(SourceFieldMapper.NAME + ".")) {
-                    newPath = path.substring(8, path.length());
-                } else {
-                    newPath = path;
-                }
-            }
-            this.pathElements = Strings.splitStringToArray(newPath, '.');
-            if (pathElements.length == 0) {
-                throw new IllegalArgumentException("path [" + path + "] is not valid");
-            }
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/Pipeline.java b/core/src/main/java/org/elasticsearch/ingest/core/Pipeline.java
deleted file mode 100644
index 7c3d673..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/Pipeline.java
+++ /dev/null
@@ -1,119 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-
-package org.elasticsearch.ingest.core;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-
-/**
- * A pipeline is a list of {@link Processor} instances grouped under a unique id.
- */
-public final class Pipeline {
-
-    private final String id;
-    private final String description;
-    private final CompoundProcessor compoundProcessor;
-
-    public Pipeline(String id, String description, CompoundProcessor compoundProcessor) {
-        this.id = id;
-        this.description = description;
-        this.compoundProcessor = compoundProcessor;
-    }
-
-    /**
-     * Modifies the data of a document to be indexed based on the processor this pipeline holds
-     */
-    public void execute(IngestDocument ingestDocument) throws Exception {
-        compoundProcessor.execute(ingestDocument);
-    }
-
-    /**
-     * The unique id of this pipeline
-     */
-    public String getId() {
-        return id;
-    }
-
-    /**
-     * An optional description of what this pipeline is doing to the data gets processed by this pipeline.
-     */
-    public String getDescription() {
-        return description;
-    }
-
-    /**
-     * Unmodifiable list containing each processor that operates on the data.
-     */
-    public List<Processor> getProcessors() {
-        return compoundProcessor.getProcessors();
-    }
-
-    /**
-     * Unmodifiable list containing each on_failure processor that operates on the data in case of
-     * exception thrown in pipeline processors
-     */
-    public List<Processor> getOnFailureProcessors() {
-        return compoundProcessor.getOnFailureProcessors();
-    }
-
-    public final static class Factory {
-        private Processor readProcessor(Map<String, Processor.Factory> processorRegistry, String type, Map<String, Object> config) throws Exception {
-            Processor.Factory factory = processorRegistry.get(type);
-            if (factory != null) {
-                List<Processor> onFailureProcessors = readProcessors("on_failure", processorRegistry, config);
-                Processor processor = factory.create(config);
-                if (config.isEmpty() == false) {
-                    throw new IllegalArgumentException("processor [" + type + "] doesn't support one or more provided configuration parameters " + Arrays.toString(config.keySet().toArray()));
-                }
-                if (onFailureProcessors.isEmpty()) {
-                    return processor;
-                }
-                return new CompoundProcessor(Collections.singletonList(processor), onFailureProcessors);
-            }
-            throw new IllegalArgumentException("No processor type exists with name [" + type + "]");
-        }
-
-        private List<Processor> readProcessors(String fieldName, Map<String, Processor.Factory> processorRegistry, Map<String, Object> config) throws Exception {
-            List<Map<String, Map<String, Object>>> onFailureProcessorConfigs = ConfigurationUtils.readOptionalList(config, fieldName);
-            List<Processor> onFailureProcessors = new ArrayList<>();
-            if (onFailureProcessorConfigs != null) {
-                for (Map<String, Map<String, Object>> processorConfigWithKey : onFailureProcessorConfigs) {
-                    for (Map.Entry<String, Map<String, Object>> entry : processorConfigWithKey.entrySet()) {
-                        onFailureProcessors.add(readProcessor(processorRegistry, entry.getKey(), entry.getValue()));
-                    }
-                }
-            }
-
-            return onFailureProcessors;
-        }
-
-        public Pipeline create(String id, Map<String, Object> config, Map<String, Processor.Factory> processorRegistry) throws Exception {
-            String description = ConfigurationUtils.readOptionalStringProperty(config, "description");
-            List<Processor> processors = readProcessors("processors", processorRegistry, config);
-            List<Processor> onFailureProcessors = readProcessors("on_failure", processorRegistry, config);
-            CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.unmodifiableList(processors), Collections.unmodifiableList(onFailureProcessors));
-            return new Pipeline(id, description, compoundProcessor);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/Processor.java b/core/src/main/java/org/elasticsearch/ingest/core/Processor.java
deleted file mode 100644
index 9c29894..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/Processor.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-
-package org.elasticsearch.ingest.core;
-
-import java.util.Map;
-
-/**
- * A processor implementation may modify the data belonging to a document.
- * Whether changes are made and what exactly is modified is up to the implementation.
- */
-public interface Processor {
-
-    /**
-     * Introspect and potentially modify the incoming data.
-     */
-    void execute(IngestDocument ingestDocument) throws Exception;
-
-    /**
-     * Gets the type of a processor
-     */
-    String getType();
-
-    /**
-     * A factory that knows how to construct a processor based on a map of maps.
-     */
-    interface Factory<P extends Processor> {
-
-        /**
-         * Creates a processor based on the specified map of maps config.
-         *
-         * Implementations are responsible for removing the used keys, so that after creating a pipeline ingest can
-         * verify if all configurations settings have been used.
-         */
-        P create(Map<String, Object> config) throws Exception;
-
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/TemplateService.java b/core/src/main/java/org/elasticsearch/ingest/core/TemplateService.java
deleted file mode 100644
index 8988c92..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/TemplateService.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.ingest.core;
-
-import java.util.Map;
-
-/**
- * Abstraction for the ingest template engine used to decouple {@link IngestDocument} from {@link org.elasticsearch.script.ScriptService}.
- * Allows to compile a template into an ingest {@link Template} object.
- * A compiled template can be executed by calling its {@link Template#execute(Map)} method.
- */
-public interface TemplateService {
-
-    Template compile(String template);
-
-    interface Template {
-
-        String execute(Map<String, Object> model);
-
-        String getKey();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/ValueSource.java b/core/src/main/java/org/elasticsearch/ingest/core/ValueSource.java
deleted file mode 100644
index 987002f..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/ValueSource.java
+++ /dev/null
@@ -1,193 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Objects;
-
-/**
- * Holds a value. If the value is requested a copy is made and optionally template snippets are resolved too.
- */
-public interface ValueSource {
-
-    /**
-     * Returns a copy of the value this ValueSource holds and resolves templates if there're any.
-     *
-     * For immutable values only a copy of the reference to the value is made.
-     *
-     * @param model The model to be used when resolving any templates
-     * @return copy of the wrapped value
-     */
-    Object copyAndResolve(Map<String, Object> model);
-
-    static ValueSource wrap(Object value, TemplateService templateService) {
-        if (value instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<Object, Object> mapValue = (Map) value;
-            Map<ValueSource, ValueSource> valueTypeMap = new HashMap<>(mapValue.size());
-            for (Map.Entry<Object, Object> entry : mapValue.entrySet()) {
-                valueTypeMap.put(wrap(entry.getKey(), templateService), wrap(entry.getValue(), templateService));
-            }
-            return new MapValue(valueTypeMap);
-        } else if (value instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> listValue = (List) value;
-            List<ValueSource> valueSourceList = new ArrayList<>(listValue.size());
-            for (Object item : listValue) {
-                valueSourceList.add(wrap(item, templateService));
-            }
-            return new ListValue(valueSourceList);
-        } else if (value == null || value instanceof Integer ||
-            value instanceof Long || value instanceof Float ||
-            value instanceof Double || value instanceof Boolean) {
-            return new ObjectValue(value);
-        } else if (value instanceof String) {
-            return new TemplatedValue(templateService.compile((String) value));
-        } else {
-            throw new IllegalArgumentException("unexpected value type [" + value.getClass() + "]");
-        }
-    }
-
-    final class MapValue implements ValueSource {
-
-        private final Map<ValueSource, ValueSource> map;
-
-        MapValue(Map<ValueSource, ValueSource> map) {
-            this.map = map;
-        }
-
-        @Override
-        public Object copyAndResolve(Map<String, Object> model) {
-            Map<Object, Object> copy = new HashMap<>();
-            for (Map.Entry<ValueSource, ValueSource> entry : this.map.entrySet()) {
-                copy.put(entry.getKey().copyAndResolve(model), entry.getValue().copyAndResolve(model));
-            }
-            return copy;
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
-
-            MapValue mapValue = (MapValue) o;
-            return map.equals(mapValue.map);
-
-        }
-
-        @Override
-        public int hashCode() {
-            return map.hashCode();
-        }
-    }
-
-    final class ListValue implements ValueSource {
-
-        private final List<ValueSource> values;
-
-        ListValue(List<ValueSource> values) {
-            this.values = values;
-        }
-
-        @Override
-        public Object copyAndResolve(Map<String, Object> model) {
-            List<Object> copy = new ArrayList<>(values.size());
-            for (ValueSource value : values) {
-                copy.add(value.copyAndResolve(model));
-            }
-            return copy;
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
-
-            ListValue listValue = (ListValue) o;
-            return values.equals(listValue.values);
-
-        }
-
-        @Override
-        public int hashCode() {
-            return values.hashCode();
-        }
-    }
-
-    final class ObjectValue implements ValueSource {
-
-        private final Object value;
-
-        ObjectValue(Object value) {
-            this.value = value;
-        }
-
-        @Override
-        public Object copyAndResolve(Map<String, Object> model) {
-            return value;
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
-
-            ObjectValue objectValue = (ObjectValue) o;
-            return Objects.equals(value, objectValue.value);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hashCode(value);
-        }
-    }
-
-    final class TemplatedValue implements ValueSource {
-
-        private final TemplateService.Template template;
-
-        TemplatedValue(TemplateService.Template template) {
-            this.template = template;
-        }
-
-        @Override
-        public Object copyAndResolve(Map<String, Object> model) {
-            return template.execute(model);
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
-
-            TemplatedValue templatedValue = (TemplatedValue) o;
-            return Objects.equals(template.getKey(), templatedValue.template.getKey());
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hashCode(template.getKey());
-        }
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/AbstractStringProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/AbstractStringProcessor.java
deleted file mode 100644
index 8fb73cf..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/AbstractStringProcessor.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.Map;
-
-/**
- * Base class for processors that manipulate strings and require a single "fields" array config value, which
- * holds a list of field names in string format.
- */
-public abstract class AbstractStringProcessor implements Processor {
-
-    private final String field;
-
-    protected AbstractStringProcessor(String field) {
-        this.field = field;
-    }
-
-    public String getField() {
-        return field;
-    }
-
-    @Override
-    public final void execute(IngestDocument document) {
-        String val = document.getFieldValue(field, String.class);
-        if (val == null) {
-            throw new IllegalArgumentException("field [" + field + "] is null, cannot process it.");
-        }
-        document.setFieldValue(field, process(val));
-    }
-
-    protected abstract String process(String value);
-
-    public static abstract class Factory<T extends AbstractStringProcessor> implements Processor.Factory<T> {
-        @Override
-        public T create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            return newProcessor(field);
-        }
-
-        protected abstract T newProcessor(String field);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/AppendProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/AppendProcessor.java
deleted file mode 100644
index 108cc5d..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/AppendProcessor.java
+++ /dev/null
@@ -1,80 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.ingest.core.ValueSource;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.Map;
-
-/**
- * Processor that appends value or values to existing lists. If the field is not present a new list holding the
- * provided values will be added. If the field is a scalar it will be converted to a single item list and the provided
- * values will be added to the newly created list.
- */
-public class AppendProcessor implements Processor {
-
-    public static final String TYPE = "append";
-
-    private final TemplateService.Template field;
-    private final ValueSource value;
-
-    AppendProcessor(TemplateService.Template field, ValueSource value) {
-        this.field = field;
-        this.value = value;
-    }
-
-    public TemplateService.Template getField() {
-        return field;
-    }
-
-    public ValueSource getValue() {
-        return value;
-    }
-
-    @Override
-    public void execute(IngestDocument ingestDocument) throws Exception {
-        ingestDocument.appendFieldValue(field, value);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static final class Factory implements Processor.Factory<AppendProcessor> {
-
-        private final TemplateService templateService;
-
-        public Factory(TemplateService templateService) {
-            this.templateService = templateService;
-        }
-
-        @Override
-        public AppendProcessor create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            Object value = ConfigurationUtils.readObject(config, "value");
-            return new AppendProcessor(templateService.compile(field), ValueSource.wrap(value, templateService));
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/ConvertProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/ConvertProcessor.java
deleted file mode 100644
index c7f260c..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/ConvertProcessor.java
+++ /dev/null
@@ -1,143 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-
-/**
- * Processor that converts fields content to a different type. Supported types are: integer, float, boolean and string.
- * Throws exception if the field is not there or the conversion fails.
- */
-public class ConvertProcessor implements Processor {
-
-    enum Type {
-        INTEGER {
-            @Override
-            public Object convert(Object value) {
-                try {
-                    return Integer.parseInt(value.toString());
-                } catch(NumberFormatException e) {
-                    throw new IllegalArgumentException("unable to convert [" + value + "] to integer", e);
-                }
-
-            }
-        }, FLOAT {
-            @Override
-            public Object convert(Object value) {
-                try {
-                    return Float.parseFloat(value.toString());
-                } catch(NumberFormatException e) {
-                    throw new IllegalArgumentException("unable to convert [" + value + "] to float", e);
-                }
-            }
-        }, BOOLEAN {
-            @Override
-            public Object convert(Object value) {
-                if (value.toString().equalsIgnoreCase("true")) {
-                    return true;
-                } else if (value.toString().equalsIgnoreCase("false")) {
-                    return false;
-                } else {
-                    throw new IllegalArgumentException("[" + value + "] is not a boolean value, cannot convert to boolean");
-                }
-            }
-        }, STRING {
-            @Override
-            public Object convert(Object value) {
-                return value.toString();
-            }
-        };
-
-        @Override
-        public final String toString() {
-            return name().toLowerCase(Locale.ROOT);
-        }
-
-        public abstract Object convert(Object value);
-
-        public static Type fromString(String type) {
-            try {
-                return Type.valueOf(type.toUpperCase(Locale.ROOT));
-            } catch(IllegalArgumentException e) {
-                throw new IllegalArgumentException("type [" + type + "] not supported, cannot convert field.", e);
-            }
-        }
-    }
-
-    public static final String TYPE = "convert";
-
-    private final String field;
-    private final Type convertType;
-
-    ConvertProcessor(String field, Type convertType) {
-        this.field = field;
-        this.convertType = convertType;
-    }
-
-    String getField() {
-        return field;
-    }
-
-    Type getConvertType() {
-        return convertType;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        Object oldValue = document.getFieldValue(field, Object.class);
-        Object newValue;
-        if (oldValue == null) {
-            throw new IllegalArgumentException("Field [" + field + "] is null, cannot be converted to type [" + convertType + "]");
-        }
-
-        if (oldValue instanceof List) {
-            List<?> list = (List<?>) oldValue;
-            List<Object> newList = new ArrayList<>();
-            for (Object value : list) {
-                newList.add(convertType.convert(value));
-            }
-            newValue = newList;
-        } else {
-            newValue = convertType.convert(oldValue);
-        }
-        document.setFieldValue(field, newValue);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory implements Processor.Factory<ConvertProcessor> {
-        @Override
-        public ConvertProcessor create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            Type convertType = Type.fromString(ConfigurationUtils.readStringProperty(config, "type"));
-            return new ConvertProcessor(field, convertType);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/DateFormat.java b/core/src/main/java/org/elasticsearch/ingest/processor/DateFormat.java
deleted file mode 100644
index 5748d37..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/DateFormat.java
+++ /dev/null
@@ -1,99 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-import org.joda.time.format.DateTimeFormat;
-import org.joda.time.format.ISODateTimeFormat;
-
-import java.util.Locale;
-import java.util.Optional;
-import java.util.function.Function;
-
-enum DateFormat {
-    Iso8601 {
-        @Override
-        Function<String, DateTime> getFunction(DateTimeZone timezone) {
-            return ISODateTimeFormat.dateTimeParser().withZone(timezone)::parseDateTime;
-        }
-    },
-    Unix {
-        @Override
-        Function<String, DateTime> getFunction(DateTimeZone timezone) {
-            return (date) -> new DateTime((long)(Float.parseFloat(date) * 1000), timezone);
-        }
-    },
-    UnixMs {
-        @Override
-        Function<String, DateTime> getFunction(DateTimeZone timezone) {
-            return (date) -> new DateTime(Long.parseLong(date), timezone);
-        }
-
-        @Override
-        public String toString() {
-            return "UNIX_MS";
-        }
-    },
-    Tai64n {
-        @Override
-        Function<String, DateTime> getFunction(DateTimeZone timezone) {
-            return (date) -> new DateTime(parseMillis(date), timezone);
-        }
-
-        private long parseMillis(String date) {
-            if (date.startsWith("@")) {
-                date = date.substring(1);
-            }
-            long base = Long.parseLong(date.substring(1, 16), 16);
-            // 1356138046000
-            long rest = Long.parseLong(date.substring(16, 24), 16);
-            return ((base * 1000) - 10000) + (rest/1000000);
-        }
-    };
-
-    abstract Function<String, DateTime> getFunction(DateTimeZone timezone);
-
-    static Optional<DateFormat> fromString(String format) {
-        switch (format) {
-            case "ISO8601":
-                return Optional.of(Iso8601);
-            case "UNIX":
-                return Optional.of(Unix);
-            case "UNIX_MS":
-                return Optional.of(UnixMs);
-            case "TAI64N":
-                return Optional.of(Tai64n);
-            default:
-                return Optional.empty();
-        }
-    }
-
-    static Function<String, DateTime> getJodaFunction(String matchFormat, DateTimeZone timezone, Locale locale) {
-        return DateTimeFormat.forPattern(matchFormat)
-                .withDefaultYear((new DateTime(DateTimeZone.UTC)).getYear())
-                .withZone(timezone).withLocale(locale)::parseDateTime;
-    }
-
-    @Override
-    public String toString() {
-        return name().toUpperCase(Locale.ROOT);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/DateProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/DateProcessor.java
deleted file mode 100644
index 46a6e92..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/DateProcessor.java
+++ /dev/null
@@ -1,137 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-import org.joda.time.format.ISODateTimeFormat;
-
-import java.util.ArrayList;
-import java.util.IllformedLocaleException;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Optional;
-import java.util.function.Function;
-
-public final class DateProcessor implements Processor {
-
-    public static final String TYPE = "date";
-    static final String DEFAULT_TARGET_FIELD = "@timestamp";
-
-    private final DateTimeZone timezone;
-    private final Locale locale;
-    private final String matchField;
-    private final String targetField;
-    private final List<String> matchFormats;
-    private final List<Function<String, DateTime>> dateParsers;
-
-    DateProcessor(DateTimeZone timezone, Locale locale, String matchField, List<String> matchFormats, String targetField) {
-        this.timezone = timezone;
-        this.locale = locale;
-        this.matchField = matchField;
-        this.targetField = targetField;
-        this.matchFormats = matchFormats;
-        this.dateParsers = new ArrayList<>();
-        for (String matchFormat : matchFormats) {
-            Optional<DateFormat> dateFormat = DateFormat.fromString(matchFormat);
-            Function<String, DateTime> stringToDateFunction;
-            if (dateFormat.isPresent()) {
-                stringToDateFunction = dateFormat.get().getFunction(timezone);
-            } else {
-                stringToDateFunction = DateFormat.getJodaFunction(matchFormat, timezone, locale);
-            }
-            dateParsers.add(stringToDateFunction);
-        }
-    }
-
-    @Override
-    public void execute(IngestDocument ingestDocument) {
-        String value = ingestDocument.getFieldValue(matchField, String.class);
-        // TODO(talevy): handle custom timestamp fields
-
-        DateTime dateTime = null;
-        Exception lastException = null;
-        for (Function<String, DateTime> dateParser : dateParsers) {
-            try {
-                dateTime = dateParser.apply(value);
-            } catch(Exception e) {
-                //try the next parser and keep track of the last exception
-                lastException = e;
-            }
-        }
-
-        if (dateTime == null) {
-            throw new IllegalArgumentException("unable to parse date [" + value + "]", lastException);
-        }
-
-        ingestDocument.setFieldValue(targetField, ISODateTimeFormat.dateTime().print(dateTime));
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    DateTimeZone getTimezone() {
-        return timezone;
-    }
-
-    Locale getLocale() {
-        return locale;
-    }
-
-    String getMatchField() {
-        return matchField;
-    }
-
-    String getTargetField() {
-        return targetField;
-    }
-
-    List<String> getMatchFormats() {
-        return matchFormats;
-    }
-
-    public static class Factory implements Processor.Factory<DateProcessor> {
-
-        @SuppressWarnings("unchecked")
-        public DateProcessor create(Map<String, Object> config) throws Exception {
-            String matchField = ConfigurationUtils.readStringProperty(config, "match_field");
-            String targetField = ConfigurationUtils.readStringProperty(config, "target_field", DEFAULT_TARGET_FIELD);
-            String timezoneString = ConfigurationUtils.readOptionalStringProperty(config, "timezone");
-            DateTimeZone timezone = timezoneString == null ? DateTimeZone.UTC : DateTimeZone.forID(timezoneString);
-            String localeString = ConfigurationUtils.readOptionalStringProperty(config, "locale");
-            Locale locale = Locale.ENGLISH;
-            if (localeString != null) {
-                try {
-                    locale = (new Locale.Builder()).setLanguageTag(localeString).build();
-                } catch (IllformedLocaleException e) {
-                    throw new IllegalArgumentException("Invalid language tag specified: " + localeString);
-                }
-            }
-            List<String> matchFormats = ConfigurationUtils.readList(config, "match_formats");
-            return new DateProcessor(timezone, locale, matchField, matchFormats, targetField);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessor.java
deleted file mode 100644
index 574e41f..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessor.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.core.TemplateService;
-
-import java.util.Map;
-
-/**
- * Processor that raises a runtime exception with a provided
- * error message.
- */
-public class FailProcessor implements Processor {
-
-    public static final String TYPE = "fail";
-
-    private final TemplateService.Template message;
-
-    FailProcessor(TemplateService.Template message) {
-        this.message = message;
-    }
-
-    public TemplateService.Template getMessage() {
-        return message;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        throw new FailProcessorException(document.renderTemplate(message));
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory implements Processor.Factory<FailProcessor> {
-
-        private final TemplateService templateService;
-
-        public Factory(TemplateService templateService) {
-            this.templateService = templateService;
-        }
-
-        @Override
-        public FailProcessor create(Map<String, Object> config) throws Exception {
-            String message = ConfigurationUtils.readStringProperty(config, "message");
-            return new FailProcessor(templateService.compile(message));
-        }
-    }
-}
-
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessorException.java b/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessorException.java
deleted file mode 100644
index 846ba40..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessorException.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-/**
- * Exception class thrown by {@link FailProcessor}.
- */
-public class FailProcessorException extends RuntimeException {
-
-    public FailProcessorException(String message) {
-        super(message);
-    }
-}
-
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/GsubProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/GsubProcessor.java
deleted file mode 100644
index c201729..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/GsubProcessor.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.Map;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-/**
- * Processor that allows to search for patterns in field content and replace them with corresponding string replacement.
- * Support fields of string type only, throws exception if a field is of a different type.
- */
-public class GsubProcessor implements Processor {
-
-    public static final String TYPE = "gsub";
-
-    private final String field;
-    private final Pattern pattern;
-    private final String replacement;
-
-    GsubProcessor(String field, Pattern pattern, String replacement) {
-        this.field = field;
-        this.pattern = pattern;
-        this.replacement = replacement;
-    }
-
-    String getField() {
-        return field;
-    }
-
-    Pattern getPattern() {
-        return pattern;
-    }
-
-    String getReplacement() {
-        return replacement;
-    }
-
-
-    @Override
-    public void execute(IngestDocument document) {
-        String oldVal = document.getFieldValue(field, String.class);
-        if (oldVal == null) {
-            throw new IllegalArgumentException("field [" + field + "] is null, cannot match pattern.");
-        }
-        Matcher matcher = pattern.matcher(oldVal);
-        String newVal = matcher.replaceAll(replacement);
-        document.setFieldValue(field, newVal);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory implements Processor.Factory<GsubProcessor> {
-        @Override
-        public GsubProcessor create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            String pattern = ConfigurationUtils.readStringProperty(config, "pattern");
-            String replacement = ConfigurationUtils.readStringProperty(config, "replacement");
-            Pattern searchPattern = Pattern.compile(pattern);
-            return new GsubProcessor(field, searchPattern, replacement);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/JoinProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/JoinProcessor.java
deleted file mode 100644
index 08ac9d1..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/JoinProcessor.java
+++ /dev/null
@@ -1,80 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.List;
-import java.util.Map;
-import java.util.stream.Collectors;
-
-/**
- * Processor that joins the different items of an array into a single string value using a separator between each item.
- * Throws exception is the specified field is not an array.
- */
-public class JoinProcessor implements Processor {
-
-    public static final String TYPE = "join";
-
-    private final String field;
-    private final String separator;
-
-    JoinProcessor(String field, String separator) {
-        this.field = field;
-        this.separator = separator;
-    }
-
-    String getField() {
-        return field;
-    }
-
-    String getSeparator() {
-        return separator;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        List<?> list = document.getFieldValue(field, List.class);
-        if (list == null) {
-            throw new IllegalArgumentException("field [" + field + "] is null, cannot join.");
-        }
-        String joined = list.stream()
-                .map(Object::toString)
-                .collect(Collectors.joining(separator));
-        document.setFieldValue(field, joined);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory implements Processor.Factory<JoinProcessor> {
-        @Override
-        public JoinProcessor create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            String separator = ConfigurationUtils.readStringProperty(config, "separator");
-            return new JoinProcessor(field, separator);
-        }
-    }
-}
-
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/LowercaseProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/LowercaseProcessor.java
deleted file mode 100644
index e3e49f7..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/LowercaseProcessor.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import java.util.Locale;
-
-/**
- * Processor that converts the content of string fields to lowercase.
- * Throws exception is the field is not of type string.
- */
-
-public class LowercaseProcessor extends AbstractStringProcessor {
-
-    public static final String TYPE = "lowercase";
-
-    LowercaseProcessor(String field) {
-        super(field);
-    }
-
-    @Override
-    protected String process(String value) {
-        return value.toLowerCase(Locale.ROOT);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory extends AbstractStringProcessor.Factory<LowercaseProcessor> {
-        @Override
-        protected LowercaseProcessor newProcessor(String field) {
-            return new LowercaseProcessor(field);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/RemoveProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/RemoveProcessor.java
deleted file mode 100644
index a3c5f76..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/RemoveProcessor.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.Map;
-
-/**
- * Processor that removes existing fields. Nothing happens if the field is not present.
- */
-public class RemoveProcessor implements Processor {
-
-    public static final String TYPE = "remove";
-
-    private final TemplateService.Template field;
-
-    RemoveProcessor(TemplateService.Template field) {
-        this.field = field;
-    }
-
-    public TemplateService.Template getField() {
-        return field;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        document.removeField(field);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory implements Processor.Factory<RemoveProcessor> {
-
-        private final TemplateService templateService;
-
-        public Factory(TemplateService templateService) {
-            this.templateService = templateService;
-        }
-
-        @Override
-        public RemoveProcessor create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            return new RemoveProcessor(templateService.compile(field));
-        }
-    }
-}
-
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/RenameProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/RenameProcessor.java
deleted file mode 100644
index 5a9e4d5..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/RenameProcessor.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.Map;
-
-/**
- * Processor that allows to rename existing fields. Will throw exception if the field is not present.
- */
-public class RenameProcessor implements Processor {
-
-    public static final String TYPE = "rename";
-
-    private final String oldFieldName;
-    private final String newFieldName;
-
-    RenameProcessor(String oldFieldName, String newFieldName) {
-        this.oldFieldName = oldFieldName;
-        this.newFieldName = newFieldName;
-    }
-
-    String getOldFieldName() {
-        return oldFieldName;
-    }
-
-    String getNewFieldName() {
-        return newFieldName;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        if (document.hasField(oldFieldName) == false) {
-            throw new IllegalArgumentException("field [" + oldFieldName + "] doesn't exist");
-        }
-        if (document.hasField(newFieldName)) {
-            throw new IllegalArgumentException("field [" + newFieldName + "] already exists");
-        }
-
-        Object oldValue = document.getFieldValue(oldFieldName, Object.class);
-        document.setFieldValue(newFieldName, oldValue);
-        try {
-            document.removeField(oldFieldName);
-        } catch (Exception e) {
-            //remove the new field if the removal of the old one failed
-            document.removeField(newFieldName);
-            throw e;
-        }
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory implements Processor.Factory<RenameProcessor> {
-        @Override
-        public RenameProcessor create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            String newField = ConfigurationUtils.readStringProperty(config, "to");
-            return new RenameProcessor(field, newField);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/SetProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/SetProcessor.java
deleted file mode 100644
index a43e605..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/SetProcessor.java
+++ /dev/null
@@ -1,79 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.ingest.core.ValueSource;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.Map;
-
-/**
- * Processor that adds new fields with their corresponding values. If the field is already present, its value
- * will be replaced with the provided one.
- */
-public class SetProcessor implements Processor {
-
-    public static final String TYPE = "set";
-
-    private final TemplateService.Template field;
-    private final ValueSource value;
-
-    SetProcessor(TemplateService.Template field, ValueSource value) {
-        this.field = field;
-        this.value = value;
-    }
-
-    public TemplateService.Template getField() {
-        return field;
-    }
-
-    public ValueSource getValue() {
-        return value;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        document.setFieldValue(field, value);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static final class Factory implements Processor.Factory<SetProcessor> {
-
-        private final TemplateService templateService;
-
-        public Factory(TemplateService templateService) {
-            this.templateService = templateService;
-        }
-
-        @Override
-        public SetProcessor create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            Object value = ConfigurationUtils.readObject(config, "value");
-            return new SetProcessor(templateService.compile(field), ValueSource.wrap(value, templateService));
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/SplitProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/SplitProcessor.java
deleted file mode 100644
index b1d9c23..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/SplitProcessor.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.Arrays;
-import java.util.Map;
-
-/**
- * Processor that splits fields content into different items based on the occurrence of a specified separator.
- * New field value will be an array containing all of the different extracted items.
- * Throws exception if the field is null or a type other than string.
- */
-public class SplitProcessor implements Processor {
-
-    public static final String TYPE = "split";
-
-    private final String field;
-    private final String separator;
-
-    SplitProcessor(String field, String separator) {
-        this.field = field;
-        this.separator = separator;
-    }
-
-    String getField() {
-        return field;
-    }
-
-    String getSeparator() {
-        return separator;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        String oldVal = document.getFieldValue(field, String.class);
-        if (oldVal == null) {
-            throw new IllegalArgumentException("field [" + field + "] is null, cannot split.");
-        }
-        document.setFieldValue(field, Arrays.asList(oldVal.split(separator)));
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory implements Processor.Factory<SplitProcessor> {
-        @Override
-        public SplitProcessor create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            return new SplitProcessor(field, ConfigurationUtils.readStringProperty(config, "separator"));
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/TrimProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/TrimProcessor.java
deleted file mode 100644
index d132551..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/TrimProcessor.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-/**
- * Processor that trims the content of string fields.
- * Throws exception is the field is not of type string.
- */
-public class TrimProcessor extends AbstractStringProcessor {
-
-    public static final String TYPE = "trim";
-
-    TrimProcessor(String field) {
-        super(field);
-    }
-
-    @Override
-    protected String process(String value) {
-        return value.trim();
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory extends AbstractStringProcessor.Factory<TrimProcessor> {
-        @Override
-        protected TrimProcessor newProcessor(String field) {
-            return new TrimProcessor(field);
-        }
-    }
-}
-
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/UppercaseProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/UppercaseProcessor.java
deleted file mode 100644
index 0f1757e..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/UppercaseProcessor.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import java.util.Locale;
-
-/**
- * Processor that converts the content of string fields to uppercase.
- * Throws exception is the field is not of type string.
- */
-public class UppercaseProcessor extends AbstractStringProcessor {
-
-    public static final String TYPE = "uppercase";
-
-    UppercaseProcessor(String field) {
-        super(field);
-    }
-
-    @Override
-    protected String process(String value) {
-        return value.toUpperCase(Locale.ROOT);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory extends AbstractStringProcessor.Factory<UppercaseProcessor> {
-        @Override
-        protected UppercaseProcessor newProcessor(String field) {
-            return new UppercaseProcessor(field);
-        }
-    }
-}
-
diff --git a/core/src/main/java/org/elasticsearch/node/Node.java b/core/src/main/java/org/elasticsearch/node/Node.java
index 11726a5..da3d1e9 100644
--- a/core/src/main/java/org/elasticsearch/node/Node.java
+++ b/core/src/main/java/org/elasticsearch/node/Node.java
@@ -69,10 +69,8 @@ import org.elasticsearch.indices.breaker.CircuitBreakerModule;
 import org.elasticsearch.indices.cache.query.IndicesQueryCache;
 import org.elasticsearch.indices.cluster.IndicesClusterStateService;
 import org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache;
-import org.elasticsearch.indices.memory.IndexingMemoryController;
 import org.elasticsearch.indices.store.IndicesStore;
 import org.elasticsearch.indices.ttl.IndicesTTLService;
-import org.elasticsearch.ingest.IngestModule;
 import org.elasticsearch.monitor.MonitorService;
 import org.elasticsearch.monitor.jvm.JvmInfo;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
@@ -189,7 +187,7 @@ public class Node implements Releasable {
             modules.add(new ClusterModule(this.settings));
             modules.add(new IndicesModule());
             modules.add(new SearchModule());
-            modules.add(new ActionModule(this.settings, false));
+            modules.add(new ActionModule(false));
             modules.add(new GatewayModule(settings));
             modules.add(new NodeClientModule());
             modules.add(new PercolatorModule());
@@ -197,7 +195,6 @@ public class Node implements Releasable {
             modules.add(new RepositoriesModule());
             modules.add(new TribeModule());
             modules.add(new AnalysisModule(environment));
-            modules.add(new IngestModule());
 
             pluginsService.processModules(modules);
 
@@ -250,7 +247,6 @@ public class Node implements Releasable {
 
         injector.getInstance(MappingUpdatedAction.class).setClient(client);
         injector.getInstance(IndicesService.class).start();
-        injector.getInstance(IndexingMemoryController.class).start();
         injector.getInstance(IndicesClusterStateService.class).start();
         injector.getInstance(IndicesTTLService.class).start();
         injector.getInstance(SnapshotsService.class).start();
@@ -309,7 +305,6 @@ public class Node implements Releasable {
         // stop any changes happening as a result of cluster state changes
         injector.getInstance(IndicesClusterStateService.class).stop();
         // we close indices first, so operations won't be allowed on it
-        injector.getInstance(IndexingMemoryController.class).stop();
         injector.getInstance(IndicesTTLService.class).stop();
         injector.getInstance(RoutingService.class).stop();
         injector.getInstance(ClusterService.class).stop();
@@ -361,7 +356,6 @@ public class Node implements Releasable {
         stopWatch.stop().start("indices_cluster");
         injector.getInstance(IndicesClusterStateService.class).close();
         stopWatch.stop().start("indices");
-        injector.getInstance(IndexingMemoryController.class).close();
         injector.getInstance(IndicesTTLService.class).close();
         injector.getInstance(IndicesService.class).close();
         // close filter/fielddata caches after indices
diff --git a/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java b/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
index 11db520..53b86e4 100644
--- a/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
+++ b/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
@@ -22,6 +22,7 @@ package org.elasticsearch.node.internal;
 import org.elasticsearch.bootstrap.BootstrapInfo;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.common.Booleans;
+import org.elasticsearch.common.Randomness;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.cli.Terminal;
 import org.elasticsearch.common.collect.Tuple;
@@ -41,7 +42,6 @@ import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.concurrent.ThreadLocalRandom;
 
 import static org.elasticsearch.common.Strings.cleanPath;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
@@ -207,7 +207,7 @@ public class InternalSettingsPreparer {
                     name = reader.readLine();
                 }
             }
-            int index = ThreadLocalRandom.current().nextInt(names.size());
+            int index = Randomness.get().nextInt(names.size());
             return names.get(index);
         } catch (IOException e) {
             throw new RuntimeException("Could not read node names list", e);
diff --git a/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java b/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java
index df20438..37ce03b 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java
@@ -77,7 +77,6 @@ public class RestBulkAction extends BaseRestHandler {
         String defaultType = request.param("type");
         String defaultRouting = request.param("routing");
         String fieldsParam = request.param("fields");
-        String defaultPipeline = request.param("pipeline");
         String[] defaultFields = fieldsParam != null ? Strings.commaDelimitedListToStringArray(fieldsParam) : null;
 
         String consistencyLevel = request.param("consistency");
@@ -86,7 +85,7 @@ public class RestBulkAction extends BaseRestHandler {
         }
         bulkRequest.timeout(request.paramAsTime("timeout", BulkShardRequest.DEFAULT_TIMEOUT));
         bulkRequest.refresh(request.paramAsBoolean("refresh", bulkRequest.refresh()));
-        bulkRequest.add(request.content(), defaultIndex, defaultType, defaultRouting, defaultFields, defaultPipeline, null, allowExplicitIndex);
+        bulkRequest.add(request.content(), defaultIndex, defaultType, defaultRouting, defaultFields, null, allowExplicitIndex);
 
         client.bulk(bulkRequest, new RestBuilderListener<BulkResponse>(channel) {
             @Override
diff --git a/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java b/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java
index e86132a..110aa90 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java
@@ -41,7 +41,7 @@ import org.elasticsearch.index.engine.SegmentsStats;
 import org.elasticsearch.index.fielddata.FieldDataStats;
 import org.elasticsearch.index.flush.FlushStats;
 import org.elasticsearch.index.get.GetStats;
-import org.elasticsearch.index.indexing.IndexingStats;
+import org.elasticsearch.index.shard.IndexingStats;
 import org.elasticsearch.index.merge.MergeStats;
 import org.elasticsearch.index.percolator.PercolateStats;
 import org.elasticsearch.index.refresh.RefreshStats;
diff --git a/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java b/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java
index 4eaec2c..13a9329 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java
@@ -77,7 +77,6 @@ public class RestIndexAction extends BaseRestHandler {
         if (request.hasParam("ttl")) {
             indexRequest.ttl(request.param("ttl"));
         }
-        indexRequest.pipeline(request.param("pipeline"));
         indexRequest.source(request.content());
         indexRequest.timeout(request.paramAsTime("timeout", IndexRequest.DEFAULT_TIMEOUT));
         indexRequest.refresh(request.paramAsBoolean("refresh", indexRequest.refresh()));
diff --git a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestDeletePipelineAction.java b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestDeletePipelineAction.java
deleted file mode 100644
index 994e030..0000000
--- a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestDeletePipelineAction.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.rest.action.ingest;
-
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.action.ingest.DeletePipelineAction;
-import org.elasticsearch.action.ingest.DeletePipelineRequest;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
-
-public class RestDeletePipelineAction extends BaseRestHandler {
-
-    @Inject
-    public RestDeletePipelineAction(Settings settings, RestController controller, Client client) {
-        super(settings, controller, client);
-        controller.registerHandler(RestRequest.Method.DELETE, "/_ingest/pipeline/{id}", this);
-    }
-
-    @Override
-    protected void handleRequest(RestRequest restRequest, RestChannel channel, Client client) throws Exception {
-        DeletePipelineRequest request = new DeletePipelineRequest();
-        request.id(restRequest.param("id"));
-        client.execute(DeletePipelineAction.INSTANCE, request, new RestStatusToXContentListener<>(channel));
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestGetPipelineAction.java b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestGetPipelineAction.java
deleted file mode 100644
index 47f41fc..0000000
--- a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestGetPipelineAction.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.rest.action.ingest;
-
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.action.ingest.GetPipelineAction;
-import org.elasticsearch.action.ingest.GetPipelineRequest;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
-
-public class RestGetPipelineAction extends BaseRestHandler {
-
-    @Inject
-    public RestGetPipelineAction(Settings settings, RestController controller, Client client) {
-        super(settings, controller, client);
-        controller.registerHandler(RestRequest.Method.GET, "/_ingest/pipeline/{id}", this);
-    }
-
-    @Override
-    protected void handleRequest(RestRequest restRequest, RestChannel channel, Client client) throws Exception {
-        GetPipelineRequest request = new GetPipelineRequest();
-        request.ids(Strings.splitStringByCommaToArray(restRequest.param("id")));
-        client.execute(GetPipelineAction.INSTANCE, request, new RestStatusToXContentListener<>(channel));
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestPutPipelineAction.java b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestPutPipelineAction.java
deleted file mode 100644
index b63b2eb..0000000
--- a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestPutPipelineAction.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.rest.action.ingest;
-
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.action.ingest.PutPipelineAction;
-import org.elasticsearch.action.ingest.PutPipelineRequest;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
-
-public class RestPutPipelineAction extends BaseRestHandler {
-
-    @Inject
-    public RestPutPipelineAction(Settings settings, RestController controller, Client client) {
-        super(settings, controller, client);
-        controller.registerHandler(RestRequest.Method.PUT, "/_ingest/pipeline/{id}", this);
-    }
-
-    @Override
-    protected void handleRequest(RestRequest restRequest, RestChannel channel, Client client) throws Exception {
-        PutPipelineRequest request = new PutPipelineRequest();
-        request.id(restRequest.param("id"));
-        if (restRequest.hasContent()) {
-            request.source(restRequest.content());
-        }
-        client.execute(PutPipelineAction.INSTANCE, request, new RestStatusToXContentListener<>(channel));
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestSimulatePipelineAction.java b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestSimulatePipelineAction.java
deleted file mode 100644
index ed859e2..0000000
--- a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestSimulatePipelineAction.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.rest.action.ingest;
-
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.action.ingest.SimulatePipelineAction;
-import org.elasticsearch.action.ingest.SimulatePipelineRequest;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.action.support.RestActions;
-import org.elasticsearch.rest.action.support.RestToXContentListener;
-
-public class RestSimulatePipelineAction extends BaseRestHandler {
-
-    @Inject
-    public RestSimulatePipelineAction(Settings settings, RestController controller, Client client) {
-        super(settings, controller, client);
-        controller.registerHandler(RestRequest.Method.POST, "/_ingest/pipeline/{id}/_simulate", this);
-        controller.registerHandler(RestRequest.Method.GET, "/_ingest/pipeline/{id}/_simulate", this);
-        controller.registerHandler(RestRequest.Method.POST, "/_ingest/pipeline/_simulate", this);
-        controller.registerHandler(RestRequest.Method.GET, "/_ingest/pipeline/_simulate", this);
-    }
-
-    @Override
-    protected void handleRequest(RestRequest restRequest, RestChannel channel, Client client) throws Exception {
-        SimulatePipelineRequest request = new SimulatePipelineRequest();
-        request.setId(restRequest.param("id"));
-        request.setVerbose(restRequest.paramAsBoolean("verbose", false));
-
-        if (RestActions.hasBodyContent(restRequest)) {
-            request.setSource(RestActions.getRestContent(restRequest));
-        }
-
-        client.execute(SimulatePipelineAction.INSTANCE, request, new RestToXContentListener<>(channel));
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptContext.java b/core/src/main/java/org/elasticsearch/script/ScriptContext.java
index 3ab2bb5..4b1b6de 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptContext.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptContext.java
@@ -37,7 +37,7 @@ public interface ScriptContext {
      */
     enum Standard implements ScriptContext {
 
-        AGGS("aggs"), SEARCH("search"), UPDATE("update"), INGEST("ingest");
+        AGGS("aggs"), SEARCH("search"), UPDATE("update");
 
         private final String key;
 
diff --git a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
index c0d63ab..0e6204d 100644
--- a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
+++ b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
@@ -38,7 +38,6 @@ import org.elasticsearch.common.util.concurrent.XRejectedExecutionHandler;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
-import org.elasticsearch.ingest.IngestModule;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -88,7 +87,6 @@ public class ThreadPool extends AbstractComponent {
         public static final String FORCE_MERGE = "force_merge";
         public static final String FETCH_SHARD_STARTED = "fetch_shard_started";
         public static final String FETCH_SHARD_STORE = "fetch_shard_store";
-        public static final String INGEST = "ingest";
     }
 
     public enum ThreadPoolType {
@@ -147,7 +145,6 @@ public class ThreadPool extends AbstractComponent {
         map.put(Names.FORCE_MERGE, ThreadPoolType.FIXED);
         map.put(Names.FETCH_SHARD_STARTED, ThreadPoolType.SCALING);
         map.put(Names.FETCH_SHARD_STORE, ThreadPoolType.SCALING);
-        map.put(Names.INGEST, ThreadPoolType.FIXED);
         THREAD_POOL_TYPES = Collections.unmodifiableMap(map);
     }
 
@@ -237,9 +234,6 @@ public class ThreadPool extends AbstractComponent {
         add(defaultExecutorTypeSettings, new ExecutorSettingsBuilder(Names.FORCE_MERGE).size(1));
         add(defaultExecutorTypeSettings, new ExecutorSettingsBuilder(Names.FETCH_SHARD_STARTED).size(availableProcessors * 2).keepAlive("5m"));
         add(defaultExecutorTypeSettings, new ExecutorSettingsBuilder(Names.FETCH_SHARD_STORE).size(availableProcessors * 2).keepAlive("5m"));
-        if (IngestModule.isIngestEnabled(settings)) {
-            add(defaultExecutorTypeSettings, new ExecutorSettingsBuilder(Names.INGEST).size(availableProcessors).queueSize(200));
-        }
 
         this.defaultExecutorTypeSettings = unmodifiableMap(defaultExecutorTypeSettings);
 
@@ -464,7 +458,7 @@ public class ThreadPool extends AbstractComponent {
                 if (ThreadPoolType.FIXED == previousInfo.getThreadPoolType()) {
                     SizeValue updatedQueueSize = getAsSizeOrUnbounded(settings, "capacity", getAsSizeOrUnbounded(settings, "queue", getAsSizeOrUnbounded(settings, "queue_size", previousInfo.getQueueSize())));
                     if (Objects.equals(previousInfo.getQueueSize(), updatedQueueSize)) {
-                        int updatedSize = settings.getAsInt("size", previousInfo.getMax());
+                        int updatedSize = applyHardSizeLimit(name, settings.getAsInt("size", previousInfo.getMax()));
                         if (previousInfo.getMax() != updatedSize) {
                             logger.debug("updating thread_pool [{}], type [{}], size [{}], queue_size [{}]", name, type, updatedSize, updatedQueueSize);
                             // if you think this code is crazy: that's because it is!
@@ -486,7 +480,7 @@ public class ThreadPool extends AbstractComponent {
                 defaultQueueSize = previousInfo.getQueueSize();
             }
 
-            int size = settings.getAsInt("size", defaultSize);
+            int size = applyHardSizeLimit(name, settings.getAsInt("size", defaultSize));
             SizeValue queueSize = getAsSizeOrUnbounded(settings, "capacity", getAsSizeOrUnbounded(settings, "queue", getAsSizeOrUnbounded(settings, "queue_size", defaultQueueSize)));
             logger.debug("creating thread_pool [{}], type [{}], size [{}], queue_size [{}]", name, type, size, queueSize);
             Executor executor = EsExecutors.newFixed(name, size, queueSize == null ? -1 : (int) queueSize.singles(), threadFactory);
@@ -539,6 +533,21 @@ public class ThreadPool extends AbstractComponent {
         throw new IllegalArgumentException("No type found [" + type + "], for [" + name + "]");
     }
 
+    private int applyHardSizeLimit(String name, int size) {
+        int availableProcessors = EsExecutors.boundedNumberOfProcessors(settings);
+        if ((name.equals(Names.BULK) || name.equals(Names.INDEX)) && size > availableProcessors) {
+            // We use a hard max size for the indexing pools, because if too many threads enter Lucene's IndexWriter, it means
+            // too many segments written, too frequently, too much merging, etc:
+            // TODO: I would love to be loud here (throw an exception if you ask for a too-big size), but I think this is dangerous
+            // because on upgrade this setting could be in cluster state and hard for the user to correct?
+            logger.warn("requested thread pool size [{}] for [{}] is too large; setting to maximum [{}] instead",
+                        size, name, availableProcessors);
+            size = availableProcessors;
+        }
+
+        return size;
+    }
+
     private void updateSettings(Settings settings) {
         Map<String, Settings> groupSettings = settings.getAsGroups();
         if (groupSettings.isEmpty()) {
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/BulkRequestModifierTests.java b/core/src/test/java/org/elasticsearch/action/ingest/BulkRequestModifierTests.java
deleted file mode 100644
index aa30c89..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/BulkRequestModifierTests.java
+++ /dev/null
@@ -1,165 +0,0 @@
-package org.elasticsearch.action.ingest;
-
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.bulk.BulkItemResponse;
-import org.elasticsearch.action.bulk.BulkRequest;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.test.ESTestCase;
-import org.hamcrest.Matchers;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.nullValue;
-import static org.mockito.Mockito.mock;
-
-public class BulkRequestModifierTests extends ESTestCase {
-
-    public void testBulkRequestModifier() {
-        int numRequests = scaledRandomIntBetween(8, 64);
-        BulkRequest bulkRequest = new BulkRequest();
-        for (int i = 0; i < numRequests; i++) {
-            bulkRequest.add(new IndexRequest("_index", "_type", String.valueOf(i)).source("{}"));
-        }
-        CaptureActionListener actionListener = new CaptureActionListener();
-        IngestActionFilter.BulkRequestModifier bulkRequestModifier = new IngestActionFilter.BulkRequestModifier(bulkRequest);
-
-        int i = 0;
-        Set<Integer> failedSlots = new HashSet<>();
-        while (bulkRequestModifier.hasNext()) {
-            bulkRequestModifier.next();
-            if (randomBoolean()) {
-                bulkRequestModifier.markCurrentItemAsFailed(new RuntimeException());
-                failedSlots.add(i);
-            }
-            i++;
-        }
-
-        assertThat(bulkRequestModifier.getBulkRequest().requests().size(), equalTo(numRequests - failedSlots.size()));
-        // simulate that we actually executed the modified bulk request:
-        ActionListener<BulkResponse> result = bulkRequestModifier.wrapActionListenerIfNeeded(actionListener);
-        result.onResponse(new BulkResponse(new BulkItemResponse[numRequests - failedSlots.size()], 0));
-
-        BulkResponse bulkResponse = actionListener.getResponse();
-        for (int j = 0; j < bulkResponse.getItems().length; j++) {
-            if (failedSlots.contains(j)) {
-                BulkItemResponse item =  bulkResponse.getItems()[j];
-                assertThat(item.isFailed(), is(true));
-                assertThat(item.getFailure().getIndex(), equalTo("_index"));
-                assertThat(item.getFailure().getType(), equalTo("_type"));
-                assertThat(item.getFailure().getId(), equalTo(String.valueOf(j)));
-                assertThat(item.getFailure().getMessage(), equalTo("java.lang.RuntimeException"));
-            } else {
-                assertThat(bulkResponse.getItems()[j], nullValue());
-            }
-        }
-    }
-
-    public void testPipelineFailures() {
-        BulkRequest originalBulkRequest = new BulkRequest();
-        for (int i = 0; i < 32; i++) {
-            originalBulkRequest.add(new IndexRequest("index", "type", String.valueOf(i)));
-        }
-
-        IngestActionFilter.BulkRequestModifier modifier = new IngestActionFilter.BulkRequestModifier(originalBulkRequest);
-        for (int i = 0; modifier.hasNext(); i++) {
-            modifier.next();
-            if (i % 2 == 0) {
-                modifier.markCurrentItemAsFailed(new RuntimeException());
-            }
-        }
-
-        // So half of the requests have "failed", so only the successful requests are left:
-        BulkRequest bulkRequest = modifier.getBulkRequest();
-        assertThat(bulkRequest.requests().size(), Matchers.equalTo(16));
-
-        List<BulkItemResponse> responses = new ArrayList<>();
-        ActionListener<BulkResponse> bulkResponseListener = modifier.wrapActionListenerIfNeeded(new ActionListener<BulkResponse>() {
-            @Override
-            public void onResponse(BulkResponse bulkItemResponses) {
-                responses.addAll(Arrays.asList(bulkItemResponses.getItems()));
-            }
-
-            @Override
-            public void onFailure(Throwable e) {
-            }
-        });
-
-        List<BulkItemResponse> originalResponses = new ArrayList<>();
-        for (ActionRequest actionRequest : bulkRequest.requests()) {
-            IndexRequest indexRequest = (IndexRequest) actionRequest;
-            IndexResponse indexResponse = new IndexResponse(new ShardId("index", 0), indexRequest.type(), indexRequest.id(), 1, true);
-            originalResponses.add(new BulkItemResponse(Integer.parseInt(indexRequest.id()), indexRequest.opType().lowercase(), indexResponse));
-        }
-        bulkResponseListener.onResponse(new BulkResponse(originalResponses.toArray(new BulkItemResponse[originalResponses.size()]), 0));
-
-        assertThat(responses.size(), Matchers.equalTo(32));
-        for (int i = 0; i < 32; i++) {
-            assertThat(responses.get(i).getId(), Matchers.equalTo(String.valueOf(i)));
-        }
-    }
-
-    public void testNoFailures() {
-        BulkRequest originalBulkRequest = new BulkRequest();
-        for (int i = 0; i < 32; i++) {
-            originalBulkRequest.add(new IndexRequest("index", "type", String.valueOf(i)));
-        }
-
-        IngestActionFilter.BulkRequestModifier modifier = new IngestActionFilter.BulkRequestModifier(originalBulkRequest);
-        while (modifier.hasNext()) {
-            modifier.next();
-        }
-
-        BulkRequest bulkRequest = modifier.getBulkRequest();
-        assertThat(bulkRequest, Matchers.sameInstance(originalBulkRequest));
-        @SuppressWarnings("unchecked")
-        ActionListener<BulkResponse> actionListener = mock(ActionListener.class);
-        assertThat(modifier.wrapActionListenerIfNeeded(actionListener), Matchers.sameInstance(actionListener));
-    }
-
-    private static class CaptureActionListener implements ActionListener<BulkResponse> {
-
-        private BulkResponse response;
-
-        @Override
-        public void onResponse(BulkResponse bulkItemResponses) {
-            this.response = bulkItemResponses ;
-        }
-
-        @Override
-        public void onFailure(Throwable e) {
-        }
-
-        public BulkResponse getResponse() {
-            return response;
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/IngestActionFilterTests.java b/core/src/test/java/org/elasticsearch/action/ingest/IngestActionFilterTests.java
deleted file mode 100644
index 91c6765..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/IngestActionFilterTests.java
+++ /dev/null
@@ -1,239 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.bulk.BulkAction;
-import org.elasticsearch.action.bulk.BulkRequest;
-import org.elasticsearch.action.delete.DeleteRequest;
-import org.elasticsearch.action.index.IndexAction;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.support.ActionFilterChain;
-import org.elasticsearch.action.update.UpdateRequest;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.IngestBootstrapper;
-import org.elasticsearch.ingest.PipelineExecutionService;
-import org.elasticsearch.ingest.PipelineStore;
-import org.elasticsearch.ingest.core.CompoundProcessor;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.tasks.Task;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.junit.Before;
-import org.mockito.stubbing.Answer;
-
-import java.util.function.Consumer;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.nullValue;
-import static org.mockito.Matchers.same;
-import static org.mockito.Mockito.any;
-import static org.mockito.Mockito.doAnswer;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.times;
-import static org.mockito.Mockito.verify;
-import static org.mockito.Mockito.verifyZeroInteractions;
-import static org.mockito.Mockito.when;
-
-public class IngestActionFilterTests extends ESTestCase {
-
-    private IngestActionFilter filter;
-    private PipelineExecutionService executionService;
-
-    @Before
-    public void setup() {
-        executionService = mock(PipelineExecutionService.class);
-        IngestBootstrapper bootstrapper = mock(IngestBootstrapper.class);
-        when(bootstrapper.getPipelineExecutionService()).thenReturn(executionService);
-        filter = new IngestActionFilter(Settings.EMPTY, bootstrapper);
-    }
-
-    public void testApplyNoPipelineId() throws Exception {
-        IndexRequest indexRequest = new IndexRequest();
-        Task task = mock(Task.class);
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
-
-        verify(actionFilterChain).proceed(task, IndexAction.NAME, indexRequest, actionListener);
-        verifyZeroInteractions(executionService, actionFilterChain);
-    }
-
-    public void testApplyBulkNoPipelineId() throws Exception {
-        BulkRequest bulkRequest = new BulkRequest();
-        bulkRequest.add(new IndexRequest());
-        Task task = mock(Task.class);
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        filter.apply(task, BulkAction.NAME, bulkRequest, actionListener, actionFilterChain);
-
-        verify(actionFilterChain).proceed(task, BulkAction.NAME, bulkRequest, actionListener);
-        verifyZeroInteractions(executionService, actionFilterChain);
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testApplyIngestIdViaRequestParam() throws Exception {
-        Task task = mock(Task.class);
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").pipeline("_id");
-        indexRequest.source("field", "value");
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
-
-        verify(executionService).execute(same(indexRequest), any(Consumer.class), any(Consumer.class));
-        verifyZeroInteractions(actionFilterChain);
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testApplyExecuted() throws Exception {
-        Task task = mock(Task.class);
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").pipeline("_id");
-        indexRequest.source("field", "value");
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        Answer answer = invocationOnMock -> {
-            @SuppressWarnings("unchecked")
-            Consumer<Boolean> listener = (Consumer) invocationOnMock.getArguments()[2];
-            listener.accept(true);
-            return null;
-        };
-        doAnswer(answer).when(executionService).execute(any(IndexRequest.class), any(Consumer.class), any(Consumer.class));
-        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
-
-        verify(executionService).execute(same(indexRequest), any(Consumer.class), any(Consumer.class));
-        verify(actionFilterChain).proceed(task, IndexAction.NAME, indexRequest, actionListener);
-        verifyZeroInteractions(actionListener);
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testApplyFailed() throws Exception {
-        Task task = mock(Task.class);
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").pipeline("_id");
-        indexRequest.source("field", "value");
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        RuntimeException exception = new RuntimeException();
-        Answer answer = invocationOnMock -> {
-            Consumer<Throwable> handler = (Consumer) invocationOnMock.getArguments()[1];
-            handler.accept(exception);
-            return null;
-        };
-        doAnswer(answer).when(executionService).execute(same(indexRequest), any(Consumer.class), any(Consumer.class));
-        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
-
-        verify(executionService).execute(same(indexRequest), any(Consumer.class), any(Consumer.class));
-        verify(actionListener).onFailure(exception);
-        verifyZeroInteractions(actionFilterChain);
-    }
-
-    public void testApplyWithBulkRequest() throws Exception {
-        Task task = mock(Task.class);
-        ThreadPool threadPool = mock(ThreadPool.class);
-        when(threadPool.executor(any())).thenReturn(Runnable::run);
-        PipelineStore store = mock(PipelineStore.class);
-
-        Processor processor = new Processor() {
-            @Override
-            public void execute(IngestDocument ingestDocument) {
-                ingestDocument.setFieldValue("field2", "value2");
-            }
-
-            @Override
-            public String getType() {
-                return null;
-            }
-        };
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", new CompoundProcessor(processor)));
-        executionService = new PipelineExecutionService(store, threadPool);
-        IngestBootstrapper bootstrapper = mock(IngestBootstrapper.class);
-        when(bootstrapper.getPipelineExecutionService()).thenReturn(executionService);
-        filter = new IngestActionFilter(Settings.EMPTY, bootstrapper);
-
-        BulkRequest bulkRequest = new BulkRequest();
-        int numRequest = scaledRandomIntBetween(8, 64);
-        for (int i = 0; i < numRequest; i++) {
-            if (rarely()) {
-                ActionRequest request;
-                if (randomBoolean()) {
-                    request = new DeleteRequest("_index", "_type", "_id");
-                } else {
-                    request = new UpdateRequest("_index", "_type", "_id");
-                }
-                bulkRequest.add(request);
-            } else {
-                IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").pipeline("_id");
-                indexRequest.source("field1", "value1");
-                bulkRequest.add(indexRequest);
-            }
-        }
-
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        filter.apply(task, BulkAction.NAME, bulkRequest, actionListener, actionFilterChain);
-
-        assertBusy(() -> {
-            verify(actionFilterChain).proceed(task, BulkAction.NAME, bulkRequest, actionListener);
-            verifyZeroInteractions(actionListener);
-
-            int assertedRequests = 0;
-            for (ActionRequest actionRequest : bulkRequest.requests()) {
-                if (actionRequest instanceof IndexRequest) {
-                    IndexRequest indexRequest = (IndexRequest) actionRequest;
-                    assertThat(indexRequest.sourceAsMap().size(), equalTo(2));
-                    assertThat(indexRequest.sourceAsMap().get("field1"), equalTo("value1"));
-                    assertThat(indexRequest.sourceAsMap().get("field2"), equalTo("value2"));
-                }
-                assertedRequests++;
-            }
-            assertThat(assertedRequests, equalTo(numRequest));
-        });
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testIndexApiSinglePipelineExecution() {
-        Answer answer = invocationOnMock -> {
-            @SuppressWarnings("unchecked")
-            Consumer<Boolean> listener = (Consumer) invocationOnMock.getArguments()[2];
-            listener.accept(true);
-            return null;
-        };
-        doAnswer(answer).when(executionService).execute(any(IndexRequest.class), any(Consumer.class), any(Consumer.class));
-
-        Task task = mock(Task.class);
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").pipeline("_id").source("field", "value");
-        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
-        assertThat(indexRequest.pipeline(), nullValue());
-        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
-        verify(executionService, times(1)).execute(same(indexRequest), any(Consumer.class), any(Consumer.class));
-        verify(actionFilterChain, times(2)).proceed(task, IndexAction.NAME, indexRequest, actionListener);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/ReloadPipelinesActionTests.java b/core/src/test/java/org/elasticsearch/action/ingest/ReloadPipelinesActionTests.java
deleted file mode 100644
index 8a0284d..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/ReloadPipelinesActionTests.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.transport.LocalTransportAddress;
-import org.elasticsearch.ingest.PipelineStore;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.transport.TransportException;
-import org.elasticsearch.transport.TransportResponseHandler;
-import org.elasticsearch.transport.TransportService;
-import org.junit.Before;
-import org.mockito.Matchers;
-
-import java.util.Collections;
-
-import static org.hamcrest.CoreMatchers.is;
-import static org.mockito.Mockito.doAnswer;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-
-public class ReloadPipelinesActionTests extends ESTestCase {
-
-    private ClusterService clusterService;
-    private TransportService transportService;
-    private ReloadPipelinesAction reloadPipelinesAction;
-
-    @Before
-    public void init() {
-        Settings settings = Settings.EMPTY;
-        PipelineStore pipelineStore = mock(PipelineStore.class);
-        clusterService = mock(ClusterService.class);
-        transportService = mock(TransportService.class);
-        reloadPipelinesAction = new ReloadPipelinesAction(settings, pipelineStore, clusterService, transportService);
-    }
-
-    public void testSuccess() {
-        int numNodes = randomIntBetween(1, 10);
-        ClusterState state = ClusterState.builder(new ClusterName("_name")).nodes(generateDiscoNodes(numNodes)).build();
-        when(clusterService.state()).thenReturn(state);
-
-        doAnswer(mock -> {
-            TransportResponseHandler handler = (TransportResponseHandler) mock.getArguments()[3];
-            for (int i = 0; i < numNodes; i++) {
-                handler.handleResponse(new ReloadPipelinesAction.ReloadPipelinesResponse());
-            }
-            return mock;
-        }).when(transportService).sendRequest(Matchers.any(), Matchers.eq(ReloadPipelinesAction.ACTION_NAME), Matchers.any(), Matchers.any());
-        reloadPipelinesAction.reloadPipelinesOnAllNodes(result -> assertThat(result, is(true)));
-    }
-
-    public void testWithAtLeastOneFailure() {
-        int numNodes = randomIntBetween(1, 10);
-
-        ClusterState state = ClusterState.builder(new ClusterName("_name")).nodes(generateDiscoNodes(numNodes)).build();
-        when(clusterService.state()).thenReturn(state);
-
-        doAnswer(mock -> {
-            TransportResponseHandler handler = (TransportResponseHandler) mock.getArguments()[3];
-            handler.handleException(new TransportException("test failure"));
-            for (int i = 1; i < numNodes; i++) {
-                if (randomBoolean()) {
-                    handler.handleResponse(new ReloadPipelinesAction.ReloadPipelinesResponse());
-                } else {
-                    handler.handleException(new TransportException("test failure"));
-                }
-            }
-            return mock;
-        }).when(transportService).sendRequest(Matchers.any(), Matchers.eq(ReloadPipelinesAction.ACTION_NAME), Matchers.any(), Matchers.any());
-        reloadPipelinesAction.reloadPipelinesOnAllNodes(result -> assertThat(result, is(false)));
-    }
-
-    private static DiscoveryNodes.Builder generateDiscoNodes(int numNodes) {
-        DiscoveryNodes.Builder discoNodes = DiscoveryNodes.builder();
-        for (int i = 0; i < numNodes; i++) {
-            String id = Integer.toString(i);
-            DiscoveryNode discoNode = new DiscoveryNode(id, id, new LocalTransportAddress(id), Collections.emptyMap(), Version.CURRENT);
-            discoNodes.put(discoNode);
-        }
-        return discoNodes;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/SimulateDocumentSimpleResultTests.java b/core/src/test/java/org/elasticsearch/action/ingest/SimulateDocumentSimpleResultTests.java
deleted file mode 100644
index dc8f7fc..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/SimulateDocumentSimpleResultTests.java
+++ /dev/null
@@ -1,58 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ingest.SimulateDocumentSimpleResult;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SimulateDocumentSimpleResultTests extends ESTestCase {
-
-    public void testSerialization() throws IOException {
-        boolean isFailure = randomBoolean();
-        SimulateDocumentSimpleResult simulateDocumentSimpleResult;
-        if (isFailure) {
-            simulateDocumentSimpleResult = new SimulateDocumentSimpleResult(new IllegalArgumentException("test"));
-        } else {
-            IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-            simulateDocumentSimpleResult = new SimulateDocumentSimpleResult(ingestDocument);
-        }
-
-        BytesStreamOutput out = new BytesStreamOutput();
-        simulateDocumentSimpleResult.writeTo(out);
-        StreamInput streamInput = StreamInput.wrap(out.bytes());
-        SimulateDocumentSimpleResult otherSimulateDocumentSimpleResult = SimulateDocumentSimpleResult.readSimulateDocumentSimpleResult(streamInput);
-
-        assertThat(otherSimulateDocumentSimpleResult.getIngestDocument(), equalTo(simulateDocumentSimpleResult.getIngestDocument()));
-        if (isFailure) {
-            assertThat(otherSimulateDocumentSimpleResult.getFailure(), instanceOf(IllegalArgumentException.class));
-            IllegalArgumentException e = (IllegalArgumentException) otherSimulateDocumentSimpleResult.getFailure();
-            assertThat(e.getMessage(), equalTo("test"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java b/core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java
deleted file mode 100644
index e292b45..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java
+++ /dev/null
@@ -1,132 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ingest.SimulateDocumentResult;
-import org.elasticsearch.action.ingest.SimulateDocumentSimpleResult;
-import org.elasticsearch.action.ingest.SimulateDocumentVerboseResult;
-import org.elasticsearch.action.ingest.SimulateExecutionService;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.TestProcessor;
-import org.elasticsearch.ingest.core.CompoundProcessor;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.junit.After;
-import org.junit.Before;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.nullValue;
-import static org.hamcrest.Matchers.sameInstance;
-
-public class SimulateExecutionServiceTests extends ESTestCase {
-
-    private ThreadPool threadPool;
-    private SimulateExecutionService executionService;
-    private IngestDocument ingestDocument;
-
-    @Before
-    public void setup() {
-        threadPool = new ThreadPool(
-                Settings.builder()
-                        .put("name", getClass().getName())
-                        .build()
-        );
-        executionService = new SimulateExecutionService(threadPool);
-        ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-    }
-
-    @After
-    public void destroy() {
-        threadPool.shutdown();
-    }
-
-    public void testExecuteVerboseItem() throws Exception {
-        TestProcessor processor = new TestProcessor("mock", ingestDocument -> {});
-        Pipeline pipeline = new Pipeline("_id", "_description", new CompoundProcessor(processor, processor));
-        SimulateDocumentResult actualItemResponse = executionService.executeDocument(pipeline, ingestDocument, true);
-        assertThat(processor.getInvokedCounter(), equalTo(2));
-        assertThat(actualItemResponse, instanceOf(SimulateDocumentVerboseResult.class));
-        SimulateDocumentVerboseResult simulateDocumentVerboseResult = (SimulateDocumentVerboseResult) actualItemResponse;
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().size(), equalTo(2));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getProcessorId(), equalTo("processor[mock]-0"));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument(), not(sameInstance(ingestDocument)));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument(), equalTo(ingestDocument));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument().getSourceAndMetadata(), not(sameInstance(ingestDocument.getSourceAndMetadata())));
-
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getFailure(), nullValue());
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getProcessorId(), equalTo("processor[mock]-1"));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument(), not(sameInstance(ingestDocument)));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument(), equalTo(ingestDocument));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument().getSourceAndMetadata(), not(sameInstance(ingestDocument.getSourceAndMetadata())));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument().getSourceAndMetadata(),
-            not(sameInstance(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument().getSourceAndMetadata())));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getFailure(), nullValue());
-    }
-
-    public void testExecuteItem() throws Exception {
-        TestProcessor processor = new TestProcessor("mock", ingestDocument -> {});
-        Pipeline pipeline = new Pipeline("_id", "_description", new CompoundProcessor(processor, processor));
-        SimulateDocumentResult actualItemResponse = executionService.executeDocument(pipeline, ingestDocument, false);
-        assertThat(processor.getInvokedCounter(), equalTo(2));
-        assertThat(actualItemResponse, instanceOf(SimulateDocumentSimpleResult.class));
-        SimulateDocumentSimpleResult simulateDocumentSimpleResult = (SimulateDocumentSimpleResult) actualItemResponse;
-        assertThat(simulateDocumentSimpleResult.getIngestDocument(), equalTo(ingestDocument));
-        assertThat(simulateDocumentSimpleResult.getFailure(), nullValue());
-    }
-
-    public void testExecuteVerboseItemWithFailure() throws Exception {
-        TestProcessor processor1 = new TestProcessor("mock", ingestDocument -> { throw new RuntimeException("processor failed"); });
-        TestProcessor processor2 = new TestProcessor("mock", ingestDocument -> {});
-        Pipeline pipeline = new Pipeline("_id", "_description", new CompoundProcessor(processor1, processor2));
-        SimulateDocumentResult actualItemResponse = executionService.executeDocument(pipeline, ingestDocument, true);
-        assertThat(processor1.getInvokedCounter(), equalTo(1));
-        assertThat(processor2.getInvokedCounter(), equalTo(1));
-        assertThat(actualItemResponse, instanceOf(SimulateDocumentVerboseResult.class));
-        SimulateDocumentVerboseResult simulateDocumentVerboseResult = (SimulateDocumentVerboseResult) actualItemResponse;
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().size(), equalTo(2));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getProcessorId(), equalTo("processor[mock]-0"));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument(), nullValue());
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getFailure(), instanceOf(RuntimeException.class));
-        RuntimeException runtimeException = (RuntimeException) simulateDocumentVerboseResult.getProcessorResults().get(0).getFailure();
-        assertThat(runtimeException.getMessage(), equalTo("processor failed"));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getProcessorId(), equalTo("processor[mock]-1"));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument(), not(sameInstance(ingestDocument)));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument(), equalTo(ingestDocument));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getFailure(), nullValue());
-    }
-
-    public void testExecuteItemWithFailure() throws Exception {
-        TestProcessor processor = new TestProcessor(ingestDocument -> { throw new RuntimeException("processor failed"); });
-        Pipeline pipeline = new Pipeline("_id", "_description", new CompoundProcessor(processor, processor));
-        SimulateDocumentResult actualItemResponse = executionService.executeDocument(pipeline, ingestDocument, false);
-        assertThat(processor.getInvokedCounter(), equalTo(1));
-        assertThat(actualItemResponse, instanceOf(SimulateDocumentSimpleResult.class));
-        SimulateDocumentSimpleResult simulateDocumentSimpleResult = (SimulateDocumentSimpleResult) actualItemResponse;
-        assertThat(simulateDocumentSimpleResult.getIngestDocument(), nullValue());
-        assertThat(simulateDocumentSimpleResult.getFailure(), instanceOf(RuntimeException.class));
-        RuntimeException runtimeException = (RuntimeException) simulateDocumentSimpleResult.getFailure();
-        assertThat(runtimeException.getMessage(), equalTo("processor failed"));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineRequestParsingTests.java b/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineRequestParsingTests.java
deleted file mode 100644
index eabc182..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineRequestParsingTests.java
+++ /dev/null
@@ -1,182 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ingest.SimulatePipelineRequest;
-import org.elasticsearch.ingest.PipelineStore;
-import org.elasticsearch.ingest.TestProcessor;
-import org.elasticsearch.ingest.core.CompoundProcessor;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-import static org.elasticsearch.action.ingest.SimulatePipelineRequest.Fields;
-import static org.elasticsearch.ingest.core.IngestDocument.MetaData.ID;
-import static org.elasticsearch.ingest.core.IngestDocument.MetaData.INDEX;
-import static org.elasticsearch.ingest.core.IngestDocument.MetaData.TYPE;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.nullValue;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-
-public class SimulatePipelineRequestParsingTests extends ESTestCase {
-
-    private PipelineStore store;
-
-    @Before
-    public void init() throws IOException {
-        TestProcessor processor = new TestProcessor(ingestDocument -> {});
-        CompoundProcessor pipelineCompoundProcessor = new CompoundProcessor(processor);
-        Pipeline pipeline = new Pipeline(SimulatePipelineRequest.SIMULATED_PIPELINE_ID, null, pipelineCompoundProcessor);
-        Map<String, Processor.Factory> processorRegistry = new HashMap<>();
-        processorRegistry.put("mock_processor", mock(Processor.Factory.class));
-        store = mock(PipelineStore.class);
-        when(store.get(SimulatePipelineRequest.SIMULATED_PIPELINE_ID)).thenReturn(pipeline);
-        when(store.getProcessorFactoryRegistry()).thenReturn(processorRegistry);
-    }
-
-    public void testParseUsingPipelineStore() throws Exception {
-        int numDocs = randomIntBetween(1, 10);
-
-        Map<String, Object> requestContent = new HashMap<>();
-        List<Map<String, Object>> docs = new ArrayList<>();
-        List<Map<String, Object>> expectedDocs = new ArrayList<>();
-        requestContent.put(Fields.DOCS, docs);
-        for (int i = 0; i < numDocs; i++) {
-            Map<String, Object> doc = new HashMap<>();
-            String index = randomAsciiOfLengthBetween(1, 10);
-            String type = randomAsciiOfLengthBetween(1, 10);
-            String id = randomAsciiOfLengthBetween(1, 10);
-            doc.put(INDEX.getFieldName(), index);
-            doc.put(TYPE.getFieldName(), type);
-            doc.put(ID.getFieldName(), id);
-            String fieldName = randomAsciiOfLengthBetween(1, 10);
-            String fieldValue = randomAsciiOfLengthBetween(1, 10);
-            doc.put(Fields.SOURCE, Collections.singletonMap(fieldName, fieldValue));
-            docs.add(doc);
-            Map<String, Object> expectedDoc = new HashMap<>();
-            expectedDoc.put(INDEX.getFieldName(), index);
-            expectedDoc.put(TYPE.getFieldName(), type);
-            expectedDoc.put(ID.getFieldName(), id);
-            expectedDoc.put(Fields.SOURCE, Collections.singletonMap(fieldName, fieldValue));
-            expectedDocs.add(expectedDoc);
-        }
-
-        SimulatePipelineRequest.Parsed actualRequest = SimulatePipelineRequest.parseWithPipelineId(SimulatePipelineRequest.SIMULATED_PIPELINE_ID, requestContent, false, store);
-        assertThat(actualRequest.isVerbose(), equalTo(false));
-        assertThat(actualRequest.getDocuments().size(), equalTo(numDocs));
-        Iterator<Map<String, Object>> expectedDocsIterator = expectedDocs.iterator();
-        for (IngestDocument ingestDocument : actualRequest.getDocuments()) {
-            Map<String, Object> expectedDocument = expectedDocsIterator.next();
-            Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
-            assertThat(metadataMap.get(INDEX), equalTo(expectedDocument.get(INDEX.getFieldName())));
-            assertThat(metadataMap.get(TYPE), equalTo(expectedDocument.get(TYPE.getFieldName())));
-            assertThat(metadataMap.get(ID), equalTo(expectedDocument.get(ID.getFieldName())));
-            assertThat(ingestDocument.getSourceAndMetadata(), equalTo(expectedDocument.get(Fields.SOURCE)));
-        }
-
-        assertThat(actualRequest.getPipeline().getId(), equalTo(SimulatePipelineRequest.SIMULATED_PIPELINE_ID));
-        assertThat(actualRequest.getPipeline().getDescription(), nullValue());
-        assertThat(actualRequest.getPipeline().getProcessors().size(), equalTo(1));
-    }
-
-    public void testParseWithProvidedPipeline() throws Exception {
-        int numDocs = randomIntBetween(1, 10);
-
-        Map<String, Object> requestContent = new HashMap<>();
-        List<Map<String, Object>> docs = new ArrayList<>();
-        List<Map<String, Object>> expectedDocs = new ArrayList<>();
-        requestContent.put(Fields.DOCS, docs);
-        for (int i = 0; i < numDocs; i++) {
-            Map<String, Object> doc = new HashMap<>();
-            String index = randomAsciiOfLengthBetween(1, 10);
-            String type = randomAsciiOfLengthBetween(1, 10);
-            String id = randomAsciiOfLengthBetween(1, 10);
-            doc.put(INDEX.getFieldName(), index);
-            doc.put(TYPE.getFieldName(), type);
-            doc.put(ID.getFieldName(), id);
-            String fieldName = randomAsciiOfLengthBetween(1, 10);
-            String fieldValue = randomAsciiOfLengthBetween(1, 10);
-            doc.put(Fields.SOURCE, Collections.singletonMap(fieldName, fieldValue));
-            docs.add(doc);
-            Map<String, Object> expectedDoc = new HashMap<>();
-            expectedDoc.put(INDEX.getFieldName(), index);
-            expectedDoc.put(TYPE.getFieldName(), type);
-            expectedDoc.put(ID.getFieldName(), id);
-            expectedDoc.put(Fields.SOURCE, Collections.singletonMap(fieldName, fieldValue));
-            expectedDocs.add(expectedDoc);
-        }
-
-        Map<String, Object> pipelineConfig = new HashMap<>();
-        List<Map<String, Object>> processors = new ArrayList<>();
-        int numProcessors = randomIntBetween(1, 10);
-        for (int i = 0; i < numProcessors; i++) {
-            Map<String, Object> processorConfig = new HashMap<>();
-            List<Map<String, Object>> onFailureProcessors = new ArrayList<>();
-            int numOnFailureProcessors = randomIntBetween(0, 1);
-            for (int j = 0; j < numOnFailureProcessors; j++) {
-                onFailureProcessors.add(Collections.singletonMap("mock_processor", Collections.emptyMap()));
-            }
-            if (numOnFailureProcessors > 0) {
-                processorConfig.put("on_failure", onFailureProcessors);
-            }
-            processors.add(Collections.singletonMap("mock_processor", processorConfig));
-        }
-        pipelineConfig.put("processors", processors);
-
-        List<Map<String, Object>> onFailureProcessors = new ArrayList<>();
-        int numOnFailureProcessors = randomIntBetween(0, 1);
-        for (int i = 0; i < numOnFailureProcessors; i++) {
-            onFailureProcessors.add(Collections.singletonMap("mock_processor", Collections.emptyMap()));
-        }
-        if (numOnFailureProcessors > 0) {
-            pipelineConfig.put("on_failure", onFailureProcessors);
-        }
-
-        requestContent.put(Fields.PIPELINE, pipelineConfig);
-
-        SimulatePipelineRequest.Parsed actualRequest = SimulatePipelineRequest.parse(requestContent, false, store);
-        assertThat(actualRequest.isVerbose(), equalTo(false));
-        assertThat(actualRequest.getDocuments().size(), equalTo(numDocs));
-        Iterator<Map<String, Object>> expectedDocsIterator = expectedDocs.iterator();
-        for (IngestDocument ingestDocument : actualRequest.getDocuments()) {
-            Map<String, Object> expectedDocument = expectedDocsIterator.next();
-            Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
-            assertThat(metadataMap.get(INDEX), equalTo(expectedDocument.get(INDEX.getFieldName())));
-            assertThat(metadataMap.get(TYPE), equalTo(expectedDocument.get(TYPE.getFieldName())));
-            assertThat(metadataMap.get(ID), equalTo(expectedDocument.get(ID.getFieldName())));
-            assertThat(ingestDocument.getSourceAndMetadata(), equalTo(expectedDocument.get(Fields.SOURCE)));
-        }
-
-        assertThat(actualRequest.getPipeline().getId(), equalTo(SimulatePipelineRequest.SIMULATED_PIPELINE_ID));
-        assertThat(actualRequest.getPipeline().getDescription(), nullValue());
-        assertThat(actualRequest.getPipeline().getProcessors().size(), equalTo(numProcessors));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineResponseTests.java b/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineResponseTests.java
deleted file mode 100644
index 905baa8..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineResponseTests.java
+++ /dev/null
@@ -1,122 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ingest.SimulateDocumentResult;
-import org.elasticsearch.action.ingest.SimulateDocumentSimpleResult;
-import org.elasticsearch.action.ingest.SimulateDocumentVerboseResult;
-import org.elasticsearch.action.ingest.SimulatePipelineResponse;
-import org.elasticsearch.action.ingest.SimulateProcessorResult;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-public class SimulatePipelineResponseTests extends ESTestCase {
-
-    public void testSerialization() throws IOException {
-        boolean isVerbose = randomBoolean();
-        int numResults = randomIntBetween(1, 10);
-        List<SimulateDocumentResult> results = new ArrayList<>(numResults);
-        for (int i = 0; i < numResults; i++) {
-            boolean isFailure = randomBoolean();
-            IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-            if (isVerbose) {
-                int numProcessors = randomIntBetween(1, 10);
-                List<SimulateProcessorResult> processorResults = new ArrayList<>(numProcessors);
-                for (int j = 0; j < numProcessors; j++) {
-                    String processorId = randomAsciiOfLengthBetween(1, 10);
-                    SimulateProcessorResult processorResult;
-                    if (isFailure) {
-                        processorResult = new SimulateProcessorResult(processorId, new IllegalArgumentException("test"));
-                    } else {
-                        processorResult = new SimulateProcessorResult(processorId, ingestDocument);
-                    }
-                    processorResults.add(processorResult);
-                }
-                results.add(new SimulateDocumentVerboseResult(processorResults));
-            } else {
-                results.add(new SimulateDocumentSimpleResult(ingestDocument));
-                SimulateDocumentSimpleResult simulateDocumentSimpleResult;
-                if (isFailure) {
-                    simulateDocumentSimpleResult = new SimulateDocumentSimpleResult(new IllegalArgumentException("test"));
-                } else {
-                    simulateDocumentSimpleResult = new SimulateDocumentSimpleResult(ingestDocument);
-                }
-                results.add(simulateDocumentSimpleResult);
-            }
-        }
-
-        SimulatePipelineResponse response = new SimulatePipelineResponse(randomAsciiOfLengthBetween(1, 10), isVerbose, results);
-        BytesStreamOutput out = new BytesStreamOutput();
-        response.writeTo(out);
-        StreamInput streamInput = StreamInput.wrap(out.bytes());
-        SimulatePipelineResponse otherResponse = new SimulatePipelineResponse();
-        otherResponse.readFrom(streamInput);
-
-        assertThat(otherResponse.getPipelineId(), equalTo(response.getPipelineId()));
-        assertThat(otherResponse.getResults().size(), equalTo(response.getResults().size()));
-
-        Iterator<SimulateDocumentResult> expectedResultIterator = response.getResults().iterator();
-        for (SimulateDocumentResult result : otherResponse.getResults()) {
-            if (isVerbose) {
-                SimulateDocumentVerboseResult expectedSimulateDocumentVerboseResult = (SimulateDocumentVerboseResult) expectedResultIterator.next();
-                assertThat(result, instanceOf(SimulateDocumentVerboseResult.class));
-                SimulateDocumentVerboseResult simulateDocumentVerboseResult = (SimulateDocumentVerboseResult) result;
-                assertThat(simulateDocumentVerboseResult.getProcessorResults().size(), equalTo(expectedSimulateDocumentVerboseResult.getProcessorResults().size()));
-                Iterator<SimulateProcessorResult> expectedProcessorResultIterator = expectedSimulateDocumentVerboseResult.getProcessorResults().iterator();
-                for (SimulateProcessorResult simulateProcessorResult : simulateDocumentVerboseResult.getProcessorResults()) {
-                    SimulateProcessorResult expectedProcessorResult = expectedProcessorResultIterator.next();
-                    assertThat(simulateProcessorResult.getProcessorId(), equalTo(expectedProcessorResult.getProcessorId()));
-                    assertThat(simulateProcessorResult.getIngestDocument(), equalTo(expectedProcessorResult.getIngestDocument()));
-                    if (expectedProcessorResult.getFailure() == null) {
-                        assertThat(simulateProcessorResult.getFailure(), nullValue());
-                    } else {
-                        assertThat(simulateProcessorResult.getFailure(), instanceOf(IllegalArgumentException.class));
-                        IllegalArgumentException e = (IllegalArgumentException) simulateProcessorResult.getFailure();
-                        assertThat(e.getMessage(), equalTo("test"));
-                    }
-                }
-            } else {
-                SimulateDocumentSimpleResult expectedSimulateDocumentSimpleResult = (SimulateDocumentSimpleResult) expectedResultIterator.next();
-                assertThat(result, instanceOf(SimulateDocumentSimpleResult.class));
-                SimulateDocumentSimpleResult simulateDocumentSimpleResult = (SimulateDocumentSimpleResult) result;
-                assertThat(simulateDocumentSimpleResult.getIngestDocument(), equalTo(expectedSimulateDocumentSimpleResult.getIngestDocument()));
-                if (expectedSimulateDocumentSimpleResult.getFailure() == null) {
-                    assertThat(simulateDocumentSimpleResult.getFailure(), nullValue());
-                } else {
-                    assertThat(simulateDocumentSimpleResult.getFailure(), instanceOf(IllegalArgumentException.class));
-                    IllegalArgumentException e = (IllegalArgumentException) simulateDocumentSimpleResult.getFailure();
-                    assertThat(e.getMessage(), equalTo("test"));
-                }
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/SimulateProcessorResultTests.java b/core/src/test/java/org/elasticsearch/action/ingest/SimulateProcessorResultTests.java
deleted file mode 100644
index 208d253..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/SimulateProcessorResultTests.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ingest.SimulateProcessorResult;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-
-public class SimulateProcessorResultTests extends ESTestCase {
-
-    public void testSerialization() throws IOException {
-        String processorId = randomAsciiOfLengthBetween(1, 10);
-        boolean isFailure = randomBoolean();
-        SimulateProcessorResult simulateProcessorResult;
-        if (isFailure) {
-            simulateProcessorResult = new SimulateProcessorResult(processorId, new IllegalArgumentException("test"));
-        } else {
-            IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-            simulateProcessorResult = new SimulateProcessorResult(processorId, ingestDocument);
-        }
-
-        BytesStreamOutput out = new BytesStreamOutput();
-        simulateProcessorResult.writeTo(out);
-        StreamInput streamInput = StreamInput.wrap(out.bytes());
-        SimulateProcessorResult otherSimulateProcessorResult = SimulateProcessorResult.readSimulateProcessorResultFrom(streamInput);
-        assertThat(otherSimulateProcessorResult.getProcessorId(), equalTo(simulateProcessorResult.getProcessorId()));
-        assertThat(otherSimulateProcessorResult.getIngestDocument(), equalTo(simulateProcessorResult.getIngestDocument()));
-        if (isFailure) {
-            assertThat(otherSimulateProcessorResult.getFailure(), instanceOf(IllegalArgumentException.class));
-            IllegalArgumentException e = (IllegalArgumentException) otherSimulateProcessorResult.getFailure();
-            assertThat(e.getMessage(), equalTo("test"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/WriteableIngestDocumentTests.java b/core/src/test/java/org/elasticsearch/action/ingest/WriteableIngestDocumentTests.java
deleted file mode 100644
index 120825a..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/WriteableIngestDocumentTests.java
+++ /dev/null
@@ -1,114 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.not;
-
-public class WriteableIngestDocumentTests extends ESTestCase {
-
-    public void testEqualsAndHashcode() throws Exception {
-        Map<String, Object> sourceAndMetadata = RandomDocumentPicks.randomSource(random());
-        int numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
-        for (int i = 0; i < numFields; i++) {
-            sourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
-        }
-        Map<String, String> ingestMetadata = new HashMap<>();
-        numFields = randomIntBetween(1, 5);
-        for (int i = 0; i < numFields; i++) {
-            ingestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
-        }
-        WriteableIngestDocument ingestDocument = new WriteableIngestDocument(new IngestDocument(sourceAndMetadata, ingestMetadata));
-
-        boolean changed = false;
-        Map<String, Object> otherSourceAndMetadata;
-        if (randomBoolean()) {
-            otherSourceAndMetadata = RandomDocumentPicks.randomSource(random());
-            changed = true;
-        } else {
-            otherSourceAndMetadata = new HashMap<>(sourceAndMetadata);
-        }
-        if (randomBoolean()) {
-            numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
-            for (int i = 0; i < numFields; i++) {
-                otherSourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
-            }
-            changed = true;
-        }
-
-        Map<String, String> otherIngestMetadata;
-        if (randomBoolean()) {
-            otherIngestMetadata = new HashMap<>();
-            numFields = randomIntBetween(1, 5);
-            for (int i = 0; i < numFields; i++) {
-                otherIngestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
-            }
-            changed = true;
-        } else {
-            otherIngestMetadata = Collections.unmodifiableMap(ingestMetadata);
-        }
-
-        WriteableIngestDocument otherIngestDocument = new WriteableIngestDocument(new IngestDocument(otherSourceAndMetadata, otherIngestMetadata));
-        if (changed) {
-            assertThat(ingestDocument, not(equalTo(otherIngestDocument)));
-            assertThat(otherIngestDocument, not(equalTo(ingestDocument)));
-        } else {
-            assertThat(ingestDocument, equalTo(otherIngestDocument));
-            assertThat(otherIngestDocument, equalTo(ingestDocument));
-            assertThat(ingestDocument.hashCode(), equalTo(otherIngestDocument.hashCode()));
-            WriteableIngestDocument thirdIngestDocument = new WriteableIngestDocument(new IngestDocument(Collections.unmodifiableMap(sourceAndMetadata), Collections.unmodifiableMap(ingestMetadata)));
-            assertThat(thirdIngestDocument, equalTo(ingestDocument));
-            assertThat(ingestDocument, equalTo(thirdIngestDocument));
-            assertThat(ingestDocument.hashCode(), equalTo(thirdIngestDocument.hashCode()));
-        }
-    }
-
-    public void testSerialization() throws IOException {
-        Map<String, Object> sourceAndMetadata = RandomDocumentPicks.randomSource(random());
-        int numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
-        for (int i = 0; i < numFields; i++) {
-            sourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
-        }
-        Map<String, String> ingestMetadata = new HashMap<>();
-        numFields = randomIntBetween(1, 5);
-        for (int i = 0; i < numFields; i++) {
-            ingestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
-        }
-        Map<String, Object> document = RandomDocumentPicks.randomSource(random());
-        WriteableIngestDocument writeableIngestDocument = new WriteableIngestDocument(new IngestDocument(sourceAndMetadata, ingestMetadata));
-
-        BytesStreamOutput out = new BytesStreamOutput();
-        writeableIngestDocument.writeTo(out);
-        StreamInput streamInput = StreamInput.wrap(out.bytes());
-        WriteableIngestDocument otherWriteableIngestDocument = WriteableIngestDocument.readWriteableIngestDocumentFrom(streamInput);
-        assertThat(otherWriteableIngestDocument, equalTo(writeableIngestDocument));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolverTests.java b/core/src/test/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolverTests.java
index 7b8eb2e..d3b3122 100644
--- a/core/src/test/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolverTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolverTests.java
@@ -192,7 +192,7 @@ public class IndexNameExpressionResolverTests extends ESTestCase {
 
         context = new IndexNameExpressionResolver.Context(state, lenientExpand);
         results = indexNameExpressionResolver.concreteIndices(context, Strings.EMPTY_ARRAY);
-        assertEquals(4, results.length);
+        assertEquals(Arrays.toString(results), 4, results.length);
 
         context = new IndexNameExpressionResolver.Context(state, IndicesOptions.lenientExpandOpen());
         results = indexNameExpressionResolver.concreteIndices(context,  "foofoo*");
@@ -867,4 +867,37 @@ public class IndexNameExpressionResolverTests extends ESTestCase {
         }
         return mdBuilder.build();
     }
+
+    public void testFilterClosedIndicesOnAliases() {
+        MetaData.Builder mdBuilder = MetaData.builder()
+            .put(indexBuilder("test-0").state(State.OPEN).putAlias(AliasMetaData.builder("alias-0")))
+            .put(indexBuilder("test-1").state(IndexMetaData.State.CLOSE).putAlias(AliasMetaData.builder("alias-1")));
+        ClusterState state = ClusterState.builder(new ClusterName("_name")).metaData(mdBuilder).build();
+
+        IndexNameExpressionResolver.Context context = new IndexNameExpressionResolver.Context(state, IndicesOptions.lenientExpandOpen());
+        String[] strings = indexNameExpressionResolver.concreteIndices(context, "alias-*");
+        assertArrayEquals(new String[] {"test-0"}, strings);
+
+        context = new IndexNameExpressionResolver.Context(state, IndicesOptions.strictExpandOpen());
+        strings = indexNameExpressionResolver.concreteIndices(context, "alias-*");
+
+        assertArrayEquals(new String[] {"test-0"}, strings);
+    }
+
+    public void testFilteringAliases() {
+        MetaData.Builder mdBuilder = MetaData.builder()
+            .put(indexBuilder("test-0").state(State.OPEN).putAlias(AliasMetaData.builder("alias-0").filter("{ \"term\": \"foo\"}")))
+            .put(indexBuilder("test-1").state(State.OPEN).putAlias(AliasMetaData.builder("alias-1")));
+        ClusterState state = ClusterState.builder(new ClusterName("_name")).metaData(mdBuilder).build();
+
+        String[] strings = indexNameExpressionResolver.filteringAliases(state, "test-0", "alias-*");
+        assertArrayEquals(new String[] {"alias-0"}, strings);
+
+        // concrete index supersedes filtering alias
+        strings = indexNameExpressionResolver.filteringAliases(state, "test-0", "test-0,alias-*");
+        assertNull(strings);
+
+        strings = indexNameExpressionResolver.filteringAliases(state, "test-0", "test-*,alias-*");
+        assertNull(strings);
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataTests.java b/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataTests.java
index 91a421e..4076286 100644
--- a/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataTests.java
@@ -20,9 +20,15 @@
 package org.elasticsearch.cluster.metadata;
 
 import org.elasticsearch.Version;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.common.xcontent.json.JsonXContent;
 import org.elasticsearch.test.ESTestCase;
 
+import java.io.IOException;
+
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.is;
 
@@ -110,4 +116,36 @@ public class MetaDataTests extends ESTestCase {
             assertThat(ex.getMessage(), is("index/alias [alias2] provided with routing value [1,2] that resolved to several routing values, rejecting operation"));
         }
     }
+
+    public void testUnknownFieldClusterMetaData() throws IOException {
+        BytesReference metadata = JsonXContent.contentBuilder()
+            .startObject()
+                .startObject("meta-data")
+                    .field("random", "value")
+                .endObject()
+            .endObject().bytes();
+        XContentParser parser = JsonXContent.jsonXContent.createParser(metadata);
+        try {
+            MetaData.Builder.fromXContent(parser);
+            fail();
+        } catch (IllegalArgumentException e) {
+            assertEquals("Unexpected field [random]", e.getMessage());
+        }
+    }
+
+    public void testUnknownFieldIndexMetaData() throws IOException {
+        BytesReference metadata = JsonXContent.contentBuilder()
+            .startObject()
+                .startObject("index_name")
+                    .field("random", "value")
+                .endObject()
+            .endObject().bytes();
+        XContentParser parser = JsonXContent.jsonXContent.createParser(metadata);
+        try {
+            IndexMetaData.Builder.fromXContent(parser);
+            fail();
+        } catch (IllegalArgumentException e) {
+            assertEquals("Unexpected field [random]", e.getMessage());
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/metadata/WildcardExpressionResolverTests.java b/core/src/test/java/org/elasticsearch/cluster/metadata/WildcardExpressionResolverTests.java
index 324086a..d9cf9f0 100644
--- a/core/src/test/java/org/elasticsearch/cluster/metadata/WildcardExpressionResolverTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/metadata/WildcardExpressionResolverTests.java
@@ -59,7 +59,7 @@ public class WildcardExpressionResolverTests extends ESTestCase {
         IndexNameExpressionResolver.WildcardExpressionResolver resolver = new IndexNameExpressionResolver.WildcardExpressionResolver();
 
         IndexNameExpressionResolver.Context context = new IndexNameExpressionResolver.Context(state, IndicesOptions.lenientExpandOpen());
-        assertThat(newHashSet(resolver.resolve(context, Arrays.asList("testYY*", "alias*"))), equalTo(newHashSet("alias1", "alias2", "alias3", "testYYY")));
+        assertThat(newHashSet(resolver.resolve(context, Arrays.asList("testYY*", "alias*"))), equalTo(newHashSet("testXXX", "testXYY", "testYYY")));
         assertThat(newHashSet(resolver.resolve(context, Arrays.asList("-kuku"))), equalTo(newHashSet("testXXX", "testXYY", "testYYY")));
         assertThat(newHashSet(resolver.resolve(context, Arrays.asList("+test*", "-testYYY"))), equalTo(newHashSet("testXXX", "testXYY")));
         assertThat(newHashSet(resolver.resolve(context, Arrays.asList("+testX*", "+testYYY"))), equalTo(newHashSet("testXXX", "testXYY", "testYYY")));
diff --git a/core/src/test/java/org/elasticsearch/common/SearchScrollIteratorTests.java b/core/src/test/java/org/elasticsearch/common/SearchScrollIteratorTests.java
deleted file mode 100644
index 886d9b9..0000000
--- a/core/src/test/java/org/elasticsearch/common/SearchScrollIteratorTests.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common;
-
-import org.elasticsearch.action.search.SearchRequest;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.search.SearchHit;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-
-import static org.hamcrest.Matchers.equalTo;
-
-// Not a real unit tests with mocks, but with a single node, because we mock the scroll
-// search behaviour and it changes then this test will not catch this.
-public class SearchScrollIteratorTests extends ESSingleNodeTestCase {
-
-    public void testSearchScrollIterator() {
-        createIndex("index");
-        int numDocs = scaledRandomIntBetween(0, 128);
-        for (int i = 0; i < numDocs; i++) {
-            client().prepareIndex("index", "type", Integer.toString(i))
-                    .setSource("field", "value" + i)
-                    .get();
-        }
-        client().admin().indices().prepareRefresh().get();
-
-        int i = 0;
-        SearchRequest searchRequest = new SearchRequest("index");
-        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
-        // randomize size, because that also controls how many actual searches will happen:
-        sourceBuilder.size(scaledRandomIntBetween(1, 10));
-        searchRequest.source(sourceBuilder);
-        Iterable<SearchHit> hits = SearchScrollIterator.createIterator(client(), TimeValue.timeValueSeconds(10), searchRequest);
-        for (SearchHit hit : hits) {
-            assertThat(hit.getId(), equalTo(Integer.toString(i)));
-            assertThat(hit.getSource().get("field"), equalTo("value" + i));
-            i++;
-        }
-        assertThat(i, equalTo(numDocs));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java b/core/src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java
index fb3c021..d6abcfe 100644
--- a/core/src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java
+++ b/core/src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java
@@ -56,7 +56,6 @@ import java.util.List;
 import java.util.Map;
 
 import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
 import static org.hamcrest.Matchers.notNullValue;
 import static org.hamcrest.Matchers.nullValue;
 
@@ -292,7 +291,6 @@ public class VersionsTests extends ESTestCase {
         }
 
         iw.close();
-        assertThat(IndexWriter.isLocked(iw.getDirectory()), is(false));
         ir.close();
         dir.close();
     }
diff --git a/core/src/test/java/org/elasticsearch/index/IndexingSlowLogTests.java b/core/src/test/java/org/elasticsearch/index/IndexingSlowLogTests.java
new file mode 100644
index 0000000..e39c0a8
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/IndexingSlowLogTests.java
@@ -0,0 +1,54 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index;
+
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.StringField;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.xcontent.json.JsonXContent;
+import org.elasticsearch.index.IndexingSlowLog.SlowLogParsedDocumentPrinter;
+import org.elasticsearch.index.mapper.ParsedDocument;
+import org.elasticsearch.test.ESTestCase;
+
+import java.io.IOException;
+
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.not;
+
+public class IndexingSlowLogTests extends ESTestCase {
+    public void testSlowLogParsedDocumentPrinterSourceToLog() throws IOException {
+        BytesReference source = JsonXContent.contentBuilder().startObject().field("foo", "bar").endObject().bytes();
+        ParsedDocument pd = new ParsedDocument(new StringField("uid", "test:id", Store.YES), new IntField("version", 1, Store.YES), "id",
+                "test", null, 0, -1, null, source, null);
+
+        // Turning off document logging doesn't log source[]
+        SlowLogParsedDocumentPrinter p = new SlowLogParsedDocumentPrinter(pd, 10, true, 0);
+        assertThat(p.toString(), not(containsString("source[")));
+
+        // Turning on document logging logs the whole thing
+        p = new SlowLogParsedDocumentPrinter(pd, 10, true, Integer.MAX_VALUE);
+        assertThat(p.toString(), containsString("source[{\"foo\":\"bar\"}]"));
+
+        // And you can truncate the source
+        p = new SlowLogParsedDocumentPrinter(pd, 10, true, 3);
+        assertThat(p.toString(), containsString("source[{\"f]"));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/NGramTokenizerFactoryTests.java b/core/src/test/java/org/elasticsearch/index/analysis/NGramTokenizerFactoryTests.java
index 6da1a77..0cb3abb 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/NGramTokenizerFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/NGramTokenizerFactoryTests.java
@@ -23,11 +23,7 @@ import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter;
-import org.apache.lucene.analysis.ngram.EdgeNGramTokenizer;
 import org.apache.lucene.analysis.ngram.Lucene43EdgeNGramTokenFilter;
-import org.apache.lucene.analysis.ngram.Lucene43EdgeNGramTokenizer;
-import org.apache.lucene.analysis.ngram.Lucene43NGramTokenizer;
-import org.apache.lucene.analysis.ngram.NGramTokenizer;
 import org.apache.lucene.analysis.reverse.ReverseStringFilter;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -118,79 +114,6 @@ public class NGramTokenizerFactoryTests extends ESTokenStreamTestCase {
                 new String[] {" a", " a!"});
     }
 
-    public void testBackwardsCompatibilityEdgeNgramTokenizer() throws Exception {
-        int iters = scaledRandomIntBetween(20, 100);
-        final Index index = new Index("test");
-        final String name = "ngr";
-        for (int i = 0; i < iters; i++) {
-            Version v = randomVersion(random());
-            if (v.onOrAfter(Version.V_0_90_2)) {
-                Builder builder = newAnalysisSettingsBuilder().put("min_gram", 2).put("max_gram", 3).put("token_chars", "letter,digit");
-                boolean compatVersion = false;
-                if ((compatVersion = random().nextBoolean())) {
-                    builder.put("version", "4." + random().nextInt(3));
-                    builder.put("side", "back");
-                }
-                Settings settings = builder.build();
-                Settings indexSettings = newAnalysisSettingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, v.id).build();
-                Tokenizer edgeNGramTokenizer = new EdgeNGramTokenizerFactory(IndexSettingsModule.newIndexSettings(index, indexSettings), null, name, settings).create();
-                edgeNGramTokenizer.setReader(new StringReader("foo bar"));
-                if (compatVersion) {
-                    assertThat(edgeNGramTokenizer, instanceOf(Lucene43EdgeNGramTokenizer.class));
-                } else {
-                    assertThat(edgeNGramTokenizer, instanceOf(EdgeNGramTokenizer.class));
-                }
-
-            } else {
-                Settings settings = newAnalysisSettingsBuilder().put("min_gram", 2).put("max_gram", 3).put("side", "back").build();
-                Settings indexSettings = newAnalysisSettingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, v.id).build();
-                Tokenizer edgeNGramTokenizer = new EdgeNGramTokenizerFactory(IndexSettingsModule.newIndexSettings(index, indexSettings), null, name, settings).create();
-                edgeNGramTokenizer.setReader(new StringReader("foo bar"));
-                assertThat(edgeNGramTokenizer, instanceOf(Lucene43EdgeNGramTokenizer.class));
-            }
-        }
-        Settings settings = newAnalysisSettingsBuilder().put("min_gram", 2).put("max_gram", 3).put("side", "back").build();
-        Settings indexSettings = newAnalysisSettingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
-        try {
-            new EdgeNGramTokenizerFactory(IndexSettingsModule.newIndexSettings(index, indexSettings), null, name, settings).create();
-            fail("should fail side:back is not supported anymore");
-        } catch (IllegalArgumentException ex) {
-        }
-
-    }
-
-    public void testBackwardsCompatibilityNgramTokenizer() throws Exception {
-        int iters = scaledRandomIntBetween(20, 100);
-        for (int i = 0; i < iters; i++) {
-            final Index index = new Index("test");
-            final String name = "ngr";
-            Version v = randomVersion(random());
-            if (v.onOrAfter(Version.V_0_90_2)) {
-                Builder builder = newAnalysisSettingsBuilder().put("min_gram", 2).put("max_gram", 3).put("token_chars", "letter,digit");
-                boolean compatVersion = false;
-                if ((compatVersion = random().nextBoolean())) {
-                    builder.put("version", "4." + random().nextInt(3));
-                }
-                Settings settings = builder.build();
-                Settings indexSettings = newAnalysisSettingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, v.id).build();
-                Tokenizer nGramTokenizer = new NGramTokenizerFactory(IndexSettingsModule.newIndexSettings(index, indexSettings), null, name, settings).create();
-                nGramTokenizer.setReader(new StringReader("foo bar"));
-                if (compatVersion) {
-                    assertThat(nGramTokenizer, instanceOf(Lucene43NGramTokenizer.class));
-                } else {
-                    assertThat(nGramTokenizer, instanceOf(NGramTokenizer.class));
-                }
-
-            } else {
-                Settings settings = newAnalysisSettingsBuilder().put("min_gram", 2).put("max_gram", 3).build();
-                Settings indexSettings = newAnalysisSettingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, v.id).build();
-                Tokenizer nGramTokenizer = new NGramTokenizerFactory(IndexSettingsModule.newIndexSettings(index, indexSettings), null, name, settings).create();
-                nGramTokenizer.setReader(new StringReader("foo bar"));
-                assertThat(nGramTokenizer, instanceOf(Lucene43NGramTokenizer.class));
-            }
-        }
-    }
-
     public void testBackwardsCompatibilityEdgeNgramTokenFilter() throws Exception {
         int iters = scaledRandomIntBetween(20, 100);
         for (int i = 0; i < iters; i++) {
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerTests.java b/core/src/test/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerTests.java
index 297cab8..1a88fcb 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerTests.java
@@ -19,8 +19,6 @@
 package org.elasticsearch.index.analysis;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.compress.CompressedXContent;
@@ -32,15 +30,11 @@ import org.elasticsearch.indices.analysis.PreBuiltAnalyzers;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
 import java.util.Locale;
 
 import static org.elasticsearch.test.VersionUtils.randomVersion;
-import static org.hamcrest.Matchers.contains;
 import static org.hamcrest.Matchers.instanceOf;
 import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.not;
 
 /**
  *
@@ -54,76 +48,6 @@ public class PreBuiltAnalyzerTests extends ESSingleNodeTestCase {
         assertThat(currentDefaultAnalyzer, is(currentStandardAnalyzer));
     }
 
-    public void testThatDefaultAndStandardAnalyzerChangedIn10Beta1() throws IOException {
-        Analyzer currentStandardAnalyzer = PreBuiltAnalyzers.STANDARD.getAnalyzer(Version.V_1_0_0_Beta1);
-        Analyzer currentDefaultAnalyzer = PreBuiltAnalyzers.DEFAULT.getAnalyzer(Version.V_1_0_0_Beta1);
-
-        // special case, these two are the same instance
-        assertThat(currentDefaultAnalyzer, is(currentStandardAnalyzer));
-        PreBuiltAnalyzers.DEFAULT.getAnalyzer(Version.V_1_0_0_Beta1);
-        final int n = scaledRandomIntBetween(10, 100);
-        Version version = Version.CURRENT;
-        for(int i = 0; i < n; i++) {
-            if (version.equals(Version.V_1_0_0_Beta1)) {
-                assertThat(currentDefaultAnalyzer, is(PreBuiltAnalyzers.DEFAULT.getAnalyzer(version)));
-            } else {
-                assertThat(currentDefaultAnalyzer, not(is(PreBuiltAnalyzers.DEFAULT.getAnalyzer(version))));
-            }
-            Analyzer analyzer = PreBuiltAnalyzers.DEFAULT.getAnalyzer(version);
-            TokenStream ts = analyzer.tokenStream("foo", "This is it Dude");
-            ts.reset();
-            CharTermAttribute charTermAttribute = ts.addAttribute(CharTermAttribute.class);
-            List<String> list = new ArrayList<>();
-            while(ts.incrementToken()) {
-                list.add(charTermAttribute.toString());
-            }
-            if (version.onOrAfter(Version.V_1_0_0_Beta1)) {
-                assertThat(list.size(), is(4));
-                assertThat(list, contains("this", "is", "it", "dude"));
-
-            } else {
-                assertThat(list.size(), is(1));
-                assertThat(list, contains("dude"));
-            }
-            ts.close();
-            version = randomVersion(random());
-        }
-    }
-
-    public void testAnalyzerChangedIn10RC1() throws IOException {
-        Analyzer pattern = PreBuiltAnalyzers.PATTERN.getAnalyzer(Version.V_1_0_0_RC1);
-        Analyzer standardHtml = PreBuiltAnalyzers.STANDARD_HTML_STRIP.getAnalyzer(Version.V_1_0_0_RC1);
-        final int n = scaledRandomIntBetween(10, 100);
-        Version version = Version.CURRENT;
-        for(int i = 0; i < n; i++) {
-            if (version.equals(Version.V_1_0_0_RC1)) {
-                assertThat(pattern, is(PreBuiltAnalyzers.PATTERN.getAnalyzer(version)));
-                assertThat(standardHtml, is(PreBuiltAnalyzers.STANDARD_HTML_STRIP.getAnalyzer(version)));
-            } else {
-                assertThat(pattern, not(is(PreBuiltAnalyzers.DEFAULT.getAnalyzer(version))));
-                assertThat(standardHtml, not(is(PreBuiltAnalyzers.DEFAULT.getAnalyzer(version))));
-            }
-            Analyzer analyzer = randomBoolean() ? PreBuiltAnalyzers.PATTERN.getAnalyzer(version) :  PreBuiltAnalyzers.STANDARD_HTML_STRIP.getAnalyzer(version);
-            TokenStream ts = analyzer.tokenStream("foo", "This is it Dude");
-            ts.reset();
-            CharTermAttribute charTermAttribute = ts.addAttribute(CharTermAttribute.class);
-            List<String> list = new ArrayList<>();
-            while(ts.incrementToken()) {
-                list.add(charTermAttribute.toString());
-            }
-            if (version.onOrAfter(Version.V_1_0_0_RC1)) {
-                assertThat(list.toString(), list.size(), is(4));
-                assertThat(list, contains("this", "is", "it", "dude"));
-
-            } else {
-                assertThat(list.size(), is(1));
-                assertThat(list, contains("dude"));
-            }
-            ts.close();
-            version = randomVersion(random());
-        }
-    }
-
     public void testThatInstancesAreTheSameAlwaysForKeywordAnalyzer() {
         assertThat(PreBuiltAnalyzers.KEYWORD.getAnalyzer(Version.CURRENT),
                 is(PreBuiltAnalyzers.KEYWORD.getAnalyzer(Version.V_0_18_0)));
diff --git a/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTests.java b/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTests.java
index d2bf6be..69831d7 100644
--- a/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTests.java
+++ b/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTests.java
@@ -31,8 +31,11 @@ import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.LogByteSizeMergePolicy;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.join.BitSetProducer;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.BitSet;
@@ -89,7 +92,8 @@ public class BitSetFilterCacheTests extends ESTestCase {
         writer.addDocument(document);
         writer.commit();
 
-        IndexReader reader = DirectoryReader.open(writer, false);
+        DirectoryReader reader = DirectoryReader.open(writer, false);
+        reader = ElasticsearchDirectoryReader.wrap(reader, new ShardId(new Index("test"), 0));
         IndexSearcher searcher = new IndexSearcher(reader);
 
         BitsetFilterCache cache = new BitsetFilterCache(INDEX_SETTINGS, warmer, new BitsetFilterCache.Listener() {
@@ -114,6 +118,7 @@ public class BitSetFilterCacheTests extends ESTestCase {
         writer.forceMerge(1);
         reader.close();
         reader = DirectoryReader.open(writer, false);
+        reader = ElasticsearchDirectoryReader.wrap(reader, new ShardId(new Index("test"), 0));
         searcher = new IndexSearcher(reader);
 
         assertThat(matchCount(filter, reader), equalTo(3));
@@ -139,7 +144,7 @@ public class BitSetFilterCacheTests extends ESTestCase {
         writer.addDocument(document);
         writer.commit();
         final DirectoryReader writerReader = DirectoryReader.open(writer, false);
-        final IndexReader reader = randomBoolean() ? writerReader : ElasticsearchDirectoryReader.wrap(writerReader, new ShardId("test", 0));
+        final IndexReader reader = ElasticsearchDirectoryReader.wrap(writerReader, new ShardId("test", 0));
 
         final AtomicLong stats = new AtomicLong();
         final AtomicInteger onCacheCalls = new AtomicInteger();
@@ -192,4 +197,39 @@ public class BitSetFilterCacheTests extends ESTestCase {
         }
     }
 
+    public void testRejectOtherIndex() throws IOException {
+        BitsetFilterCache cache = new BitsetFilterCache(INDEX_SETTINGS, warmer, new BitsetFilterCache.Listener() {
+            @Override
+            public void onCache(ShardId shardId, Accountable accountable) {
+
+            }
+
+            @Override
+            public void onRemoval(ShardId shardId, Accountable accountable) {
+
+            }
+        });
+        
+        Directory dir = newDirectory();
+        IndexWriter writer = new IndexWriter(
+                dir,
+                newIndexWriterConfig()
+        );
+        writer.addDocument(new Document());
+        DirectoryReader reader = DirectoryReader.open(writer, true);
+        writer.close();
+        reader = ElasticsearchDirectoryReader.wrap(reader, new ShardId(new Index("test2"), 0));
+        
+        BitSetProducer producer = cache.getBitSetProducer(new MatchAllDocsQuery());
+        
+        try {
+            producer.getBitSet(reader.leaves().get(0));
+            fail();
+        } catch (IllegalStateException expected) {
+            assertEquals("Trying to load bit set for index [test2] with cache of index [test]", expected.getMessage());
+        } finally {
+            IOUtils.close(reader, dir);
+        }
+    }
+
 }
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTestCase.java b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTestCase.java
index 024a90c..82b2cca 100644
--- a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTestCase.java
@@ -46,12 +46,15 @@ import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.TestUtil;
 import org.apache.lucene.util.UnicodeUtil;
+import org.elasticsearch.common.lucene.index.ElasticsearchDirectoryReader;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.Index;
 import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource;
 import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource.Nested;
 import org.elasticsearch.index.fielddata.fieldcomparator.BytesRefFieldComparatorSource;
 import org.elasticsearch.index.fielddata.ordinals.GlobalOrdinalsIndexFieldData;
+import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.search.MultiValueMode;
 
 import java.io.IOException;
@@ -385,7 +388,9 @@ public abstract class AbstractStringFieldDataTestCase extends AbstractFieldDataI
                 writer.commit();
             }
         }
-        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, true));
+        DirectoryReader directoryReader = DirectoryReader.open(writer, true);
+        directoryReader = ElasticsearchDirectoryReader.wrap(directoryReader, new ShardId(new Index("test"), 0));
+        IndexSearcher searcher = new IndexSearcher(directoryReader);
         IndexFieldData<?> fieldData = getForField("text");
         final Object missingValue;
         switch (randomInt(4)) {
diff --git a/core/src/test/java/org/elasticsearch/index/indexing/IndexingSlowLogTests.java b/core/src/test/java/org/elasticsearch/index/indexing/IndexingSlowLogTests.java
deleted file mode 100644
index ccbef68..0000000
--- a/core/src/test/java/org/elasticsearch/index/indexing/IndexingSlowLogTests.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.indexing;
-
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.IntField;
-import org.apache.lucene.document.StringField;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.index.indexing.IndexingSlowLog.SlowLogParsedDocumentPrinter;
-import org.elasticsearch.index.mapper.ParsedDocument;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.not;
-
-public class IndexingSlowLogTests extends ESTestCase {
-    public void testSlowLogParsedDocumentPrinterSourceToLog() throws IOException {
-        BytesReference source = JsonXContent.contentBuilder().startObject().field("foo", "bar").endObject().bytes();
-        ParsedDocument pd = new ParsedDocument(new StringField("uid", "test:id", Store.YES), new IntField("version", 1, Store.YES), "id",
-                "test", null, 0, -1, null, source, null);
-
-        // Turning off document logging doesn't log source[]
-        SlowLogParsedDocumentPrinter p = new SlowLogParsedDocumentPrinter(pd, 10, true, 0);
-        assertThat(p.toString(), not(containsString("source[")));
-
-        // Turning on document logging logs the whole thing
-        p = new SlowLogParsedDocumentPrinter(pd, 10, true, Integer.MAX_VALUE);
-        assertThat(p.toString(), containsString("source[{\"foo\":\"bar\"}]"));
-
-        // And you can truncate the source
-        p = new SlowLogParsedDocumentPrinter(pd, 10, true, 3);
-        assertThat(p.toString(), containsString("source[{\"f]"));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/search/nested/AbstractNumberNestedSortingTestCase.java b/core/src/test/java/org/elasticsearch/index/search/nested/AbstractNumberNestedSortingTestCase.java
index fa27323..65dfd8a 100644
--- a/core/src/test/java/org/elasticsearch/index/search/nested/AbstractNumberNestedSortingTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/search/nested/AbstractNumberNestedSortingTestCase.java
@@ -36,11 +36,14 @@ import org.apache.lucene.search.TopFieldDocs;
 import org.apache.lucene.search.join.QueryBitSetProducer;
 import org.apache.lucene.search.join.ScoreMode;
 import org.apache.lucene.search.join.ToParentBlockJoinQuery;
+import org.elasticsearch.common.lucene.index.ElasticsearchDirectoryReader;
 import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.index.Index;
 import org.elasticsearch.index.fielddata.AbstractFieldDataTestCase;
 import org.elasticsearch.index.fielddata.IndexFieldData;
 import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource;
 import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource.Nested;
+import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.search.MultiValueMode;
 
 import java.io.IOException;
@@ -216,7 +219,9 @@ public abstract class AbstractNumberNestedSortingTestCase extends AbstractFieldD
         writer.addDocument(document);
 
         MultiValueMode sortMode = MultiValueMode.SUM;
-        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, false));
+        DirectoryReader directoryReader = DirectoryReader.open(writer, false);
+        directoryReader = ElasticsearchDirectoryReader.wrap(directoryReader, new ShardId(new Index("test"), 0));
+        IndexSearcher searcher = new IndexSearcher(directoryReader);
         Query parentFilter = new TermQuery(new Term("__type", "parent"));
         Query childFilter = Queries.not(parentFilter);
         XFieldComparatorSource nestedComparatorSource = createFieldComparator("field2", sortMode, null, createNested(searcher, parentFilter, childFilter));
diff --git a/core/src/test/java/org/elasticsearch/index/search/nested/NestedSortingTests.java b/core/src/test/java/org/elasticsearch/index/search/nested/NestedSortingTests.java
index 49af5f0..a58fea8 100644
--- a/core/src/test/java/org/elasticsearch/index/search/nested/NestedSortingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/search/nested/NestedSortingTests.java
@@ -40,8 +40,10 @@ import org.apache.lucene.search.join.ScoreMode;
 import org.apache.lucene.search.join.ToParentBlockJoinQuery;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.TestUtil;
+import org.elasticsearch.common.lucene.index.ElasticsearchDirectoryReader;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.Index;
 import org.elasticsearch.index.fielddata.AbstractFieldDataTestCase;
 import org.elasticsearch.index.fielddata.FieldDataType;
 import org.elasticsearch.index.fielddata.IndexFieldData;
@@ -49,6 +51,7 @@ import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource;
 import org.elasticsearch.index.fielddata.NoOrdinalsStringFieldDataTests;
 import org.elasticsearch.index.fielddata.fieldcomparator.BytesRefFieldComparatorSource;
 import org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData;
+import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.search.MultiValueMode;
 
 import java.io.IOException;
@@ -91,7 +94,9 @@ public class NestedSortingTests extends AbstractFieldDataTestCase {
         writer.commit();
 
         MultiValueMode sortMode = randomFrom(Arrays.asList(MultiValueMode.MIN, MultiValueMode.MAX));
-        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, false));
+        DirectoryReader reader = DirectoryReader.open(writer, false);
+        reader = ElasticsearchDirectoryReader.wrap(reader, new ShardId(new Index("test"), 0));
+        IndexSearcher searcher = new IndexSearcher(reader);
         PagedBytesIndexFieldData indexFieldData1 = getForField("f");
         IndexFieldData<?> indexFieldData2 = NoOrdinalsStringFieldDataTests.hideOrdinals(indexFieldData1);
         final String missingValue = randomBoolean() ? null : TestUtil.randomSimpleString(getRandom(), 2);
@@ -274,7 +279,9 @@ public class NestedSortingTests extends AbstractFieldDataTestCase {
         writer.addDocument(document);
 
         MultiValueMode sortMode = MultiValueMode.MIN;
-        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, false));
+        DirectoryReader reader = DirectoryReader.open(writer, false);
+        reader = ElasticsearchDirectoryReader.wrap(reader, new ShardId(new Index("test"), 0));
+        IndexSearcher searcher = new IndexSearcher(reader);
         PagedBytesIndexFieldData indexFieldData = getForField("field2");
         Query parentFilter = new TermQuery(new Term("__type", "parent"));
         Query childFilter = Queries.not(parentFilter);
diff --git a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
index 64ec036..d18e279 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
@@ -76,8 +76,6 @@ import org.elasticsearch.index.engine.EngineException;
 import org.elasticsearch.index.fielddata.FieldDataStats;
 import org.elasticsearch.index.fielddata.IndexFieldData;
 import org.elasticsearch.index.flush.FlushStats;
-import org.elasticsearch.index.indexing.IndexingOperationListener;
-import org.elasticsearch.index.indexing.ShardIndexingService;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.Mapping;
 import org.elasticsearch.index.mapper.ParseContext;
@@ -100,19 +98,23 @@ import java.io.IOException;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashSet;
+import java.util.List;
 import java.util.Set;
 import java.util.concurrent.BrokenBarrierException;
 import java.util.concurrent.CyclicBarrier;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicInteger;
 
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_VERSION_CREATED;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.ToXContent.EMPTY_PARAMS;
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
@@ -609,76 +611,76 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         return new ParsedDocument(uidField, versionField, id, type, routing, timestamp, ttl, Arrays.asList(document), source, mappingUpdate);
     }
 
-    public void testPreIndex() throws IOException {
-        createIndex("testpreindex");
+    public void testIndexingOperationsListeners() throws IOException {
+        createIndex("test_iol");
         ensureGreen();
+        client().prepareIndex("test_iol", "test", "0").setSource("{\"foo\" : \"bar\"}").setRefresh(true).get();
         IndicesService indicesService = getInstanceFromNode(IndicesService.class);
-        IndexService test = indicesService.indexService("testpreindex");
+        IndexService test = indicesService.indexService("test_iol");
         IndexShard shard = test.getShardOrNull(0);
-        ShardIndexingService shardIndexingService = shard.indexingService();
-        final AtomicBoolean preIndexCalled = new AtomicBoolean(false);
-
-        shardIndexingService.addListener(new IndexingOperationListener() {
+        AtomicInteger preIndex = new AtomicInteger();
+        AtomicInteger postIndex = new AtomicInteger();
+        AtomicInteger postIndexException = new AtomicInteger();
+        AtomicInteger preDelete = new AtomicInteger();
+        AtomicInteger postDelete = new AtomicInteger();
+        AtomicInteger postDeleteException = new AtomicInteger();
+        shard = reinitWithWrapper(test, shard, null, new IndexingOperationListener() {
             @Override
             public Engine.Index preIndex(Engine.Index operation) {
-                preIndexCalled.set(true);
-                return super.preIndex(operation);
+                preIndex.incrementAndGet();
+                return operation;
             }
-        });
-
-        ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, new ParseContext.Document(), new BytesArray(new byte[]{1}), null);
-        Engine.Index index = new Engine.Index(new Term("_uid", "1"), doc);
-        shard.index(index);
-        assertTrue(preIndexCalled.get());
-    }
 
-    public void testPostIndex() throws IOException {
-        createIndex("testpostindex");
-        ensureGreen();
-        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
-        IndexService test = indicesService.indexService("testpostindex");
-        IndexShard shard = test.getShardOrNull(0);
-        ShardIndexingService shardIndexingService = shard.indexingService();
-        final AtomicBoolean postIndexCalled = new AtomicBoolean(false);
-
-        shardIndexingService.addListener(new IndexingOperationListener() {
             @Override
             public void postIndex(Engine.Index index) {
-                postIndexCalled.set(true);
-                super.postIndex(index);
+                postIndex.incrementAndGet();
             }
-        });
 
-        ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, new ParseContext.Document(), new BytesArray(new byte[]{1}), null);
-        Engine.Index index = new Engine.Index(new Term("_uid", "1"), doc);
-        shard.index(index);
-        assertTrue(postIndexCalled.get());
-    }
-
-    public void testPostIndexWithException() throws IOException {
-        createIndex("testpostindexwithexception");
-        ensureGreen();
-        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
-        IndexService test = indicesService.indexService("testpostindexwithexception");
-        IndexShard shard = test.getShardOrNull(0);
-        ShardIndexingService shardIndexingService = shard.indexingService();
+            @Override
+            public void postIndex(Engine.Index index, Throwable ex) {
+                postIndexException.incrementAndGet();
+            }
 
-        shard.close("Unexpected close", true);
-        shard.state = IndexShardState.STARTED; // It will generate exception
+            @Override
+            public Engine.Delete preDelete(Engine.Delete delete) {
+                preDelete.incrementAndGet();
+                return delete;
+            }
 
-        final AtomicBoolean postIndexWithExceptionCalled = new AtomicBoolean(false);
+            @Override
+            public void postDelete(Engine.Delete delete) {
+                postDelete.incrementAndGet();
+            }
 
-        shardIndexingService.addListener(new IndexingOperationListener() {
             @Override
-            public void postIndex(Engine.Index index, Throwable ex) {
-                assertNotNull(ex);
-                postIndexWithExceptionCalled.set(true);
-                super.postIndex(index, ex);
+            public void postDelete(Engine.Delete delete, Throwable ex) {
+                postDeleteException.incrementAndGet();
+
             }
         });
 
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, new ParseContext.Document(), new BytesArray(new byte[]{1}), null);
         Engine.Index index = new Engine.Index(new Term("_uid", "1"), doc);
+        shard.index(index);
+        assertEquals(1, preIndex.get());
+        assertEquals(1, postIndex.get());
+        assertEquals(0, postIndexException.get());
+        assertEquals(0, preDelete.get());
+        assertEquals(0, postDelete.get());
+        assertEquals(0, postDeleteException.get());
+
+        Engine.Delete delete = new Engine.Delete("test", "1", new Term("_uid", "1"));
+        shard.delete(delete);
+
+        assertEquals(1, preIndex.get());
+        assertEquals(1, postIndex.get());
+        assertEquals(0, postIndexException.get());
+        assertEquals(1, preDelete.get());
+        assertEquals(1, postDelete.get());
+        assertEquals(0, postDeleteException.get());
+
+        shard.close("Unexpected close", true);
+        shard.state = IndexShardState.STARTED; // It will generate exception
 
         try {
             shard.index(index);
@@ -687,7 +689,26 @@ public class IndexShardTests extends ESSingleNodeTestCase {
 
         }
 
-        assertTrue(postIndexWithExceptionCalled.get());
+        assertEquals(2, preIndex.get());
+        assertEquals(1, postIndex.get());
+        assertEquals(1, postIndexException.get());
+        assertEquals(1, preDelete.get());
+        assertEquals(1, postDelete.get());
+        assertEquals(0, postDeleteException.get());
+        try {
+            shard.delete(delete);
+            fail();
+        }catch (IllegalIndexShardStateException e){
+
+        }
+
+        assertEquals(2, preIndex.get());
+        assertEquals(1, postIndex.get());
+        assertEquals(1, postIndexException.get());
+        assertEquals(2, preDelete.get());
+        assertEquals(1, postDelete.get());
+        assertEquals(1, postDeleteException.get());
+
     }
 
     public void testMaybeFlush() throws Exception {
@@ -1038,11 +1059,11 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         // test will fail due to unclosed searchers if the searcher is not released
     }
 
-    private final IndexShard reinitWithWrapper(IndexService indexService, IndexShard shard, IndexSearcherWrapper wrapper) throws IOException {
+    private final IndexShard reinitWithWrapper(IndexService indexService, IndexShard shard, IndexSearcherWrapper wrapper, IndexingOperationListener... listeners) throws IOException {
         ShardRouting routing = new ShardRouting(shard.routingEntry());
         shard.close("simon says", true);
         NodeServicesProvider indexServices = indexService.getIndexServices();
-        IndexShard newShard = new IndexShard(shard.shardId(), indexService.getIndexSettings(), shard.shardPath(), shard.store(), indexService.cache(), indexService.mapperService(), indexService.similarityService(), indexService.fieldData(), shard.getEngineFactory(), indexService.getIndexEventListener(), wrapper, indexServices);
+        IndexShard newShard = new IndexShard(shard.shardId(), indexService.getIndexSettings(), shard.shardPath(), shard.store(), indexService.cache(), indexService.mapperService(), indexService.similarityService(), indexService.fieldData(), shard.getEngineFactory(), indexService.getIndexEventListener(), wrapper, indexServices, listeners);
         ShardRoutingHelper.reinit(routing);
         newShard.updateRoutingEntry(routing, false);
         DiscoveryNode localNode = new DiscoveryNode("foo", DummyTransportAddress.INSTANCE, Version.CURRENT);
@@ -1054,4 +1075,29 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         return newShard;
     }
 
+    public void testTranslogRecoverySyncsTranslog() throws IOException {
+        createIndex("testindexfortranslogsync");
+        client().admin().indices().preparePutMapping("testindexfortranslogsync").setType("testtype").setSource(jsonBuilder().startObject()
+            .startObject("testtype")
+            .startObject("properties")
+            .startObject("foo")
+            .field("type", "string")
+            .endObject()
+            .endObject().endObject().endObject()).get();
+        ensureGreen();
+        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
+        IndexService test = indicesService.indexService("testindexfortranslogsync");
+        IndexShard shard = test.getShardOrNull(0);
+        ShardRouting routing = new ShardRouting(shard.routingEntry());
+        test.removeShard(0, "b/c britta says so");
+        IndexShard newShard = test.createShard(routing);
+        DiscoveryNode localNode = new DiscoveryNode("foo", DummyTransportAddress.INSTANCE, Version.CURRENT);
+        newShard.markAsRecovering("for testing", new RecoveryState(newShard.shardId(), routing.primary(), RecoveryState.Type.REPLICA, localNode, localNode));
+        List<Translog.Operation> operations = new ArrayList<>();
+        operations.add(new Translog.Index("testtype", "1", jsonBuilder().startObject().field("foo", "bar").endObject().bytes().toBytes()));
+        newShard.prepareForIndexRecovery();
+        newShard.performTranslogRecovery(true);
+        newShard.performBatchRecovery(operations);
+        assertFalse(newShard.getTranslog().syncNeeded());
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/shard/IndexingOperationListenerTests.java b/core/src/test/java/org/elasticsearch/index/shard/IndexingOperationListenerTests.java
new file mode 100644
index 0000000..92bbf06
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/shard/IndexingOperationListenerTests.java
@@ -0,0 +1,162 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.shard;
+
+import org.apache.lucene.index.Term;
+import org.elasticsearch.index.engine.Engine;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.atomic.AtomicInteger;
+
+public class IndexingOperationListenerTests extends ESTestCase{
+
+    // this test also tests if calls are correct if one or more listeners throw exceptions
+    public void testListenersAreExecuted() {
+        AtomicInteger preIndex = new AtomicInteger();
+        AtomicInteger postIndex = new AtomicInteger();
+        AtomicInteger postIndexException = new AtomicInteger();
+        AtomicInteger preDelete = new AtomicInteger();
+        AtomicInteger postDelete = new AtomicInteger();
+        AtomicInteger postDeleteException = new AtomicInteger();
+        IndexingOperationListener listener = new IndexingOperationListener() {
+            @Override
+            public Engine.Index preIndex(Engine.Index operation) {
+                preIndex.incrementAndGet();
+                return operation;
+            }
+
+            @Override
+            public void postIndex(Engine.Index index) {
+                postIndex.incrementAndGet();
+            }
+
+            @Override
+            public void postIndex(Engine.Index index, Throwable ex) {
+                postIndexException.incrementAndGet();
+            }
+
+            @Override
+            public Engine.Delete preDelete(Engine.Delete delete) {
+                preDelete.incrementAndGet();
+                return delete;
+            }
+
+            @Override
+            public void postDelete(Engine.Delete delete) {
+                postDelete.incrementAndGet();
+            }
+
+            @Override
+            public void postDelete(Engine.Delete delete, Throwable ex) {
+                postDeleteException.incrementAndGet();
+            }
+        };
+
+        IndexingOperationListener throwingListener = new IndexingOperationListener() {
+            @Override
+            public Engine.Index preIndex(Engine.Index operation) {
+                throw new RuntimeException();
+            }
+
+            @Override
+            public void postIndex(Engine.Index index) {
+                throw new RuntimeException();            }
+
+            @Override
+            public void postIndex(Engine.Index index, Throwable ex) {
+                throw new RuntimeException();            }
+
+            @Override
+            public Engine.Delete preDelete(Engine.Delete delete) {
+                throw new RuntimeException();
+            }
+
+            @Override
+            public void postDelete(Engine.Delete delete) {
+                throw new RuntimeException();            }
+
+            @Override
+            public void postDelete(Engine.Delete delete, Throwable ex) {
+                throw new RuntimeException();
+            }
+        };
+        final List<IndexingOperationListener> indexingOperationListeners = new ArrayList<>(Arrays.asList(listener, listener));
+        if (randomBoolean()) {
+            indexingOperationListeners.add(throwingListener);
+            if (randomBoolean()) {
+                indexingOperationListeners.add(throwingListener);
+            }
+        }
+        Collections.shuffle(indexingOperationListeners, random());
+        IndexingOperationListener.CompositeListener compositeListener = new IndexingOperationListener.CompositeListener(indexingOperationListeners, logger);
+        Engine.Delete delete = new Engine.Delete("test", "1", new Term("_uid", "1"));
+        Engine.Index index = new Engine.Index(new Term("_uid", "1"), null);
+        compositeListener.postDelete(delete);
+        assertEquals(0, preIndex.get());
+        assertEquals(0, postIndex.get());
+        assertEquals(0, postIndexException.get());
+        assertEquals(0, preDelete.get());
+        assertEquals(2, postDelete.get());
+        assertEquals(0, postDeleteException.get());
+
+        compositeListener.postDelete(delete, new RuntimeException());
+        assertEquals(0, preIndex.get());
+        assertEquals(0, postIndex.get());
+        assertEquals(0, postIndexException.get());
+        assertEquals(0, preDelete.get());
+        assertEquals(2, postDelete.get());
+        assertEquals(2, postDeleteException.get());
+
+        compositeListener.preDelete(delete);
+        assertEquals(0, preIndex.get());
+        assertEquals(0, postIndex.get());
+        assertEquals(0, postIndexException.get());
+        assertEquals(2, preDelete.get());
+        assertEquals(2, postDelete.get());
+        assertEquals(2, postDeleteException.get());
+
+        compositeListener.postIndex(index);
+        assertEquals(0, preIndex.get());
+        assertEquals(2, postIndex.get());
+        assertEquals(0, postIndexException.get());
+        assertEquals(2, preDelete.get());
+        assertEquals(2, postDelete.get());
+        assertEquals(2, postDeleteException.get());
+
+        compositeListener.postIndex(index, new RuntimeException());
+        assertEquals(0, preIndex.get());
+        assertEquals(2, postIndex.get());
+        assertEquals(2, postIndexException.get());
+        assertEquals(2, preDelete.get());
+        assertEquals(2, postDelete.get());
+        assertEquals(2, postDeleteException.get());
+
+        compositeListener.preIndex(index);
+        assertEquals(2, preIndex.get());
+        assertEquals(2, postIndex.get());
+        assertEquals(2, postIndexException.get());
+        assertEquals(2, preDelete.get());
+        assertEquals(2, postDelete.get());
+        assertEquals(2, postDeleteException.get());
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java b/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
index ddd8c2f..da5e5c8 100644
--- a/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
+++ b/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
@@ -1836,6 +1836,12 @@ public class TranslogTests extends ESTestCase {
                             syncedDocs.clear();
                         }
                     }
+                    // we survived all the randomness!!!
+                    // lets close the translog and if it succeeds we are all synced again. If we don't do this we will close
+                    // it in the finally block but miss to copy over unsynced docs to syncedDocs and fail the assertion down the road...
+                    failableTLog.close();
+                    syncedDocs.addAll(unsynced);
+                    unsynced.clear();
                 } catch (TranslogException | MockDirectoryWrapper.FakeIOException ex) {
                     // fair enough
                 } catch (IOException ex) {
diff --git a/core/src/test/java/org/elasticsearch/indices/IndexingMemoryControllerIT.java b/core/src/test/java/org/elasticsearch/indices/IndexingMemoryControllerIT.java
new file mode 100644
index 0000000..a9e4b35
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/indices/IndexingMemoryControllerIT.java
@@ -0,0 +1,99 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.indices;
+
+import org.elasticsearch.cluster.ClusterName;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.common.util.concurrent.EsExecutors;
+import org.elasticsearch.index.shard.IndexShard;
+import org.elasticsearch.node.internal.InternalSettingsPreparer;
+import org.elasticsearch.test.ESIntegTestCase;
+
+
+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)
+public class IndexingMemoryControllerIT extends ESIntegTestCase {
+    private long getIWBufferSize(String indexName) {
+        return client().admin().indices().prepareStats(indexName).get().getTotal().getSegments().getIndexWriterMaxMemoryInBytes();
+    }
+
+    public void testIndexBufferPushedToEngine() throws InterruptedException {
+        createNode(Settings.builder().put(IndexShard.INDEX_SHARD_INACTIVE_TIME_SETTING, "100000h",
+                                          IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "32mb",
+                                          IndexShard.INDEX_REFRESH_INTERVAL, "-1").build());
+
+        // Create two active indices, sharing 32 MB indexing buffer:
+        prepareCreate("test3").setSettings(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1, IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0).get();
+        prepareCreate("test4").setSettings(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1, IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0).get();
+
+        ensureGreen();
+
+        index("test3", "type", "1", "f", 1);
+        index("test4", "type", "1", "f", 1);
+
+        // .. then make sure we really pushed the update (16 MB for each) down to the IndexWriter, even if refresh nor flush occurs:
+        if (awaitBusy(() -> getIWBufferSize("test3") == 16*1024*1024) == false) {
+            fail("failed to update shard indexing buffer size for test3 index to 16 MB; got: " + getIWBufferSize("test3"));
+        }
+        if (awaitBusy(() -> getIWBufferSize("test4") == 16*1024*1024) == false) {
+            fail("failed to update shard indexing buffer size for test4 index to 16 MB; got: " + getIWBufferSize("test4"));
+        }
+
+        client().admin().indices().prepareDelete("test4").get();
+        if (awaitBusy(() -> getIWBufferSize("test3") == 32 * 1024 * 1024) == false) {
+            fail("failed to update shard indexing buffer size for test3 index to 32 MB; got: " + getIWBufferSize("test4"));
+        }
+
+    }
+
+    public void testInactivePushedToShard() throws InterruptedException {
+        createNode(Settings.builder().put(IndexShard.INDEX_SHARD_INACTIVE_TIME_SETTING, "100ms",
+                IndexingMemoryController.SHARD_INACTIVE_INTERVAL_TIME_SETTING, "100ms",
+                IndexShard.INDEX_REFRESH_INTERVAL, "-1").build());
+
+        // Create two active indices, sharing 32 MB indexing buffer:
+        prepareCreate("test1").setSettings(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1, IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0).get();
+
+        ensureGreen();
+
+        index("test1", "type", "1", "f", 1);
+
+        // make shard the shard buffer was set to inactive size
+        final ByteSizeValue inactiveBuffer = IndexingMemoryController.INACTIVE_SHARD_INDEXING_BUFFER;
+        if (awaitBusy(() -> getIWBufferSize("test1") == inactiveBuffer.bytes()) == false) {
+            fail("failed to update shard indexing buffer size for test1 index to [" + inactiveBuffer + "]; got: " + getIWBufferSize("test1"));
+        }
+    }
+
+    private void createNode(Settings settings) {
+        internalCluster().startNode(Settings.builder()
+                        .put(ClusterName.SETTING, "IndexingMemoryControllerIT")
+                        .put("node.name", "IndexingMemoryControllerIT")
+                        .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
+                        .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)
+                        .put(EsExecutors.PROCESSORS, 1) // limit the number of threads created
+                        .put("http.enabled", false)
+                        .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true) // make sure we get what we set :)
+                        .put(IndexingMemoryController.SHARD_INACTIVE_INTERVAL_TIME_SETTING, "100ms")
+                        .put(settings)
+        );
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/indices/IndexingMemoryControllerTests.java b/core/src/test/java/org/elasticsearch/indices/IndexingMemoryControllerTests.java
new file mode 100644
index 0000000..19f91be
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/indices/IndexingMemoryControllerTests.java
@@ -0,0 +1,266 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.indices;
+
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.index.IndexService;
+import org.elasticsearch.index.shard.IndexShard;
+import org.elasticsearch.test.ESSingleNodeTestCase;
+import org.elasticsearch.threadpool.ThreadPool;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.ScheduledFuture;
+
+import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
+import static org.hamcrest.Matchers.equalTo;
+
+public class IndexingMemoryControllerTests extends ESSingleNodeTestCase {
+
+    static class MockController extends IndexingMemoryController {
+
+        final static ByteSizeValue INACTIVE = new ByteSizeValue(-1);
+
+        final Map<IndexShard, ByteSizeValue> indexingBuffers = new HashMap<>();
+
+        final Map<IndexShard, Long> lastIndexTimeNanos = new HashMap<>();
+        final Set<IndexShard> activeShards = new HashSet<>();
+
+        long currentTimeSec = TimeValue.timeValueNanos(System.nanoTime()).seconds();
+
+        public MockController(Settings settings) {
+            super(Settings.builder()
+                    .put(SHARD_INACTIVE_INTERVAL_TIME_SETTING, "200h") // disable it
+                    .put(IndexShard.INDEX_SHARD_INACTIVE_TIME_SETTING, "1ms") // nearly immediate
+                    .put(settings)
+                    .build(),
+                null, null, 100 * 1024 * 1024); // fix jvm mem size to 100mb
+        }
+
+        public void deleteShard(IndexShard id) {
+            indexingBuffers.remove(id);
+        }
+
+        public void assertBuffers(IndexShard id, ByteSizeValue indexing) {
+            assertThat(indexingBuffers.get(id), equalTo(indexing));
+        }
+
+        public void assertInactive(IndexShard id) {
+            assertThat(indexingBuffers.get(id), equalTo(INACTIVE));
+        }
+
+        @Override
+        protected long currentTimeInNanos() {
+            return TimeValue.timeValueSeconds(currentTimeSec).nanos();
+        }
+
+        @Override
+        protected List<IndexShard> availableShards() {
+            return new ArrayList<>(indexingBuffers.keySet());
+        }
+
+        @Override
+        protected boolean shardAvailable(IndexShard shard) {
+            return indexingBuffers.containsKey(shard);
+        }
+
+        @Override
+        protected void updateShardBuffers(IndexShard shard, ByteSizeValue shardIndexingBufferSize) {
+            indexingBuffers.put(shard, shardIndexingBufferSize);
+        }
+
+        @Override
+        protected boolean checkIdle(IndexShard shard) {
+            final TimeValue inactiveTime = settings.getAsTime(IndexShard.INDEX_SHARD_INACTIVE_TIME_SETTING, TimeValue.timeValueMinutes(5));
+            Long ns = lastIndexTimeNanos.get(shard);
+            if (ns == null) {
+                return true;
+            } else if (currentTimeInNanos() - ns >= inactiveTime.nanos()) {
+                indexingBuffers.put(shard, INACTIVE);
+                activeShards.remove(shard);
+                return true;
+            } else {
+                return false;
+            }
+        }
+
+        public void incrementTimeSec(int sec) {
+            currentTimeSec += sec;
+        }
+
+        public void simulateIndexing(IndexShard shard) {
+            lastIndexTimeNanos.put(shard, currentTimeInNanos());
+            if (indexingBuffers.containsKey(shard) == false) {
+                // First time we are seeing this shard; start it off with inactive buffers as IndexShard does:
+                indexingBuffers.put(shard, IndexingMemoryController.INACTIVE_SHARD_INDEXING_BUFFER);
+            }
+            activeShards.add(shard);
+            forceCheck();
+        }
+
+        @Override
+        protected ScheduledFuture<?> scheduleTask(ThreadPool threadPool) {
+            return null;
+        }
+    }
+
+    public void testShardAdditionAndRemoval() {
+        createIndex("test", Settings.builder().put(SETTING_NUMBER_OF_SHARDS, 3).put(SETTING_NUMBER_OF_REPLICAS, 0).build());
+        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
+        IndexService test = indicesService.indexService("test");
+
+        MockController controller = new MockController(Settings.builder()
+            .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "10mb").build());
+        IndexShard shard0 = test.getShard(0);
+        controller.simulateIndexing(shard0);
+        controller.assertBuffers(shard0, new ByteSizeValue(10, ByteSizeUnit.MB)); // translog is maxed at 64K
+
+        // add another shard
+        IndexShard shard1 = test.getShard(1);
+        controller.simulateIndexing(shard1);
+        controller.assertBuffers(shard0, new ByteSizeValue(5, ByteSizeUnit.MB));
+        controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB));
+
+        // remove first shard
+        controller.deleteShard(shard0);
+        controller.forceCheck();
+        controller.assertBuffers(shard1, new ByteSizeValue(10, ByteSizeUnit.MB)); // translog is maxed at 64K
+
+        // remove second shard
+        controller.deleteShard(shard1);
+        controller.forceCheck();
+
+        // add a new one
+        IndexShard shard2 = test.getShard(2);
+        controller.simulateIndexing(shard2);
+        controller.assertBuffers(shard2, new ByteSizeValue(10, ByteSizeUnit.MB)); // translog is maxed at 64K
+    }
+
+    public void testActiveInactive() {
+        createIndex("test", Settings.builder().put(SETTING_NUMBER_OF_SHARDS, 2).put(SETTING_NUMBER_OF_REPLICAS, 0).build());
+        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
+        IndexService test = indicesService.indexService("test");
+
+        MockController controller = new MockController(Settings.builder()
+            .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "10mb")
+            .put(IndexShard.INDEX_SHARD_INACTIVE_TIME_SETTING, "5s")
+            .build());
+
+        IndexShard shard0 = test.getShard(0);
+        controller.simulateIndexing(shard0);
+        IndexShard shard1 = test.getShard(1);
+        controller.simulateIndexing(shard1);
+        controller.assertBuffers(shard0, new ByteSizeValue(5, ByteSizeUnit.MB));
+        controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB));
+
+        // index into both shards, move the clock and see that they are still active
+        controller.simulateIndexing(shard0);
+        controller.simulateIndexing(shard1);
+
+        controller.incrementTimeSec(10);
+        controller.forceCheck();
+
+        // both shards now inactive
+        controller.assertInactive(shard0);
+        controller.assertInactive(shard1);
+
+        // index into one shard only, see it becomes active
+        controller.simulateIndexing(shard0);
+        controller.assertBuffers(shard0, new ByteSizeValue(10, ByteSizeUnit.MB));
+        controller.assertInactive(shard1);
+
+        controller.incrementTimeSec(3); // increment but not enough to become inactive
+        controller.forceCheck();
+        controller.assertBuffers(shard0, new ByteSizeValue(10, ByteSizeUnit.MB));
+        controller.assertInactive(shard1);
+
+        controller.incrementTimeSec(3); // increment some more
+        controller.forceCheck();
+        controller.assertInactive(shard0);
+        controller.assertInactive(shard1);
+
+        // index some and shard becomes immediately active
+        controller.simulateIndexing(shard1);
+        controller.assertInactive(shard0);
+        controller.assertBuffers(shard1, new ByteSizeValue(10, ByteSizeUnit.MB));
+    }
+
+    public void testMinShardBufferSizes() {
+        MockController controller = new MockController(Settings.builder()
+            .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "10mb")
+            .put(IndexingMemoryController.MIN_SHARD_INDEX_BUFFER_SIZE_SETTING, "6mb")
+            .put(IndexingMemoryController.MIN_SHARD_TRANSLOG_BUFFER_SIZE_SETTING, "40kb").build());
+
+        assertTwoActiveShards(controller, new ByteSizeValue(6, ByteSizeUnit.MB), new ByteSizeValue(40, ByteSizeUnit.KB));
+    }
+
+    public void testMaxShardBufferSizes() {
+        MockController controller = new MockController(Settings.builder()
+            .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "10mb")
+            .put(IndexingMemoryController.MAX_SHARD_INDEX_BUFFER_SIZE_SETTING, "3mb")
+            .put(IndexingMemoryController.MAX_SHARD_TRANSLOG_BUFFER_SIZE_SETTING, "10kb").build());
+
+        assertTwoActiveShards(controller, new ByteSizeValue(3, ByteSizeUnit.MB), new ByteSizeValue(10, ByteSizeUnit.KB));
+    }
+
+    public void testRelativeBufferSizes() {
+        MockController controller = new MockController(Settings.builder()
+            .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "50%")
+            .build());
+
+        assertThat(controller.indexingBufferSize(), equalTo(new ByteSizeValue(50, ByteSizeUnit.MB)));
+    }
+
+
+    public void testMinBufferSizes() {
+        MockController controller = new MockController(Settings.builder()
+            .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "0.001%")
+            .put(IndexingMemoryController.MIN_INDEX_BUFFER_SIZE_SETTING, "6mb").build());
+
+        assertThat(controller.indexingBufferSize(), equalTo(new ByteSizeValue(6, ByteSizeUnit.MB)));
+    }
+
+    public void testMaxBufferSizes() {
+        MockController controller = new MockController(Settings.builder()
+            .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "90%")
+            .put(IndexingMemoryController.MAX_INDEX_BUFFER_SIZE_SETTING, "6mb").build());
+
+        assertThat(controller.indexingBufferSize(), equalTo(new ByteSizeValue(6, ByteSizeUnit.MB)));
+    }
+
+    protected void assertTwoActiveShards(MockController controller, ByteSizeValue indexBufferSize, ByteSizeValue translogBufferSize) {
+        createIndex("test", Settings.builder().put(SETTING_NUMBER_OF_SHARDS, 2).put(SETTING_NUMBER_OF_REPLICAS, 0).build());
+        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
+        IndexService test = indicesService.indexService("test");
+        IndexShard shard0 = test.getShard(0);
+        controller.simulateIndexing(shard0);
+        IndexShard shard1 = test.getShard(1);
+        controller.simulateIndexing(shard1);
+        controller.assertBuffers(shard0, indexBufferSize);
+        controller.assertBuffers(shard1, indexBufferSize);
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java b/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java
deleted file mode 100644
index f8de882..0000000
--- a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java
+++ /dev/null
@@ -1,99 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.indices.memory;
-
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.util.concurrent.EsExecutors;
-import org.elasticsearch.index.shard.IndexShard;
-import org.elasticsearch.node.internal.InternalSettingsPreparer;
-import org.elasticsearch.test.ESIntegTestCase;
-
-
-@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)
-public class IndexingMemoryControllerIT extends ESIntegTestCase {
-    private long getIWBufferSize(String indexName) {
-        return client().admin().indices().prepareStats(indexName).get().getTotal().getSegments().getIndexWriterMaxMemoryInBytes();
-    }
-
-    public void testIndexBufferPushedToEngine() throws InterruptedException {
-        createNode(Settings.builder().put(IndexShard.INDEX_SHARD_INACTIVE_TIME_SETTING, "100000h",
-                                          IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "32mb",
-                                          IndexShard.INDEX_REFRESH_INTERVAL, "-1").build());
-
-        // Create two active indices, sharing 32 MB indexing buffer:
-        prepareCreate("test3").setSettings(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1, IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0).get();
-        prepareCreate("test4").setSettings(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1, IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0).get();
-
-        ensureGreen();
-
-        index("test3", "type", "1", "f", 1);
-        index("test4", "type", "1", "f", 1);
-
-        // .. then make sure we really pushed the update (16 MB for each) down to the IndexWriter, even if refresh nor flush occurs:
-        if (awaitBusy(() -> getIWBufferSize("test3") == 16*1024*1024) == false) {
-            fail("failed to update shard indexing buffer size for test3 index to 16 MB; got: " + getIWBufferSize("test3"));
-        }
-        if (awaitBusy(() -> getIWBufferSize("test4") == 16*1024*1024) == false) {
-            fail("failed to update shard indexing buffer size for test4 index to 16 MB; got: " + getIWBufferSize("test4"));
-        }
-
-        client().admin().indices().prepareDelete("test4").get();
-        if (awaitBusy(() -> getIWBufferSize("test3") == 32 * 1024 * 1024) == false) {
-            fail("failed to update shard indexing buffer size for test3 index to 32 MB; got: " + getIWBufferSize("test4"));
-        }
-
-    }
-
-    public void testInactivePushedToShard() throws InterruptedException {
-        createNode(Settings.builder().put(IndexShard.INDEX_SHARD_INACTIVE_TIME_SETTING, "100ms",
-                IndexingMemoryController.SHARD_INACTIVE_INTERVAL_TIME_SETTING, "100ms",
-                IndexShard.INDEX_REFRESH_INTERVAL, "-1").build());
-
-        // Create two active indices, sharing 32 MB indexing buffer:
-        prepareCreate("test1").setSettings(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1, IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0).get();
-
-        ensureGreen();
-
-        index("test1", "type", "1", "f", 1);
-
-        // make shard the shard buffer was set to inactive size
-        final ByteSizeValue inactiveBuffer = IndexingMemoryController.INACTIVE_SHARD_INDEXING_BUFFER;
-        if (awaitBusy(() -> getIWBufferSize("test1") == inactiveBuffer.bytes()) == false) {
-            fail("failed to update shard indexing buffer size for test1 index to [" + inactiveBuffer + "]; got: " + getIWBufferSize("test1"));
-        }
-    }
-
-    private void createNode(Settings settings) {
-        internalCluster().startNode(Settings.builder()
-                        .put(ClusterName.SETTING, "IndexingMemoryControllerIT")
-                        .put("node.name", "IndexingMemoryControllerIT")
-                        .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
-                        .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)
-                        .put(EsExecutors.PROCESSORS, 1) // limit the number of threads created
-                        .put("http.enabled", false)
-                        .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true) // make sure we get what we set :)
-                        .put(IndexingMemoryController.SHARD_INACTIVE_INTERVAL_TIME_SETTING, "100ms")
-                        .put(settings)
-        );
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java b/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java
deleted file mode 100644
index d980c3c..0000000
--- a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java
+++ /dev/null
@@ -1,260 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.indices.memory;
-
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.shard.IndexShard;
-import org.elasticsearch.indices.IndicesService;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.hamcrest.Matchers.equalTo;
-
-public class IndexingMemoryControllerTests extends ESSingleNodeTestCase {
-
-    static class MockController extends IndexingMemoryController {
-
-        final static ByteSizeValue INACTIVE = new ByteSizeValue(-1);
-
-        final Map<IndexShard, ByteSizeValue> indexingBuffers = new HashMap<>();
-
-        final Map<IndexShard, Long> lastIndexTimeNanos = new HashMap<>();
-        final Set<IndexShard> activeShards = new HashSet<>();
-
-        long currentTimeSec = TimeValue.timeValueNanos(System.nanoTime()).seconds();
-
-        public MockController(Settings settings) {
-            super(Settings.builder()
-                    .put(SHARD_INACTIVE_INTERVAL_TIME_SETTING, "200h") // disable it
-                    .put(IndexShard.INDEX_SHARD_INACTIVE_TIME_SETTING, "1ms") // nearly immediate
-                    .put(settings)
-                    .build(),
-                null, null, 100 * 1024 * 1024); // fix jvm mem size to 100mb
-        }
-
-        public void deleteShard(IndexShard id) {
-            indexingBuffers.remove(id);
-        }
-
-        public void assertBuffers(IndexShard id, ByteSizeValue indexing) {
-            assertThat(indexingBuffers.get(id), equalTo(indexing));
-        }
-
-        public void assertInactive(IndexShard id) {
-            assertThat(indexingBuffers.get(id), equalTo(INACTIVE));
-        }
-
-        @Override
-        protected long currentTimeInNanos() {
-            return TimeValue.timeValueSeconds(currentTimeSec).nanos();
-        }
-
-        @Override
-        protected List<IndexShard> availableShards() {
-            return new ArrayList<>(indexingBuffers.keySet());
-        }
-
-        @Override
-        protected boolean shardAvailable(IndexShard shard) {
-            return indexingBuffers.containsKey(shard);
-        }
-
-        @Override
-        protected void updateShardBuffers(IndexShard shard, ByteSizeValue shardIndexingBufferSize) {
-            indexingBuffers.put(shard, shardIndexingBufferSize);
-        }
-
-        @Override
-        protected boolean checkIdle(IndexShard shard) {
-            final TimeValue inactiveTime = settings.getAsTime(IndexShard.INDEX_SHARD_INACTIVE_TIME_SETTING, TimeValue.timeValueMinutes(5));
-            Long ns = lastIndexTimeNanos.get(shard);
-            if (ns == null) {
-                return true;
-            } else if (currentTimeInNanos() - ns >= inactiveTime.nanos()) {
-                indexingBuffers.put(shard, INACTIVE);
-                activeShards.remove(shard);
-                return true;
-            } else {
-                return false;
-            }
-        }
-
-        public void incrementTimeSec(int sec) {
-            currentTimeSec += sec;
-        }
-
-        public void simulateIndexing(IndexShard shard) {
-            lastIndexTimeNanos.put(shard, currentTimeInNanos());
-            if (indexingBuffers.containsKey(shard) == false) {
-                // First time we are seeing this shard; start it off with inactive buffers as IndexShard does:
-                indexingBuffers.put(shard, IndexingMemoryController.INACTIVE_SHARD_INDEXING_BUFFER);
-            }
-            activeShards.add(shard);
-            forceCheck();
-        }
-    }
-
-    public void testShardAdditionAndRemoval() {
-        createIndex("test", Settings.builder().put(SETTING_NUMBER_OF_SHARDS, 3).put(SETTING_NUMBER_OF_REPLICAS, 0).build());
-        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
-        IndexService test = indicesService.indexService("test");
-
-        MockController controller = new MockController(Settings.builder()
-            .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "10mb").build());
-        IndexShard shard0 = test.getShard(0);
-        controller.simulateIndexing(shard0);
-        controller.assertBuffers(shard0, new ByteSizeValue(10, ByteSizeUnit.MB)); // translog is maxed at 64K
-
-        // add another shard
-        IndexShard shard1 = test.getShard(1);
-        controller.simulateIndexing(shard1);
-        controller.assertBuffers(shard0, new ByteSizeValue(5, ByteSizeUnit.MB));
-        controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB));
-
-        // remove first shard
-        controller.deleteShard(shard0);
-        controller.forceCheck();
-        controller.assertBuffers(shard1, new ByteSizeValue(10, ByteSizeUnit.MB)); // translog is maxed at 64K
-
-        // remove second shard
-        controller.deleteShard(shard1);
-        controller.forceCheck();
-
-        // add a new one
-        IndexShard shard2 = test.getShard(2);
-        controller.simulateIndexing(shard2);
-        controller.assertBuffers(shard2, new ByteSizeValue(10, ByteSizeUnit.MB)); // translog is maxed at 64K
-    }
-
-    public void testActiveInactive() {
-        createIndex("test", Settings.builder().put(SETTING_NUMBER_OF_SHARDS, 2).put(SETTING_NUMBER_OF_REPLICAS, 0).build());
-        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
-        IndexService test = indicesService.indexService("test");
-
-        MockController controller = new MockController(Settings.builder()
-            .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "10mb")
-            .put(IndexShard.INDEX_SHARD_INACTIVE_TIME_SETTING, "5s")
-            .build());
-
-        IndexShard shard0 = test.getShard(0);
-        controller.simulateIndexing(shard0);
-        IndexShard shard1 = test.getShard(1);
-        controller.simulateIndexing(shard1);
-        controller.assertBuffers(shard0, new ByteSizeValue(5, ByteSizeUnit.MB));
-        controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB));
-
-        // index into both shards, move the clock and see that they are still active
-        controller.simulateIndexing(shard0);
-        controller.simulateIndexing(shard1);
-
-        controller.incrementTimeSec(10);
-        controller.forceCheck();
-
-        // both shards now inactive
-        controller.assertInactive(shard0);
-        controller.assertInactive(shard1);
-
-        // index into one shard only, see it becomes active
-        controller.simulateIndexing(shard0);
-        controller.assertBuffers(shard0, new ByteSizeValue(10, ByteSizeUnit.MB));
-        controller.assertInactive(shard1);
-
-        controller.incrementTimeSec(3); // increment but not enough to become inactive
-        controller.forceCheck();
-        controller.assertBuffers(shard0, new ByteSizeValue(10, ByteSizeUnit.MB));
-        controller.assertInactive(shard1);
-
-        controller.incrementTimeSec(3); // increment some more
-        controller.forceCheck();
-        controller.assertInactive(shard0);
-        controller.assertInactive(shard1);
-
-        // index some and shard becomes immediately active
-        controller.simulateIndexing(shard1);
-        controller.assertInactive(shard0);
-        controller.assertBuffers(shard1, new ByteSizeValue(10, ByteSizeUnit.MB));
-    }
-
-    public void testMinShardBufferSizes() {
-        MockController controller = new MockController(Settings.builder()
-            .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "10mb")
-            .put(IndexingMemoryController.MIN_SHARD_INDEX_BUFFER_SIZE_SETTING, "6mb")
-            .put(IndexingMemoryController.MIN_SHARD_TRANSLOG_BUFFER_SIZE_SETTING, "40kb").build());
-
-        assertTwoActiveShards(controller, new ByteSizeValue(6, ByteSizeUnit.MB), new ByteSizeValue(40, ByteSizeUnit.KB));
-    }
-
-    public void testMaxShardBufferSizes() {
-        MockController controller = new MockController(Settings.builder()
-            .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "10mb")
-            .put(IndexingMemoryController.MAX_SHARD_INDEX_BUFFER_SIZE_SETTING, "3mb")
-            .put(IndexingMemoryController.MAX_SHARD_TRANSLOG_BUFFER_SIZE_SETTING, "10kb").build());
-
-        assertTwoActiveShards(controller, new ByteSizeValue(3, ByteSizeUnit.MB), new ByteSizeValue(10, ByteSizeUnit.KB));
-    }
-
-    public void testRelativeBufferSizes() {
-        MockController controller = new MockController(Settings.builder()
-            .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "50%")
-            .build());
-
-        assertThat(controller.indexingBufferSize(), equalTo(new ByteSizeValue(50, ByteSizeUnit.MB)));
-    }
-
-
-    public void testMinBufferSizes() {
-        MockController controller = new MockController(Settings.builder()
-            .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "0.001%")
-            .put(IndexingMemoryController.MIN_INDEX_BUFFER_SIZE_SETTING, "6mb").build());
-
-        assertThat(controller.indexingBufferSize(), equalTo(new ByteSizeValue(6, ByteSizeUnit.MB)));
-    }
-
-    public void testMaxBufferSizes() {
-        MockController controller = new MockController(Settings.builder()
-            .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "90%")
-            .put(IndexingMemoryController.MAX_INDEX_BUFFER_SIZE_SETTING, "6mb").build());
-
-        assertThat(controller.indexingBufferSize(), equalTo(new ByteSizeValue(6, ByteSizeUnit.MB)));
-    }
-
-    protected void assertTwoActiveShards(MockController controller, ByteSizeValue indexBufferSize, ByteSizeValue translogBufferSize) {
-        createIndex("test", Settings.builder().put(SETTING_NUMBER_OF_SHARDS, 2).put(SETTING_NUMBER_OF_REPLICAS, 0).build());
-        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
-        IndexService test = indicesService.indexService("test");
-        IndexShard shard0 = test.getShard(0);
-        controller.simulateIndexing(shard0);
-        IndexShard shard1 = test.getShard(1);
-        controller.simulateIndexing(shard1);
-        controller.assertBuffers(shard0, indexBufferSize);
-        controller.assertBuffers(shard1, indexBufferSize);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java b/core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java
index d1cb219..b32cfef 100644
--- a/core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java
@@ -63,8 +63,6 @@ import static org.hamcrest.Matchers.nullValue;
  *
  */
 public class SimpleIndexTemplateIT extends ESIntegTestCase {
-
-    @AwaitsFix(bugUrl = "temporarily ignored till we have removed the ingest index template")
     public void testSimpleIndexTemplateTests() throws Exception {
         // clean all templates setup by the framework.
         client().admin().indices().prepareDeleteTemplate("*").get();
@@ -315,7 +313,6 @@ public class SimpleIndexTemplateIT extends ESIntegTestCase {
         }
     }
 
-    @AwaitsFix(bugUrl = "temporarily ignored till we have removed the ingest index template")
     public void testInvalidSettings() throws Exception {
         // clean all templates setup by the framework.
         client().admin().indices().prepareDeleteTemplate("*").get();
diff --git a/core/src/test/java/org/elasticsearch/ingest/IngestBootstrapperTests.java b/core/src/test/java/org/elasticsearch/ingest/IngestBootstrapperTests.java
deleted file mode 100644
index a352c3a..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/IngestBootstrapperTests.java
+++ /dev/null
@@ -1,276 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.ClusterChangedEvent;
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.block.ClusterBlocks;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.routing.IndexRoutingTable;
-import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
-import org.elasticsearch.cluster.routing.RoutingTable;
-import org.elasticsearch.cluster.routing.ShardRoutingState;
-import org.elasticsearch.cluster.routing.TestShardRouting;
-import org.elasticsearch.cluster.routing.UnassignedInfo;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.text.Text;
-import org.elasticsearch.discovery.DiscoverySettings;
-import org.elasticsearch.gateway.GatewayService;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.search.internal.InternalSearchHit;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-import org.junit.Before;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.core.Is.is;
-import static org.mockito.Matchers.any;
-import static org.mockito.Matchers.anyString;
-import static org.mockito.Mockito.doThrow;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.never;
-import static org.mockito.Mockito.times;
-import static org.mockito.Mockito.verify;
-import static org.mockito.Mockito.when;
-
-public class IngestBootstrapperTests extends ESTestCase {
-
-    private PipelineStore store;
-    private IngestBootstrapper bootstrapper;
-
-    @Before
-    public void init() {
-        ThreadPool threadPool = mock(ThreadPool.class);
-        when(threadPool.executor(any())).thenReturn(Runnable::run);
-        ClusterService clusterService = mock(ClusterService.class);
-        store = mock(PipelineStore.class);
-        when(store.isStarted()).thenReturn(false);
-        PipelineExecutionService pipelineExecutionService = mock(PipelineExecutionService.class);
-        bootstrapper = new IngestBootstrapper(Settings.EMPTY, threadPool, clusterService, store, pipelineExecutionService);
-    }
-
-    public void testStartAndStopInBackground() throws Exception {
-        ThreadPool threadPool = new ThreadPool("test");
-        Client client = mock(Client.class);
-        TransportService transportService = mock(TransportService.class);
-
-        ClusterService clusterService = mock(ClusterService.class);
-        when(client.search(any())).thenReturn(PipelineStoreTests.expectedSearchReponse(Collections.emptyList()));
-        when(client.searchScroll(any())).thenReturn(PipelineStoreTests.expectedSearchReponse(Collections.emptyList()));
-        Settings settings = Settings.EMPTY;
-        PipelineStore store = new PipelineStore(settings, clusterService, transportService);
-        IngestBootstrapper bootstrapper = new IngestBootstrapper(
-                settings, threadPool, clusterService, store, null
-        );
-        bootstrapper.setClient(client);
-
-        List<InternalSearchHit> hits = new ArrayList<>();
-        hits.add(new InternalSearchHit(0, "1", new Text("type"), Collections.emptyMap())
-                .sourceRef(new BytesArray("{\"description\": \"_description1\"}"))
-        );
-        when(client.search(any())).thenReturn(PipelineStoreTests.expectedSearchReponse(hits));
-        when(client.get(any())).thenReturn(PipelineStoreTests.expectedGetResponse(true));
-
-        try {
-            store.get("1");
-            fail("IllegalStateException expected");
-        } catch (IllegalStateException e) {
-            assertThat(e.getMessage(), equalTo("pipeline store isn't ready yet"));
-        }
-
-        MetaData metadata = MetaData.builder()
-            .put(IndexTemplateMetaData.builder(IngestBootstrapper.INGEST_INDEX_TEMPLATE_NAME))
-            .build();
-        bootstrapper.startPipelineStore(metadata);
-        assertBusy(() -> {
-            assertThat(store.isStarted(), is(true));
-            assertThat(store.get("1"), notNullValue());
-            assertThat(store.get("1").getId(), equalTo("1"));
-            assertThat(store.get("1").getDescription(), equalTo("_description1"));
-        });
-
-        bootstrapper.stopPipelineStore("testing stop");
-        assertBusy(() -> assertThat(store.isStarted(), is(false)));
-
-        // the map internal search hit holds gets emptied after use, which is ok, but in this test we need to reset the source:
-        hits.get(0).sourceRef(new BytesArray("{\"description\": \"_description1\"}"));
-        hits.add(new InternalSearchHit(0, "2", new Text("type"), Collections.emptyMap())
-                .sourceRef(new BytesArray("{\"description\": \"_description2\"}"))
-        );
-        bootstrapper.startPipelineStore(metadata);
-        assertBusy(() -> {
-            assertThat(store.isStarted(), is(true));
-            assertThat(store.get("1"), notNullValue());
-            assertThat(store.get("1").getId(), equalTo("1"));
-            assertThat(store.get("1").getDescription(), equalTo("_description1"));
-            assertThat(store.get("2"), notNullValue());
-            assertThat(store.get("2").getId(), equalTo("2"));
-            assertThat(store.get("2").getDescription(), equalTo("_description2"));
-        });
-        threadPool.shutdown();
-    }
-
-    public void testPipelineStoreBootstrappingGlobalStateNotRecoveredBlock() throws Exception {
-        ClusterState.Builder csBuilder = new ClusterState.Builder(new ClusterName("_name"));
-        csBuilder.blocks(ClusterBlocks.builder().addGlobalBlock(GatewayService.STATE_NOT_RECOVERED_BLOCK));
-        ClusterState cs = csBuilder.metaData(MetaData.builder()).build();
-        bootstrapper.clusterChanged(new ClusterChangedEvent("test", cs, cs));
-        verify(store, never()).start();
-        verify(store, never()).stop(anyString());
-    }
-
-    public void testPipelineStoreBootstrappingGlobalStateNoMasterBlock() throws Exception {
-        ClusterState.Builder csBuilder = new ClusterState.Builder(new ClusterName("_name"));
-        csBuilder.blocks(ClusterBlocks.builder()
-            .addGlobalBlock(randomBoolean() ? DiscoverySettings.NO_MASTER_BLOCK_WRITES : DiscoverySettings.NO_MASTER_BLOCK_ALL));
-        ClusterState cs = csBuilder.metaData(
-            MetaData.builder()
-                .put(IndexTemplateMetaData.builder(IngestBootstrapper.INGEST_INDEX_TEMPLATE_NAME))
-        ).build();
-
-        // We're not started and there is a no master block, doing nothing:
-        bootstrapper.clusterChanged(new ClusterChangedEvent("test", cs, cs));
-        verify(store, never()).start();
-        verify(store, never()).stop(anyString());
-
-        // We're started and there is a no master block, so we stop the store:
-        when(store.isStarted()).thenReturn(true);
-        bootstrapper.clusterChanged(new ClusterChangedEvent("test", cs, cs));
-        verify(store, never()).start();
-        verify(store, times(1)).stop(anyString());
-    }
-
-    public void testPipelineStoreBootstrappingNoIngestIndex() throws Exception {
-        ClusterState.Builder csBuilder = new ClusterState.Builder(new ClusterName("_name"));
-        ClusterState cs = csBuilder.metaData(MetaData.builder()
-            .put(IndexTemplateMetaData.builder(IngestBootstrapper.INGEST_INDEX_TEMPLATE_NAME)))
-            .build();
-        bootstrapper.clusterChanged(new ClusterChangedEvent("test", cs, cs));
-        verify(store, times(1)).start();
-    }
-
-    public void testPipelineStoreBootstrappingIngestIndexShardsNotStarted() throws Exception {
-        // .ingest index, but not all primary shards started:
-        ClusterState.Builder csBuilder = new ClusterState.Builder(new ClusterName("_name"));
-        MetaData.Builder metaDateBuilder = MetaData.builder();
-        metaDateBuilder.put(IndexTemplateMetaData.builder(IngestBootstrapper.INGEST_INDEX_TEMPLATE_NAME));
-        RoutingTable.Builder routingTableBuilder = RoutingTable.builder();
-        Settings settings = settings(Version.CURRENT)
-                .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
-                .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1)
-                .build();
-        metaDateBuilder.put(IndexMetaData.builder(PipelineStore.INDEX).settings(settings).numberOfShards(1).numberOfReplicas(1));
-        IndexRoutingTable.Builder indexRoutingTableBuilder = IndexRoutingTable.builder(PipelineStore.INDEX);
-        indexRoutingTableBuilder.addIndexShard(new IndexShardRoutingTable.Builder(new ShardId(PipelineStore.INDEX, 0))
-                .addShard(TestShardRouting.newShardRouting(PipelineStore.INDEX, 0, "_node_id", null, null, true, ShardRoutingState.INITIALIZING, 1, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "")))
-                .build());
-        indexRoutingTableBuilder.addReplica();
-        routingTableBuilder.add(indexRoutingTableBuilder.build());
-        csBuilder.metaData(metaDateBuilder);
-        csBuilder.routingTable(routingTableBuilder.build());
-        ClusterState cs = csBuilder.build();
-
-        // We're not running and the cluster state isn't ready, so we don't start.
-        bootstrapper.clusterChanged(new ClusterChangedEvent("test", cs, cs));
-        verify(store, never()).start();
-        verify(store, never()).stop(anyString());
-
-        // We're running and the cluster state indicates that all our shards are unassigned, so we stop.
-        when(store.isStarted()).thenReturn(true);
-        bootstrapper.clusterChanged(new ClusterChangedEvent("test", cs, cs));
-        verify(store, never()).start();
-        verify(store, times(1)).stop(anyString());
-    }
-
-    public void testPipelineStoreBootstrappingIngestIndexShardsStarted() throws Exception {
-        // .ingest index, but not all primary shards started:
-        ClusterState.Builder csBuilder = new ClusterState.Builder(new ClusterName("_name"));
-        MetaData.Builder metaDateBuilder = MetaData.builder();
-        metaDateBuilder.put(IndexTemplateMetaData.builder(IngestBootstrapper.INGEST_INDEX_TEMPLATE_NAME));
-        RoutingTable.Builder routingTableBuilder = RoutingTable.builder();
-        Settings settings = settings(Version.CURRENT)
-                .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
-                .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1)
-                .build();
-        metaDateBuilder.put(IndexMetaData.builder(PipelineStore.INDEX).settings(settings).numberOfShards(1).numberOfReplicas(1));
-        IndexRoutingTable.Builder indexRoutingTableBuilder = IndexRoutingTable.builder(PipelineStore.INDEX);
-        indexRoutingTableBuilder.addIndexShard(new IndexShardRoutingTable.Builder(new ShardId(PipelineStore.INDEX, 0))
-                .addShard(TestShardRouting.newShardRouting(PipelineStore.INDEX, 0, "_node_id", null, null, true, ShardRoutingState.STARTED, 1, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "")))
-                .build());
-        indexRoutingTableBuilder.addReplica();
-        routingTableBuilder.add(indexRoutingTableBuilder.build());
-        csBuilder.metaData(metaDateBuilder);
-        csBuilder.routingTable(routingTableBuilder.build());
-        ClusterState cs = csBuilder.build();
-
-        // We're not running and the cluster state is ready, so we start.
-        bootstrapper.clusterChanged(new ClusterChangedEvent("test", cs, cs));
-        verify(store, times(1)).start();
-        verify(store, never()).stop(anyString());
-
-        // We're running and the cluster state is good, so we do nothing.
-        when(store.isStarted()).thenReturn(true);
-        bootstrapper.clusterChanged(new ClusterChangedEvent("test", cs, cs));
-        verify(store, times(1)).start();
-        verify(store, never()).stop(anyString());
-    }
-
-    public void testPipelineStoreBootstrappingFailure() throws Exception {
-        // .ingest index, but not all primary shards started:
-        ClusterState.Builder csBuilder = new ClusterState.Builder(new ClusterName("_name"));
-        MetaData.Builder metaDateBuilder = MetaData.builder();
-        metaDateBuilder.put(IndexTemplateMetaData.builder(IngestBootstrapper.INGEST_INDEX_TEMPLATE_NAME));
-        RoutingTable.Builder routingTableBuilder = RoutingTable.builder();
-        Settings settings = settings(Version.CURRENT)
-            .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
-            .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1)
-            .build();
-        metaDateBuilder.put(IndexMetaData.builder(PipelineStore.INDEX).settings(settings).numberOfShards(1).numberOfReplicas(1));
-        IndexRoutingTable.Builder indexRoutingTableBuilder = IndexRoutingTable.builder(PipelineStore.INDEX);
-        indexRoutingTableBuilder.addIndexShard(new IndexShardRoutingTable.Builder(new ShardId(PipelineStore.INDEX, 0))
-            .addShard(TestShardRouting.newShardRouting(PipelineStore.INDEX, 0, "_node_id", null, null, true, ShardRoutingState.STARTED, 1, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "")))
-            .build());
-        indexRoutingTableBuilder.addReplica();
-        routingTableBuilder.add(indexRoutingTableBuilder.build());
-        csBuilder.metaData(metaDateBuilder);
-        csBuilder.routingTable(routingTableBuilder.build());
-        ClusterState cs = csBuilder.build();
-
-        // fail the first call with an runtime exception and subsequent calls just return:
-        doThrow(new RuntimeException()).doNothing().when(store).start();
-        bootstrapper.clusterChanged(new ClusterChangedEvent("test", cs, cs));
-        verify(store, times(2)).start();
-        verify(store, never()).stop(anyString());
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/IngestClientIT.java b/core/src/test/java/org/elasticsearch/ingest/IngestClientIT.java
deleted file mode 100644
index 3767aac..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/IngestClientIT.java
+++ /dev/null
@@ -1,225 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.action.bulk.BulkItemResponse;
-import org.elasticsearch.action.bulk.BulkRequest;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.action.ingest.DeletePipelineAction;
-import org.elasticsearch.action.ingest.DeletePipelineRequestBuilder;
-import org.elasticsearch.action.ingest.GetPipelineAction;
-import org.elasticsearch.action.ingest.GetPipelineRequestBuilder;
-import org.elasticsearch.action.ingest.GetPipelineResponse;
-import org.elasticsearch.action.ingest.PutPipelineAction;
-import org.elasticsearch.action.ingest.PutPipelineRequestBuilder;
-import org.elasticsearch.action.ingest.SimulateDocumentSimpleResult;
-import org.elasticsearch.action.ingest.SimulatePipelineAction;
-import org.elasticsearch.action.ingest.SimulatePipelineRequestBuilder;
-import org.elasticsearch.action.ingest.SimulatePipelineResponse;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.ESIntegTestCase;
-
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.nullValue;
-import static org.hamcrest.core.Is.is;
-
-public class IngestClientIT extends ESIntegTestCase {
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(IngestPlugin.class);
-    }
-
-    public void testSimulate() throws Exception {
-        new PutPipelineRequestBuilder(client(), PutPipelineAction.INSTANCE)
-                .setId("_id")
-                .setSource(jsonBuilder().startObject()
-                        .field("description", "my_pipeline")
-                        .startArray("processors")
-                        .startObject()
-                        .startObject("test")
-                        .endObject()
-                        .endObject()
-                        .endArray()
-                        .endObject().bytes())
-                .get();
-        GetPipelineResponse getResponse = new GetPipelineRequestBuilder(client(), GetPipelineAction.INSTANCE)
-                .setIds("_id")
-                .get();
-        assertThat(getResponse.isFound(), is(true));
-        assertThat(getResponse.pipelines().size(), equalTo(1));
-        assertThat(getResponse.pipelines().get(0).getId(), equalTo("_id"));
-
-        SimulatePipelineResponse response = new SimulatePipelineRequestBuilder(client(), SimulatePipelineAction.INSTANCE)
-                .setId("_id")
-                .setSource(jsonBuilder().startObject()
-                        .startArray("docs")
-                        .startObject()
-                        .field("_index", "index")
-                        .field("_type", "type")
-                        .field("_id", "id")
-                        .startObject("_source")
-                        .field("foo", "bar")
-                        .field("fail", false)
-                        .endObject()
-                        .endObject()
-                        .endArray()
-                        .endObject().bytes())
-                .get();
-
-        assertThat(response.isVerbose(), equalTo(false));
-        assertThat(response.getPipelineId(), equalTo("_id"));
-        assertThat(response.getResults().size(), equalTo(1));
-        assertThat(response.getResults().get(0), instanceOf(SimulateDocumentSimpleResult.class));
-        SimulateDocumentSimpleResult simulateDocumentSimpleResult = (SimulateDocumentSimpleResult) response.getResults().get(0);
-        Map<String, Object> source = new HashMap<>();
-        source.put("foo", "bar");
-        source.put("fail", false);
-        source.put("processed", true);
-        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, source);
-        assertThat(simulateDocumentSimpleResult.getIngestDocument().getSourceAndMetadata(), equalTo(ingestDocument.getSourceAndMetadata()));
-        assertThat(simulateDocumentSimpleResult.getFailure(), nullValue());
-    }
-
-    public void testBulkWithIngestFailures() throws Exception {
-        createIndex("index");
-
-        new PutPipelineRequestBuilder(client(), PutPipelineAction.INSTANCE)
-            .setId("_id")
-            .setSource(jsonBuilder().startObject()
-                .field("description", "my_pipeline")
-                .startArray("processors")
-                .startObject()
-                .startObject("test")
-                .endObject()
-                .endObject()
-                .endArray()
-                .endObject().bytes())
-            .get();
-
-        int numRequests = scaledRandomIntBetween(32, 128);
-        BulkRequest bulkRequest = new BulkRequest();
-        for (int i = 0; i < numRequests; i++) {
-            IndexRequest indexRequest = new IndexRequest("index", "type", Integer.toString(i)).pipeline("_id");
-            indexRequest.source("field", "value", "fail", i % 2 == 0);
-            bulkRequest.add(indexRequest);
-        }
-
-        BulkResponse response = client().bulk(bulkRequest).actionGet();
-        assertThat(response.getItems().length, equalTo(bulkRequest.requests().size()));
-        for (int i = 0; i < bulkRequest.requests().size(); i++) {
-            BulkItemResponse itemResponse = response.getItems()[i];
-            if (i % 2 == 0) {
-                BulkItemResponse.Failure failure = itemResponse.getFailure();
-                assertThat(failure.getMessage(), equalTo("java.lang.IllegalArgumentException: test processor failed"));
-            } else {
-                IndexResponse indexResponse = itemResponse.getResponse();
-                assertThat(indexResponse.getId(), equalTo(Integer.toString(i)));
-                assertThat(indexResponse.isCreated(), is(true));
-            }
-        }
-    }
-
-    public void test() throws Exception {
-        new PutPipelineRequestBuilder(client(), PutPipelineAction.INSTANCE)
-                .setId("_id")
-                .setSource(jsonBuilder().startObject()
-                        .field("description", "my_pipeline")
-                        .startArray("processors")
-                        .startObject()
-                        .startObject("test")
-                        .endObject()
-                        .endObject()
-                        .endArray()
-                        .endObject().bytes())
-                .get();
-        GetPipelineResponse getResponse = new GetPipelineRequestBuilder(client(), GetPipelineAction.INSTANCE)
-                .setIds("_id")
-                .get();
-        assertThat(getResponse.isFound(), is(true));
-        assertThat(getResponse.pipelines().size(), equalTo(1));
-        assertThat(getResponse.pipelines().get(0).getId(), equalTo("_id"));
-
-        client().prepareIndex("test", "type", "1").setPipeline("_id").setSource("field", "value", "fail", false).get();
-
-        Map<String, Object> doc = client().prepareGet("test", "type", "1")
-                .get().getSourceAsMap();
-        assertThat(doc.get("field"), equalTo("value"));
-        assertThat(doc.get("processed"), equalTo(true));
-
-        client().prepareBulk().add(
-                client().prepareIndex("test", "type", "2").setSource("field", "value2", "fail", false).setPipeline("_id")).get();
-        doc = client().prepareGet("test", "type", "2").get().getSourceAsMap();
-        assertThat(doc.get("field"), equalTo("value2"));
-        assertThat(doc.get("processed"), equalTo(true));
-
-        DeleteResponse response = new DeletePipelineRequestBuilder(client(), DeletePipelineAction.INSTANCE)
-                .setId("_id")
-                .get();
-        assertThat(response.isFound(), is(true));
-        assertThat(response.getId(), equalTo("_id"));
-
-        getResponse = new GetPipelineRequestBuilder(client(), GetPipelineAction.INSTANCE)
-                .setIds("_id")
-                .get();
-        assertThat(getResponse.isFound(), is(false));
-        assertThat(getResponse.pipelines().size(), equalTo(0));
-    }
-
-    @Override
-    protected Collection<Class<? extends Plugin>> getMockPlugins() {
-        return Collections.emptyList();
-    }
-
-    public static class IngestPlugin extends Plugin {
-
-        @Override
-        public String name() {
-            return "ingest";
-        }
-
-        @Override
-        public String description() {
-            return "ingest mock";
-        }
-
-        public void onModule(IngestModule ingestModule) {
-            ingestModule.registerProcessor("test", (environment, templateService) -> config ->
-                new TestProcessor("test", ingestDocument -> {
-                    ingestDocument.setFieldValue("processed", true);
-                    if (ingestDocument.getFieldValue("fail", Boolean.class)) {
-                        throw new IllegalArgumentException("test processor failed");
-                    }
-                })
-            );
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/PipelineExecutionServiceTests.java b/core/src/test/java/org/elasticsearch/ingest/PipelineExecutionServiceTests.java
deleted file mode 100644
index a0fc2b8..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/PipelineExecutionServiceTests.java
+++ /dev/null
@@ -1,364 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.bulk.BulkRequest;
-import org.elasticsearch.action.delete.DeleteRequest;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.update.UpdateRequest;
-import org.elasticsearch.common.collect.Tuple;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.ingest.core.CompoundProcessor;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.hamcrest.CustomTypeSafeMatcher;
-import org.junit.Before;
-import org.mockito.ArgumentMatcher;
-import org.mockito.invocation.InvocationOnMock;
-
-import java.util.Collections;
-import java.util.Map;
-import java.util.Objects;
-import java.util.function.Consumer;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.mockito.Matchers.any;
-import static org.mockito.Matchers.anyBoolean;
-import static org.mockito.Matchers.anyString;
-import static org.mockito.Matchers.argThat;
-import static org.mockito.Mockito.doAnswer;
-import static org.mockito.Mockito.doThrow;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.never;
-import static org.mockito.Mockito.times;
-import static org.mockito.Mockito.verify;
-import static org.mockito.Mockito.when;
-
-public class PipelineExecutionServiceTests extends ESTestCase {
-
-    private PipelineStore store;
-    private PipelineExecutionService executionService;
-
-    @Before
-    public void setup() {
-        store = mock(PipelineStore.class);
-        ThreadPool threadPool = mock(ThreadPool.class);
-        when(threadPool.executor(anyString())).thenReturn(Runnable::run);
-        executionService = new PipelineExecutionService(store, threadPool);
-    }
-
-    public void testExecuteIndexPipelineDoesNotExist() {
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).pipeline("_id");
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        try {
-            executionService.execute(indexRequest, failureHandler, completionHandler);
-            fail("IllegalArgumentException expected");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("pipeline with id [_id] does not exist"));
-        }
-        verify(failureHandler, never()).accept(any(Throwable.class));
-        verify(completionHandler, never()).accept(anyBoolean());
-    }
-
-    public void testExecuteBulkPipelineDoesNotExist() {
-        CompoundProcessor processor = mock(CompoundProcessor.class);
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", processor));
-        BulkRequest bulkRequest = new BulkRequest();
-
-        IndexRequest indexRequest1 = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).pipeline("_id");
-        bulkRequest.add(indexRequest1);
-        IndexRequest indexRequest2 = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).pipeline("does_not_exist");
-        bulkRequest.add(indexRequest2);
-        @SuppressWarnings("unchecked")
-        Consumer<Tuple<IndexRequest, Throwable>> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(bulkRequest.requests(), failureHandler, completionHandler);
-        verify(failureHandler, times(1)).accept(argThat(new CustomTypeSafeMatcher<Tuple<IndexRequest,Throwable>>("failure handler was not called with the expected arguments") {
-            @Override
-            protected boolean matchesSafely(Tuple<IndexRequest, Throwable> item) {
-                if( item.v1() != indexRequest2) {
-                    return false;
-                }
-                if (item.v2() instanceof IllegalArgumentException == false) {
-                    return false;
-                }
-                IllegalArgumentException iae = (IllegalArgumentException) item.v2();
-                return "pipeline with id [does_not_exist] does not exist".equals(iae.getMessage());
-            }
-
-        }));
-        verify(completionHandler, times(1)).accept(anyBoolean());
-    }
-
-    public void testExecuteSuccess() throws Exception {
-        CompoundProcessor processor = mock(CompoundProcessor.class);
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", processor));
-
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).pipeline("_id");
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-        verify(failureHandler, never()).accept(any());
-        verify(completionHandler, times(1)).accept(true);
-    }
-
-    public void testExecutePropagateAllMetaDataUpdates() throws Exception {
-        CompoundProcessor processor = mock(CompoundProcessor.class);
-        doAnswer((InvocationOnMock invocationOnMock) -> {
-            IngestDocument ingestDocument = (IngestDocument) invocationOnMock.getArguments()[0];
-            for (IngestDocument.MetaData metaData : IngestDocument.MetaData.values()) {
-                if (metaData == IngestDocument.MetaData.TTL) {
-                    ingestDocument.setFieldValue(IngestDocument.MetaData.TTL.getFieldName(), "5w");
-                } else {
-                    ingestDocument.setFieldValue(metaData.getFieldName(), "update" + metaData.getFieldName());
-                }
-
-            }
-            return null;
-        }).when(processor).execute(any());
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", processor));
-
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).pipeline("_id");
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-        verify(processor).execute(any());
-        verify(failureHandler, never()).accept(any());
-        verify(completionHandler, times(1)).accept(true);
-
-        assertThat(indexRequest.index(), equalTo("update_index"));
-        assertThat(indexRequest.type(), equalTo("update_type"));
-        assertThat(indexRequest.id(), equalTo("update_id"));
-        assertThat(indexRequest.routing(), equalTo("update_routing"));
-        assertThat(indexRequest.parent(), equalTo("update_parent"));
-        assertThat(indexRequest.timestamp(), equalTo("update_timestamp"));
-        assertThat(indexRequest.ttl(), equalTo(new TimeValue(3024000000L)));
-    }
-
-    public void testExecuteFailure() throws Exception {
-        CompoundProcessor processor = mock(CompoundProcessor.class);
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", processor));
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).pipeline("_id");
-        doThrow(new RuntimeException()).when(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-        verify(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        verify(failureHandler, times(1)).accept(any(RuntimeException.class));
-        verify(completionHandler, never()).accept(anyBoolean());
-    }
-
-    public void testExecuteSuccessWithOnFailure() throws Exception {
-        Processor processor = mock(Processor.class);
-        Processor onFailureProcessor = mock(Processor.class);
-        CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.singletonList(processor), Collections.singletonList(new CompoundProcessor(onFailureProcessor)));
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", compoundProcessor));
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).pipeline("_id");
-        doThrow(new RuntimeException()).when(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-        verify(failureHandler, never()).accept(any(RuntimeException.class));
-        verify(completionHandler, times(1)).accept(true);
-    }
-
-    public void testExecuteFailureWithOnFailure() throws Exception {
-        Processor processor = mock(Processor.class);
-        Processor onFailureProcessor = mock(Processor.class);
-        CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.singletonList(processor), Collections.singletonList(new CompoundProcessor(onFailureProcessor)));
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", compoundProcessor));
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).pipeline("_id");
-        doThrow(new RuntimeException()).when(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        doThrow(new RuntimeException()).when(onFailureProcessor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-        verify(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        verify(failureHandler, times(1)).accept(any(RuntimeException.class));
-        verify(completionHandler, never()).accept(anyBoolean());
-    }
-
-    public void testExecuteFailureWithNestedOnFailure() throws Exception {
-        Processor processor = mock(Processor.class);
-        Processor onFailureProcessor = mock(Processor.class);
-        Processor onFailureOnFailureProcessor = mock(Processor.class);
-        CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.singletonList(processor),
-            Collections.singletonList(new CompoundProcessor(Collections.singletonList(onFailureProcessor), Collections.singletonList(onFailureOnFailureProcessor))));
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", compoundProcessor));
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).pipeline("_id");
-        doThrow(new RuntimeException()).when(onFailureOnFailureProcessor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        doThrow(new RuntimeException()).when(onFailureProcessor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        doThrow(new RuntimeException()).when(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-        verify(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        verify(failureHandler, times(1)).accept(any(RuntimeException.class));
-        verify(completionHandler, never()).accept(anyBoolean());
-    }
-
-    public void testExecuteSetTTL() throws Exception {
-        Processor processor = new TestProcessor(ingestDocument -> ingestDocument.setFieldValue("_ttl", "5d"));
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", new CompoundProcessor(processor)));
-
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).pipeline("_id");
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-
-        assertThat(indexRequest.ttl(), equalTo(TimeValue.parseTimeValue("5d", null, "ttl")));
-        verify(failureHandler, never()).accept(any());
-        verify(completionHandler, times(1)).accept(true);
-    }
-
-    public void testExecuteSetInvalidTTL() throws Exception {
-        Processor processor = new TestProcessor(ingestDocument -> ingestDocument.setFieldValue("_ttl", "abc"));
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", new CompoundProcessor(processor)));
-
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).pipeline("_id");
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-        verify(failureHandler, times(1)).accept(any(ElasticsearchParseException.class));
-        verify(completionHandler, never()).accept(anyBoolean());
-    }
-
-    public void testExecuteProvidedTTL() throws Exception {
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", mock(CompoundProcessor.class)));
-
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").pipeline("_id")
-                .source(Collections.emptyMap())
-                .ttl(1000L);
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-
-        assertThat(indexRequest.ttl(), equalTo(new TimeValue(1000L)));
-        verify(failureHandler, never()).accept(any());
-        verify(completionHandler, times(1)).accept(true);
-    }
-
-    public void testBulkRequestExecutionWithFailures() throws Exception {
-        BulkRequest bulkRequest = new BulkRequest();
-        String pipelineId = "_id";
-
-        int numRequest = scaledRandomIntBetween(8, 64);
-        int numIndexRequests = 0;
-        for (int i = 0; i < numRequest; i++) {
-            ActionRequest request;
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    request = new DeleteRequest("_index", "_type", "_id");
-                } else {
-                    request = new UpdateRequest("_index", "_type", "_id");
-                }
-            } else {
-                IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").pipeline(pipelineId);
-                indexRequest.source("field1", "value1");
-                request = indexRequest;
-                numIndexRequests++;
-            }
-            bulkRequest.add(request);
-        }
-
-        CompoundProcessor processor = mock(CompoundProcessor.class);
-        Exception error = new RuntimeException();
-        doThrow(error).when(processor).execute(any());
-        when(store.get(pipelineId)).thenReturn(new Pipeline(pipelineId, null, processor));
-
-        Consumer<Tuple<IndexRequest, Throwable>> requestItemErrorHandler = mock(Consumer.class);
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(bulkRequest.requests(), requestItemErrorHandler, completionHandler);
-
-        verify(requestItemErrorHandler, times(numIndexRequests)).accept(new Tuple<>(any(IndexRequest.class), error));
-        verify(completionHandler, times(1)).accept(true);
-    }
-
-    public void testBulkRequestExecution() throws Exception {
-        BulkRequest bulkRequest = new BulkRequest();
-        String pipelineId = "_id";
-
-        int numRequest = scaledRandomIntBetween(8, 64);
-        for (int i = 0; i < numRequest; i++) {
-            IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").pipeline(pipelineId);
-            indexRequest.source("field1", "value1");
-            bulkRequest.add(indexRequest);
-        }
-
-        when(store.get(pipelineId)).thenReturn(new Pipeline(pipelineId, null, new CompoundProcessor()));
-
-        @SuppressWarnings("unchecked")
-        Consumer<Tuple<IndexRequest, Throwable>> requestItemErrorHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(bulkRequest.requests(), requestItemErrorHandler, completionHandler);
-
-        verify(requestItemErrorHandler, never()).accept(any());
-        verify(completionHandler, times(1)).accept(true);
-    }
-
-    private IngestDocument eqID(String index, String type, String id, Map<String, Object> source) {
-        return argThat(new IngestDocumentMatcher(index, type, id, source));
-    }
-
-    private class IngestDocumentMatcher extends ArgumentMatcher<IngestDocument> {
-
-        private final IngestDocument ingestDocument;
-
-        public IngestDocumentMatcher(String index, String type, String id, Map<String, Object> source) {
-            this.ingestDocument = new IngestDocument(index, type, id, null, null, null, null, source);
-        }
-
-        @Override
-        public boolean matches(Object o) {
-            if (o.getClass() == IngestDocument.class) {
-                IngestDocument otherIngestDocument = (IngestDocument) o;
-                //ingest metadata will not be the same (timestamp differs every time)
-                return Objects.equals(ingestDocument.getSourceAndMetadata(), otherIngestDocument.getSourceAndMetadata());
-            }
-            return false;
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/PipelineFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/PipelineFactoryTests.java
deleted file mode 100644
index e1a46e7..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/PipelineFactoryTests.java
+++ /dev/null
@@ -1,95 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class PipelineFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        Map<String, Object> processorConfig = new HashMap<>();
-        Map<String, Object> pipelineConfig = new HashMap<>();
-        pipelineConfig.put("description", "_description");
-        pipelineConfig.put("processors", Collections.singletonList(Collections.singletonMap("test", processorConfig)));
-        Pipeline.Factory factory = new Pipeline.Factory();
-        Map<String, Processor.Factory> processorRegistry = Collections.singletonMap("test", new TestProcessor.Factory());
-        Pipeline pipeline = factory.create("_id", pipelineConfig, processorRegistry);
-        assertThat(pipeline.getId(), equalTo("_id"));
-        assertThat(pipeline.getDescription(), equalTo("_description"));
-        assertThat(pipeline.getProcessors().size(), equalTo(1));
-        assertThat(pipeline.getProcessors().get(0).getType(), equalTo("test-processor"));
-    }
-
-    public void testCreateWithPipelineOnFailure() throws Exception {
-        Map<String, Object> processorConfig = new HashMap<>();
-        Map<String, Object> pipelineConfig = new HashMap<>();
-        pipelineConfig.put("description", "_description");
-        pipelineConfig.put("processors", Collections.singletonList(Collections.singletonMap("test", processorConfig)));
-        pipelineConfig.put("on_failure", Collections.singletonList(Collections.singletonMap("test", processorConfig)));
-        Pipeline.Factory factory = new Pipeline.Factory();
-        Map<String, Processor.Factory> processorRegistry = Collections.singletonMap("test", new TestProcessor.Factory());
-        Pipeline pipeline = factory.create("_id", pipelineConfig, processorRegistry);
-        assertThat(pipeline.getId(), equalTo("_id"));
-        assertThat(pipeline.getDescription(), equalTo("_description"));
-        assertThat(pipeline.getProcessors().size(), equalTo(1));
-        assertThat(pipeline.getProcessors().get(0).getType(), equalTo("test-processor"));
-        assertThat(pipeline.getOnFailureProcessors().size(), equalTo(1));
-        assertThat(pipeline.getOnFailureProcessors().get(0).getType(), equalTo("test-processor"));
-    }
-
-    public void testCreateUnusedProcessorOptions() throws Exception {
-        Map<String, Object> processorConfig = new HashMap<>();
-        processorConfig.put("unused", "value");
-        Map<String, Object> pipelineConfig = new HashMap<>();
-        pipelineConfig.put("description", "_description");
-        pipelineConfig.put("processors", Collections.singletonList(Collections.singletonMap("test", processorConfig)));
-        Pipeline.Factory factory = new Pipeline.Factory();
-        Map<String, Processor.Factory> processorRegistry = Collections.singletonMap("test", new TestProcessor.Factory());
-        try {
-            factory.create("_id", pipelineConfig, processorRegistry);
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("processor [test] doesn't support one or more provided configuration parameters [unused]"));
-        }
-    }
-
-    public void testCreateProcessorsWithOnFailureProperties() throws Exception {
-        Map<String, Object> processorConfig = new HashMap<>();
-        processorConfig.put("on_failure", Collections.singletonList(Collections.singletonMap("test", new HashMap<>())));
-
-        Map<String, Object> pipelineConfig = new HashMap<>();
-        pipelineConfig.put("description", "_description");
-        pipelineConfig.put("processors", Collections.singletonList(Collections.singletonMap("test", processorConfig)));
-        Pipeline.Factory factory = new Pipeline.Factory();
-        Map<String, Processor.Factory> processorRegistry = Collections.singletonMap("test", new TestProcessor.Factory());
-        Pipeline pipeline = factory.create("_id", pipelineConfig, processorRegistry);
-        assertThat(pipeline.getId(), equalTo("_id"));
-        assertThat(pipeline.getDescription(), equalTo("_description"));
-        assertThat(pipeline.getProcessors().size(), equalTo(1));
-        assertThat(pipeline.getProcessors().get(0).getType(), equalTo("compound[test-processor]"));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java b/core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java
deleted file mode 100644
index 57086cc..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java
+++ /dev/null
@@ -1,462 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.action.ActionFuture;
-import org.elasticsearch.action.get.GetRequest;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.support.PlainActionFuture;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.text.Text;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.get.GetResult;
-import org.elasticsearch.search.internal.InternalSearchHit;
-import org.elasticsearch.search.internal.InternalSearchHits;
-import org.elasticsearch.search.internal.InternalSearchResponse;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.transport.TransportService;
-import org.junit.Before;
-import org.mockito.ArgumentMatcher;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Objects;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.TimeUnit;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.nullValue;
-import static org.mockito.Matchers.any;
-import static org.mockito.Matchers.argThat;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-
-public class PipelineStoreTests extends ESTestCase {
-
-    private PipelineStore store;
-    private Client client;
-
-    @Before
-    public void init() throws Exception {
-        Settings settings = Settings.EMPTY;
-        ClusterService clusterService = mock(ClusterService.class);
-        TransportService transportService = mock(TransportService.class);
-
-        client = mock(Client.class);
-        when(client.search(any())).thenReturn(expectedSearchReponse(Collections.emptyList()));
-        when(client.searchScroll(any())).thenReturn(expectedSearchReponse(Collections.emptyList()));
-        store = new PipelineStore(settings, clusterService, transportService);
-        store.setClient(client);
-        store.start();
-    }
-
-    public void testUpdatePipeline() throws Exception {
-        List<InternalSearchHit> hits = new ArrayList<>();
-        hits.add(new InternalSearchHit(0, "1", new Text("type"), Collections.emptyMap())
-                .sourceRef(new BytesArray("{\"description\": \"_description1\"}"))
-        );
-
-        when(client.search(any())).thenReturn(expectedSearchReponse(hits));
-        when(client.get(any())).thenReturn(expectedGetResponse(true));
-        assertThat(store.get("1"), nullValue());
-
-        store.updatePipelines();
-        assertThat(store.get("1").getId(), equalTo("1"));
-        assertThat(store.get("1").getDescription(), equalTo("_description1"));
-
-        when(client.get(any())).thenReturn(expectedGetResponse(true));
-        hits.add(new InternalSearchHit(0, "2", new Text("type"), Collections.emptyMap())
-                        .sourceRef(new BytesArray("{\"description\": \"_description2\"}"))
-        );
-        store.updatePipelines();
-        assertThat(store.get("1").getId(), equalTo("1"));
-        assertThat(store.get("1").getDescription(), equalTo("_description1"));
-        assertThat(store.get("2").getId(), equalTo("2"));
-        assertThat(store.get("2").getDescription(), equalTo("_description2"));
-
-        hits.remove(1);
-        when(client.get(eqGetRequest(PipelineStore.INDEX, PipelineStore.TYPE, "2"))).thenReturn(expectedGetResponse(false));
-        store.updatePipelines();
-        assertThat(store.get("1").getId(), equalTo("1"));
-        assertThat(store.get("1").getDescription(), equalTo("_description1"));
-        assertThat(store.get("2"), nullValue());
-    }
-
-    public void testGetReference() throws Exception {
-        // fill the store up for the test:
-        List<InternalSearchHit> hits = new ArrayList<>();
-        hits.add(new InternalSearchHit(0, "foo", new Text("type"), Collections.emptyMap()).sourceRef(new BytesArray("{\"description\": \"_description\"}")));
-        hits.add(new InternalSearchHit(0, "bar", new Text("type"), Collections.emptyMap()).sourceRef(new BytesArray("{\"description\": \"_description\"}")));
-        hits.add(new InternalSearchHit(0, "foobar", new Text("type"), Collections.emptyMap()).sourceRef(new BytesArray("{\"description\": \"_description\"}")));
-        when(client.search(any())).thenReturn(expectedSearchReponse(hits));
-        store.updatePipelines();
-
-        List<PipelineDefinition> result = store.getReference("foo");
-        assertThat(result.size(), equalTo(1));
-        assertThat(result.get(0).getPipeline().getId(), equalTo("foo"));
-
-        result = store.getReference("foo*");
-        // to make sure the order is consistent in the test:
-        result.sort((first, second) -> {
-            return first.getPipeline().getId().compareTo(second.getPipeline().getId());
-        });
-        assertThat(result.size(), equalTo(2));
-        assertThat(result.get(0).getPipeline().getId(), equalTo("foo"));
-        assertThat(result.get(1).getPipeline().getId(), equalTo("foobar"));
-
-        result = store.getReference("bar*");
-        assertThat(result.size(), equalTo(1));
-        assertThat(result.get(0).getPipeline().getId(), equalTo("bar"));
-
-        result = store.getReference("*");
-        // to make sure the order is consistent in the test:
-        result.sort((first, second) -> {
-            return first.getPipeline().getId().compareTo(second.getPipeline().getId());
-        });
-        assertThat(result.size(), equalTo(3));
-        assertThat(result.get(0).getPipeline().getId(), equalTo("bar"));
-        assertThat(result.get(1).getPipeline().getId(), equalTo("foo"));
-        assertThat(result.get(2).getPipeline().getId(), equalTo("foobar"));
-
-        result = store.getReference("foo", "bar");
-        assertThat(result.size(), equalTo(2));
-        assertThat(result.get(0).getPipeline().getId(), equalTo("foo"));
-        assertThat(result.get(1).getPipeline().getId(), equalTo("bar"));
-    }
-
-    public void testValidateIngestIndex() throws Exception {
-        // ingest index doesn't exist:
-        ClusterState state = ClusterState.builder(new ClusterName("_name"))
-            .metaData(MetaData.builder())
-            .build();
-        assertThat(store.isIngestIndexPresent(state), equalTo(false));
-
-        // ingest index does exist and is valid:
-        IndexMetaData.Builder indexMetaData = IndexMetaData.builder(PipelineStore.INDEX)
-            .settings(Settings.builder()
-                .put(PipelineStore.INGEST_INDEX_SETTING)
-                .put("index.version.created", Version.CURRENT)
-            )
-            .putMapping(PipelineStore.TYPE, PipelineStore.PIPELINE_MAPPING);
-        state = ClusterState.builder(new ClusterName("_name"))
-            .metaData(MetaData.builder().put(indexMetaData))
-            .build();
-        assertThat(store.isIngestIndexPresent(state), equalTo(true));
-
-        // fails, has dynamic mapping
-        indexMetaData = IndexMetaData.builder(PipelineStore.INDEX)
-            .settings(Settings.builder()
-                .put(PipelineStore.INGEST_INDEX_SETTING)
-                .put("index.mapper.dynamic", true)
-                .put("index.version.created", Version.CURRENT)
-            )
-            .putMapping(PipelineStore.TYPE, PipelineStore.PIPELINE_MAPPING);
-        state = ClusterState.builder(new ClusterName("_name"))
-            .metaData(MetaData.builder().put(indexMetaData))
-            .build();
-        try {
-            store.isIngestIndexPresent(state);
-            fail("exception expected");
-        } catch (IllegalStateException e) {
-            assertThat(e.getMessage(), equalTo("illegal ingest index setting, [index.mapper.dynamic] setting is [true] while [false] is expected"));
-        }
-
-        // fails, incorrect number of primary shards
-        indexMetaData = IndexMetaData.builder(PipelineStore.INDEX)
-            .settings(Settings.builder()
-                .put(PipelineStore.INGEST_INDEX_SETTING)
-                .put("index.number_of_shards", 2)
-                .put("index.version.created", Version.CURRENT)
-            )
-            .putMapping(PipelineStore.TYPE, PipelineStore.PIPELINE_MAPPING);
-        state = ClusterState.builder(new ClusterName("_name"))
-            .metaData(MetaData.builder().put(indexMetaData))
-            .build();
-        try {
-            store.isIngestIndexPresent(state);
-            fail("exception expected");
-        } catch (IllegalStateException e) {
-            assertThat(e.getMessage(), equalTo("illegal ingest index setting, [index.number_of_shards] setting is [2] while [1] is expected"));
-        }
-
-        // fails, incorrect number of replica shards
-        indexMetaData = IndexMetaData.builder(PipelineStore.INDEX)
-            .settings(Settings.builder()
-                .put(PipelineStore.INGEST_INDEX_SETTING)
-                .put("index.number_of_replicas", 2)
-                .put("index.version.created", Version.CURRENT)
-            )
-            .putMapping(PipelineStore.TYPE, PipelineStore.PIPELINE_MAPPING);
-        state = ClusterState.builder(new ClusterName("_name"))
-            .metaData(MetaData.builder().put(indexMetaData))
-            .build();
-        try {
-            store.isIngestIndexPresent(state);
-            fail("exception expected");
-        } catch (IllegalStateException e) {
-            assertThat(e.getMessage(), equalTo("illegal ingest index setting, [index.number_of_replicas] setting is [2] while [1] is expected"));
-        }
-
-        // fails not a strict mapping:
-        String mapping = XContentFactory.jsonBuilder().startObject()
-            .startObject("_all")
-                .field("enabled", false)
-            .endObject()
-            .startObject("properties")
-                .startObject("processors")
-                    .field("type", "object")
-                    .field("enabled", false)
-                    .field("dynamic", true)
-                .endObject()
-                .startObject("on_failure")
-                    .field("type", "object")
-                    .field("enabled", false)
-                    .field("dynamic", true)
-                .endObject()
-                .startObject("description")
-                    .field("type", "string")
-                .endObject()
-            .endObject()
-            .endObject().string();
-        indexMetaData = IndexMetaData.builder(PipelineStore.INDEX)
-            .settings(Settings.builder()
-                .put(PipelineStore.INGEST_INDEX_SETTING)
-                .put("index.version.created", Version.CURRENT)
-            )
-            .putMapping(PipelineStore.TYPE, mapping);
-        state = ClusterState.builder(new ClusterName("_name"))
-            .metaData(MetaData.builder().put(indexMetaData))
-            .build();
-        try {
-            store.isIngestIndexPresent(state);
-            fail("exception expected");
-        } catch (IllegalStateException e) {
-            assertThat(e.getMessage(), equalTo("illegal ingest mapping, pipeline mapping must be strict"));
-        }
-
-        // fails _all field is enabled:
-        mapping = XContentFactory.jsonBuilder().startObject()
-            .field("dynamic", "strict")
-            .startObject("_all")
-                .field("enabled", true)
-            .endObject()
-            .startObject("properties")
-                .startObject("processors")
-                    .field("type", "object")
-                    .field("enabled", false)
-                    .field("dynamic", "true")
-                .endObject()
-                .startObject("on_failure")
-                    .field("type", "object")
-                    .field("enabled", false)
-                    .field("dynamic", "true")
-                .endObject()
-                .startObject("description")
-                    .field("type", "string")
-                .endObject()
-            .endObject()
-            .endObject().string();
-        indexMetaData = IndexMetaData.builder(PipelineStore.INDEX)
-            .settings(Settings.builder()
-                .put(PipelineStore.INGEST_INDEX_SETTING)
-                .put("index.version.created", Version.CURRENT)
-            )
-            .putMapping(PipelineStore.TYPE, mapping);
-        state = ClusterState.builder(new ClusterName("_name"))
-            .metaData(MetaData.builder().put(indexMetaData))
-            .build();
-        try {
-            store.isIngestIndexPresent(state);
-            fail("exception expected");
-        } catch (IllegalStateException e) {
-            assertThat(e.getMessage(), equalTo("illegal ingest mapping, _all field is enabled"));
-        }
-
-        // fails processor field not of type object:
-        mapping = XContentFactory.jsonBuilder().startObject()
-            .field("dynamic", "strict")
-            .startObject("_all")
-                .field("enabled", false)
-            .endObject()
-            .startObject("properties")
-                .startObject("processors")
-                    .field("type", "nested")
-                    .field("enabled", false)
-                    .field("dynamic", "true")
-                .endObject()
-                .startObject("on_failure")
-                    .field("type", "object")
-                    .field("enabled", false)
-                    .field("dynamic", "true")
-                .endObject()
-                .startObject("description")
-                    .field("type", "string")
-                .endObject()
-            .endObject()
-            .endObject().string();
-        indexMetaData = IndexMetaData.builder(PipelineStore.INDEX)
-            .settings(Settings.builder()
-                .put(PipelineStore.INGEST_INDEX_SETTING)
-                .put("index.version.created", Version.CURRENT)
-            )
-            .putMapping(PipelineStore.TYPE, mapping);
-        state = ClusterState.builder(new ClusterName("_name"))
-            .metaData(MetaData.builder().put(indexMetaData))
-            .build();
-        try {
-            store.isIngestIndexPresent(state);
-            fail("exception expected");
-        } catch (IllegalStateException e) {
-            assertThat(e.getMessage(), equalTo("illegal ingest mapping, processors field's type is [nested] while [object] is expected"));
-        }
-
-        // fails processor field enabled option is true:
-        mapping = XContentFactory.jsonBuilder().startObject()
-            .field("dynamic", "strict")
-            .startObject("_all")
-                .field("enabled", false)
-            .endObject()
-            .startObject("properties")
-                .startObject("processors")
-                    .field("type", "object")
-                    .field("enabled", true)
-                    .field("dynamic", "true")
-                .endObject()
-                .startObject("on_failure")
-                    .field("type", "object")
-                    .field("enabled", false)
-                    .field("dynamic", "true")
-                .endObject()
-                .startObject("description")
-                    .field("type", "string")
-                .endObject()
-            .endObject()
-            .endObject().string();
-        indexMetaData = IndexMetaData.builder(PipelineStore.INDEX)
-            .settings(Settings.builder()
-                .put(PipelineStore.INGEST_INDEX_SETTING)
-                .put("index.version.created", Version.CURRENT)
-            )
-            .putMapping(PipelineStore.TYPE, mapping);
-        state = ClusterState.builder(new ClusterName("_name"))
-            .metaData(MetaData.builder().put(indexMetaData))
-            .build();
-        try {
-            store.isIngestIndexPresent(state);
-            fail("exception expected");
-        } catch (IllegalStateException e) {
-            assertThat(e.getMessage(), equalTo("illegal ingest mapping, processors field enabled option is [true] while [false] is expected"));
-        }
-
-        // fails processor field dynamic option is false:
-        mapping = XContentFactory.jsonBuilder().startObject()
-            .field("dynamic", "strict")
-            .startObject("_all")
-                .field("enabled", false)
-            .endObject()
-            .startObject("properties")
-                .startObject("processors")
-                    .field("type", "object")
-                    .field("enabled", false)
-                    .field("dynamic", "false")
-                .endObject()
-                .startObject("on_failure")
-                    .field("type", "object")
-                    .field("enabled", false)
-                    .field("dynamic", "true")
-                .endObject()
-                .startObject("description")
-                    .field("type", "string")
-                .endObject()
-            .endObject()
-            .endObject().string();
-        indexMetaData = IndexMetaData.builder(PipelineStore.INDEX)
-            .settings(Settings.builder()
-                .put(PipelineStore.INGEST_INDEX_SETTING)
-                .put("index.version.created", Version.CURRENT)
-            )
-            .putMapping(PipelineStore.TYPE, mapping);
-        state = ClusterState.builder(new ClusterName("_name"))
-            .metaData(MetaData.builder().put(indexMetaData))
-            .build();
-        try {
-            store.isIngestIndexPresent(state);
-            fail("exception expected");
-        } catch (IllegalStateException e) {
-            assertThat(e.getMessage(), equalTo("illegal ingest mapping, processors field dynamic option is [false] while [true] is expected"));
-        }
-    }
-
-    static ActionFuture<SearchResponse> expectedSearchReponse(List<InternalSearchHit> hits) {
-        return new PlainActionFuture<SearchResponse>() {
-
-            @Override
-            public SearchResponse get(long timeout, TimeUnit unit) {
-                InternalSearchHits hits1 = new InternalSearchHits(hits.toArray(new InternalSearchHit[0]), hits.size(), 1f);
-                return new SearchResponse(new InternalSearchResponse(hits1, null, null, null, false, null), "_scrollId", 1, 1, 1, null);
-            }
-        };
-    }
-
-    static ActionFuture<GetResponse> expectedGetResponse(boolean exists) {
-        return new PlainActionFuture<GetResponse>() {
-            @Override
-            public GetResponse get() throws InterruptedException, ExecutionException {
-                return new GetResponse(new GetResult("_index", "_type", "_id", 1, exists, null, null));
-            }
-        };
-    }
-
-    static GetRequest eqGetRequest(String index, String type, String id) {
-        return argThat(new GetRequestMatcher(index, type, id));
-    }
-
-    static class GetRequestMatcher extends ArgumentMatcher<GetRequest> {
-
-        private final String index;
-        private final String type;
-        private final String id;
-
-        public GetRequestMatcher(String index, String type, String id) {
-            this.index = index;
-            this.type = type;
-            this.id = id;
-        }
-
-        @Override
-        public boolean matches(Object o) {
-            GetRequest getRequest = (GetRequest) o;
-            return Objects.equals(getRequest.index(), index) &&
-                    Objects.equals(getRequest.type(), type) &&
-                    Objects.equals(getRequest.id(), id);
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/ProcessorsRegistryTests.java b/core/src/test/java/org/elasticsearch/ingest/ProcessorsRegistryTests.java
deleted file mode 100644
index 2869fff..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/ProcessorsRegistryTests.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Map;
-import java.util.Set;
-import java.util.function.BiFunction;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class ProcessorsRegistryTests extends ESTestCase {
-
-    public void testAddProcessor() {
-        ProcessorsRegistry processorsRegistry = new ProcessorsRegistry();
-        TestProcessor.Factory factory1 = new TestProcessor.Factory();
-        processorsRegistry.registerProcessor("1", (environment, templateService) -> factory1);
-        TestProcessor.Factory factory2 = new TestProcessor.Factory();
-        processorsRegistry.registerProcessor("2", (environment, templateService) -> factory2);
-        TestProcessor.Factory factory3 = new TestProcessor.Factory();
-        try {
-            processorsRegistry.registerProcessor("1", (environment, templateService) -> factory3);
-            fail("addProcessor should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("Processor factory already registered for name [1]"));
-        }
-
-        Set<Map.Entry<String, BiFunction<Environment, TemplateService, Processor.Factory<?>>>> entrySet = processorsRegistry.entrySet();
-        assertThat(entrySet.size(), equalTo(2));
-        for (Map.Entry<String, BiFunction<Environment, TemplateService, Processor.Factory<?>>> entry : entrySet) {
-            if (entry.getKey().equals("1")) {
-                assertThat(entry.getValue().apply(null, null), equalTo(factory1));
-            } else if (entry.getKey().equals("2")) {
-                assertThat(entry.getValue().apply(null, null), equalTo(factory2));
-            } else {
-                fail("unexpected processor id [" + entry.getKey() + "]");
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/core/CompoundProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/core/CompoundProcessorTests.java
deleted file mode 100644
index 6cc38e1..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/core/CompoundProcessorTests.java
+++ /dev/null
@@ -1,111 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-import org.elasticsearch.ingest.TestProcessor;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.Matchers.is;
-
-public class CompoundProcessorTests extends ESTestCase {
-    private IngestDocument ingestDocument;
-
-    @Before
-    public void init() {
-        ingestDocument = new IngestDocument(new HashMap<>(), new HashMap<>());
-    }
-
-    public void testEmpty() throws Exception {
-        CompoundProcessor processor = new CompoundProcessor();
-        assertThat(processor.getProcessors().isEmpty(), is(true));
-        assertThat(processor.getOnFailureProcessors().isEmpty(), is(true));
-        processor.execute(ingestDocument);
-    }
-
-    public void testSingleProcessor() throws Exception {
-        TestProcessor processor = new TestProcessor(ingestDocument -> {});
-        CompoundProcessor compoundProcessor = new CompoundProcessor(processor);
-        assertThat(compoundProcessor.getProcessors().size(), equalTo(1));
-        assertThat(compoundProcessor.getProcessors().get(0), equalTo(processor));
-        assertThat(compoundProcessor.getOnFailureProcessors().isEmpty(), is(true));
-        compoundProcessor.execute(ingestDocument);
-        assertThat(processor.getInvokedCounter(), equalTo(1));
-    }
-
-    public void testSingleProcessorWithException() throws Exception {
-        TestProcessor processor = new TestProcessor(ingestDocument -> {throw new RuntimeException("error");});
-        CompoundProcessor compoundProcessor = new CompoundProcessor(processor);
-        assertThat(compoundProcessor.getProcessors().size(), equalTo(1));
-        assertThat(compoundProcessor.getProcessors().get(0), equalTo(processor));
-        assertThat(compoundProcessor.getOnFailureProcessors().isEmpty(), is(true));
-        try {
-            compoundProcessor.execute(ingestDocument);
-            fail("should throw exception");
-        } catch (Exception e) {
-            assertThat(e.getMessage(), equalTo("error"));
-        }
-        assertThat(processor.getInvokedCounter(), equalTo(1));
-    }
-
-    public void testSingleProcessorWithOnFailureProcessor() throws Exception {
-        TestProcessor processor1 = new TestProcessor("first", ingestDocument -> {throw new RuntimeException("error");});
-        TestProcessor processor2 = new TestProcessor(ingestDocument -> {
-            Map<String, String> ingestMetadata = ingestDocument.getIngestMetadata();
-            assertThat(ingestMetadata.size(), equalTo(2));
-            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_MESSAGE_FIELD), equalTo("error"));
-            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_PROCESSOR_FIELD), equalTo("first"));
-        });
-
-        CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.singletonList(processor1), Collections.singletonList(processor2));
-        compoundProcessor.execute(ingestDocument);
-
-        assertThat(processor1.getInvokedCounter(), equalTo(1));
-        assertThat(processor2.getInvokedCounter(), equalTo(1));
-    }
-
-    public void testSingleProcessorWithNestedFailures() throws Exception {
-        TestProcessor processor = new TestProcessor("first", ingestDocument -> {throw new RuntimeException("error");});
-        TestProcessor processorToFail = new TestProcessor("second", ingestDocument -> {
-            Map<String, String> ingestMetadata = ingestDocument.getIngestMetadata();
-            assertThat(ingestMetadata.size(), equalTo(2));
-            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_MESSAGE_FIELD), equalTo("error"));
-            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_PROCESSOR_FIELD), equalTo("first"));
-            throw new RuntimeException("error");
-        });
-        TestProcessor lastProcessor = new TestProcessor(ingestDocument -> {
-            Map<String, String> ingestMetadata = ingestDocument.getIngestMetadata();
-            assertThat(ingestMetadata.size(), equalTo(2));
-            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_MESSAGE_FIELD), equalTo("error"));
-            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_PROCESSOR_FIELD), equalTo("second"));
-        });
-        CompoundProcessor compoundOnFailProcessor = new CompoundProcessor(Collections.singletonList(processorToFail), Collections.singletonList(lastProcessor));
-        CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.singletonList(processor), Collections.singletonList(compoundOnFailProcessor));
-        compoundProcessor.execute(ingestDocument);
-
-        assertThat(processorToFail.getInvokedCounter(), equalTo(1));
-        assertThat(lastProcessor.getInvokedCounter(), equalTo(1));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/core/ConfigurationUtilsTests.java b/core/src/test/java/org/elasticsearch/ingest/core/ConfigurationUtilsTests.java
deleted file mode 100644
index 958378f..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/core/ConfigurationUtilsTests.java
+++ /dev/null
@@ -1,69 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-
-
-public class ConfigurationUtilsTests extends ESTestCase {
-    private Map<String, Object> config;
-
-    @Before
-    public void setConfig() {
-        config = new HashMap<>();
-        config.put("foo", "bar");
-        config.put("arr", Arrays.asList("1", "2", "3"));
-        List<Integer> list = new ArrayList<>();
-        list.add(2);
-        config.put("int", list);
-        config.put("ip", "127.0.0.1");
-        Map<String, Object> fizz = new HashMap<>();
-        fizz.put("buzz", "hello world");
-        config.put("fizz", fizz);
-    }
-
-    public void testReadStringProperty() {
-        String val = ConfigurationUtils.readStringProperty(config, "foo");
-        assertThat(val, equalTo("bar"));
-    }
-
-    public void testReadStringPropertyInvalidType() {
-        try {
-            ConfigurationUtils.readStringProperty(config, "arr");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("property [arr] isn't a string, but of type [java.util.Arrays$ArrayList]"));
-        }
-    }
-
-    // TODO(talevy): Issue with generics. This test should fail, "int" is of type List<Integer>
-    public void testOptional_InvalidType() {
-        List<String> val = ConfigurationUtils.readList(config, "int");
-        assertThat(val, equalTo(Arrays.asList(2)));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/core/IngestDocumentTests.java b/core/src/test/java/org/elasticsearch/ingest/core/IngestDocumentTests.java
deleted file mode 100644
index 56d1fa7..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/core/IngestDocumentTests.java
+++ /dev/null
@@ -1,976 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.text.DateFormat;
-import java.text.SimpleDateFormat;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.Date;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.both;
-import static org.hamcrest.Matchers.endsWith;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThanOrEqualTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.lessThanOrEqualTo;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
-import static org.hamcrest.Matchers.sameInstance;
-
-public class IngestDocumentTests extends ESTestCase {
-
-    private IngestDocument ingestDocument;
-
-    @Before
-    public void setIngestDocument() {
-        Map<String, Object> document = new HashMap<>();
-        Map<String, Object> ingestMap = new HashMap<>();
-        ingestMap.put("timestamp", "bogus_timestamp");
-        document.put("_ingest", ingestMap);
-        document.put("foo", "bar");
-        document.put("int", 123);
-        Map<String, Object> innerObject = new HashMap<>();
-        innerObject.put("buzz", "hello world");
-        innerObject.put("foo_null", null);
-        innerObject.put("1", "bar");
-        List<String> innerInnerList = new ArrayList<>();
-        innerInnerList.add("item1");
-        List<Object> innerList = new ArrayList<>();
-        innerList.add(innerInnerList);
-        innerObject.put("list", innerList);
-        document.put("fizz", innerObject);
-        List<Map<String, Object>> list = new ArrayList<>();
-        Map<String, Object> value = new HashMap<>();
-        value.put("field", "value");
-        list.add(value);
-        list.add(null);
-
-        document.put("list", list);
-        ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
-    }
-
-    public void testSimpleGetFieldValue() {
-        assertThat(ingestDocument.getFieldValue("foo", String.class), equalTo("bar"));
-        assertThat(ingestDocument.getFieldValue("int", Integer.class), equalTo(123));
-        assertThat(ingestDocument.getFieldValue("_source.foo", String.class), equalTo("bar"));
-        assertThat(ingestDocument.getFieldValue("_source.int", Integer.class), equalTo(123));
-        assertThat(ingestDocument.getFieldValue("_index", String.class), equalTo("index"));
-        assertThat(ingestDocument.getFieldValue("_type", String.class), equalTo("type"));
-        assertThat(ingestDocument.getFieldValue("_id", String.class), equalTo("id"));
-        assertThat(ingestDocument.getFieldValue("_ingest.timestamp", String.class), both(notNullValue()).and(not(equalTo("bogus_timestamp"))));
-        assertThat(ingestDocument.getFieldValue("_source._ingest.timestamp", String.class), equalTo("bogus_timestamp"));
-    }
-
-    public void testGetSourceObject() {
-        try {
-            ingestDocument.getFieldValue("_source", Object.class);
-            fail("get field value should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [_source] not present as part of path [_source]"));
-        }
-    }
-
-    public void testGetIngestObject() {
-        assertThat(ingestDocument.getFieldValue("_ingest", Map.class), notNullValue());
-    }
-
-    public void testGetEmptyPathAfterStrippingOutPrefix() {
-        try {
-            ingestDocument.getFieldValue("_source.", Object.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_source.] is not valid"));
-        }
-
-        try {
-            ingestDocument.getFieldValue("_ingest.", Object.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_ingest.] is not valid"));
-        }
-    }
-
-    public void testGetFieldValueNullValue() {
-        assertThat(ingestDocument.getFieldValue("fizz.foo_null", Object.class), nullValue());
-    }
-
-    public void testSimpleGetFieldValueTypeMismatch() {
-        try {
-            ingestDocument.getFieldValue("int", String.class);
-            fail("getFieldValue should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [int] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
-        }
-
-        try {
-            ingestDocument.getFieldValue("foo", Integer.class);
-            fail("getFieldValue should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [foo] of type [java.lang.String] cannot be cast to [java.lang.Integer]"));
-        }
-    }
-
-    public void testNestedGetFieldValue() {
-        assertThat(ingestDocument.getFieldValue("fizz.buzz", String.class), equalTo("hello world"));
-        assertThat(ingestDocument.getFieldValue("fizz.1", String.class), equalTo("bar"));
-    }
-
-    public void testNestedGetFieldValueTypeMismatch() {
-        try {
-            ingestDocument.getFieldValue("foo.foo.bar", String.class);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot resolve [foo] from object of type [java.lang.String] as part of path [foo.foo.bar]"));
-        }
-    }
-
-    public void testListGetFieldValue() {
-        assertThat(ingestDocument.getFieldValue("list.0.field", String.class), equalTo("value"));
-    }
-
-    public void testListGetFieldValueNull() {
-        assertThat(ingestDocument.getFieldValue("list.1", String.class), nullValue());
-    }
-
-    public void testListGetFieldValueIndexNotNumeric() {
-        try {
-            ingestDocument.getFieldValue("list.test.field", String.class);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[test] is not an integer, cannot be used as an index as part of path [list.test.field]"));
-        }
-    }
-
-    public void testListGetFieldValueIndexOutOfBounds() {
-        try {
-            ingestDocument.getFieldValue("list.10.field", String.class);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[10] is out of bounds for array with length [2] as part of path [list.10.field]"));
-        }
-    }
-
-    public void testGetFieldValueNotFound() {
-        try {
-            ingestDocument.getFieldValue("not.here", String.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [not] not present as part of path [not.here]"));
-        }
-    }
-
-    public void testGetFieldValueNotFoundNullParent() {
-        try {
-            ingestDocument.getFieldValue("fizz.foo_null.not_there", String.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot resolve [not_there] from null as part of path [fizz.foo_null.not_there]"));
-        }
-    }
-
-    public void testGetFieldValueNull() {
-        try {
-            ingestDocument.getFieldValue(null, String.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testGetFieldValueEmpty() {
-        try {
-            ingestDocument.getFieldValue("", String.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testHasField() {
-        assertTrue(ingestDocument.hasField("fizz"));
-        assertTrue(ingestDocument.hasField("_index"));
-        assertTrue(ingestDocument.hasField("_type"));
-        assertTrue(ingestDocument.hasField("_id"));
-        assertTrue(ingestDocument.hasField("_source.fizz"));
-        assertTrue(ingestDocument.hasField("_ingest.timestamp"));
-    }
-
-    public void testHasFieldNested() {
-        assertTrue(ingestDocument.hasField("fizz.buzz"));
-        assertTrue(ingestDocument.hasField("_source._ingest.timestamp"));
-    }
-
-    public void testListHasField() {
-        assertTrue(ingestDocument.hasField("list.0.field"));
-    }
-
-    public void testListHasFieldNull() {
-        assertTrue(ingestDocument.hasField("list.1"));
-    }
-
-    public void testListHasFieldIndexOutOfBounds() {
-        assertFalse(ingestDocument.hasField("list.10"));
-    }
-
-    public void testListHasFieldIndexNotNumeric() {
-        assertFalse(ingestDocument.hasField("list.test"));
-    }
-
-    public void testNestedHasFieldTypeMismatch() {
-        assertFalse(ingestDocument.hasField("foo.foo.bar"));
-    }
-
-    public void testHasFieldNotFound() {
-        assertFalse(ingestDocument.hasField("not.here"));
-    }
-
-    public void testHasFieldNotFoundNullParent() {
-        assertFalse(ingestDocument.hasField("fizz.foo_null.not_there"));
-    }
-
-    public void testHasFieldNestedNotFound() {
-        assertFalse(ingestDocument.hasField("fizz.doesnotexist"));
-    }
-
-    public void testHasFieldNull() {
-        try {
-            ingestDocument.hasField(null);
-            fail("has field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testHasFieldNullValue() {
-        assertTrue(ingestDocument.hasField("fizz.foo_null"));
-    }
-
-    public void testHasFieldEmpty() {
-        try {
-            ingestDocument.hasField("");
-            fail("has field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testHasFieldSourceObject() {
-        assertThat(ingestDocument.hasField("_source"), equalTo(false));
-    }
-
-    public void testHasFieldIngestObject() {
-        assertThat(ingestDocument.hasField("_ingest"), equalTo(true));
-    }
-
-    public void testHasFieldEmptyPathAfterStrippingOutPrefix() {
-        try {
-            ingestDocument.hasField("_source.");
-            fail("has field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_source.] is not valid"));
-        }
-
-        try {
-            ingestDocument.hasField("_ingest.");
-            fail("has field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_ingest.] is not valid"));
-        }
-    }
-
-    public void testSimpleSetFieldValue() {
-        ingestDocument.setFieldValue("new_field", "foo");
-        assertThat(ingestDocument.getSourceAndMetadata().get("new_field"), equalTo("foo"));
-        ingestDocument.setFieldValue("_ttl", "ttl");
-        assertThat(ingestDocument.getSourceAndMetadata().get("_ttl"), equalTo("ttl"));
-        ingestDocument.setFieldValue("_source.another_field", "bar");
-        assertThat(ingestDocument.getSourceAndMetadata().get("another_field"), equalTo("bar"));
-        ingestDocument.setFieldValue("_ingest.new_field", "new_value");
-        assertThat(ingestDocument.getIngestMetadata().size(), equalTo(2));
-        assertThat(ingestDocument.getIngestMetadata().get("new_field"), equalTo("new_value"));
-        ingestDocument.setFieldValue("_ingest.timestamp", "timestamp");
-        assertThat(ingestDocument.getIngestMetadata().get("timestamp"), equalTo("timestamp"));
-    }
-
-    public void testSetFieldValueNullValue() {
-        ingestDocument.setFieldValue("new_field", null);
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("new_field"), equalTo(true));
-        assertThat(ingestDocument.getSourceAndMetadata().get("new_field"), nullValue());
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testNestedSetFieldValue() {
-        ingestDocument.setFieldValue("a.b.c.d", "foo");
-        assertThat(ingestDocument.getSourceAndMetadata().get("a"), instanceOf(Map.class));
-        Map<String, Object> a = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("a");
-        assertThat(a.get("b"), instanceOf(Map.class));
-        Map<String, Object> b = (Map<String, Object>) a.get("b");
-        assertThat(b.get("c"), instanceOf(Map.class));
-        Map<String, Object> c = (Map<String, Object>) b.get("c");
-        assertThat(c.get("d"), instanceOf(String.class));
-        String d = (String) c.get("d");
-        assertThat(d, equalTo("foo"));
-    }
-
-    public void testSetFieldValueOnExistingField() {
-        ingestDocument.setFieldValue("foo", "newbar");
-        assertThat(ingestDocument.getSourceAndMetadata().get("foo"), equalTo("newbar"));
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testSetFieldValueOnExistingParent() {
-        ingestDocument.setFieldValue("fizz.new", "bar");
-        assertThat(ingestDocument.getSourceAndMetadata().get("fizz"), instanceOf(Map.class));
-        Map<String, Object> innerMap = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(innerMap.get("new"), instanceOf(String.class));
-        String value = (String) innerMap.get("new");
-        assertThat(value, equalTo("bar"));
-    }
-
-    public void testSetFieldValueOnExistingParentTypeMismatch() {
-        try {
-            ingestDocument.setFieldValue("fizz.buzz.new", "bar");
-            fail("add field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot set [new] with parent object of type [java.lang.String] as part of path [fizz.buzz.new]"));
-        }
-    }
-
-    public void testSetFieldValueOnExistingNullParent() {
-        try {
-            ingestDocument.setFieldValue("fizz.foo_null.test", "bar");
-            fail("add field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot set [test] with null parent as part of path [fizz.foo_null.test]"));
-        }
-    }
-
-    public void testSetFieldValueNullName() {
-        try {
-            ingestDocument.setFieldValue(null, "bar");
-            fail("add field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testSetSourceObject() {
-        ingestDocument.setFieldValue("_source", "value");
-        assertThat(ingestDocument.getSourceAndMetadata().get("_source"), equalTo("value"));
-    }
-
-    public void testSetIngestObject() {
-        ingestDocument.setFieldValue("_ingest", "value");
-        assertThat(ingestDocument.getSourceAndMetadata().get("_ingest"), equalTo("value"));
-    }
-
-    public void testSetIngestSourceObject() {
-        //test that we don't strip out the _source prefix when _ingest is used
-        ingestDocument.setFieldValue("_ingest._source", "value");
-        assertThat(ingestDocument.getIngestMetadata().get("_source"), equalTo("value"));
-    }
-
-    public void testSetEmptyPathAfterStrippingOutPrefix() {
-        try {
-            ingestDocument.setFieldValue("_source.", "value");
-            fail("set field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_source.] is not valid"));
-        }
-
-        try {
-            ingestDocument.setFieldValue("_ingest.", "_value");
-            fail("set field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_ingest.] is not valid"));
-        }
-    }
-
-    public void testListSetFieldValueNoIndexProvided() {
-        ingestDocument.setFieldValue("list", "value");
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(String.class));
-        assertThat(object, equalTo("value"));
-    }
-
-    public void testListAppendFieldValue() {
-        ingestDocument.appendFieldValue("list", "new_value");
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(3));
-        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
-        assertThat(list.get(1), nullValue());
-        assertThat(list.get(2), equalTo("new_value"));
-    }
-
-    public void testListAppendFieldValues() {
-        ingestDocument.appendFieldValue("list", Arrays.asList("item1", "item2", "item3"));
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(5));
-        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
-        assertThat(list.get(1), nullValue());
-        assertThat(list.get(2), equalTo("item1"));
-        assertThat(list.get(3), equalTo("item2"));
-        assertThat(list.get(4), equalTo("item3"));
-    }
-
-    public void testAppendFieldValueToNonExistingList() {
-        ingestDocument.appendFieldValue("non_existing_list", "new_value");
-        Object object = ingestDocument.getSourceAndMetadata().get("non_existing_list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(1));
-        assertThat(list.get(0), equalTo("new_value"));
-    }
-
-    public void testAppendFieldValuesToNonExistingList() {
-        ingestDocument.appendFieldValue("non_existing_list", Arrays.asList("item1", "item2", "item3"));
-        Object object = ingestDocument.getSourceAndMetadata().get("non_existing_list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(3));
-        assertThat(list.get(0), equalTo("item1"));
-        assertThat(list.get(1), equalTo("item2"));
-        assertThat(list.get(2), equalTo("item3"));
-    }
-
-    public void testAppendFieldValueConvertStringToList() {
-        ingestDocument.appendFieldValue("fizz.buzz", "new_value");
-        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        object = map.get("buzz");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), equalTo("hello world"));
-        assertThat(list.get(1), equalTo("new_value"));
-    }
-
-    public void testAppendFieldValuesConvertStringToList() {
-        ingestDocument.appendFieldValue("fizz.buzz", Arrays.asList("item1", "item2", "item3"));
-        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        object = map.get("buzz");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(4));
-        assertThat(list.get(0), equalTo("hello world"));
-        assertThat(list.get(1), equalTo("item1"));
-        assertThat(list.get(2), equalTo("item2"));
-        assertThat(list.get(3), equalTo("item3"));
-    }
-
-    public void testAppendFieldValueConvertIntegerToList() {
-        ingestDocument.appendFieldValue("int", 456);
-        Object object = ingestDocument.getSourceAndMetadata().get("int");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), equalTo(123));
-        assertThat(list.get(1), equalTo(456));
-    }
-
-    public void testAppendFieldValuesConvertIntegerToList() {
-        ingestDocument.appendFieldValue("int", Arrays.asList(456, 789));
-        Object object = ingestDocument.getSourceAndMetadata().get("int");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(3));
-        assertThat(list.get(0), equalTo(123));
-        assertThat(list.get(1), equalTo(456));
-        assertThat(list.get(2), equalTo(789));
-    }
-
-    public void testAppendFieldValueConvertMapToList() {
-        ingestDocument.appendFieldValue("fizz", Collections.singletonMap("field", "value"));
-        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(object, instanceOf(List.class));
-        List<?> list = (List<?>) object;
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) list.get(0);
-        assertThat(map.size(), equalTo(4));
-        assertThat(list.get(1), equalTo(Collections.singletonMap("field", "value")));
-    }
-
-    public void testAppendFieldValueToNull() {
-        ingestDocument.appendFieldValue("fizz.foo_null", "new_value");
-        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        object = map.get("foo_null");
-        assertThat(object, instanceOf(List.class));
-        List<?> list = (List<?>) object;
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), nullValue());
-        assertThat(list.get(1), equalTo("new_value"));
-    }
-
-    public void testAppendFieldValueToListElement() {
-        ingestDocument.appendFieldValue("fizz.list.0", "item2");
-        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        object = map.get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(1));
-        object = list.get(0);
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<String> innerList = (List<String>) object;
-        assertThat(innerList.size(), equalTo(2));
-        assertThat(innerList.get(0), equalTo("item1"));
-        assertThat(innerList.get(1), equalTo("item2"));
-    }
-
-    public void testAppendFieldValuesToListElement() {
-        ingestDocument.appendFieldValue("fizz.list.0", Arrays.asList("item2", "item3", "item4"));
-        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        object = map.get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(1));
-        object = list.get(0);
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<String> innerList = (List<String>) object;
-        assertThat(innerList.size(), equalTo(4));
-        assertThat(innerList.get(0), equalTo("item1"));
-        assertThat(innerList.get(1), equalTo("item2"));
-        assertThat(innerList.get(2), equalTo("item3"));
-        assertThat(innerList.get(3), equalTo("item4"));
-    }
-
-    public void testAppendFieldValueConvertStringListElementToList() {
-        ingestDocument.appendFieldValue("fizz.list.0.0", "new_value");
-        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        object = map.get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(1));
-        object = list.get(0);
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> innerList = (List<Object>) object;
-        object = innerList.get(0);
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<String> innerInnerList = (List<String>) object;
-        assertThat(innerInnerList.size(), equalTo(2));
-        assertThat(innerInnerList.get(0), equalTo("item1"));
-        assertThat(innerInnerList.get(1), equalTo("new_value"));
-    }
-
-    public void testAppendFieldValuesConvertStringListElementToList() {
-        ingestDocument.appendFieldValue("fizz.list.0.0", Arrays.asList("item2", "item3", "item4"));
-        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        object = map.get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(1));
-        object = list.get(0);
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> innerList = (List<Object>) object;
-        object = innerList.get(0);
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<String> innerInnerList = (List<String>) object;
-        assertThat(innerInnerList.size(), equalTo(4));
-        assertThat(innerInnerList.get(0), equalTo("item1"));
-        assertThat(innerInnerList.get(1), equalTo("item2"));
-        assertThat(innerInnerList.get(2), equalTo("item3"));
-        assertThat(innerInnerList.get(3), equalTo("item4"));
-    }
-
-    public void testAppendFieldValueListElementConvertMapToList() {
-        ingestDocument.appendFieldValue("list.0", Collections.singletonMap("item2", "value2"));
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        List<?> list = (List<?>) object;
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), instanceOf(List.class));
-        assertThat(list.get(1), nullValue());
-        list = (List<?>) list.get(0);
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
-        assertThat(list.get(1), equalTo(Collections.singletonMap("item2", "value2")));
-    }
-
-    public void testAppendFieldValueToNullListElement() {
-        ingestDocument.appendFieldValue("list.1", "new_value");
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        List<?> list = (List<?>) object;
-        assertThat(list.get(1), instanceOf(List.class));
-        list = (List<?>) list.get(1);
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), nullValue());
-        assertThat(list.get(1), equalTo("new_value"));
-    }
-
-    public void testAppendFieldValueToListOfMaps() {
-        ingestDocument.appendFieldValue("list", Collections.singletonMap("item2", "value2"));
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(3));
-        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
-        assertThat(list.get(1), nullValue());
-        assertThat(list.get(2), equalTo(Collections.singletonMap("item2", "value2")));
-    }
-
-    public void testListSetFieldValueIndexProvided() {
-        ingestDocument.setFieldValue("list.1", "value");
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
-        assertThat(list.get(1), equalTo("value"));
-    }
-
-    public void testSetFieldValueListAsPartOfPath() {
-        ingestDocument.setFieldValue("list.0.field", "new_value");
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "new_value")));
-        assertThat(list.get(1), nullValue());
-    }
-
-    public void testListSetFieldValueIndexNotNumeric() {
-        try {
-            ingestDocument.setFieldValue("list.test", "value");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[test] is not an integer, cannot be used as an index as part of path [list.test]"));
-        }
-
-        try {
-            ingestDocument.setFieldValue("list.test.field", "new_value");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[test] is not an integer, cannot be used as an index as part of path [list.test.field]"));
-        }
-    }
-
-    public void testListSetFieldValueIndexOutOfBounds() {
-        try {
-            ingestDocument.setFieldValue("list.10", "value");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[10] is out of bounds for array with length [2] as part of path [list.10]"));
-        }
-
-        try {
-            ingestDocument.setFieldValue("list.10.field", "value");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[10] is out of bounds for array with length [2] as part of path [list.10.field]"));
-        }
-    }
-
-    public void testSetFieldValueEmptyName() {
-        try {
-            ingestDocument.setFieldValue("", "bar");
-            fail("add field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testRemoveField() {
-        ingestDocument.removeField("foo");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(7));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("foo"), equalTo(false));
-        ingestDocument.removeField("_index");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(6));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("_index"), equalTo(false));
-        ingestDocument.removeField("_source.fizz");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(5));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("fizz"), equalTo(false));
-        assertThat(ingestDocument.getIngestMetadata().size(), equalTo(1));
-        ingestDocument.removeField("_ingest.timestamp");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(5));
-        assertThat(ingestDocument.getIngestMetadata().size(), equalTo(0));
-    }
-
-    public void testRemoveInnerField() {
-        ingestDocument.removeField("fizz.buzz");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
-        assertThat(ingestDocument.getSourceAndMetadata().get("fizz"), instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(map.size(), equalTo(3));
-        assertThat(map.containsKey("buzz"), equalTo(false));
-
-        ingestDocument.removeField("fizz.foo_null");
-        assertThat(map.size(), equalTo(2));
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("fizz"), equalTo(true));
-
-        ingestDocument.removeField("fizz.1");
-        assertThat(map.size(), equalTo(1));
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("fizz"), equalTo(true));
-
-        ingestDocument.removeField("fizz.list");
-        assertThat(map.size(), equalTo(0));
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("fizz"), equalTo(true));
-    }
-
-    public void testRemoveNonExistingField() {
-        try {
-            ingestDocument.removeField("does_not_exist");
-            fail("remove field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [does_not_exist] not present as part of path [does_not_exist]"));
-        }
-    }
-
-    public void testRemoveExistingParentTypeMismatch() {
-        try {
-            ingestDocument.removeField("foo.foo.bar");
-            fail("remove field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot resolve [foo] from object of type [java.lang.String] as part of path [foo.foo.bar]"));
-        }
-    }
-
-    public void testRemoveSourceObject() {
-        try {
-            ingestDocument.removeField("_source");
-            fail("remove field should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [_source] not present as part of path [_source]"));
-        }
-    }
-
-    public void testRemoveIngestObject() {
-        ingestDocument.removeField("_ingest");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(7));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("_ingest"), equalTo(false));
-    }
-
-    public void testRemoveEmptyPathAfterStrippingOutPrefix() {
-        try {
-            ingestDocument.removeField("_source.");
-            fail("set field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_source.] is not valid"));
-        }
-
-        try {
-            ingestDocument.removeField("_ingest.");
-            fail("set field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_ingest.] is not valid"));
-        }
-    }
-
-    public void testListRemoveField() {
-        ingestDocument.removeField("list.0.field");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("list"), equalTo(true));
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(2));
-        object = list.get(0);
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        assertThat(map.size(), equalTo(0));
-        ingestDocument.removeField("list.0");
-        assertThat(list.size(), equalTo(1));
-        assertThat(list.get(0), nullValue());
-    }
-
-    public void testRemoveFieldValueNotFoundNullParent() {
-        try {
-            ingestDocument.removeField("fizz.foo_null.not_there");
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot remove [not_there] from null as part of path [fizz.foo_null.not_there]"));
-        }
-    }
-
-    public void testNestedRemoveFieldTypeMismatch() {
-        try {
-            ingestDocument.removeField("fizz.1.bar");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot remove [bar] from object of type [java.lang.String] as part of path [fizz.1.bar]"));
-        }
-    }
-
-    public void testListRemoveFieldIndexNotNumeric() {
-        try {
-            ingestDocument.removeField("list.test");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[test] is not an integer, cannot be used as an index as part of path [list.test]"));
-        }
-    }
-
-    public void testListRemoveFieldIndexOutOfBounds() {
-        try {
-            ingestDocument.removeField("list.10");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[10] is out of bounds for array with length [2] as part of path [list.10]"));
-        }
-    }
-
-    public void testRemoveNullField() {
-        try {
-            ingestDocument.removeField((String) null);
-            fail("remove field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testRemoveEmptyField() {
-        try {
-            ingestDocument.removeField("");
-            fail("remove field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testEqualsAndHashcode() throws Exception {
-        Map<String, Object> sourceAndMetadata = RandomDocumentPicks.randomSource(random());
-        int numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
-        for (int i = 0; i < numFields; i++) {
-            sourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
-        }
-        Map<String, String> ingestMetadata = new HashMap<>();
-        numFields = randomIntBetween(1, 5);
-        for (int i = 0; i < numFields; i++) {
-            ingestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
-        }
-        IngestDocument ingestDocument = new IngestDocument(sourceAndMetadata, ingestMetadata);
-
-        boolean changed = false;
-        Map<String, Object> otherSourceAndMetadata;
-        if (randomBoolean()) {
-            otherSourceAndMetadata = RandomDocumentPicks.randomSource(random());
-            changed = true;
-        } else {
-            otherSourceAndMetadata = new HashMap<>(sourceAndMetadata);
-        }
-        if (randomBoolean()) {
-            numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
-            for (int i = 0; i < numFields; i++) {
-                otherSourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
-            }
-            changed = true;
-        }
-
-        Map<String, String> otherIngestMetadata;
-        if (randomBoolean()) {
-            otherIngestMetadata = new HashMap<>();
-            numFields = randomIntBetween(1, 5);
-            for (int i = 0; i < numFields; i++) {
-                otherIngestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
-            }
-            changed = true;
-        } else {
-            otherIngestMetadata = Collections.unmodifiableMap(ingestMetadata);
-        }
-
-        IngestDocument otherIngestDocument = new IngestDocument(otherSourceAndMetadata, otherIngestMetadata);
-        if (changed) {
-            assertThat(ingestDocument, not(equalTo(otherIngestDocument)));
-            assertThat(otherIngestDocument, not(equalTo(ingestDocument)));
-        } else {
-            assertThat(ingestDocument, equalTo(otherIngestDocument));
-            assertThat(otherIngestDocument, equalTo(ingestDocument));
-            assertThat(ingestDocument.hashCode(), equalTo(otherIngestDocument.hashCode()));
-            IngestDocument thirdIngestDocument = new IngestDocument(Collections.unmodifiableMap(sourceAndMetadata), Collections.unmodifiableMap(ingestMetadata));
-            assertThat(thirdIngestDocument, equalTo(ingestDocument));
-            assertThat(ingestDocument, equalTo(thirdIngestDocument));
-            assertThat(ingestDocument.hashCode(), equalTo(thirdIngestDocument.hashCode()));
-        }
-    }
-
-    public void testIngestMetadataTimestamp() throws Exception {
-        long before = System.currentTimeMillis();
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        long after = System.currentTimeMillis();
-        String timestampString = ingestDocument.getIngestMetadata().get("timestamp");
-        assertThat(timestampString, notNullValue());
-        assertThat(timestampString, endsWith("+0000"));
-        DateFormat df = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSSZZ", Locale.ROOT);
-        Date timestamp = df.parse(timestampString);
-        assertThat(timestamp.getTime(), greaterThanOrEqualTo(before));
-        assertThat(timestamp.getTime(), lessThanOrEqualTo(after));
-    }
-
-    public void testCopyConstructor() {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        IngestDocument copy = new IngestDocument(ingestDocument);
-        assertThat(ingestDocument.getSourceAndMetadata(), not(sameInstance(copy.getSourceAndMetadata())));
-        assertThat(ingestDocument.getSourceAndMetadata(), equalTo(copy.getSourceAndMetadata()));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/core/ValueSourceTests.java b/core/src/test/java/org/elasticsearch/ingest/core/ValueSourceTests.java
deleted file mode 100644
index f2aa9f3..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/core/ValueSourceTests.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.sameInstance;
-
-public class ValueSourceTests extends ESTestCase {
-
-    public void testDeepCopy() {
-        int iterations = scaledRandomIntBetween(8, 64);
-        for (int i = 0; i < iterations; i++) {
-            Map<String, Object> map = RandomDocumentPicks.randomSource(random());
-            ValueSource valueSource = ValueSource.wrap(map, TestTemplateService.instance());
-            Object copy = valueSource.copyAndResolve(Collections.emptyMap());
-            assertThat("iteration: " + i, copy, equalTo(map));
-            assertThat("iteration: " + i, copy, not(sameInstance(map)));
-        }
-    }
-
-    public void testCopyDoesNotChangeProvidedMap() {
-        Map<String, Object> myPreciousMap = new HashMap<>();
-        myPreciousMap.put("field2", "value2");
-
-        IngestDocument ingestDocument = new IngestDocument(new HashMap<>(), new HashMap<>());
-        ingestDocument.setFieldValue(TestTemplateService.instance().compile("field1"), ValueSource.wrap(myPreciousMap, TestTemplateService.instance()));
-        ingestDocument.removeField("field1.field2");
-
-        assertThat(myPreciousMap.size(), equalTo(1));
-        assertThat(myPreciousMap.get("field2"), equalTo("value2"));
-    }
-
-    public void testCopyDoesNotChangeProvidedList() {
-        List<String> myPreciousList = new ArrayList<>();
-        myPreciousList.add("value");
-
-        IngestDocument ingestDocument = new IngestDocument(new HashMap<>(), new HashMap<>());
-        ingestDocument.setFieldValue(TestTemplateService.instance().compile("field1"), ValueSource.wrap(myPreciousList, TestTemplateService.instance()));
-        ingestDocument.removeField("field1.0");
-
-        assertThat(myPreciousList.size(), equalTo(1));
-        assertThat(myPreciousList.get(0), equalTo("value"));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/AbstractStringProcessorTestCase.java b/core/src/test/java/org/elasticsearch/ingest/processor/AbstractStringProcessorTestCase.java
deleted file mode 100644
index 1113a4b..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/AbstractStringProcessorTestCase.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public abstract class AbstractStringProcessorTestCase extends ESTestCase {
-
-    protected abstract AbstractStringProcessor newProcessor(String field);
-
-    protected String modifyInput(String input) {
-        return input;
-    }
-
-    protected abstract String expectedResult(String input);
-
-    public void testProcessor() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldValue = RandomDocumentPicks.randomString(random());
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, modifyInput(fieldValue));
-        Processor processor = newProcessor(fieldName);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo(expectedResult(fieldValue)));
-    }
-
-    public void testFieldNotFound() throws Exception {
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = newProcessor(fieldName);
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        try {
-            processor.execute(ingestDocument);
-            fail("processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-
-    public void testNullValue() throws Exception {
-        Processor processor = newProcessor("field");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
-        try {
-            processor.execute(ingestDocument);
-            fail("processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [field] is null, cannot process it."));
-        }
-    }
-
-    public void testNonStringValue() throws Exception {
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = newProcessor(fieldName);
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        ingestDocument.setFieldValue(fieldName, randomInt());
-        try {
-            processor.execute(ingestDocument);
-            fail("processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorFactoryTests.java
deleted file mode 100644
index 09ce465..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorFactoryTests.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class AppendProcessorFactoryTests extends ESTestCase {
-
-    private AppendProcessor.Factory factory;
-
-    @Before
-    public void init() {
-        factory = new AppendProcessor.Factory(TestTemplateService.instance());
-    }
-
-    public void testCreate() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        Object value;
-        if (randomBoolean()) {
-            value = "value1";
-        } else {
-            value = Arrays.asList("value1", "value2", "value3");
-        }
-        config.put("value", value);
-        AppendProcessor setProcessor = factory.create(config);
-        assertThat(setProcessor.getField().execute(Collections.emptyMap()), equalTo("field1"));
-        assertThat(setProcessor.getValue().copyAndResolve(Collections.emptyMap()), equalTo(value));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("value", "value1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoValuePresent() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [value] is missing"));
-        }
-    }
-
-    public void testCreateNullValue() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("value", null);
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [value] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorTests.java
deleted file mode 100644
index 7853709..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorTests.java
+++ /dev/null
@@ -1,209 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.ingest.core.ValueSource;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.not;
-import static org.hamcrest.CoreMatchers.sameInstance;
-
-public class AppendProcessorTests extends ESTestCase {
-
-    public void testAppendValuesToExistingList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        Scalar scalar = randomFrom(Scalar.values());
-        List<Object> list = new ArrayList<>();
-        int size = randomIntBetween(0, 10);
-        for (int i = 0; i < size; i++) {
-            list.add(scalar.randomValue());
-        }
-        List<Object> checkList = new ArrayList<>(list);
-        String field = RandomDocumentPicks.addRandomField(random(), ingestDocument, list);
-        List<Object> values = new ArrayList<>();
-        Processor appendProcessor;
-        if (randomBoolean()) {
-            Object value = scalar.randomValue();
-            values.add(value);
-            appendProcessor = createAppendProcessor(field, value);
-        } else {
-            int valuesSize = randomIntBetween(0, 10);
-            for (int i = 0; i < valuesSize; i++) {
-                values.add(scalar.randomValue());
-            }
-            appendProcessor = createAppendProcessor(field, values);
-        }
-        appendProcessor.execute(ingestDocument);
-        Object fieldValue = ingestDocument.getFieldValue(field, Object.class);
-        assertThat(fieldValue, sameInstance(list));
-        assertThat(list.size(), equalTo(size + values.size()));
-        for (int i = 0; i < size; i++) {
-            assertThat(list.get(i), equalTo(checkList.get(i)));
-        }
-        for (int i = size; i < size + values.size(); i++) {
-            assertThat(list.get(i), equalTo(values.get(i - size)));
-        }
-    }
-
-    public void testAppendValuesToNonExistingList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String field = RandomDocumentPicks.randomFieldName(random());
-        Scalar scalar = randomFrom(Scalar.values());
-        List<Object> values = new ArrayList<>();
-        Processor appendProcessor;
-        if (randomBoolean()) {
-            Object value = scalar.randomValue();
-            values.add(value);
-            appendProcessor = createAppendProcessor(field, value);
-        } else {
-            int valuesSize = randomIntBetween(0, 10);
-            for (int i = 0; i < valuesSize; i++) {
-                values.add(scalar.randomValue());
-            }
-            appendProcessor = createAppendProcessor(field, values);
-        }
-        appendProcessor.execute(ingestDocument);
-        List list = ingestDocument.getFieldValue(field, List.class);
-        assertThat(list, not(sameInstance(values)));
-        assertThat(list, equalTo(values));
-    }
-
-    public void testConvertScalarToList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        Scalar scalar = randomFrom(Scalar.values());
-        Object initialValue = scalar.randomValue();
-        String field = RandomDocumentPicks.addRandomField(random(), ingestDocument, initialValue);
-        List<Object> values = new ArrayList<>();
-        Processor appendProcessor;
-        if (randomBoolean()) {
-            Object value = scalar.randomValue();
-            values.add(value);
-            appendProcessor = createAppendProcessor(field, value);
-        } else {
-            int valuesSize = randomIntBetween(0, 10);
-            for (int i = 0; i < valuesSize; i++) {
-                values.add(scalar.randomValue());
-            }
-            appendProcessor = createAppendProcessor(field, values);
-        }
-        appendProcessor.execute(ingestDocument);
-        List fieldValue = ingestDocument.getFieldValue(field, List.class);
-        assertThat(fieldValue.size(), equalTo(values.size() + 1));
-        assertThat(fieldValue.get(0), equalTo(initialValue));
-        for (int i = 1; i < values.size() + 1; i++) {
-            assertThat(fieldValue.get(i), equalTo(values.get(i - 1)));
-        }
-    }
-
-    public void testAppendMetadata() throws Exception {
-        //here any metadata field value becomes a list, which won't make sense in most of the cases,
-        // but support for append is streamlined like for set so we test it
-        IngestDocument.MetaData randomMetaData = randomFrom(IngestDocument.MetaData.values());
-        List<String> values = new ArrayList<>();
-        Processor appendProcessor;
-        if (randomBoolean()) {
-            String value = randomAsciiOfLengthBetween(1, 10);
-            values.add(value);
-            appendProcessor = createAppendProcessor(randomMetaData.getFieldName(), value);
-        } else {
-            int valuesSize = randomIntBetween(0, 10);
-            for (int i = 0; i < valuesSize; i++) {
-                values.add(randomAsciiOfLengthBetween(1, 10));
-            }
-            appendProcessor = createAppendProcessor(randomMetaData.getFieldName(), values);
-        }
-
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        Object initialValue = ingestDocument.getSourceAndMetadata().get(randomMetaData.getFieldName());
-        appendProcessor.execute(ingestDocument);
-        List list = ingestDocument.getFieldValue(randomMetaData.getFieldName(), List.class);
-        if (initialValue == null) {
-            assertThat(list, equalTo(values));
-        } else {
-            assertThat(list.size(), equalTo(values.size() + 1));
-            assertThat(list.get(0), equalTo(initialValue));
-            for (int i = 1; i < list.size(); i++) {
-                assertThat(list.get(i), equalTo(values.get(i - 1)));
-            }
-        }
-    }
-
-    private static Processor createAppendProcessor(String fieldName, Object fieldValue) {
-        TemplateService templateService = TestTemplateService.instance();
-        return new AppendProcessor(templateService.compile(fieldName), ValueSource.wrap(fieldValue, templateService));
-    }
-
-    private enum Scalar {
-        INTEGER {
-            @Override
-            Object randomValue() {
-                return randomInt();
-            }
-        }, DOUBLE {
-            @Override
-            Object randomValue() {
-                return randomDouble();
-            }
-        }, FLOAT {
-            @Override
-            Object randomValue() {
-                return randomFloat();
-            }
-        }, BOOLEAN {
-            @Override
-            Object randomValue() {
-                return randomBoolean();
-            }
-        }, STRING {
-            @Override
-            Object randomValue() {
-                return randomAsciiOfLengthBetween(1, 10);
-            }
-        }, MAP {
-            @Override
-            Object randomValue() {
-                int numItems = randomIntBetween(1, 10);
-                Map<String, Object> map = new HashMap<>(numItems);
-                for (int i = 0; i < numItems; i++) {
-                    map.put(randomAsciiOfLengthBetween(1, 10), randomFrom(Scalar.values()).randomValue());
-                }
-                return map;
-            }
-        }, NULL {
-            @Override
-            Object randomValue() {
-                return null;
-            }
-        };
-
-        abstract Object randomValue();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorFactoryTests.java
deleted file mode 100644
index 4feb20d..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorFactoryTests.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.processor.ConvertProcessor;
-import org.elasticsearch.test.ESTestCase;
-import org.hamcrest.Matchers;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class ConvertProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        ConvertProcessor.Factory factory = new ConvertProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        ConvertProcessor.Type type = randomFrom(ConvertProcessor.Type.values());
-        config.put("field", "field1");
-        config.put("type", type.toString());
-        ConvertProcessor convertProcessor = factory.create(config);
-        assertThat(convertProcessor.getField(), equalTo("field1"));
-        assertThat(convertProcessor.getConvertType(), equalTo(type));
-    }
-
-    public void testCreateUnsupportedType() throws Exception {
-        ConvertProcessor.Factory factory = new ConvertProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String type = "type-" + randomAsciiOfLengthBetween(1, 10);
-        config.put("field", "field1");
-        config.put("type", type);
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), Matchers.equalTo("type [" + type + "] not supported, cannot convert field."));
-        }
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        ConvertProcessor.Factory factory = new ConvertProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String type = "type-" + randomAsciiOfLengthBetween(1, 10);
-        config.put("type", type);
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), Matchers.equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoTypePresent() throws Exception {
-        ConvertProcessor.Factory factory = new ConvertProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), Matchers.equalTo("required property [type] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorTests.java
deleted file mode 100644
index 040cac4..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorTests.java
+++ /dev/null
@@ -1,270 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-
-import static org.elasticsearch.ingest.processor.ConvertProcessor.Type;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class ConvertProcessorTests extends ESTestCase {
-
-    public void testConvertInt() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int randomInt = randomInt();
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, randomInt);
-        Processor processor = new ConvertProcessor(fieldName, Type.INTEGER);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, Integer.class), equalTo(randomInt));
-    }
-
-    public void testConvertIntList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        List<String> fieldValue = new ArrayList<>();
-        List<Integer> expectedList = new ArrayList<>();
-        for (int j = 0; j < numItems; j++) {
-            int randomInt = randomInt();
-            fieldValue.add(Integer.toString(randomInt));
-            expectedList.add(randomInt);
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new ConvertProcessor(fieldName, Type.INTEGER);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(expectedList));
-    }
-
-    public void testConvertIntError() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        String value = "string-" + randomAsciiOfLengthBetween(1, 10);
-        ingestDocument.setFieldValue(fieldName, value);
-
-        Processor processor = new ConvertProcessor(fieldName, Type.INTEGER);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("unable to convert [" + value + "] to integer"));
-        }
-    }
-
-    public void testConvertFloat() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        Map<String, Float> expectedResult = new HashMap<>();
-        float randomFloat = randomFloat();
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, randomFloat);
-        expectedResult.put(fieldName, randomFloat);
-
-        Processor processor = new ConvertProcessor(fieldName, Type.FLOAT);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, Float.class), equalTo(randomFloat));
-    }
-
-    public void testConvertFloatList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        List<String> fieldValue = new ArrayList<>();
-        List<Float> expectedList = new ArrayList<>();
-        for (int j = 0; j < numItems; j++) {
-            float randomFloat = randomFloat();
-            fieldValue.add(Float.toString(randomFloat));
-            expectedList.add(randomFloat);
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new ConvertProcessor(fieldName, Type.FLOAT);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(expectedList));
-    }
-
-    public void testConvertFloatError() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        String value = "string-" + randomAsciiOfLengthBetween(1, 10);
-        ingestDocument.setFieldValue(fieldName, value);
-
-        Processor processor = new ConvertProcessor(fieldName, Type.FLOAT);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("unable to convert [" + value + "] to float"));
-        }
-    }
-
-    public void testConvertBoolean() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        Map<String, Type> fields = new HashMap<>();
-        Map<String, Boolean> expectedResult = new HashMap<>();
-        boolean randomBoolean = randomBoolean();
-        String booleanString = Boolean.toString(randomBoolean);
-        if (randomBoolean) {
-            booleanString = booleanString.toUpperCase(Locale.ROOT);
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, booleanString);
-
-        Processor processor = new ConvertProcessor(fieldName, Type.BOOLEAN);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, Boolean.class), equalTo(randomBoolean));
-    }
-
-    public void testConvertBooleanList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        List<String> fieldValue = new ArrayList<>();
-        List<Boolean> expectedList = new ArrayList<>();
-        for (int j = 0; j < numItems; j++) {
-            boolean randomBoolean = randomBoolean();
-            String booleanString = Boolean.toString(randomBoolean);
-            if (randomBoolean) {
-                booleanString = booleanString.toUpperCase(Locale.ROOT);
-            }
-            fieldValue.add(booleanString);
-            expectedList.add(randomBoolean);
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new ConvertProcessor(fieldName, Type.BOOLEAN);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(expectedList));
-    }
-
-    public void testConvertBooleanError() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        String fieldValue;
-        if (randomBoolean()) {
-            fieldValue = "string-" + randomAsciiOfLengthBetween(1, 10);
-        } else {
-            //verify that only proper boolean values are supported and we are strict about it
-            fieldValue = randomFrom("on", "off", "yes", "no", "0", "1");
-        }
-        ingestDocument.setFieldValue(fieldName, fieldValue);
-
-        Processor processor = new ConvertProcessor(fieldName, Type.BOOLEAN);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(Exception e) {
-            assertThat(e.getMessage(), equalTo("[" + fieldValue + "] is not a boolean value, cannot convert to boolean"));
-        }
-    }
-
-    public void testConvertString() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        Object fieldValue;
-        String expectedFieldValue;
-        switch(randomIntBetween(0, 2)) {
-            case 0:
-                float randomFloat = randomFloat();
-                fieldValue = randomFloat;
-                expectedFieldValue = Float.toString(randomFloat);
-                break;
-            case 1:
-                int randomInt = randomInt();
-                fieldValue = randomInt;
-                expectedFieldValue = Integer.toString(randomInt);
-                break;
-            case 2:
-                boolean randomBoolean = randomBoolean();
-                fieldValue = randomBoolean;
-                expectedFieldValue = Boolean.toString(randomBoolean);
-                break;
-            default:
-                throw new UnsupportedOperationException();
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-
-        Processor processor = new ConvertProcessor(fieldName, Type.STRING);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo(expectedFieldValue));
-    }
-
-    public void testConvertStringList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        List<Object> fieldValue = new ArrayList<>();
-        List<String> expectedList = new ArrayList<>();
-        for (int j = 0; j < numItems; j++) {
-            Object randomValue;
-            String randomValueString;
-            switch(randomIntBetween(0, 2)) {
-                case 0:
-                    float randomFloat = randomFloat();
-                    randomValue = randomFloat;
-                    randomValueString = Float.toString(randomFloat);
-                    break;
-                case 1:
-                    int randomInt = randomInt();
-                    randomValue = randomInt;
-                    randomValueString = Integer.toString(randomInt);
-                    break;
-                case 2:
-                    boolean randomBoolean = randomBoolean();
-                    randomValue = randomBoolean;
-                    randomValueString = Boolean.toString(randomBoolean);
-                    break;
-                default:
-                    throw new UnsupportedOperationException();
-            }
-            fieldValue.add(randomValue);
-            expectedList.add(randomValueString);
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new ConvertProcessor(fieldName, Type.STRING);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(expectedList));
-    }
-
-    public void testConvertNonExistingField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Type type = randomFrom(Type.values());
-        Processor processor = new ConvertProcessor(fieldName, type);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-
-    public void testConvertNullField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
-        Type type = randomFrom(Type.values());
-        Processor processor = new ConvertProcessor("field", type);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("Field [field] is null, cannot be converted to type [" + type + "]"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/DateFormatTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/DateFormatTests.java
deleted file mode 100644
index c53b420..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/DateFormatTests.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.processor.DateFormat;
-import org.elasticsearch.test.ESTestCase;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-
-import java.time.Instant;
-import java.time.ZoneId;
-import java.time.format.DateTimeFormatter;
-import java.util.Locale;
-import java.util.Optional;
-import java.util.function.Function;
-
-import static org.hamcrest.core.IsEqual.equalTo;
-
-public class DateFormatTests extends ESTestCase {
-
-    public void testParseJoda() {
-        Function<String, DateTime> jodaFunction = DateFormat.getJodaFunction("MMM dd HH:mm:ss Z", DateTimeZone.forOffsetHours(-8), Locale.ENGLISH);
-        assertThat(Instant.ofEpochMilli(jodaFunction.apply("Nov 24 01:29:01 -0800").getMillis())
-                        .atZone(ZoneId.of("GMT-8"))
-                        .format(DateTimeFormatter.ofPattern("MM dd HH:mm:ss", Locale.ENGLISH)),
-                equalTo("11 24 01:29:01"));
-    }
-
-    public void testParseUnixMs() {
-        assertThat(DateFormat.UnixMs.getFunction(DateTimeZone.UTC).apply("1000500").getMillis(), equalTo(1000500L));
-    }
-
-    public void testParseUnix() {
-        assertThat(DateFormat.Unix.getFunction(DateTimeZone.UTC).apply("1000.5").getMillis(), equalTo(1000500L));
-    }
-
-    public void testParseISO8601() {
-        assertThat(DateFormat.Iso8601.getFunction(DateTimeZone.UTC).apply("2001-01-01T00:00:00-0800").getMillis(), equalTo(978336000000L));
-    }
-
-    public void testParseISO8601Failure() {
-        Function<String, DateTime> function = DateFormat.Iso8601.getFunction(DateTimeZone.UTC);
-        try {
-            function.apply("2001-01-0:00-0800");
-            fail("parse should have failed");
-        } catch(IllegalArgumentException e) {
-            //all good
-        }
-    }
-
-    public void testTAI64NParse() {
-        String input = "4000000050d506482dbdf024";
-        String expected = "2012-12-22T03:00:46.767+02:00";
-        assertThat(DateFormat.Tai64n.getFunction(DateTimeZone.forOffsetHours(2)).apply((randomBoolean() ? "@" : "") + input).toString(), equalTo(expected));
-    }
-
-    public void testFromString() {
-        assertThat(DateFormat.fromString("UNIX_MS"), equalTo(Optional.of(DateFormat.UnixMs)));
-        assertThat(DateFormat.fromString("unix_ms"), equalTo(Optional.empty()));
-        assertThat(DateFormat.fromString("UNIX"), equalTo(Optional.of(DateFormat.Unix)));
-        assertThat(DateFormat.fromString("unix"), equalTo(Optional.empty()));
-        assertThat(DateFormat.fromString("ISO8601"), equalTo(Optional.of(DateFormat.Iso8601)));
-        assertThat(DateFormat.fromString("iso8601"), equalTo(Optional.empty()));
-        assertThat(DateFormat.fromString("TAI64N"), equalTo(Optional.of(DateFormat.Tai64n)));
-        assertThat(DateFormat.fromString("tai64n"), equalTo(Optional.empty()));
-        assertThat(DateFormat.fromString("prefix-" + randomAsciiOfLengthBetween(1, 10)), equalTo(Optional.empty()));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorFactoryTests.java
deleted file mode 100644
index 708b164..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorFactoryTests.java
+++ /dev/null
@@ -1,186 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.test.ESTestCase;
-import org.joda.time.DateTimeZone;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class DateProcessorFactoryTests extends ESTestCase {
-
-    public void testBuildDefaults() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
-
-        DateProcessor processor = factory.create(config);
-        assertThat(processor.getMatchField(), equalTo(sourceField));
-        assertThat(processor.getTargetField(), equalTo(DateProcessor.DEFAULT_TARGET_FIELD));
-        assertThat(processor.getMatchFormats(), equalTo(Collections.singletonList("dd/MM/yyyyy")));
-        assertThat(processor.getLocale(), equalTo(Locale.ENGLISH));
-        assertThat(processor.getTimezone(), equalTo(DateTimeZone.UTC));
-    }
-
-    public void testMatchFieldIsMandatory() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String targetField = randomAsciiOfLengthBetween(1, 10);
-        config.put("target_field", targetField);
-        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
-
-        try {
-            factory.create(config);
-            fail("processor creation should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("required property [match_field] is missing"));
-        }
-    }
-
-    public void testMatchFormatsIsMandatory() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        String targetField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("target_field", targetField);
-
-        try {
-            factory.create(config);
-            fail("processor creation should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("required property [match_formats] is missing"));
-        }
-    }
-
-    public void testParseLocale() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
-        Locale locale = randomLocale(random());
-        config.put("locale", locale.toLanguageTag());
-
-        DateProcessor processor = factory.create(config);
-        assertThat(processor.getLocale().toLanguageTag(), equalTo(locale.toLanguageTag()));
-    }
-
-    public void testParseInvalidLocale() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
-        config.put("locale", "invalid_locale");
-        try {
-            factory.create(config);
-            fail("should fail with invalid locale");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("Invalid language tag specified: invalid_locale"));
-        }
-    }
-
-    public void testParseTimezone() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
-
-        DateTimeZone timezone = randomTimezone();
-        config.put("timezone", timezone.getID());
-        DateProcessor processor = factory.create(config);
-        assertThat(processor.getTimezone(), equalTo(timezone));
-    }
-
-    public void testParseInvalidTimezone() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
-        config.put("timezone", "invalid_timezone");
-        try {
-            factory.create(config);
-            fail("invalid timezone should fail");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("The datetime zone id 'invalid_timezone' is not recognised"));
-        }
-    }
-
-    //we generate a timezone out of the available ones in joda, some available in the jdk are not available in joda by default
-    private static DateTimeZone randomTimezone() {
-        List<String> ids = new ArrayList<>(DateTimeZone.getAvailableIDs());
-        Collections.sort(ids);
-        return DateTimeZone.forID(randomFrom(ids));
-    }
-
-
-    public void testParseMatchFormats() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", Arrays.asList("dd/MM/yyyy", "dd-MM-yyyy"));
-
-        DateProcessor processor = factory.create(config);
-        assertThat(processor.getMatchFormats(), equalTo(Arrays.asList("dd/MM/yyyy", "dd-MM-yyyy")));
-    }
-
-    public void testParseMatchFormatsFailure() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", "dd/MM/yyyy");
-
-        try {
-            factory.create(config);
-            fail("processor creation should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("property [match_formats] isn't a list, but of type [java.lang.String]"));
-        }
-    }
-
-    public void testParseTargetField() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        String targetField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("target_field", targetField);
-        config.put("match_formats", Arrays.asList("dd/MM/yyyy", "dd-MM-yyyy"));
-
-        DateProcessor processor = factory.create(config);
-        assertThat(processor.getTargetField(), equalTo(targetField));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorTests.java
deleted file mode 100644
index f7aba42..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorTests.java
+++ /dev/null
@@ -1,137 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.containsString;
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class DateProcessorTests extends ESTestCase {
-
-    public void testJodaPattern() {
-        DateProcessor dateProcessor = new DateProcessor(DateTimeZone.forID("Europe/Amsterdam"), Locale.ENGLISH,
-                "date_as_string", Collections.singletonList("yyyy dd MM hh:mm:ss"), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "2010 12 06 11:05:15");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T11:05:15.000+02:00"));
-    }
-
-    public void testJodaPatternMultipleFormats() {
-        List<String> matchFormats = new ArrayList<>();
-        matchFormats.add("yyyy dd MM");
-        matchFormats.add("dd/MM/yyyy");
-        matchFormats.add("dd-MM-yyyy");
-        DateProcessor dateProcessor = new DateProcessor(DateTimeZone.forID("Europe/Amsterdam"), Locale.ENGLISH,
-                "date_as_string", matchFormats, "date_as_date");
-
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "2010 12 06");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T00:00:00.000+02:00"));
-
-        document = new HashMap<>();
-        document.put("date_as_string", "12/06/2010");
-        ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T00:00:00.000+02:00"));
-
-        document = new HashMap<>();
-        document.put("date_as_string", "12-06-2010");
-        ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T00:00:00.000+02:00"));
-
-        document = new HashMap<>();
-        document.put("date_as_string", "2010");
-        ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        try {
-            dateProcessor.execute(ingestDocument);
-            fail("processor should have failed due to not supported date format");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("unable to parse date [2010]"));
-        }
-    }
-
-    public void testJodaPatternLocale() {
-        DateProcessor dateProcessor = new DateProcessor(DateTimeZone.forID("Europe/Amsterdam"), Locale.ITALIAN,
-                "date_as_string", Collections.singletonList("yyyy dd MMM"), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "2010 12 giugno");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T00:00:00.000+02:00"));
-    }
-
-    public void testJodaPatternDefaultYear() {
-        DateProcessor dateProcessor = new DateProcessor(DateTimeZone.forID("Europe/Amsterdam"), Locale.ENGLISH,
-                "date_as_string", Collections.singletonList("dd/MM"), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "12/06");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo(DateTime.now().getYear() + "-06-12T00:00:00.000+02:00"));
-    }
-
-    public void testTAI64N() {
-        DateProcessor dateProcessor = new DateProcessor(DateTimeZone.forOffsetHours(2), randomLocale(random()),
-                "date_as_string", Collections.singletonList(DateFormat.Tai64n.toString()), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        String dateAsString = (randomBoolean() ? "@" : "") + "4000000050d506482dbdf024";
-        document.put("date_as_string", dateAsString);
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2012-12-22T03:00:46.767+02:00"));
-    }
-
-    public void testUnixMs() {
-        DateProcessor dateProcessor = new DateProcessor(DateTimeZone.UTC, randomLocale(random()),
-                "date_as_string", Collections.singletonList(DateFormat.UnixMs.toString()), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "1000500");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("1970-01-01T00:16:40.500Z"));
-    }
-
-    public void testUnix() {
-        DateProcessor dateProcessor = new DateProcessor(DateTimeZone.UTC, randomLocale(random()),
-                "date_as_string", Collections.singletonList(DateFormat.Unix.toString()), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "1000.5");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("1970-01-01T00:16:40.500Z"));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorFactoryTests.java
deleted file mode 100644
index 4793cb6..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorFactoryTests.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.ingest.processor.FailProcessor;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class FailProcessorFactoryTests extends ESTestCase {
-
-    private FailProcessor.Factory factory;
-
-    @Before
-    public void init() {
-        factory = new FailProcessor.Factory(TestTemplateService.instance());
-    }
-
-    public void testCreate() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("message", "error");
-        FailProcessor failProcessor = factory.create(config);
-        assertThat(failProcessor.getMessage().execute(Collections.emptyMap()), equalTo("error"));
-    }
-
-    public void testCreateMissingMessageField() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [message] is missing"));
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorTests.java
deleted file mode 100644
index 0ee3068..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorTests.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.test.ESTestCase;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class FailProcessorTests extends ESTestCase {
-
-    public void test() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String message = randomAsciiOfLength(10);
-        Processor processor = new FailProcessor(new TestTemplateService.MockTemplate(message));
-        try {
-            processor.execute(ingestDocument);
-            fail("fail processor should throw an exception");
-        } catch (FailProcessorException e) {
-            assertThat(e.getMessage(), equalTo(message));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorFactoryTests.java
deleted file mode 100644
index 7a7377b..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorFactoryTests.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.processor.GsubProcessor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class GsubProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        GsubProcessor.Factory factory = new GsubProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("pattern", "\\.");
-        config.put("replacement", "-");
-        GsubProcessor gsubProcessor = factory.create(config);
-        assertThat(gsubProcessor.getField(), equalTo("field1"));
-        assertThat(gsubProcessor.getPattern().toString(), equalTo("\\."));
-        assertThat(gsubProcessor.getReplacement(), equalTo("-"));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        GsubProcessor.Factory factory = new GsubProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("pattern", "\\.");
-        config.put("replacement", "-");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoPatternPresent() throws Exception {
-        GsubProcessor.Factory factory = new GsubProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("replacement", "-");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [pattern] is missing"));
-        }
-    }
-
-    public void testCreateNoReplacementPresent() throws Exception {
-        GsubProcessor.Factory factory = new GsubProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("pattern", "\\.");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [replacement] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorTests.java
deleted file mode 100644
index 9c7a9bd..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorTests.java
+++ /dev/null
@@ -1,79 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.regex.Pattern;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class GsubProcessorTests extends ESTestCase {
-
-    public void testGsub() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, "127.0.0.1");
-        Processor processor = new GsubProcessor(fieldName, Pattern.compile("\\."), "-");
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo("127-0-0-1"));
-    }
-
-    public void testGsubNotAStringValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        ingestDocument.setFieldValue(fieldName, 123);
-        Processor processor = new GsubProcessor(fieldName, Pattern.compile("\\."), "-");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execution should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
-        }
-    }
-
-    public void testGsubFieldNotFound() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new GsubProcessor(fieldName, Pattern.compile("\\."), "-");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execution should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-
-    public void testGsubNullValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
-        Processor processor = new GsubProcessor("field", Pattern.compile("\\."), "-");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execution should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [field] is null, cannot match pattern."));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorFactoryTests.java
deleted file mode 100644
index 2d7bee1..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorFactoryTests.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class JoinProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        JoinProcessor.Factory factory = new JoinProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("separator", "-");
-        JoinProcessor joinProcessor = factory.create(config);
-        assertThat(joinProcessor.getField(), equalTo("field1"));
-        assertThat(joinProcessor.getSeparator(), equalTo("-"));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        JoinProcessor.Factory factory = new JoinProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("separator", "-");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoSeparatorPresent() throws Exception {
-        JoinProcessor.Factory factory = new JoinProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [separator] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorTests.java
deleted file mode 100644
index cbd4dd6..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorTests.java
+++ /dev/null
@@ -1,111 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class JoinProcessorTests extends ESTestCase {
-
-    private static final String[] SEPARATORS = new String[]{"-", "_", "."};
-
-    public void testJoinStrings() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        String separator = randomFrom(SEPARATORS);
-        List<String> fieldValue = new ArrayList<>(numItems);
-        String expectedResult = "";
-        for (int j = 0; j < numItems; j++) {
-            String value = randomAsciiOfLengthBetween(1, 10);
-            fieldValue.add(value);
-            expectedResult += value;
-            if (j < numItems - 1) {
-                expectedResult += separator;
-            }
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new JoinProcessor(fieldName, separator);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo(expectedResult));
-    }
-
-    public void testJoinIntegers() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        String separator = randomFrom(SEPARATORS);
-        List<Integer> fieldValue = new ArrayList<>(numItems);
-        String expectedResult = "";
-        for (int j = 0; j < numItems; j++) {
-            int value = randomInt();
-            fieldValue.add(value);
-            expectedResult += value;
-            if (j < numItems - 1) {
-                expectedResult += separator;
-            }
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new JoinProcessor(fieldName, separator);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo(expectedResult));
-    }
-
-    public void testJoinNonListField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        ingestDocument.setFieldValue(fieldName, randomAsciiOfLengthBetween(1, 10));
-        Processor processor = new JoinProcessor(fieldName, "-");
-        try {
-            processor.execute(ingestDocument);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.String] cannot be cast to [java.util.List]"));
-        }
-    }
-
-    public void testJoinNonExistingField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new JoinProcessor(fieldName, "-");
-        try {
-            processor.execute(ingestDocument);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-
-    public void testJoinNullValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
-        Processor processor = new JoinProcessor("field", "-");
-        try {
-            processor.execute(ingestDocument);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [field] is null, cannot join."));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorFactoryTests.java
deleted file mode 100644
index 478bb70..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorFactoryTests.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class LowercaseProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        LowercaseProcessor.Factory factory = new LowercaseProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        LowercaseProcessor uppercaseProcessor = factory.create(config);
-        assertThat(uppercaseProcessor.getField(), equalTo("field1"));
-    }
-
-    public void testCreateMissingField() throws Exception {
-        LowercaseProcessor.Factory factory = new LowercaseProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorTests.java
deleted file mode 100644
index 898e431..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorTests.java
+++ /dev/null
@@ -1,34 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import java.util.Locale;
-
-public class LowercaseProcessorTests extends AbstractStringProcessorTestCase {
-    @Override
-    protected AbstractStringProcessor newProcessor(String field) {
-        return new LowercaseProcessor(field);
-    }
-
-    @Override
-    protected String expectedResult(String input) {
-        return input.toLowerCase(Locale.ROOT);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorFactoryTests.java
deleted file mode 100644
index 6c0e899..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorFactoryTests.java
+++ /dev/null
@@ -1,58 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class RemoveProcessorFactoryTests extends ESTestCase {
-
-    private RemoveProcessor.Factory factory;
-
-    @Before
-    public void init() {
-        factory = new RemoveProcessor.Factory(TestTemplateService.instance());
-    }
-
-    public void testCreate() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        RemoveProcessor removeProcessor = factory.create(config);
-        assertThat(removeProcessor.getField().execute(Collections.emptyMap()), equalTo("field1"));
-    }
-
-    public void testCreateMissingField() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorTests.java
deleted file mode 100644
index 891dc57..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorTests.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class RemoveProcessorTests extends ESTestCase {
-
-    public void testRemoveFields() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String field = RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument);
-        Processor processor = new RemoveProcessor(new TestTemplateService.MockTemplate(field));
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.hasField(field), equalTo(false));
-    }
-
-    public void testRemoveNonExistingField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new RemoveProcessor(new TestTemplateService.MockTemplate(fieldName));
-        try {
-            processor.execute(ingestDocument);
-            fail("remove field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorFactoryTests.java
deleted file mode 100644
index e0298a9..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorFactoryTests.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class RenameProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        RenameProcessor.Factory factory = new RenameProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "old_field");
-        config.put("to", "new_field");
-        RenameProcessor renameProcessor = factory.create(config);
-        assertThat(renameProcessor.getOldFieldName(), equalTo("old_field"));
-        assertThat(renameProcessor.getNewFieldName(), equalTo("new_field"));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        RenameProcessor.Factory factory = new RenameProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("to", "new_field");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoToPresent() throws Exception {
-        RenameProcessor.Factory factory = new RenameProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "old_field");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [to] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorTests.java
deleted file mode 100644
index c42ca82..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorTests.java
+++ /dev/null
@@ -1,175 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.nullValue;
-
-public class RenameProcessorTests extends ESTestCase {
-
-    public void testRename() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldName = RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument);
-        Object fieldValue = ingestDocument.getFieldValue(fieldName, Object.class);
-        String newFieldName;
-        do {
-            newFieldName = RandomDocumentPicks.randomFieldName(random());
-        } while (RandomDocumentPicks.canAddField(newFieldName, ingestDocument) == false || newFieldName.equals(fieldName));
-        Processor processor = new RenameProcessor(fieldName, newFieldName);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(newFieldName, Object.class), equalTo(fieldValue));
-    }
-
-    public void testRenameArrayElement() throws Exception {
-        Map<String, Object> document = new HashMap<>();
-        List<String> list = new ArrayList<>();
-        list.add("item1");
-        list.add("item2");
-        list.add("item3");
-        document.put("list", list);
-        List<Map<String, String>> one = new ArrayList<>();
-        one.add(Collections.singletonMap("one", "one"));
-        one.add(Collections.singletonMap("two", "two"));
-        document.put("one", one);
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-
-        Processor processor = new RenameProcessor("list.0", "item");
-        processor.execute(ingestDocument);
-        Object actualObject = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(actualObject, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<String> actualList = (List<String>) actualObject;
-        assertThat(actualList.size(), equalTo(2));
-        assertThat(actualList.get(0), equalTo("item2"));
-        assertThat(actualList.get(1), equalTo("item3"));
-        actualObject = ingestDocument.getSourceAndMetadata().get("item");
-        assertThat(actualObject, instanceOf(String.class));
-        assertThat(actualObject, equalTo("item1"));
-
-        processor = new RenameProcessor("list.0", "list.3");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[3] is out of bounds for array with length [2] as part of path [list.3]"));
-            assertThat(actualList.size(), equalTo(2));
-            assertThat(actualList.get(0), equalTo("item2"));
-            assertThat(actualList.get(1), equalTo("item3"));
-        }
-    }
-
-    public void testRenameNonExistingField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new RenameProcessor(fieldName, RandomDocumentPicks.randomFieldName(random()));
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] doesn't exist"));
-        }
-    }
-
-    public void testRenameNewFieldAlreadyExists() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldName = RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument);
-        Processor processor = new RenameProcessor(RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument), fieldName);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] already exists"));
-        }
-    }
-
-    public void testRenameExistingFieldNullValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        ingestDocument.setFieldValue(fieldName, null);
-        String newFieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new RenameProcessor(fieldName, newFieldName);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.hasField(fieldName), equalTo(false));
-        assertThat(ingestDocument.hasField(newFieldName), equalTo(true));
-        assertThat(ingestDocument.getFieldValue(newFieldName, Object.class), nullValue());
-    }
-
-    public void testRenameAtomicOperationSetFails() throws Exception {
-        Map<String, Object> source = new HashMap<String, Object>() {
-            private static final long serialVersionUID = 362498820763181265L;
-            @Override
-            public Object put(String key, Object value) {
-                if (key.equals("new_field")) {
-                    throw new UnsupportedOperationException();
-                }
-                return super.put(key, value);
-            }
-        };
-        source.put("list", Collections.singletonList("item"));
-
-        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
-        Processor processor = new RenameProcessor("list", "new_field");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(UnsupportedOperationException e) {
-            //the set failed, the old field has not been removed
-            assertThat(ingestDocument.getSourceAndMetadata().containsKey("list"), equalTo(true));
-            assertThat(ingestDocument.getSourceAndMetadata().containsKey("new_field"), equalTo(false));
-        }
-    }
-
-    public void testRenameAtomicOperationRemoveFails() throws Exception {
-        Map<String, Object> source = new HashMap<String, Object>() {
-            private static final long serialVersionUID = 362498820763181265L;
-            @Override
-            public Object remove(Object key) {
-                if (key.equals("list")) {
-                    throw new UnsupportedOperationException();
-                }
-                return super.remove(key);
-            }
-        };
-        source.put("list", Collections.singletonList("item"));
-
-        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
-        Processor processor = new RenameProcessor("list", "new_field");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch (UnsupportedOperationException e) {
-            //the set failed, the old field has not been removed
-            assertThat(ingestDocument.getSourceAndMetadata().containsKey("list"), equalTo(true));
-            assertThat(ingestDocument.getSourceAndMetadata().containsKey("new_field"), equalTo(false));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorFactoryTests.java
deleted file mode 100644
index 22ece63..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorFactoryTests.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class SetProcessorFactoryTests extends ESTestCase {
-
-    private SetProcessor.Factory factory;
-
-    @Before
-    public void init() {
-        factory = new SetProcessor.Factory(TestTemplateService.instance());
-    }
-
-    public void testCreate() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("value", "value1");
-        SetProcessor setProcessor = factory.create(config);
-        assertThat(setProcessor.getField().execute(Collections.emptyMap()), equalTo("field1"));
-        assertThat(setProcessor.getValue().copyAndResolve(Collections.emptyMap()), equalTo("value1"));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("value", "value1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoValuePresent() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [value] is missing"));
-        }
-    }
-
-    public void testCreateNullValue() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("value", null);
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [value] is missing"));
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorTests.java
deleted file mode 100644
index b66cc24..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorTests.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.ingest.core.ValueSource;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-import org.hamcrest.Matchers;
-
-import java.util.HashMap;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class SetProcessorTests extends ESTestCase {
-
-    public void testSetExistingFields() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldName = RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument);
-        Object fieldValue = RandomDocumentPicks.randomFieldValue(random());
-        Processor processor = createSetProcessor(fieldName, fieldValue);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.hasField(fieldName), equalTo(true));
-        assertThat(ingestDocument.getFieldValue(fieldName, Object.class), equalTo(fieldValue));
-    }
-
-    public void testSetNewFields() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        //used to verify that there are no conflicts between subsequent fields going to be added
-        IngestDocument testIngestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        Object fieldValue = RandomDocumentPicks.randomFieldValue(random());
-        String fieldName = RandomDocumentPicks.addRandomField(random(), testIngestDocument, fieldValue);
-        Processor processor = createSetProcessor(fieldName, fieldValue);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.hasField(fieldName), equalTo(true));
-        assertThat(ingestDocument.getFieldValue(fieldName, Object.class), equalTo(fieldValue));
-    }
-
-    public void testSetFieldsTypeMismatch() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        ingestDocument.setFieldValue("field", "value");
-        Processor processor = createSetProcessor("field.inner", "value");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot set [inner] with parent object of type [java.lang.String] as part of path [field.inner]"));
-        }
-    }
-
-    public void testSetMetadata() throws Exception {
-        IngestDocument.MetaData randomMetaData = randomFrom(IngestDocument.MetaData.values());
-        Processor processor = createSetProcessor(randomMetaData.getFieldName(), "_value");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(randomMetaData.getFieldName(), String.class), Matchers.equalTo("_value"));
-    }
-
-    private static Processor createSetProcessor(String fieldName, Object fieldValue) {
-        TemplateService templateService = TestTemplateService.instance();
-        return new SetProcessor(templateService.compile(fieldName), ValueSource.wrap(fieldValue, templateService));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorFactoryTests.java
deleted file mode 100644
index d9cf2a0..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorFactoryTests.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class SplitProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        SplitProcessor.Factory factory = new SplitProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("separator", "\\.");
-        SplitProcessor splitProcessor = factory.create(config);
-        assertThat(splitProcessor.getField(), equalTo("field1"));
-        assertThat(splitProcessor.getSeparator(), equalTo("\\."));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        SplitProcessor.Factory factory = new SplitProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("separator", "\\.");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoSeparatorPresent() throws Exception {
-        SplitProcessor.Factory factory = new SplitProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [separator] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorTests.java
deleted file mode 100644
index d5a587f..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorTests.java
+++ /dev/null
@@ -1,80 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class SplitProcessorTests extends ESTestCase {
-
-    public void testSplit() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, "127.0.0.1");
-        Processor processor = new SplitProcessor(fieldName, "\\.");
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(Arrays.asList("127", "0", "0", "1")));
-    }
-
-    public void testSplitFieldNotFound() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new SplitProcessor(fieldName, "\\.");
-        try {
-            processor.execute(ingestDocument);
-            fail("split processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-
-    public void testSplitNullValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
-        Processor processor = new SplitProcessor("field", "\\.");
-        try {
-            processor.execute(ingestDocument);
-            fail("split processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [field] is null, cannot split."));
-        }
-    }
-
-    public void testSplitNonStringValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        ingestDocument.setFieldValue(fieldName, randomInt());
-        Processor processor = new SplitProcessor(fieldName, "\\.");
-        try {
-            processor.execute(ingestDocument);
-            fail("split processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorFactoryTests.java
deleted file mode 100644
index f7bcf83..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorFactoryTests.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class TrimProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        TrimProcessor.Factory factory = new TrimProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        TrimProcessor uppercaseProcessor = factory.create(config);
-        assertThat(uppercaseProcessor.getField(), equalTo("field1"));
-    }
-
-    public void testCreateMissingField() throws Exception {
-        TrimProcessor.Factory factory = new TrimProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorTests.java
deleted file mode 100644
index 265040e..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorTests.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-public class TrimProcessorTests extends AbstractStringProcessorTestCase {
-
-    @Override
-    protected AbstractStringProcessor newProcessor(String field) {
-        return new TrimProcessor(field);
-    }
-
-    @Override
-    protected String modifyInput(String input) {
-        String updatedFieldValue = "";
-        updatedFieldValue = addWhitespaces(updatedFieldValue);
-        updatedFieldValue += input;
-        updatedFieldValue = addWhitespaces(updatedFieldValue);
-        return updatedFieldValue;
-    }
-
-    @Override
-    protected String expectedResult(String input) {
-        return input.trim();
-    }
-
-    private static String addWhitespaces(String input) {
-        int prefixLength = randomIntBetween(0, 10);
-        for (int i = 0; i < prefixLength; i++) {
-            input += ' ';
-        }
-        return input;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorFactoryTests.java
deleted file mode 100644
index c683e8e..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorFactoryTests.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class UppercaseProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        UppercaseProcessor.Factory factory = new UppercaseProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        UppercaseProcessor uppercaseProcessor = factory.create(config);
-        assertThat(uppercaseProcessor.getField(), equalTo("field1"));
-    }
-
-    public void testCreateMissingField() throws Exception {
-        UppercaseProcessor.Factory factory = new UppercaseProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorTests.java
deleted file mode 100644
index 343fff1..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorTests.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import java.util.Locale;
-
-public class UppercaseProcessorTests extends AbstractStringProcessorTestCase {
-
-    @Override
-    protected AbstractStringProcessor newProcessor(String field) {
-        return new UppercaseProcessor(field);
-    }
-
-    @Override
-    protected String expectedResult(String input) {
-        return input.toUpperCase(Locale.ROOT);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/percolator/RecoveryPercolatorIT.java b/core/src/test/java/org/elasticsearch/percolator/RecoveryPercolatorIT.java
index 57eb2e3..69afaa7 100644
--- a/core/src/test/java/org/elasticsearch/percolator/RecoveryPercolatorIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/RecoveryPercolatorIT.java
@@ -219,18 +219,9 @@ public class RecoveryPercolatorIT extends ESIntegTestCase {
         assertThat(response.getMatches()[0].getId().string(), equalTo("100"));
     }
 
-    public void testSinglePercolatorRecovery() throws Exception {
-        percolatorRecovery(false);
-    }
-
-    @AwaitsFix(bugUrl = "sometimes reprodes with: gradle :core:integTest -Dtests.seed=21DDCAA92013B00C -Dtests.class=org.elasticsearch.percolator.RecoveryPercolatorIT -Dtests.method=\"testMultiPercolatorRecovery\"")
-    public void testMultiPercolatorRecovery() throws Exception {
-        percolatorRecovery(true);
-    }
-
-    // 3 nodes, 2 primary + 2 replicas per primary, so each node should have a copy of the data.
-    // We only start and stop nodes 2 and 3, so all requests should succeed and never be partial.
-    private void percolatorRecovery(final boolean multiPercolate) throws Exception {
+    public void testPercolatorRecovery() throws Exception {
+        // 3 nodes, 2 primary + 2 replicas per primary, so each node should have a copy of the data.
+        // We only start and stop nodes 2 and 3, so all requests should succeed and never be partial.
         internalCluster().startNode(settingsBuilder().put("node.stay", true));
         internalCluster().startNode(settingsBuilder().put("node.stay", false));
         internalCluster().startNode(settingsBuilder().put("node.stay", false));
@@ -260,56 +251,26 @@ public class RecoveryPercolatorIT extends ESIntegTestCase {
 
         final AtomicBoolean run = new AtomicBoolean(true);
         final AtomicReference<Throwable> error = new AtomicReference<>();
-        Runnable r = new Runnable() {
-            @Override
-            public void run() {
-                try {
-                    while (run.get()) {
-                        if (multiPercolate) {
-                            MultiPercolateRequestBuilder builder = client
-                                    .prepareMultiPercolate();
-                            int numPercolateRequest = randomIntBetween(50, 100);
-
-                            for (int i = 0; i < numPercolateRequest; i++) {
-                                PercolateRequestBuilder percolateBuilder = client.preparePercolate()
-                                        .setIndices("test").setDocumentType("type").setSize(numQueries);
-                                if (randomBoolean()) {
-                                    percolateBuilder.setGetRequest(Requests.getRequest("test").type("type").id("1"));
-                                } else {
-                                    percolateBuilder.setPercolateDoc(docBuilder().setDoc(document));
-                                }
-                                builder.add(percolateBuilder);
-                            }
-
-                            MultiPercolateResponse response = builder.get();
-                            assertThat(response.items().length, equalTo(numPercolateRequest));
-                            for (MultiPercolateResponse.Item item : response) {
-                                assertThat(item.isFailure(), equalTo(false));
-                                assertNoFailures(item.getResponse());
-                                assertThat(item.getResponse().getSuccessfulShards(), equalTo(item.getResponse().getTotalShards()));
-                                assertThat(item.getResponse().getCount(), equalTo((long) numQueries));
-                                assertThat(item.getResponse().getMatches().length, equalTo(numQueries));
-                            }
-                        } else {
-                            PercolateRequestBuilder percolateBuilder = client.preparePercolate()
-                                    .setIndices("test").setDocumentType("type").setSize(numQueries);
-                            if (randomBoolean()) {
-                                percolateBuilder.setPercolateDoc(docBuilder().setDoc(document));
-                            } else {
-                                percolateBuilder.setGetRequest(Requests.getRequest("test").type("type").id("1"));
-                            }
-                            PercolateResponse response = percolateBuilder.get();
-                            assertNoFailures(response);
-                            assertThat(response.getSuccessfulShards(), equalTo(response.getTotalShards()));
-                            assertThat(response.getCount(), equalTo((long) numQueries));
-                            assertThat(response.getMatches().length, equalTo(numQueries));
-                        }
+        Runnable r = () -> {
+            try {
+                while (run.get()) {
+                    PercolateRequestBuilder percolateBuilder = client.preparePercolate()
+                        .setIndices("test").setDocumentType("type").setSize(numQueries);
+                    if (randomBoolean()) {
+                        percolateBuilder.setPercolateDoc(docBuilder().setDoc(document));
+                    } else {
+                        percolateBuilder.setGetRequest(Requests.getRequest("test").type("type").id("1"));
                     }
-                } catch (Throwable t) {
-                    logger.info("Error in percolate thread...", t);
-                    run.set(false);
-                    error.set(t);
+                    PercolateResponse response = percolateBuilder.get();
+                    assertNoFailures(response);
+                    assertThat(response.getSuccessfulShards(), equalTo(response.getTotalShards()));
+                    assertThat(response.getCount(), equalTo((long) numQueries));
+                    assertThat(response.getMatches().length, equalTo(numQueries));
                 }
+            } catch (Throwable t) {
+                logger.info("Error in percolate thread...", t);
+                run.set(false);
+                error.set(t);
             }
         };
         Thread t = new Thread(r);
diff --git a/core/src/test/java/org/elasticsearch/routing/AliasResolveRoutingIT.java b/core/src/test/java/org/elasticsearch/routing/AliasResolveRoutingIT.java
index db21fef..e73f7e5 100644
--- a/core/src/test/java/org/elasticsearch/routing/AliasResolveRoutingIT.java
+++ b/core/src/test/java/org/elasticsearch/routing/AliasResolveRoutingIT.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.routing;
 
+import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.common.Priority;
@@ -27,9 +28,12 @@ import org.elasticsearch.test.ESIntegTestCase;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
+import java.util.concurrent.ExecutionException;
 
 import static org.elasticsearch.cluster.metadata.AliasAction.newAddAliasAction;
 import static org.elasticsearch.common.util.set.Sets.newHashSet;
+import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.nullValue;
 
@@ -37,6 +41,22 @@ import static org.hamcrest.Matchers.nullValue;
  *
  */
 public class AliasResolveRoutingIT extends ESIntegTestCase {
+
+
+    // see https://github.com/elastic/elasticsearch/issues/13278
+    public void testSearchClosedWildcardIndex() throws ExecutionException, InterruptedException {
+        createIndex("test-0");
+        createIndex("test-1");
+        ensureGreen();
+        client().admin().indices().prepareAliases().addAlias("test-0", "alias-0").addAlias("test-1", "alias-1").get();
+        client().admin().indices().prepareClose("test-1").get();
+        indexRandom(true, client().prepareIndex("test-0", "type1", "1").setSource("field1", "the quick brown fox jumps"),
+            client().prepareIndex("test-0", "type1", "2").setSource("field1", "quick brown"),
+            client().prepareIndex("test-0", "type1", "3").setSource("field1", "quick"));
+        refresh("test-*");
+        assertHitCount(client().prepareSearch().setIndices("alias-*").setIndicesOptions(IndicesOptions.lenientExpandOpen()).setQuery(matchQuery("_all", "quick")).get(), 3l);
+    }
+
     public void testResolveIndexRouting() throws Exception {
         createIndex("test1");
         createIndex("test2");
diff --git a/core/src/test/java/org/elasticsearch/script/FileScriptTests.java b/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
index 37f2ebb..987aef9 100644
--- a/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
+++ b/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
@@ -63,8 +63,7 @@ public class FileScriptTests extends ESTestCase {
             .put("script.engine." + MockScriptEngine.NAME + ".file.aggs", false)
             .put("script.engine." + MockScriptEngine.NAME + ".file.search", false)
             .put("script.engine." + MockScriptEngine.NAME + ".file.mapping", false)
-            .put("script.engine." + MockScriptEngine.NAME + ".file.update", false)
-            .put("script.engine." + MockScriptEngine.NAME + ".file.ingest", false).build();
+            .put("script.engine." + MockScriptEngine.NAME + ".file.update", false).build();
         ScriptService scriptService = makeScriptService(settings);
         Script script = new Script("script1", ScriptService.ScriptType.FILE, MockScriptEngine.NAME, null);
         for (ScriptContext context : ScriptContext.Standard.values()) {
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
index b5ef5d9..17e8fd3 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
@@ -34,10 +34,13 @@ import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.store.Directory;
 import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
 import org.elasticsearch.common.compress.CompressedXContent;
+import org.elasticsearch.common.lucene.index.ElasticsearchDirectoryReader;
 import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.index.Index;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
+import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.BucketCollector;
@@ -110,7 +113,8 @@ public class NestedAggregatorTests extends ESSingleNodeTestCase {
         indexWriter.commit();
         indexWriter.close();
 
-        DirectoryReader directoryReader =  DirectoryReader.open(directory);
+        DirectoryReader directoryReader = DirectoryReader.open(directory);
+        directoryReader = ElasticsearchDirectoryReader.wrap(directoryReader, new ShardId(new Index("test"), 0));
         IndexSearcher searcher = new IndexSearcher(directoryReader);
 
         IndexService indexService = createIndex("test");
diff --git a/core/src/test/java/org/elasticsearch/snapshots/AbstractSnapshotIntegTestCase.java b/core/src/test/java/org/elasticsearch/snapshots/AbstractSnapshotIntegTestCase.java
index 1ef3fdd..c6e9320 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/AbstractSnapshotIntegTestCase.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/AbstractSnapshotIntegTestCase.java
@@ -176,7 +176,10 @@ public abstract class AbstractSnapshotIntegTestCase extends ESIntegTestCase {
         private long stopWaitingAt = -1;
 
         public BlockingClusterStateListener(ClusterService clusterService, String blockOn, String countOn, Priority passThroughPriority) {
-            this(clusterService, blockOn, countOn, passThroughPriority, TimeValue.timeValueMinutes(1));
+            // Waiting for the 70 seconds here to make sure that the last check at 65 sec mark in assertBusyPendingTasks has a chance
+            // to finish before we timeout on the cluster state block. Otherwise the last check in assertBusyPendingTasks kicks in
+            // after the cluster state block clean up takes place and it's assert doesn't reflect the actual failure
+            this(clusterService, blockOn, countOn, passThroughPriority, TimeValue.timeValueSeconds(70));
         }
 
         public BlockingClusterStateListener(ClusterService clusterService, final String blockOn, final String countOn, Priority passThroughPriority, TimeValue timeout) {
diff --git a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
index 669527f..e8ff967 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
@@ -1943,7 +1943,7 @@ public class SharedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTestCas
                         .put("compress", randomBoolean())
                         .put("chunk_size", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));
 
-        assertAcked(prepareCreate("test-idx", 0, settingsBuilder().put("number_of_shards", between(1, 20))
+        assertAcked(prepareCreate("test-idx", 0, settingsBuilder().put("number_of_shards", between(1, 10))
                 .put("number_of_replicas", 0)));
         ensureGreen();
 
diff --git a/core/src/test/java/org/elasticsearch/threadpool/ThreadPoolTests.java b/core/src/test/java/org/elasticsearch/threadpool/ThreadPoolTests.java
deleted file mode 100644
index 7488de3..0000000
--- a/core/src/test/java/org/elasticsearch/threadpool/ThreadPoolTests.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.threadpool;
-
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.test.ESTestCase;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.not;
-
-public class ThreadPoolTests extends ESTestCase {
-
-    public void testIngestThreadPoolNotStartedWithIngestDisabled() throws Exception {
-        Settings settings = Settings.builder().put("name", "test").put("node.ingest", false).build();
-        ThreadPool threadPool = null;
-        try {
-            threadPool = new ThreadPool(settings);
-            for (ThreadPool.Info info : threadPool.info()) {
-                assertThat(info.getName(), not(equalTo("ingest")));
-            }
-        } finally {
-            if (threadPool != null) {
-                terminate(threadPool);
-            }
-        }
-    }
-
-    public void testIngestThreadPoolStartedWithIngestEnabled() throws Exception {
-        Settings settings = Settings.builder().put("name", "test").put("node.ingest", true).build();
-        ThreadPool threadPool = null;
-        try {
-            threadPool = new ThreadPool(settings);
-            boolean ingestFound = false;
-            for (ThreadPool.Info info : threadPool.info()) {
-                if (info.getName().equals("ingest")) {
-                    ingestFound = true;
-                    break;
-                }
-            }
-            assertThat(ingestFound, equalTo(true));
-        } finally {
-            if (threadPool != null) {
-                terminate(threadPool);
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java b/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
index ef03181..09653c1 100644
--- a/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
+++ b/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
@@ -21,6 +21,7 @@ package org.elasticsearch.threadpool;
 
 import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.util.concurrent.EsExecutors;
 import org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.threadpool.ThreadPool.Names;
@@ -28,7 +29,6 @@ import org.elasticsearch.threadpool.ThreadPool.Names;
 import java.lang.reflect.Field;
 import java.util.Arrays;
 import java.util.HashSet;
-import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.Executor;
@@ -47,7 +47,6 @@ import static org.hamcrest.Matchers.sameInstance;
 /**
  */
 public class UpdateThreadPoolSettingsTests extends ESTestCase {
-
     public void testCorrectThreadPoolTypePermittedInSettings() throws InterruptedException {
         String threadPoolName = randomThreadPoolName();
         ThreadPool.ThreadPoolType correctThreadPoolType = ThreadPool.THREAD_POOL_TYPES.get(threadPoolName);
@@ -91,6 +90,51 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         }
     }
 
+    public void testIndexingThreadPoolsMaxSize() throws InterruptedException {
+        String threadPoolName = randomThreadPoolName();
+        for (String name : new String[] {ThreadPool.Names.BULK, ThreadPool.Names.INDEX}) {
+            ThreadPool threadPool = null;
+            try {
+
+                int maxSize = EsExecutors.boundedNumberOfProcessors(Settings.EMPTY);
+
+                // try to create a too-big (maxSize+1) thread pool
+                threadPool = new ThreadPool(settingsBuilder()
+                                               .put("name", "testIndexingThreadPoolsMaxSize")
+                                               .put("threadpool." + name + ".size", maxSize+1)
+                                               .build());
+
+                // confirm it clipped us at the maxSize:
+                assertEquals(maxSize, ((ThreadPoolExecutor) threadPool.executor(name)).getMaximumPoolSize());
+
+                ClusterSettings clusterSettings = new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
+                threadPool.setClusterSettings(clusterSettings);
+
+                // update it to a tiny size:
+                clusterSettings.applySettings(
+                        settingsBuilder()
+                        .put("threadpool." + name + ".size", 1)
+                        .build()
+                );
+
+                // confirm it worked:
+                assertEquals(1, ((ThreadPoolExecutor) threadPool.executor(name)).getMaximumPoolSize());
+
+                // try to update to too-big size:
+                clusterSettings.applySettings(
+                        settingsBuilder()
+                        .put("threadpool." + name + ".size", maxSize+1)
+                        .build()
+                );
+
+                // confirm it clipped us at the maxSize:
+                assertEquals(maxSize, ((ThreadPoolExecutor) threadPool.executor(name)).getMaximumPoolSize());
+            } finally {
+                terminateThreadPoolIfNeeded(threadPool);
+            }
+        }
+    }
+
     public void testUpdateSettingsCanNotChangeThreadPoolType() throws InterruptedException {
         String threadPoolName = randomThreadPoolName();
         ThreadPool.ThreadPoolType invalidThreadPoolType = randomIncorrectThreadPoolType(threadPoolName);
@@ -167,6 +211,14 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         }
     }
 
+    private static int getExpectedThreadPoolSize(Settings settings, String name, int size) {
+        if (name.equals(ThreadPool.Names.BULK) || name.equals(ThreadPool.Names.INDEX)) {
+            return Math.min(size, EsExecutors.boundedNumberOfProcessors(settings));
+        } else {
+            return size;
+        }
+    }
+
     public void testFixedExecutorType() throws InterruptedException {
         String threadPoolName = randomThreadPool(ThreadPool.ThreadPoolType.FIXED);
         ThreadPool threadPool = null;
@@ -181,12 +233,14 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
             Settings settings = clusterSettings.applySettings(settingsBuilder()
                     .put("threadpool." + threadPoolName + ".size", "15")
                     .build());
+
+            int expectedSize = getExpectedThreadPoolSize(nodeSettings, threadPoolName, 15);
             assertEquals(info(threadPool, threadPoolName).getThreadPoolType(), ThreadPool.ThreadPoolType.FIXED);
             assertThat(threadPool.executor(threadPoolName), instanceOf(EsThreadPoolExecutor.class));
-            assertThat(((EsThreadPoolExecutor) threadPool.executor(threadPoolName)).getCorePoolSize(), equalTo(15));
-            assertThat(((EsThreadPoolExecutor) threadPool.executor(threadPoolName)).getMaximumPoolSize(), equalTo(15));
-            assertThat(info(threadPool, threadPoolName).getMin(), equalTo(15));
-            assertThat(info(threadPool, threadPoolName).getMax(), equalTo(15));
+            assertThat(((EsThreadPoolExecutor) threadPool.executor(threadPoolName)).getCorePoolSize(), equalTo(expectedSize));
+            assertThat(((EsThreadPoolExecutor) threadPool.executor(threadPoolName)).getMaximumPoolSize(), equalTo(expectedSize));
+            assertThat(info(threadPool, threadPoolName).getMin(), equalTo(expectedSize));
+            assertThat(info(threadPool, threadPoolName).getMax(), equalTo(expectedSize));
             // keep alive does not apply to fixed thread pools
             assertThat(((EsThreadPoolExecutor) threadPool.executor(threadPoolName)).getKeepAliveTime(TimeUnit.MINUTES), equalTo(0L));
 
@@ -196,20 +250,23 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
             // Make sure keep alive value is not used
             assertThat(info(threadPool, threadPoolName).getKeepAlive(), nullValue());
             // Make sure keep pool size value were reused
-            assertThat(info(threadPool, threadPoolName).getMin(), equalTo(15));
-            assertThat(info(threadPool, threadPoolName).getMax(), equalTo(15));
+            assertThat(info(threadPool, threadPoolName).getMin(), equalTo(expectedSize));
+            assertThat(info(threadPool, threadPoolName).getMax(), equalTo(expectedSize));
             assertThat(threadPool.executor(threadPoolName), instanceOf(EsThreadPoolExecutor.class));
-            assertThat(((EsThreadPoolExecutor) threadPool.executor(threadPoolName)).getCorePoolSize(), equalTo(15));
-            assertThat(((EsThreadPoolExecutor) threadPool.executor(threadPoolName)).getMaximumPoolSize(), equalTo(15));
+            assertThat(((EsThreadPoolExecutor) threadPool.executor(threadPoolName)).getCorePoolSize(), equalTo(expectedSize));
+            assertThat(((EsThreadPoolExecutor) threadPool.executor(threadPoolName)).getMaximumPoolSize(), equalTo(expectedSize));
 
             // Change size
             Executor oldExecutor = threadPool.executor(threadPoolName);
             settings = clusterSettings.applySettings(settingsBuilder().put(settings).put("threadpool." + threadPoolName + ".size", "10").build());
+
+            expectedSize = getExpectedThreadPoolSize(nodeSettings, threadPoolName, 10);
+
             // Make sure size values changed
-            assertThat(info(threadPool, threadPoolName).getMax(), equalTo(10));
-            assertThat(info(threadPool, threadPoolName).getMin(), equalTo(10));
-            assertThat(((EsThreadPoolExecutor) threadPool.executor(threadPoolName)).getMaximumPoolSize(), equalTo(10));
-            assertThat(((EsThreadPoolExecutor) threadPool.executor(threadPoolName)).getCorePoolSize(), equalTo(10));
+            assertThat(info(threadPool, threadPoolName).getMax(), equalTo(expectedSize));
+            assertThat(info(threadPool, threadPoolName).getMin(), equalTo(expectedSize));
+            assertThat(((EsThreadPoolExecutor) threadPool.executor(threadPoolName)).getMaximumPoolSize(), equalTo(expectedSize));
+            assertThat(((EsThreadPoolExecutor) threadPool.executor(threadPoolName)).getCorePoolSize(), equalTo(expectedSize));
             // Make sure executor didn't change
             assertEquals(info(threadPool, threadPoolName).getThreadPoolType(), ThreadPool.ThreadPoolType.FIXED);
             assertThat(threadPool.executor(threadPoolName), sameInstance(oldExecutor));
@@ -395,10 +452,11 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         Set<ThreadPool.ThreadPoolType> set = new HashSet<>();
         set.addAll(Arrays.asList(ThreadPool.ThreadPoolType.values()));
         set.remove(ThreadPool.THREAD_POOL_TYPES.get(threadPoolName));
-        return randomFrom(set.toArray(new ThreadPool.ThreadPoolType[set.size()]));
+        ThreadPool.ThreadPoolType invalidThreadPoolType = randomFrom(set.toArray(new ThreadPool.ThreadPoolType[set.size()]));
+        return invalidThreadPoolType;
     }
 
     private String randomThreadPool(ThreadPool.ThreadPoolType type) {
-        return randomFrom(ThreadPool.THREAD_POOL_TYPES.entrySet().stream().filter(t -> t.getValue().equals(type)).map(Map.Entry::getKey).collect(Collectors.toList()));
+        return randomFrom(ThreadPool.THREAD_POOL_TYPES.entrySet().stream().filter(t -> t.getValue().equals(type)).map(t -> t.getKey()).collect(Collectors.toList()));
     }
 }
diff --git a/dev-tools/prepare_release_candidate.py b/dev-tools/prepare_release_candidate.py
index 31b0704..e0baa75 100644
--- a/dev-tools/prepare_release_candidate.py
+++ b/dev-tools/prepare_release_candidate.py
@@ -356,7 +356,8 @@ if __name__ == "__main__":
   debs3_list_cmd = 'deb-s3 list -b %s --prefix %s' % (bucket, debs3_prefix)
   debs3_verify_cmd = 'deb-s3 verify -b %s --prefix %s' % (bucket, debs3_prefix)
   rpms3_prefix = 'elasticsearch/staging/%s-%s/repos/%s/centos' % (release_version, shortHash, package_repo_version)
-  rpms3_upload_cmd = 'rpm-s3 -v -b %s -p %s --sign --visibility public-read -k 100 %s' % (bucket, rpms3_prefix, rpm)
+  # external-1 is the alias name for the us-east-1 region. This is used by rpm-s3 to construct the hostname
+  rpms3_upload_cmd = 'rpm-s3 -v -b %s -p %s --sign --visibility public-read -k 100 %s -r external-1' % (bucket, rpms3_prefix, rpm)
 
   if deploy_s3:
     run(s3cmd_sync_to_staging_bucket_cmd)
diff --git a/docs/plugins/ingest.asciidoc b/docs/plugins/ingest.asciidoc
deleted file mode 100644
index e41f9bb..0000000
--- a/docs/plugins/ingest.asciidoc
+++ /dev/null
@@ -1,911 +0,0 @@
-[[ingest]]
-== Ingest Plugin
-
-The ingest plugin can be used to pre-process documents before the actual indexing takes place.
-This pre-processing happens by the ingest plugin that intercepts bulk and index requests, applies the
-transformations and then passes the documents back to the index or bulk APIs.
-
-The ingest plugin is disabled by default. In order to enable the ingest plugin the following
-setting should be configured in the elasticsearch.yml file:
-
-[source,yaml]
---------------------------------------------------
-node.ingest: true
---------------------------------------------------
-
-The ingest plugin can be installed and enabled on any node. It is possible to run ingest
-on an master and or data node or have dedicated client nodes that run with ingest.
-
-In order to pre-process document before indexing the `pipeline` parameter should be used
-on an index or bulk request to tell the ingest plugin what pipeline is going to be used.
-
-[source,js]
---------------------------------------------------
-PUT /my-index/my-type/my-id?ingest=my_pipeline_id
-{
-  ...
-}
---------------------------------------------------
-// AUTOSENSE
-
-=== Processors
-
-==== Set processor
-Sets one field and associates it with the specified value. If the field already exists,
-its value will be replaced with the provided one.
-
-[source,js]
---------------------------------------------------
-{
-  "set": {
-    "field": "field1",
-    "value": 582.1
-  }
-}
---------------------------------------------------
-
-==== Append processor
-Appends one or more values to an existing array if the field already exists and it is an array.
-Converts a scalar to an array and appends one or more values to it if the field exists and it is a scalar.
-Creates an array containing the provided values if the fields doesn't exist.
-Accepts a single value or an array of values.
-
-[source,js]
---------------------------------------------------
-{
-  "append": {
-    "field": "field1"
-    "value": ["item2", "item3", "item4"]
-  }
-}
---------------------------------------------------
-
-==== Remove processor
-Removes an existing field. If the field doesn't exist, an exception will be thrown
-
-[source,js]
---------------------------------------------------
-{
-  "remove": {
-    "field": "foo"
-  }
-}
---------------------------------------------------
-
-==== Rename processor
-Renames an existing field. If the field doesn't exist, an exception will be thrown. Also, the new field
-name must not exist.
-
-[source,js]
---------------------------------------------------
-{
-  "rename": {
-    "field": "foo"
-  }
-}
---------------------------------------------------
-
-
-==== Convert processor
-Converts an existing field's value to a different type, like turning a string to an integer.
-If the field value is an array, all members will be converted.
-
-The supported types include: `integer`, `float`, `string`, and `boolean`.
-
-`boolean` will set the field to true if its string value is equal to `true` (ignore case), to
-false if its string value is equal to `false` (ignore case) and it will throw exception otherwise.
-
-[source,js]
---------------------------------------------------
-{
-  "convert": {
-    "field" : "foo"
-    "type": "integer"
-  }
-}
---------------------------------------------------
-
-==== Gsub processor
-Converts a string field by applying a regular expression and a replacement.
-If the field is not a string, the processor will throw an exception.
-
-This configuration takes a `field` for the field name, `pattern` for the
-pattern to be replaced, and `replacement` for the string to replace the matching patterns with.
-
-
-[source,js]
---------------------------------------------------
-{
-  "gsub": {
-    "field": "field1",
-    "pattern": "\.",
-    "replacement": "-"
-  }
-}
---------------------------------------------------
-
-==== Join processor
-Joins each element of an array into a single string using a separator character between each element.
-Throws error when the field is not an array.
-
-[source,js]
---------------------------------------------------
-{
-  "join": {
-    "field": "joined_array_field",
-    "separator": "-"
-  }
-}
---------------------------------------------------
-
-==== Split processor
-Split a field to an array using a separator character. Only works on string fields.
-
-[source,js]
---------------------------------------------------
-{
-  "split": {
-    "field": ","
-  }
-}
---------------------------------------------------
-
-==== Lowercase processor
-Converts a string to its lowercase equivalent.
-
-[source,js]
---------------------------------------------------
-{
-  "lowercase": {
-    "field": "foo"
-  }
-}
---------------------------------------------------
-
-==== Uppercase processor
-Converts a string to its uppercase equivalent.
-
-[source,js]
---------------------------------------------------
-{
-  "uppercase": {
-    "field": "foo"
-  }
-}
---------------------------------------------------
-
-==== Trim processor
-Trims whitespace from field. NOTE: this only works on leading and trailing whitespaces.
-
-[source,js]
---------------------------------------------------
-{
-  "trim": {
-    "field": "foo"
-  }
-}
---------------------------------------------------
-
-==== Grok Processor
-
-The Grok Processor extracts structured fields out of a single text field within a document. You choose which field to
-extract matched fields from, as well as the Grok Pattern you expect will match. A Grok Pattern is like a regular
-expression that supports aliased expressions that can be reused.
-
-This tool is perfect for syslog logs, apache and other webserver logs, mysql logs, and in general, any log format
-that is generally written for humans and not computer consumption.
-
-The processor comes packaged with over 120 reusable patterns that are located at `$ES_HOME/config/ingest/grok/patterns`.
-Here, you can add your own custom grok pattern files with custom grok expressions to be used by the processor.
-
-If you need help building patterns to match your logs, you will find the <http://grokdebug.herokuapp.com> and
-<http://grokconstructor.appspot.com/> applications quite useful!
-
-===== Grok Basics
-
-Grok sits on top of regular expressions, so any regular expressions are valid in grok as well.
-The regular expression library is Oniguruma, and you can see the full supported regexp syntax
-https://github.com/kkos/oniguruma/blob/master/doc/RE[on the Onigiruma site].
-
-Grok works by leveraging this regular expression language to allow naming existing patterns and combining them into more
-complex patterns that match your fields.
-
-The syntax for re-using a grok pattern comes in three forms: `%{SYNTAX:SEMANTIC}`, `%{SYNTAX}`, `%{SYNTAX:SEMANTIC:TYPE}`.
-
-The `SYNTAX` is the name of the pattern that will match your text. For example, `3.44` will be matched by the `NUMBER`
-pattern and `55.3.244.1` will be matched by the `IP` pattern. The syntax is how you match. `NUMBER` and `IP` are both
-patterns that are provided within the default patterns set.
-
-The `SEMANTIC` is the identifier you give to the piece of text being matched. For example, `3.44` could be the
-duration of an event, so you could call it simply `duration`. Further, a string `55.3.244.1` might identify
-the `client` making a request.
-
-The `TYPE` is the type you wish to cast your named field. `int` and `float` are currently the only types supported for coercion.
-
-For example, here is a grok pattern that would match the above example given. We would like to match a text with the following
-contents:
-
-[source,js]
---------------------------------------------------
-3.44 55.3.244.1
---------------------------------------------------
-
-We may know that the above message is a number followed by an IP-address. We can match this text with the following
-Grok expression.
-
-[source,js]
---------------------------------------------------
-%{NUMBER:duration} %{IP:client}
---------------------------------------------------
-
-===== Custom Patterns and Pattern Files
-
-The Grok Processor comes pre-packaged with a base set of pattern files. These patterns may not always have
-what you are looking for. These pattern files have a very basic format. Each line describes a named pattern with
-the following format:
-
-[source,js]
---------------------------------------------------
-NAME ' '+ PATTERN '\n'
---------------------------------------------------
-
-You can add this pattern to an existing file, or add your own file in the patterns directory here: `$ES_HOME/config/ingest/grok/patterns`.
-The Ingest Plugin will pick up files in this directory to be loaded into the grok processor's known patterns. These patterns are loaded
-at startup, so you will need to do a restart your ingest node if you wish to update these files while running.
-
-Example snippet of pattern definitions found in the `grok-patterns` patterns file:
-
-[source,js]
---------------------------------------------------
-YEAR (?>\d\d){1,2}
-HOUR (?:2[0123]|[01]?[0-9])
-MINUTE (?:[0-5][0-9])
-SECOND (?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)
-TIME (?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])
---------------------------------------------------
-
-===== Using Grok Processor in a Pipeline
-
-[[grok-options]]
-.Grok Options
-[options="header"]
-|======
-| Name                   | Required  | Default             | Description
-| `match_field`          | yes       | -                   | The field to use for grok expression parsing
-| `match_pattern`        | yes       | -                   | The grok expression to match and extract named captures with
-| `pattern_definitions`  | no        | -                   | A map of pattern-name and pattern tuples defining custom patterns to be used by the current processor. Patterns matching existing names will override the pre-existing definition.
-|======
-
-Here is an example of using the provided patterns to extract out and name structured fields from a string field in
-a document.
-
-[source,js]
---------------------------------------------------
-{
-  "message": "55.3.244.1 GET /index.html 15824 0.043"
-}
---------------------------------------------------
-
-The pattern for this could be
-
-[source]
---------------------------------------------------
-%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}
---------------------------------------------------
-
-An example pipeline for processing the above document using Grok:
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors": [
-    {
-      "grok": {
-        "match_field": "message",
-        "match_pattern": "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}"
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-This pipeline will insert these named captures as new fields within the document, like so:
-
-[source,js]
---------------------------------------------------
-{
-  "message": "55.3.244.1 GET /index.html 15824 0.043",
-  "client": "55.3.244.1",
-  "method": "GET",
-  "request": "/index.html",
-  "bytes": 15824,
-  "duration": "0.043"
-}
---------------------------------------------------
-
-An example of a pipeline specifying custom pattern definitions:
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors": [
-    {
-      "grok": {
-        "match_field": "message",
-        "match_pattern": "my %{FAVORITE_DOG:dog} is colored %{RGB:color}"
-        "pattern_definitions" : {
-          "FAVORITE_DOG" : "beagle",
-          "RGB" : "RED|GREEN|BLUE"
-        }
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-
-==== Geoip processor
-
-The GeoIP processor adds information about the geographical location of IP addresses, based on data from the Maxmind databases.
-This processor adds this information by default under the `geoip` field.
-
-The ingest plugin ships by default with the GeoLite2 City and GeoLite2 Country geoip2 databases from Maxmind made available
-under the CCA-ShareAlike 3.0 license. For more details see, http://dev.maxmind.com/geoip/geoip2/geolite2/
-
-The GeoIP processor can run with other geoip2 databases from Maxmind. The files must be copied into the geoip config directory
-and the `database_file` option should be used to specify the filename of the custom database. The geoip config directory
-is located at `$ES_HOME/config/ingest/geoip` and holds the shipped databases too.
-
-[[geoip-options]]
-.Geoip options
-[options="header"]
-|======
-| Name                   | Required  | Default                                                                            | Description
-| `source_field`         | yes       | -                                                                                  | The field to get the ip address or hostname from for the geographical lookup.
-| `target_field`         | no        | geoip                                                                              | The field that will hold the geographical information looked up from the Maxmind database.
-| `database_file`        | no        | GeoLite2-City.mmdb                                                                 | The database filename in the geoip config directory. The ingest plugin ships with the GeoLite2-City.mmdb and GeoLite2-Country.mmdb files.
-| `fields`               | no        | [`continent_name`, `country_iso_code`, `region_name`, `city_name`, `location`] <1> | Controls what properties are added to the `target_field` based on the geoip lookup.
-|======
-
-<1> Depends on what is available in `database_field`:
-* If the GeoLite2 City database is used then the following fields may be added under the `target_field`: `ip`,
-`country_iso_code`, `country_name`, `continent_name`, `region_name`, `city_name`, `timezone`, `latitude`, `longitude`
-and `location`. The fields actually added depend on what has been found and which fields were configured in `fields`.
-* If the GeoLite2 Country database is used then the following fields may be added under the `target_field`: `ip`,
-`country_iso_code`, `country_name` and `continent_name`.The fields actually added depend on what has been found and which fields were configured in `fields`.
-
-An example that uses the default city database and adds the geographical information to the `geoip` field based on the `ip` field:
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors" : [
-    {
-      "geoip" : {
-        "source_field" : "ip"
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-An example that uses the default country database and add the geographical information to the `geo` field based on the `ip` field`:
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors" : [
-    {
-      "geoip" : {
-        "source_field" : "ip",
-        "target_field" : "geo",
-        "database_file" : "GeoLite2-Country.mmdb"
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-==== Date processor
-
-The date processor is used for parsing dates from fields, and then using that date or timestamp as the timestamp for that document.
-The date processor adds by default the parsed date as a new field called `@timestamp`, configurable by setting the `target_field`
-configuration parameter. Multiple date formats are supported as part of the same date processor definition. They will be used
-sequentially to attempt parsing the date field, in the same order they were defined as part of the processor definition.
-
-[[date-options]]
-.Date options
-[options="header"]
-|======
-| Name                   | Required  | Default             | Description
-| `match_field`          | yes       | -                   | The field to get the date from.
-| `target_field`         | no        | @timestamp          | The field that will hold the parsed date.
-| `match_formats`        | yes       | -                   | Array of the expected date formats. Can be a joda pattern or one of the following formats: ISO8601, UNIX, UNIX_MS, TAI64N.
-| `timezone`             | no        | UTC                 | The timezone to use when parsing the date.
-| `locale`               | no        | ENGLISH             | The locale to use when parsing the date, relevant when parsing month names or week days.
-|======
-
-An example that adds the parsed date to the `timestamp` field based on the `initial_date` field:
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors" : [
-    {
-      "date" : {
-        "match_field" : "initial_date",
-        "target_field" : "timestamp",
-        "match_formats" : ["dd/MM/yyyy hh:mm:ss"],
-        "timezone" : "Europe/Amsterdam"
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-==== Fail processor
-The Fail Processor is used to raise an exception. This is useful for when
-a user expects a pipeline to fail and wishes to relay a specific message
-to the requester.
-
-[source,js]
---------------------------------------------------
-{
-  "fail": {
-    "message": "an error message"
-  }
-}
---------------------------------------------------
-
-=== Accessing data in pipelines
-
-Processors in pipelines have read and write access to documents that pass through the pipeline.
-The fields in the source of a document and its metadata fields are accessible.
-
-Accessing a field in the source is straightforward and one can refer to fields by
-their name. For example:
-
-[source,js]
---------------------------------------------------
-{
-  "set": {
-    "field": "my_field"
-    "value": 582.1
-  }
-}
---------------------------------------------------
-
-On top of this fields from the source are always accessible via the `_source` prefix:
-
-[source,js]
---------------------------------------------------
-{
-  "set": {
-    "field": "_source.my_field"
-    "value": 582.1
-  }
-}
---------------------------------------------------
-
-Metadata fields can also be accessed in the same way as fields from the source. This
-is possible because Elasticsearch doesn't allow fields in the source that have the
-same name as metadata fields.
-
-The following example sets the id of a document to `1`:
-
-[source,js]
---------------------------------------------------
-{
-  "set": {
-    "field": "_id"
-    "value": "1"
-  }
-}
---------------------------------------------------
-
-The following metadata fields are accessible by a processor: `_index`, `_type`, `_id`, `_routing`, `_parent`,
-`_timestamp` and `_ttl`.
-
-Beyond metadata fields and source fields, the ingest plugin also adds ingest metadata to documents being processed.
-These metadata properties are accessible under the `_ingest` key. Currently the ingest plugin adds the ingest timestamp
-under `_ingest.timestamp` key to the ingest metadata, which is the time the ingest plugin received the index or bulk
-request to pre-process. But any processor is free to add more ingest related metadata to it. Ingest metadata is transient
-and is lost after a document has been processed by the pipeline and thus ingest metadata won't be indexed.
-
-The following example adds a field with the name `received` and the value is the ingest timestamp:
-
-[source,js]
---------------------------------------------------
-{
-  "set": {
-    "field": "received"
-    "value": "{{_ingest.timestamp}}"
-  }
-}
---------------------------------------------------
-
-As opposed to Elasticsearch metadata fields, the ingest metadata field name _ingest can be used as a valid field name
-in the source of a document. Use _source._ingest to refer to it, otherwise _ingest will be interpreted as ingest
-metadata fields by the ingest plugin.
-
-A number of processor settings also support templating. Settings that support templating can have zero or more
-template snippets. A template snippet begins with `{{` and ends with `}}`.
-Accessing fields and metafields in templates is exactly the same as via regular processor field settings.
-
-In this example a field by the name `field_c` is added and its value is a concatenation of
-the values of `field_a` and `field_b`.
-
-[source,js]
---------------------------------------------------
-{
-  "set": {
-    "field": "field_c"
-    "value": "{{field_a}} {{field_b}}"
-  }
-}
---------------------------------------------------
-
-The following example changes the index a document is going to be indexed into. The index a document will be redirected
-to depends on the field in the source with name `geoip.country_iso_code`.
-
-[source,js]
---------------------------------------------------
-{
-  "set": {
-    "field": "_index"
-    "value": "{{geoip.country_iso_code}}"
-  }
-}
---------------------------------------------------
-
-=== Ingest APIs
-
-==== Put pipeline API
-
-The put pipeline api adds pipelines and updates existing pipelines in the cluster.
-
-[source,js]
---------------------------------------------------
-PUT _ingest/pipeline/my-pipeline-id
-{
-  "description" : "describe pipeline",
-  "processors" : [
-    {
-      "simple" : {
-        // settings
-      }
-    },
-    // other processors
-  ]
-}
---------------------------------------------------
-// AUTOSENSE
-
-NOTE: The put pipeline api also instructs all ingest nodes to reload their in-memory representation of pipelines, so that
-      pipeline changes take immediately in effect.
-
-==== Get pipeline API
-
-The get pipeline api returns pipelines based on id. This api always returns a local reference of the pipeline.
-
-[source,js]
---------------------------------------------------
-GET _ingest/pipeline/my-pipeline-id
---------------------------------------------------
-// AUTOSENSE
-
-Example response:
-
-[source,js]
---------------------------------------------------
-{
-   "my-pipeline-id": {
-      "_source" : {
-        "description": "describe pipeline",
-        "processors": [
-          {
-            "simple" : {
-              // settings
-            }
-          },
-          // other processors
-        ]
-      },
-      "_version" : 0
-   }
-}
---------------------------------------------------
-
-For each returned pipeline the source and the version is returned.
-The version is useful for knowing what version of the pipeline the node has.
-Multiple ids can be provided at the same time. Also wildcards are supported.
-
-==== Delete pipeline API
-
-The delete pipeline api deletes pipelines by id.
-
-[source,js]
---------------------------------------------------
-DELETE _ingest/pipeline/my-pipeline-id
---------------------------------------------------
-// AUTOSENSE
-
-==== Simulate pipeline API
-
-The simulate pipeline api executes a specific pipeline against
-the set of documents provided in the body of the request.
-
-A simulate request may call upon an existing pipeline to be executed
-against the provided documents, or supply a pipeline definition in
-the body of the request.
-
-Here is the structure of a simulate request with a provided pipeline:
-
-[source,js]
---------------------------------------------------
-POST _ingest/pipeline/_simulate
-{
-  "pipeline" : {
-    // pipeline definition here
-  },
-  "docs" : [
-    { /** first document **/ },
-    { /** second document **/ },
-    // ...
-  ]
-}
---------------------------------------------------
-
-Here is the structure of a simulate request against a pre-existing pipeline:
-
-[source,js]
---------------------------------------------------
-POST _ingest/pipeline/my-pipeline-id/_simulate
-{
-  "docs" : [
-    { /** first document **/ },
-    { /** second document **/ },
-    // ...
-  ]
-}
---------------------------------------------------
-
-
-Here is an example simulate request with a provided pipeline and its response:
-
-[source,js]
---------------------------------------------------
-POST _ingest/pipeline/_simulate
-{
-  "pipeline" :
-  {
-    "description": "_description",
-    "processors": [
-      {
-        "set" : {
-          "field" : "field2",
-          "value" : "_value"
-        }
-      }
-    ]
-  },
-  "docs": [
-    {
-      "_index": "index",
-      "_type": "type",
-      "_id": "id",
-      "_source": {
-        "foo": "bar"
-      }
-    },
-    {
-      "_index": "index",
-      "_type": "type",
-      "_id": "id",
-      "_source": {
-        "foo": "rab"
-      }
-    }
-  ]
-}
---------------------------------------------------
-// AUTOSENSE
-
-response:
-
-[source,js]
---------------------------------------------------
-{
-   "docs": [
-      {
-         "doc": {
-            "_id": "id",
-            "_ttl": null,
-            "_parent": null,
-            "_index": "index",
-            "_routing": null,
-            "_type": "type",
-            "_timestamp": null,
-            "_source": {
-               "field2": "_value",
-               "foo": "bar"
-            },
-            "_ingest": {
-               "timestamp": "2016-01-04T23:53:27.186+0000"
-            }
-         }
-      },
-      {
-         "doc": {
-            "_id": "id",
-            "_ttl": null,
-            "_parent": null,
-            "_index": "index",
-            "_routing": null,
-            "_type": "type",
-            "_timestamp": null,
-            "_source": {
-               "field2": "_value",
-               "foo": "rab"
-            },
-            "_ingest": {
-               "timestamp": "2016-01-04T23:53:27.186+0000"
-            }
-         }
-      }
-   ]
-}
---------------------------------------------------
-
-It is often useful to see how each processor affects the ingest document
-as it is passed through the pipeline. To see the intermediate results of
-each processor in the simulat request, a `verbose` parameter may be added
-to the request
-
-Here is an example verbose request and its response:
-
-
-[source,js]
---------------------------------------------------
-POST _ingest/pipeline/_simulate?verbose
-{
-  "pipeline" :
-  {
-    "description": "_description",
-    "processors": [
-      {
-        "set" : {
-          "field" : "field2",
-          "value" : "_value2"
-        }
-      },
-      {
-        "set" : {
-          "field" : "field3",
-          "value" : "_value3"
-        }
-      }
-    ]
-  },
-  "docs": [
-    {
-      "_index": "index",
-      "_type": "type",
-      "_id": "id",
-      "_source": {
-        "foo": "bar"
-      }
-    },
-    {
-      "_index": "index",
-      "_type": "type",
-      "_id": "id",
-      "_source": {
-        "foo": "rab"
-      }
-    }
-  ]
-}
---------------------------------------------------
-// AUTOSENSE
-
-response:
-
-[source,js]
---------------------------------------------------
-{
-   "docs": [
-      {
-         "processor_results": [
-            {
-               "processor_id": "processor[set]-0",
-               "doc": {
-                  "_id": "id",
-                  "_ttl": null,
-                  "_parent": null,
-                  "_index": "index",
-                  "_routing": null,
-                  "_type": "type",
-                  "_timestamp": null,
-                  "_source": {
-                     "field2": "_value2",
-                     "foo": "bar"
-                  },
-                  "_ingest": {
-                     "timestamp": "2016-01-05T00:02:51.383+0000"
-                  }
-               }
-            },
-            {
-               "processor_id": "processor[set]-1",
-               "doc": {
-                  "_id": "id",
-                  "_ttl": null,
-                  "_parent": null,
-                  "_index": "index",
-                  "_routing": null,
-                  "_type": "type",
-                  "_timestamp": null,
-                  "_source": {
-                     "field3": "_value3",
-                     "field2": "_value2",
-                     "foo": "bar"
-                  },
-                  "_ingest": {
-                     "timestamp": "2016-01-05T00:02:51.383+0000"
-                  }
-               }
-            }
-         ]
-      },
-      {
-         "processor_results": [
-            {
-               "processor_id": "processor[set]-0",
-               "doc": {
-                  "_id": "id",
-                  "_ttl": null,
-                  "_parent": null,
-                  "_index": "index",
-                  "_routing": null,
-                  "_type": "type",
-                  "_timestamp": null,
-                  "_source": {
-                     "field2": "_value2",
-                     "foo": "rab"
-                  },
-                  "_ingest": {
-                     "timestamp": "2016-01-05T00:02:51.384+0000"
-                  }
-               }
-            },
-            {
-               "processor_id": "processor[set]-1",
-               "doc": {
-                  "_id": "id",
-                  "_ttl": null,
-                  "_parent": null,
-                  "_index": "index",
-                  "_routing": null,
-                  "_type": "type",
-                  "_timestamp": null,
-                  "_source": {
-                     "field3": "_value3",
-                     "field2": "_value2",
-                     "foo": "rab"
-                  },
-                  "_ingest": {
-                     "timestamp": "2016-01-05T00:02:51.384+0000"
-                  }
-               }
-            }
-         ]
-      }
-   ]
-}
---------------------------------------------------
diff --git a/docs/reference/indices/aliases.asciidoc b/docs/reference/indices/aliases.asciidoc
index 78e871d..cb8f652 100644
--- a/docs/reference/indices/aliases.asciidoc
+++ b/docs/reference/indices/aliases.asciidoc
@@ -340,7 +340,7 @@ Possible options:
     multiple alias names separated by a comma.
 
 `ignore_unavailable`::
-    What to do is an specified index name doesn't
+    What to do if an specified index name doesn't
     exist. If set to `true` then those indices are ignored.
 
 The rest endpoint is: `/{index}/_alias/{alias}`.
diff --git a/docs/reference/indices/delete-index.asciidoc b/docs/reference/indices/delete-index.asciidoc
index 25d1766..5c652ac 100644
--- a/docs/reference/indices/delete-index.asciidoc
+++ b/docs/reference/indices/delete-index.asciidoc
@@ -11,9 +11,8 @@ $ curl -XDELETE 'http://localhost:9200/twitter/'
 The above example deletes an index called `twitter`. Specifying an index,
 alias or wildcard expression is required.
 
-The delete index API can also be applied to more than one index, or on
-all indices (be careful!) by using `_all` or `*` as index.
+The delete index API can also be applied to more than one index, by either using a comma separated list, or on all indices (be careful!) by using `_all` or `*` as index.
 
 In order to disable allowing to delete indices via wildcards or `_all`,
 set `action.destructive_requires_name` setting in the config to `true`.
-This setting can also be changed via the cluster update settings api.
\ No newline at end of file
+This setting can also be changed via the cluster update settings api.
diff --git a/docs/reference/mapping/types/nested.asciidoc b/docs/reference/mapping/types/nested.asciidoc
index b4bb06e..e13b94c 100644
--- a/docs/reference/mapping/types/nested.asciidoc
+++ b/docs/reference/mapping/types/nested.asciidoc
@@ -55,7 +55,7 @@ GET my_index/_search
     "bool": {
       "must": [
         { "match": { "user.first": "Alice" }},
-        { "match": { "user.last":  "White" }}
+        { "match": { "user.last":  "Smith" }}
       ]
     }
   }
diff --git a/docs/reference/migration/migrate_2_0/crud.asciidoc b/docs/reference/migration/migrate_2_0/crud.asciidoc
index f79306a..ef3ba93 100644
--- a/docs/reference/migration/migrate_2_0/crud.asciidoc
+++ b/docs/reference/migration/migrate_2_0/crud.asciidoc
@@ -32,7 +32,7 @@ In addition, the following routing-related node settings have been deprecated:
 
 The delete API used to be broadcast to all shards in the index which meant
 that, when using custom routing, the `routing` parameter was optional. Now,
-the delete request is forwarded only to the document holding the shard. If you
+the delete request is forwarded only to the shard holding the document. If you
 are using custom routing then you should specify the `routing` value when
 deleting a document, just as is already required for the `index`, `create`,
 and `update` APIs.
diff --git a/docs/reference/modules/snapshots.asciidoc b/docs/reference/modules/snapshots.asciidoc
index dbb9f39..969a74f 100644
--- a/docs/reference/modules/snapshots.asciidoc
+++ b/docs/reference/modules/snapshots.asciidoc
@@ -160,7 +160,7 @@ shared file system repository.
 Other repository backends are available in these official plugins:
 
 * {plugins}/repository-s3.html[repository-s3] for S3 repository support
-* https://github.com/elasticsearch/elasticsearch-hadoop/tree/master/repository-hdfs[HDFS Plugin] for Hadoop environments
+* {plugins}/repository-hdfs.html[repository-hdfs] for HDFS repository support in Hadoop environments
 * {plugins}/repository-azure.html[repository-azure] for Azure storage repositories
 
 [float]
diff --git a/docs/reference/search.asciidoc b/docs/reference/search.asciidoc
index da7d2e5..a4de20e 100644
--- a/docs/reference/search.asciidoc
+++ b/docs/reference/search.asciidoc
@@ -73,6 +73,19 @@ the request with two different groups:
 }
 --------------------------------------------------
 
+[float]
+[[global-search-timeout]]
+== Global Search Timeout
+
+Individual searches can have a timeout as part of the
+<<search-request-body>>. Since search requests can originate from many
+sources, Elasticsearch has a dynamic cluster-level setting for a global
+search timeout that applies to all search requests that do not set a
+timeout in the <<search-request-body>>. The default value is no global
+timeout. The setting key is `search.default_search_timeout` and can be
+set using the <<cluster-update-settings>> endpoints. Setting this value
+to `-1` resets the global search timeout to no timeout.
+
 --
 
 include::search/search.asciidoc[]
diff --git a/docs/reference/search/request/from-size.asciidoc b/docs/reference/search/request/from-size.asciidoc
index d19b850..0804ff2 100644
--- a/docs/reference/search/request/from-size.asciidoc
+++ b/docs/reference/search/request/from-size.asciidoc
@@ -21,6 +21,5 @@ defaults to `10`.
 --------------------------------------------------
 
 Note that `from` + `size` can not be more than the `index.max_result_window`
-index setting which defaults to 10,000. See the
-{ref}/search-request-scroll.html[Scroll] api for more efficient ways to do deep
-scrolling.
+index setting which defaults to 10,000. See the <<search-request-scroll,Scroll>>
+API for more efficient ways to do deep scrolling.
diff --git a/docs/reference/search/request/inner-hits.asciidoc b/docs/reference/search/request/inner-hits.asciidoc
index a79071b..09d995b 100644
--- a/docs/reference/search/request/inner-hits.asciidoc
+++ b/docs/reference/search/request/inner-hits.asciidoc
@@ -11,7 +11,7 @@ it's very useful to know which inner nested objects (in the case of nested) or c
 of parent/child) caused certain information to be returned. The inner hits feature can be used for this. This feature
 returns per search hit in the search response additional nested hits that caused a search hit to match in a different scope.
 
-Inner hits can be used by defining a `inner_hits` definition on a `nested`, `has_child` or `has_parent` query and filter.
+Inner hits can be used by defining an `inner_hits` definition on a `nested`, `has_child` or `has_parent` query and filter.
 The structure looks like this:
 
 [source,js]
@@ -23,7 +23,7 @@ The structure looks like this:
 }
 --------------------------------------------------
 
-If `_inner_hits` is defined on a query that supports it then each search hit will contain a `inner_hits` json object with the following structure:
+If `_inner_hits` is defined on a query that supports it then each search hit will contain an `inner_hits` json object with the following structure:
 
 [source,js]
 --------------------------------------------------
@@ -234,7 +234,7 @@ An example of a response snippet that could be generated from the above search r
 Besides defining inner hits on query and filters, inner hits can also be defined as a top level construct alongside the
 `query` and `aggregations` definition. The main reason for using the top level inner hits definition is to let the
 inner hits return documents that don't match with the main query. Also inner hits definitions can be nested via the
-top level notation. Other then that the inner hit definition inside the query should be used, because that is the most
+top level notation. Other than that, the inner hit definition inside the query should be used because that is the most
 compact way for defining inner hits.
 
 The following snippet explains the basic structure of inner hits defined at the top level of the search request body:
@@ -254,7 +254,7 @@ The following snippet explains the basic structure of inner hits defined at the
 }
 --------------------------------------------------
 
-Inside the `inner_hits` definition, first the name if the inner hit is defined then whether the inner_hit
+Inside the `inner_hits` definition, first the name of the inner hit is defined then whether the inner_hit
 is a nested by defining `path` or a parent/child based definition by defining `type`. The next object layer contains
 the name of the nested object field if the inner_hits is nested or the parent or child type if the inner_hit definition
 is parent/child based.
diff --git a/modules/ingest-grok/build.gradle b/modules/ingest-grok/build.gradle
deleted file mode 100644
index 2672234..0000000
--- a/modules/ingest-grok/build.gradle
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-esplugin {
-    description 'Ingest processor that uses grok patterns to split text'
-    classname 'org.elasticsearch.ingest.grok.IngestGrokPlugin'
-}
-
-dependencies {
-    compile 'org.jruby.joni:joni:2.1.6'
-    // joni dependencies:
-    compile 'org.jruby.jcodings:jcodings:1.0.12'
-}
-
-compileJava.options.compilerArgs << "-Xlint:-rawtypes,-unchecked,-serial"
-compileTestJava.options.compilerArgs << "-Xlint:-rawtypes,-unchecked"
-
-thirdPartyAudit.excludes = [
-        // joni has AsmCompilerSupport, but that isn't being used:
-        'org.objectweb.asm.ClassWriter',
-        'org.objectweb.asm.MethodVisitor',
-        'org.objectweb.asm.Opcodes',
-]
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/jcodings-1.0.12.jar.sha1 b/modules/ingest-grok/licenses/jcodings-1.0.12.jar.sha1
deleted file mode 100644
index b097e32..0000000
--- a/modules/ingest-grok/licenses/jcodings-1.0.12.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-6bc17079fcaa8823ea8cd0d4c66516335b558db8
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/jcodings-LICENSE.txt b/modules/ingest-grok/licenses/jcodings-LICENSE.txt
deleted file mode 100644
index a3fdf73..0000000
--- a/modules/ingest-grok/licenses/jcodings-LICENSE.txt
+++ /dev/null
@@ -1,17 +0,0 @@
-Permission is hereby granted, free of charge, to any person obtaining a copy of
-this software and associated documentation files (the "Software"), to deal in
-the Software without restriction, including without limitation the rights to
-use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
-of the Software, and to permit persons to whom the Software is furnished to do
-so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/jcodings-NOTICE.txt b/modules/ingest-grok/licenses/jcodings-NOTICE.txt
deleted file mode 100644
index f6c4948..0000000
--- a/modules/ingest-grok/licenses/jcodings-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
-JCodings is released under the MIT License.
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/joni-2.1.6.jar.sha1 b/modules/ingest-grok/licenses/joni-2.1.6.jar.sha1
deleted file mode 100644
index 48abe13..0000000
--- a/modules/ingest-grok/licenses/joni-2.1.6.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-0f23c95a06eaecbc8c74c7458a8bfd13e4fd2d3a
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/joni-LICENSE.txt b/modules/ingest-grok/licenses/joni-LICENSE.txt
deleted file mode 100644
index a3fdf73..0000000
--- a/modules/ingest-grok/licenses/joni-LICENSE.txt
+++ /dev/null
@@ -1,17 +0,0 @@
-Permission is hereby granted, free of charge, to any person obtaining a copy of
-this software and associated documentation files (the "Software"), to deal in
-the Software without restriction, including without limitation the rights to
-use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
-of the Software, and to permit persons to whom the Software is furnished to do
-so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/joni-NOTICE.txt b/modules/ingest-grok/licenses/joni-NOTICE.txt
deleted file mode 100644
index 45bc517..0000000
--- a/modules/ingest-grok/licenses/joni-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
-Joni is released under the MIT License.
diff --git a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/Grok.java b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/Grok.java
deleted file mode 100644
index 228a2cb..0000000
--- a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/Grok.java
+++ /dev/null
@@ -1,158 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.grok;
-
-import org.jcodings.specific.UTF8Encoding;
-import org.joni.Matcher;
-import org.joni.NameEntry;
-import org.joni.Option;
-import org.joni.Regex;
-import org.joni.Region;
-import org.joni.Syntax;
-import org.joni.exception.ValueException;
-
-import java.nio.charset.StandardCharsets;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Locale;
-import java.util.Map;
-
-final class Grok {
-
-    private static final String NAME_GROUP = "name";
-    private static final String SUBNAME_GROUP = "subname";
-    private static final String PATTERN_GROUP = "pattern";
-    private static final String DEFINITION_GROUP = "definition";
-    private static final String GROK_PATTERN =
-            "%\\{" +
-            "(?<name>" +
-            "(?<pattern>[A-z0-9]+)" +
-            "(?::(?<subname>[A-z0-9_:.-]+))?" +
-            ")" +
-            "(?:=(?<definition>" +
-            "(?:" +
-            "(?:[^{}]+|\\.+)+" +
-            ")+" +
-            ")" +
-            ")?" + "\\}";
-    private static final Regex GROK_PATTERN_REGEX = new Regex(GROK_PATTERN.getBytes(StandardCharsets.UTF_8), 0, GROK_PATTERN.getBytes(StandardCharsets.UTF_8).length, Option.NONE, UTF8Encoding.INSTANCE, Syntax.DEFAULT);
-    private final Map<String, String> patternBank;
-    private final boolean namedCaptures;
-    private final Regex compiledExpression;
-    private final String expression;
-
-
-    public Grok(Map<String, String> patternBank, String grokPattern) {
-        this(patternBank, grokPattern, true);
-    }
-
-    @SuppressWarnings("unchecked")
-    public Grok(Map<String, String> patternBank, String grokPattern, boolean namedCaptures) {
-        this.patternBank = patternBank;
-        this.namedCaptures = namedCaptures;
-
-        this.expression = toRegex(grokPattern);
-        byte[] expressionBytes = expression.getBytes(StandardCharsets.UTF_8);
-        this.compiledExpression = new Regex(expressionBytes, 0, expressionBytes.length, Option.DEFAULT, UTF8Encoding.INSTANCE);
-    }
-
-
-    public String groupMatch(String name, Region region, String pattern) {
-        try {
-            int number = GROK_PATTERN_REGEX.nameToBackrefNumber(name.getBytes(StandardCharsets.UTF_8), 0, name.getBytes(StandardCharsets.UTF_8).length, region);
-            int begin = region.beg[number];
-            int end = region.end[number];
-            return new String(pattern.getBytes(StandardCharsets.UTF_8), begin, end - begin, StandardCharsets.UTF_8);
-        } catch (StringIndexOutOfBoundsException e) {
-            return null;
-        } catch (ValueException e) {
-            return null;
-        }
-    }
-
-    /**
-     * converts a grok expression into a named regex expression
-     *
-     * @return named regex expression
-     */
-    public String toRegex(String grokPattern) {
-        byte[] grokPatternBytes = grokPattern.getBytes(StandardCharsets.UTF_8);
-        Matcher matcher = GROK_PATTERN_REGEX.matcher(grokPatternBytes);
-
-        int result = matcher.search(0, grokPatternBytes.length, Option.NONE);
-        if (result != -1) {
-            Region region = matcher.getEagerRegion();
-            String namedPatternRef = groupMatch(NAME_GROUP, region, grokPattern);
-            String subName = groupMatch(SUBNAME_GROUP, region, grokPattern);
-            // TODO(tal): Support definitions
-            String definition = groupMatch(DEFINITION_GROUP, region, grokPattern);
-            String patternName = groupMatch(PATTERN_GROUP, region, grokPattern);
-            String pattern = patternBank.get(patternName);
-
-            String grokPart;
-            if (namedCaptures && subName != null) {
-                grokPart = String.format(Locale.US, "(?<%s>%s)", namedPatternRef, pattern);
-            } else if (!namedCaptures) {
-                grokPart = String.format(Locale.US, "(?<%s>%s)", patternName + "_" + String.valueOf(result), pattern);
-            } else {
-                grokPart = String.format(Locale.US, "(?:%s)", pattern);
-            }
-
-            String start = new String(grokPatternBytes, 0, result, StandardCharsets.UTF_8);
-            String rest = new String(grokPatternBytes, region.end[0], grokPatternBytes.length - region.end[0], StandardCharsets.UTF_8);
-            return start + toRegex(grokPart + rest);
-        }
-
-        return grokPattern;
-    }
-
-    public boolean match(String text) {
-        Matcher matcher = compiledExpression.matcher(text.getBytes(StandardCharsets.UTF_8));
-        int result = matcher.search(0, text.length(), Option.DEFAULT);
-        return (result != -1);
-    }
-
-    public Map<String, Object> captures(String text) {
-        byte[] textAsBytes = text.getBytes(StandardCharsets.UTF_8);
-        Map<String, Object> fields = new HashMap<>();
-        Matcher matcher = compiledExpression.matcher(textAsBytes);
-        int result = matcher.search(0, textAsBytes.length, Option.DEFAULT);
-        if (result != -1) {
-            Region region = matcher.getEagerRegion();
-            for (Iterator<NameEntry> entry = compiledExpression.namedBackrefIterator(); entry.hasNext();) {
-                NameEntry e = entry.next();
-                int number = e.getBackRefs()[0];
-
-                String groupName = new String(e.name, e.nameP, e.nameEnd - e.nameP, StandardCharsets.UTF_8);
-                String matchValue = null;
-                if (region.beg[number] >= 0) {
-                    matchValue = new String(textAsBytes, region.beg[number], region.end[number] - region.beg[number], StandardCharsets.UTF_8);
-                }
-                GrokMatchGroup match = new GrokMatchGroup(groupName, matchValue);
-                fields.put(match.getName(), match.getValue());
-            }
-        } else {
-            return null;
-        }
-
-        return fields;
-    }
-}
-
diff --git a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokMatchGroup.java b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokMatchGroup.java
deleted file mode 100644
index 2cebf62..0000000
--- a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokMatchGroup.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.grok;
-
-final class GrokMatchGroup {
-    private static final String DEFAULT_TYPE = "string";
-    private final String patternName;
-    private final String fieldName;
-    private final String type;
-    private final String groupValue;
-
-    public GrokMatchGroup(String groupName, String groupValue) {
-        String[] parts = groupName.split(":");
-        patternName = parts[0];
-        if (parts.length >= 2) {
-            fieldName = parts[1];
-        } else {
-            fieldName = null;
-        }
-
-        if (parts.length == 3) {
-            type = parts[2];
-        } else {
-            type = DEFAULT_TYPE;
-        }
-        this.groupValue = groupValue;
-    }
-
-    public String getName() {
-        return (fieldName == null) ? patternName : fieldName;
-    }
-
-    public Object getValue() {
-        if (groupValue == null) { return null; }
-
-        switch(type) {
-            case "int":
-                return Integer.parseInt(groupValue);
-            case "float":
-                return Float.parseFloat(groupValue);
-            default:
-                return groupValue;
-        }
-    }
-}
diff --git a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokProcessor.java b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokProcessor.java
deleted file mode 100644
index d38ea96..0000000
--- a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokProcessor.java
+++ /dev/null
@@ -1,121 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.grok;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.nio.charset.StandardCharsets;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-public final class GrokProcessor implements Processor {
-
-    public static final String TYPE = "grok";
-
-    private final String matchField;
-    private final Grok grok;
-
-    public GrokProcessor(Grok grok, String matchField) {
-        this.matchField = matchField;
-        this.grok = grok;
-    }
-
-    @Override
-    public void execute(IngestDocument ingestDocument) throws Exception {
-        String fieldValue = ingestDocument.getFieldValue(matchField, String.class);
-        Map<String, Object> matches = grok.captures(fieldValue);
-        if (matches != null) {
-            matches.forEach((k, v) -> ingestDocument.setFieldValue(k, v));
-        } else {
-            throw new IllegalArgumentException("Grok expression does not match field value: [" + fieldValue + "]");
-        }
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    String getMatchField() {
-        return matchField;
-    }
-
-    Grok getGrok() {
-        return grok;
-    }
-
-    public final static class Factory implements Processor.Factory<GrokProcessor> {
-
-        private final static String[] PATTERN_NAMES = new String[] {
-            "aws", "bacula", "bro", "exim", "firewalls", "grok-patterns", "haproxy",
-            "java", "junos", "linux-syslog", "mcollective-patterns", "mongodb", "nagios",
-            "postgresql", "rails", "redis", "ruby"
-        };
-        private final Map<String, String> builtinPatternBank;
-
-        public Factory() throws IOException {
-            Map<String, String> builtinPatterns = new HashMap<>();
-            for (String pattern : PATTERN_NAMES) {
-                try(InputStream is = getClass().getResourceAsStream("/patterns/" + pattern)) {
-                    loadBankFromStream(builtinPatterns, is);
-                }
-            }
-            this.builtinPatternBank = Collections.unmodifiableMap(builtinPatterns);
-        }
-
-        static void loadBankFromStream(Map<String, String> patternBank, InputStream inputStream) throws IOException {
-            String line;
-            BufferedReader br = new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8));
-            while ((line = br.readLine()) != null) {
-                String trimmedLine = line.replaceAll("^\\s+", "");
-                if (trimmedLine.startsWith("#") || trimmedLine.length() == 0) {
-                    continue;
-                }
-
-                String[] parts = trimmedLine.split("\\s+", 2);
-                if (parts.length == 2) {
-                    patternBank.put(parts[0], parts[1]);
-                }
-            }
-        }
-
-        public GrokProcessor create(Map<String, Object> config) throws Exception {
-            String matchField = ConfigurationUtils.readStringProperty(config, "field");
-            String matchPattern = ConfigurationUtils.readStringProperty(config, "pattern");
-            Map<String, String> customPatternBank = ConfigurationUtils.readOptionalMap(config, "pattern_definitions");
-            Map<String, String> patternBank = new HashMap<>(builtinPatternBank);
-            if (customPatternBank != null) {
-                patternBank.putAll(customPatternBank);
-            }
-
-            Grok grok = new Grok(patternBank, matchPattern);
-            return new GrokProcessor(grok, matchField);
-        }
-
-    }
-
-}
diff --git a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/IngestGrokPlugin.java b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/IngestGrokPlugin.java
deleted file mode 100644
index 2b61b5e..0000000
--- a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/IngestGrokPlugin.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.grok;
-
-import org.elasticsearch.ingest.IngestModule;
-import org.elasticsearch.plugins.Plugin;
-
-import java.io.IOException;
-
-public class IngestGrokPlugin extends Plugin {
-
-    @Override
-    public String name() {
-        return "ingest-grok";
-    }
-
-    @Override
-    public String description() {
-        return "Ingest processor that uses grok patterns to split text";
-    }
-
-    public void onModule(IngestModule ingestModule) {
-        ingestModule.registerProcessor(GrokProcessor.TYPE, (environment, templateService) -> {
-            try {
-                return new GrokProcessor.Factory();
-            } catch (IOException e) {
-                throw new RuntimeException(e);
-            }
-        });
-    }
-}
diff --git a/modules/ingest-grok/src/main/resources/patterns/aws b/modules/ingest-grok/src/main/resources/patterns/aws
deleted file mode 100644
index 71edbc9..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/aws
+++ /dev/null
@@ -1,11 +0,0 @@
-S3_REQUEST_LINE (?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})
-
-S3_ACCESS_LOG %{WORD:owner} %{NOTSPACE:bucket} \[%{HTTPDATE:timestamp}\] %{IP:clientip} %{NOTSPACE:requester} %{NOTSPACE:request_id} %{NOTSPACE:operation} %{NOTSPACE:key} (?:"%{S3_REQUEST_LINE}"|-) (?:%{INT:response:int}|-) (?:-|%{NOTSPACE:error_code}) (?:%{INT:bytes:int}|-) (?:%{INT:object_size:int}|-) (?:%{INT:request_time_ms:int}|-) (?:%{INT:turnaround_time_ms:int}|-) (?:%{QS:referrer}|-) (?:"?%{QS:agent}"?|-) (?:-|%{NOTSPACE:version_id})
-
-ELB_URIPATHPARAM %{URIPATH:path}(?:%{URIPARAM:params})?
-
-ELB_URI %{URIPROTO:proto}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST:urihost})?(?:%{ELB_URIPATHPARAM})?
-
-ELB_REQUEST_LINE (?:%{WORD:verb} %{ELB_URI:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})
-
-ELB_ACCESS_LOG %{TIMESTAMP_ISO8601:timestamp} %{NOTSPACE:elb} %{IP:clientip}:%{INT:clientport:int} (?:(%{IP:backendip}:?:%{INT:backendport:int})|-) %{NUMBER:request_processing_time:float} %{NUMBER:backend_processing_time:float} %{NUMBER:response_processing_time:float} %{INT:response:int} %{INT:backend_response:int} %{INT:received_bytes:int} %{INT:bytes:int} "%{ELB_REQUEST_LINE}"
diff --git a/modules/ingest-grok/src/main/resources/patterns/bacula b/modules/ingest-grok/src/main/resources/patterns/bacula
deleted file mode 100644
index d80dfe5..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/bacula
+++ /dev/null
@@ -1,50 +0,0 @@
-BACULA_TIMESTAMP %{MONTHDAY}-%{MONTH} %{HOUR}:%{MINUTE}
-BACULA_HOST [a-zA-Z0-9-]+
-BACULA_VOLUME %{USER}
-BACULA_DEVICE %{USER}
-BACULA_DEVICEPATH %{UNIXPATH}
-BACULA_CAPACITY %{INT}{1,3}(,%{INT}{3})*
-BACULA_VERSION %{USER}
-BACULA_JOB %{USER}
-
-BACULA_LOG_MAX_CAPACITY User defined maximum volume capacity %{BACULA_CAPACITY} exceeded on device \"%{BACULA_DEVICE:device}\" \(%{BACULA_DEVICEPATH}\)
-BACULA_LOG_END_VOLUME End of medium on Volume \"%{BACULA_VOLUME:volume}\" Bytes=%{BACULA_CAPACITY} Blocks=%{BACULA_CAPACITY} at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}.
-BACULA_LOG_NEW_VOLUME Created new Volume \"%{BACULA_VOLUME:volume}\" in catalog.
-BACULA_LOG_NEW_LABEL Labeled new Volume \"%{BACULA_VOLUME:volume}\" on device \"%{BACULA_DEVICE:device}\" \(%{BACULA_DEVICEPATH}\).
-BACULA_LOG_WROTE_LABEL Wrote label to prelabeled Volume \"%{BACULA_VOLUME:volume}\" on device \"%{BACULA_DEVICE}\" \(%{BACULA_DEVICEPATH}\)
-BACULA_LOG_NEW_MOUNT New volume \"%{BACULA_VOLUME:volume}\" mounted on device \"%{BACULA_DEVICE:device}\" \(%{BACULA_DEVICEPATH}\) at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}.
-BACULA_LOG_NOOPEN \s+Cannot open %{DATA}: ERR=%{GREEDYDATA:berror}
-BACULA_LOG_NOOPENDIR \s+Could not open directory %{DATA}: ERR=%{GREEDYDATA:berror}
-BACULA_LOG_NOSTAT \s+Could not stat %{DATA}: ERR=%{GREEDYDATA:berror}
-BACULA_LOG_NOJOBS There are no more Jobs associated with Volume \"%{BACULA_VOLUME:volume}\". Marking it purged.
-BACULA_LOG_ALL_RECORDS_PRUNED All records pruned from Volume \"%{BACULA_VOLUME:volume}\"; marking it \"Purged\"
-BACULA_LOG_BEGIN_PRUNE_JOBS Begin pruning Jobs older than %{INT} month %{INT} days .
-BACULA_LOG_BEGIN_PRUNE_FILES Begin pruning Files.
-BACULA_LOG_PRUNED_JOBS Pruned %{INT} Jobs* for client %{BACULA_HOST:client} from catalog.
-BACULA_LOG_PRUNED_FILES Pruned Files from %{INT} Jobs* for client %{BACULA_HOST:client} from catalog.
-BACULA_LOG_ENDPRUNE End auto prune.
-BACULA_LOG_STARTJOB Start Backup JobId %{INT}, Job=%{BACULA_JOB:job}
-BACULA_LOG_STARTRESTORE Start Restore Job %{BACULA_JOB:job}
-BACULA_LOG_USEDEVICE Using Device \"%{BACULA_DEVICE:device}\"
-BACULA_LOG_DIFF_FS \s+%{UNIXPATH} is a different filesystem. Will not descend from %{UNIXPATH} into it.
-BACULA_LOG_JOBEND Job write elapsed time = %{DATA:elapsed}, Transfer rate = %{NUMBER} (K|M|G)? Bytes/second
-BACULA_LOG_NOPRUNE_JOBS No Jobs found to prune.
-BACULA_LOG_NOPRUNE_FILES No Files found to prune.
-BACULA_LOG_VOLUME_PREVWRITTEN Volume \"%{BACULA_VOLUME:volume}\" previously written, moving to end of data.
-BACULA_LOG_READYAPPEND Ready to append to end of Volume \"%{BACULA_VOLUME:volume}\" size=%{INT}
-BACULA_LOG_CANCELLING Cancelling duplicate JobId=%{INT}.
-BACULA_LOG_MARKCANCEL JobId %{INT}, Job %{BACULA_JOB:job} marked to be canceled.
-BACULA_LOG_CLIENT_RBJ shell command: run ClientRunBeforeJob \"%{GREEDYDATA:runjob}\"
-BACULA_LOG_VSS (Generate )?VSS (Writer)?
-BACULA_LOG_MAXSTART Fatal error: Job canceled because max start delay time exceeded.
-BACULA_LOG_DUPLICATE Fatal error: JobId %{INT:duplicate} already running. Duplicate job not allowed.
-BACULA_LOG_NOJOBSTAT Fatal error: No Job status returned from FD.
-BACULA_LOG_FATAL_CONN Fatal error: bsock.c:133 Unable to connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})
-BACULA_LOG_NO_CONNECT Warning: bsock.c:127 Could not connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})
-BACULA_LOG_NO_AUTH Fatal error: Unable to authenticate with File daemon at %{HOSTNAME}. Possible causes:
-BACULA_LOG_NOSUIT No prior or suitable Full backup found in catalog. Doing FULL backup.
-BACULA_LOG_NOPRIOR No prior Full backup Job record found.
-
-BACULA_LOG_JOB (Error: )?Bacula %{BACULA_HOST} %{BACULA_VERSION} \(%{BACULA_VERSION}\):
-
-BACULA_LOGLINE %{BACULA_TIMESTAMP:bts} %{BACULA_HOST:hostname} JobId %{INT:jobid}: (%{BACULA_LOG_MAX_CAPACITY}|%{BACULA_LOG_END_VOLUME}|%{BACULA_LOG_NEW_VOLUME}|%{BACULA_LOG_NEW_LABEL}|%{BACULA_LOG_WROTE_LABEL}|%{BACULA_LOG_NEW_MOUNT}|%{BACULA_LOG_NOOPEN}|%{BACULA_LOG_NOOPENDIR}|%{BACULA_LOG_NOSTAT}|%{BACULA_LOG_NOJOBS}|%{BACULA_LOG_ALL_RECORDS_PRUNED}|%{BACULA_LOG_BEGIN_PRUNE_JOBS}|%{BACULA_LOG_BEGIN_PRUNE_FILES}|%{BACULA_LOG_PRUNED_JOBS}|%{BACULA_LOG_PRUNED_FILES}|%{BACULA_LOG_ENDPRUNE}|%{BACULA_LOG_STARTJOB}|%{BACULA_LOG_STARTRESTORE}|%{BACULA_LOG_USEDEVICE}|%{BACULA_LOG_DIFF_FS}|%{BACULA_LOG_JOBEND}|%{BACULA_LOG_NOPRUNE_JOBS}|%{BACULA_LOG_NOPRUNE_FILES}|%{BACULA_LOG_VOLUME_PREVWRITTEN}|%{BACULA_LOG_READYAPPEND}|%{BACULA_LOG_CANCELLING}|%{BACULA_LOG_MARKCANCEL}|%{BACULA_LOG_CLIENT_RBJ}|%{BACULA_LOG_VSS}|%{BACULA_LOG_MAXSTART}|%{BACULA_LOG_DUPLICATE}|%{BACULA_LOG_NOJOBSTAT}|%{BACULA_LOG_FATAL_CONN}|%{BACULA_LOG_NO_CONNECT}|%{BACULA_LOG_NO_AUTH}|%{BACULA_LOG_NOSUIT}|%{BACULA_LOG_JOB}|%{BACULA_LOG_NOPRIOR})
diff --git a/modules/ingest-grok/src/main/resources/patterns/bro b/modules/ingest-grok/src/main/resources/patterns/bro
deleted file mode 100644
index 31b138b..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/bro
+++ /dev/null
@@ -1,13 +0,0 @@
-# https://www.bro.org/sphinx/script-reference/log-files.html
-
-# http.log
-BRO_HTTP %{NUMBER:ts}\t%{NOTSPACE:uid}\t%{IP:orig_h}\t%{INT:orig_p}\t%{IP:resp_h}\t%{INT:resp_p}\t%{INT:trans_depth}\t%{GREEDYDATA:method}\t%{GREEDYDATA:domain}\t%{GREEDYDATA:uri}\t%{GREEDYDATA:referrer}\t%{GREEDYDATA:user_agent}\t%{NUMBER:request_body_len}\t%{NUMBER:response_body_len}\t%{GREEDYDATA:status_code}\t%{GREEDYDATA:status_msg}\t%{GREEDYDATA:info_code}\t%{GREEDYDATA:info_msg}\t%{GREEDYDATA:filename}\t%{GREEDYDATA:bro_tags}\t%{GREEDYDATA:username}\t%{GREEDYDATA:password}\t%{GREEDYDATA:proxied}\t%{GREEDYDATA:orig_fuids}\t%{GREEDYDATA:orig_mime_types}\t%{GREEDYDATA:resp_fuids}\t%{GREEDYDATA:resp_mime_types}
-
-# dns.log
-BRO_DNS %{NUMBER:ts}\t%{NOTSPACE:uid}\t%{IP:orig_h}\t%{INT:orig_p}\t%{IP:resp_h}\t%{INT:resp_p}\t%{WORD:proto}\t%{INT:trans_id}\t%{GREEDYDATA:query}\t%{GREEDYDATA:qclass}\t%{GREEDYDATA:qclass_name}\t%{GREEDYDATA:qtype}\t%{GREEDYDATA:qtype_name}\t%{GREEDYDATA:rcode}\t%{GREEDYDATA:rcode_name}\t%{GREEDYDATA:AA}\t%{GREEDYDATA:TC}\t%{GREEDYDATA:RD}\t%{GREEDYDATA:RA}\t%{GREEDYDATA:Z}\t%{GREEDYDATA:answers}\t%{GREEDYDATA:TTLs}\t%{GREEDYDATA:rejected}
-
-# conn.log
-BRO_CONN %{NUMBER:ts}\t%{NOTSPACE:uid}\t%{IP:orig_h}\t%{INT:orig_p}\t%{IP:resp_h}\t%{INT:resp_p}\t%{WORD:proto}\t%{GREEDYDATA:service}\t%{NUMBER:duration}\t%{NUMBER:orig_bytes}\t%{NUMBER:resp_bytes}\t%{GREEDYDATA:conn_state}\t%{GREEDYDATA:local_orig}\t%{GREEDYDATA:missed_bytes}\t%{GREEDYDATA:history}\t%{GREEDYDATA:orig_pkts}\t%{GREEDYDATA:orig_ip_bytes}\t%{GREEDYDATA:resp_pkts}\t%{GREEDYDATA:resp_ip_bytes}\t%{GREEDYDATA:tunnel_parents}
-
-# files.log
-BRO_FILES %{NUMBER:ts}\t%{NOTSPACE:fuid}\t%{IP:tx_hosts}\t%{IP:rx_hosts}\t%{NOTSPACE:conn_uids}\t%{GREEDYDATA:source}\t%{GREEDYDATA:depth}\t%{GREEDYDATA:analyzers}\t%{GREEDYDATA:mime_type}\t%{GREEDYDATA:filename}\t%{GREEDYDATA:duration}\t%{GREEDYDATA:local_orig}\t%{GREEDYDATA:is_orig}\t%{GREEDYDATA:seen_bytes}\t%{GREEDYDATA:total_bytes}\t%{GREEDYDATA:missing_bytes}\t%{GREEDYDATA:overflow_bytes}\t%{GREEDYDATA:timedout}\t%{GREEDYDATA:parent_fuid}\t%{GREEDYDATA:md5}\t%{GREEDYDATA:sha1}\t%{GREEDYDATA:sha256}\t%{GREEDYDATA:extracted}
diff --git a/modules/ingest-grok/src/main/resources/patterns/exim b/modules/ingest-grok/src/main/resources/patterns/exim
deleted file mode 100644
index 68c4e5c..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/exim
+++ /dev/null
@@ -1,13 +0,0 @@
-EXIM_MSGID [0-9A-Za-z]{6}-[0-9A-Za-z]{6}-[0-9A-Za-z]{2}
-EXIM_FLAGS (<=|[-=>*]>|[*]{2}|==)
-EXIM_DATE %{YEAR:exim_year}-%{MONTHNUM:exim_month}-%{MONTHDAY:exim_day} %{TIME:exim_time}
-EXIM_PID \[%{POSINT}\]
-EXIM_QT ((\d+y)?(\d+w)?(\d+d)?(\d+h)?(\d+m)?(\d+s)?)
-EXIM_EXCLUDE_TERMS (Message is frozen|(Start|End) queue run| Warning: | retry time not reached | no (IP address|host name) found for (IP address|host) | unexpected disconnection while reading SMTP command | no immediate delivery: |another process is handling this message)
-EXIM_REMOTE_HOST (H=(%{NOTSPACE:remote_hostname} )?(\(%{NOTSPACE:remote_heloname}\) )?\[%{IP:remote_host}\])
-EXIM_INTERFACE (I=\[%{IP:exim_interface}\](:%{NUMBER:exim_interface_port}))
-EXIM_PROTOCOL (P=%{NOTSPACE:protocol})
-EXIM_MSG_SIZE (S=%{NUMBER:exim_msg_size})
-EXIM_HEADER_ID (id=%{NOTSPACE:exim_header_id})
-EXIM_SUBJECT (T=%{QS:exim_subject})
-
diff --git a/modules/ingest-grok/src/main/resources/patterns/firewalls b/modules/ingest-grok/src/main/resources/patterns/firewalls
deleted file mode 100644
index 03c3e5a..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/firewalls
+++ /dev/null
@@ -1,86 +0,0 @@
-# NetScreen firewall logs
-NETSCREENSESSIONLOG %{SYSLOGTIMESTAMP:date} %{IPORHOST:device} %{IPORHOST}: NetScreen device_id=%{WORD:device_id}%{DATA}: start_time=%{QUOTEDSTRING:start_time} duration=%{INT:duration} policy_id=%{INT:policy_id} service=%{DATA:service} proto=%{INT:proto} src zone=%{WORD:src_zone} dst zone=%{WORD:dst_zone} action=%{WORD:action} sent=%{INT:sent} rcvd=%{INT:rcvd} src=%{IPORHOST:src_ip} dst=%{IPORHOST:dst_ip} src_port=%{INT:src_port} dst_port=%{INT:dst_port} src-xlated ip=%{IPORHOST:src_xlated_ip} port=%{INT:src_xlated_port} dst-xlated ip=%{IPORHOST:dst_xlated_ip} port=%{INT:dst_xlated_port} session_id=%{INT:session_id} reason=%{GREEDYDATA:reason}
-
-#== Cisco ASA ==
-CISCO_TAGGED_SYSLOG ^<%{POSINT:syslog_pri}>%{CISCOTIMESTAMP:timestamp}( %{SYSLOGHOST:sysloghost})? ?: %%{CISCOTAG:ciscotag}:
-CISCOTIMESTAMP %{MONTH} +%{MONTHDAY}(?: %{YEAR})? %{TIME}
-CISCOTAG [A-Z0-9]+-%{INT}-(?:[A-Z0-9_]+)
-# Common Particles
-CISCO_ACTION Built|Teardown|Deny|Denied|denied|requested|permitted|denied by ACL|discarded|est-allowed|Dropping|created|deleted
-CISCO_REASON Duplicate TCP SYN|Failed to locate egress interface|Invalid transport field|No matching connection|DNS Response|DNS Query|(?:%{WORD}\s*)*
-CISCO_DIRECTION Inbound|inbound|Outbound|outbound
-CISCO_INTERVAL first hit|%{INT}-second interval
-CISCO_XLATE_TYPE static|dynamic
-# ASA-1-104001
-CISCOFW104001 \((?:Primary|Secondary)\) Switching to ACTIVE - %{GREEDYDATA:switch_reason}
-# ASA-1-104002
-CISCOFW104002 \((?:Primary|Secondary)\) Switching to STANDBY - %{GREEDYDATA:switch_reason}
-# ASA-1-104003
-CISCOFW104003 \((?:Primary|Secondary)\) Switching to FAILED\.
-# ASA-1-104004
-CISCOFW104004 \((?:Primary|Secondary)\) Switching to OK\.
-# ASA-1-105003
-CISCOFW105003 \((?:Primary|Secondary)\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} waiting
-# ASA-1-105004
-CISCOFW105004 \((?:Primary|Secondary)\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} normal
-# ASA-1-105005
-CISCOFW105005 \((?:Primary|Secondary)\) Lost Failover communications with mate on [Ii]nterface %{GREEDYDATA:interface_name}
-# ASA-1-105008
-CISCOFW105008 \((?:Primary|Secondary)\) Testing [Ii]nterface %{GREEDYDATA:interface_name}
-# ASA-1-105009
-CISCOFW105009 \((?:Primary|Secondary)\) Testing on [Ii]nterface %{GREEDYDATA:interface_name} (?:Passed|Failed)
-# ASA-2-106001
-CISCOFW106001 %{CISCO_DIRECTION:direction} %{WORD:protocol} connection %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{GREEDYDATA:tcp_flags} on interface %{GREEDYDATA:interface}
-# ASA-2-106006, ASA-2-106007, ASA-2-106010
-CISCOFW106006_106007_106010 %{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} (?:from|src) %{IP:src_ip}/%{INT:src_port}(\(%{DATA:src_fwuser}\))? (?:to|dst) %{IP:dst_ip}/%{INT:dst_port}(\(%{DATA:dst_fwuser}\))? (?:on interface %{DATA:interface}|due to %{CISCO_REASON:reason})
-# ASA-3-106014
-CISCOFW106014 %{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} src %{DATA:src_interface}:%{IP:src_ip}(\(%{DATA:src_fwuser}\))? dst %{DATA:dst_interface}:%{IP:dst_ip}(\(%{DATA:dst_fwuser}\))? \(type %{INT:icmp_type}, code %{INT:icmp_code}\)
-# ASA-6-106015
-CISCOFW106015 %{CISCO_ACTION:action} %{WORD:protocol} \(%{DATA:policy_id}\) from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{DATA:tcp_flags}  on interface %{GREEDYDATA:interface}
-# ASA-1-106021
-CISCOFW106021 %{CISCO_ACTION:action} %{WORD:protocol} reverse path check from %{IP:src_ip} to %{IP:dst_ip} on interface %{GREEDYDATA:interface}
-# ASA-4-106023
-CISCOFW106023 %{CISCO_ACTION:action}( protocol)? %{WORD:protocol} src %{DATA:src_interface}:%{DATA:src_ip}(/%{INT:src_port})?(\(%{DATA:src_fwuser}\))? dst %{DATA:dst_interface}:%{DATA:dst_ip}(/%{INT:dst_port})?(\(%{DATA:dst_fwuser}\))?( \(type %{INT:icmp_type}, code %{INT:icmp_code}\))? by access-group "?%{DATA:policy_id}"? \[%{DATA:hashcode1}, %{DATA:hashcode2}\]
-# ASA-4-106100, ASA-4-106102, ASA-4-106103
-CISCOFW106100_2_3 access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} for user '%{DATA:src_fwuser}' %{DATA:src_interface}/%{IP:src_ip}\(%{INT:src_port}\) -> %{DATA:dst_interface}/%{IP:dst_ip}\(%{INT:dst_port}\) hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \[%{DATA:hashcode1}, %{DATA:hashcode2}\]
-# ASA-5-106100
-CISCOFW106100 access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} %{DATA:src_interface}/%{IP:src_ip}\(%{INT:src_port}\)(\(%{DATA:src_fwuser}\))? -> %{DATA:dst_interface}/%{IP:dst_ip}\(%{INT:dst_port}\)(\(%{DATA:src_fwuser}\))? hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \[%{DATA:hashcode1}, %{DATA:hashcode2}\]
-# ASA-6-110002
-CISCOFW110002 %{CISCO_REASON:reason} for %{WORD:protocol} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}
-# ASA-6-302010
-CISCOFW302010 %{INT:connection_count} in use, %{INT:connection_count_max} most used
-# ASA-6-302013, ASA-6-302014, ASA-6-302015, ASA-6-302016
-CISCOFW302013_302014_302015_302016 %{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection %{INT:connection_id} for %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port}( \(%{IP:src_mapped_ip}/%{INT:src_mapped_port}\))?(\(%{DATA:src_fwuser}\))? to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}( \(%{IP:dst_mapped_ip}/%{INT:dst_mapped_port}\))?(\(%{DATA:dst_fwuser}\))?( duration %{TIME:duration} bytes %{INT:bytes})?(?: %{CISCO_REASON:reason})?( \(%{DATA:user}\))?
-# ASA-6-302020, ASA-6-302021
-CISCOFW302020_302021 %{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection for faddr %{IP:dst_ip}/%{INT:icmp_seq_num}(?:\(%{DATA:fwuser}\))? gaddr %{IP:src_xlated_ip}/%{INT:icmp_code_xlated} laddr %{IP:src_ip}/%{INT:icmp_code}( \(%{DATA:user}\))?
-# ASA-6-305011
-CISCOFW305011 %{CISCO_ACTION:action} %{CISCO_XLATE_TYPE:xlate_type} %{WORD:protocol} translation from %{DATA:src_interface}:%{IP:src_ip}(/%{INT:src_port})?(\(%{DATA:src_fwuser}\))? to %{DATA:src_xlated_interface}:%{IP:src_xlated_ip}/%{DATA:src_xlated_port}
-# ASA-3-313001, ASA-3-313004, ASA-3-313008
-CISCOFW313001_313004_313008 %{CISCO_ACTION:action} %{WORD:protocol} type=%{INT:icmp_type}, code=%{INT:icmp_code} from %{IP:src_ip} on interface %{DATA:interface}( to %{IP:dst_ip})?
-# ASA-4-313005
-CISCOFW313005 %{CISCO_REASON:reason} for %{WORD:protocol} error message: %{WORD:err_protocol} src %{DATA:err_src_interface}:%{IP:err_src_ip}(\(%{DATA:err_src_fwuser}\))? dst %{DATA:err_dst_interface}:%{IP:err_dst_ip}(\(%{DATA:err_dst_fwuser}\))? \(type %{INT:err_icmp_type}, code %{INT:err_icmp_code}\) on %{DATA:interface} interface\.  Original IP payload: %{WORD:protocol} src %{IP:orig_src_ip}/%{INT:orig_src_port}(\(%{DATA:orig_src_fwuser}\))? dst %{IP:orig_dst_ip}/%{INT:orig_dst_port}(\(%{DATA:orig_dst_fwuser}\))?
-# ASA-5-321001
-CISCOFW321001 Resource '%{WORD:resource_name}' limit of %{POSINT:resource_limit} reached for system
-# ASA-4-402117
-CISCOFW402117 %{WORD:protocol}: Received a non-IPSec packet \(protocol= %{WORD:orig_protocol}\) from %{IP:src_ip} to %{IP:dst_ip}
-# ASA-4-402119
-CISCOFW402119 %{WORD:protocol}: Received an %{WORD:orig_protocol} packet \(SPI= %{DATA:spi}, sequence number= %{DATA:seq_num}\) from %{IP:src_ip} \(user= %{DATA:user}\) to %{IP:dst_ip} that failed anti-replay checking
-# ASA-4-419001
-CISCOFW419001 %{CISCO_ACTION:action} %{WORD:protocol} packet from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}, reason: %{GREEDYDATA:reason}
-# ASA-4-419002
-CISCOFW419002 %{CISCO_REASON:reason} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port} with different initial sequence number
-# ASA-4-500004
-CISCOFW500004 %{CISCO_REASON:reason} for protocol=%{WORD:protocol}, from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}
-# ASA-6-602303, ASA-6-602304
-CISCOFW602303_602304 %{WORD:protocol}: An %{CISCO_DIRECTION:direction} %{GREEDYDATA:tunnel_type} SA \(SPI= %{DATA:spi}\) between %{IP:src_ip} and %{IP:dst_ip} \(user= %{DATA:user}\) has been %{CISCO_ACTION:action}
-# ASA-7-710001, ASA-7-710002, ASA-7-710003, ASA-7-710005, ASA-7-710006
-CISCOFW710001_710002_710003_710005_710006 %{WORD:protocol} (?:request|access) %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}
-# ASA-6-713172
-CISCOFW713172 Group = %{GREEDYDATA:group}, IP = %{IP:src_ip}, Automatic NAT Detection Status:\s+Remote end\s*%{DATA:is_remote_natted}\s*behind a NAT device\s+This\s+end\s*%{DATA:is_local_natted}\s*behind a NAT device
-# ASA-4-733100
-CISCOFW733100 \[\s*%{DATA:drop_type}\s*\] drop %{DATA:drop_rate_id} exceeded. Current burst rate is %{INT:drop_rate_current_burst} per second, max configured rate is %{INT:drop_rate_max_burst}; Current average rate is %{INT:drop_rate_current_avg} per second, max configured rate is %{INT:drop_rate_max_avg}; Cumulative total count is %{INT:drop_total_count}
-#== End Cisco ASA ==
-
-# Shorewall firewall logs
-SHOREWALL (%{SYSLOGTIMESTAMP:timestamp}) (%{WORD:nf_host}) kernel:.*Shorewall:(%{WORD:nf_action1})?:(%{WORD:nf_action2})?.*IN=(%{USERNAME:nf_in_interface})?.*(OUT= *MAC=(%{COMMONMAC:nf_dst_mac}):(%{COMMONMAC:nf_src_mac})?|OUT=%{USERNAME:nf_out_interface}).*SRC=(%{IPV4:nf_src_ip}).*DST=(%{IPV4:nf_dst_ip}).*LEN=(%{WORD:nf_len}).?*TOS=(%{WORD:nf_tos}).?*PREC=(%{WORD:nf_prec}).?*TTL=(%{INT:nf_ttl}).?*ID=(%{INT:nf_id}).?*PROTO=(%{WORD:nf_protocol}).?*SPT=(%{INT:nf_src_port}?.*DPT=%{INT:nf_dst_port}?.*)
-#== End Shorewall
diff --git a/modules/ingest-grok/src/main/resources/patterns/grok-patterns b/modules/ingest-grok/src/main/resources/patterns/grok-patterns
deleted file mode 100644
index cb4c3ff..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/grok-patterns
+++ /dev/null
@@ -1,102 +0,0 @@
-USERNAME [a-zA-Z0-9._-]+
-USER %{USERNAME}
-EMAILLOCALPART [a-zA-Z][a-zA-Z0-9_.+-=:]+
-EMAILADDRESS %{EMAILLOCALPART}@%{HOSTNAME}
-HTTPDUSER %{EMAILADDRESS}|%{USER}
-INT (?:[+-]?(?:[0-9]+))
-BASE10NUM (?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\.[0-9]+)?)|(?:\.[0-9]+)))
-NUMBER (?:%{BASE10NUM})
-BASE16NUM (?<![0-9A-Fa-f])(?:[+-]?(?:0x)?(?:[0-9A-Fa-f]+))
-BASE16FLOAT \b(?<![0-9A-Fa-f.])(?:[+-]?(?:0x)?(?:(?:[0-9A-Fa-f]+(?:\.[0-9A-Fa-f]*)?)|(?:\.[0-9A-Fa-f]+)))\b
-
-POSINT \b(?:[1-9][0-9]*)\b
-NONNEGINT \b(?:[0-9]+)\b
-WORD \b\w+\b
-NOTSPACE \S+
-SPACE \s*
-DATA .*?
-GREEDYDATA .*
-QUOTEDSTRING (?>(?<!\\)(?>"(?>\\.|[^\\"]+)+"|""|(?>'(?>\\.|[^\\']+)+')|''|(?>`(?>\\.|[^\\`]+)+`)|``))
-UUID [A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12}
-
-# Networking
-MAC (?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC})
-CISCOMAC (?:(?:[A-Fa-f0-9]{4}\.){2}[A-Fa-f0-9]{4})
-WINDOWSMAC (?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2})
-COMMONMAC (?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2})
-IPV6 ((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:)))(%.+)?
-IPV4 (?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9])
-IP (?:%{IPV6}|%{IPV4})
-HOSTNAME \b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\.?|\b)
-IPORHOST (?:%{IP}|%{HOSTNAME})
-HOSTPORT %{IPORHOST}:%{POSINT}
-
-# paths
-PATH (?:%{UNIXPATH}|%{WINPATH})
-UNIXPATH (/([\w_%!$@:.,~-]+|\\.)*)+
-TTY (?:/dev/(pts|tty([pq])?)(\w+)?/?(?:[0-9]+))
-WINPATH (?>[A-Za-z]+:|\\)(?:\\[^\\?*]*)+
-URIPROTO [A-Za-z]+(\+[A-Za-z+]+)?
-URIHOST %{IPORHOST}(?::%{POSINT:port})?
-# uripath comes loosely from RFC1738, but mostly from what Firefox
-# doesn't turn into %XX
-URIPATH (?:/[A-Za-z0-9$.+!*'(){},~:;=@#%_\-]*)+
-#URIPARAM \?(?:[A-Za-z0-9]+(?:=(?:[^&]*))?(?:&(?:[A-Za-z0-9]+(?:=(?:[^&]*))?)?)*)?
-URIPARAM \?[A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\-\[\]<>]*
-URIPATHPARAM %{URIPATH}(?:%{URIPARAM})?
-URI %{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATHPARAM})?
-
-# Months: January, Feb, 3, 03, 12, December
-MONTH \b(?:Jan(?:uary|uar)?|Feb(?:ruary|ruar)?|M(?:a|ä)?r(?:ch|z)?|Apr(?:il)?|Ma(?:y|i)?|Jun(?:e|i)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|O(?:c|k)?t(?:ober)?|Nov(?:ember)?|De(?:c|z)(?:ember)?)\b
-MONTHNUM (?:0?[1-9]|1[0-2])
-MONTHNUM2 (?:0[1-9]|1[0-2])
-MONTHDAY (?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])
-
-# Days: Monday, Tue, Thu, etc...
-DAY (?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)
-
-# Years?
-YEAR (?>\d\d){1,2}
-HOUR (?:2[0123]|[01]?[0-9])
-MINUTE (?:[0-5][0-9])
-# '60' is a leap second in most time standards and thus is valid.
-SECOND (?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)
-TIME (?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])
-# datestamp is YYYY/MM/DD-HH:MM:SS.UUUU (or something like it)
-DATE_US %{MONTHNUM}[/-]%{MONTHDAY}[/-]%{YEAR}
-DATE_EU %{MONTHDAY}[./-]%{MONTHNUM}[./-]%{YEAR}
-ISO8601_TIMEZONE (?:Z|[+-]%{HOUR}(?::?%{MINUTE}))
-ISO8601_SECOND (?:%{SECOND}|60)
-ISO8601_HOUR (?:2[0123]|[01][0-9])
-TIMESTAMP_ISO8601 %{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{ISO8601_HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?
-DATE %{DATE_US}|%{DATE_EU}
-DATESTAMP %{DATE}[- ]%{TIME}
-TZ (?:[PMCE][SD]T|UTC)
-DATESTAMP_RFC822 %{DAY} %{MONTH} %{MONTHDAY} %{YEAR} %{TIME} %{TZ}
-DATESTAMP_RFC2822 %{DAY}, %{MONTHDAY} %{MONTH} %{YEAR} %{TIME} %{ISO8601_TIMEZONE}
-DATESTAMP_OTHER %{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{TZ} %{YEAR}
-DATESTAMP_EVENTLOG %{YEAR}%{MONTHNUM2}%{MONTHDAY}%{HOUR}%{MINUTE}%{SECOND}
-HTTPDERROR_DATE %{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}
-
-# Syslog Dates: Month Day HH:MM:SS
-SYSLOGTIMESTAMP %{MONTH} +%{MONTHDAY} %{TIME}
-PROG [\x21-\x5a\x5c\x5e-\x7e]+
-SYSLOGPROG %{PROG:program}(?:\[%{POSINT:pid}\])?
-SYSLOGHOST %{IPORHOST}
-SYSLOGFACILITY <%{NONNEGINT:facility}.%{NONNEGINT:priority}>
-HTTPDATE %{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}
-
-# Shortcuts
-QS %{QUOTEDSTRING}
-
-# Log formats
-SYSLOGBASE %{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:
-COMMONAPACHELOG %{IPORHOST:clientip} %{HTTPDUSER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})" %{NUMBER:response} (?:%{NUMBER:bytes}|-)
-COMBINEDAPACHELOG %{COMMONAPACHELOG} %{QS:referrer} %{QS:agent}
-HTTPD20_ERRORLOG \[%{HTTPDERROR_DATE:timestamp}\] \[%{LOGLEVEL:loglevel}\] (?:\[client %{IPORHOST:clientip}\] ){0,1}%{GREEDYDATA:errormsg}
-HTTPD24_ERRORLOG \[%{HTTPDERROR_DATE:timestamp}\] \[%{WORD:module}:%{LOGLEVEL:loglevel}\] \[pid %{POSINT:pid}:tid %{NUMBER:tid}\]( \(%{POSINT:proxy_errorcode}\)%{DATA:proxy_errormessage}:)?( \[client %{IPORHOST:client}:%{POSINT:clientport}\])? %{DATA:errorcode}: %{GREEDYDATA:message}
-HTTPD_ERRORLOG %{HTTPD20_ERRORLOG}|%{HTTPD24_ERRORLOG}
-
-
-# Log Levels
-LOGLEVEL ([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)
diff --git a/modules/ingest-grok/src/main/resources/patterns/haproxy b/modules/ingest-grok/src/main/resources/patterns/haproxy
deleted file mode 100644
index ddabd19..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/haproxy
+++ /dev/null
@@ -1,39 +0,0 @@
-## These patterns were tested w/ haproxy-1.4.15
-
-## Documentation of the haproxy log formats can be found at the following links:
-## http://code.google.com/p/haproxy-docs/wiki/HTTPLogFormat
-## http://code.google.com/p/haproxy-docs/wiki/TCPLogFormat
-
-HAPROXYTIME (?!<[0-9])%{HOUR:haproxy_hour}:%{MINUTE:haproxy_minute}(?::%{SECOND:haproxy_second})(?![0-9])
-HAPROXYDATE %{MONTHDAY:haproxy_monthday}/%{MONTH:haproxy_month}/%{YEAR:haproxy_year}:%{HAPROXYTIME:haproxy_time}.%{INT:haproxy_milliseconds}
-
-# Override these default patterns to parse out what is captured in your haproxy.cfg
-HAPROXYCAPTUREDREQUESTHEADERS %{DATA:captured_request_headers}
-HAPROXYCAPTUREDRESPONSEHEADERS %{DATA:captured_response_headers}
-
-# Example:
-#  These haproxy config lines will add data to the logs that are captured
-#  by the patterns below. Place them in your custom patterns directory to
-#  override the defaults.
-#
-#  capture request header Host len 40
-#  capture request header X-Forwarded-For len 50
-#  capture request header Accept-Language len 50
-#  capture request header Referer len 200
-#  capture request header User-Agent len 200
-#
-#  capture response header Content-Type len 30
-#  capture response header Content-Encoding len 10
-#  capture response header Cache-Control len 200
-#  capture response header Last-Modified len 200
-#
-# HAPROXYCAPTUREDREQUESTHEADERS %{DATA:request_header_host}\|%{DATA:request_header_x_forwarded_for}\|%{DATA:request_header_accept_language}\|%{DATA:request_header_referer}\|%{DATA:request_header_user_agent}
-# HAPROXYCAPTUREDRESPONSEHEADERS %{DATA:response_header_content_type}\|%{DATA:response_header_content_encoding}\|%{DATA:response_header_cache_control}\|%{DATA:response_header_last_modified}
-
-# parse a haproxy 'httplog' line
-HAPROXYHTTPBASE %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\{%{HAPROXYCAPTUREDREQUESTHEADERS}\})?( )?(\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\})?( )?"(<BADREQ>|(%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version})?))?"
-
-HAPROXYHTTP (?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{HAPROXYHTTPBASE}
-
-# parse a haproxy 'tcplog' line
-HAPROXYTCP (?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_queue}/%{INT:time_backend_connect}/%{NOTSPACE:time_duration} %{NOTSPACE:bytes_read} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue}
diff --git a/modules/ingest-grok/src/main/resources/patterns/java b/modules/ingest-grok/src/main/resources/patterns/java
deleted file mode 100644
index e968006..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/java
+++ /dev/null
@@ -1,20 +0,0 @@
-JAVACLASS (?:[a-zA-Z$_][a-zA-Z$_0-9]*\.)*[a-zA-Z$_][a-zA-Z$_0-9]*
-#Space is an allowed character to match special cases like 'Native Method' or 'Unknown Source'
-JAVAFILE (?:[A-Za-z0-9_. -]+)
-#Allow special <init> method
-JAVAMETHOD (?:(<init>)|[a-zA-Z$_][a-zA-Z$_0-9]*)
-#Line number is optional in special cases 'Native method' or 'Unknown source'
-JAVASTACKTRACEPART %{SPACE}at %{JAVACLASS:class}\.%{JAVAMETHOD:method}\(%{JAVAFILE:file}(?::%{NUMBER:line})?\)
-# Java Logs
-JAVATHREAD (?:[A-Z]{2}-Processor[\d]+)
-JAVACLASS (?:[a-zA-Z0-9-]+\.)+[A-Za-z0-9$]+
-JAVAFILE (?:[A-Za-z0-9_.-]+)
-JAVASTACKTRACEPART at %{JAVACLASS:class}\.%{WORD:method}\(%{JAVAFILE:file}:%{NUMBER:line}\)
-JAVALOGMESSAGE (.*)
-# MMM dd, yyyy HH:mm:ss eg: Jan 9, 2014 7:13:13 AM
-CATALINA_DATESTAMP %{MONTH} %{MONTHDAY}, 20%{YEAR} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) (?:AM|PM)
-# yyyy-MM-dd HH:mm:ss,SSS ZZZ eg: 2014-01-09 17:32:25,527 -0800
-TOMCAT_DATESTAMP 20%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) %{ISO8601_TIMEZONE}
-CATALINALOG %{CATALINA_DATESTAMP:timestamp} %{JAVACLASS:class} %{JAVALOGMESSAGE:logmessage}
-# 2014-01-09 20:03:28,269 -0800 | ERROR | com.example.service.ExampleService - something compeletely unexpected happened...
-TOMCATLOG %{TOMCAT_DATESTAMP:timestamp} \| %{LOGLEVEL:level} \| %{JAVACLASS:class} - %{JAVALOGMESSAGE:logmessage}
diff --git a/modules/ingest-grok/src/main/resources/patterns/junos b/modules/ingest-grok/src/main/resources/patterns/junos
deleted file mode 100644
index 4eea59d..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/junos
+++ /dev/null
@@ -1,9 +0,0 @@
-# JUNOS 11.4 RT_FLOW patterns
-RT_FLOW_EVENT (RT_FLOW_SESSION_CREATE|RT_FLOW_SESSION_CLOSE|RT_FLOW_SESSION_DENY)
-
-RT_FLOW1 %{RT_FLOW_EVENT:event}: %{GREEDYDATA:close-reason}: %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} \d+\(%{DATA:sent}\) \d+\(%{DATA:received}\) %{INT:elapsed-time} .*
-
-RT_FLOW2 %{RT_FLOW_EVENT:event}: session created %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} .*
-
-RT_FLOW3 %{RT_FLOW_EVENT:event}: session denied %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{INT:protocol-id}\(\d\) %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} .*
-
diff --git a/modules/ingest-grok/src/main/resources/patterns/linux-syslog b/modules/ingest-grok/src/main/resources/patterns/linux-syslog
deleted file mode 100644
index dcffb41..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/linux-syslog
+++ /dev/null
@@ -1,16 +0,0 @@
-SYSLOG5424PRINTASCII [!-~]+
-
-SYSLOGBASE2 (?:%{SYSLOGTIMESTAMP:timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource}+(?: %{SYSLOGPROG}:|)
-SYSLOGPAMSESSION %{SYSLOGBASE} (?=%{GREEDYDATA:message})%{WORD:pam_module}\(%{DATA:pam_caller}\): session %{WORD:pam_session_state} for user %{USERNAME:username}(?: by %{GREEDYDATA:pam_by})?
-
-CRON_ACTION [A-Z ]+
-CRONLOG %{SYSLOGBASE} \(%{USER:user}\) %{CRON_ACTION:action} \(%{DATA:message}\)
-
-SYSLOGLINE %{SYSLOGBASE2} %{GREEDYDATA:message}
-
-# IETF 5424 syslog(8) format (see http://www.rfc-editor.org/info/rfc5424)
-SYSLOG5424PRI <%{NONNEGINT:syslog5424_pri}>
-SYSLOG5424SD \[%{DATA}\]+
-SYSLOG5424BASE %{SYSLOG5424PRI}%{NONNEGINT:syslog5424_ver} +(?:%{TIMESTAMP_ISO8601:syslog5424_ts}|-) +(?:%{HOSTNAME:syslog5424_host}|-) +(-|%{SYSLOG5424PRINTASCII:syslog5424_app}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_proc}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_msgid}) +(?:%{SYSLOG5424SD:syslog5424_sd}|-|)
-
-SYSLOG5424LINE %{SYSLOG5424BASE} +%{GREEDYDATA:syslog5424_msg}
diff --git a/modules/ingest-grok/src/main/resources/patterns/mcollective-patterns b/modules/ingest-grok/src/main/resources/patterns/mcollective-patterns
deleted file mode 100644
index bb2f7f9..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/mcollective-patterns
+++ /dev/null
@@ -1,4 +0,0 @@
-# Remember, these can be multi-line events.
-MCOLLECTIVE ., \[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\]%{SPACE}%{LOGLEVEL:event_level}
-
-MCOLLECTIVEAUDIT %{TIMESTAMP_ISO8601:timestamp}:
diff --git a/modules/ingest-grok/src/main/resources/patterns/mongodb b/modules/ingest-grok/src/main/resources/patterns/mongodb
deleted file mode 100644
index 78a4300..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/mongodb
+++ /dev/null
@@ -1,7 +0,0 @@
-MONGO_LOG %{SYSLOGTIMESTAMP:timestamp} \[%{WORD:component}\] %{GREEDYDATA:message}
-MONGO_QUERY \{ (?<={ ).*(?= } ntoreturn:) \}
-MONGO_SLOWQUERY %{WORD} %{MONGO_WORDDASH:database}\.%{MONGO_WORDDASH:collection} %{WORD}: %{MONGO_QUERY:query} %{WORD}:%{NONNEGINT:ntoreturn} %{WORD}:%{NONNEGINT:ntoskip} %{WORD}:%{NONNEGINT:nscanned}.*nreturned:%{NONNEGINT:nreturned}..+ (?<duration>[0-9]+)ms
-MONGO_WORDDASH \b[\w-]+\b
-MONGO3_SEVERITY \w
-MONGO3_COMPONENT %{WORD}|-
-MONGO3_LOG %{TIMESTAMP_ISO8601:timestamp} %{MONGO3_SEVERITY:severity} %{MONGO3_COMPONENT:component}%{SPACE}(?:\[%{DATA:context}\])? %{GREEDYDATA:message}
diff --git a/modules/ingest-grok/src/main/resources/patterns/nagios b/modules/ingest-grok/src/main/resources/patterns/nagios
deleted file mode 100644
index f4a98bf..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/nagios
+++ /dev/null
@@ -1,124 +0,0 @@
-##################################################################################
-##################################################################################
-# Chop Nagios log files to smithereens!
-#
-# A set of GROK filters to process logfiles generated by Nagios.
-# While it does not, this set intends to cover all possible Nagios logs.
-#
-# Some more work needs to be done to cover all External Commands:
-# http://old.nagios.org/developerinfo/externalcommands/commandlist.php
-#
-# If you need some support on these rules please contact:
-# Jelle Smet http://smetj.net
-#
-#################################################################################
-#################################################################################
-
-NAGIOSTIME \[%{NUMBER:nagios_epoch}\]
-
-###############################################
-######## Begin nagios log types
-###############################################
-NAGIOS_TYPE_CURRENT_SERVICE_STATE CURRENT SERVICE STATE
-NAGIOS_TYPE_CURRENT_HOST_STATE CURRENT HOST STATE
-
-NAGIOS_TYPE_SERVICE_NOTIFICATION SERVICE NOTIFICATION
-NAGIOS_TYPE_HOST_NOTIFICATION HOST NOTIFICATION
-
-NAGIOS_TYPE_SERVICE_ALERT SERVICE ALERT
-NAGIOS_TYPE_HOST_ALERT HOST ALERT
-
-NAGIOS_TYPE_SERVICE_FLAPPING_ALERT SERVICE FLAPPING ALERT
-NAGIOS_TYPE_HOST_FLAPPING_ALERT HOST FLAPPING ALERT
-
-NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT SERVICE DOWNTIME ALERT
-NAGIOS_TYPE_HOST_DOWNTIME_ALERT HOST DOWNTIME ALERT
-
-NAGIOS_TYPE_PASSIVE_SERVICE_CHECK PASSIVE SERVICE CHECK
-NAGIOS_TYPE_PASSIVE_HOST_CHECK PASSIVE HOST CHECK
-
-NAGIOS_TYPE_SERVICE_EVENT_HANDLER SERVICE EVENT HANDLER
-NAGIOS_TYPE_HOST_EVENT_HANDLER HOST EVENT HANDLER
-
-NAGIOS_TYPE_EXTERNAL_COMMAND EXTERNAL COMMAND
-NAGIOS_TYPE_TIMEPERIOD_TRANSITION TIMEPERIOD TRANSITION
-###############################################
-######## End nagios log types
-###############################################
-
-###############################################
-######## Begin external check types
-###############################################
-NAGIOS_EC_DISABLE_SVC_CHECK DISABLE_SVC_CHECK
-NAGIOS_EC_ENABLE_SVC_CHECK ENABLE_SVC_CHECK
-NAGIOS_EC_DISABLE_HOST_CHECK DISABLE_HOST_CHECK
-NAGIOS_EC_ENABLE_HOST_CHECK ENABLE_HOST_CHECK
-NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT PROCESS_SERVICE_CHECK_RESULT
-NAGIOS_EC_PROCESS_HOST_CHECK_RESULT PROCESS_HOST_CHECK_RESULT
-NAGIOS_EC_SCHEDULE_SERVICE_DOWNTIME SCHEDULE_SERVICE_DOWNTIME
-NAGIOS_EC_SCHEDULE_HOST_DOWNTIME SCHEDULE_HOST_DOWNTIME
-NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS DISABLE_HOST_SVC_NOTIFICATIONS
-NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS ENABLE_HOST_SVC_NOTIFICATIONS
-NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS DISABLE_HOST_NOTIFICATIONS
-NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS ENABLE_HOST_NOTIFICATIONS
-NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS DISABLE_SVC_NOTIFICATIONS
-NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS ENABLE_SVC_NOTIFICATIONS
-###############################################
-######## End external check types
-###############################################
-NAGIOS_WARNING Warning:%{SPACE}%{GREEDYDATA:nagios_message}
-
-NAGIOS_CURRENT_SERVICE_STATE %{NAGIOS_TYPE_CURRENT_SERVICE_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}
-NAGIOS_CURRENT_HOST_STATE %{NAGIOS_TYPE_CURRENT_HOST_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}
-
-NAGIOS_SERVICE_NOTIFICATION %{NAGIOS_TYPE_SERVICE_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}
-NAGIOS_HOST_NOTIFICATION %{NAGIOS_TYPE_HOST_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}
-
-NAGIOS_SERVICE_ALERT %{NAGIOS_TYPE_SERVICE_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}
-NAGIOS_HOST_ALERT %{NAGIOS_TYPE_HOST_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}
-
-NAGIOS_SERVICE_FLAPPING_ALERT %{NAGIOS_TYPE_SERVICE_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}
-NAGIOS_HOST_FLAPPING_ALERT %{NAGIOS_TYPE_HOST_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}
-
-NAGIOS_SERVICE_DOWNTIME_ALERT %{NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
-NAGIOS_HOST_DOWNTIME_ALERT %{NAGIOS_TYPE_HOST_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
-
-NAGIOS_PASSIVE_SERVICE_CHECK %{NAGIOS_TYPE_PASSIVE_SERVICE_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
-NAGIOS_PASSIVE_HOST_CHECK %{NAGIOS_TYPE_PASSIVE_HOST_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
-
-NAGIOS_SERVICE_EVENT_HANDLER %{NAGIOS_TYPE_SERVICE_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}
-NAGIOS_HOST_EVENT_HANDLER %{NAGIOS_TYPE_HOST_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}
-
-NAGIOS_TIMEPERIOD_TRANSITION %{NAGIOS_TYPE_TIMEPERIOD_TRANSITION:nagios_type}: %{DATA:nagios_service};%{DATA:nagios_unknown1};%{DATA:nagios_unknown2}
-
-####################
-#### External checks
-####################
-
-#Disable host & service check
-NAGIOS_EC_LINE_DISABLE_SVC_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}
-NAGIOS_EC_LINE_DISABLE_HOST_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}
-
-#Enable host & service check
-NAGIOS_EC_LINE_ENABLE_SVC_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}
-NAGIOS_EC_LINE_ENABLE_HOST_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}
-
-#Process host & service check
-NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}
-NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_HOST_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}
-
-#Disable host & service notifications
-NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}
-NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}
-NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}
-
-#Enable host & service notifications
-NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}
-NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}
-NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}
-
-#Schedule host & service downtime
-NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_SCHEDULE_HOST_DOWNTIME:nagios_command};%{DATA:nagios_hostname};%{NUMBER:nagios_start_time};%{NUMBER:nagios_end_time};%{NUMBER:nagios_fixed};%{NUMBER:nagios_trigger_id};%{NUMBER:nagios_duration};%{DATA:author};%{DATA:comment}
-
-#End matching line
-NAGIOSLOGLINE %{NAGIOSTIME} (?:%{NAGIOS_WARNING}|%{NAGIOS_CURRENT_SERVICE_STATE}|%{NAGIOS_CURRENT_HOST_STATE}|%{NAGIOS_SERVICE_NOTIFICATION}|%{NAGIOS_HOST_NOTIFICATION}|%{NAGIOS_SERVICE_ALERT}|%{NAGIOS_HOST_ALERT}|%{NAGIOS_SERVICE_FLAPPING_ALERT}|%{NAGIOS_HOST_FLAPPING_ALERT}|%{NAGIOS_SERVICE_DOWNTIME_ALERT}|%{NAGIOS_HOST_DOWNTIME_ALERT}|%{NAGIOS_PASSIVE_SERVICE_CHECK}|%{NAGIOS_PASSIVE_HOST_CHECK}|%{NAGIOS_SERVICE_EVENT_HANDLER}|%{NAGIOS_HOST_EVENT_HANDLER}|%{NAGIOS_TIMEPERIOD_TRANSITION}|%{NAGIOS_EC_LINE_DISABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_ENABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_DISABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_ENABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT}|%{NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT}|%{NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME}|%{NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS})
diff --git a/modules/ingest-grok/src/main/resources/patterns/postgresql b/modules/ingest-grok/src/main/resources/patterns/postgresql
deleted file mode 100644
index c5b3e90..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/postgresql
+++ /dev/null
@@ -1,3 +0,0 @@
-# Default postgresql pg_log format pattern
-POSTGRESQL %{DATESTAMP:timestamp} %{TZ} %{DATA:user_id} %{GREEDYDATA:connection_id} %{POSINT:pid}
-
diff --git a/modules/ingest-grok/src/main/resources/patterns/rails b/modules/ingest-grok/src/main/resources/patterns/rails
deleted file mode 100644
index 68a50c7..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/rails
+++ /dev/null
@@ -1,13 +0,0 @@
-RUUID \h{32}
-# rails controller with action
-RCONTROLLER (?<controller>[^#]+)#(?<action>\w+)
-
-# this will often be the only line:
-RAILS3HEAD (?m)Started %{WORD:verb} "%{URIPATHPARAM:request}" for %{IPORHOST:clientip} at (?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:%{MINUTE}:%{SECOND} %{ISO8601_TIMEZONE})
-# for some a strange reason, params are stripped of {} - not sure that's a good idea.
-RPROCESSING \W*Processing by %{RCONTROLLER} as (?<format>\S+)(?:\W*Parameters: {%{DATA:params}}\W*)?
-RAILS3FOOT Completed %{NUMBER:response}%{DATA} in %{NUMBER:totalms}ms %{RAILS3PROFILE}%{GREEDYDATA}
-RAILS3PROFILE (?:\(Views: %{NUMBER:viewms}ms \| ActiveRecord: %{NUMBER:activerecordms}ms|\(ActiveRecord: %{NUMBER:activerecordms}ms)?
-
-# putting it all together
-RAILS3 %{RAILS3HEAD}(?:%{RPROCESSING})?(?<context>(?:%{DATA}\n)*)(?:%{RAILS3FOOT})?
diff --git a/modules/ingest-grok/src/main/resources/patterns/redis b/modules/ingest-grok/src/main/resources/patterns/redis
deleted file mode 100644
index 8655c4f..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/redis
+++ /dev/null
@@ -1,3 +0,0 @@
-REDISTIMESTAMP %{MONTHDAY} %{MONTH} %{TIME}
-REDISLOG \[%{POSINT:pid}\] %{REDISTIMESTAMP:timestamp} \* 
-
diff --git a/modules/ingest-grok/src/main/resources/patterns/ruby b/modules/ingest-grok/src/main/resources/patterns/ruby
deleted file mode 100644
index b1729cd..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/ruby
+++ /dev/null
@@ -1,2 +0,0 @@
-RUBY_LOGLEVEL (?:DEBUG|FATAL|ERROR|WARN|INFO)
-RUBY_LOGGER [DFEWI], \[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\] *%{RUBY_LOGLEVEL:loglevel} -- +%{DATA:progname}: %{GREEDYDATA:message}
diff --git a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorFactoryTests.java b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorFactoryTests.java
deleted file mode 100644
index 11c1024..0000000
--- a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorFactoryTests.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.grok;
-
-import org.elasticsearch.ingest.grok.GrokProcessor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.notNullValue;
-
-public class GrokProcessorFactoryTests extends ESTestCase {
-
-    public void testBuild() throws Exception {
-        GrokProcessor.Factory factory = new GrokProcessor.Factory();
-
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "_field");
-        config.put("pattern", "(?<foo>\\w+)");
-        GrokProcessor processor = factory.create(config);
-        assertThat(processor.getMatchField(), equalTo("_field"));
-        assertThat(processor.getGrok(), notNullValue());
-    }
-
-    public void testCreateWithCustomPatterns() throws Exception {
-        GrokProcessor.Factory factory = new GrokProcessor.Factory();
-
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "_field");
-        config.put("pattern", "%{MY_PATTERN:name}!");
-        config.put("pattern_definitions", Collections.singletonMap("MY_PATTERN", "foo"));
-        GrokProcessor processor = factory.create(config);
-        assertThat(processor.getMatchField(), equalTo("_field"));
-        assertThat(processor.getGrok(), notNullValue());
-        assertThat(processor.getGrok().match("foo!"), equalTo(true));
-    }
-}
diff --git a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorTests.java b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorTests.java
deleted file mode 100644
index bb2de7e..0000000
--- a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorTests.java
+++ /dev/null
@@ -1,86 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.grok;
-
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.grok.Grok;
-import org.elasticsearch.ingest.grok.GrokProcessor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-
-import static org.hamcrest.Matchers.equalTo;
-
-
-public class GrokProcessorTests extends ESTestCase {
-
-    public void testMatch() throws Exception {
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        IngestDocument doc = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        doc.setFieldValue(fieldName, "1");
-        Grok grok = new Grok(Collections.singletonMap("ONE", "1"), "%{ONE:one}");
-        GrokProcessor processor = new GrokProcessor(grok, fieldName);
-        processor.execute(doc);
-        assertThat(doc.getFieldValue("one", String.class), equalTo("1"));
-    }
-
-    public void testNoMatch() {
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        IngestDocument doc = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        doc.setFieldValue(fieldName, "23");
-        Grok grok = new Grok(Collections.singletonMap("ONE", "1"), "%{ONE:one}");
-        GrokProcessor processor = new GrokProcessor(grok, fieldName);
-        try {
-            processor.execute(doc);
-            fail();
-        } catch (Exception e) {
-            assertThat(e.getMessage(), equalTo("Grok expression does not match field value: [23]"));
-        }
-    }
-
-    public void testNotStringField() {
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        IngestDocument doc = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        doc.setFieldValue(fieldName, 1);
-        Grok grok = new Grok(Collections.singletonMap("ONE", "1"), "%{ONE:one}");
-        GrokProcessor processor = new GrokProcessor(grok, fieldName);
-        try {
-            processor.execute(doc);
-            fail();
-        } catch (Exception e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
-        }
-    }
-
-    public void testMissingField() {
-        String fieldName = "foo.bar";
-        IngestDocument doc = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        Grok grok = new Grok(Collections.singletonMap("ONE", "1"), "%{ONE:one}");
-        GrokProcessor processor = new GrokProcessor(grok, fieldName);
-        try {
-            processor.execute(doc);
-            fail();
-        } catch (Exception e) {
-            assertThat(e.getMessage(), equalTo("field [foo] not present as part of path [foo.bar]"));
-        }
-    }
-}
diff --git a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokTests.java b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokTests.java
deleted file mode 100644
index df9a769..0000000
--- a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokTests.java
+++ /dev/null
@@ -1,294 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.grok;
-
-import org.elasticsearch.ingest.grok.Grok;
-import org.elasticsearch.ingest.grok.GrokProcessor;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.nullValue;
-
-
-public class GrokTests extends ESTestCase {
-    private Map<String, String> basePatterns;
-
-    private Map<String, String> newBankFromStreams(InputStream... inputStreams) throws IOException {
-        Map<String, String> patternBank = new HashMap<>();
-
-        for (InputStream is : inputStreams) {
-            GrokProcessor.Factory.loadBankFromStream(patternBank, is);
-        }
-
-        return patternBank;
-    }
-
-    @Before
-    public void setup() throws IOException {
-        basePatterns = newBankFromStreams(
-                getClass().getResourceAsStream("/patterns/grok-patterns"),
-                getClass().getResourceAsStream("/patterns/linux-syslog")
-        );
-    }
-
-    public void testSimpleSyslogLine() {
-        String line = "Mar 16 00:01:25 evita postfix/smtpd[1713]: connect from camomile.cloud9.net[168.100.1.3]";
-        Grok grok = new Grok(basePatterns, "%{SYSLOGLINE}");
-        Map<String, Object> matches = grok.captures(line);
-        assertEquals("evita", matches.get("logsource"));
-        assertEquals("Mar 16 00:01:25", matches.get("timestamp"));
-        assertEquals("connect from camomile.cloud9.net[168.100.1.3]", matches.get("message"));
-        assertEquals("postfix/smtpd", matches.get("program"));
-        assertEquals("1713", matches.get("pid"));
-    }
-
-    public void testSyslog5424Line() {
-        String line = "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug 4123 - [id1 foo=\\\"bar\\\"][id2 baz=\\\"something\\\"] Hello, syslog.";
-        Grok grok = new Grok(basePatterns, "%{SYSLOG5424LINE}");
-        Map<String, Object> matches = grok.captures(line);
-        assertEquals("191", matches.get("syslog5424_pri"));
-        assertEquals("1", matches.get("syslog5424_ver"));
-        assertEquals("2009-06-30T18:30:00+02:00", matches.get("syslog5424_ts"));
-        assertEquals("paxton.local", matches.get("syslog5424_host"));
-        assertEquals("grokdebug", matches.get("syslog5424_app"));
-        assertEquals("4123", matches.get("syslog5424_proc"));
-        assertEquals(null, matches.get("syslog5424_msgid"));
-        assertEquals("[id1 foo=\\\"bar\\\"][id2 baz=\\\"something\\\"]", matches.get("syslog5424_sd"));
-        assertEquals("Hello, syslog.", matches.get("syslog5424_msg"));
-    }
-
-    public void testDatePattern() {
-        String line = "fancy 12-12-12 12:12:12";
-        Grok grok = new Grok(basePatterns, "(?<timestamp>%{DATE_EU} %{TIME})");
-        Map<String, Object> matches = grok.captures(line);
-        assertEquals("12-12-12 12:12:12", matches.get("timestamp"));
-    }
-
-    public void testNilCoercedValues() {
-        Grok grok = new Grok(basePatterns, "test (N/A|%{BASE10NUM:duration:float}ms)");
-        Map<String, Object> matches = grok.captures("test 28.4ms");
-        assertEquals(28.4f, matches.get("duration"));
-        matches = grok.captures("test N/A");
-        assertEquals(null, matches.get("duration"));
-    }
-
-    public void testNilWithNoCoercion() {
-        Grok grok = new Grok(basePatterns, "test (N/A|%{BASE10NUM:duration}ms)");
-        Map<String, Object> matches = grok.captures("test 28.4ms");
-        assertEquals("28.4", matches.get("duration"));
-        matches = grok.captures("test N/A");
-        assertEquals(null, matches.get("duration"));
-    }
-
-    public void testUnicodeSyslog() {
-        Grok grok = new Grok(basePatterns, "<%{POSINT:syslog_pri}>%{SPACE}%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{PROG:syslog_program}(:?)(?:\\[%{GREEDYDATA:syslog_pid}\\])?(:?) %{GREEDYDATA:syslog_message}");
-        Map<String, Object> matches = grok.captures("<22>Jan  4 07:50:46 mailmaster postfix/policy-spf[9454]: : SPF permerror (Junk encountered in record 'v=spf1 mx a:mail.domain.no ip4:192.168.0.4 �all'): Envelope-from: email@domain.no");
-        assertThat(matches.get("syslog_pri"), equalTo("22"));
-        assertThat(matches.get("syslog_program"), equalTo("postfix/policy-spf"));
-        assertThat(matches.get("tags"), nullValue());
-    }
-
-    public void testNamedFieldsWithWholeTextMatch() {
-        Grok grok = new Grok(basePatterns, "%{DATE_EU:stimestamp}");
-        Map<String, Object> matches = grok.captures("11/01/01");
-        assertThat(matches.get("stimestamp"), equalTo("11/01/01"));
-    }
-
-    public void testWithOniguramaNamedCaptures() {
-        Grok grok = new Grok(basePatterns, "(?<foo>\\w+)");
-        Map<String, Object> matches = grok.captures("hello world");
-        assertThat(matches.get("foo"), equalTo("hello"));
-    }
-
-    public void testISO8601() {
-        Grok grok = new Grok(basePatterns, "^%{TIMESTAMP_ISO8601}$");
-        List<String> timeMessages = Arrays.asList(
-                "2001-01-01T00:00:00",
-                "1974-03-02T04:09:09",
-                "2010-05-03T08:18:18+00:00",
-                "2004-07-04T12:27:27-00:00",
-                "2001-09-05T16:36:36+0000",
-                "2001-11-06T20:45:45-0000",
-                "2001-12-07T23:54:54Z",
-                "2001-01-01T00:00:00.123456",
-                "1974-03-02T04:09:09.123456",
-                "2010-05-03T08:18:18.123456+00:00",
-                "2004-07-04T12:27:27.123456-00:00",
-                "2001-09-05T16:36:36.123456+0000",
-                "2001-11-06T20:45:45.123456-0000",
-                "2001-12-07T23:54:54.123456Z",
-                "2001-12-07T23:54:60.123456Z" // '60' second is a leap second.
-        );
-        for (String msg : timeMessages) {
-            assertThat(grok.match(msg), is(true));
-        }
-    }
-
-    public void testNotISO8601() {
-        Grok grok = new Grok(basePatterns, "^%{TIMESTAMP_ISO8601}$");
-        List<String> timeMessages = Arrays.asList(
-                "2001-13-01T00:00:00", // invalid month
-                "2001-00-01T00:00:00", // invalid month
-                "2001-01-00T00:00:00", // invalid day
-                "2001-01-32T00:00:00", // invalid day
-                "2001-01-aT00:00:00", // invalid day
-                "2001-01-1aT00:00:00", // invalid day
-                "2001-01-01Ta0:00:00", // invalid hour
-                "2001-01-01T0:00:00", // invalid hour
-                "2001-01-01T25:00:00", // invalid hour
-                "2001-01-01T01:60:00", // invalid minute
-                "2001-01-01T00:aa:00", // invalid minute
-                "2001-01-01T00:00:aa", // invalid second
-                "2001-01-01T00:00:-1", // invalid second
-                "2001-01-01T00:00:61", // invalid second
-                "2001-01-01T00:00:00A", // invalid timezone
-                "2001-01-01T00:00:00+", // invalid timezone
-                "2001-01-01T00:00:00+25", // invalid timezone
-                "2001-01-01T00:00:00+2500", // invalid timezone
-                "2001-01-01T00:00:00+25:00", // invalid timezone
-                "2001-01-01T00:00:00-25", // invalid timezone
-                "2001-01-01T00:00:00-2500", // invalid timezone
-                "2001-01-01T00:00:00-00:61" // invalid timezone
-        );
-        for (String msg : timeMessages) {
-            assertThat(grok.match(msg), is(false));
-        }
-    }
-
-    public void testNoNamedCaptures() {
-        Map<String, String> bank = new HashMap<>();
-
-        bank.put("NAME", "Tal");
-        bank.put("EXCITED_NAME", "!!!%{NAME:name}!!!");
-        bank.put("TEST", "hello world");
-
-        String text = "wowza !!!Tal!!! - Tal";
-        String pattern = "%{EXCITED_NAME} - %{NAME}";
-        Grok g = new Grok(bank, pattern, false);
-
-        assertEquals("(?<EXCITED_NAME_0>!!!(?<NAME_21>Tal)!!!) - (?<NAME_22>Tal)", g.toRegex(pattern));
-        assertEquals(true, g.match(text));
-
-        Object actual = g.captures(text);
-        Map<String, Object> expected = new HashMap<>();
-        expected.put("EXCITED_NAME_0", "!!!Tal!!!");
-        expected.put("NAME_21", "Tal");
-        expected.put("NAME_22", "Tal");
-        assertEquals(expected, actual);
-    }
-
-    public void testNumericCapturesCoercion() {
-        Map<String, String> bank = new HashMap<>();
-        bank.put("BASE10NUM", "(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))");
-        bank.put("NUMBER", "(?:%{BASE10NUM})");
-
-        String pattern = "%{NUMBER:bytes:float} %{NUMBER:status} %{NUMBER}";
-        Grok g = new Grok(bank, pattern);
-
-        String text = "12009.34 200 9032";
-        Map<String, Object> expected = new HashMap<>();
-        expected.put("bytes", 12009.34f);
-        expected.put("status", "200");
-        Map<String, Object> actual = g.captures(text);
-
-        assertEquals(expected, actual);
-    }
-
-    public void testApacheLog() {
-        String logLine = "31.184.238.164 - - [24/Jul/2014:05:35:37 +0530] \"GET /logs/access.log HTTP/1.0\" 200 69849 \"http://8rursodiol.enjin.com\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.12785 YaBrowser/13.12.1599.12785 Safari/537.36\" \"www.dlwindianrailways.com\"";
-        Grok grok = new Grok(basePatterns, "%{COMBINEDAPACHELOG}");
-        Map<String, Object> matches = grok.captures(logLine);
-
-        assertEquals("31.184.238.164", matches.get("clientip"));
-        assertEquals("-", matches.get("ident"));
-        assertEquals("-", matches.get("auth"));
-        assertEquals("24/Jul/2014:05:35:37 +0530", matches.get("timestamp"));
-        assertEquals("GET", matches.get("verb"));
-        assertEquals("/logs/access.log", matches.get("request"));
-        assertEquals("1.0", matches.get("httpversion"));
-        assertEquals("200", matches.get("response"));
-        assertEquals("69849", matches.get("bytes"));
-        assertEquals("\"http://8rursodiol.enjin.com\"", matches.get("referrer"));
-        assertEquals(null, matches.get("port"));
-        assertEquals("\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.12785 YaBrowser/13.12.1599.12785 Safari/537.36\"", matches.get("agent"));
-    }
-
-    public void testComplete() {
-        Map<String, String> bank = new HashMap<>();
-        bank.put("MONTHDAY", "(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])");
-        bank.put("MONTH", "\\b(?:Jan(?:uary|uar)?|Feb(?:ruary|ruar)?|M(?:a|ä)?r(?:ch|z)?|Apr(?:il)?|Ma(?:y|i)?|Jun(?:e|i)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|O(?:c|k)?t(?:ober)?|Nov(?:ember)?|De(?:c|z)(?:ember)?)\\b");
-        bank.put("MINUTE", "(?:[0-5][0-9])");
-        bank.put("YEAR", "(?>\\d\\d){1,2}");
-        bank.put("HOUR", "(?:2[0123]|[01]?[0-9])");
-        bank.put("SECOND", "(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)");
-        bank.put("TIME", "(?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])");
-        bank.put("INT", "(?:[+-]?(?:[0-9]+))");
-        bank.put("HTTPDATE", "%{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}");
-        bank.put("WORD", "\\b\\w+\\b");
-        bank.put("BASE10NUM", "(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))");
-        bank.put("NUMBER", "(?:%{BASE10NUM})");
-        bank.put("IPV6", "((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?");
-        bank.put("IPV4", "(?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9])");
-        bank.put("IP", "(?:%{IPV6}|%{IPV4})");
-        bank.put("HOSTNAME", "\\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b)");
-        bank.put("IPORHOST", "(?:%{IP}|%{HOSTNAME})");
-        bank.put("USER", "[a-zA-Z0-9._-]+");
-        bank.put("DATA", ".*?");
-        bank.put("QS", "(?>(?<!\\\\)(?>\"(?>\\\\.|[^\\\\\"]+)+\"|\"\"|(?>'(?>\\\\.|[^\\\\']+)+')|''|(?>`(?>\\\\.|[^\\\\`]+)+`)|``))");
-
-        String text = "83.149.9.216 - - [19/Jul/2015:08:13:42 +0000] \"GET /presentations/logstash-monitorama-2013/images/kibana-dashboard3.png HTTP/1.1\" 200 171717 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"";
-        String pattern = "%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}";
-
-        Grok grok = new Grok(bank, pattern);
-
-        Map<String, Object> expected = new HashMap<>();
-        expected.put("clientip", "83.149.9.216");
-        expected.put("ident", "-");
-        expected.put("auth", "-");
-        expected.put("timestamp", "19/Jul/2015:08:13:42 +0000");
-        expected.put("verb", "GET");
-        expected.put("request", "/presentations/logstash-monitorama-2013/images/kibana-dashboard3.png");
-        expected.put("httpversion", "1.1");
-        expected.put("response", 200);
-        expected.put("bytes", 171717);
-        expected.put("referrer", "\"http://semicomplete.com/presentations/logstash-monitorama-2013/\"");
-        expected.put("agent", "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"");
-
-        Map<String, Object> actual = grok.captures(text);
-
-        assertEquals(expected, actual);
-    }
-
-    public void testNoMatch() {
-        Map<String, String> bank = new HashMap<>();
-        bank.put("MONTHDAY", "(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])");
-        Grok grok = new Grok(bank, "%{MONTHDAY:greatday}");
-        assertThat(grok.captures("nomatch"), nullValue());
-    }
-}
diff --git a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/IngestGrokRestIT.java b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/IngestGrokRestIT.java
deleted file mode 100644
index 7073c4e..0000000
--- a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/IngestGrokRestIT.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.grok;
-
-import com.carrotsearch.randomizedtesting.annotations.Name;
-import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
-import org.elasticsearch.ingest.grok.IngestGrokPlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.rest.ESRestTestCase;
-import org.elasticsearch.test.rest.RestTestCandidate;
-import org.elasticsearch.test.rest.parser.RestTestParseException;
-
-import java.io.IOException;
-import java.util.Collection;
-
-public class IngestGrokRestIT extends ESRestTestCase {
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(IngestGrokPlugin.class);
-    }
-
-    public IngestGrokRestIT(@Name("yaml") RestTestCandidate testCandidate) {
-        super(testCandidate);
-    }
-
-    @ParametersFactory
-    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
-        return ESRestTestCase.createParameters(0, 1);
-    }
-}
-
diff --git a/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/10_basic.yaml b/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/10_basic.yaml
deleted file mode 100644
index 68d1fc6..0000000
--- a/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/10_basic.yaml
+++ /dev/null
@@ -1,12 +0,0 @@
-"Ingest grok installed":
-    - do:
-        cluster.state: {}
-
-    # Get master node id
-    - set: { master_node: master }
-
-    - do:
-        nodes.info: {}
-
-    - match:  { nodes.$master.modules.0.name: ingest-grok  }
-    - match:  { nodes.$master.modules.0.jvm: true  }
diff --git a/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/20_grok.yaml b/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/20_grok.yaml
deleted file mode 100644
index 70b1c8a..0000000
--- a/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/20_grok.yaml
+++ /dev/null
@@ -1,109 +0,0 @@
----
-"Test Grok Pipeline":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "grok" : {
-                  "field" : "field1",
-                  "pattern" : "%{NUMBER:val:float} %{NUMBER:status:int} <%{WORD:msg}>"
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field1: "123.42 400 <foo>"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.val: 123.42 }
-  - match: { _source.status: 400 }
-  - match: { _source.msg: "foo" }
-
----
-"Test Grok Pipeline With Custom Pattern":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "grok" : {
-                  "field" : "field1",
-                  "pattern" : "<%{MY_PATTERN:msg}>",
-                  "pattern_definitions" : {
-                    "MY_PATTERN" : "foo"
-                  }
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field1: "<foo>"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.msg: "foo" }
-
----
-"Test Grok Pipeline With Custom Pattern Sharing Same Name As Another":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "grok" : {
-                  "field" : "field1",
-                  "pattern" : "<%{NUMBER:msg}>",
-                  "pattern_definitions" : {
-                    "NUMBER" : "foo"
-                  }
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field1: "<foo>"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.msg: "foo" }
diff --git a/modules/lang-groovy/build.gradle b/modules/lang-groovy/build.gradle
index 7db4eab..73ad604 100644
--- a/modules/lang-groovy/build.gradle
+++ b/modules/lang-groovy/build.gradle
@@ -26,8 +26,8 @@ dependencies {
   compile 'org.codehaus.groovy:groovy:2.4.4:indy'
 }
 
-compileJava.options.compilerArgs << '-Xlint:-rawtypes,-unchecked,-cast,-deprecation'
-compileTestJava.options.compilerArgs << '-Xlint:-rawtypes,-unchecked,-cast,-deprecation'
+compileJava.options.compilerArgs << '-Xlint:-rawtypes,-unchecked,-cast'
+compileTestJava.options.compilerArgs << '-Xlint:-rawtypes,-unchecked,-cast'
 
 integTest {
   cluster {
diff --git a/plugins/analysis-icu/build.gradle b/plugins/analysis-icu/build.gradle
index a662f72..9ed155b 100644
--- a/plugins/analysis-icu/build.gradle
+++ b/plugins/analysis-icu/build.gradle
@@ -30,6 +30,3 @@ dependencies {
 dependencyLicenses {
   mapping from: /lucene-.*/, to: 'lucene'
 }
-
-compileJava.options.compilerArgs << "-Xlint:-deprecation"
-
diff --git a/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuCollationTokenFilterFactory.java b/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuCollationTokenFilterFactory.java
index 1e7cd1b..a8b71f3 100644
--- a/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuCollationTokenFilterFactory.java
+++ b/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuCollationTokenFilterFactory.java
@@ -19,18 +19,19 @@
 
 package org.elasticsearch.index.analysis;
 
-import com.ibm.icu.text.Collator;
-import com.ibm.icu.text.RuleBasedCollator;
-import com.ibm.icu.util.ULocale;
+import java.io.IOException;
+import java.nio.charset.Charset;
+import java.nio.file.Files;
+
 import org.apache.lucene.analysis.TokenStream;
 import org.elasticsearch.common.io.Streams;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.IndexSettings;
 
-import java.io.IOException;
-import java.nio.charset.Charset;
-import java.nio.file.Files;
+import com.ibm.icu.text.Collator;
+import com.ibm.icu.text.RuleBasedCollator;
+import com.ibm.icu.util.ULocale;
 
 /**
  * An ICU based collation token filter. There are two ways to configure collation:
@@ -45,6 +46,7 @@ public class IcuCollationTokenFilterFactory extends AbstractTokenFilterFactory {
 
     private final Collator collator;
 
+    @SuppressWarnings("deprecation") // Intentionally sets deprecated options for backwards compatibility
     public IcuCollationTokenFilterFactory(IndexSettings indexSettings, Environment environment, String name, Settings settings) {
         super(indexSettings, name, settings);
 
@@ -165,6 +167,7 @@ public class IcuCollationTokenFilterFactory extends AbstractTokenFilterFactory {
     }
 
     @Override
+    @SuppressWarnings("deprecation") // Constructs a deprecated filter for backwards compatibility
     public TokenStream create(TokenStream tokenStream) {
         return new ICUCollationKeyFilter(tokenStream, collator);
     }
diff --git a/plugins/discovery-azure/build.gradle b/plugins/discovery-azure/build.gradle
index 12b479e..f6b56c9 100644
--- a/plugins/discovery-azure/build.gradle
+++ b/plugins/discovery-azure/build.gradle
@@ -57,8 +57,6 @@ dependencyLicenses {
 }
 
 compileJava.options.compilerArgs << '-Xlint:-path,-serial,-unchecked'
-// TODO: why is deprecation needed here but not in maven....?
-compileJava.options.compilerArgs << '-Xlint:-deprecation'
 
 thirdPartyAudit.excludes = [
   // classes are missing
diff --git a/plugins/discovery-ec2/build.gradle b/plugins/discovery-ec2/build.gradle
index 403b263..e8baa1e 100644
--- a/plugins/discovery-ec2/build.gradle
+++ b/plugins/discovery-ec2/build.gradle
@@ -42,8 +42,6 @@ dependencyLicenses {
   mapping from: /jackson-.*/, to: 'jackson'
 }
 
-compileJava.options.compilerArgs << '-Xlint:-rawtypes,-deprecation'
-
 test {
   // this is needed for insecure plugins, remove if possible!
   systemProperty 'tests.artifact', project.name 
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java
index d71d9df..e97dd94 100644
--- a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.cloud.aws;
 
 import com.amazonaws.services.ec2.AmazonEC2;
+
 import org.elasticsearch.common.component.LifecycleComponent;
 
 public interface AwsEc2Service extends LifecycleComponent<AwsEc2Service> {
@@ -33,10 +34,6 @@ public interface AwsEc2Service extends LifecycleComponent<AwsEc2Service> {
         public static final String PROXY_PASSWORD = "cloud.aws.proxy.password";
         public static final String SIGNER = "cloud.aws.signer";
         public static final String REGION = "cloud.aws.region";
-        @Deprecated
-        public static final String DEPRECATED_PROXY_HOST = "cloud.aws.proxy_host";
-        @Deprecated
-        public static final String DEPRECATED_PROXY_PORT = "cloud.aws.proxy_port";
     }
 
     final class CLOUD_EC2 {
@@ -49,10 +46,6 @@ public interface AwsEc2Service extends LifecycleComponent<AwsEc2Service> {
         public static final String PROXY_PASSWORD = "cloud.aws.ec2.proxy.password";
         public static final String SIGNER = "cloud.aws.ec2.signer";
         public static final String ENDPOINT = "cloud.aws.ec2.endpoint";
-        @Deprecated
-        public static final String DEPRECATED_PROXY_HOST = "cloud.aws.ec2.proxy_host";
-        @Deprecated
-        public static final String DEPRECATED_PROXY_PORT = "cloud.aws.ec2.proxy_port";
     }
 
     final class DISCOVERY_EC2 {
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java
index 349a513..4830945 100644
--- a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java
@@ -33,6 +33,7 @@ import com.amazonaws.internal.StaticCredentialsProvider;
 import com.amazonaws.retry.RetryPolicy;
 import com.amazonaws.services.ec2.AmazonEC2;
 import com.amazonaws.services.ec2.AmazonEC2Client;
+
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.cloud.aws.network.Ec2NameResolver;
 import org.elasticsearch.cloud.aws.node.Ec2CustomNodeAttributes;
@@ -71,6 +72,7 @@ public class AwsEc2ServiceImpl extends AbstractLifecycleComponent<AwsEc2Service>
         discoveryNodeService.addCustomAttributeProvider(new Ec2CustomNodeAttributes(settings));
     }
 
+    @Override
     public synchronized AmazonEC2 client() {
         if (client != null) {
             return client;
@@ -91,11 +93,11 @@ public class AwsEc2ServiceImpl extends AbstractLifecycleComponent<AwsEc2Service>
         String account = settings.get(CLOUD_EC2.KEY, settings.get(CLOUD_AWS.KEY));
         String key = settings.get(CLOUD_EC2.SECRET, settings.get(CLOUD_AWS.SECRET));
 
-        String proxyHost = settings.get(CLOUD_AWS.PROXY_HOST, settings.get(CLOUD_AWS.DEPRECATED_PROXY_HOST));
-        proxyHost = settings.get(CLOUD_EC2.PROXY_HOST, settings.get(CLOUD_EC2.DEPRECATED_PROXY_HOST, proxyHost));
+        String proxyHost = settings.get(CLOUD_AWS.PROXY_HOST);
+        proxyHost = settings.get(CLOUD_EC2.PROXY_HOST, proxyHost);
         if (proxyHost != null) {
-            String portString = settings.get(CLOUD_AWS.PROXY_PORT, settings.get(CLOUD_AWS.DEPRECATED_PROXY_PORT, "80"));
-            portString = settings.get(CLOUD_EC2.PROXY_PORT, settings.get(CLOUD_EC2.DEPRECATED_PROXY_PORT, portString));
+            String portString = settings.get(CLOUD_AWS.PROXY_PORT, "80");
+            portString = settings.get(CLOUD_EC2.PROXY_PORT, portString);
             Integer proxyPort;
             try {
                 proxyPort = Integer.parseInt(portString, 10);
@@ -135,7 +137,7 @@ public class AwsEc2ServiceImpl extends AbstractLifecycleComponent<AwsEc2Service>
                                                      int retriesAttempted) {
                         // with 10 retries the max delay time is 320s/320000ms (10 * 2^5 * 1 * 1000)
                         logger.warn("EC2 API request failed, retry again. Reason was:", exception);
-                        return 1000L * (long) (10d * Math.pow(2, ((double) retriesAttempted) / 2.0d) * (1.0d + rand.nextDouble()));
+                        return 1000L * (long) (10d * Math.pow(2, retriesAttempted / 2.0d) * (1.0d + rand.nextDouble()));
                     }
                 },
                 10,
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/plugin/discovery/ec2/Ec2DiscoveryPlugin.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/plugin/discovery/ec2/Ec2DiscoveryPlugin.java
index ffa76c6..3b3d206 100644
--- a/plugins/discovery-ec2/src/main/java/org/elasticsearch/plugin/discovery/ec2/Ec2DiscoveryPlugin.java
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/plugin/discovery/ec2/Ec2DiscoveryPlugin.java
@@ -19,6 +19,11 @@
 
 package org.elasticsearch.plugin.discovery.ec2;
 
+import java.security.AccessController;
+import java.security.PrivilegedAction;
+import java.util.ArrayList;
+import java.util.Collection;
+
 import org.elasticsearch.SpecialPermission;
 import org.elasticsearch.cloud.aws.AwsEc2ServiceImpl;
 import org.elasticsearch.cloud.aws.Ec2Module;
@@ -32,16 +37,11 @@ import org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider;
 import org.elasticsearch.discovery.ec2.Ec2Discovery;
 import org.elasticsearch.plugins.Plugin;
 
-import java.security.AccessController;
-import java.security.PrivilegedAction;
-import java.util.ArrayList;
-import java.util.Collection;
-
 /**
  *
  */
 public class Ec2DiscoveryPlugin extends Plugin {
-  
+
     // ClientConfiguration clinit has some classloader problems
     // TODO: fix that
     static {
@@ -87,6 +87,7 @@ public class Ec2DiscoveryPlugin extends Plugin {
     }
 
     @Override
+    @SuppressWarnings("rawtypes") // Supertype uses rawtype
     public Collection<Class<? extends LifecycleComponent>> nodeServices() {
         Collection<Class<? extends LifecycleComponent>> services = new ArrayList<>();
         services.add(AwsEc2ServiceImpl.class);
diff --git a/plugins/discovery-multicast/build.gradle b/plugins/discovery-multicast/build.gradle
index f48f628..295f28c 100644
--- a/plugins/discovery-multicast/build.gradle
+++ b/plugins/discovery-multicast/build.gradle
@@ -21,5 +21,3 @@ esplugin {
   description 'The Multicast Discovery plugin allows discovery other nodes using multicast requests'
   classname 'org.elasticsearch.plugin.discovery.multicast.MulticastDiscoveryPlugin'
 }
-
-compileJava.options.compilerArgs << "-Xlint:-deprecation"
diff --git a/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java b/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java
index 82bf1bf..81f8ed2 100644
--- a/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java
+++ b/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java
@@ -19,7 +19,18 @@
 
 package org.elasticsearch.plugin.discovery.multicast;
 
-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+import java.io.IOException;
+import java.net.InetAddress;
+import java.net.SocketAddress;
+import java.security.AccessController;
+import java.security.PrivilegedExceptionAction;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.atomic.AtomicReference;
+
 import org.apache.lucene.util.Constants;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.SpecialPermission;
@@ -55,17 +66,7 @@ import org.elasticsearch.transport.TransportRequestHandler;
 import org.elasticsearch.transport.TransportResponse;
 import org.elasticsearch.transport.TransportService;
 
-import java.io.IOException;
-import java.net.InetAddress;
-import java.net.SocketAddress;
-import java.security.AccessController;
-import java.security.PrivilegedExceptionAction;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.atomic.AtomicReference;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
 
 import static org.elasticsearch.cluster.node.DiscoveryNode.readNode;
 import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
@@ -144,13 +145,9 @@ public class MulticastZenPing extends AbstractLifecycleComponent<ZenPing> implem
             boolean shared = settings.getAsBoolean("discovery.zen.ping.multicast.shared", Constants.MAC_OS_X);
             // OSX does not correctly send multicasts FROM the right interface
             boolean deferToInterface = settings.getAsBoolean("discovery.zen.ping.multicast.defer_group_to_set_interface", Constants.MAC_OS_X);
-            // don't use publish address, the use case for that is e.g. a firewall or proxy and
-            // may not even be bound to an interface on this machine! use the first bound address.
-            List<InetAddress> addresses = Arrays.asList(networkService.resolveBindHostAddresses(address == null ? null : new String[] { address }));
-            NetworkUtils.sortAddresses(addresses);
 
             final MulticastChannel.Config config = new MulticastChannel.Config(port, group, bufferSize, ttl,
-                                                                               addresses.get(0), deferToInterface);
+                    getMulticastInterface(), deferToInterface);
             SecurityManager sm = System.getSecurityManager();
             if (sm != null) {
                 sm.checkPermission(new SpecialPermission());
@@ -167,6 +164,16 @@ public class MulticastZenPing extends AbstractLifecycleComponent<ZenPing> implem
         }
     }
 
+
+    @SuppressWarnings("deprecation") // Used to support funky configuration options
+    private InetAddress getMulticastInterface() throws IOException {
+        // don't use publish address, the use case for that is e.g. a firewall or proxy and
+        // may not even be bound to an interface on this machine! use the first bound address.
+        List<InetAddress> addresses = Arrays.asList(networkService.resolveBindHostAddresses(address == null ? null : new String[] { address }));
+        NetworkUtils.sortAddresses(addresses);
+        return addresses.get(0);
+    }
+
     @Override
     protected void doStop() {
         if (multicastChannel != null) {
diff --git a/plugins/ingest-geoip/build.gradle b/plugins/ingest-geoip/build.gradle
deleted file mode 100644
index 7eee668..0000000
--- a/plugins/ingest-geoip/build.gradle
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-esplugin {
-  description 'Ingest processor that uses looksup geo data based on ip adresses using the Maxmind geo database'
-  classname 'org.elasticsearch.ingest.geoip.IngestGeoIpPlugin'
-}
-
-dependencies {
-  compile ('com.maxmind.geoip2:geoip2:2.4.0')
-  // geoip2 dependencies:
-  compile('com.fasterxml.jackson.core:jackson-annotations:2.5.0')
-  compile('com.fasterxml.jackson.core:jackson-databind:2.5.3')
-  compile('com.maxmind.db:maxmind-db:1.0.1')
-
-  testCompile 'org.elasticsearch:geolite2-databases:20151029'
-}
-
-task copyDefaultGeoIp2DatabaseFiles(type: Copy) {
-  from { zipTree(configurations.testCompile.files.find { it.name.contains('geolite2-databases')}) }
-  into "${project.buildDir}/ingest-geoip"
-  include "*.mmdb"
-}
-
-project.bundlePlugin.dependsOn(copyDefaultGeoIp2DatabaseFiles)
-
-compileJava.options.compilerArgs << "-Xlint:-rawtypes,-unchecked,-serial"
-compileTestJava.options.compilerArgs << "-Xlint:-rawtypes,-unchecked"
-
-bundlePlugin {
-  from("${project.buildDir}/ingest-geoip") {
-    into 'config/'
-  }
-}
-
-thirdPartyAudit.excludes = [
-  // geoip WebServiceClient needs Google http client, but we're not using WebServiceClient:
-  'com.google.api.client.http.HttpTransport',
-  'com.google.api.client.http.GenericUrl',
-  'com.google.api.client.http.HttpResponse',
-  'com.google.api.client.http.HttpRequestFactory',
-  'com.google.api.client.http.HttpRequest',
-  'com.google.api.client.http.HttpHeaders',
-  'com.google.api.client.http.HttpResponseException',
-  'com.google.api.client.http.javanet.NetHttpTransport',
-  'com.google.api.client.http.javanet.NetHttpTransport',
-]
\ No newline at end of file
diff --git a/plugins/ingest-geoip/licenses/geoip2-2.4.0.jar.sha1 b/plugins/ingest-geoip/licenses/geoip2-2.4.0.jar.sha1
deleted file mode 100644
index 485286f..0000000
--- a/plugins/ingest-geoip/licenses/geoip2-2.4.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-ad40667ae87138e0aed075d2c15884497fa64acc
\ No newline at end of file
diff --git a/plugins/ingest-geoip/licenses/geoip2-LICENSE.txt b/plugins/ingest-geoip/licenses/geoip2-LICENSE.txt
deleted file mode 100644
index 7a4a3ea..0000000
--- a/plugins/ingest-geoip/licenses/geoip2-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
\ No newline at end of file
diff --git a/plugins/ingest-geoip/licenses/geoip2-NOTICE.txt b/plugins/ingest-geoip/licenses/geoip2-NOTICE.txt
deleted file mode 100644
index 448b71d..0000000
--- a/plugins/ingest-geoip/licenses/geoip2-NOTICE.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-This software is Copyright (c) 2013 by MaxMind, Inc.
-
-This is free software, licensed under the Apache License, Version 2.0.
\ No newline at end of file
diff --git a/plugins/ingest-geoip/licenses/jackson-annotations-2.5.0.jar.sha1 b/plugins/ingest-geoip/licenses/jackson-annotations-2.5.0.jar.sha1
deleted file mode 100644
index 862ac6f..0000000
--- a/plugins/ingest-geoip/licenses/jackson-annotations-2.5.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-a2a55a3375bc1cef830ca426d68d2ea22961190e
diff --git a/plugins/ingest-geoip/licenses/jackson-annotations-LICENSE b/plugins/ingest-geoip/licenses/jackson-annotations-LICENSE
deleted file mode 100644
index f5f45d2..0000000
--- a/plugins/ingest-geoip/licenses/jackson-annotations-LICENSE
+++ /dev/null
@@ -1,8 +0,0 @@
-This copy of Jackson JSON processor streaming parser/generator is licensed under the
-Apache (Software) License, version 2.0 ("the License").
-See the License for details about distribution rights, and the
-specific rights regarding derivate works.
-
-You may obtain a copy of the License at:
-
-http://www.apache.org/licenses/LICENSE-2.0
diff --git a/plugins/ingest-geoip/licenses/jackson-annotations-NOTICE b/plugins/ingest-geoip/licenses/jackson-annotations-NOTICE
deleted file mode 100644
index 4c976b7..0000000
--- a/plugins/ingest-geoip/licenses/jackson-annotations-NOTICE
+++ /dev/null
@@ -1,20 +0,0 @@
-# Jackson JSON processor
-
-Jackson is a high-performance, Free/Open Source JSON processing library.
-It was originally written by Tatu Saloranta (tatu.saloranta@iki.fi), and has
-been in development since 2007.
-It is currently developed by a community of developers, as well as supported
-commercially by FasterXML.com.
-
-## Licensing
-
-Jackson core and extension components may licensed under different licenses.
-To find the details that apply to this artifact see the accompanying LICENSE file.
-For more information, including possible other licensing options, contact
-FasterXML.com (http://fasterxml.com).
-
-## Credits
-
-A list of contributors may be found from CREDITS file, which is included
-in some artifacts (usually source distributions); but is always available
-from the source code management (SCM) system project uses.
diff --git a/plugins/ingest-geoip/licenses/jackson-databind-2.5.3.jar.sha1 b/plugins/ingest-geoip/licenses/jackson-databind-2.5.3.jar.sha1
deleted file mode 100644
index cdc6695..0000000
--- a/plugins/ingest-geoip/licenses/jackson-databind-2.5.3.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-c37875ff66127d93e5f672708cb2dcc14c8232ab
diff --git a/plugins/ingest-geoip/licenses/jackson-databind-LICENSE b/plugins/ingest-geoip/licenses/jackson-databind-LICENSE
deleted file mode 100644
index f5f45d2..0000000
--- a/plugins/ingest-geoip/licenses/jackson-databind-LICENSE
+++ /dev/null
@@ -1,8 +0,0 @@
-This copy of Jackson JSON processor streaming parser/generator is licensed under the
-Apache (Software) License, version 2.0 ("the License").
-See the License for details about distribution rights, and the
-specific rights regarding derivate works.
-
-You may obtain a copy of the License at:
-
-http://www.apache.org/licenses/LICENSE-2.0
diff --git a/plugins/ingest-geoip/licenses/jackson-databind-NOTICE b/plugins/ingest-geoip/licenses/jackson-databind-NOTICE
deleted file mode 100644
index 4c976b7..0000000
--- a/plugins/ingest-geoip/licenses/jackson-databind-NOTICE
+++ /dev/null
@@ -1,20 +0,0 @@
-# Jackson JSON processor
-
-Jackson is a high-performance, Free/Open Source JSON processing library.
-It was originally written by Tatu Saloranta (tatu.saloranta@iki.fi), and has
-been in development since 2007.
-It is currently developed by a community of developers, as well as supported
-commercially by FasterXML.com.
-
-## Licensing
-
-Jackson core and extension components may licensed under different licenses.
-To find the details that apply to this artifact see the accompanying LICENSE file.
-For more information, including possible other licensing options, contact
-FasterXML.com (http://fasterxml.com).
-
-## Credits
-
-A list of contributors may be found from CREDITS file, which is included
-in some artifacts (usually source distributions); but is always available
-from the source code management (SCM) system project uses.
diff --git a/plugins/ingest-geoip/licenses/maxmind-db-1.0.1.jar.sha1 b/plugins/ingest-geoip/licenses/maxmind-db-1.0.1.jar.sha1
deleted file mode 100644
index 6cb749e..0000000
--- a/plugins/ingest-geoip/licenses/maxmind-db-1.0.1.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-305429b84dbcd1cc3d393686f412cdcaec9cdbe6
\ No newline at end of file
diff --git a/plugins/ingest-geoip/licenses/maxmind-db-LICENSE.txt b/plugins/ingest-geoip/licenses/maxmind-db-LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/ingest-geoip/licenses/maxmind-db-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/ingest-geoip/licenses/maxmind-db-NOTICE.txt b/plugins/ingest-geoip/licenses/maxmind-db-NOTICE.txt
deleted file mode 100644
index 1ebe2b0..0000000
--- a/plugins/ingest-geoip/licenses/maxmind-db-NOTICE.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-This software is Copyright (c) 2014 by MaxMind, Inc.
-
-This is free software, licensed under the Apache License, Version 2.0.
diff --git a/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/GeoIpProcessor.java b/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/GeoIpProcessor.java
deleted file mode 100644
index 9f65c76..0000000
--- a/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/GeoIpProcessor.java
+++ /dev/null
@@ -1,319 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.geoip;
-
-import com.maxmind.geoip2.DatabaseReader;
-import com.maxmind.geoip2.exception.AddressNotFoundException;
-import com.maxmind.geoip2.model.CityResponse;
-import com.maxmind.geoip2.model.CountryResponse;
-import com.maxmind.geoip2.record.City;
-import com.maxmind.geoip2.record.Continent;
-import com.maxmind.geoip2.record.Country;
-import com.maxmind.geoip2.record.Location;
-import com.maxmind.geoip2.record.Subdivision;
-import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.SpecialPermission;
-import org.elasticsearch.common.network.InetAddresses;
-import org.elasticsearch.common.network.NetworkAddress;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.io.InputStream;
-import java.net.InetAddress;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.nio.file.PathMatcher;
-import java.nio.file.StandardOpenOption;
-import java.security.AccessController;
-import java.security.PrivilegedAction;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.EnumSet;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Set;
-import java.util.stream.Stream;
-
-import static org.elasticsearch.ingest.core.ConfigurationUtils.readOptionalList;
-import static org.elasticsearch.ingest.core.ConfigurationUtils.readStringProperty;
-
-public final class GeoIpProcessor implements Processor {
-
-    public static final String TYPE = "geoip";
-
-    private final String sourceField;
-    private final String targetField;
-    private final DatabaseReader dbReader;
-    private final Set<Field> fields;
-
-    GeoIpProcessor(String sourceField, DatabaseReader dbReader, String targetField, Set<Field> fields) throws IOException {
-        this.sourceField = sourceField;
-        this.targetField = targetField;
-        this.dbReader = dbReader;
-        this.fields = fields;
-    }
-
-    @Override
-    public void execute(IngestDocument ingestDocument) {
-        String ip = ingestDocument.getFieldValue(sourceField, String.class);
-        final InetAddress ipAddress = InetAddresses.forString(ip);
-
-        Map<String, Object> geoData;
-        switch (dbReader.getMetadata().getDatabaseType()) {
-            case "GeoLite2-City":
-                try {
-                    geoData = retrieveCityGeoData(ipAddress);
-                } catch (AddressNotFoundRuntimeException e) {
-                    geoData = Collections.emptyMap();
-                }
-                break;
-            case "GeoLite2-Country":
-                try {
-                    geoData = retrieveCountryGeoData(ipAddress);
-                } catch (AddressNotFoundRuntimeException e) {
-                    geoData = Collections.emptyMap();
-                }
-                break;
-            default:
-                throw new IllegalStateException("Unsupported database type [" + dbReader.getMetadata().getDatabaseType() + "]");
-        }
-        ingestDocument.setFieldValue(targetField, geoData);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    String getSourceField() {
-        return sourceField;
-    }
-
-    String getTargetField() {
-        return targetField;
-    }
-
-    DatabaseReader getDbReader() {
-        return dbReader;
-    }
-
-    Set<Field> getFields() {
-        return fields;
-    }
-
-    private Map<String, Object> retrieveCityGeoData(InetAddress ipAddress) {
-        SecurityManager sm = System.getSecurityManager();
-        if (sm != null) {
-            sm.checkPermission(new SpecialPermission());
-        }
-        CityResponse response = AccessController.doPrivileged((PrivilegedAction<CityResponse>) () -> {
-            try {
-                return dbReader.city(ipAddress);
-            } catch (AddressNotFoundException e) {
-                throw new AddressNotFoundRuntimeException(e);
-            } catch (Exception e) {
-                throw new RuntimeException(e);
-            }
-        });
-
-        Country country = response.getCountry();
-        City city = response.getCity();
-        Location location = response.getLocation();
-        Continent continent = response.getContinent();
-        Subdivision subdivision = response.getMostSpecificSubdivision();
-
-        Map<String, Object> geoData = new HashMap<>();
-        for (Field field : fields) {
-            switch (field) {
-                case IP:
-                    geoData.put("ip", NetworkAddress.formatAddress(ipAddress));
-                    break;
-                case COUNTRY_ISO_CODE:
-                    geoData.put("country_iso_code", country.getIsoCode());
-                    break;
-                case COUNTRY_NAME:
-                    geoData.put("country_name", country.getName());
-                    break;
-                case CONTINENT_NAME:
-                    geoData.put("continent_name", continent.getName());
-                    break;
-                case REGION_NAME:
-                    geoData.put("region_name", subdivision.getName());
-                    break;
-                case CITY_NAME:
-                    geoData.put("city_name", city.getName());
-                    break;
-                case TIMEZONE:
-                    geoData.put("timezone", location.getTimeZone());
-                    break;
-                case LATITUDE:
-                    geoData.put("latitude", location.getLatitude());
-                    break;
-                case LONGITUDE:
-                    geoData.put("longitude", location.getLongitude());
-                    break;
-                case LOCATION:
-                    if (location.getLatitude() != null && location.getLongitude() != null) {
-                        geoData.put("location", Arrays.asList(location.getLongitude(), location.getLatitude()));
-                    }
-                    break;
-            }
-        }
-        return geoData;
-    }
-
-    private Map<String, Object> retrieveCountryGeoData(InetAddress ipAddress) {
-        SecurityManager sm = System.getSecurityManager();
-        if (sm != null) {
-            sm.checkPermission(new SpecialPermission());
-        }
-        CountryResponse response = AccessController.doPrivileged((PrivilegedAction<CountryResponse>) () -> {
-            try {
-                return dbReader.country(ipAddress);
-            } catch (AddressNotFoundException e) {
-                throw new AddressNotFoundRuntimeException(e);
-            } catch (Exception e) {
-                throw new RuntimeException(e);
-            }
-        });
-
-        Country country = response.getCountry();
-        Continent continent = response.getContinent();
-
-        Map<String, Object> geoData = new HashMap<>();
-        for (Field field : fields) {
-            switch (field) {
-                case IP:
-                    geoData.put("ip", NetworkAddress.formatAddress(ipAddress));
-                    break;
-                case COUNTRY_ISO_CODE:
-                    geoData.put("country_iso_code", country.getIsoCode());
-                    break;
-                case COUNTRY_NAME:
-                    geoData.put("country_name", country.getName());
-                    break;
-                case CONTINENT_NAME:
-                    geoData.put("continent_name", continent.getName());
-                    break;
-            }
-        }
-        return geoData;
-    }
-
-    public static final class Factory implements Processor.Factory<GeoIpProcessor>, Closeable {
-
-        static final Set<Field> DEFAULT_FIELDS = EnumSet.of(
-                Field.CONTINENT_NAME, Field.COUNTRY_ISO_CODE, Field.REGION_NAME, Field.CITY_NAME, Field.LOCATION
-        );
-
-        private final Map<String, DatabaseReader> databaseReaders;
-
-        public Factory(Path configDirectory) {
-            Path geoIpConfigDirectory = configDirectory.resolve("ingest-geoip");
-            if (Files.exists(geoIpConfigDirectory) == false && Files.isDirectory(geoIpConfigDirectory)) {
-                throw new IllegalStateException("the geoip directory [" + geoIpConfigDirectory  + "] containing databases doesn't exist");
-            }
-
-            try (Stream<Path> databaseFiles = Files.list(geoIpConfigDirectory)) {
-                Map<String, DatabaseReader> databaseReaders = new HashMap<>();
-                PathMatcher pathMatcher = geoIpConfigDirectory.getFileSystem().getPathMatcher("glob:**.mmdb");
-                // Use iterator instead of forEach otherwise IOException needs to be caught twice...
-                Iterator<Path> iterator = databaseFiles.iterator();
-                while (iterator.hasNext()) {
-                    Path databasePath = iterator.next();
-                    if (Files.isRegularFile(databasePath) && pathMatcher.matches(databasePath)) {
-                        try (InputStream inputStream = Files.newInputStream(databasePath, StandardOpenOption.READ)) {
-                            databaseReaders.put(databasePath.getFileName().toString(), new DatabaseReader.Builder(inputStream).build());
-                        }
-                    }
-                }
-                this.databaseReaders = Collections.unmodifiableMap(databaseReaders);
-            } catch (IOException e) {
-                throw new RuntimeException(e);
-            }
-        }
-
-        public GeoIpProcessor create(Map<String, Object> config) throws Exception {
-            String ipField = readStringProperty(config, "source_field");
-            String targetField = readStringProperty(config, "target_field", "geoip");
-            String databaseFile = readStringProperty(config, "database_file", "GeoLite2-City.mmdb");
-            List<String> fieldNames = readOptionalList(config, "fields");
-
-            final Set<Field> fields;
-            if (fieldNames != null) {
-                fields = EnumSet.noneOf(Field.class);
-                for (String fieldName : fieldNames) {
-                    try {
-                        fields.add(Field.parse(fieldName));
-                    } catch (Exception e) {
-                        throw new IllegalArgumentException("illegal field option [" + fieldName +"]. valid values are [" + Arrays.toString(Field.values()) +"]", e);
-                    }
-                }
-            } else {
-                fields = DEFAULT_FIELDS;
-            }
-
-            DatabaseReader databaseReader = databaseReaders.get(databaseFile);
-            if (databaseReader == null) {
-                throw new IllegalArgumentException("database file [" + databaseFile + "] doesn't exist");
-            }
-            return new GeoIpProcessor(ipField, databaseReader, targetField, fields);
-        }
-
-        @Override
-        public void close() throws IOException {
-            IOUtils.close(databaseReaders.values());
-        }
-    }
-
-    // Geoip2's AddressNotFoundException is checked and due to the fact that we need run their code
-    // inside a PrivilegedAction code block, we are forced to catch any checked exception and rethrow
-    // it with an unchecked exception.
-    private final static class AddressNotFoundRuntimeException extends RuntimeException {
-
-        public AddressNotFoundRuntimeException(Throwable cause) {
-            super(cause);
-        }
-    }
-
-    public enum Field {
-
-        IP,
-        COUNTRY_ISO_CODE,
-        COUNTRY_NAME,
-        CONTINENT_NAME,
-        REGION_NAME,
-        CITY_NAME,
-        TIMEZONE,
-        LATITUDE,
-        LONGITUDE,
-        LOCATION;
-
-        public static Field parse(String value) {
-            return valueOf(value.toUpperCase(Locale.ROOT));
-        }
-    }
-
-}
diff --git a/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/IngestGeoIpPlugin.java b/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/IngestGeoIpPlugin.java
deleted file mode 100644
index 81cecb7..0000000
--- a/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/IngestGeoIpPlugin.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.geoip;
-
-import org.elasticsearch.ingest.IngestModule;
-import org.elasticsearch.plugins.Plugin;
-
-public class IngestGeoIpPlugin extends Plugin {
-
-    @Override
-    public String name() {
-        return "ingest-geoip";
-    }
-
-    @Override
-    public String description() {
-        return "Plugin that allows to plug in ingest processors";
-    }
-
-    public void onModule(IngestModule ingestModule) {
-        ingestModule.registerProcessor(GeoIpProcessor.TYPE, (environment, templateService) -> new GeoIpProcessor.Factory(environment.configFile()));
-    }
-}
diff --git a/plugins/ingest-geoip/src/main/plugin-metadata/plugin-security.policy b/plugins/ingest-geoip/src/main/plugin-metadata/plugin-security.policy
deleted file mode 100644
index f49d15d..0000000
--- a/plugins/ingest-geoip/src/main/plugin-metadata/plugin-security.policy
+++ /dev/null
@@ -1,27 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-grant {
-  // needed because jackson-databind is using Class#getDeclaredConstructors(), Class#getDeclaredMethods() and
-  // Class#getDeclaredAnnotations() to find all public, private, protected, package protected and
-  // private constructors, methods or annotations. Just locating all public constructors, methods and annotations
-  // should be enough, so this permission wouldn't then be needed. Unfortunately this is not what jackson-databind does
-  // or can be configured to do.
-  permission java.lang.RuntimePermission "accessDeclaredMembers";
-};
diff --git a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorFactoryTests.java b/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorFactoryTests.java
deleted file mode 100644
index 78dd86d..0000000
--- a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorFactoryTests.java
+++ /dev/null
@@ -1,144 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.geoip;
-
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.StreamsUtils;
-import org.junit.Before;
-
-import java.io.ByteArrayInputStream;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.EnumSet;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Set;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.sameInstance;
-
-public class GeoIpProcessorFactoryTests extends ESTestCase {
-
-    private Path configDir;
-
-    @Before
-    public void prepareConfigDirectory() throws Exception {
-        this.configDir = createTempDir();
-        Path geoIpConfigDir = configDir.resolve("ingest-geoip");
-        Files.createDirectories(geoIpConfigDir);
-        Files.copy(new ByteArrayInputStream(StreamsUtils.copyToBytesFromClasspath("/GeoLite2-City.mmdb")), geoIpConfigDir.resolve("GeoLite2-City.mmdb"));
-        Files.copy(new ByteArrayInputStream(StreamsUtils.copyToBytesFromClasspath("/GeoLite2-Country.mmdb")), geoIpConfigDir.resolve("GeoLite2-Country.mmdb"));
-    }
-
-    public void testBuild_defaults() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(configDir);
-
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-
-        GeoIpProcessor processor = factory.create(config);
-        assertThat(processor.getSourceField(), equalTo("_field"));
-        assertThat(processor.getTargetField(), equalTo("geoip"));
-        assertThat(processor.getDbReader().getMetadata().getDatabaseType(), equalTo("GeoLite2-City"));
-        assertThat(processor.getFields(), sameInstance(GeoIpProcessor.Factory.DEFAULT_FIELDS));
-    }
-
-    public void testBuild_targetField() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(configDir);
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("target_field", "_field");
-        GeoIpProcessor processor = factory.create(config);
-        assertThat(processor.getSourceField(), equalTo("_field"));
-        assertThat(processor.getTargetField(), equalTo("_field"));
-    }
-
-    public void testBuild_dbFile() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(configDir);
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("database_file", "GeoLite2-Country.mmdb");
-        GeoIpProcessor processor = factory.create(config);
-        assertThat(processor.getSourceField(), equalTo("_field"));
-        assertThat(processor.getTargetField(), equalTo("geoip"));
-        assertThat(processor.getDbReader().getMetadata().getDatabaseType(), equalTo("GeoLite2-Country"));
-    }
-
-    public void testBuild_nonExistingDbFile() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(configDir);
-
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("database_file", "does-not-exist.mmdb");
-        try {
-            factory.create(config);
-            fail("Exception expected");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("database file [does-not-exist.mmdb] doesn't exist"));
-        }
-    }
-
-    public void testBuild_fields() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(configDir);
-
-        Set<GeoIpProcessor.Field> fields = EnumSet.noneOf(GeoIpProcessor.Field.class);
-        List<String> fieldNames = new ArrayList<>();
-        int numFields = scaledRandomIntBetween(1, GeoIpProcessor.Field.values().length);
-        for (int i = 0; i < numFields; i++) {
-            GeoIpProcessor.Field field = GeoIpProcessor.Field.values()[i];
-            fields.add(field);
-            fieldNames.add(field.name().toLowerCase(Locale.ROOT));
-        }
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("fields", fieldNames);
-        GeoIpProcessor processor = factory.create(config);
-        assertThat(processor.getSourceField(), equalTo("_field"));
-        assertThat(processor.getFields(), equalTo(fields));
-    }
-
-    public void testBuild_illegalFieldOption() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(configDir);
-
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("fields", Collections.singletonList("invalid"));
-        try {
-            factory.create(config);
-            fail("exception expected");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("illegal field option [invalid]. valid values are [[IP, COUNTRY_ISO_CODE, COUNTRY_NAME, CONTINENT_NAME, REGION_NAME, CITY_NAME, TIMEZONE, LATITUDE, LONGITUDE, LOCATION]]"));
-        }
-
-        config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("fields", "invalid");
-        try {
-            factory.create(config);
-            fail("exception expected");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("property [fields] isn't a list, but of type [java.lang.String]"));
-        }
-    }
-}
diff --git a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorTests.java b/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorTests.java
deleted file mode 100644
index b3b4110..0000000
--- a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorTests.java
+++ /dev/null
@@ -1,112 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.geoip;
-
-import com.maxmind.geoip2.DatabaseReader;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.InputStream;
-import java.util.Arrays;
-import java.util.EnumSet;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class GeoIpProcessorTests extends ESTestCase {
-
-    public void testCity() throws Exception {
-        InputStream database = GeoIpProcessor.class.getResourceAsStream("/GeoLite2-City.mmdb");
-        GeoIpProcessor processor = new GeoIpProcessor("source_field", new DatabaseReader.Builder(database).build(), "target_field", EnumSet.allOf(GeoIpProcessor.Field.class));
-
-        Map<String, Object> document = new HashMap<>();
-        document.put("source_field", "82.170.213.79");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        processor.execute(ingestDocument);
-
-        assertThat(ingestDocument.getSourceAndMetadata().get("source_field"), equalTo("82.170.213.79"));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> geoData = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("target_field");
-        assertThat(geoData.size(), equalTo(10));
-        assertThat(geoData.get("ip"), equalTo("82.170.213.79"));
-        assertThat(geoData.get("country_iso_code"), equalTo("NL"));
-        assertThat(geoData.get("country_name"), equalTo("Netherlands"));
-        assertThat(geoData.get("continent_name"), equalTo("Europe"));
-        assertThat(geoData.get("region_name"), equalTo("North Holland"));
-        assertThat(geoData.get("city_name"), equalTo("Amsterdam"));
-        assertThat(geoData.get("timezone"), equalTo("Europe/Amsterdam"));
-        assertThat(geoData.get("latitude"), equalTo(52.374));
-        assertThat(geoData.get("longitude"), equalTo(4.8897));
-        assertThat(geoData.get("location"), equalTo(Arrays.asList(4.8897d, 52.374d)));
-    }
-
-    public void testCountry() throws Exception {
-        InputStream database = GeoIpProcessor.class.getResourceAsStream("/GeoLite2-Country.mmdb");
-        GeoIpProcessor processor = new GeoIpProcessor("source_field", new DatabaseReader.Builder(database).build(), "target_field", EnumSet.allOf(GeoIpProcessor.Field.class));
-
-        Map<String, Object> document = new HashMap<>();
-        document.put("source_field", "82.170.213.79");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        processor.execute(ingestDocument);
-
-        assertThat(ingestDocument.getSourceAndMetadata().get("source_field"), equalTo("82.170.213.79"));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> geoData = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("target_field");
-        assertThat(geoData.size(), equalTo(4));
-        assertThat(geoData.get("ip"), equalTo("82.170.213.79"));
-        assertThat(geoData.get("country_iso_code"), equalTo("NL"));
-        assertThat(geoData.get("country_name"), equalTo("Netherlands"));
-        assertThat(geoData.get("continent_name"), equalTo("Europe"));
-    }
-
-    public void testAddressIsNotInTheDatabase() throws Exception {
-        InputStream database = GeoIpProcessor.class.getResourceAsStream("/GeoLite2-City.mmdb");
-        GeoIpProcessor processor = new GeoIpProcessor("source_field", new DatabaseReader.Builder(database).build(), "target_field", EnumSet.allOf(GeoIpProcessor.Field.class));
-
-        Map<String, Object> document = new HashMap<>();
-        document.put("source_field", "202.45.11.11");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        processor.execute(ingestDocument);
-        @SuppressWarnings("unchecked")
-        Map<String, Object> geoData = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("target_field");
-        assertThat(geoData.size(), equalTo(0));
-    }
-
-    /** Don't silently do DNS lookups or anything trappy on bogus data */
-    public void testInvalid() throws Exception {
-        InputStream database = GeoIpProcessor.class.getResourceAsStream("/GeoLite2-City.mmdb");
-        GeoIpProcessor processor = new GeoIpProcessor("source_field", new DatabaseReader.Builder(database).build(), "target_field", EnumSet.allOf(GeoIpProcessor.Field.class));
-
-        Map<String, Object> document = new HashMap<>();
-        document.put("source_field", "www.google.com");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        try {
-            processor.execute(ingestDocument);
-            fail("did not get expected exception");
-        } catch (IllegalArgumentException expected) {
-            assertNotNull(expected.getMessage());
-            assertThat(expected.getMessage(), containsString("not an IP string literal"));
-        }
-    }
-
-}
diff --git a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/IngestGeoIpRestIT.java b/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/IngestGeoIpRestIT.java
deleted file mode 100644
index fed5345..0000000
--- a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/IngestGeoIpRestIT.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.geoip;
-
-import com.carrotsearch.randomizedtesting.annotations.Name;
-import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.rest.ESRestTestCase;
-import org.elasticsearch.test.rest.RestTestCandidate;
-import org.elasticsearch.test.rest.parser.RestTestParseException;
-
-import java.io.IOException;
-import java.util.Collection;
-
-public class IngestGeoIpRestIT extends ESRestTestCase {
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(IngestGeoIpPlugin.class);
-    }
-
-    public IngestGeoIpRestIT(@Name("yaml") RestTestCandidate testCandidate) {
-        super(testCandidate);
-    }
-
-    @ParametersFactory
-    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
-        return ESRestTestCase.createParameters(0, 1);
-    }
-}
-
diff --git a/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/10_basic.yaml b/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/10_basic.yaml
deleted file mode 100644
index 9a82d95..0000000
--- a/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/10_basic.yaml
+++ /dev/null
@@ -1,6 +0,0 @@
-"Ingest plugin installed":
-    - do:
-        cluster.stats: {}
-
-    - match:  { nodes.plugins.0.name: ingest-geoip  }
-    - match:  { nodes.plugins.0.jvm: true  }
diff --git a/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/20_geoip_processor.yaml b/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/20_geoip_processor.yaml
deleted file mode 100644
index 91e0c7a..0000000
--- a/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/20_geoip_processor.yaml
+++ /dev/null
@@ -1,124 +0,0 @@
----
-"Test geoip processor with defaults":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "geoip" : {
-                  "source_field" : "field1"
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field1: "128.101.101.101"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.field1: "128.101.101.101" }
-  - length: { _source.geoip: 5 }
-  - match: { _source.geoip.city_name: "Minneapolis" }
-  - match: { _source.geoip.country_iso_code: "US" }
-  - match: { _source.geoip.location: [-93.2166, 44.9759] }
-  - match: { _source.geoip.region_name: "Minnesota" }
-  - match: { _source.geoip.continent_name: "North America" }
-
----
-"Test geoip processor with fields":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "geoip" : {
-                  "source_field" : "field1",
-                  "fields" : ["city_name", "country_iso_code", "ip", "latitude", "longitude", "location", "timezone", "country_name", "region_name", "continent_name"]
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field1: "128.101.101.101"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.field1: "128.101.101.101" }
-  - length: { _source.geoip: 10 }
-  - match: { _source.geoip.city_name: "Minneapolis" }
-  - match: { _source.geoip.country_iso_code: "US" }
-  - match: { _source.geoip.ip: "128.101.101.101" }
-  - match: { _source.geoip.latitude: 44.9759 }
-  - match: { _source.geoip.longitude: -93.2166 }
-  - match: { _source.geoip.location: [-93.2166, 44.9759] }
-  - match: { _source.geoip.timezone: "America/Chicago" }
-  - match: { _source.geoip.country_name: "United States" }
-  - match: { _source.geoip.region_name: "Minnesota" }
-  - match: { _source.geoip.continent_name: "North America" }
-
----
-"Test geoip processor with different database file":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "geoip" : {
-                  "source_field" : "field1",
-                  "database_file" : "GeoLite2-Country.mmdb"
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field1: "128.101.101.101"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.field1: "128.101.101.101" }
-  - length: { _source.geoip: 2 }
-  - match: { _source.geoip.country_iso_code: "US" }
-  - match: { _source.geoip.continent_name: "North America" }
diff --git a/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTests.java b/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTests.java
index 2aa6e13..b6be9f6 100644
--- a/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTests.java
+++ b/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.script.javascript;
 
+import org.elasticsearch.common.Randomness;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.script.CompiledScript;
 import org.elasticsearch.script.ExecutableScript;
@@ -30,7 +31,6 @@ import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.CyclicBarrier;
-import java.util.concurrent.ThreadLocalRandom;
 import java.util.concurrent.atomic.AtomicBoolean;
 
 import static org.hamcrest.Matchers.equalTo;
@@ -53,8 +53,8 @@ public class JavaScriptScriptMultiThreadedTests extends ESTestCase {
                 public void run() {
                     try {
                         barrier.await();
-                        long x = ThreadLocalRandom.current().nextInt();
-                        long y = ThreadLocalRandom.current().nextInt();
+                        long x = Randomness.get().nextInt();
+                        long y = Randomness.get().nextInt();
                         long addition = x + y;
                         Map<String, Object> vars = new HashMap<String, Object>();
                         vars.put("x", x);
@@ -95,12 +95,12 @@ public class JavaScriptScriptMultiThreadedTests extends ESTestCase {
                 public void run() {
                     try {
                         barrier.await();
-                        long x = ThreadLocalRandom.current().nextInt();
+                        long x = Randomness.get().nextInt();
                         Map<String, Object> vars = new HashMap<String, Object>();
                         vars.put("x", x);
                         ExecutableScript script = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testExecutableNoRuntimeParams", "js", compiled), vars);
                         for (int i = 0; i < 100000; i++) {
-                            long y = ThreadLocalRandom.current().nextInt();
+                            long y = Randomness.get().nextInt();
                             long addition = x + y;
                             script.setNextVar("y", y);
                             long result = ((Number) script.run()).longValue();
@@ -139,8 +139,8 @@ public class JavaScriptScriptMultiThreadedTests extends ESTestCase {
                         barrier.await();
                         Map<String, Object> runtimeVars = new HashMap<String, Object>();
                         for (int i = 0; i < 100000; i++) {
-                            long x = ThreadLocalRandom.current().nextInt();
-                            long y = ThreadLocalRandom.current().nextInt();
+                            long x = Randomness.get().nextInt();
+                            long y = Randomness.get().nextInt();
                             long addition = x + y;
                             runtimeVars.put("x", x);
                             runtimeVars.put("y", y);
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Def.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Def.java
index bd9b146..c7a8ce4 100644
--- a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Def.java
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Def.java
@@ -19,18 +19,18 @@
 
 package org.elasticsearch.plan.a;
 
+import org.elasticsearch.plan.a.Definition.Cast;
+import org.elasticsearch.plan.a.Definition.Field;
+import org.elasticsearch.plan.a.Definition.Method;
+import org.elasticsearch.plan.a.Definition.Struct;
+import org.elasticsearch.plan.a.Definition.Transform;
+import org.elasticsearch.plan.a.Definition.Type;
+
 import java.lang.invoke.MethodHandle;
 import java.lang.reflect.Array;
 import java.util.List;
 import java.util.Map;
 
-import static org.elasticsearch.plan.a.Definition.Cast;
-import static org.elasticsearch.plan.a.Definition.Field;
-import static org.elasticsearch.plan.a.Definition.Method;
-import static org.elasticsearch.plan.a.Definition.Struct;
-import static org.elasticsearch.plan.a.Definition.Transform;
-import static org.elasticsearch.plan.a.Definition.Type;
-
 public class Def {
     public static Object methodCall(final Object owner, final String name, final Definition definition,
                                     final Object[] arguments, final boolean[] typesafe) {
@@ -70,7 +70,7 @@ public class Def {
         }
     }
 
-    @SuppressWarnings("unchecked")
+    @SuppressWarnings({ "unchecked", "rawtypes" })
     public static void fieldStore(final Object owner, Object value, final String name,
                                   final Definition definition, final boolean typesafe) {
         final Field field = getField(owner, name, definition);
@@ -117,7 +117,7 @@ public class Def {
         }
     }
 
-    @SuppressWarnings("unchecked")
+    @SuppressWarnings("rawtypes")
     public static Object fieldLoad(final Object owner, final String name, final Definition definition) {
         if (owner.getClass().isArray() && "length".equals(name)) {
             return Array.getLength(owner);
@@ -163,7 +163,7 @@ public class Def {
         }
     }
 
-    @SuppressWarnings("unchecked")
+    @SuppressWarnings({ "unchecked", "rawtypes" })
     public static void arrayStore(final Object array, Object index, Object value, final Definition definition,
                                   final boolean indexsafe, final boolean valuesafe) {
         if (array instanceof Map) {
@@ -206,7 +206,7 @@ public class Def {
         }
     }
 
-    @SuppressWarnings("unchecked")
+    @SuppressWarnings("rawtypes")
     public static Object arrayLoad(final Object array, Object index,
                                    final Definition definition, final boolean indexsafe) {
         if (array instanceof Map) {
@@ -257,7 +257,7 @@ public class Def {
                 }
             }
 
-            for (final Class iface : clazz.getInterfaces()) {
+            for (final Class<?> iface : clazz.getInterfaces()) {
                 struct = definition.classes.get(iface);
 
                 if (struct != null) {
@@ -303,7 +303,7 @@ public class Def {
                 }
             }
 
-            for (final Class iface : clazz.getInterfaces()) {
+            for (final Class<?> iface : clazz.getInterfaces()) {
                 struct = definition.classes.get(iface);
 
                 if (struct != null) {
@@ -348,7 +348,7 @@ public class Def {
                 break;
             }
 
-            for (final Class iface : fromClass.getInterfaces()) {
+            for (final Class<?> iface : fromClass.getInterfaces()) {
                 fromStruct = definition.classes.get(iface);
 
                 if (fromStruct != null) {
@@ -371,7 +371,7 @@ public class Def {
                     break;
                 }
 
-                for (final Class iface : toClass.getInterfaces()) {
+                for (final Class<?> iface : toClass.getInterfaces()) {
                     toStruct = definition.classes.get(iface);
 
                     if (toStruct != null) {
@@ -442,28 +442,28 @@ public class Def {
                 }
             } else if (right instanceof Character) {
                 if (left instanceof Double) {
-                    return ((Number)left).doubleValue() * (double)(char)right;
+                    return ((Number)left).doubleValue() * (char)right;
                 } else if (left instanceof Float) {
-                    return ((Number)left).floatValue() * (float)(char)right;
+                    return ((Number)left).floatValue() * (char)right;
                 } else if (left instanceof Long) {
-                    return ((Number)left).longValue() * (long)(char)right;
+                    return ((Number)left).longValue() * (char)right;
                 } else {
-                    return ((Number)left).intValue() * (int)(char)right;
+                    return ((Number)left).intValue() * (char)right;
                 }
             }
         } else if (left instanceof Character) {
             if (right instanceof Number) {
                 if (right instanceof Double) {
-                    return (double)(char)left * ((Number)right).doubleValue();
+                    return (char)left * ((Number)right).doubleValue();
                 } else if (right instanceof Float) {
-                    return (float)(char)left * ((Number)right).floatValue();
+                    return (char)left * ((Number)right).floatValue();
                 } else if (right instanceof Long) {
-                    return (long)(char)left * ((Number)right).longValue();
+                    return (char)left * ((Number)right).longValue();
                 } else {
-                    return (int)(char)left * ((Number)right).intValue();
+                    return (char)left * ((Number)right).intValue();
                 }
             } else if (right instanceof Character) {
-                return (int)(char)left * (int)(char)right;
+                return (char)left * (char)right;
             }
         }
 
@@ -485,28 +485,28 @@ public class Def {
                 }
             } else if (right instanceof Character) {
                 if (left instanceof Double) {
-                    return ((Number)left).doubleValue() / (double)(char)right;
+                    return ((Number)left).doubleValue() / (char)right;
                 } else if (left instanceof Float) {
-                    return ((Number)left).floatValue() / (float)(char)right;
+                    return ((Number)left).floatValue() / (char)right;
                 } else if (left instanceof Long) {
-                    return ((Number)left).longValue() / (long)(char)right;
+                    return ((Number)left).longValue() / (char)right;
                 } else {
-                    return ((Number)left).intValue() / (int)(char)right;
+                    return ((Number)left).intValue() / (char)right;
                 }
             }
         } else if (left instanceof Character) {
             if (right instanceof Number) {
                 if (right instanceof Double) {
-                    return (double)(char)left / ((Number)right).doubleValue();
+                    return (char)left / ((Number)right).doubleValue();
                 } else if (right instanceof Float) {
-                    return (float)(char)left / ((Number)right).floatValue();
+                    return (char)left / ((Number)right).floatValue();
                 } else if (right instanceof Long) {
-                    return (long)(char)left / ((Number)right).longValue();
+                    return (char)left / ((Number)right).longValue();
                 } else {
-                    return (int)(char)left / ((Number)right).intValue();
+                    return (char)left / ((Number)right).intValue();
                 }
             } else if (right instanceof Character) {
-                return (int)(char)left / (int)(char)right;
+                return (char)left / (char)right;
             }
         }
 
@@ -528,28 +528,28 @@ public class Def {
                 }
             } else if (right instanceof Character) {
                 if (left instanceof Double) {
-                    return ((Number)left).doubleValue() % (double)(char)right;
+                    return ((Number)left).doubleValue() % (char)right;
                 } else if (left instanceof Float) {
-                    return ((Number)left).floatValue() % (float)(char)right;
+                    return ((Number)left).floatValue() % (char)right;
                 } else if (left instanceof Long) {
-                    return ((Number)left).longValue() % (long)(char)right;
+                    return ((Number)left).longValue() % (char)right;
                 } else {
-                    return ((Number)left).intValue() % (int)(char)right;
+                    return ((Number)left).intValue() % (char)right;
                 }
             }
         } else if (left instanceof Character) {
             if (right instanceof Number) {
                 if (right instanceof Double) {
-                    return (double)(char)left % ((Number)right).doubleValue();
+                    return (char)left % ((Number)right).doubleValue();
                 } else if (right instanceof Float) {
-                    return (float)(char)left % ((Number)right).floatValue();
+                    return (char)left % ((Number)right).floatValue();
                 } else if (right instanceof Long) {
-                    return (long)(char)left % ((Number)right).longValue();
+                    return (char)left % ((Number)right).longValue();
                 } else {
-                    return (int)(char)left % ((Number)right).intValue();
+                    return (char)left % ((Number)right).intValue();
                 }
             } else if (right instanceof Character) {
-                return (int)(char)left % (int)(char)right;
+                return (char)left % (char)right;
             }
         }
 
@@ -573,28 +573,28 @@ public class Def {
                 }
             } else if (right instanceof Character) {
                 if (left instanceof Double) {
-                    return ((Number)left).doubleValue() + (double)(char)right;
+                    return ((Number)left).doubleValue() + (char)right;
                 } else if (left instanceof Float) {
-                    return ((Number)left).floatValue() + (float)(char)right;
+                    return ((Number)left).floatValue() + (char)right;
                 } else if (left instanceof Long) {
-                    return ((Number)left).longValue() + (long)(char)right;
+                    return ((Number)left).longValue() + (char)right;
                 } else {
-                    return ((Number)left).intValue() + (int)(char)right;
+                    return ((Number)left).intValue() + (char)right;
                 }
             }
         } else if (left instanceof Character) {
             if (right instanceof Number) {
                 if (right instanceof Double) {
-                    return (double)(char)left + ((Number)right).doubleValue();
+                    return (char)left + ((Number)right).doubleValue();
                 } else if (right instanceof Float) {
-                    return (float)(char)left + ((Number)right).floatValue();
+                    return (char)left + ((Number)right).floatValue();
                 } else if (right instanceof Long) {
-                    return (long)(char)left + ((Number)right).longValue();
+                    return (char)left + ((Number)right).longValue();
                 } else {
-                    return (int)(char)left + ((Number)right).intValue();
+                    return (char)left + ((Number)right).intValue();
                 }
             } else if (right instanceof Character) {
-                return (int)(char)left + (int)(char)right;
+                return (char)left + (char)right;
             }
         }
 
@@ -616,28 +616,28 @@ public class Def {
                 }
             } else if (right instanceof Character) {
                 if (left instanceof Double) {
-                    return ((Number)left).doubleValue() - (double)(char)right;
+                    return ((Number)left).doubleValue() - (char)right;
                 } else if (left instanceof Float) {
-                    return ((Number)left).floatValue() - (float)(char)right;
+                    return ((Number)left).floatValue() - (char)right;
                 } else if (left instanceof Long) {
-                    return ((Number)left).longValue() - (long)(char)right;
+                    return ((Number)left).longValue() - (char)right;
                 } else {
-                    return ((Number)left).intValue() - (int)(char)right;
+                    return ((Number)left).intValue() - (char)right;
                 }
             }
         } else if (left instanceof Character) {
             if (right instanceof Number) {
                 if (right instanceof Double) {
-                    return (double)(char)left - ((Number)right).doubleValue();
+                    return (char)left - ((Number)right).doubleValue();
                 } else if (right instanceof Float) {
-                    return (float)(char)left - ((Number)right).floatValue();
+                    return (char)left - ((Number)right).floatValue();
                 } else if (right instanceof Long) {
-                    return (long)(char)left - ((Number)right).longValue();
+                    return (char)left - ((Number)right).longValue();
                 } else {
-                    return (int)(char)left - ((Number)right).intValue();
+                    return (char)left - ((Number)right).intValue();
                 }
             } else if (right instanceof Character) {
-                return (int)(char)left - (int)(char)right;
+                return (char)left - (char)right;
             }
         }
 
@@ -657,9 +657,9 @@ public class Def {
                 }
             } else if (right instanceof Character) {
                 if (left instanceof Double || left instanceof Float || left instanceof Long) {
-                    return ((Number)left).longValue() << (long)(char)right;
+                    return ((Number)left).longValue() << (char)right;
                 } else {
-                    return ((Number)left).intValue() << (int)(char)right;
+                    return ((Number)left).intValue() << (char)right;
                 }
             }
         } else if (left instanceof Character) {
@@ -667,10 +667,10 @@ public class Def {
                 if (right instanceof Double || right instanceof Float || right instanceof Long) {
                     return (long)(char)left << ((Number)right).longValue();
                 } else {
-                    return (int)(char)left << ((Number)right).intValue();
+                    return (char)left << ((Number)right).intValue();
                 }
             } else if (right instanceof Character) {
-                return (int)(char)left << (int)(char)right;
+                return (char)left << (char)right;
             }
         }
 
@@ -690,9 +690,9 @@ public class Def {
                 }
             } else if (right instanceof Character) {
                 if (left instanceof Double || left instanceof Float || left instanceof Long) {
-                    return ((Number)left).longValue() >> (long)(char)right;
+                    return ((Number)left).longValue() >> (char)right;
                 } else {
-                    return ((Number)left).intValue() >> (int)(char)right;
+                    return ((Number)left).intValue() >> (char)right;
                 }
             }
         } else if (left instanceof Character) {
@@ -700,10 +700,10 @@ public class Def {
                 if (right instanceof Double || right instanceof Float || right instanceof Long) {
                     return (long)(char)left >> ((Number)right).longValue();
                 } else {
-                    return (int)(char)left >> ((Number)right).intValue();
+                    return (char)left >> ((Number)right).intValue();
                 }
             } else if (right instanceof Character) {
-                return (int)(char)left >> (int)(char)right;
+                return (char)left >> (char)right;
             }
         }
 
@@ -723,9 +723,9 @@ public class Def {
                 }
             } else if (right instanceof Character) {
                 if (left instanceof Double || left instanceof Float || left instanceof Long) {
-                    return ((Number)left).longValue() >>> (long)(char)right;
+                    return ((Number)left).longValue() >>> (char)right;
                 } else {
-                    return ((Number)left).intValue() >>> (int)(char)right;
+                    return ((Number)left).intValue() >>> (char)right;
                 }
             }
         } else if (left instanceof Character) {
@@ -733,10 +733,10 @@ public class Def {
                 if (right instanceof Double || right instanceof Float || right instanceof Long) {
                     return (long)(char)left >>> ((Number)right).longValue();
                 } else {
-                    return (int)(char)left >>> ((Number)right).intValue();
+                    return (char)left >>> ((Number)right).intValue();
                 }
             } else if (right instanceof Character) {
-                return (int)(char)left >>> (int)(char)right;
+                return (char)left >>> (char)right;
             }
         }
 
@@ -758,20 +758,20 @@ public class Def {
                 }
             } else if (right instanceof Character) {
                 if (left instanceof Double || left instanceof Float || left instanceof Long) {
-                    return ((Number)left).longValue() & (long)(char)right;
+                    return ((Number)left).longValue() & (char)right;
                 } else {
-                    return ((Number)left).intValue() & (int)(char)right;
+                    return ((Number)left).intValue() & (char)right;
                 }
             }
         } else if (left instanceof Character) {
             if (right instanceof Number) {
                 if (right instanceof Double || right instanceof Float || right instanceof Long) {
-                    return (long)(char)left & ((Number)right).longValue();
+                    return (char)left & ((Number)right).longValue();
                 } else {
-                    return (int)(char)left & ((Number)right).intValue();
+                    return (char)left & ((Number)right).intValue();
                 }
             } else if (right instanceof Character) {
-                return (int)(char)left & (int)(char)right;
+                return (char)left & (char)right;
             }
         }
 
@@ -793,20 +793,20 @@ public class Def {
                 }
             } else if (right instanceof Character) {
                 if (left instanceof Double || left instanceof Float || left instanceof Long) {
-                    return ((Number)left).longValue() ^ (long)(char)right;
+                    return ((Number)left).longValue() ^ (char)right;
                 } else {
-                    return ((Number)left).intValue() ^ (int)(char)right;
+                    return ((Number)left).intValue() ^ (char)right;
                 }
             }
         } else if (left instanceof Character) {
             if (right instanceof Number) {
                 if (right instanceof Double || right instanceof Float || right instanceof Long) {
-                    return (long)(char)left ^ ((Number)right).longValue();
+                    return (char)left ^ ((Number)right).longValue();
                 } else {
-                    return (int)(char)left ^ ((Number)right).intValue();
+                    return (char)left ^ ((Number)right).intValue();
                 }
             } else if (right instanceof Character) {
-                return (int)(char)left ^ (int)(char)right;
+                return (char)left ^ (char)right;
             }
         }
 
@@ -828,20 +828,20 @@ public class Def {
                 }
             } else if (right instanceof Character) {
                 if (left instanceof Double || left instanceof Float || left instanceof Long) {
-                    return ((Number)left).longValue() | (long)(char)right;
+                    return ((Number)left).longValue() | (char)right;
                 } else {
-                    return ((Number)left).intValue() | (int)(char)right;
+                    return ((Number)left).intValue() | (char)right;
                 }
             }
         } else if (left instanceof Character) {
             if (right instanceof Number) {
                 if (right instanceof Double || right instanceof Float || right instanceof Long) {
-                    return (long)(char)left | ((Number)right).longValue();
+                    return (char)left | ((Number)right).longValue();
                 } else {
-                    return (int)(char)left | ((Number)right).intValue();
+                    return (char)left | ((Number)right).intValue();
                 }
             } else if (right instanceof Character) {
-                return (int)(char)left | (int)(char)right;
+                return (char)left | (char)right;
             }
         }
 
@@ -855,48 +855,48 @@ public class Def {
                 if (right instanceof Number) {
                     return (double)left == ((Number)right).doubleValue();
                 } else if (right instanceof Character) {
-                    return (double)left == (double)(char)right;
+                    return (double)left == (char)right;
                 }
             } else if (right instanceof Double) {
                 if (left instanceof Number) {
                     return ((Number)left).doubleValue() == (double)right;
                 } else if (left instanceof Character) {
-                    return (double)(char)left == ((Number)right).doubleValue();
+                    return (char)left == ((Number)right).doubleValue();
                 }
             } else if (left instanceof Float) {
                 if (right instanceof Number) {
                     return (float)left == ((Number)right).floatValue();
                 } else if (right instanceof Character) {
-                    return (float)left == (float)(char)right;
+                    return (float)left == (char)right;
                 }
             } else if (right instanceof Float) {
                 if (left instanceof Number) {
                     return ((Number)left).floatValue() == (float)right;
                 } else if (left instanceof Character) {
-                    return (float)(char)left == ((Number)right).floatValue();
+                    return (char)left == ((Number)right).floatValue();
                 }
             } else if (left instanceof Long) {
                 if (right instanceof Number) {
                     return (long)left == ((Number)right).longValue();
                 } else if (right instanceof Character) {
-                    return (long)left == (long)(char)right;
+                    return (long)left == (char)right;
                 }
             } else if (right instanceof Long) {
                 if (left instanceof Number) {
                     return ((Number)left).longValue() == (long)right;
                 } else if (left instanceof Character) {
-                    return (long)(char)left == ((Number)right).longValue();
+                    return (char)left == ((Number)right).longValue();
                 }
             } else if (left instanceof Number) {
                 if (right instanceof Number) {
                     return ((Number)left).intValue() == ((Number)right).intValue();
                 } else if (right instanceof Character) {
-                    return ((Number)left).intValue() == (int)(char)right;
+                    return ((Number)left).intValue() == (char)right;
                 }
             } else if (right instanceof Number && left instanceof Character) {
-                return (int)(char)left == ((Number)right).intValue();
+                return (char)left == ((Number)right).intValue();
             } else if (left instanceof Character && right instanceof Character) {
-                return (int)(char)left == (int)(char)right;
+                return (char)left == (char)right;
             }
 
             return left.equals(right);
@@ -919,28 +919,28 @@ public class Def {
                 }
             } else if (right instanceof Character) {
                 if (left instanceof Double) {
-                    return ((Number)left).doubleValue() < (double)(char)right;
+                    return ((Number)left).doubleValue() < (char)right;
                 } else if (left instanceof Float) {
-                    return ((Number)left).floatValue() < (float)(char)right;
+                    return ((Number)left).floatValue() < (char)right;
                 } else if (left instanceof Long) {
-                    return ((Number)left).longValue() < (long)(char)right;
+                    return ((Number)left).longValue() < (char)right;
                 } else {
-                    return ((Number)left).intValue() < (int)(char)right;
+                    return ((Number)left).intValue() < (char)right;
                 }
             }
         } else if (left instanceof Character) {
             if (right instanceof Number) {
                 if (right instanceof Double) {
-                    return (double)(char)left < ((Number)right).doubleValue();
+                    return (char)left < ((Number)right).doubleValue();
                 } else if (right instanceof Float) {
-                    return (float)(char)left < ((Number)right).floatValue();
+                    return (char)left < ((Number)right).floatValue();
                 } else if (right instanceof Long) {
-                    return (long)(char)left < ((Number)right).longValue();
+                    return (char)left < ((Number)right).longValue();
                 } else {
-                    return (int)(char)left < ((Number)right).intValue();
+                    return (char)left < ((Number)right).intValue();
                 }
             } else if (right instanceof Character) {
-                return (int)(char)left < (int)(char)right;
+                return (char)left < (char)right;
             }
         }
 
@@ -962,28 +962,28 @@ public class Def {
                 }
             } else if (right instanceof Character) {
                 if (left instanceof Double) {
-                    return ((Number)left).doubleValue() <= (double)(char)right;
+                    return ((Number)left).doubleValue() <= (char)right;
                 } else if (left instanceof Float) {
-                    return ((Number)left).floatValue() <= (float)(char)right;
+                    return ((Number)left).floatValue() <= (char)right;
                 } else if (left instanceof Long) {
-                    return ((Number)left).longValue() <= (long)(char)right;
+                    return ((Number)left).longValue() <= (char)right;
                 } else {
-                    return ((Number)left).intValue() <= (int)(char)right;
+                    return ((Number)left).intValue() <= (char)right;
                 }
             }
         } else if (left instanceof Character) {
             if (right instanceof Number) {
                 if (right instanceof Double) {
-                    return (double)(char)left <= ((Number)right).doubleValue();
+                    return (char)left <= ((Number)right).doubleValue();
                 } else if (right instanceof Float) {
-                    return (float)(char)left <= ((Number)right).floatValue();
+                    return (char)left <= ((Number)right).floatValue();
                 } else if (right instanceof Long) {
-                    return (long)(char)left <= ((Number)right).longValue();
+                    return (char)left <= ((Number)right).longValue();
                 } else {
-                    return (int)(char)left <= ((Number)right).intValue();
+                    return (char)left <= ((Number)right).intValue();
                 }
             } else if (right instanceof Character) {
-                return (int)(char)left <= (int)(char)right;
+                return (char)left <= (char)right;
             }
         }
 
@@ -1005,28 +1005,28 @@ public class Def {
                 }
             } else if (right instanceof Character) {
                 if (left instanceof Double) {
-                    return ((Number)left).doubleValue() > (double)(char)right;
+                    return ((Number)left).doubleValue() > (char)right;
                 } else if (left instanceof Float) {
-                    return ((Number)left).floatValue() > (float)(char)right;
+                    return ((Number)left).floatValue() > (char)right;
                 } else if (left instanceof Long) {
-                    return ((Number)left).longValue() > (long)(char)right;
+                    return ((Number)left).longValue() > (char)right;
                 } else {
-                    return ((Number)left).intValue() > (int)(char)right;
+                    return ((Number)left).intValue() > (char)right;
                 }
             }
         } else if (left instanceof Character) {
             if (right instanceof Number) {
                 if (right instanceof Double) {
-                    return (double)(char)left > ((Number)right).doubleValue();
+                    return (char)left > ((Number)right).doubleValue();
                 } else if (right instanceof Float) {
-                    return (float)(char)left > ((Number)right).floatValue();
+                    return (char)left > ((Number)right).floatValue();
                 } else if (right instanceof Long) {
-                    return (long)(char)left > ((Number)right).longValue();
+                    return (char)left > ((Number)right).longValue();
                 } else {
-                    return (int)(char)left > ((Number)right).intValue();
+                    return (char)left > ((Number)right).intValue();
                 }
             } else if (right instanceof Character) {
-                return (int)(char)left > (int)(char)right;
+                return (char)left > (char)right;
             }
         }
 
@@ -1048,28 +1048,28 @@ public class Def {
                 }
             } else if (right instanceof Character) {
                 if (left instanceof Double) {
-                    return ((Number)left).doubleValue() >= (double)(char)right;
+                    return ((Number)left).doubleValue() >= (char)right;
                 } else if (left instanceof Float) {
-                    return ((Number)left).floatValue() >= (float)(char)right;
+                    return ((Number)left).floatValue() >= (char)right;
                 } else if (left instanceof Long) {
-                    return ((Number)left).longValue() >= (long)(char)right;
+                    return ((Number)left).longValue() >= (char)right;
                 } else {
-                    return ((Number)left).intValue() >= (int)(char)right;
+                    return ((Number)left).intValue() >= (char)right;
                 }
             }
         } else if (left instanceof Character) {
             if (right instanceof Number) {
                 if (right instanceof Double) {
-                    return (double)(char)left >= ((Number)right).doubleValue();
+                    return (char)left >= ((Number)right).doubleValue();
                 } else if (right instanceof Float) {
-                    return (float)(char)left >= ((Number)right).floatValue();
+                    return (char)left >= ((Number)right).floatValue();
                 } else if (right instanceof Long) {
-                    return (long)(char)left >= ((Number)right).longValue();
+                    return (char)left >= ((Number)right).longValue();
                 } else {
-                    return (int)(char)left >= ((Number)right).intValue();
+                    return (char)left >= ((Number)right).intValue();
                 }
             } else if (right instanceof Character) {
-                return (int)(char)left >= (int)(char)right;
+                return (char)left >= (char)right;
             }
         }
 
@@ -1121,7 +1121,7 @@ public class Def {
         if (value instanceof Boolean) {
             return ((Boolean)value) ? 1 : 0;
         } else if (value instanceof Character) {
-            return (int)(char)value;
+            return (char)value;
         } else {
             return ((Number)value).intValue();
         }
@@ -1131,7 +1131,7 @@ public class Def {
         if (value instanceof Boolean) {
             return ((Boolean)value) ? 1L : 0;
         } else if (value instanceof Character) {
-            return (long)(char)value;
+            return (char)value;
         } else {
             return ((Number)value).longValue();
         }
@@ -1141,7 +1141,7 @@ public class Def {
         if (value instanceof Boolean) {
             return ((Boolean)value) ? (float)1 : 0;
         } else if (value instanceof Character) {
-            return (float)(char)value;
+            return (char)value;
         } else {
             return ((Number)value).floatValue();
         }
@@ -1151,7 +1151,7 @@ public class Def {
         if (value instanceof Boolean) {
             return ((Boolean)value) ? (double)1 : 0;
         } else if (value instanceof Character) {
-            return (double)(char)value;
+            return (char)value;
         } else {
             return ((Number)value).doubleValue();
         }
diff --git a/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTests.java b/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTests.java
index 06d3da0..f24cc88 100644
--- a/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTests.java
+++ b/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.script.python;
 
+import org.elasticsearch.common.Randomness;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.script.CompiledScript;
 import org.elasticsearch.script.ExecutableScript;
@@ -30,7 +31,6 @@ import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.CyclicBarrier;
-import java.util.concurrent.ThreadLocalRandom;
 import java.util.concurrent.atomic.AtomicBoolean;
 
 import static org.hamcrest.Matchers.equalTo;
@@ -55,8 +55,8 @@ public class PythonScriptMultiThreadedTests extends ESTestCase {
                 public void run() {
                     try {
                         barrier.await();
-                        long x = ThreadLocalRandom.current().nextInt();
-                        long y = ThreadLocalRandom.current().nextInt();
+                        long x = Randomness.get().nextInt();
+                        long y = Randomness.get().nextInt();
                         long addition = x + y;
                         Map<String, Object> vars = new HashMap<String, Object>();
                         vars.put("x", x);
@@ -97,13 +97,13 @@ public class PythonScriptMultiThreadedTests extends ESTestCase {
 //                @Override public void run() {
 //                    try {
 //                        barrier.await();
-//                        long x = ThreadLocalRandom.current().nextInt();
+//                        long x = Randomness.get().nextInt();
 //                        Map<String, Object> vars = new HashMap<String, Object>();
 //                        vars.put("x", x);
 //                        ExecutableScript script = se.executable(compiled, vars);
 //                        Map<String, Object> runtimeVars = new HashMap<String, Object>();
 //                        for (int i = 0; i < 100000; i++) {
-//                            long y = ThreadLocalRandom.current().nextInt();
+//                            long y = Randomness.get().nextInt();
 //                            long addition = x + y;
 //                            runtimeVars.put("y", y);
 //                            long result = ((Number) script.run(runtimeVars)).longValue();
@@ -143,8 +143,8 @@ public class PythonScriptMultiThreadedTests extends ESTestCase {
                         barrier.await();
                         Map<String, Object> runtimeVars = new HashMap<String, Object>();
                         for (int i = 0; i < 10000; i++) {
-                            long x = ThreadLocalRandom.current().nextInt();
-                            long y = ThreadLocalRandom.current().nextInt();
+                            long x = Randomness.get().nextInt();
+                            long y = Randomness.get().nextInt();
                             long addition = x + y;
                             runtimeVars.put("x", x);
                             runtimeVars.put("y", y);
diff --git a/plugins/mapper-attachments/build.gradle b/plugins/mapper-attachments/build.gradle
index bbe89aa..70741c2 100644
--- a/plugins/mapper-attachments/build.gradle
+++ b/plugins/mapper-attachments/build.gradle
@@ -61,7 +61,7 @@ dependencies {
   compile 'org.apache.commons:commons-compress:1.10'
 }
 
-compileJava.options.compilerArgs << '-Xlint:-cast,-deprecation,-rawtypes'
+compileJava.options.compilerArgs << '-Xlint:-cast,-rawtypes'
 
 forbiddenPatterns {
   exclude '**/*.docx'
diff --git a/plugins/mapper-attachments/src/main/java/org/elasticsearch/mapper/attachments/AttachmentMapper.java b/plugins/mapper-attachments/src/main/java/org/elasticsearch/mapper/attachments/AttachmentMapper.java
index d43b5df..7c54e6f 100644
--- a/plugins/mapper-attachments/src/main/java/org/elasticsearch/mapper/attachments/AttachmentMapper.java
+++ b/plugins/mapper-attachments/src/main/java/org/elasticsearch/mapper/attachments/AttachmentMapper.java
@@ -100,6 +100,7 @@ public class AttachmentMapper extends FieldMapper {
             super(ref);
         }
 
+        @Override
         public AttachmentMapper.AttachmentFieldType clone() {
             return new AttachmentMapper.AttachmentFieldType(this);
         }
@@ -109,6 +110,7 @@ public class AttachmentMapper extends FieldMapper {
             return CONTENT_TYPE;
         }
 
+        @Override
         public String value(Object value) {
             return value == null?null:value.toString();
         }
@@ -292,7 +294,7 @@ public class AttachmentMapper extends FieldMapper {
                 type = "string";
             }
             Mapper.TypeParser typeParser = parserContext.typeParser(type);
-            Mapper.Builder<?, ?> mapperBuilder = typeParser.parse(propName, (Map<String, Object>) propNode, parserContext);
+            Mapper.Builder<?, ?> mapperBuilder = typeParser.parse(propName, propNode, parserContext);
 
             return mapperBuilder;
         }
@@ -414,6 +416,7 @@ public class AttachmentMapper extends FieldMapper {
     }
 
     @Override
+    @SuppressWarnings("deprecation") // https://github.com/elastic/elasticsearch/issues/15843
     public Mapper parse(ParseContext context) throws IOException {
         byte[] content = null;
         String contentType = null;
diff --git a/plugins/repository-azure/build.gradle b/plugins/repository-azure/build.gradle
index f0c21bb..85a927c 100644
--- a/plugins/repository-azure/build.gradle
+++ b/plugins/repository-azure/build.gradle
@@ -35,6 +35,5 @@ dependencyLicenses {
   mapping from: /stax-.*/, to: 'stax'
 }
 
-compileJava.options.compilerArgs << '-Xlint:-deprecation,-serial'
-compileTestJava.options.compilerArgs << '-Xlint:-deprecation'
+compileJava.options.compilerArgs << '-Xlint:-serial'
 
diff --git a/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageService.java b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageService.java
index 9ed909c..c154f78 100644
--- a/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageService.java
+++ b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageService.java
@@ -21,6 +21,7 @@ package org.elasticsearch.cloud.azure.storage;
 
 import com.microsoft.azure.storage.LocationMode;
 import com.microsoft.azure.storage.StorageException;
+
 import org.elasticsearch.common.blobstore.BlobMetaData;
 
 import java.io.InputStream;
@@ -36,10 +37,6 @@ public interface AzureStorageService {
 
     final class Storage {
         public static final String PREFIX = "cloud.azure.storage.";
-        @Deprecated
-        public static final String ACCOUNT_DEPRECATED = "cloud.azure.storage.account";
-        @Deprecated
-        public static final String KEY_DEPRECATED = "cloud.azure.storage.key";
 
         public static final String TIMEOUT = "cloud.azure.storage.timeout";
 
diff --git a/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettings.java b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettings.java
index c7380e2..75414f0 100644
--- a/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettings.java
+++ b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettings.java
@@ -82,52 +82,42 @@ public class AzureStorageSettings {
         AzureStorageSettings primaryStorage = null;
         Map<String, AzureStorageSettings> secondaryStorage = new HashMap<>();
 
-        // We check for deprecated settings
-        String account = settings.get(Storage.ACCOUNT_DEPRECATED);
-        String key = settings.get(Storage.KEY_DEPRECATED);
-
         TimeValue globalTimeout = settings.getAsTime(Storage.TIMEOUT, TimeValue.timeValueMinutes(5));
 
-        if (account != null) {
-            logger.warn("[{}] and [{}] have been deprecated. Use now [{}xxx.account] and [{}xxx.key] where xxx is any name",
-                    Storage.ACCOUNT_DEPRECATED, Storage.KEY_DEPRECATED, Storage.PREFIX, Storage.PREFIX);
-            primaryStorage = new AzureStorageSettings(null, account, key, globalTimeout);
-        } else {
-            Settings storageSettings = settings.getByPrefix(Storage.PREFIX);
-            if (storageSettings != null) {
-                Map<String, Object> asMap = storageSettings.getAsStructuredMap();
-                for (Map.Entry<String, Object> storage : asMap.entrySet()) {
-                    if (storage.getValue() instanceof Map) {
-                        @SuppressWarnings("unchecked")
-                        Map<String, String> map = (Map) storage.getValue();
-                        TimeValue timeout = TimeValue.parseTimeValue(map.get("timeout"), globalTimeout, Storage.PREFIX + storage.getKey() + ".timeout");
-                        AzureStorageSettings current = new AzureStorageSettings(storage.getKey(), map.get("account"), map.get("key"), timeout);
-                        boolean activeByDefault = Boolean.parseBoolean(map.getOrDefault("default", "false"));
-                        if (activeByDefault) {
-                            if (primaryStorage == null) {
-                                primaryStorage = current;
-                            } else {
-                                logger.warn("default storage settings has already been defined. You can not define it to [{}]", storage.getKey());
-                                secondaryStorage.put(storage.getKey(), current);
-                            }
+        Settings storageSettings = settings.getByPrefix(Storage.PREFIX);
+        if (storageSettings != null) {
+            Map<String, Object> asMap = storageSettings.getAsStructuredMap();
+            for (Map.Entry<String, Object> storage : asMap.entrySet()) {
+                if (storage.getValue() instanceof Map) {
+                    @SuppressWarnings("unchecked")
+                    Map<String, String> map = (Map) storage.getValue();
+                    TimeValue timeout = TimeValue.parseTimeValue(map.get("timeout"), globalTimeout, Storage.PREFIX + storage.getKey() + ".timeout");
+                    AzureStorageSettings current = new AzureStorageSettings(storage.getKey(), map.get("account"), map.get("key"), timeout);
+                    boolean activeByDefault = Boolean.parseBoolean(map.getOrDefault("default", "false"));
+                    if (activeByDefault) {
+                        if (primaryStorage == null) {
+                            primaryStorage = current;
                         } else {
+                            logger.warn("default storage settings has already been defined. You can not define it to [{}]", storage.getKey());
                             secondaryStorage.put(storage.getKey(), current);
                         }
+                    } else {
+                        secondaryStorage.put(storage.getKey(), current);
                     }
                 }
-                // If we did not set any default storage, we should complain and define it
-                if (primaryStorage == null && secondaryStorage.isEmpty() == false) {
-                    Map.Entry<String, AzureStorageSettings> fallback = secondaryStorage.entrySet().iterator().next();
-                    // We only warn if the number of secondary storage if > to 1
-                    // If the user defined only one storage account, that's fine. We know it's the default one.
-                    if (secondaryStorage.size() > 1) {
-                        logger.warn("no default storage settings has been defined. " +
-                                "Add \"default\": true to the settings you want to activate by default. " +
-                                "Forcing default to [{}].", fallback.getKey());
-                    }
-                    primaryStorage = fallback.getValue();
-                    secondaryStorage.remove(fallback.getKey());
+            }
+            // If we did not set any default storage, we should complain and define it
+            if (primaryStorage == null && secondaryStorage.isEmpty() == false) {
+                Map.Entry<String, AzureStorageSettings> fallback = secondaryStorage.entrySet().iterator().next();
+                // We only warn if the number of secondary storage if > to 1
+                // If the user defined only one storage account, that's fine. We know it's the default one.
+                if (secondaryStorage.size() > 1) {
+                    logger.warn("no default storage settings has been defined. " +
+                            "Add \"default\": true to the settings you want to activate by default. " +
+                            "Forcing default to [{}].", fallback.getKey());
                 }
+                primaryStorage = fallback.getValue();
+                secondaryStorage.remove(fallback.getKey());
             }
         }
 
diff --git a/plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTestCase.java b/plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTestCase.java
index b3e8789..3c32ade 100644
--- a/plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTestCase.java
+++ b/plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTestCase.java
@@ -21,6 +21,7 @@ package org.elasticsearch.cloud.azure;
 
 import com.microsoft.azure.storage.LocationMode;
 import com.microsoft.azure.storage.StorageException;
+
 import org.elasticsearch.cloud.azure.storage.AzureStorageService;
 import org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage;
 import org.elasticsearch.cloud.azure.storage.AzureStorageServiceMock;
@@ -79,11 +80,6 @@ public abstract class AbstractAzureRepositoryServiceTestCase extends AbstractAzu
     protected Settings nodeSettings(int nodeOrdinal) {
         Settings.Builder builder = Settings.settingsBuilder()
                 .put(Storage.CONTAINER, "snapshots");
-
-        // We use sometime deprecated settings in tests
-        builder.put(Storage.ACCOUNT_DEPRECATED, "mock_azure_account")
-                .put(Storage.KEY_DEPRECATED, "mock_azure_key");
-
         return builder.build();
     }
 
diff --git a/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSettingsParserTest.java b/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSettingsParserTest.java
index 59e8b89..aec8506 100644
--- a/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSettingsParserTest.java
+++ b/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSettingsParserTest.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.repositories.azure;
 
 import org.apache.lucene.util.LuceneTestCase;
-import org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage;
 import org.elasticsearch.cloud.azure.storage.AzureStorageSettings;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.settings.Settings;
@@ -66,19 +65,6 @@ public class AzureSettingsParserTest extends LuceneTestCase {
         assertThat(tuple.v2().keySet(), hasSize(0));
     }
 
-    public void testDeprecatedSettings() {
-        Settings settings = Settings.builder()
-                .put(Storage.ACCOUNT_DEPRECATED, "myaccount1")
-                .put(Storage.KEY_DEPRECATED, "mykey1")
-                .build();
-
-        Tuple<AzureStorageSettings, Map<String, AzureStorageSettings>> tuple = AzureStorageSettings.parse(settings);
-        assertThat(tuple.v1(), notNullValue());
-        assertThat(tuple.v1().getAccount(), is("myaccount1"));
-        assertThat(tuple.v1().getKey(), is("mykey1"));
-        assertThat(tuple.v2().keySet(), hasSize(0));
-    }
-
     public void testParseTwoSettingsNoDefault() {
         Settings settings = Settings.builder()
                 .put("cloud.azure.storage.azure1.account", "myaccount1")
diff --git a/plugins/repository-hdfs/build.gradle b/plugins/repository-hdfs/build.gradle
index 68ab6f5..924f8cd 100644
--- a/plugins/repository-hdfs/build.gradle
+++ b/plugins/repository-hdfs/build.gradle
@@ -96,8 +96,6 @@ integTest {
   }
 }
 
-compileJava.options.compilerArgs << '-Xlint:-deprecation,-rawtypes'
-
 thirdPartyAudit.excludes = [
   // classes are missing, because we added hadoop jars one by one until tests pass.
   'com.google.gson.stream.JsonReader', 
diff --git a/plugins/repository-s3/build.gradle b/plugins/repository-s3/build.gradle
index b11aa73..a083309 100644
--- a/plugins/repository-s3/build.gradle
+++ b/plugins/repository-s3/build.gradle
@@ -43,8 +43,6 @@ dependencyLicenses {
   mapping from: /jackson-.*/, to: 'jackson'
 }
 
-compileJava.options.compilerArgs << '-Xlint:-deprecation,-rawtypes'
-
 test {
   // this is needed for insecure plugins, remove if possible!
   systemProperty 'tests.artifact', project.name 
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java
index 711b8db..55c4b58 100644
--- a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.cloud.aws;
 
 import com.amazonaws.services.s3.AmazonS3;
+
 import org.elasticsearch.common.component.LifecycleComponent;
 
 /**
@@ -37,10 +38,6 @@ public interface AwsS3Service extends LifecycleComponent<AwsS3Service> {
         public static final String PROXY_PASSWORD = "cloud.aws.proxy.password";
         public static final String SIGNER = "cloud.aws.signer";
         public static final String REGION = "cloud.aws.region";
-        @Deprecated
-        public static final String DEPRECATED_PROXY_HOST = "cloud.aws.proxy_host";
-        @Deprecated
-        public static final String DEPRECATED_PROXY_PORT = "cloud.aws.proxy_port";
     }
 
     final class CLOUD_S3 {
@@ -53,10 +50,6 @@ public interface AwsS3Service extends LifecycleComponent<AwsS3Service> {
         public static final String PROXY_PASSWORD = "cloud.aws.s3.proxy.password";
         public static final String SIGNER = "cloud.aws.s3.signer";
         public static final String ENDPOINT = "cloud.aws.s3.endpoint";
-        @Deprecated
-        public static final String DEPRECATED_PROXY_HOST = "cloud.aws.s3.proxy_host";
-        @Deprecated
-        public static final String DEPRECATED_PROXY_PORT = "cloud.aws.s3.proxy_port";
     }
 
     final class REPOSITORY_S3 {
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
index 51594c0..90b79fd 100644
--- a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
@@ -31,6 +31,7 @@ import com.amazonaws.http.IdleConnectionReaper;
 import com.amazonaws.internal.StaticCredentialsProvider;
 import com.amazonaws.services.s3.AmazonS3;
 import com.amazonaws.services.s3.AmazonS3Client;
+
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
@@ -94,7 +95,6 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
         return getClient(endpoint, protocol, account, key, maxRetries);
     }
 
-
     private synchronized AmazonS3 getClient(String endpoint, String protocol, String account, String key, Integer maxRetries) {
         Tuple<String, String> clientDescriptor = new Tuple<String, String>(endpoint, account);
         AmazonS3Client client = clients.get(clientDescriptor);
@@ -119,11 +119,11 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
             throw new IllegalArgumentException("No protocol supported [" + protocol + "], can either be [http] or [https]");
         }
 
-        String proxyHost = settings.get(CLOUD_AWS.PROXY_HOST, settings.get(CLOUD_AWS.DEPRECATED_PROXY_HOST));
-        proxyHost = settings.get(CLOUD_S3.PROXY_HOST, settings.get(CLOUD_S3.DEPRECATED_PROXY_HOST, proxyHost));
+        String proxyHost = settings.get(CLOUD_AWS.PROXY_HOST);
+        proxyHost = settings.get(CLOUD_S3.PROXY_HOST, proxyHost);
         if (proxyHost != null) {
-            String portString = settings.get(CLOUD_AWS.PROXY_PORT, settings.get(CLOUD_AWS.DEPRECATED_PROXY_PORT, "80"));
-            portString = settings.get(CLOUD_S3.PROXY_PORT, settings.get(CLOUD_S3.DEPRECATED_PROXY_PORT, portString));
+            String portString = settings.get(CLOUD_AWS.PROXY_PORT, "80");
+            portString = settings.get(CLOUD_S3.PROXY_PORT, portString);
             Integer proxyPort;
             try {
                 proxyPort = Integer.parseInt(portString, 10);
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java b/plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java
index a730572..2b486e5 100644
--- a/plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java
@@ -77,6 +77,7 @@ public class S3RepositoryPlugin extends Plugin {
     }
 
     @Override
+    @SuppressWarnings("rawtypes") // Supertype declaration has raw types
     public Collection<Class<? extends LifecycleComponent>> nodeServices() {
         return Collections.<Class<? extends LifecycleComponent>>singleton(S3Module.getS3ServiceImpl());
     }
diff --git a/qa/ingest-disabled/build.gradle b/qa/ingest-disabled/build.gradle
deleted file mode 100644
index ca71697..0000000
--- a/qa/ingest-disabled/build.gradle
+++ /dev/null
@@ -1,26 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-apply plugin: 'elasticsearch.rest-test'
-
-integTest {
-    cluster {
-        systemProperty 'es.node.ingest', 'false'
-    }
-}
diff --git a/qa/ingest-disabled/src/test/java/org/elasticsearch/smoketest/IngestDisabledIT.java b/qa/ingest-disabled/src/test/java/org/elasticsearch/smoketest/IngestDisabledIT.java
deleted file mode 100644
index e162807..0000000
--- a/qa/ingest-disabled/src/test/java/org/elasticsearch/smoketest/IngestDisabledIT.java
+++ /dev/null
@@ -1,41 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.smoketest;
-
-import com.carrotsearch.randomizedtesting.annotations.Name;
-import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
-import org.elasticsearch.test.rest.ESRestTestCase;
-import org.elasticsearch.test.rest.RestTestCandidate;
-import org.elasticsearch.test.rest.parser.RestTestParseException;
-
-import java.io.IOException;
-
-public class IngestDisabledIT extends ESRestTestCase {
-
-    public IngestDisabledIT(@Name("yaml") RestTestCandidate testCandidate) {
-        super(testCandidate);
-    }
-
-    @ParametersFactory
-    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
-        return ESRestTestCase.createParameters(0, 1);
-    }
-
-}
diff --git a/qa/ingest-disabled/src/test/resources/rest-api-spec/test/ingest_mustache/10_ingest_disabled.yaml b/qa/ingest-disabled/src/test/resources/rest-api-spec/test/ingest_mustache/10_ingest_disabled.yaml
deleted file mode 100644
index b2b983e..0000000
--- a/qa/ingest-disabled/src/test/resources/rest-api-spec/test/ingest_mustache/10_ingest_disabled.yaml
+++ /dev/null
@@ -1,129 +0,0 @@
----
-"Test ingest CRUD APIS work fine when node.ingest is set to false":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field2",
-                  "value": "_value"
-                }
-              }
-            ]
-          }
-  - match: { _index: ".ingest" }
-  - match: { _type: "pipeline" }
-  - match: { _version: 1 }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      ingest.get_pipeline:
-        id: "my_pipeline"
-  - match: { my_pipeline._source.description: "_description" }
-  - match: { my_pipeline._version: 1 }
-
-  - do:
-      ingest.delete_pipeline:
-        id: "my_pipeline"
-  - match: { _index: ".ingest" }
-  - match: { _type: "pipeline" }
-  - match: { _version: 2 }
-  - match: { _id: "my_pipeline" }
-  - match: { found: true }
-
----
-"Test ingest simulate API works fine when node.ingest is set to false":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field2",
-                  "value" : "_value"
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      ingest.simulate:
-        id: "my_pipeline"
-        body: >
-          {
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-  - match: { docs.0.doc._source.foo: "bar" }
-  - match: { docs.0.doc._source.field2: "_value" }
-  - length: { docs.0.doc._ingest: 1 }
-  - is_true: docs.0.doc._ingest.timestamp
-
----
-"Test index api with pipeline id fails when node.ingest is set to false":
-  - do:
-      catch: /node.ingest is set to false, cannot execute pipeline/
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline_1"
-        body: {
-          field1: "1",
-          field2: "2",
-          field3: "3"
-        }
-
----
-"Test bulk api with pipeline id fails when node.ingest is set to false":
-  - do:
-      catch: /node.ingest is set to false, cannot execute pipeline/
-      bulk:
-        pipeline: "my_pipeline_1"
-        body:
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id
-          - f1: v1
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id2
-          - f1: v2
-
----
-"Test bulk api that contains a single index call with pipeline id fails when node.ingest is set to false":
-  - do:
-      catch: /node.ingest is set to false, cannot execute pipeline/
-      bulk:
-        body:
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id
-          - f1: v1
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id2
-              pipeline: my_pipeline_1
-          - f1: v2
-
diff --git a/qa/ingest-with-mustache/build.gradle b/qa/ingest-with-mustache/build.gradle
deleted file mode 100644
index e5ca482..0000000
--- a/qa/ingest-with-mustache/build.gradle
+++ /dev/null
@@ -1,24 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-apply plugin: 'elasticsearch.rest-test'
-
-dependencies {
-    testCompile project(path: ':modules:lang-mustache', configuration: 'runtime')
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/AbstractMustacheTests.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/AbstractMustacheTests.java
deleted file mode 100644
index 57165e6..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/AbstractMustacheTests.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.ingest.InternalTemplateService;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.script.ScriptContextRegistry;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.Collections;
-
-public abstract class AbstractMustacheTests extends ESTestCase {
-
-    protected TemplateService templateService;
-
-    @Before
-    public void init() throws Exception {
-        Settings settings = Settings.builder()
-            .put("path.home", createTempDir())
-            .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING, false)
-            .build();
-        MustacheScriptEngineService mustache = new MustacheScriptEngineService(settings);
-        ScriptContextRegistry registry = new ScriptContextRegistry(Collections.emptyList());
-        ScriptService scriptService = new ScriptService(
-            settings, new Environment(settings), Collections.singleton(mustache), null, registry
-        );
-        templateService = new InternalTemplateService(scriptService);
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestDocumentMustacheIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestDocumentMustacheIT.java
deleted file mode 100644
index 1b080fe..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestDocumentMustacheIT.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ValueSource;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class IngestDocumentMustacheIT extends AbstractMustacheTests {
-
-    public void testAccessMetaDataViaTemplate() {
-        Map<String, Object> document = new HashMap<>();
-        document.put("foo", "bar");
-        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("1 {{foo}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("1 bar"));
-
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("2 {{_source.foo}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("2 bar"));
-    }
-
-    public void testAccessMapMetaDataViaTemplate() {
-        Map<String, Object> document = new HashMap<>();
-        Map<String, Object> innerObject = new HashMap<>();
-        innerObject.put("bar", "hello bar");
-        innerObject.put("baz", "hello baz");
-        innerObject.put("qux", Collections.singletonMap("fubar", "hello qux and fubar"));
-        document.put("foo", innerObject);
-        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("1 {{foo.bar}} {{foo.baz}} {{foo.qux.fubar}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("1 hello bar hello baz hello qux and fubar"));
-
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("2 {{_source.foo.bar}} {{_source.foo.baz}} {{_source.foo.qux.fubar}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("2 hello bar hello baz hello qux and fubar"));
-    }
-
-    public void testAccessListMetaDataViaTemplate() {
-        Map<String, Object> document = new HashMap<>();
-        document.put("list1", Arrays.asList("foo", "bar", null));
-        List<Map<String, Object>> list = new ArrayList<>();
-        Map<String, Object> value = new HashMap<>();
-        value.put("field", "value");
-        list.add(value);
-        list.add(null);
-        document.put("list2", list);
-        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
-        // TODO: fix index based lookups in lists:
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("1 {{list1}} {{list2}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("1 [foo, bar, null] [{field=value}, null]"));
-
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("2 {{_source.list1}} {{_source.list2}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("2 [foo, bar, null] [{field=value}, null]"));
-    }
-
-    public void testAccessIngestMetadataViaTemplate() {
-        Map<String, Object> document = new HashMap<>();
-        Map<String, Object> ingestMap = new HashMap<>();
-        ingestMap.put("timestamp", "bogus_timestamp");
-        document.put("_ingest", ingestMap);
-        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
-        ingestDocument.setFieldValue(templateService.compile("ingest_timestamp"), ValueSource.wrap("{{_ingest.timestamp}} and {{_source._ingest.timestamp}}", templateService));
-        assertThat(ingestDocument.getFieldValue("ingest_timestamp", String.class), equalTo(ingestDocument.getIngestMetadata().get("timestamp") + " and bogus_timestamp"));
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheRemoveProcessorIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheRemoveProcessorIT.java
deleted file mode 100644
index e94765a..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheRemoveProcessorIT.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.processor.RemoveProcessor;
-import org.hamcrest.CoreMatchers;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-public class IngestMustacheRemoveProcessorIT extends AbstractMustacheTests {
-
-    public void testRemoveProcessorMustacheExpression() throws Exception {
-        RemoveProcessor.Factory factory = new RemoveProcessor.Factory(templateService);
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field{{var}}");
-        RemoveProcessor processor = factory.create(config);
-        assertThat(processor.getField().execute(Collections.singletonMap("var", "_value")), CoreMatchers.equalTo("field_value"));
-    }
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheSetProcessorIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheSetProcessorIT.java
deleted file mode 100644
index 6846679..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheSetProcessorIT.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ValueSource;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.processor.SetProcessor;
-import org.hamcrest.Matchers;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class IngestMustacheSetProcessorIT extends AbstractMustacheTests {
-
-    public void testExpression() throws Exception {
-        SetProcessor processor = createSetProcessor("_index", "text {{var}}");
-        assertThat(processor.getValue(), instanceOf(ValueSource.TemplatedValue.class));
-        assertThat(processor.getValue().copyAndResolve(Collections.singletonMap("var", "_value")), equalTo("text _value"));
-    }
-
-    public void testSetMetadataWithTemplates() throws Exception {
-        IngestDocument.MetaData randomMetaData = randomFrom(IngestDocument.MetaData.values());
-        Processor processor = createSetProcessor(randomMetaData.getFieldName(), "_value {{field}}");
-        IngestDocument ingestDocument = createIngestDocument(Collections.singletonMap("field", "value"));
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(randomMetaData.getFieldName(), String.class), Matchers.equalTo("_value value"));
-    }
-
-    public void testSetWithTemplates() throws Exception {
-        IngestDocument.MetaData randomMetaData = randomFrom(IngestDocument.MetaData.INDEX, IngestDocument.MetaData.TYPE, IngestDocument.MetaData.ID);
-        Processor processor = createSetProcessor("field{{_type}}", "_value {{" + randomMetaData.getFieldName() + "}}");
-        IngestDocument ingestDocument = createIngestDocument(new HashMap<>());
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("field_type", String.class), Matchers.equalTo("_value " + ingestDocument.getFieldValue(randomMetaData.getFieldName(), String.class)));
-    }
-
-    private SetProcessor createSetProcessor(String fieldName, Object fieldValue) throws Exception {
-        SetProcessor.Factory factory = new SetProcessor.Factory(templateService);
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", fieldName);
-        config.put("value", fieldValue);
-        return factory.create(config);
-    }
-
-    private IngestDocument createIngestDocument(Map<String, Object> source) {
-        return new IngestDocument("_index", "_type", "_id", null, null, null, null, source);
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/TemplateServiceIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/TemplateServiceIT.java
deleted file mode 100644
index 1d1579f..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/TemplateServiceIT.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.core.TemplateService;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class TemplateServiceIT extends AbstractMustacheTests {
-
-    public void testTemplates() {
-        Map<String, Object> model = new HashMap<>();
-        model.put("fielda", "value1");
-        model.put("fieldb", Collections.singletonMap("fieldc", "value3"));
-
-        TemplateService.Template template = templateService.compile("{{fielda}}/{{fieldb}}/{{fieldb.fieldc}}");
-        assertThat(template.execute(model), equalTo("value1/{fieldc=value3}/value3"));
-    }
-
-    public void testWrongTemplateUsage() {
-        Map<String, Object> model = Collections.emptyMap();
-        TemplateService.Template template = templateService.compile("value");
-        assertThat(template.execute(model), equalTo("value"));
-
-        template = templateService.compile("value {{");
-        assertThat(template.execute(model), equalTo("value {{"));
-        template = templateService.compile("value {{abc");
-        assertThat(template.execute(model), equalTo("value {{abc"));
-        template = templateService.compile("value }}");
-        assertThat(template.execute(model), equalTo("value }}"));
-        template = templateService.compile("value }} {{");
-        assertThat(template.execute(model), equalTo("value }} {{"));
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/ValueSourceMustacheIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/ValueSourceMustacheIT.java
deleted file mode 100644
index 18085b94..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/ValueSourceMustacheIT.java
+++ /dev/null
@@ -1,76 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ValueSource;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class ValueSourceMustacheIT extends AbstractMustacheTests {
-
-    public void testValueSourceWithTemplates() {
-        Map<String, Object> model = new HashMap<>();
-        model.put("field1", "value1");
-        model.put("field2", Collections.singletonMap("field3", "value3"));
-
-        ValueSource valueSource = ValueSource.wrap("{{field1}}/{{field2}}/{{field2.field3}}", templateService);
-        assertThat(valueSource, instanceOf(ValueSource.TemplatedValue.class));
-        assertThat(valueSource.copyAndResolve(model), equalTo("value1/{field3=value3}/value3"));
-
-        valueSource = ValueSource.wrap(Arrays.asList("_value", "{{field1}}"), templateService);
-        assertThat(valueSource, instanceOf(ValueSource.ListValue.class));
-        List<String> result = (List<String>) valueSource.copyAndResolve(model);
-        assertThat(result.size(), equalTo(2));
-        assertThat(result.get(0), equalTo("_value"));
-        assertThat(result.get(1), equalTo("value1"));
-
-        Map<String, Object> map = new HashMap<>();
-        map.put("field1", "{{field1}}");
-        map.put("field2", Collections.singletonMap("field3", "{{field2.field3}}"));
-        map.put("field4", "_value");
-        valueSource = ValueSource.wrap(map, templateService);
-        assertThat(valueSource, instanceOf(ValueSource.MapValue.class));
-        Map<String, Object> resultMap = (Map<String, Object>) valueSource.copyAndResolve(model);
-        assertThat(resultMap.size(), equalTo(3));
-        assertThat(resultMap.get("field1"), equalTo("value1"));
-        assertThat(((Map) resultMap.get("field2")).size(), equalTo(1));
-        assertThat(((Map) resultMap.get("field2")).get("field3"), equalTo("value3"));
-        assertThat(resultMap.get("field4"), equalTo("_value"));
-    }
-
-    public void testAccessSourceViaTemplate() {
-        IngestDocument ingestDocument = new IngestDocument("marvel", "type", "id", null, null, null, null, new HashMap<>());
-        assertThat(ingestDocument.hasField("marvel"), is(false));
-        ingestDocument.setFieldValue(templateService.compile("{{_index}}"), ValueSource.wrap("{{_index}}", templateService));
-        assertThat(ingestDocument.getFieldValue("marvel", String.class), equalTo("marvel"));
-        ingestDocument.removeField(templateService.compile("{{marvel}}"));
-        assertThat(ingestDocument.hasField("index"), is(false));
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/smoketest/IngestWithMustacheIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/smoketest/IngestWithMustacheIT.java
deleted file mode 100644
index 73f64d4..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/smoketest/IngestWithMustacheIT.java
+++ /dev/null
@@ -1,41 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.smoketest;
-
-import com.carrotsearch.randomizedtesting.annotations.Name;
-import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
-import org.elasticsearch.test.rest.ESRestTestCase;
-import org.elasticsearch.test.rest.RestTestCandidate;
-import org.elasticsearch.test.rest.parser.RestTestParseException;
-
-import java.io.IOException;
-
-public class IngestWithMustacheIT extends ESRestTestCase {
-
-    public IngestWithMustacheIT(@Name("yaml") RestTestCandidate testCandidate) {
-        super(testCandidate);
-    }
-
-    @ParametersFactory
-    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
-        return ESRestTestCase.createParameters(0, 1);
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/resources/rest-api-spec/test/ingest_mustache/10_pipeline_with_mustache_templates.yaml b/qa/ingest-with-mustache/src/test/resources/rest-api-spec/test/ingest_mustache/10_pipeline_with_mustache_templates.yaml
deleted file mode 100644
index e65e68f..0000000
--- a/qa/ingest-with-mustache/src/test/resources/rest-api-spec/test/ingest_mustache/10_pipeline_with_mustache_templates.yaml
+++ /dev/null
@@ -1,220 +0,0 @@
----
-"Test metadata templating":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline_1"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "index_type_id",
-                  "value": "{{_index}}/{{_type}}/{{_id}}"
-                }
-              },
-              {
-                "append" : {
-                  "field" : "metadata",
-                  "value": ["{{_index}}", "{{_type}}", "{{_id}}"]
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline_1" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline_1"
-        body: {}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - length: { _source: 2 }
-  - match: { _source.index_type_id: "test/test/1" }
-  - match: { _source.metadata: ["test", "test", "1"] }
-
----
-"Test templating":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline_1"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field4",
-                  "value": "{{field1}}/{{field2}}/{{field3}}"
-                }
-              },
-              {
-                "append" : {
-                  "field" : "metadata",
-                  "value": ["{{field1}}", "{{field2}}", "{{field3}}"]
-                }
-              }
-
-            ]
-          }
-  - match: { _id: "my_pipeline_1" }
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline_2"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "{{field1}}",
-                  "value": "value"
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline_2" }
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline_3"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "remove" : {
-                  "field" : "{{field_to_remove}}"
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline_3" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline_1"
-        body: {
-          metadata: "0",
-          field1: "1",
-          field2: "2",
-          field3: "3"
-        }
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - length: { _source: 5 }
-  - match: { _source.field1: "1" }
-  - match: { _source.field2: "2" }
-  - match: { _source.field3: "3" }
-  - match: { _source.field4: "1/2/3" }
-  - match: { _source.metadata: ["0","1","2","3"] }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline_2"
-        body: {
-          field1: "field2"
-        }
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - length: { _source: 2 }
-  - match: { _source.field1: "field2" }
-  - match: { _source.field2: "value" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline_3"
-        body: {
-          field_to_remove: "field2",
-          field2: "2",
-        }
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - length: { _source: 1 }
-  - match: { _source.field_to_remove: "field2" }
-
----
-"Test on_failure metadata context templating":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_handled_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "remove" : {
-                  "field" : "field_to_remove",
-                  "on_failure" : [
-                    {
-                      "set" : {
-                        "field" : "error",
-                        "value" : "processor [{{ _ingest.on_failure_processor }}]: {{ _ingest.on_failure_message }}"
-                      }
-                    }
-                  ]
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_handled_pipeline" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_handled_pipeline"
-        body: {
-          do_nothing: "foo",
-        }
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - length: { _source: 2 }
-  - match: { _source.do_nothing: "foo" }
-  - match: { _source.error: "processor [remove]: field [field_to_remove] not present as part of path [field_to_remove]" }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/bulk.json b/rest-api-spec/src/main/resources/rest-api-spec/api/bulk.json
index 590054b..577a03f 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/bulk.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/bulk.json
@@ -40,10 +40,6 @@
         "fields": {
           "type": "list",
           "description" : "Default comma-separated list of fields to return in the response for updates"
-        },
-        "pipeline" : {
-          "type" : "string",
-          "description" : "The pipeline id to preprocess incoming documents with"
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/index.json b/rest-api-spec/src/main/resources/rest-api-spec/api/index.json
index 5c13f67..1b8f714 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/index.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/index.json
@@ -65,10 +65,6 @@
           "type" : "enum",
           "options" : ["internal", "external", "external_gte", "force"],
           "description" : "Specific version type"
-        },
-        "pipeline" : {
-          "type" : "string",
-          "description" : "The pipeline id to preprocess incoming documents with"
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.delete_pipeline.json b/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.delete_pipeline.json
deleted file mode 100644
index 69b8f53..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.delete_pipeline.json
+++ /dev/null
@@ -1,20 +0,0 @@
-{
-  "ingest.delete_pipeline": {
-    "documentation": "https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest.html",
-    "methods": [ "DELETE" ],
-    "url": {
-      "path": "/_ingest/pipeline/{id}",
-      "paths": [ "/_ingest/pipeline/{id}" ],
-      "parts": {
-        "id": {
-          "type" : "string",
-          "description" : "Pipeline ID",
-          "required" : true
-        }
-      },
-      "params": {
-      }
-    },
-    "body": null
-  }
-}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.get_pipeline.json b/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.get_pipeline.json
deleted file mode 100644
index 71772a2..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.get_pipeline.json
+++ /dev/null
@@ -1,20 +0,0 @@
-{
-  "ingest.get_pipeline": {
-    "documentation": "https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest.html",
-    "methods": [ "GET" ],
-    "url": {
-      "path": "/_ingest/pipeline/{id}",
-      "paths": [ "/_ingest/pipeline/{id}" ],
-      "parts": {
-        "id": {
-          "type" : "string",
-          "description" : "Comma separated list of pipeline ids. Wildcards supported",
-          "required" : true
-        }
-      },
-      "params": {
-      }
-    },
-    "body": null
-  }
-}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.put_pipeline.json b/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.put_pipeline.json
deleted file mode 100644
index fd88d35..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.put_pipeline.json
+++ /dev/null
@@ -1,23 +0,0 @@
-{
-  "ingest.put_pipeline": {
-    "documentation": "https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest.html",
-    "methods": [ "PUT" ],
-    "url": {
-      "path": "/_ingest/pipeline/{id}",
-      "paths": [ "/_ingest/pipeline/{id}" ],
-      "parts": {
-        "id": {
-          "type" : "string",
-          "description" : "Pipeline ID",
-          "required" : true
-        }
-      },
-      "params": {
-      }
-    },
-    "body": {
-      "description" : "The ingest definition",
-      "required" : true
-    }    
-  }
-}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.simulate.json b/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.simulate.json
deleted file mode 100644
index a4904ce..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.simulate.json
+++ /dev/null
@@ -1,28 +0,0 @@
-{
-  "ingest.simulate": {
-    "documentation": "https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest.html",
-    "methods": [ "GET", "POST" ],
-    "url": {
-      "path": "/_ingest/pipeline/_simulate",
-      "paths": [ "/_ingest/pipeline/_simulate", "/_ingest/pipeline/{id}/_simulate/" ],
-      "parts": {
-        "id": {
-          "type" : "string",
-          "description" : "Pipeline ID",
-          "required" : false
-        }
-      },
-      "params": {
-        "verbose": {
-          "type" : "boolean",
-          "description" : "Verbose mode. Display data output for each processor in executed pipeline",
-          "default" : false
-        }
-      }
-    },
-    "body": {
-      "description" : "The simulate definition",
-      "required" : true
-    }    
-  }
-}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/10_crud.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/10_crud.yaml
deleted file mode 100644
index 5a62247..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/10_crud.yaml
+++ /dev/null
@@ -1,108 +0,0 @@
----
-"Test basic pipeline crud":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field2",
-                  "value": "_value"
-                }
-              }
-            ]
-          }
-  - match: { _index: ".ingest" }
-  - match: { _type: "pipeline" }
-  - match: { _version: 1 }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      ingest.get_pipeline:
-        id: "my_pipeline"
-  - match: { my_pipeline._source.description: "_description" }
-  - match: { my_pipeline._version: 1 }
-
-  - do:
-      ingest.delete_pipeline:
-        id: "my_pipeline"
-  - match: { _index: ".ingest" }
-  - match: { _type: "pipeline" }
-  - match: { _version: 2 }
-  - match: { _id: "my_pipeline" }
-  - match: { found: true }
-
-  - do:
-      catch: missing
-      ingest.get_pipeline:
-        id: "my_pipeline"
-
----
-"Test invalid config":
-  - do:
-      catch: param
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                }
-              }
-            ]
-          }
-
----
-"Test basic pipeline with on_failure in processor":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field2",
-                  "value": "_value",
-                  "on_failure": [
-                    {
-                      "set" : {
-                        "field" : "field2",
-                        "value" : "_failed_value"
-                      }
-                    }
-                  ]
-                }
-              }
-            ]
-          }
-  - match: { _index: ".ingest" }
-  - match: { _type: "pipeline" }
-  - match: { _version: 1 }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      ingest.get_pipeline:
-        id: "my_pipeline"
-  - match: { my_pipeline._source.description: "_description" }
-  - match: { my_pipeline._version: 1 }
-
-  - do:
-      ingest.delete_pipeline:
-        id: "my_pipeline"
-  - match: { _index: ".ingest" }
-  - match: { _type: "pipeline" }
-  - match: { _version: 2 }
-  - match: { _id: "my_pipeline" }
-  - match: { found: true }
-
-  - do:
-      catch: missing
-      ingest.get_pipeline:
-        id: "my_pipeline"
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/20_date_processor.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/20_date_processor.yaml
deleted file mode 100644
index 8852e5e..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/20_date_processor.yaml
+++ /dev/null
@@ -1,37 +0,0 @@
----
-"Test date processor":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "date" : {
-                  "match_field" : "date_source_field",
-                  "target_field" : "date_target_field",
-                  "match_formats" : ["dd/MM/yyyy"],
-                  "timezone" : "Europe/Amsterdam"
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {date_source_field: "12/06/2010"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.date_source_field: "12/06/2010" }
-  - match: { _source.date_target_field: "2010-06-12T00:00:00.000+02:00" }
-
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/30_mutate.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/30_mutate.yaml
deleted file mode 100644
index a0a29e9..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/30_mutate.yaml
+++ /dev/null
@@ -1,150 +0,0 @@
----
-"Test mutate processors":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "new_field",
-                  "value": "new_value"
-                }
-              },
-              {
-                "append" : {
-                  "field" : "new_field",
-                  "value": ["item2", "item3", "item4"]
-                }
-              },
-              {
-                "rename" : {
-                  "field" : "field_to_rename",
-                  "to": "renamed_field"
-                }
-              },
-              {
-                "remove" : {
-                  "field" : "field_to_remove"
-                }
-              },
-              {
-                "lowercase" : {
-                  "field" : "field_to_lowercase"
-                }
-              },
-              {
-                "uppercase" : {
-                  "field" : "field_to_uppercase"
-                }
-              },
-              {
-                "trim" : {
-                  "field" : "field_to_trim"
-                }
-              },
-              {
-                "split" : {
-                  "field" : "field_to_split",
-                  "separator": "-"
-                }
-              },
-              {
-                "join" : {
-                  "field" : "field_to_join",
-                  "separator": "-"
-                }
-              },
-              {
-                "convert" : {
-                  "field" : "field_to_convert",
-                  "type": "integer"
-                }
-              },
-              {
-                "gsub" : {
-                  "field": "field_to_gsub",
-                  "pattern" : "-",
-                  "replacement" : "."
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {
-          field_to_rename: "value",
-          field_to_remove: "old_value",
-          field_to_lowercase: "LOWERCASE",
-          field_to_uppercase: "uppercase",
-          field_to_trim: "   trimmed   ",
-          field_to_split: "127-0-0-1",
-          field_to_join: ["127","0","0","1"],
-          field_to_convert: ["127","0","0","1"],
-          field_to_gsub: "127-0-0-1"
-        }
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - is_false: _source.field_to_rename
-  - is_false: _source.field_to_remove
-  - match: { _source.new_field: ["new_value", "item2", "item3", "item4"] }
-  - match: { _source.renamed_field: "value" }
-  - match: { _source.field_to_lowercase: "lowercase" }
-  - match: { _source.field_to_uppercase: "UPPERCASE" }
-  - match: { _source.field_to_trim: "trimmed" }
-  - match: { _source.field_to_split: ["127","0","0","1"] }
-  - match: { _source.field_to_join: "127-0-0-1" }
-  - match: { _source.field_to_convert: [127,0,0,1] }
-  - match: { _source.field_to_gsub: "127.0.0.1" }
-
----
-"Test metadata":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "_index",
-                  "value" : "surprise"
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field: "value"}
-
-  - do:
-      get:
-        index: surprise
-        type: test
-        id: 1
-  - length: { _source: 1 }
-  - match: { _source.field: "value" }
-
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/40_simulate.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/40_simulate.yaml
deleted file mode 100644
index 9947129..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/40_simulate.yaml
+++ /dev/null
@@ -1,317 +0,0 @@
----
-"Test simulate with stored ingest pipeline":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field2",
-                  "value" : "_value"
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      ingest.simulate:
-        id: "my_pipeline"
-        body: >
-          {
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-  - match: { docs.0.doc._source.foo: "bar" }
-  - match: { docs.0.doc._source.field2: "_value" }
-  - length: { docs.0.doc._ingest: 1 }
-  - is_true: docs.0.doc._ingest.timestamp
-
----
-"Test simulate with provided pipeline definition":
-  - do:
-      ingest.simulate:
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "set" : {
-                    "field" : "field2",
-                    "value" : "_value"
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-
----
-"Test simulate without index type and id":
-  - do:
-      ingest.simulate:
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "set" : {
-                    "field" : "field2",
-                    "value" : "_value"
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-
----
-"Test simulate with provided pipeline definition with on_failure block":
-  - do:
-      ingest.simulate:
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "rename" : {
-                    "field" : "does_not_exist",
-                    "to" : "field2",
-                    "on_failure" : [
-                      {
-                        "set" : {
-                          "field" : "field2",
-                          "value" : "_value"
-                        }
-                      }
-                    ]
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-  - match: { docs.0.doc._source.foo: "bar" }
-  - match: { docs.0.doc._source.field2: "_value" }
-  - length: { docs.0.doc._ingest: 1 }
-  - is_true: docs.0.doc._ingest.timestamp
-
----
-"Test simulate with no provided pipeline or pipeline_id":
-  - do:
-      catch: request
-      ingest.simulate:
-        body: >
-          {
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { error: 3 }
-  - match: { status: 400 }
-  - match: { error.type: "illegal_argument_exception" }
-  - match: { error.reason: "required property [pipeline] is missing" }
-
----
-"Test simulate with verbose flag":
-  - do:
-      ingest.simulate:
-        verbose: true
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "set" : {
-                    "field" : "field2",
-                    "value" : "_value"
-                  }
-                },
-                {
-                  "set" : {
-                    "field" : "field3",
-                    "value" : "third_val"
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-  - length: { docs.0.processor_results: 2 }
-  - match: { docs.0.processor_results.0.processor_id: "processor[set]-0" }
-  - length: { docs.0.processor_results.0.doc._source: 2 }
-  - match: { docs.0.processor_results.0.doc._source.foo: "bar" }
-  - match: { docs.0.processor_results.0.doc._source.field2: "_value" }
-  - length: { docs.0.processor_results.0.doc._ingest: 1 }
-  - is_true: docs.0.processor_results.0.doc._ingest.timestamp
-  - length: { docs.0.processor_results.1.doc._source: 3 }
-  - match: { docs.0.processor_results.1.doc._source.foo: "bar" }
-  - match: { docs.0.processor_results.1.doc._source.field2: "_value" }
-  - match: { docs.0.processor_results.1.doc._source.field3: "third_val" }
-  - length: { docs.0.processor_results.1.doc._ingest: 1 }
-  - is_true: docs.0.processor_results.1.doc._ingest.timestamp
-
----
-"Test simulate with exception thrown":
-  - do:
-      ingest.simulate:
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "uppercase" : {
-                    "field" : "foo"
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "not_foo": "bar"
-                }
-              },
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id2",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 2 }
-  - match: { docs.0.error.type: "illegal_argument_exception" }
-  - match: { docs.1.doc._source.foo: "BAR" }
-  - length: { docs.1.doc._ingest: 1 }
-  - is_true: docs.1.doc._ingest.timestamp
-
----
-"Test verbose simulate with exception thrown":
-  - do:
-      ingest.simulate:
-        verbose: true
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "convert" : {
-                    "field" : "foo",
-                    "type" : "integer"
-                  }
-                },
-                {
-                  "uppercase" : {
-                    "field" : "bar"
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar",
-                  "bar": "hello"
-                }
-              },
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id2",
-                "_source": {
-                  "foo": "5",
-                  "bar": "hello"
-                }
-              }
-            ]
-          }
-  - length: { docs: 2 }
-  - length: { docs.0.processor_results: 2 }
-  - match: { docs.0.processor_results.0.error.type: "illegal_argument_exception" }
-  - match: { docs.0.processor_results.1.doc._index: "index" }
-  - match: { docs.0.processor_results.1.doc._type: "type" }
-  - match: { docs.0.processor_results.1.doc._id: "id" }
-  - match: { docs.0.processor_results.1.doc._source.foo: "bar" }
-  - match: { docs.0.processor_results.1.doc._source.bar: "HELLO" }
-  - length: { docs.0.processor_results.1.doc._ingest: 1 }
-  - is_true: docs.0.processor_results.1.doc._ingest.timestamp
-  - match: { docs.1.processor_results.0.doc._source.foo: 5 }
-  - match: { docs.1.processor_results.0.doc._source.bar: "hello" }
-  - length: { docs.1.processor_results.0.doc._ingest: 1 }
-  - is_true: docs.1.processor_results.0.doc._ingest.timestamp
-  - match: { docs.1.processor_results.1.doc._source.foo: 5 }
-  - match: { docs.1.processor_results.1.doc._source.bar: "HELLO" }
-  - length: { docs.1.processor_results.1.doc._ingest: 1 }
-  - is_true: docs.1.processor_results.1.doc._ingest.timestamp
-
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/50_on_failure.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/50_on_failure.yaml
deleted file mode 100644
index a01b0da..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/50_on_failure.yaml
+++ /dev/null
@@ -1,108 +0,0 @@
----
-"Test Pipeline With On Failure Block":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "_executed",
-                  "value" : true
-                }
-              },
-              {
-                "date" : {
-                  "match_field" : "date",
-                  "target_field" : "date",
-                  "match_formats" : ["yyyy"]
-                }
-              }
-            ],
-            "on_failure" : [
-              {
-                "set" : {
-                  "field" : "_failed",
-                  "value" : true
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field1: "value1"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.field1: "value1" }
-  - match: { _source._executed: true }
-  - match: { _source._failed: true }
-
----
-"Test Pipeline With Nested Processor On Failures":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "rename" : {
-                  "field" : "foofield",
-                  "to" : "field1",
-                  "on_failure" : [
-                    {
-                      "set" : {
-                        "field" : "foofield",
-                        "value" : "exists"
-                      }
-                    },
-                    {
-                      "rename" : {
-                        "field" : "foofield2",
-                        "to" : "field1",
-                        "on_failure" : [
-                          {
-                            "set" : {
-                            "field" : "foofield2",
-                            "value" : "ran"
-                            }
-                          }
-                        ]
-                      }
-                    }
-                  ]
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field1: "value1"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.field1: "value1" }
-  - match: { _source.foofield: "exists" }
-  - match: { _source.foofield2: "ran" }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/60_fail.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/60_fail.yaml
deleted file mode 100644
index d491a95..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/60_fail.yaml
+++ /dev/null
@@ -1,68 +0,0 @@
----
-"Test Fail Processor":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "fail" : {
-                  "message" : "error_message"
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      catch: request
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {}
-
----
-"Test fail with on_failure":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "fail" : {
-                  "message" : "error",
-                  "on_failure" : [
-                    {
-                      "set" : {
-                        "field" : "error_message",
-                        "value" : "fail_processor_ran"
-                      }
-                    }
-                  ]
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.error_message: "fail_processor_ran" }
-
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/70_bulk.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/70_bulk.yaml
deleted file mode 100644
index b70f05a..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/70_bulk.yaml
+++ /dev/null
@@ -1,105 +0,0 @@
-setup:
-  - do:
-      ingest.put_pipeline:
-        id: "pipeline1"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field1",
-                  "value": "value1"
-                }
-              }
-            ]
-          }
-
-  - do:
-      ingest.put_pipeline:
-        id: "pipeline2"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field2",
-                  "value": "value2"
-                }
-              }
-            ]
-          }
-
----
-"Test bulk request without default pipeline":
-
-  - do:
-      bulk:
-        body:
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id1
-              pipeline: pipeline1
-          - f1: v1
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id2
-          - f1: v2
-
-  - do:
-      get:
-        index: test_index
-        type: test_type
-        id: test_id1
-
-  - match: {_source.field1: value1}
-  - is_false: _source.field2
-
-  - do:
-      get:
-        index: test_index
-        type: test_type
-        id: test_id2
-
-  - is_false: _source.field1
-  - is_false: _source.field2
-
----
-"Test bulk request with default pipeline":
-
-  - do:
-      bulk:
-        pipeline: pipeline1
-        body:
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id1
-          - f1: v1
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id2
-              pipeline: pipeline2
-          - f1: v2
-  - do:
-      get:
-        index: test_index
-        type: test_type
-        id: test_id1
-
-  - match: {_source.field1: value1}
-  - is_false: _source.field2
-
-  - do:
-      get:
-        index: test_index
-        type: test_type
-        id: test_id2
-
-  - is_false: _source.field1
-  - match: {_source.field2: value2}
-
diff --git a/settings.gradle b/settings.gradle
index 39e0b4f..55126b3 100644
--- a/settings.gradle
+++ b/settings.gradle
@@ -14,7 +14,6 @@ List projects = [
   'modules:lang-expression',
   'modules:lang-groovy',
   'modules:lang-mustache',
-  'modules:ingest-grok',
   'plugins:analysis-icu',
   'plugins:analysis-kuromoji',
   'plugins:analysis-phonetic',
@@ -25,7 +24,6 @@ List projects = [
   'plugins:discovery-ec2',
   'plugins:discovery-gce',
   'plugins:discovery-multicast',
-  'plugins:ingest-geoip',
   'plugins:lang-javascript',
   'plugins:lang-plan-a',
   'plugins:lang-python',
@@ -42,8 +40,6 @@ List projects = [
   'qa:smoke-test-client',
   'qa:smoke-test-multinode',
   'qa:smoke-test-plugins',
-  'qa:ingest-with-mustache',
-  'qa:ingest-disabled',
   'qa:vagrant',
 ]
 
diff --git a/test/framework/build.gradle b/test/framework/build.gradle
index 5c607e1..46728d0 100644
--- a/test/framework/build.gradle
+++ b/test/framework/build.gradle
@@ -33,7 +33,7 @@ dependencies {
   compile 'org.elasticsearch:securemock:1.2'
 }
 
-compileJava.options.compilerArgs << '-Xlint:-cast,-deprecation,-rawtypes,-serial,-try,-unchecked'
+compileJava.options.compilerArgs << '-Xlint:-cast,-rawtypes,-serial,-try,-unchecked'
 compileTestJava.options.compilerArgs << '-Xlint:-rawtypes'
 
 // the main files are actually test files, so use the appopriate forbidden api sigs
diff --git a/test/framework/src/main/java/org/elasticsearch/common/io/PathUtilsForTesting.java b/test/framework/src/main/java/org/elasticsearch/common/io/PathUtilsForTesting.java
index fee053e..36f766b 100644
--- a/test/framework/src/main/java/org/elasticsearch/common/io/PathUtilsForTesting.java
+++ b/test/framework/src/main/java/org/elasticsearch/common/io/PathUtilsForTesting.java
@@ -23,21 +23,22 @@ import org.apache.lucene.util.LuceneTestCase;
 
 import java.nio.file.FileSystem;
 
-/** 
- * Exposes some package private stuff in PathUtils for framework purposes only! 
+/**
+ * Exposes some package private stuff in PathUtils for framework purposes only!
  */
 public class PathUtilsForTesting {
-    
+
     /** Sets a new default filesystem for testing */
+    @SuppressWarnings("deprecation") // https://github.com/elastic/elasticsearch/issues/15845
     public static void setup() {
         installMock(LuceneTestCase.getBaseTempDirForTestClass().getFileSystem());
     }
-    
+
     /** Installs a mock filesystem for testing */
     public static void installMock(FileSystem mock) {
         PathUtils.DEFAULT = mock;
     }
-    
+
     /** Resets filesystem back to the real system default */
     public static void teardown() {
         PathUtils.DEFAULT = PathUtils.ACTUAL_DEFAULT;
diff --git a/test/framework/src/main/java/org/elasticsearch/ingest/RandomDocumentPicks.java b/test/framework/src/main/java/org/elasticsearch/ingest/RandomDocumentPicks.java
deleted file mode 100644
index 3f350cf..0000000
--- a/test/framework/src/main/java/org/elasticsearch/ingest/RandomDocumentPicks.java
+++ /dev/null
@@ -1,238 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import com.carrotsearch.randomizedtesting.generators.RandomInts;
-import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.ingest.core.IngestDocument;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.TreeMap;
-
-public final class RandomDocumentPicks {
-
-    private RandomDocumentPicks() {
-
-    }
-
-    /**
-     * Returns a random field name. Can be a leaf field name or the
-     * path to refer to a field name using the dot notation.
-     */
-    public static String randomFieldName(Random random) {
-        int numLevels = RandomInts.randomIntBetween(random, 1, 5);
-        String fieldName = "";
-        for (int i = 0; i < numLevels; i++) {
-            if (i > 0) {
-                fieldName += ".";
-            }
-            fieldName += randomString(random);
-        }
-        return fieldName;
-    }
-
-    /**
-     * Returns a random leaf field name.
-     */
-    public static String randomLeafFieldName(Random random) {
-        String fieldName;
-        do {
-            fieldName = randomString(random);
-        } while (fieldName.contains("."));
-        return fieldName;
-    }
-
-    /**
-     * Returns a randomly selected existing field name out of the fields that are contained
-     * in the document provided as an argument.
-     */
-    public static String randomExistingFieldName(Random random, IngestDocument ingestDocument) {
-        Map<String, Object> source = new TreeMap<>(ingestDocument.getSourceAndMetadata());
-        Map.Entry<String, Object> randomEntry = RandomPicks.randomFrom(random, source.entrySet());
-        String key = randomEntry.getKey();
-        while (randomEntry.getValue() instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> map = (Map<String, Object>) randomEntry.getValue();
-            Map<String, Object> treeMap = new TreeMap<>(map);
-            randomEntry = RandomPicks.randomFrom(random, treeMap.entrySet());
-            key += "." + randomEntry.getKey();
-        }
-        assert ingestDocument.getFieldValue(key, Object.class) != null;
-        return key;
-    }
-
-    /**
-     * Adds a random non existing field to the provided document and associates it
-     * with the provided value. The field will be added at a random position within the document,
-     * not necessarily at the top level using a leaf field name.
-     */
-    public static String addRandomField(Random random, IngestDocument ingestDocument, Object value) {
-        String fieldName;
-        do {
-            fieldName = randomFieldName(random);
-        } while (canAddField(fieldName, ingestDocument) == false);
-        ingestDocument.setFieldValue(fieldName, value);
-        return fieldName;
-    }
-
-    /**
-     * Checks whether the provided field name can be safely added to the provided document.
-     * When the provided field name holds the path using the dot notation, we have to make sure
-     * that each node of the tree either doesn't exist or is a map, otherwise new fields cannot be added.
-     */
-    public static boolean canAddField(String path, IngestDocument ingestDocument) {
-        String[] pathElements = Strings.splitStringToArray(path, '.');
-        Map<String, Object> innerMap = ingestDocument.getSourceAndMetadata();
-        if (pathElements.length > 1) {
-            for (int i = 0; i < pathElements.length - 1; i++) {
-                Object currentLevel = innerMap.get(pathElements[i]);
-                if (currentLevel == null) {
-                    return true;
-                }
-                if (currentLevel instanceof Map == false) {
-                    return false;
-                }
-                @SuppressWarnings("unchecked")
-                Map<String, Object> map = (Map<String, Object>) currentLevel;
-                innerMap = map;
-            }
-        }
-        String leafKey = pathElements[pathElements.length - 1];
-        return innerMap.containsKey(leafKey) == false;
-    }
-
-    /**
-     * Generates a random document and random metadata
-     */
-    public static IngestDocument randomIngestDocument(Random random) {
-        return randomIngestDocument(random, randomSource(random));
-    }
-
-    /**
-     * Generates a document that holds random metadata and the document provided as a map argument
-     */
-    public static IngestDocument randomIngestDocument(Random random, Map<String, Object> source) {
-        String index = randomString(random);
-        String type = randomString(random);
-        String id = randomString(random);
-        String routing = null;
-        if (random.nextBoolean()) {
-            routing = randomString(random);
-        }
-        String parent = null;
-        if (random.nextBoolean()) {
-            parent = randomString(random);
-        }
-        String timestamp = null;
-        if (random.nextBoolean()) {
-            timestamp = randomString(random);
-        }
-        String ttl = null;
-        if (random.nextBoolean()) {
-            ttl = randomString(random);
-        }
-        return new IngestDocument(index, type, id, routing, parent, timestamp, ttl, source);
-    }
-
-    public static Map<String, Object> randomSource(Random random) {
-        Map<String, Object> document = new HashMap<>();
-        addRandomFields(random, document, 0);
-        return document;
-    }
-
-    /**
-     * Generates a random field value, can be a string, a number, a list of an object itself.
-     */
-    public static Object randomFieldValue(Random random) {
-        return randomFieldValue(random, 0);
-    }
-
-    private static Object randomFieldValue(Random random, int currentDepth) {
-        switch(RandomInts.randomIntBetween(random, 0, 8)) {
-            case 0:
-                return randomString(random);
-            case 1:
-                return random.nextInt();
-            case 2:
-                return random.nextBoolean();
-            case 3:
-                return random.nextDouble();
-            case 4:
-                List<String> stringList = new ArrayList<>();
-                int numStringItems = RandomInts.randomIntBetween(random, 1, 10);
-                for (int j = 0; j < numStringItems; j++) {
-                    stringList.add(randomString(random));
-                }
-                return stringList;
-            case 5:
-                List<Integer> intList = new ArrayList<>();
-                int numIntItems = RandomInts.randomIntBetween(random, 1, 10);
-                for (int j = 0; j < numIntItems; j++) {
-                    intList.add(random.nextInt());
-                }
-                return intList;
-            case 6:
-                List<Boolean> booleanList = new ArrayList<>();
-                int numBooleanItems = RandomInts.randomIntBetween(random, 1, 10);
-                for (int j = 0; j < numBooleanItems; j++) {
-                    booleanList.add(random.nextBoolean());
-                }
-                return booleanList;
-            case 7:
-                List<Double> doubleList = new ArrayList<>();
-                int numDoubleItems = RandomInts.randomIntBetween(random, 1, 10);
-                for (int j = 0; j < numDoubleItems; j++) {
-                    doubleList.add(random.nextDouble());
-                }
-                return doubleList;
-            case 8:
-                Map<String, Object> newNode = new HashMap<>();
-                addRandomFields(random, newNode, ++currentDepth);
-                return newNode;
-            default:
-                throw new UnsupportedOperationException();
-        }
-    }
-
-    public static String randomString(Random random) {
-        if (random.nextBoolean()) {
-            return RandomStrings.randomAsciiOfLengthBetween(random, 1, 10);
-        }
-        return RandomStrings.randomUnicodeOfCodepointLengthBetween(random, 1, 10);
-    }
-
-    private static void addRandomFields(Random random, Map<String, Object> parentNode, int currentDepth) {
-        if (currentDepth > 5) {
-            return;
-        }
-        int numFields = RandomInts.randomIntBetween(random, 1, 10);
-        for (int i = 0; i < numFields; i++) {
-            String fieldName = randomLeafFieldName(random);
-            Object fieldValue = randomFieldValue(random, currentDepth);
-            parentNode.put(fieldName, fieldValue);
-        }
-    }
-}
diff --git a/test/framework/src/main/java/org/elasticsearch/ingest/TestProcessor.java b/test/framework/src/main/java/org/elasticsearch/ingest/TestProcessor.java
deleted file mode 100644
index 5c4dd70..0000000
--- a/test/framework/src/main/java/org/elasticsearch/ingest/TestProcessor.java
+++ /dev/null
@@ -1,69 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.function.Consumer;
-
-/**
- * Processor used for testing, keeps track of how many times it is invoked and
- * accepts a {@link Consumer} of {@link IngestDocument} to be called when executed.
- */
-public class TestProcessor implements Processor {
-
-    private final String type;
-    private final Consumer<IngestDocument> ingestDocumentConsumer;
-    private final AtomicInteger invokedCounter = new AtomicInteger();
-
-    public TestProcessor(Consumer<IngestDocument> ingestDocumentConsumer) {
-        this("test-processor", ingestDocumentConsumer);
-    }
-
-    public TestProcessor(String type, Consumer<IngestDocument> ingestDocumentConsumer) {
-        this.ingestDocumentConsumer = ingestDocumentConsumer;
-        this.type = type;
-    }
-
-    @Override
-    public void execute(IngestDocument ingestDocument) throws Exception {
-        invokedCounter.incrementAndGet();
-        ingestDocumentConsumer.accept(ingestDocument);
-    }
-
-    @Override
-    public String getType() {
-        return type;
-    }
-
-    public int getInvokedCounter() {
-        return invokedCounter.get();
-    }
-
-    public static final class Factory implements Processor.Factory<TestProcessor> {
-        @Override
-        public TestProcessor create(Map<String, Object> config) throws Exception {
-            return new TestProcessor(ingestDocument -> {});
-        }
-    }
-}
diff --git a/test/framework/src/main/java/org/elasticsearch/ingest/TestTemplateService.java b/test/framework/src/main/java/org/elasticsearch/ingest/TestTemplateService.java
deleted file mode 100644
index 9330db1..0000000
--- a/test/framework/src/main/java/org/elasticsearch/ingest/TestTemplateService.java
+++ /dev/null
@@ -1,58 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.core.TemplateService;
-
-import java.util.Map;
-
-public class TestTemplateService implements TemplateService {
-
-    public static TemplateService instance() {
-        return new TestTemplateService();
-    }
-
-    private TestTemplateService() {
-    }
-
-    @Override
-    public Template compile(String template) {
-        return new MockTemplate(template);
-    }
-
-    public static class MockTemplate implements TemplateService.Template {
-
-        private final String expected;
-
-        public MockTemplate(String expected) {
-            this.expected = expected;
-        }
-
-        @Override
-        public String execute(Map<String, Object> model) {
-            return expected;
-        }
-
-        @Override
-        public String getKey() {
-            return expected;
-        }
-    }
-}
diff --git a/test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java b/test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java
index ea2796a..81dbc38 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java
@@ -24,6 +24,7 @@ import com.carrotsearch.randomizedtesting.SysGlobals;
 import com.carrotsearch.randomizedtesting.generators.RandomInts;
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
 import com.carrotsearch.randomizedtesting.generators.RandomStrings;
+
 import org.apache.lucene.store.StoreRateLimiting;
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.ElasticsearchException;
@@ -117,7 +118,6 @@ import java.util.function.Predicate;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
-import static junit.framework.Assert.fail;
 import static org.apache.lucene.util.LuceneTestCase.TEST_NIGHTLY;
 import static org.apache.lucene.util.LuceneTestCase.rarely;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
@@ -127,6 +127,7 @@ import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.greaterThan;
 import static org.hamcrest.Matchers.greaterThanOrEqualTo;
 import static org.junit.Assert.assertThat;
+import static org.junit.Assert.fail;
 
 /**
  * InternalTestCluster manages a set of JVM private nodes and allows convenient access to them.
@@ -1045,6 +1046,7 @@ public final class InternalTestCluster extends TestCluster {
         }
     }
 
+    @SuppressWarnings("deprecation") // https://github.com/elastic/elasticsearch/issues/15844
     private void randomlyResetClients() throws IOException {
         // only reset the clients on nightly tests, it causes heavy load...
         if (RandomizedTest.isNightly() && rarely(random)) {
diff --git a/test/framework/src/main/java/org/elasticsearch/test/disruption/LongGCDisruption.java b/test/framework/src/main/java/org/elasticsearch/test/disruption/LongGCDisruption.java
index 1e8dcb1..591540e 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/disruption/LongGCDisruption.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/disruption/LongGCDisruption.java
@@ -75,6 +75,7 @@ public class LongGCDisruption extends SingleNodeDisruption {
         return TimeValue.timeValueMillis(0);
     }
 
+    @SuppressWarnings("deprecation") // stops/resumes threads intentionally
     @SuppressForbidden(reason = "stops/resumes threads intentionally")
     protected boolean stopNodeThreads(String node, Set<Thread> nodeThreads) {
         Thread[] allThreads = null;
@@ -118,6 +119,7 @@ public class LongGCDisruption extends SingleNodeDisruption {
         return stopped;
     }
 
+    @SuppressWarnings("deprecation") // stops/resumes threads intentionally
     @SuppressForbidden(reason = "stops/resumes threads intentionally")
     protected void resumeThreads(Set<Thread> threads) {
         for (Thread thread : threads) {
diff --git a/test/framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java b/test/framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
index 9d8ad7f..61755f7 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
@@ -513,9 +513,9 @@ public class ElasticsearchAssertions {
     public static <T extends Query> T assertBooleanSubQuery(Query query, Class<T> subqueryType, int i) {
         assertThat(query, instanceOf(BooleanQuery.class));
         BooleanQuery q = (BooleanQuery) query;
-        assertThat(q.getClauses().length, greaterThan(i));
-        assertThat(q.getClauses()[i].getQuery(), instanceOf(subqueryType));
-        return (T) q.getClauses()[i].getQuery();
+        assertThat(q.clauses().size(), greaterThan(i));
+        assertThat(q.clauses().get(i).getQuery(), instanceOf(subqueryType));
+        return subqueryType.cast(q.clauses().get(i).getQuery());
     }
 
     /**
diff --git a/test/framework/src/main/java/org/elasticsearch/test/store/MockFSDirectoryService.java b/test/framework/src/main/java/org/elasticsearch/test/store/MockFSDirectoryService.java
index 58a7278..25c29f0 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/store/MockFSDirectoryService.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/store/MockFSDirectoryService.java
@@ -21,11 +21,13 @@ package org.elasticsearch.test.store;
 
 import com.carrotsearch.randomizedtesting.SeedUtils;
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
+
 import org.apache.lucene.index.CheckIndex;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.store.BaseDirectoryWrapper;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.LockFactory;
+import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.store.MockDirectoryWrapper;
 import org.apache.lucene.store.StoreRateLimiting;
 import org.apache.lucene.util.LuceneTestCase;
@@ -105,6 +107,7 @@ public class MockFSDirectoryService extends FsDirectoryService {
         throw new UnsupportedOperationException();
     }
 
+    @SuppressWarnings("deprecation") // https://github.com/elastic/elasticsearch/issues/15846
     public static void checkIndex(ESLogger logger, Store store, ShardId shardId) {
         if (store.tryIncRef()) {
             logger.info("start check index");
@@ -113,10 +116,6 @@ public class MockFSDirectoryService extends FsDirectoryService {
                 if (!Lucene.indexExists(dir)) {
                     return;
                 }
-                if (IndexWriter.isLocked(dir)) {
-                    ESTestCase.checkIndexFailed = true;
-                    throw new IllegalStateException("IndexWriter is still open on shard " + shardId);
-                }
                 try (CheckIndex checkIndex = new CheckIndex(dir)) {
                     BytesStreamOutput os = new BytesStreamOutput();
                     PrintStream out = new PrintStream(os, false, StandardCharsets.UTF_8.name());
@@ -134,6 +133,9 @@ public class MockFSDirectoryService extends FsDirectoryService {
                             logger.debug("check index [success]\n{}", new String(os.bytes().toBytes(), StandardCharsets.UTF_8));
                         }
                     }
+                } catch (LockObtainFailedException e) {
+                    ESTestCase.checkIndexFailed = true;
+                    throw new IllegalStateException("IndexWriter is still open on shard " + shardId, e);
                 }
             } catch (Exception e) {
                 logger.warn("failed to check index", e);
