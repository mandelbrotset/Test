diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index 6c5923e..4491737 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -74,7 +74,7 @@ Then sit back and wait. There will probably be discussion about the pull request
 Contributing to the Elasticsearch codebase
 ------------------------------------------
 
-**Repository:** [https://github.com/elasticsearch/elasticsearch](https://github.com/elastic/elasticsearch)
+**Repository:** [https://github.com/elastic/elasticsearch](https://github.com/elastic/elasticsearch)
 
 Make sure you have [Maven](http://maven.apache.org) installed, as Elasticsearch uses it as its build system. Integration with IntelliJ and Eclipse should work out of the box. Eclipse users can automatically configure their IDE by running `mvn eclipse:eclipse` and then importing the project into their workspace: `File > Import > Existing project into workspace` and make sure to select `Search for nested projects...` option as Elasticsearch is a multi-module maven project. Additionally you will want to ensure that Eclipse is using 2048m of heap by modifying `eclipse.ini` accordingly to avoid GC overhead errors. Please make sure the [m2e-connector](http://marketplace.eclipse.org/content/m2e-connector-maven-dependency-plugin) is not installed in your Eclipse distribution as it will interfere with setup performed by `mvn eclipse:eclipse`.
 
diff --git a/Vagrantfile b/Vagrantfile
index ab0e322..7c76e23 100644
--- a/Vagrantfile
+++ b/Vagrantfile
@@ -32,7 +32,10 @@ Vagrant.configure(2) do |config|
   end
   config.vm.define "vivid" do |config|
     config.vm.box = "ubuntu/vivid64"
-    ubuntu_common config
+    ubuntu_common config, extra: <<-SHELL
+      # Install Jayatana so we can work around it being present.
+      [ -f /usr/share/java/jayatanaag.jar ] || install jayatana
+    SHELL
   end
   # Wheezy's backports don't contain Openjdk 8 and the backflips required to
   # get the sun jdk on there just aren't worth it. We have jessie for testing
@@ -116,11 +119,11 @@ SOURCE_PROMPT
   end
 end
 
-def ubuntu_common(config)
-  deb_common config, 'apt-add-repository -y ppa:openjdk-r/ppa > /dev/null 2>&1', 'openjdk-r-*'
+def ubuntu_common(config, extra: '')
+  deb_common config, 'apt-add-repository -y ppa:openjdk-r/ppa > /dev/null 2>&1', 'openjdk-r-*', extra: extra
 end
 
-def deb_common(config, add_openjdk_repository_command, openjdk_list)
+def deb_common(config, add_openjdk_repository_command, openjdk_list, extra: '')
   # http://foo-o-rama.com/vagrant--stdin-is-not-a-tty--fix.html
   config.vm.provision "fix-no-tty", type: "shell" do |s|
       s.privileged = false
@@ -137,6 +140,7 @@ def deb_common(config, add_openjdk_repository_command, openjdk_list)
         (echo "Importing java-8 ppa" &&
           #{add_openjdk_repository_command} &&
           apt-get update)
+      #{extra}
 SHELL
   )
 end
diff --git a/core/.local-3.0.0-SNAPSHOT-test-execution-times.log b/core/.local-3.0.0-SNAPSHOT-test-execution-times.log
deleted file mode 100644
index 8b79a35..0000000
--- a/core/.local-3.0.0-SNAPSHOT-test-execution-times.log
+++ /dev/null
@@ -1,559 +0,0 @@
-org.apache.lucene.analysis.miscellaneous.TruncateTokenFilterTests=190
-org.apache.lucene.analysis.miscellaneous.UniqueTokenFilterTests=187
-org.apache.lucene.queries.BlendedTermQueryTests=696
-org.apache.lucene.queries.MinDocQueryTests=503
-org.apache.lucene.search.postingshighlight.CustomPassageFormatterTests=69
-org.apache.lucene.search.postingshighlight.CustomPostingsHighlighterTests=599
-org.apache.lucene.search.postingshighlight.CustomSeparatorBreakIteratorTests=99
-org.apache.lucene.util.SloppyMathTests=734
-org.elasticsearch.ESExceptionTests=701
-org.elasticsearch.ExceptionSerializationTests=3740
-org.elasticsearch.NamingConventionTests=1061
-org.elasticsearch.SpecialPermissionTests=90
-org.elasticsearch.VersionTests=179
-org.elasticsearch.action.OriginalIndicesTests=66
-org.elasticsearch.action.admin.cluster.health.ClusterHealthResponsesTests=120
-org.elasticsearch.action.admin.cluster.state.ClusterStateRequestTests=32
-org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilderTests=51
-org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequestTests=60
-org.elasticsearch.action.admin.indices.segments.IndicesSegmentsRequestTests=2294
-org.elasticsearch.action.admin.indices.shards.IndicesShardStoreResponseTests=61
-org.elasticsearch.action.admin.indices.stats.IndicesStatsTests=2832
-org.elasticsearch.action.admin.indices.template.put.MetaDataIndexTemplateServiceTests=72
-org.elasticsearch.action.admin.indices.warmer.put.PutWarmerRequestTests=98
-org.elasticsearch.action.bulk.BulkRequestTests=578
-org.elasticsearch.action.count.CountRequestBuilderTests=495
-org.elasticsearch.action.count.CountRequestTests=21
-org.elasticsearch.action.count.CountResponseTests=63
-org.elasticsearch.action.fieldstats.FieldStatsRequestTests=45
-org.elasticsearch.action.get.MultiGetShardRequestTests=81
-org.elasticsearch.action.index.IndexRequestBuilderTests=372
-org.elasticsearch.action.index.IndexRequestTests=78
-org.elasticsearch.action.indexedscripts.get.GetIndexedScriptRequestTests=58
-org.elasticsearch.action.percolate.MultiPercolatorRequestTests=144
-org.elasticsearch.action.search.MultiSearchRequestTests=57
-org.elasticsearch.action.search.SearchRequestBuilderTests=291
-org.elasticsearch.action.support.IndicesOptionsTests=83
-org.elasticsearch.action.support.ListenableActionFutureTests=55
-org.elasticsearch.action.support.TransportActionFilterChainTests=52
-org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeActionTests=110
-org.elasticsearch.action.support.replication.BroadcastReplicationTests=151
-org.elasticsearch.action.support.replication.ShardReplicationTests=236
-org.elasticsearch.action.termvectors.TermVectorsUnitTests=293
-org.elasticsearch.action.update.UpdateRequestTests=67
-org.elasticsearch.bootstrap.BootstrapCliParserTests=73
-org.elasticsearch.bootstrap.ESPolicyTests=55
-org.elasticsearch.bootstrap.JNANativesTests=77
-org.elasticsearch.bootstrap.JarHellTests=171
-org.elasticsearch.bootstrap.JavaVersionTests=65
-org.elasticsearch.bootstrap.SeccompTests=123
-org.elasticsearch.bootstrap.SecurityTests=238
-org.elasticsearch.client.node.NodeClientHeadersTests=355
-org.elasticsearch.client.transport.TransportClientHeadersTests=640
-org.elasticsearch.client.transport.TransportClientNodesServiceTests=3307
-org.elasticsearch.cluster.ClusterModuleTests=73
-org.elasticsearch.cluster.ClusterStateTests=17
-org.elasticsearch.cluster.DiskUsageTests=107
-org.elasticsearch.cluster.block.ClusterBlockTests=41
-org.elasticsearch.cluster.metadata.DateMathExpressionResolverTests=158
-org.elasticsearch.cluster.metadata.HumanReadableIndexSettingsTests=41
-org.elasticsearch.cluster.metadata.IndexNameExpressionResolverTests=370
-org.elasticsearch.cluster.metadata.MappingMetaDataParserTests=103
-org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeServiceTests=26
-org.elasticsearch.cluster.metadata.ToAndFromJsonMetaDataTests=122
-org.elasticsearch.cluster.metadata.WildcardExpressionResolverTests=80
-org.elasticsearch.cluster.node.DiscoveryNodeFiltersTests=62
-org.elasticsearch.cluster.routing.AllocationIdTests=79
-org.elasticsearch.cluster.routing.RoutingBackwardCompatibilityTests=3477
-org.elasticsearch.cluster.routing.RoutingServiceTests=368
-org.elasticsearch.cluster.routing.RoutingTableTests=123
-org.elasticsearch.cluster.routing.ShardRoutingTests=179
-org.elasticsearch.cluster.routing.UnassignedInfoTests=146
-org.elasticsearch.cluster.routing.allocation.AddIncrementallyTests=97
-org.elasticsearch.cluster.routing.allocation.AllocationCommandsTests=137
-org.elasticsearch.cluster.routing.allocation.AllocationPriorityTests=34
-org.elasticsearch.cluster.routing.allocation.AwarenessAllocationTests=334
-org.elasticsearch.cluster.routing.allocation.BalanceConfigurationTests=426
-org.elasticsearch.cluster.routing.allocation.BalanceUnbalancedClusterTests=9557
-org.elasticsearch.cluster.routing.allocation.ClusterRebalanceRoutingTests=908
-org.elasticsearch.cluster.routing.allocation.ConcurrentRebalanceRoutingTests=157
-org.elasticsearch.cluster.routing.allocation.DeadNodesAllocationTests=72
-org.elasticsearch.cluster.routing.allocation.ElectReplicaAsPrimaryDuringRelocationTests=50
-org.elasticsearch.cluster.routing.allocation.ExpectedShardSizeAllocationTests=127
-org.elasticsearch.cluster.routing.allocation.FailedNodeRoutingTests=48
-org.elasticsearch.cluster.routing.allocation.FailedShardsRoutingTests=151
-org.elasticsearch.cluster.routing.allocation.FilterRoutingTests=53
-org.elasticsearch.cluster.routing.allocation.IndexBalanceTests=118
-org.elasticsearch.cluster.routing.allocation.NodeVersionAllocationDeciderTests=424
-org.elasticsearch.cluster.routing.allocation.PreferLocalPrimariesToRelocatingPrimariesTests=75
-org.elasticsearch.cluster.routing.allocation.PreferPrimaryAllocationTests=97
-org.elasticsearch.cluster.routing.allocation.PrimaryElectionRoutingTests=337
-org.elasticsearch.cluster.routing.allocation.PrimaryNotRelocatedWhileBeingRecoveredTests=2581
-org.elasticsearch.cluster.routing.allocation.RandomAllocationDeciderTests=53
-org.elasticsearch.cluster.routing.allocation.RebalanceAfterActiveTests=79
-org.elasticsearch.cluster.routing.allocation.ReplicaAllocatedAfterPrimaryTests=98
-org.elasticsearch.cluster.routing.allocation.RoutingNodesIntegrityTests=47
-org.elasticsearch.cluster.routing.allocation.SameShardRoutingTests=99
-org.elasticsearch.cluster.routing.allocation.ShardVersioningTests=54
-org.elasticsearch.cluster.routing.allocation.ShardsLimitAllocationTests=69
-org.elasticsearch.cluster.routing.allocation.SingleShardNoReplicasRoutingTests=209
-org.elasticsearch.cluster.routing.allocation.SingleShardOneReplicaRoutingTests=57
-org.elasticsearch.cluster.routing.allocation.StartedShardsRoutingTests=83
-org.elasticsearch.cluster.routing.allocation.TenShardsOneReplicaRoutingTests=39
-org.elasticsearch.cluster.routing.allocation.ThrottlingAllocationTests=14
-org.elasticsearch.cluster.routing.allocation.UpdateNumberOfReplicasTests=30
-org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDeciderTests=75
-org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDeciderUnitTests=107
-org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationTests=108
-org.elasticsearch.cluster.routing.operation.hash.murmur3.Murmur3HashFunctionTests=71
-org.elasticsearch.cluster.serialization.ClusterSerializationTests=25
-org.elasticsearch.cluster.serialization.ClusterStateToStringTests=42
-org.elasticsearch.cluster.serialization.DiffableTests=95
-org.elasticsearch.cluster.settings.SettingsValidatorTests=53
-org.elasticsearch.cluster.structure.RoutingIteratorTests=1016
-org.elasticsearch.codecs.CodecTests=9816
-org.elasticsearch.common.Base64Tests=2127
-org.elasticsearch.common.BooleansTests=54
-org.elasticsearch.common.ChannelsTests=222
-org.elasticsearch.common.ParseFieldTests=92
-org.elasticsearch.common.PidFileTests=2205
-org.elasticsearch.common.StringsTests=96
-org.elasticsearch.common.TableTests=90
-org.elasticsearch.common.UUIDTests=1844
-org.elasticsearch.common.blobstore.BlobStoreTests=44
-org.elasticsearch.common.breaker.MemoryCircuitBreakerTests=187
-org.elasticsearch.common.bytes.BytesReferenceTests=42
-org.elasticsearch.common.bytes.PagedBytesReferenceTests=890
-org.elasticsearch.common.cli.CheckFileCommandTests=462
-org.elasticsearch.common.cli.CliToolTests=195
-org.elasticsearch.common.cli.TerminalTests=111
-org.elasticsearch.common.collect.CopyOnWriteHashMapTests=138
-org.elasticsearch.common.compress.deflate.DeflateCompressedStreamTests=3050
-org.elasticsearch.common.compress.deflate.DeflateXContentTests=1022
-org.elasticsearch.common.compress.lzf.CorruptedCompressorTests=47
-org.elasticsearch.common.compress.lzf.LZFCompressedStreamTests=3845
-org.elasticsearch.common.compress.lzf.LZFXContentTests=738
-org.elasticsearch.common.geo.GeoDistanceTests=183
-org.elasticsearch.common.geo.GeoHashTests=603
-org.elasticsearch.common.geo.GeoJSONShapeParserTests=271
-org.elasticsearch.common.geo.ShapeBuilderTests=649
-org.elasticsearch.common.geo.ShapeRelationTests=63
-org.elasticsearch.common.geo.SpatialStrategyTests=141
-org.elasticsearch.common.hash.MessageDigestsTests=6973
-org.elasticsearch.common.hashing.MurmurHash3Tests=55
-org.elasticsearch.common.hppc.HppcMapsTests=46
-org.elasticsearch.common.io.FileSystemUtilsTests=2105
-org.elasticsearch.common.io.StreamsTests=134
-org.elasticsearch.common.io.stream.BytesStreamsTests=552
-org.elasticsearch.common.joda.DateMathParserTests=127
-org.elasticsearch.common.logging.jdk.JDKESLoggerTests=43
-org.elasticsearch.common.logging.log4j.Log4jESLoggerTests=135
-org.elasticsearch.common.logging.log4j.LoggingConfigurationTests=2414
-org.elasticsearch.common.lucene.IndexCacheableQueryTests=124
-org.elasticsearch.common.lucene.LuceneTests=1704
-org.elasticsearch.common.lucene.ShardCoreKeyMapTests=177
-org.elasticsearch.common.lucene.all.SimpleAllTests=2588
-org.elasticsearch.common.lucene.index.ESDirectoryReaderTests=323
-org.elasticsearch.common.lucene.index.FreqTermsEnumTests=682
-org.elasticsearch.common.lucene.search.MultiPhrasePrefixQueryTests=58
-org.elasticsearch.common.lucene.search.function.ScriptScoreFunctionTests=54
-org.elasticsearch.common.lucene.search.morelikethis.MoreLikeThisQueryTests=69
-org.elasticsearch.common.lucene.search.morelikethis.XMoreLikeThisTests=262
-org.elasticsearch.common.lucene.store.ByteArrayIndexInputTests=112
-org.elasticsearch.common.lucene.store.InputStreamIndexInputTests=89
-org.elasticsearch.common.lucene.uid.VersionsTests=500
-org.elasticsearch.common.math.MathUtilsTests=133
-org.elasticsearch.common.network.NetworkAddressTests=2234
-org.elasticsearch.common.network.NetworkServiceTests=103
-org.elasticsearch.common.network.NetworkUtilsTests=37
-org.elasticsearch.common.path.PathTrieTests=67
-org.elasticsearch.common.property.PropertyPlaceholderTests=22
-org.elasticsearch.common.recycler.ConcurrentRecyclerTests=57
-org.elasticsearch.common.recycler.LockedRecyclerTests=105
-org.elasticsearch.common.recycler.NoneRecyclerTests=15
-org.elasticsearch.common.recycler.QueueRecyclerTests=76
-org.elasticsearch.common.regex.RegexTests=103
-org.elasticsearch.common.rounding.RoundingTests=59
-org.elasticsearch.common.rounding.TimeZoneRoundingTests=180
-org.elasticsearch.common.settings.SettingsFilterTests=9
-org.elasticsearch.common.settings.SettingsTests=137
-org.elasticsearch.common.settings.loader.JsonSettingsLoaderTests=29
-org.elasticsearch.common.settings.loader.PropertiesSettingsLoaderTests=64
-org.elasticsearch.common.settings.loader.YamlSettingsLoaderTests=214
-org.elasticsearch.common.transport.BoundTransportAddressTests=139
-org.elasticsearch.common.unit.ByteSizeUnitTests=77
-org.elasticsearch.common.unit.ByteSizeValueTests=106
-org.elasticsearch.common.unit.DistanceUnitTests=58
-org.elasticsearch.common.unit.FuzzinessTests=55
-org.elasticsearch.common.unit.RatioValueTests=49
-org.elasticsearch.common.unit.SizeValueTests=88
-org.elasticsearch.common.unit.TimeValueTests=120
-org.elasticsearch.common.util.ArrayUtilsTests=43
-org.elasticsearch.common.util.BigArraysTests=4095
-org.elasticsearch.common.util.ByteUtilsTests=103
-org.elasticsearch.common.util.BytesRefHashTests=1372
-org.elasticsearch.common.util.CancellableThreadsTests=37
-org.elasticsearch.common.util.CollectionUtilsTests=219
-org.elasticsearch.common.util.LongHashTests=501
-org.elasticsearch.common.util.LongObjectHashMapTests=820
-org.elasticsearch.common.util.MultiDataPathUpgraderTests=984
-org.elasticsearch.common.util.SingleObjectCacheTests=107
-org.elasticsearch.common.util.URIPatternTests=128
-org.elasticsearch.common.util.concurrent.CountDownTests=266
-org.elasticsearch.common.util.concurrent.EsExecutorsTests=351
-org.elasticsearch.common.util.concurrent.PrioritizedExecutorsTests=436
-org.elasticsearch.common.util.concurrent.RefCountedTests=111
-org.elasticsearch.common.util.iterable.IterablesTests=16
-org.elasticsearch.common.xcontent.ObjectParserTests=110
-org.elasticsearch.common.xcontent.XContentFactoryTests=16
-org.elasticsearch.common.xcontent.builder.BuilderRawFieldTests=93
-org.elasticsearch.common.xcontent.builder.XContentBuilderTests=90
-org.elasticsearch.common.xcontent.cbor.CborXContentParserTests=65
-org.elasticsearch.common.xcontent.cbor.JsonVsCborTests=33
-org.elasticsearch.common.xcontent.smile.JsonVsSmileTests=38
-org.elasticsearch.common.xcontent.support.XContentHelperTests=44
-org.elasticsearch.common.xcontent.support.XContentMapValuesTests=102
-org.elasticsearch.common.xcontent.support.filtering.CborFilteringGeneratorTests=100
-org.elasticsearch.common.xcontent.support.filtering.JsonFilteringGeneratorTests=156
-org.elasticsearch.common.xcontent.support.filtering.SmileFilteringGeneratorTests=177
-org.elasticsearch.common.xcontent.support.filtering.YamlFilteringGeneratorTests=121
-org.elasticsearch.deps.jackson.JacksonLocationTests=20
-org.elasticsearch.deps.joda.SimpleJodaTests=223
-org.elasticsearch.deps.lucene.SimpleLuceneTests=432
-org.elasticsearch.deps.lucene.VectorHighlighterTests=354
-org.elasticsearch.discovery.BlockingClusterStatePublishResponseHandlerTests=21
-org.elasticsearch.discovery.DiscoveryModuleTests=27
-org.elasticsearch.discovery.ZenFaultDetectionTests=270
-org.elasticsearch.discovery.zen.ElectMasterServiceTests=50
-org.elasticsearch.discovery.zen.NodeJoinControllerTests=251
-org.elasticsearch.discovery.zen.ZenDiscoveryUnitTests=65
-org.elasticsearch.discovery.zen.ZenPingTests=16
-org.elasticsearch.discovery.zen.publish.PendingClusterStatesQueueTests=179
-org.elasticsearch.discovery.zen.publish.PublishClusterStateActionTests=887
-org.elasticsearch.env.EnvironmentTests=85
-org.elasticsearch.env.NodeEnvironmentTests=678
-org.elasticsearch.fieldstats.FieldStatsTests=1846
-org.elasticsearch.gateway.AsyncShardFetchTests=400
-org.elasticsearch.gateway.DanglingIndicesStateTests=43
-org.elasticsearch.gateway.GatewayMetaStateTests=202
-org.elasticsearch.gateway.GatewayModuleTests=14
-org.elasticsearch.gateway.GatewayServiceTests=62
-org.elasticsearch.gateway.GatewayTests=66
-org.elasticsearch.gateway.MetaDataStateFormatTests=232
-org.elasticsearch.gateway.MetaStateServiceTests=255
-org.elasticsearch.gateway.PrimaryShardAllocatorTests=85
-org.elasticsearch.gateway.PriorityComparatorTests=49
-org.elasticsearch.gateway.ReplicaShardAllocatorTests=64
-org.elasticsearch.http.netty.NettyHttpChannelTests=100
-org.elasticsearch.http.netty.NettyHttpServerPipeliningTests=3173
-org.elasticsearch.http.netty.pipelining.HttpPipeliningHandlerTests=335
-org.elasticsearch.index.IndexModuleTests=66
-org.elasticsearch.index.IndexServiceTests=15
-org.elasticsearch.index.VersionTypeTests=26
-org.elasticsearch.index.aliases.IndexAliasesServiceTests=318
-org.elasticsearch.index.analysis.ASCIIFoldingTokenFilterFactoryTests=765
-org.elasticsearch.index.analysis.AnalysisFactoryTests=150
-org.elasticsearch.index.analysis.AnalysisModuleTests=371
-org.elasticsearch.index.analysis.AnalysisTests=34
-org.elasticsearch.index.analysis.AnalyzerBackwardsCompatTests=1446
-org.elasticsearch.index.analysis.CJKFilterFactoryTests=39
-org.elasticsearch.index.analysis.CharFilterTests=94
-org.elasticsearch.index.analysis.CompoundAnalysisTests=171
-org.elasticsearch.index.analysis.HunspellTokenFilterFactoryTests=2896
-org.elasticsearch.index.analysis.KeepFilterFactoryTests=53
-org.elasticsearch.index.analysis.KeepTypesFilterFactoryTests=82
-org.elasticsearch.index.analysis.LimitTokenCountFilterFactoryTests=54
-org.elasticsearch.index.analysis.NGramTokenizerFactoryTests=63
-org.elasticsearch.index.analysis.NumericAnalyzerTests=13
-org.elasticsearch.index.analysis.PatternAnalyzerTests=1636
-org.elasticsearch.index.analysis.PatternCaptureTokenFilterTests=147
-org.elasticsearch.index.analysis.PreBuiltAnalyzerProviderFactoryTests=48
-org.elasticsearch.index.analysis.PreBuiltAnalyzerTests=293
-org.elasticsearch.index.analysis.PreBuiltCharFilterFactoryFactoryTests=86
-org.elasticsearch.index.analysis.PreBuiltTokenFilterFactoryFactoryTests=128
-org.elasticsearch.index.analysis.PreBuiltTokenizerFactoryFactoryTests=26
-org.elasticsearch.index.analysis.ShingleTokenFilterFactoryTests=136
-org.elasticsearch.index.analysis.SnowballAnalyzerTests=52
-org.elasticsearch.index.analysis.StemmerTokenFilterFactoryTests=2163
-org.elasticsearch.index.analysis.StopAnalyzerTests=3016
-org.elasticsearch.index.analysis.StopTokenFilterTests=375
-org.elasticsearch.index.analysis.WordDelimiterTokenFilterFactoryTests=336
-org.elasticsearch.index.analysis.commongrams.CommonGramsTokenFilterFactoryTests=359
-org.elasticsearch.index.analysis.synonyms.SynonymsAnalysisTests=166
-org.elasticsearch.index.cache.IndexCacheModuleTests=33
-org.elasticsearch.index.cache.bitset.BitSetFilterCacheTests=657
-org.elasticsearch.index.codec.CodecTests=479
-org.elasticsearch.index.codec.postingformat.PostingsFormatTests=6900
-org.elasticsearch.index.engine.CommitStatsTests=76
-org.elasticsearch.index.engine.InternalEngineSettingsTests=323
-org.elasticsearch.index.engine.InternalEngineTests=13034
-org.elasticsearch.index.engine.ShadowEngineTests=3902
-org.elasticsearch.index.fielddata.BinaryDVFieldDataTests=503
-org.elasticsearch.index.fielddata.DisabledFieldDataFormatTests=1035
-org.elasticsearch.index.fielddata.DoubleFieldDataTests=943
-org.elasticsearch.index.fielddata.DuelFieldDataTests=6036
-org.elasticsearch.index.fielddata.FieldDataTests=152
-org.elasticsearch.index.fielddata.FilterFieldDataTests=650
-org.elasticsearch.index.fielddata.FloatFieldDataTests=1246
-org.elasticsearch.index.fielddata.IndexFieldDataServiceTests=2723
-org.elasticsearch.index.fielddata.LongFieldDataTests=4912
-org.elasticsearch.index.fielddata.NoOrdinalsStringFieldDataTests=5655
-org.elasticsearch.index.fielddata.PagedBytesStringFieldDataTests=5923
-org.elasticsearch.index.fielddata.ParentChildFieldDataTests=1012
-org.elasticsearch.index.fielddata.ScriptDocValuesTests=224
-org.elasticsearch.index.fielddata.SortedSetDVStringFieldDataTests=5307
-org.elasticsearch.index.fielddata.fieldcomparator.ReplaceMissingTests=127
-org.elasticsearch.index.fielddata.ordinals.MultiOrdinalsTests=132
-org.elasticsearch.index.fielddata.ordinals.SingleOrdinalsTests=436
-org.elasticsearch.index.indexing.IndexingSlowLogTests=49
-org.elasticsearch.index.mapper.DocumentParserTests=371
-org.elasticsearch.index.mapper.DynamicMappingTests=1335
-org.elasticsearch.index.mapper.FieldTypeLookupTests=29
-org.elasticsearch.index.mapper.MapperServiceTests=230
-org.elasticsearch.index.mapper.UidTests=57
-org.elasticsearch.index.mapper.all.SimpleAllMapperTests=1376
-org.elasticsearch.index.mapper.binary.BinaryMappingTests=3554
-org.elasticsearch.index.mapper.boost.CustomBoostMappingTests=243
-org.elasticsearch.index.mapper.boost.FieldLevelBoostTests=2704
-org.elasticsearch.index.mapper.camelcase.CamelCaseFieldNameTests=358
-org.elasticsearch.index.mapper.completion.CompletionFieldMapperTests=429
-org.elasticsearch.index.mapper.compound.CompoundTypesTests=332
-org.elasticsearch.index.mapper.copyto.CopyToMapperTests=940
-org.elasticsearch.index.mapper.core.BinaryFieldTypeTests=95
-org.elasticsearch.index.mapper.core.BooleanFieldMapperTests=414
-org.elasticsearch.index.mapper.core.BooleanFieldTypeTests=133
-org.elasticsearch.index.mapper.core.ByteFieldTypeTests=86
-org.elasticsearch.index.mapper.core.CompletionFieldTypeTests=113
-org.elasticsearch.index.mapper.core.DateFieldTypeTests=73
-org.elasticsearch.index.mapper.core.DoubleFieldTypeTests=54
-org.elasticsearch.index.mapper.core.FloatFieldTypeTests=82
-org.elasticsearch.index.mapper.core.IntegerFieldTypeTests=56
-org.elasticsearch.index.mapper.core.LongFieldTypeTests=66
-org.elasticsearch.index.mapper.core.ShortFieldTypeTests=66
-org.elasticsearch.index.mapper.core.StringFieldTypeTests=79
-org.elasticsearch.index.mapper.core.TokenCountFieldMapperTests=300
-org.elasticsearch.index.mapper.date.DateBackwardsCompatibilityTests=1544
-org.elasticsearch.index.mapper.date.SimpleDateMappingTests=378
-org.elasticsearch.index.mapper.dynamictemplate.genericstore.GenericStoreDynamicTemplateTests=3256
-org.elasticsearch.index.mapper.dynamictemplate.pathmatch.PathMatchDynamicTemplateTests=303
-org.elasticsearch.index.mapper.dynamictemplate.simple.SimpleDynamicTemplatesTests=259
-org.elasticsearch.index.mapper.externalvalues.SimpleExternalMappingTests=539
-org.elasticsearch.index.mapper.geo.GeoEncodingTests=113
-org.elasticsearch.index.mapper.geo.GeoPointFieldMapperTests=2345
-org.elasticsearch.index.mapper.geo.GeoPointFieldTypeTests=113
-org.elasticsearch.index.mapper.geo.GeoShapeFieldMapperTests=768
-org.elasticsearch.index.mapper.geo.GeoShapeFieldTypeTests=92
-org.elasticsearch.index.mapper.geo.GeohashMappingGeoPointTests=308
-org.elasticsearch.index.mapper.id.IdMappingTests=712
-org.elasticsearch.index.mapper.index.IndexTypeMapperTests=330
-org.elasticsearch.index.mapper.internal.AllFieldTypeTests=95
-org.elasticsearch.index.mapper.internal.FieldNamesFieldMapperTests=636
-org.elasticsearch.index.mapper.internal.FieldNamesFieldTypeTests=119
-org.elasticsearch.index.mapper.internal.IdFieldTypeTests=54
-org.elasticsearch.index.mapper.internal.IndexFieldTypeTests=49
-org.elasticsearch.index.mapper.internal.ParentFieldMapperTests=83
-org.elasticsearch.index.mapper.internal.ParentFieldTypeTests=75
-org.elasticsearch.index.mapper.internal.RoutingFieldTypeTests=72
-org.elasticsearch.index.mapper.internal.SourceFieldTypeTests=129
-org.elasticsearch.index.mapper.internal.TimestampFieldTypeTests=61
-org.elasticsearch.index.mapper.internal.TypeFieldTypeTests=53
-org.elasticsearch.index.mapper.internal.UidFieldTypeTests=30
-org.elasticsearch.index.mapper.internal.VersionFieldTypeTests=39
-org.elasticsearch.index.mapper.ip.SimpleIpMappingTests=592
-org.elasticsearch.index.mapper.lucene.DoubleIndexingDocTests=142
-org.elasticsearch.index.mapper.lucene.StoredNumericValuesTests=328
-org.elasticsearch.index.mapper.merge.TestMergeMapperTests=1501
-org.elasticsearch.index.mapper.multifield.MultiFieldTests=633
-org.elasticsearch.index.mapper.multifield.merge.JavaMultiFieldMergeTests=218
-org.elasticsearch.index.mapper.nested.NestedMappingTests=749
-org.elasticsearch.index.mapper.null_value.NullValueTests=5152
-org.elasticsearch.index.mapper.numeric.SimpleNumericTests=884
-org.elasticsearch.index.mapper.object.NullValueObjectMappingTests=299
-org.elasticsearch.index.mapper.object.SimpleObjectMappingTests=728
-org.elasticsearch.index.mapper.parent.ParentMappingTests=294
-org.elasticsearch.index.mapper.path.PathMapperTests=515
-org.elasticsearch.index.mapper.routing.RoutingTypeMapperTests=497
-org.elasticsearch.index.mapper.simple.SimpleMapperTests=258
-org.elasticsearch.index.mapper.source.CompressSourceMappingTests=2902
-org.elasticsearch.index.mapper.source.DefaultSourceMappingTests=1323
-org.elasticsearch.index.mapper.string.SimpleStringMappingTests=177
-org.elasticsearch.index.mapper.string.StringFieldMapperPositionIncrementGapTests=702
-org.elasticsearch.index.mapper.timestamp.TimestampMappingTests=2991
-org.elasticsearch.index.mapper.ttl.TTLMappingTests=1822
-org.elasticsearch.index.mapper.typelevels.ParseDocumentTypeLevelsTests=440
-org.elasticsearch.index.mapper.typelevels.ParseMappingTypeLevelTests=187
-org.elasticsearch.index.mapper.update.UpdateMappingTests=843
-org.elasticsearch.index.query.BoolQueryBuilderTests=154
-org.elasticsearch.index.query.BoostingQueryBuilderTests=183
-org.elasticsearch.index.query.CombineFunctionTests=53
-org.elasticsearch.index.query.CommonTermsQueryBuilderTests=95
-org.elasticsearch.index.query.CommonTermsQueryParserTests=709
-org.elasticsearch.index.query.ConstantScoreQueryBuilderTests=285
-org.elasticsearch.index.query.DisMaxQueryBuilderTests=330
-org.elasticsearch.index.query.ExistsQueryBuilderTests=139
-org.elasticsearch.index.query.FieldMaskingSpanQueryBuilderTests=152
-org.elasticsearch.index.query.FuzzyQueryBuilderTests=210
-org.elasticsearch.index.query.GeoBoundingBoxQueryBuilderTests=315
-org.elasticsearch.index.query.GeoDistanceQueryBuilderTests=192
-org.elasticsearch.index.query.GeoDistanceRangeQueryTests=156
-org.elasticsearch.index.query.GeoPolygonQueryBuilderTests=445
-org.elasticsearch.index.query.GeoShapeQueryBuilderTests=246
-org.elasticsearch.index.query.GeohashCellQueryBuilderTests=85
-org.elasticsearch.index.query.HasChildQueryBuilderTests=255
-org.elasticsearch.index.query.HasChildQueryParserTests=82
-org.elasticsearch.index.query.HasParentQueryBuilderTests=336
-org.elasticsearch.index.query.IdsQueryBuilderTests=197
-org.elasticsearch.index.query.IndicesQueryBuilderTests=279
-org.elasticsearch.index.query.MatchAllQueryBuilderTests=188
-org.elasticsearch.index.query.MatchNoneQueryBuilderTests=257
-org.elasticsearch.index.query.MatchQueryBuilderTests=2712
-org.elasticsearch.index.query.MissingQueryBuilderTests=180
-org.elasticsearch.index.query.MoreLikeThisQueryBuilderTests=3351
-org.elasticsearch.index.query.MultiMatchQueryBuilderTests=59
-org.elasticsearch.index.query.NestedQueryBuilderTests=193
-org.elasticsearch.index.query.NotQueryBuilderTests=3071
-org.elasticsearch.index.query.OperatorTests=90
-org.elasticsearch.index.query.PrefixQueryBuilderTests=149
-org.elasticsearch.index.query.QueryFilterBuilderTests=100
-org.elasticsearch.index.query.QueryStringQueryBuilderTests=490
-org.elasticsearch.index.query.RangeQueryBuilderTests=577
-org.elasticsearch.index.query.RegexpQueryBuilderTests=235
-org.elasticsearch.index.query.ScoreModeTests=52
-org.elasticsearch.index.query.ScriptQueryBuilderTests=108
-org.elasticsearch.index.query.SimpleQueryStringBuilderTests=158
-org.elasticsearch.index.query.SpanContainingQueryBuilderTests=213
-org.elasticsearch.index.query.SpanFirstQueryBuilderTests=105
-org.elasticsearch.index.query.SpanMultiTermQueryBuilderTests=1847
-org.elasticsearch.index.query.SpanNearQueryBuilderTests=91
-org.elasticsearch.index.query.SpanNotQueryBuilderTests=589
-org.elasticsearch.index.query.SpanOrQueryBuilderTests=2712
-org.elasticsearch.index.query.SpanTermQueryBuilderTests=85
-org.elasticsearch.index.query.SpanWithinQueryBuilderTests=61
-org.elasticsearch.index.query.TemplateQueryBuilderTests=417
-org.elasticsearch.index.query.TemplateQueryParserTests=288
-org.elasticsearch.index.query.TermQueryBuilderTests=49
-org.elasticsearch.index.query.TermsQueryBuilderTests=69
-org.elasticsearch.index.query.TypeQueryBuilderTests=258
-org.elasticsearch.index.query.WildcardQueryBuilderTests=38
-org.elasticsearch.index.query.WrapperQueryBuilderTests=138
-org.elasticsearch.index.query.functionscore.FieldValueFactorFunctionModifierTests=69
-org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilderTests=227
-org.elasticsearch.index.query.functionscore.ScoreFunctionBuilderTests=53
-org.elasticsearch.index.query.support.QueryInnerHitsTests=53
-org.elasticsearch.index.search.MultiMatchQueryTests=135
-org.elasticsearch.index.search.geo.GeoPointParsingTests=90
-org.elasticsearch.index.search.geo.GeoUtilsTests=142
-org.elasticsearch.index.search.nested.DoubleNestedSortingTests=269
-org.elasticsearch.index.search.nested.FloatNestedSortingTests=575
-org.elasticsearch.index.search.nested.LongNestedSortingTests=305
-org.elasticsearch.index.search.nested.NestedSortingTests=264
-org.elasticsearch.index.shard.CommitPointsTests=58
-org.elasticsearch.index.shard.IndexShardTests=2664
-org.elasticsearch.index.shard.MergePolicySettingsTests=85
-org.elasticsearch.index.shard.NewPathForShardTests=87
-org.elasticsearch.index.shard.ShardPathTests=123
-org.elasticsearch.index.shard.ShardUtilsTests=69
-org.elasticsearch.index.shard.VersionFieldUpgraderTests=27
-org.elasticsearch.index.similarity.SimilarityTests=1282
-org.elasticsearch.index.snapshots.blobstore.FileInfoTests=350
-org.elasticsearch.index.snapshots.blobstore.SlicedInputStreamTests=28
-org.elasticsearch.index.store.DirectoryUtilsTests=179
-org.elasticsearch.index.store.IndexStoreBWCTests=397
-org.elasticsearch.index.store.IndexStoreTests=13
-org.elasticsearch.index.store.LegacyVerificationTests=44
-org.elasticsearch.index.store.StoreTests=433
-org.elasticsearch.index.translog.BufferedTranslogTests=4946
-org.elasticsearch.index.translog.TranslogTests=4070
-org.elasticsearch.index.translog.TranslogVersionTests=42
-org.elasticsearch.indices.IndicesLifecycleListenerSingleNodeTests=611
-org.elasticsearch.indices.IndicesModuleTests=1493
-org.elasticsearch.indices.IndicesServiceTests=5140
-org.elasticsearch.indices.cache.query.terms.TermsLookupTests=22
-org.elasticsearch.indices.flush.SyncedFlushSingleNodeTests=1243
-org.elasticsearch.indices.flush.SyncedFlushUnitTests=64
-org.elasticsearch.indices.memory.IndexingMemoryControllerTests=65
-org.elasticsearch.indices.memory.breaker.CircuitBreakerUnitTests=101
-org.elasticsearch.indices.recovery.RecoverySourceHandlerTests=691
-org.elasticsearch.indices.recovery.RecoveryStateTests=153
-org.elasticsearch.indices.recovery.RecoveryStatusTests=62
-org.elasticsearch.indices.recovery.StartRecoveryRequestTests=84
-org.elasticsearch.indices.store.IndicesStoreTests=83
-org.elasticsearch.monitor.fs.FsProbeTests=41
-org.elasticsearch.monitor.jvm.JvmStatsTests=43
-org.elasticsearch.monitor.os.OsProbeTests=45
-org.elasticsearch.monitor.process.ProcessProbeTests=42
-org.elasticsearch.node.internal.InternalSettingsPreparerTests=140
-org.elasticsearch.plugins.PluginInfoTests=372
-org.elasticsearch.plugins.PluginManagerCliTests=153
-org.elasticsearch.plugins.PluginManagerUnitTests=51
-org.elasticsearch.plugins.PluginsServiceTests=68
-org.elasticsearch.recovery.RecoveriesCollectionTests=430
-org.elasticsearch.recovery.RecoverySettingsTests=569
-org.elasticsearch.rest.BytesRestResponseTests=83
-org.elasticsearch.rest.HeadersAndContextCopyClientTests=194
-org.elasticsearch.rest.RestFilterChainTests=77
-org.elasticsearch.rest.RestRequestTests=39
-org.elasticsearch.rest.action.support.RestTableTests=88
-org.elasticsearch.rest.util.RestUtilsTests=85
-org.elasticsearch.script.FileScriptTests=39
-org.elasticsearch.script.NativeScriptTests=111
-org.elasticsearch.script.ScriptContextRegistryTests=27
-org.elasticsearch.script.ScriptContextTests=85
-org.elasticsearch.script.ScriptModesTests=115
-org.elasticsearch.script.ScriptParameterParserTests=173
-org.elasticsearch.script.ScriptServiceTests=421
-org.elasticsearch.script.mustache.MustacheScriptEngineTests=115
-org.elasticsearch.script.mustache.MustacheTests=65
-org.elasticsearch.search.MultiValueModeTests=149
-org.elasticsearch.search.SearchModuleTests=89
-org.elasticsearch.search.SearchServiceTests=1170
-org.elasticsearch.search.aggregations.AggregationCollectorTests=644
-org.elasticsearch.search.aggregations.bucket.nested.NestedAggregatorTests=419
-org.elasticsearch.search.aggregations.bucket.significant.SignificanceHeuristicTests=120
-org.elasticsearch.search.aggregations.metrics.cardinality.HyperLogLogPlusPlusTests=695
-org.elasticsearch.search.aggregations.pipeline.PipelineAggregationHelperTests=27
-org.elasticsearch.search.aggregations.pipeline.moving.avg.MovAvgUnitTests=122
-org.elasticsearch.search.aggregations.support.MissingValuesTests=52
-org.elasticsearch.search.aggregations.support.PathTests=90
-org.elasticsearch.search.aggregations.support.ScriptValuesTests=67
-org.elasticsearch.search.builder.SearchSourceBuilderTests=49
-org.elasticsearch.search.compress.SearchSourceCompressTests=3136
-org.elasticsearch.search.fetch.innerhits.NestedChildrenFilterTests=128
-org.elasticsearch.search.internal.InternalSearchHitTests=46
-org.elasticsearch.search.query.QueryPhaseTests=185
-org.elasticsearch.search.sort.SortParserTests=319
-org.elasticsearch.search.stats.SearchStatsUnitTests=50
-org.elasticsearch.search.suggest.CompletionTokenStreamTests=160
-org.elasticsearch.search.suggest.completion.CompletionPostingsFormatTests=1319
-org.elasticsearch.search.suggest.context.GeoLocationContextMappingTests=109
-org.elasticsearch.search.suggest.phrase.NoisyChannelSpellCheckerTests=1409
-org.elasticsearch.snapshots.SnapshotRequestsTests=30
-org.elasticsearch.snapshots.SnapshotUtilsTests=30
-org.elasticsearch.test.rest.test.AssertionParsersTests=30
-org.elasticsearch.test.rest.test.DoSectionParserTests=198
-org.elasticsearch.test.rest.test.FileUtilsTests=60
-org.elasticsearch.test.rest.test.JsonPathTests=83
-org.elasticsearch.test.rest.test.RestApiParserFailingTests=102
-org.elasticsearch.test.rest.test.RestApiParserTests=73
-org.elasticsearch.test.rest.test.RestTestParserTests=145
-org.elasticsearch.test.rest.test.SetSectionParserTests=116
-org.elasticsearch.test.rest.test.SetupSectionParserTests=248
-org.elasticsearch.test.rest.test.SkipSectionParserTests=95
-org.elasticsearch.test.rest.test.TestSectionParserTests=134
-org.elasticsearch.test.test.InternalTestClusterTests=40
-org.elasticsearch.test.test.LoggingListenerTests=72
-org.elasticsearch.test.test.VersionUtilsTests=28
-org.elasticsearch.threadpool.ThreadPoolSerializationTests=56
-org.elasticsearch.threadpool.ThreadPoolStatsTests=99
-org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests=24
-org.elasticsearch.transport.NettySizeHeaderFrameDecoderTests=183
-org.elasticsearch.transport.TransportMessageTests=51
-org.elasticsearch.transport.local.SimpleLocalTransportTests=1174
-org.elasticsearch.transport.netty.KeyedLockTests=414
-org.elasticsearch.transport.netty.NettyScheduledPingTests=1662
-org.elasticsearch.transport.netty.NettyTransportMultiPortTests=382
-org.elasticsearch.transport.netty.NettyTransportTests=137
-org.elasticsearch.transport.netty.SimpleNettyTransportTests=5528
-org.elasticsearch.tribe.TribeUnitTests=2098
-org.elasticsearch.watcher.FileWatcherTests=203
-org.elasticsearch.watcher.ResourceWatcherServiceTests=101
diff --git a/core/pom.xml b/core/pom.xml
index 97c9bee..2424c8e 100644
--- a/core/pom.xml
+++ b/core/pom.xml
@@ -314,45 +314,6 @@
                     </execution>
                 </executions>
             </plugin>
-            <plugin>
-                <groupId>de.thetaphi</groupId>
-                <artifactId>forbiddenapis</artifactId>
-                <version>1.8</version>
-
-                <executions>
-                    <execution>
-                        <id>check-forbidden-apis-in-cluster</id>
-                            <configuration>
-                                 <targetVersion>${maven.compiler.target}</targetVersion>
-                                 <!-- disallow undocumented classes like sun.misc.Unsafe: -->
-                                 <internalRuntimeForbidden>true</internalRuntimeForbidden>
-                                 <!-- if the used Java version is too new, don't fail, just do nothing: -->
-                                 <failOnUnsupportedJava>false</failOnUnsupportedJava>
-                                 <bundledSignatures>
-                                     <!-- This will automatically choose the right signatures based on 'targetVersion': -->
-                                     <bundledSignature>jdk-unsafe</bundledSignature>
-                                     <bundledSignature>jdk-deprecated</bundledSignature>
-                                     <bundledSignature>jdk-system-out</bundledSignature>
-                                 </bundledSignatures>
-                                 <signaturesFiles>
-                                      <signaturesFile>${elasticsearch.tools.directory}/forbidden/core-signatures.txt</signaturesFile>
-                                      <signaturesFile>${elasticsearch.tools.directory}/forbidden/all-signatures.txt</signaturesFile>
-                                      <signaturesFile>${elasticsearch.tools.directory}/forbidden/third-party-signatures.txt</signaturesFile>
-                                      <signaturesFile>${elasticsearch.tools.directory}/forbidden/cluster-signatures.txt</signaturesFile>
-                                 </signaturesFiles>
-                                 <signatures>${forbidden.signatures}</signatures>
-                                 <includes>
-                                     <include>org/elasticsearch/cluster/**/*.class</include>
-                                 </includes>
-                                 <suppressAnnotations><annotation>**.SuppressForbidden</annotation></suppressAnnotations>
-                             </configuration>
-                         <phase>compile</phase>
-                         <goals>
-                             <goal>check</goal>
-                         </goals>
-                     </execution>
-                 </executions>
-             </plugin>
         </plugins>
       <pluginManagement>
         <plugins>
@@ -372,12 +333,17 @@
                     <excludes>
                         <!-- Guice -->
                         <exclude>src/main/java/org/elasticsearch/common/inject/**</exclude>
-                        <exclude>src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java</exclude>
+                        <!-- Forks of Lucene classes -->
                         <exclude>src/main/java/org/apache/lucene/**/X*.java</exclude>
-                        <!-- t-digest -->
-                        <exclude>src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestState.java</exclude>
                         <!-- netty pipelining -->
                         <exclude>src/main/java/org/elasticsearch/http/netty/pipelining/**</exclude>
+                        <!-- Guava -->
+                        <exclude>src/main/java/org/elasticsearch/common/network/InetAddresses.java</exclude>
+                        <exclude>src/test/java/org/elasticsearch/common/network/InetAddressesTests.java</exclude>
+                        <exclude>src/test/java/org/elasticsearch/common/collect/EvictingQueueTests.java</exclude>
+                        <!-- Joda -->
+                        <exclude>src/main/java/org/joda/time/base/BaseDateTime.java</exclude>
+                        <exclude>src/main/java/org/joda/time/format/StrictISODateTimeFormat.java</exclude>
                     </excludes>
                 </configuration>
             </plugin>
@@ -396,22 +362,5 @@
             </activation>
             <!-- not including license-maven-plugin is sufficent to expose default license -->
         </profile>
-        <profile>
-            <id>dev</id>
-            <build>
-                <plugins>
-                    <plugin>
-                        <groupId>de.thetaphi</groupId>
-                        <artifactId>forbiddenapis</artifactId>
-                        <executions>
-                            <execution>
-                                <id>check-forbidden-apis-in-cluster</id>
-                                <phase>none</phase>
-                            </execution>
-                        </executions>
-                    </plugin>
-                </plugins>
-            </build>
-        </profile>
     </profiles>
 </project>
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java b/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
index 3ef6e5a..ca1524f 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
@@ -19,19 +19,14 @@
 
 package org.apache.lucene.queryparser.classic;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.DisjunctionMaxQuery;
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.MatchNoDocsQuery;
-import org.apache.lucene.search.MultiPhraseQuery;
-import org.apache.lucene.search.PhraseQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.search.*;
 import org.apache.lucene.util.automaton.RegExp;
+import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.index.mapper.MappedFieldType;
@@ -43,12 +38,9 @@ import org.elasticsearch.index.query.support.QueryParsers;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
 import java.util.Objects;
 
-import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfNeeded;
 
 /**
@@ -60,13 +52,13 @@ import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfN
  */
 public class MapperQueryParser extends QueryParser {
 
-    public static final Map<String, FieldQueryExtension> FIELD_QUERY_EXTENSIONS;
+    public static final ImmutableMap<String, FieldQueryExtension> fieldQueryExtensions;
 
     static {
-        Map<String, FieldQueryExtension> fieldQueryExtensions = new HashMap<>();
-        fieldQueryExtensions.put(ExistsFieldQueryExtension.NAME, new ExistsFieldQueryExtension());
-        fieldQueryExtensions.put(MissingFieldQueryExtension.NAME, new MissingFieldQueryExtension());
-        FIELD_QUERY_EXTENSIONS = unmodifiableMap(fieldQueryExtensions);
+        fieldQueryExtensions = ImmutableMap.<String, FieldQueryExtension>builder()
+                .put(ExistsFieldQueryExtension.NAME, new ExistsFieldQueryExtension())
+                .put(MissingFieldQueryExtension.NAME, new MissingFieldQueryExtension())
+                .build();
     }
 
     private final QueryShardContext context;
@@ -132,7 +124,7 @@ public class MapperQueryParser extends QueryParser {
 
     @Override
     public Query getFieldQuery(String field, String queryText, boolean quoted) throws ParseException {
-        FieldQueryExtension fieldQueryExtension = FIELD_QUERY_EXTENSIONS.get(field);
+        FieldQueryExtension fieldQueryExtension = fieldQueryExtensions.get(field);
         if (fieldQueryExtension != null) {
             return fieldQueryExtension.query(context, queryText);
         }
@@ -548,7 +540,7 @@ public class MapperQueryParser extends QueryParser {
                     return newMatchAllDocsQuery();
                 }
                 // effectively, we check if a field exists or not
-                return FIELD_QUERY_EXTENSIONS.get(ExistsFieldQueryExtension.NAME).query(context, actualField);
+                return fieldQueryExtensions.get(ExistsFieldQueryExtension.NAME).query(context, actualField);
             }
         }
         if (lowercaseExpandedTerms) {
diff --git a/core/src/main/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatter.java b/core/src/main/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatter.java
index 75ad81b..2f7d538 100644
--- a/core/src/main/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatter.java
+++ b/core/src/main/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatter.java
@@ -1,19 +1,20 @@
 /*
- * Licensed to Elasticsearch under one
- * or more contributor license agreements. See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership. Elasticsearch licenses this
- * file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
  *
- *     http://www.apache.org/licenses/LICENSE-2.0
+ *    http://www.apache.org/licenses/LICENSE-2.0
  *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
  */
 
 package org.apache.lucene.search.postingshighlight;
diff --git a/core/src/main/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighter.java b/core/src/main/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighter.java
index 67373ef..30f57b2 100644
--- a/core/src/main/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighter.java
+++ b/core/src/main/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighter.java
@@ -1,19 +1,20 @@
 /*
- * Licensed to Elasticsearch under one
- * or more contributor license agreements. See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership. Elasticsearch licenses this
- * file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
  *
- *     http://www.apache.org/licenses/LICENSE-2.0
+ *    http://www.apache.org/licenses/LICENSE-2.0
  *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
  */
 
 package org.apache.lucene.search.postingshighlight;
diff --git a/core/src/main/java/org/apache/lucene/search/postingshighlight/Snippet.java b/core/src/main/java/org/apache/lucene/search/postingshighlight/Snippet.java
index a756de6..f3bfa1b 100644
--- a/core/src/main/java/org/apache/lucene/search/postingshighlight/Snippet.java
+++ b/core/src/main/java/org/apache/lucene/search/postingshighlight/Snippet.java
@@ -1,19 +1,20 @@
 /*
- * Licensed to Elasticsearch under one
- * or more contributor license agreements. See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership. Elasticsearch licenses this
- * file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
  *
- *     http://www.apache.org/licenses/LICENSE-2.0
+ *    http://www.apache.org/licenses/LICENSE-2.0
  *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
  */
 
 package org.apache.lucene.search.postingshighlight;
diff --git a/core/src/main/java/org/elasticsearch/ElasticsearchException.java b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
index 87348f9..62eb374 100644
--- a/core/src/main/java/org/elasticsearch/ElasticsearchException.java
+++ b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
@@ -482,7 +482,7 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
         RESOURCE_NOT_FOUND_EXCEPTION(org.elasticsearch.ResourceNotFoundException.class, org.elasticsearch.ResourceNotFoundException::new, 19),
         ACTION_TRANSPORT_EXCEPTION(org.elasticsearch.transport.ActionTransportException.class, org.elasticsearch.transport.ActionTransportException::new, 20),
         ELASTICSEARCH_GENERATION_EXCEPTION(org.elasticsearch.ElasticsearchGenerationException.class, org.elasticsearch.ElasticsearchGenerationException::new, 21),
-        CREATE_FAILED_ENGINE_EXCEPTION(org.elasticsearch.index.engine.CreateFailedEngineException.class, org.elasticsearch.index.engine.CreateFailedEngineException::new, 22),
+        //      22 was CreateFailedEngineException
         INDEX_SHARD_STARTED_EXCEPTION(org.elasticsearch.index.shard.IndexShardStartedException.class, org.elasticsearch.index.shard.IndexShardStartedException::new, 23),
         SEARCH_CONTEXT_MISSING_EXCEPTION(org.elasticsearch.search.SearchContextMissingException.class, org.elasticsearch.search.SearchContextMissingException::new, 24),
         SCRIPT_EXCEPTION(org.elasticsearch.script.ScriptException.class, org.elasticsearch.script.ScriptException::new, 25),
@@ -514,7 +514,7 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
         INDEX_SHARD_ALREADY_EXISTS_EXCEPTION(org.elasticsearch.index.IndexShardAlreadyExistsException.class, org.elasticsearch.index.IndexShardAlreadyExistsException::new, 51),
         VERSION_CONFLICT_ENGINE_EXCEPTION(org.elasticsearch.index.engine.VersionConflictEngineException.class, org.elasticsearch.index.engine.VersionConflictEngineException::new, 52),
         ENGINE_EXCEPTION(org.elasticsearch.index.engine.EngineException.class, org.elasticsearch.index.engine.EngineException::new, 53),
-        DOCUMENT_ALREADY_EXISTS_EXCEPTION(org.elasticsearch.index.engine.DocumentAlreadyExistsException.class, org.elasticsearch.index.engine.DocumentAlreadyExistsException::new, 54),
+        // 54 was DocumentAlreadyExistsException, which is superseded by VersionConflictEngineException
         NO_SUCH_NODE_EXCEPTION(org.elasticsearch.action.NoSuchNodeException.class, org.elasticsearch.action.NoSuchNodeException::new, 55),
         SETTINGS_EXCEPTION(org.elasticsearch.common.settings.SettingsException.class, org.elasticsearch.common.settings.SettingsException::new, 56),
         INDEX_TEMPLATE_MISSING_EXCEPTION(org.elasticsearch.indices.IndexTemplateMissingException.class, org.elasticsearch.indices.IndexTemplateMissingException::new, 57),
@@ -524,7 +524,7 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
         ROUTING_VALIDATION_EXCEPTION(org.elasticsearch.cluster.routing.RoutingValidationException.class, org.elasticsearch.cluster.routing.RoutingValidationException::new, 61),
         NOT_SERIALIZABLE_EXCEPTION_WRAPPER(org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper.class, org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper::new, 62),
         ALIAS_FILTER_PARSING_EXCEPTION(org.elasticsearch.indices.AliasFilterParsingException.class, org.elasticsearch.indices.AliasFilterParsingException::new, 63),
-        DELETE_BY_QUERY_FAILED_ENGINE_EXCEPTION(org.elasticsearch.index.engine.DeleteByQueryFailedEngineException.class, org.elasticsearch.index.engine.DeleteByQueryFailedEngineException::new, 64),
+        // 64 was DeleteByQueryFailedEngineException, which was removed in 3.0
         GATEWAY_EXCEPTION(org.elasticsearch.gateway.GatewayException.class, org.elasticsearch.gateway.GatewayException::new, 65),
         INDEX_SHARD_NOT_RECOVERING_EXCEPTION(org.elasticsearch.index.shard.IndexShardNotRecoveringException.class, org.elasticsearch.index.shard.IndexShardNotRecoveringException::new, 66),
         HTTP_EXCEPTION(org.elasticsearch.http.HttpException.class, org.elasticsearch.http.HttpException::new, 67),
diff --git a/core/src/main/java/org/elasticsearch/Version.java b/core/src/main/java/org/elasticsearch/Version.java
index 3fa9533..a610d8d 100644
--- a/core/src/main/java/org/elasticsearch/Version.java
+++ b/core/src/main/java/org/elasticsearch/Version.java
@@ -259,6 +259,8 @@ public class Version {
     public static final Version V_2_0_0_beta1 = new Version(V_2_0_0_beta1_ID, false, org.apache.lucene.util.Version.LUCENE_5_2_1);
     public static final int V_2_0_0_beta2_ID = 2000002;
     public static final Version V_2_0_0_beta2 = new Version(V_2_0_0_beta2_ID, false, org.apache.lucene.util.Version.LUCENE_5_2_1);
+    public static final int V_2_0_0_rc1_ID = 2000051;
+    public static final Version V_2_0_0_rc1 = new Version(V_2_0_0_rc1_ID, false, org.apache.lucene.util.Version.LUCENE_5_2_1);
     public static final int V_2_0_0_ID = 2000099;
     public static final Version V_2_0_0 = new Version(V_2_0_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_2_1);
     public static final int V_2_1_0_ID = 2010099;
@@ -287,6 +289,8 @@ public class Version {
                 return V_2_1_0;
             case V_2_0_0_ID:
                 return V_2_0_0;
+            case V_2_0_0_rc1_ID:
+                return V_2_0_0_rc1;
             case V_2_0_0_beta2_ID:
                 return V_2_0_0_beta2;
             case V_2_0_0_beta1_ID:
diff --git a/core/src/main/java/org/elasticsearch/action/ActionModule.java b/core/src/main/java/org/elasticsearch/action/ActionModule.java
index cc41cc0..f8634b1 100644
--- a/core/src/main/java/org/elasticsearch/action/ActionModule.java
+++ b/core/src/main/java/org/elasticsearch/action/ActionModule.java
@@ -121,8 +121,8 @@ import org.elasticsearch.action.admin.indices.upgrade.post.UpgradeAction;
 import org.elasticsearch.action.admin.indices.upgrade.post.UpgradeSettingsAction;
 import org.elasticsearch.action.admin.indices.validate.query.TransportValidateQueryAction;
 import org.elasticsearch.action.admin.indices.validate.query.ValidateQueryAction;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateAction;
-import org.elasticsearch.action.admin.indices.validate.template.TransportRenderSearchTemplateAction;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateAction;
+import org.elasticsearch.action.admin.cluster.validate.template.TransportRenderSearchTemplateAction;
 import org.elasticsearch.action.admin.indices.warmer.delete.DeleteWarmerAction;
 import org.elasticsearch.action.admin.indices.warmer.delete.TransportDeleteWarmerAction;
 import org.elasticsearch.action.admin.indices.warmer.get.GetWarmersAction;
diff --git a/core/src/main/java/org/elasticsearch/action/UnavailableShardsException.java b/core/src/main/java/org/elasticsearch/action/UnavailableShardsException.java
index dd0968e..ff31bb7 100644
--- a/core/src/main/java/org/elasticsearch/action/UnavailableShardsException.java
+++ b/core/src/main/java/org/elasticsearch/action/UnavailableShardsException.java
@@ -21,6 +21,7 @@ package org.elasticsearch.action;
 
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.collect.HppcMaps;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.rest.RestStatus;
@@ -32,8 +33,8 @@ import java.io.IOException;
  */
 public class UnavailableShardsException extends ElasticsearchException {
 
-    public UnavailableShardsException(@Nullable ShardId shardId, String message) {
-        super(buildMessage(shardId, message));
+    public UnavailableShardsException(@Nullable ShardId shardId, String message, Object... args) {
+        super(buildMessage(shardId, message), args);
     }
 
     private static String buildMessage(ShardId shardId, String message) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java
index 2d68385..908a25a 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.cluster.node.info;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Build;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.support.nodes.BaseNodeResponse;
@@ -35,11 +36,8 @@ import org.elasticsearch.threadpool.ThreadPoolInfo;
 import org.elasticsearch.transport.TransportInfo;
 
 import java.io.IOException;
-import java.util.HashMap;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * Node information (static, does not change over time).
  */
@@ -77,7 +75,7 @@ public class NodeInfo extends BaseNodeResponse {
     NodeInfo() {
     }
 
-    public NodeInfo(Version version, Build build, DiscoveryNode node, @Nullable Map<String, String> serviceAttributes, @Nullable Settings settings,
+    public NodeInfo(Version version, Build build, DiscoveryNode node, @Nullable ImmutableMap<String, String> serviceAttributes, @Nullable Settings settings,
                     @Nullable OsInfo os, @Nullable ProcessInfo process, @Nullable JvmInfo jvm, @Nullable ThreadPoolInfo threadPool,
                     @Nullable TransportInfo transport, @Nullable HttpInfo http, @Nullable PluginsInfo plugins) {
         super(node);
@@ -188,12 +186,12 @@ public class NodeInfo extends BaseNodeResponse {
         version = Version.readVersion(in);
         build = Build.readBuild(in);
         if (in.readBoolean()) {
-            Map<String, String> builder = new HashMap<>();
+            ImmutableMap.Builder<String, String> builder = ImmutableMap.builder();
             int size = in.readVInt();
             for (int i = 0; i < size; i++) {
                 builder.put(in.readString(), in.readString());
             }
-            serviceAttributes = unmodifiableMap(builder);
+            serviceAttributes = builder.build();
         }
         if (in.readBoolean()) {
             settings = Settings.readSettingsFromStream(in);
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java
index 108bb31..65033f3 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.action.admin.cluster.node.info;
 
-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
 import org.elasticsearch.action.support.nodes.BaseNodesResponse;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -87,8 +85,8 @@ public class NodesInfoResponse extends BaseNodesResponse<NodeInfo> implements To
 
             if (!nodeInfo.getNode().attributes().isEmpty()) {
                 builder.startObject("attributes");
-                for (ObjectObjectCursor<String, String> attr : nodeInfo.getNode().attributes()) {
-                    builder.field(attr.key, attr.value, XContentBuilder.FieldCaseConversion.NONE);
+                for (Map.Entry<String, String> attr : nodeInfo.getNode().attributes().entrySet()) {
+                    builder.field(attr.getKey(), attr.getValue(), XContentBuilder.FieldCaseConversion.NONE);
                 }
                 builder.endObject();
             }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodeStats.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodeStats.java
index 4cd050c..c437a44 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodeStats.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodeStats.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.action.admin.cluster.node.stats;
 
-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
 import org.elasticsearch.action.support.nodes.BaseNodeResponse;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.Nullable;
@@ -40,6 +38,7 @@ import org.elasticsearch.threadpool.ThreadPoolStats;
 import org.elasticsearch.transport.TransportStats;
 
 import java.io.IOException;
+import java.util.Map;
 
 /**
  * Node statistics (dynamic, changes depending on when created).
@@ -282,8 +281,8 @@ public class NodeStats extends BaseNodeResponse implements ToXContent {
 
             if (!getNode().attributes().isEmpty()) {
                 builder.startObject("attributes");
-                for (ObjectObjectCursor<String, String> attr : getNode().attributes()) {
-                    builder.field(attr.key, attr.value, XContentBuilder.FieldCaseConversion.NONE);
+                for (Map.Entry<String, String> attr : getNode().attributes().entrySet()) {
+                    builder.field(attr.getKey(), attr.getValue(), XContentBuilder.FieldCaseConversion.NONE);
                 }
                 builder.endObject();
             }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexStatus.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexStatus.java
index 5999fc3..961914e 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexStatus.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexStatus.java
@@ -19,18 +19,16 @@
 
 package org.elasticsearch.action.admin.cluster.snapshots.status;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 
 import java.io.IOException;
 import java.util.Collection;
-import java.util.HashMap;
 import java.util.Iterator;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * Represents snapshot status of all shards in the index
  */
@@ -47,14 +45,14 @@ public class SnapshotIndexStatus implements Iterable<SnapshotIndexShardStatus>,
     SnapshotIndexStatus(String index, Collection<SnapshotIndexShardStatus> shards) {
         this.index = index;
 
-        Map<Integer, SnapshotIndexShardStatus> indexShards = new HashMap<>();
+        ImmutableMap.Builder<Integer, SnapshotIndexShardStatus> builder = ImmutableMap.builder();
         stats = new SnapshotStats();
         for (SnapshotIndexShardStatus shard : shards) {
-            indexShards.put(shard.getShardId(), shard);
+            builder.put(shard.getShardId(), shard);
             stats.add(shard.getStats());
         }
         shardsStats = new SnapshotShardsStats(shards);
-        this.indexShards = unmodifiableMap(indexShards);
+        indexShards = builder.build();
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java
index 860b414..91b890b 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.cluster.snapshots.status;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.SnapshotsInProgress.State;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -32,14 +33,11 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collections;
-import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * Status of a snapshot
  */
@@ -51,7 +49,7 @@ public class SnapshotStatus implements ToXContent, Streamable {
 
     private List<SnapshotIndexShardStatus> shards;
 
-    private Map<String, SnapshotIndexStatus> indicesStatus;
+    private ImmutableMap<String, SnapshotIndexStatus> indicesStatus;
 
     private SnapshotShardsStats shardsStats;
 
@@ -102,7 +100,7 @@ public class SnapshotStatus implements ToXContent, Streamable {
             return this.indicesStatus;
         }
 
-        Map<String, SnapshotIndexStatus> indicesStatus = new HashMap<>();
+        ImmutableMap.Builder<String, SnapshotIndexStatus> indicesStatus = ImmutableMap.builder();
 
         Set<String> indices = new HashSet<>();
         for (SnapshotIndexShardStatus shard : shards) {
@@ -118,7 +116,7 @@ public class SnapshotStatus implements ToXContent, Streamable {
             }
             indicesStatus.put(index, new SnapshotIndexStatus(index, shards));
         }
-        this.indicesStatus = unmodifiableMap(indicesStatus);
+        this.indicesStatus = indicesStatus.build();
         return this.indicesStatus;
 
     }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java
index 0f9bd67..5af92dc 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java
@@ -19,10 +19,7 @@
 
 package org.elasticsearch.action.admin.cluster.snapshots.status;
 
-import com.carrotsearch.hppc.cursors.ObjectCursor;
-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
 import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.master.TransportMasterNodeAction;
@@ -98,9 +95,9 @@ public class TransportSnapshotsStatusAction extends TransportMasterNodeAction<Sn
 
         Set<String> nodesIds = new HashSet<>();
         for (SnapshotsInProgress.Entry entry : currentSnapshots) {
-            for (ObjectCursor<SnapshotsInProgress.ShardSnapshotStatus> status : entry.shards().values()) {
-                if (status.value.nodeId() != null) {
-                    nodesIds.add(status.value.nodeId());
+            for (SnapshotsInProgress.ShardSnapshotStatus status : entry.shards().values()) {
+                if (status.nodeId() != null) {
+                    nodesIds.add(status.nodeId());
                 }
             }
         }
@@ -154,15 +151,15 @@ public class TransportSnapshotsStatusAction extends TransportMasterNodeAction<Sn
             for (SnapshotsInProgress.Entry entry : currentSnapshots) {
                 currentSnapshotIds.add(entry.snapshotId());
                 List<SnapshotIndexShardStatus> shardStatusBuilder = new ArrayList<>();
-                for (ObjectObjectCursor<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shardEntry : entry.shards()) {
-                    SnapshotsInProgress.ShardSnapshotStatus status = shardEntry.value;
+                for (ImmutableMap.Entry<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shardEntry : entry.shards().entrySet()) {
+                    SnapshotsInProgress.ShardSnapshotStatus status = shardEntry.getValue();
                     if (status.nodeId() != null) {
                         // We should have information about this shard from the shard:
                         TransportNodesSnapshotsStatus.NodeSnapshotStatus nodeStatus = nodeSnapshotStatusMap.get(status.nodeId());
                         if (nodeStatus != null) {
                             Map<ShardId, SnapshotIndexShardStatus> shardStatues = nodeStatus.status().get(entry.snapshotId());
                             if (shardStatues != null) {
-                                SnapshotIndexShardStatus shardStatus = shardStatues.get(shardEntry.key);
+                                SnapshotIndexShardStatus shardStatus = shardStatues.get(shardEntry.getKey());
                                 if (shardStatus != null) {
                                     // We have full information about this shard
                                     shardStatusBuilder.add(shardStatus);
@@ -172,7 +169,7 @@ public class TransportSnapshotsStatusAction extends TransportMasterNodeAction<Sn
                         }
                     }
                     final SnapshotIndexShardStage stage;
-                    switch (shardEntry.value.state()) {
+                    switch (shardEntry.getValue().state()) {
                         case FAILED:
                         case ABORTED:
                         case MISSING:
@@ -187,9 +184,9 @@ public class TransportSnapshotsStatusAction extends TransportMasterNodeAction<Sn
                             stage = SnapshotIndexShardStage.DONE;
                             break;
                         default:
-                            throw new IllegalArgumentException("Unknown snapshot state " + shardEntry.value.state());
+                            throw new IllegalArgumentException("Unknown snapshot state " + shardEntry.getValue().state());
                     }
-                    SnapshotIndexShardStatus shardStatus = new SnapshotIndexShardStatus(shardEntry.key, stage);
+                    SnapshotIndexShardStatus shardStatus = new SnapshotIndexShardStatus(shardEntry.getKey(), stage);
                     shardStatusBuilder.add(shardStatus);
                 }
                 builder.add(new SnapshotStatus(entry.snapshotId(), entry.state(), Collections.unmodifiableList(shardStatusBuilder)));
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java
index a3708f6..54ea5a9 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.action.admin.cluster.state;
 
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.master.TransportMasterNodeReadAction;
@@ -90,7 +89,7 @@ public class TransportClusterStateAction extends TransportMasterNodeReadAction<C
                         routingTableBuilder.add(currentState.routingTable().getIndicesRouting().get(filteredIndex));
                     }
                 }
-                builder.routingTable(routingTableBuilder.build());
+                builder.routingTable(routingTableBuilder);
             } else {
                 builder.routingTable(currentState.routingTable());
             }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateAction.java
new file mode 100644
index 0000000..427d0c4
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateAction.java
@@ -0,0 +1,44 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.admin.cluster.validate.template;
+
+import org.elasticsearch.action.Action;
+import org.elasticsearch.client.ElasticsearchClient;
+
+public class RenderSearchTemplateAction extends Action<RenderSearchTemplateRequest, RenderSearchTemplateResponse, RenderSearchTemplateRequestBuilder> {
+
+    public static final RenderSearchTemplateAction INSTANCE = new RenderSearchTemplateAction();
+    public static final String NAME = "cluster:admin/render/template/search";
+
+    public RenderSearchTemplateAction() {
+        super(NAME);
+    }
+
+    @Override
+    public RenderSearchTemplateRequestBuilder newRequestBuilder(ElasticsearchClient client) {
+        return new RenderSearchTemplateRequestBuilder(client, this);
+    }
+
+    @Override
+    public RenderSearchTemplateResponse newResponse() {
+        return new RenderSearchTemplateResponse();
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateRequest.java
new file mode 100644
index 0000000..a51090e
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateRequest.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.admin.cluster.validate.template;
+
+import org.elasticsearch.action.ActionRequest;
+import org.elasticsearch.action.ActionRequestValidationException;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.script.Template;
+
+import java.io.IOException;
+
+public class RenderSearchTemplateRequest extends ActionRequest<RenderSearchTemplateRequest> {
+
+    private Template template;
+    
+    public void template(Template template) {
+        this.template = template;
+    }
+    
+    public Template template() {
+        return template;
+    }
+    
+    @Override
+    public ActionRequestValidationException validate() {
+        ActionRequestValidationException exception = null;
+        if (template == null) {
+            exception = new ActionRequestValidationException();
+            exception.addValidationError("template must not be null");
+        }
+        return exception;
+    }
+    
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        super.writeTo(out);
+        boolean hasTemplate = template!= null;
+        out.writeBoolean(hasTemplate);
+        if (hasTemplate) {
+            template.writeTo(out);
+        }
+    }
+    
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        super.readFrom(in);
+        if (in.readBoolean()) {
+            template = Template.readTemplate(in);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateRequestBuilder.java
new file mode 100644
index 0000000..f7e3da1
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateRequestBuilder.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.admin.cluster.validate.template;
+
+import org.elasticsearch.action.ActionRequestBuilder;
+import org.elasticsearch.client.ElasticsearchClient;
+import org.elasticsearch.script.Template;
+
+public class RenderSearchTemplateRequestBuilder extends ActionRequestBuilder<RenderSearchTemplateRequest, RenderSearchTemplateResponse, RenderSearchTemplateRequestBuilder> {
+
+    public RenderSearchTemplateRequestBuilder(ElasticsearchClient client,
+            RenderSearchTemplateAction action) {
+        super(client, action, new RenderSearchTemplateRequest());
+    }
+    
+    public RenderSearchTemplateRequestBuilder template(Template template) {
+        request.template(template);
+        return this;
+    }
+    
+    public Template template() {
+        return request.template();
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateResponse.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateResponse.java
new file mode 100644
index 0000000..d14a9a4
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateResponse.java
@@ -0,0 +1,68 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.admin.cluster.validate.template;
+
+import org.elasticsearch.action.ActionResponse;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
+
+public class RenderSearchTemplateResponse extends ActionResponse implements ToXContent {
+
+    private BytesReference source;
+
+    public BytesReference source() {
+        return source;
+    }
+    
+    public void source(BytesReference source) {
+        this.source = source;
+    }
+    
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        super.writeTo(out);
+        boolean hasSource = source != null;
+        out.writeBoolean(hasSource);
+        if (hasSource) {
+            out.writeBytesReference(source);
+        }
+    }
+    
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        super.readFrom(in);
+        if (in.readBoolean()) {
+            source = in.readBytesReference();
+        }
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        builder.rawField("template_output", source);
+        builder.endObject();
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/TransportRenderSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/TransportRenderSearchTemplateAction.java
new file mode 100644
index 0000000..5fe8297
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/TransportRenderSearchTemplateAction.java
@@ -0,0 +1,67 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.admin.cluster.validate.template;
+
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.support.ActionFilters;
+import org.elasticsearch.action.support.HandledTransportAction;
+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.util.concurrent.AbstractRunnable;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.ScriptContext;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.TransportService;
+
+public class TransportRenderSearchTemplateAction extends HandledTransportAction<RenderSearchTemplateRequest, RenderSearchTemplateResponse> {
+
+    private final ScriptService scriptService;
+
+    @Inject
+    public TransportRenderSearchTemplateAction(ScriptService scriptService, Settings settings, ThreadPool threadPool,
+            TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {
+        super(settings, RenderSearchTemplateAction.NAME, threadPool, transportService, actionFilters, indexNameExpressionResolver, RenderSearchTemplateRequest::new);
+        this.scriptService = scriptService;
+    }
+
+    @Override
+    protected void doExecute(final RenderSearchTemplateRequest request, final ActionListener<RenderSearchTemplateResponse> listener) {
+        threadPool.generic().execute(new AbstractRunnable() {
+
+            @Override
+            public void onFailure(Throwable t) {
+                listener.onFailure(t);
+            }
+
+            @Override
+            protected void doRun() throws Exception {
+                ExecutableScript executable = scriptService.executable(request.template(), ScriptContext.Standard.SEARCH, request);
+                BytesReference processedTemplate = (BytesReference) executable.run();
+                RenderSearchTemplateResponse response = new RenderSearchTemplateResponse();
+                response.source(processedTemplate);
+                listener.onResponse(response);
+            }
+        });
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java
index 12ef72b..24820ba 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.action.admin.indices.mapping.get;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
@@ -33,13 +35,12 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
-import static java.util.Collections.emptyMap;
 import static java.util.Collections.unmodifiableMap;
 
 /** Response object for {@link GetFieldMappingsRequest} API */
 public class GetFieldMappingsResponse extends ActionResponse implements ToXContent {
 
-    private Map<String, Map<String, Map<String, FieldMappingMetaData>>> mappings = emptyMap();
+    private Map<String, Map<String, Map<String, FieldMappingMetaData>>> mappings = ImmutableMap.of();
 
     GetFieldMappingsResponse(Map<String, Map<String, Map<String, FieldMappingMetaData>>> mappings) {
         this.mappings = mappings;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java
index 82d6c21..060d94a 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java
@@ -25,18 +25,16 @@ import org.elasticsearch.action.support.HandledTransportAction;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
-import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReferenceArray;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  */
 public class TransportGetFieldMappingsAction extends HandledTransportAction<GetFieldMappingsRequest, GetFieldMappingsResponse> {
@@ -90,7 +88,7 @@ public class TransportGetFieldMappingsAction extends HandledTransportAction<GetF
     }
 
     private GetFieldMappingsResponse merge(AtomicReferenceArray<Object> indexResponses) {
-        Map<String, Map<String, Map<String, GetFieldMappingsResponse.FieldMappingMetaData>>> mergedResponses = new HashMap<>();
+        MapBuilder<String, Map<String, Map<String, GetFieldMappingsResponse.FieldMappingMetaData>>> mergedResponses = MapBuilder.newMapBuilder();
         for (int i = 0; i < indexResponses.length(); i++) {
             Object element = indexResponses.get(i);
             if (element instanceof GetFieldMappingsResponse) {
@@ -98,6 +96,6 @@ public class TransportGetFieldMappingsAction extends HandledTransportAction<GetF
                 mergedResponses.putAll(response.mappings());
             }
         }
-        return new GetFieldMappingsResponse(unmodifiableMap(mergedResponses));
+        return new GetFieldMappingsResponse(mergedResponses.immutableMap());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java
index c71f60e..a7b780a 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.action.admin.indices.mapping.get;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.admin.indices.mapping.get.GetFieldMappingsResponse.FieldMappingMetaData;
 import org.elasticsearch.action.support.ActionFilters;
@@ -54,7 +56,6 @@ import java.util.Iterator;
 import java.util.Map;
 import java.util.stream.Collectors;
 
-import static java.util.Collections.singletonMap;
 import static org.elasticsearch.common.util.CollectionUtils.newLinkedList;
 
 /**
@@ -109,13 +110,13 @@ public class TransportGetFieldMappingsIndexAction extends TransportSingleShardAc
         MapBuilder<String, Map<String, FieldMappingMetaData>> typeMappings = new MapBuilder<>();
         for (String type : typeIntersection) {
             DocumentMapper documentMapper = indexService.mapperService().documentMapper(type);
-            Map<String, FieldMappingMetaData> fieldMapping = findFieldMappingsByType(documentMapper, request);
+            ImmutableMap<String, FieldMappingMetaData> fieldMapping = findFieldMappingsByType(documentMapper, request);
             if (!fieldMapping.isEmpty()) {
                 typeMappings.put(type, fieldMapping);
             }
         }
 
-        return new GetFieldMappingsResponse(singletonMap(shardId.getIndex(), typeMappings.immutableMap()));
+        return new GetFieldMappingsResponse(ImmutableMap.of(shardId.getIndex(), typeMappings.immutableMap()));
     }
 
     @Override
@@ -165,7 +166,7 @@ public class TransportGetFieldMappingsIndexAction extends TransportSingleShardAc
         }
     };
 
-    private Map<String, FieldMappingMetaData> findFieldMappingsByType(DocumentMapper documentMapper, GetFieldMappingsIndexRequest request) {
+    private ImmutableMap<String, FieldMappingMetaData> findFieldMappingsByType(DocumentMapper documentMapper, GetFieldMappingsIndexRequest request) {
         MapBuilder<String, FieldMappingMetaData> fieldMappings = new MapBuilder<>();
         final DocumentFieldMappers allFieldMappers = documentMapper.mappers();
         for (String field : request.fields()) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/segments/IndexShardSegments.java b/core/src/main/java/org/elasticsearch/action/admin/indices/segments/IndexShardSegments.java
index 4132c7b..7cbb664 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/segments/IndexShardSegments.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/segments/IndexShardSegments.java
@@ -19,9 +19,9 @@
 
 package org.elasticsearch.action.admin.indices.segments;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.index.shard.ShardId;
 
+import java.util.Arrays;
 import java.util.Iterator;
 
 public class IndexShardSegments implements Iterable<ShardSegments> {
@@ -49,6 +49,6 @@ public class IndexShardSegments implements Iterable<ShardSegments> {
 
     @Override
     public Iterator<ShardSegments> iterator() {
-        return Iterators.forArray(shards);
+        return Arrays.stream(shards).iterator();
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndexShardStats.java b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndexShardStats.java
index e599468..dd4cc64 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndexShardStats.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndexShardStats.java
@@ -19,13 +19,13 @@
 
 package org.elasticsearch.action.admin.indices.stats;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
 import org.elasticsearch.index.shard.ShardId;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.Iterator;
 
 /**
@@ -57,7 +57,7 @@ public class IndexShardStats implements Iterable<ShardStats>, Streamable {
 
     @Override
     public Iterator<ShardStats> iterator() {
-        return Iterators.forArray(shards);
+        return Arrays.stream(shards).iterator();
     }
 
     private CommonStats total = null;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsResponse.java
index 4f427b5..5cb94b2 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsResponse.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.indices.stats;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.action.ShardOperationFailedException;
 import org.elasticsearch.action.support.broadcast.BroadcastResponse;
 import org.elasticsearch.cluster.routing.ShardRouting;
@@ -37,15 +38,13 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  */
 public class IndicesStatsResponse extends BroadcastResponse implements ToXContent {
 
     private ShardStats[] shards;
 
-    private Map<ShardRouting, CommonStats> shardStatsMap;
+    private ImmutableMap<ShardRouting, CommonStats> shardStatsMap;
 
     IndicesStatsResponse() {
 
@@ -56,15 +55,16 @@ public class IndicesStatsResponse extends BroadcastResponse implements ToXConten
         this.shards = shards;
     }
 
-    public Map<ShardRouting, CommonStats> asMap() {
-        if (this.shardStatsMap == null) {
-            Map<ShardRouting, CommonStats> shardStatsMap = new HashMap<>();
+    public ImmutableMap<ShardRouting, CommonStats> asMap() {
+        if (shardStatsMap == null) {
+            ImmutableMap.Builder<ShardRouting, CommonStats> mb = ImmutableMap.builder();
             for (ShardStats ss : shards) {
-                shardStatsMap.put(ss.getShardRouting(), ss.getStats());
+                mb.put(ss.getShardRouting(), ss.getStats());
             }
-            this.shardStatsMap = unmodifiableMap(shardStatsMap);
+
+            shardStatsMap = mb.build();
         }
-        return this.shardStatsMap;
+        return shardStatsMap;
     }
 
     public ShardStats[] getShards() {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/IndexShardUpgradeStatus.java b/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/IndexShardUpgradeStatus.java
index e1cd163..c4b5820 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/IndexShardUpgradeStatus.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/IndexShardUpgradeStatus.java
@@ -19,9 +19,9 @@
 
 package org.elasticsearch.action.admin.indices.upgrade.get;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.index.shard.ShardId;
 
+import java.util.Arrays;
 import java.util.Iterator;
 
 public class IndexShardUpgradeStatus implements Iterable<ShardUpgradeStatus> {
@@ -49,7 +49,7 @@ public class IndexShardUpgradeStatus implements Iterable<ShardUpgradeStatus> {
 
     @Override
     public Iterator<ShardUpgradeStatus> iterator() {
-        return Iterators.forArray(shards);
+        return Arrays.stream(shards).iterator();
     }
 
     public long getTotalBytes() {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateAction.java
deleted file mode 100644
index 0cc7158..0000000
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateAction.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.admin.indices.validate.template;
-
-import org.elasticsearch.action.Action;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class RenderSearchTemplateAction extends Action<RenderSearchTemplateRequest, RenderSearchTemplateResponse, RenderSearchTemplateRequestBuilder> {
-
-    public static final RenderSearchTemplateAction INSTANCE = new RenderSearchTemplateAction();
-    public static final String NAME = "indices:admin/render/template/search";
-
-    public RenderSearchTemplateAction() {
-        super(NAME);
-    }
-
-    @Override
-    public RenderSearchTemplateRequestBuilder newRequestBuilder(ElasticsearchClient client) {
-        return new RenderSearchTemplateRequestBuilder(client, this);
-    }
-
-    @Override
-    public RenderSearchTemplateResponse newResponse() {
-        return new RenderSearchTemplateResponse();
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateRequest.java
deleted file mode 100644
index bde255f..0000000
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateRequest.java
+++ /dev/null
@@ -1,69 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.admin.indices.validate.template;
-
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionRequestValidationException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.script.Template;
-
-import java.io.IOException;
-
-public class RenderSearchTemplateRequest extends ActionRequest<RenderSearchTemplateRequest> {
-
-    private Template template;
-    
-    public void template(Template template) {
-        this.template = template;
-    }
-    
-    public Template template() {
-        return template;
-    }
-    
-    @Override
-    public ActionRequestValidationException validate() {
-        ActionRequestValidationException exception = null;
-        if (template == null) {
-            exception = new ActionRequestValidationException();
-            exception.addValidationError("template must not be null");
-        }
-        return exception;
-    }
-    
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        boolean hasTemplate = template!= null;
-        out.writeBoolean(hasTemplate);
-        if (hasTemplate) {
-            template.writeTo(out);
-        }
-    }
-    
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        if (in.readBoolean()) {
-            template = Template.readTemplate(in);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateRequestBuilder.java
deleted file mode 100644
index 493dc7e..0000000
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateRequestBuilder.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.admin.indices.validate.template;
-
-import org.elasticsearch.action.ActionRequestBuilder;
-import org.elasticsearch.client.ElasticsearchClient;
-import org.elasticsearch.script.Template;
-
-public class RenderSearchTemplateRequestBuilder extends ActionRequestBuilder<RenderSearchTemplateRequest, RenderSearchTemplateResponse, RenderSearchTemplateRequestBuilder> {
-
-    public RenderSearchTemplateRequestBuilder(ElasticsearchClient client,
-            RenderSearchTemplateAction action) {
-        super(client, action, new RenderSearchTemplateRequest());
-    }
-    
-    public RenderSearchTemplateRequestBuilder template(Template template) {
-        request.template(template);
-        return this;
-    }
-    
-    public Template template() {
-        return request.template();
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateResponse.java
deleted file mode 100644
index 2d3ca01..0000000
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateResponse.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.admin.indices.validate.template;
-
-import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-
-import java.io.IOException;
-
-public class RenderSearchTemplateResponse extends ActionResponse implements ToXContent {
-
-    private BytesReference source;
-
-    public BytesReference source() {
-        return source;
-    }
-    
-    public void source(BytesReference source) {
-        this.source = source;
-    }
-    
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        boolean hasSource = source != null;
-        out.writeBoolean(hasSource);
-        if (hasSource) {
-            out.writeBytesReference(source);
-        }
-    }
-    
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        if (in.readBoolean()) {
-            source = in.readBytesReference();
-        }
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        builder.rawField("template_output", source);
-        builder.endObject();
-        return builder;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/TransportRenderSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/TransportRenderSearchTemplateAction.java
deleted file mode 100644
index e9208ec..0000000
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/TransportRenderSearchTemplateAction.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.admin.indices.validate.template;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.support.ActionFilters;
-import org.elasticsearch.action.support.HandledTransportAction;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.util.concurrent.AbstractRunnable;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-public class TransportRenderSearchTemplateAction extends HandledTransportAction<RenderSearchTemplateRequest, RenderSearchTemplateResponse> {
-
-    private final ScriptService scriptService;
-
-    @Inject
-    public TransportRenderSearchTemplateAction(ScriptService scriptService, Settings settings, ThreadPool threadPool,
-            TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {
-        super(settings, RenderSearchTemplateAction.NAME, threadPool, transportService, actionFilters, indexNameExpressionResolver, RenderSearchTemplateRequest::new);
-        this.scriptService = scriptService;
-    }
-
-    @Override
-    protected void doExecute(final RenderSearchTemplateRequest request, final ActionListener<RenderSearchTemplateResponse> listener) {
-        threadPool.generic().execute(new AbstractRunnable() {
-
-            @Override
-            public void onFailure(Throwable t) {
-                listener.onFailure(t);
-            }
-
-            @Override
-            protected void doRun() throws Exception {
-                ExecutableScript executable = scriptService.executable(request.template(), ScriptContext.Standard.SEARCH, request);
-                BytesReference processedTemplate = (BytesReference) executable.run();
-                RenderSearchTemplateResponse response = new RenderSearchTemplateResponse();
-                response.source(processedTemplate);
-                listener.onResponse(response);
-            }
-        });
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkResponse.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkResponse.java
index 50525fb..2076086 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkResponse.java
@@ -19,13 +19,13 @@
 
 package org.elasticsearch.action.bulk;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.TimeValue;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.Iterator;
 
 /**
@@ -95,7 +95,7 @@ public class BulkResponse extends ActionResponse implements Iterable<BulkItemRes
 
     @Override
     public Iterator<BulkItemResponse> iterator() {
-        return Iterators.forArray(responses);
+        return Arrays.stream(responses).iterator();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java b/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
index c071885..0f00b87 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
@@ -47,7 +47,6 @@ import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.VersionType;
-import org.elasticsearch.index.engine.DocumentAlreadyExistsException;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.index.mapper.Mapping;
@@ -97,6 +96,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
     protected TransportRequestOptions transportOptions() {
         return BulkAction.INSTANCE.transportOptions(settings);
     }
+
     @Override
     protected BulkShardResponse newResponseInstance() {
         return new BulkShardResponse();
@@ -416,7 +416,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                 } catch (Throwable t) {
                     t = ExceptionsHelper.unwrapCause(t);
                     boolean retry = false;
-                    if (t instanceof VersionConflictEngineException || (t instanceof DocumentAlreadyExistsException && translate.operation() == UpdateHelper.Operation.UPSERT)) {
+                    if (t instanceof VersionConflictEngineException) {
                         retry = true;
                     }
                     return new UpdateResult(translate, indexRequest, retry, t, null);
@@ -460,20 +460,12 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                     SourceToParse sourceToParse = SourceToParse.source(SourceToParse.Origin.REPLICA, indexRequest.source()).index(shardId.getIndex()).type(indexRequest.type()).id(indexRequest.id())
                             .routing(indexRequest.routing()).parent(indexRequest.parent()).timestamp(indexRequest.timestamp()).ttl(indexRequest.ttl());
 
-                    final Engine.IndexingOperation operation;
-                    if (indexRequest.opType() == IndexRequest.OpType.INDEX) {
-                        operation = indexShard.prepareIndex(sourceToParse, indexRequest.version(), indexRequest.versionType(), Engine.Operation.Origin.REPLICA);
-                    } else {
-                        assert indexRequest.opType() == IndexRequest.OpType.CREATE : indexRequest.opType();
-                        operation = indexShard.prepareCreate(sourceToParse,
-                                indexRequest.version(), indexRequest.versionType(),
-                                Engine.Operation.Origin.REPLICA);
-                    }
+                    final Engine.Index operation = indexShard.prepareIndex(sourceToParse, indexRequest.version(), indexRequest.versionType(), Engine.Operation.Origin.REPLICA);
                     Mapping update = operation.parsedDoc().dynamicMappingsUpdate();
                     if (update != null) {
                         throw new RetryOnReplicaException(shardId, "Mappings are not available on the replica yet, triggered update: " + update);
                     }
-                    operation.execute(indexShard);
+                    indexShard.index(operation);
                     location = locationToSync(location, operation.getTranslogLocation());
                 } catch (Throwable e) {
                     // if its not an ignore replica failure, we need to make sure to bubble up the failure
@@ -500,7 +492,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
             }
         }
 
-       processAfter(request.refresh(), indexShard, location);
+        processAfter(request.refresh(), indexShard, location);
     }
 
     private void applyVersion(BulkItemRequest item, long version, VersionType versionType) {
diff --git a/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java b/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
index 3cc6f06..46f998f 100644
--- a/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
@@ -174,7 +174,12 @@ public class TransportExistsAction extends TransportBroadcastAction<ExistsReques
             }
             context.preProcess();
             try {
-                boolean exists = Lucene.exists(context, context.query(), Lucene.createExistsCollector());
+                boolean exists;
+                try {
+                    exists = Lucene.exists(context.searcher(), context.query());
+                } finally {
+                    context.clearReleasables(SearchContext.Lifetime.COLLECTION);
+                }
                 return new ShardExistsResponse(request.shardId(), exists);
             } catch (Exception e) {
                 throw new QueryPhaseExecutionException(context, "failed to execute exists", e);
diff --git a/core/src/main/java/org/elasticsearch/action/get/MultiGetRequest.java b/core/src/main/java/org/elasticsearch/action/get/MultiGetRequest.java
index 0d03bbb..030b0ee 100644
--- a/core/src/main/java/org/elasticsearch/action/get/MultiGetRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/get/MultiGetRequest.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.action.get;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.action.*;
 import org.elasticsearch.action.support.IndicesOptions;
@@ -27,6 +26,7 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
@@ -37,10 +37,7 @@ import org.elasticsearch.index.VersionType;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Iterator;
-import java.util.List;
+import java.util.*;
 
 public class MultiGetRequest extends ActionRequest<MultiGetRequest> implements Iterable<MultiGetRequest.Item>, CompositeIndicesRequest, RealtimeRequest {
 
@@ -498,7 +495,7 @@ public class MultiGetRequest extends ActionRequest<MultiGetRequest> implements I
 
     @Override
     public Iterator<Item> iterator() {
-        return Iterators.unmodifiableIterator(items.iterator());
+        return Collections.unmodifiableCollection(items).iterator();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/get/MultiGetResponse.java b/core/src/main/java/org/elasticsearch/action/get/MultiGetResponse.java
index 7abfb2b..32e10b8 100644
--- a/core/src/main/java/org/elasticsearch/action/get/MultiGetResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/get/MultiGetResponse.java
@@ -19,10 +19,8 @@
 
 package org.elasticsearch.action.get;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.action.percolate.PercolateResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
@@ -31,7 +29,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 
 import java.io.IOException;
-import java.util.Collections;
+import java.util.Arrays;
 import java.util.Iterator;
 
 public class MultiGetResponse extends ActionResponse implements Iterable<MultiGetItemResponse>, ToXContent {
@@ -126,7 +124,7 @@ public class MultiGetResponse extends ActionResponse implements Iterable<MultiGe
 
     @Override
     public Iterator<MultiGetItemResponse> iterator() {
-        return Iterators.forArray(responses);
+        return Arrays.stream(responses).iterator();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java b/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
index c171ae9..ad7b9c1 100644
--- a/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
@@ -49,14 +49,14 @@ import static org.elasticsearch.action.ValidateActions.addValidationError;
 /**
  * Index request to index a typed JSON document into a specific index and make it searchable. Best
  * created using {@link org.elasticsearch.client.Requests#indexRequest(String)}.
- * <p>
+ *
  * The index requires the {@link #index()}, {@link #type(String)}, {@link #id(String)} and
  * {@link #source(byte[])} to be set.
- * <p>
+ *
  * The source (content to index) can be set in its bytes form using ({@link #source(byte[])}),
  * its string form ({@link #source(String)}) or using a {@link org.elasticsearch.common.xcontent.XContentBuilder}
  * ({@link #source(org.elasticsearch.common.xcontent.XContentBuilder)}).
- * <p>
+ *
  * If the {@link #id(String)} is not set, it will be automatically generated.
  *
  * @see IndexResponse
@@ -114,7 +114,7 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
 
         public static OpType fromString(String sOpType) {
             String lowersOpType = sOpType.toLowerCase(Locale.ROOT);
-            switch(lowersOpType){
+            switch (lowersOpType) {
                 case "create":
                     return OpType.CREATE;
                 case "index":
@@ -216,6 +216,14 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
         if (source == null) {
             validationException = addValidationError("source is missing", validationException);
         }
+
+        if (opType() == OpType.CREATE) {
+            if (versionType != VersionType.INTERNAL || version != Versions.MATCH_DELETED) {
+                validationException = addValidationError("create operations do not support versioning. use index instead", validationException);
+                return validationException;
+            }
+        }
+
         if (!versionType.validateVersionForWrites(version)) {
             validationException = addValidationError("illegal version value [" + version + "] for version type [" + versionType.name() + "]", validationException);
         }
@@ -370,7 +378,7 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
 
     /**
      * Sets the document source to index.
-     * <p>
+     *
      * Note, its preferable to either set it using {@link #source(org.elasticsearch.common.xcontent.XContentBuilder)}
      * or using the {@link #source(byte[])}.
      */
@@ -480,6 +488,10 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
      */
     public IndexRequest opType(OpType opType) {
         this.opType = opType;
+        if (opType == OpType.CREATE) {
+            version(Versions.MATCH_DELETED);
+            versionType(VersionType.INTERNAL);
+        }
         return this;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java b/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
index 3e98f1a..63b8237 100644
--- a/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
@@ -54,7 +54,7 @@ import org.elasticsearch.transport.TransportService;
 
 /**
  * Performs the index operation.
- * <p>
+ *
  * Allows for the following settings:
  * <ul>
  * <li><b>autoCreateIndex</b>: When set to <tt>true</tt>, will automatically create an index if one does not exists.
@@ -167,6 +167,7 @@ public class TransportIndexAction extends TransportReplicationAction<IndexReques
         IndexShard indexShard = indexService.getShard(shardRequest.shardId.id());
 
         final WriteResult<IndexResponse> result = executeIndexRequestOnPrimary(null, request, indexShard);
+
         final IndexResponse response = result.response;
         final Translog.Location location = result.location;
         processAfter(request.refresh(), indexShard, location);
@@ -180,18 +181,12 @@ public class TransportIndexAction extends TransportReplicationAction<IndexReques
         SourceToParse sourceToParse = SourceToParse.source(SourceToParse.Origin.REPLICA, request.source()).index(shardId.getIndex()).type(request.type()).id(request.id())
                 .routing(request.routing()).parent(request.parent()).timestamp(request.timestamp()).ttl(request.ttl());
 
-        final Engine.IndexingOperation operation;
-        if (request.opType() == IndexRequest.OpType.INDEX) {
-            operation = indexShard.prepareIndex(sourceToParse, request.version(), request.versionType(), Engine.Operation.Origin.REPLICA);
-        } else {
-            assert request.opType() == IndexRequest.OpType.CREATE : request.opType();
-            operation = indexShard.prepareCreate(sourceToParse, request.version(), request.versionType(), Engine.Operation.Origin.REPLICA);
-        }
+        final Engine.Index operation = indexShard.prepareIndex(sourceToParse, request.version(), request.versionType(), Engine.Operation.Origin.REPLICA);
         Mapping update = operation.parsedDoc().dynamicMappingsUpdate();
         if (update != null) {
             throw new RetryOnReplicaException(shardId, "Mappings are not available on the replica yet, triggered update: " + update);
         }
-        operation.execute(indexShard);
+        indexShard.index(operation);
         processAfter(request.refresh(), indexShard, operation.getTranslogLocation());
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateResponse.java b/core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateResponse.java
index 4a32fc1..812d525 100644
--- a/core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateResponse.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.action.percolate;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.Nullable;
@@ -30,6 +29,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.Iterator;
 
 /**
@@ -52,7 +52,7 @@ public class MultiPercolateResponse extends ActionResponse implements Iterable<M
 
     @Override
     public Iterator<Item> iterator() {
-        return Iterators.forArray(items);
+        return Arrays.stream(items).iterator();
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/search/ClearScrollResponse.java b/core/src/main/java/org/elasticsearch/action/search/ClearScrollResponse.java
index ffe4763..3540daa 100644
--- a/core/src/main/java/org/elasticsearch/action/search/ClearScrollResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/search/ClearScrollResponse.java
@@ -19,12 +19,12 @@
 
 package org.elasticsearch.action.search;
 
-import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.StatusToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.rest.RestStatus;
 
 import java.io.IOException;
@@ -69,6 +69,8 @@ public class ClearScrollResponse extends ActionResponse implements StatusToXCont
 
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.field(Fields.SUCCEEDED, succeeded);
+        builder.field(Fields.NUMFREED, numFreed);
         return builder;
     }
 
@@ -85,4 +87,10 @@ public class ClearScrollResponse extends ActionResponse implements StatusToXCont
         out.writeBoolean(succeeded);
         out.writeVInt(numFreed);
     }
+
+    static final class Fields {
+        static final XContentBuilderString SUCCEEDED = new XContentBuilderString("succeeded");
+        static final XContentBuilderString NUMFREED = new XContentBuilderString("num_freed");
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/action/search/MultiSearchResponse.java b/core/src/main/java/org/elasticsearch/action/search/MultiSearchResponse.java
index 8074565..0a9d619 100644
--- a/core/src/main/java/org/elasticsearch/action/search/MultiSearchResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/search/MultiSearchResponse.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.action.search;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.Nullable;
@@ -32,7 +31,7 @@ import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.common.xcontent.XContentFactory;
 
 import java.io.IOException;
-import java.util.Collections;
+import java.util.Arrays;
 import java.util.Iterator;
 
 /**
@@ -122,7 +121,7 @@ public class MultiSearchResponse extends ActionResponse implements Iterable<Mult
 
     @Override
     public Iterator<Item> iterator() {
-        return Iterators.forArray(items);
+        return Arrays.stream(items).iterator();
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchHelper.java b/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchHelper.java
index 2b99b6f..e4472c7 100644
--- a/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchHelper.java
+++ b/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchHelper.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.search.type;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRefBuilder;
 import org.elasticsearch.action.search.SearchRequest;
@@ -37,8 +38,6 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
-import static java.util.Collections.emptyMap;
-
 /**
  *
  */
@@ -113,7 +112,7 @@ public abstract class TransportSearchHelper {
         Map<String, String> attributes;
         int attributesSize = Integer.parseInt(elements[index++]);
         if (attributesSize == 0) {
-            attributes = emptyMap();
+            attributes = ImmutableMap.of();
         } else {
             attributes = new HashMap<>(attributesSize);
             for (int i = 0; i < attributesSize; i++) {
diff --git a/core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java b/core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java
index 118e112..0597c26 100644
--- a/core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java
@@ -30,6 +30,7 @@ import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.*;
@@ -95,17 +96,22 @@ public abstract class TransportNodesAction<NodesRequest extends BaseNodesRequest
 
         private final NodesRequest request;
         private final String[] nodesIds;
+        private final DiscoveryNode[] nodes;
         private final ActionListener<NodesResponse> listener;
-        private final ClusterState clusterState;
         private final AtomicReferenceArray<Object> responses;
         private final AtomicInteger counter = new AtomicInteger();
 
         private AsyncAction(NodesRequest request, ActionListener<NodesResponse> listener) {
             this.request = request;
             this.listener = listener;
-            clusterState = clusterService.state();
+            ClusterState clusterState = clusterService.state();
             String[] nodesIds = resolveNodes(request, clusterState);
             this.nodesIds = filterNodeIds(clusterState.nodes(), nodesIds);
+            ImmutableOpenMap<String, DiscoveryNode> nodes = clusterState.nodes().nodes();
+            this.nodes = new DiscoveryNode[nodesIds.length];
+            for (int i = 0; i < nodesIds.length; i++) {
+                this.nodes[i] = nodes.get(nodesIds[i]);
+            }
             this.responses = new AtomicReferenceArray<>(this.nodesIds.length);
         }
 
@@ -128,7 +134,7 @@ public abstract class TransportNodesAction<NodesRequest extends BaseNodesRequest
             for (int i = 0; i < nodesIds.length; i++) {
                 final String nodeId = nodesIds[i];
                 final int idx = i;
-                final DiscoveryNode node = clusterState.nodes().nodes().get(nodeId);
+                final DiscoveryNode node = nodes[i];
                 try {
                     if (node == null) {
                         onFailure(idx, nodeId, new NoSuchNodeException(nodeId));
diff --git a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
index d7fca31..f3db2b6 100644
--- a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
@@ -56,7 +56,6 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.AbstractRunnable;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.engine.DocumentAlreadyExistsException;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.index.mapper.Mapping;
@@ -188,9 +187,6 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         if (cause instanceof VersionConflictEngineException) {
             return true;
         }
-        if (cause instanceof DocumentAlreadyExistsException) {
-            return true;
-        }
         return false;
     }
 
@@ -1036,22 +1032,17 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
 
     /** Utility method to create either an index or a create operation depending
      *  on the {@link OpType} of the request. */
-    private final Engine.IndexingOperation prepareIndexOperationOnPrimary(BulkShardRequest shardRequest, IndexRequest request, IndexShard indexShard) {
+    private final Engine.Index prepareIndexOperationOnPrimary(BulkShardRequest shardRequest, IndexRequest request, IndexShard indexShard) {
         SourceToParse sourceToParse = SourceToParse.source(SourceToParse.Origin.PRIMARY, request.source()).index(request.index()).type(request.type()).id(request.id())
                 .routing(request.routing()).parent(request.parent()).timestamp(request.timestamp()).ttl(request.ttl());
-        if (request.opType() == IndexRequest.OpType.INDEX) {
             return indexShard.prepareIndex(sourceToParse, request.version(), request.versionType(), Engine.Operation.Origin.PRIMARY);
-        } else {
-            assert request.opType() == IndexRequest.OpType.CREATE : request.opType();
-            return indexShard.prepareCreate(sourceToParse,
-                    request.version(), request.versionType(), Engine.Operation.Origin.PRIMARY);
-        }
+
     }
 
     /** Execute the given {@link IndexRequest} on a primary shard, throwing a
      *  {@link RetryOnPrimaryException} if the operation needs to be re-tried. */
     protected final WriteResult<IndexResponse> executeIndexRequestOnPrimary(BulkShardRequest shardRequest, IndexRequest request, IndexShard indexShard) throws Throwable {
-        Engine.IndexingOperation operation = prepareIndexOperationOnPrimary(shardRequest, request, indexShard);
+        Engine.Index operation = prepareIndexOperationOnPrimary(shardRequest, request, indexShard);
         Mapping update = operation.parsedDoc().dynamicMappingsUpdate();
         final ShardId shardId = indexShard.shardId();
         if (update != null) {
@@ -1064,7 +1055,7 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
                         "Dynamics mappings are not available on the node that holds the primary yet");
             }
         }
-        final boolean created = operation.execute(indexShard);
+        final boolean created = indexShard.index(operation);
 
         // update the version on request so it will happen on the replicas
         final long version = operation.version();
diff --git a/core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java b/core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java
index 2e815da..5f4f942 100644
--- a/core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.support.single.instance;
 
+import org.elasticsearch.ElasticsearchTimeoutException;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.action.UnavailableShardsException;
@@ -35,6 +36,7 @@ import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.ShardIterator;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.logging.support.LoggerMessageFormat;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.index.shard.ShardId;
@@ -42,6 +44,7 @@ import org.elasticsearch.node.NodeClosedException;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.*;
 
+import java.util.concurrent.TimeoutException;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.function.Supplier;
 
@@ -111,9 +114,8 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
         private volatile ClusterStateObserver observer;
         private ShardIterator shardIt;
         private DiscoveryNodes nodes;
-        private final AtomicBoolean operationStarted = new AtomicBoolean();
 
-        private AsyncSingleAction(Request request, ActionListener<Response> listener) {
+        AsyncSingleAction(Request request, ActionListener<Response> listener) {
             this.request = request;
             this.listener = listener;
         }
@@ -123,14 +125,14 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
             doStart();
         }
 
-        protected boolean doStart() {
+        protected void doStart() {
             nodes = observer.observedState().nodes();
             try {
                 ClusterBlockException blockException = checkGlobalBlock(observer.observedState());
                 if (blockException != null) {
                     if (blockException.retryable()) {
                         retry(blockException);
-                        return false;
+                        return;
                     } else {
                         throw blockException;
                     }
@@ -138,13 +140,14 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
                 request.concreteIndex(indexNameExpressionResolver.concreteSingleIndex(observer.observedState(), request));
                 // check if we need to execute, and if not, return
                 if (!resolveRequest(observer.observedState(), request, listener)) {
-                    return true;
+                    listener.onFailure(new IllegalStateException(LoggerMessageFormat.format("{} request {} could not be resolved", new ShardId(request.index, request.shardId), actionName)));
+                    return;
                 }
                 blockException = checkRequestBlock(observer.observedState(), request);
                 if (blockException != null) {
                     if (blockException.retryable()) {
                         retry(blockException);
-                        return false;
+                        return;
                     } else {
                         throw blockException;
                     }
@@ -152,13 +155,13 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
                 shardIt = shards(observer.observedState(), request);
             } catch (Throwable e) {
                 listener.onFailure(e);
-                return true;
+                return;
             }
 
             // no shardIt, might be in the case between index gateway recovery and shardIt initialization
             if (shardIt.size() == 0) {
                 retry(null);
-                return false;
+                return;
             }
 
             // this transport only make sense with an iterator that returns a single shard routing (like primary)
@@ -169,11 +172,7 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
 
             if (!shard.active()) {
                 retry(null);
-                return false;
-            }
-
-            if (!operationStarted.compareAndSet(false, true)) {
-                return true;
+                return;
             }
 
             request.shardId = shardIt.shardId().id();
@@ -197,24 +196,30 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
 
                 @Override
                 public void handleException(TransportException exp) {
+                    Throwable cause = exp.unwrapCause();
                     // if we got disconnected from the node, or the node / shard is not in the right state (being closed)
-                    if (exp.unwrapCause() instanceof ConnectTransportException || exp.unwrapCause() instanceof NodeClosedException ||
+                    if (cause instanceof ConnectTransportException || cause instanceof NodeClosedException ||
                             retryOnFailure(exp)) {
-                        operationStarted.set(false);
-                        // we already marked it as started when we executed it (removed the listener) so pass false
-                        // to re-add to the cluster listener
-                        retry(null);
+                        retry(cause);
                     } else {
                         listener.onFailure(exp);
                     }
                 }
             });
-            return true;
         }
 
         void retry(final @Nullable Throwable failure) {
             if (observer.isTimedOut()) {
                 // we running as a last attempt after a timeout has happened. don't retry
+                Throwable listenFailure = failure;
+                if (listenFailure == null) {
+                    if (shardIt == null) {
+                        listenFailure = new UnavailableShardsException(new ShardId(request.concreteIndex(), -1), "Timeout waiting for [{}], request: {}", request.timeout(), actionName);
+                    } else {
+                        listenFailure = new UnavailableShardsException(shardIt.shardId(), "[{}] shardIt, [{}] active : Timeout waiting for [{}], request: {}", shardIt.size(), shardIt.sizeActive(), request.timeout(), actionName);
+                    }
+                }
+                listener.onFailure(listenFailure);
                 return;
             }
 
@@ -232,17 +237,7 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
                 @Override
                 public void onTimeout(TimeValue timeout) {
                     // just to be on the safe side, see if we can start it now?
-                    if (!doStart()) {
-                        Throwable listenFailure = failure;
-                        if (listenFailure == null) {
-                            if (shardIt == null) {
-                                listenFailure = new UnavailableShardsException(new ShardId(request.concreteIndex(), -1), "Timeout waiting for [" + timeout + "], request: " + request.toString());
-                            } else {
-                                listenFailure = new UnavailableShardsException(shardIt.shardId(), "[" + shardIt.size() + "] shardIt, [" + shardIt.sizeActive() + "] active : Timeout waiting for [" + timeout + "], request: " + request.toString());
-                            }
-                        }
-                        listener.onFailure(listenFailure);
-                    }
+                    doStart();
                 }
             }, request.timeout());
         }
diff --git a/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsRequest.java b/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsRequest.java
index c6db738..6d880a7 100644
--- a/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsRequest.java
@@ -19,11 +19,11 @@
 
 package org.elasticsearch.action.termvectors;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.action.*;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentFactory;
@@ -74,7 +74,7 @@ public class MultiTermVectorsRequest extends ActionRequest<MultiTermVectorsReque
 
     @Override
     public Iterator<TermVectorsRequest> iterator() {
-        return Iterators.unmodifiableIterator(requests.iterator());
+        return Collections.unmodifiableCollection(requests).iterator();
     }
 
     public boolean isEmpty() {
diff --git a/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsResponse.java b/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsResponse.java
index fe013d5..6eb3b32 100644
--- a/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsResponse.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.action.termvectors;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -30,6 +29,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.Iterator;
 
 public class MultiTermVectorsResponse extends ActionResponse implements Iterable<MultiTermVectorsItemResponse>, ToXContent {
@@ -120,7 +120,7 @@ public class MultiTermVectorsResponse extends ActionResponse implements Iterable
 
     @Override
     public Iterator<MultiTermVectorsItemResponse> iterator() {
-        return Iterators.forArray(responses);
+        return Arrays.stream(responses).iterator();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java b/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
index 7479416..2a639c8 100644
--- a/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
+++ b/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
@@ -48,9 +48,8 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.index.engine.DocumentAlreadyExistsException;
-import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.index.IndexService;
+import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndexAlreadyExistsException;
 import org.elasticsearch.indices.IndicesService;
@@ -170,7 +169,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
         final UpdateHelper.Result result = updateHelper.prepare(request, indexShard);
         switch (result.operation()) {
             case UPSERT:
-                IndexRequest upsertRequest = new IndexRequest((IndexRequest)result.action(), request);
+                IndexRequest upsertRequest = new IndexRequest(result.action(), request);
                 // we fetch it from the index request so we don't generate the bytes twice, its already done in the index request
                 final BytesReference upsertSourceBytes = upsertRequest.source();
                 indexAction.execute(upsertRequest, new ActionListener<IndexResponse>() {
@@ -189,7 +188,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
                     @Override
                     public void onFailure(Throwable e) {
                         e = ExceptionsHelper.unwrapCause(e);
-                        if (e instanceof VersionConflictEngineException || e instanceof DocumentAlreadyExistsException) {
+                        if (e instanceof VersionConflictEngineException) {
                             if (retryCount < request.retryOnConflict()) {
                                 threadPool.executor(executor()).execute(new ActionRunnable<UpdateResponse>(listener) {
                                     @Override
@@ -205,7 +204,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
                 });
                 break;
             case INDEX:
-                IndexRequest indexRequest = new IndexRequest((IndexRequest)result.action(), request);
+                IndexRequest indexRequest = new IndexRequest(result.action(), request);
                 // we fetch it from the index request so we don't generate the bytes twice, its already done in the index request
                 final BytesReference indexSourceBytes = indexRequest.source();
                 indexAction.execute(indexRequest, new ActionListener<IndexResponse>() {
@@ -235,7 +234,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
                 });
                 break;
             case DELETE:
-                DeleteRequest deleteRequest = new DeleteRequest((DeleteRequest)result.action(), request);
+                DeleteRequest deleteRequest = new DeleteRequest(result.action(), request);
                 deleteAction.execute(deleteRequest, new ActionListener<DeleteResponse>() {
                     @Override
                     public void onResponse(DeleteResponse response) {
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
index 542444b..9ebb2c9 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
@@ -26,7 +26,6 @@ import org.elasticsearch.common.PidFile;
 import org.elasticsearch.common.SuppressForbidden;
 import org.elasticsearch.common.cli.CliTool;
 import org.elasticsearch.common.cli.Terminal;
-import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.CreationException;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.logging.ESLogger;
@@ -249,13 +248,13 @@ final class Bootstrap {
 
         Environment environment = initialSettings(foreground);
         Settings settings = environment.settings();
+        setupLogging(settings, environment);
+        checkForCustomConfFile();
 
         if (environment.pidFile() != null) {
             PidFile.create(environment.pidFile(), true);
         }
 
-        setupLogging(settings, environment);
-
         if (System.getProperty("es.max-open-files", "false").equals("true")) {
             ESLogger logger = Loggers.getLogger(Bootstrap.class);
             logger.info("max_open_files [{}]", ProcessProbe.getInstance().getMaxFileDescriptorCount());
@@ -330,4 +329,21 @@ final class Bootstrap {
             System.err.flush();
         }
     }
+
+    private static void checkForCustomConfFile() {
+        String confFileSetting = System.getProperty("es.default.config");
+        checkUnsetAndMaybeExit(confFileSetting, "es.default.config");
+        confFileSetting = System.getProperty("es.config");
+        checkUnsetAndMaybeExit(confFileSetting, "es.config");
+        confFileSetting = System.getProperty("elasticsearch.config");
+        checkUnsetAndMaybeExit(confFileSetting, "elasticsearch.config");
+    }
+
+    private static void checkUnsetAndMaybeExit(String confFileSetting, String settingName) {
+        if (confFileSetting != null && confFileSetting.isEmpty() == false) {
+            ESLogger logger = Loggers.getLogger(Bootstrap.class);
+            logger.info("{} is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.", settingName);
+            System.exit(1);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Security.java b/core/src/main/java/org/elasticsearch/bootstrap/Security.java
index b27048d..66dda6e 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Security.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Security.java
@@ -165,7 +165,7 @@ final class Security {
         Map<String,String> m = new HashMap<>();
         m.put("repository-s3",       "org.elasticsearch.plugin.repository.s3.S3RepositoryPlugin");
         m.put("discovery-ec2",       "org.elasticsearch.plugin.discovery.ec2.Ec2DiscoveryPlugin");
-        m.put("cloud-gce",           "org.elasticsearch.plugin.cloud.gce.CloudGcePlugin");
+        m.put("discovery-gce",       "org.elasticsearch.plugin.discovery.gce.GceDiscoveryPlugin");
         m.put("lang-expression",     "org.elasticsearch.script.expression.ExpressionPlugin");
         m.put("lang-groovy",         "org.elasticsearch.script.groovy.GroovyPlugin");
         m.put("lang-javascript",     "org.elasticsearch.plugin.javascript.JavaScriptPlugin");
diff --git a/core/src/main/java/org/elasticsearch/client/Client.java b/core/src/main/java/org/elasticsearch/client/Client.java
index eafac2b..6e0b0b2 100644
--- a/core/src/main/java/org/elasticsearch/client/Client.java
+++ b/core/src/main/java/org/elasticsearch/client/Client.java
@@ -21,9 +21,6 @@ package org.elasticsearch.client;
 
 import org.elasticsearch.action.ActionFuture;
 import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateRequest;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateRequestBuilder;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateResponse;
 import org.elasticsearch.action.bulk.BulkRequest;
 import org.elasticsearch.action.bulk.BulkRequestBuilder;
 import org.elasticsearch.action.bulk.BulkResponse;
diff --git a/core/src/main/java/org/elasticsearch/client/ClusterAdminClient.java b/core/src/main/java/org/elasticsearch/client/ClusterAdminClient.java
index c3eb515..1be22b2 100644
--- a/core/src/main/java/org/elasticsearch/client/ClusterAdminClient.java
+++ b/core/src/main/java/org/elasticsearch/client/ClusterAdminClient.java
@@ -77,6 +77,9 @@ import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;
 import org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksRequest;
 import org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksRequestBuilder;
 import org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksResponse;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateRequest;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateRequestBuilder;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateResponse;
 
 /**
  * Administrative actions/operations against indices.
@@ -423,4 +426,25 @@ public interface ClusterAdminClient extends ElasticsearchClient {
      */
     SnapshotsStatusRequestBuilder prepareSnapshotStatus();
 
+
+    /**
+     * Return the rendered search request for a given search template.
+     *
+     * @param request The request
+     * @return The result future
+     */
+    ActionFuture<RenderSearchTemplateResponse> renderSearchTemplate(RenderSearchTemplateRequest request);
+
+    /**
+     * Return the rendered search request for a given search template.
+     *
+     * @param request  The request
+     * @param listener A listener to be notified of the result
+     */
+    void renderSearchTemplate(RenderSearchTemplateRequest request, ActionListener<RenderSearchTemplateResponse> listener);
+
+    /**
+     * Return the rendered search request for a given search template.
+     */
+    RenderSearchTemplateRequestBuilder prepareRenderSearchTemplate();
 }
diff --git a/core/src/main/java/org/elasticsearch/client/IndicesAdminClient.java b/core/src/main/java/org/elasticsearch/client/IndicesAdminClient.java
index 755bf33..75cae17 100644
--- a/core/src/main/java/org/elasticsearch/client/IndicesAdminClient.java
+++ b/core/src/main/java/org/elasticsearch/client/IndicesAdminClient.java
@@ -105,9 +105,6 @@ import org.elasticsearch.action.admin.indices.upgrade.post.UpgradeResponse;
 import org.elasticsearch.action.admin.indices.validate.query.ValidateQueryRequest;
 import org.elasticsearch.action.admin.indices.validate.query.ValidateQueryRequestBuilder;
 import org.elasticsearch.action.admin.indices.validate.query.ValidateQueryResponse;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateRequest;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateRequestBuilder;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateResponse;
 import org.elasticsearch.action.admin.indices.warmer.delete.DeleteWarmerRequest;
 import org.elasticsearch.action.admin.indices.warmer.delete.DeleteWarmerRequestBuilder;
 import org.elasticsearch.action.admin.indices.warmer.delete.DeleteWarmerResponse;
@@ -747,27 +744,6 @@ public interface IndicesAdminClient extends ElasticsearchClient {
     ValidateQueryRequestBuilder prepareValidateQuery(String... indices);
 
     /**
-     * Return the rendered search request for a given search template.
-     *
-     * @param request The request
-     * @return The result future
-     */
-    ActionFuture<RenderSearchTemplateResponse> renderSearchTemplate(RenderSearchTemplateRequest request);
-
-    /**
-     * Return the rendered search request for a given search template.
-     *
-     * @param request  The request
-     * @param listener A listener to be notified of the result
-     */
-    void renderSearchTemplate(RenderSearchTemplateRequest request, ActionListener<RenderSearchTemplateResponse> listener);
-
-    /**
-     * Return the rendered search request for a given search template.
-     */
-    RenderSearchTemplateRequestBuilder prepareRenderSearchTemplate();
-
-    /**
      * Puts an index search warmer to be applies when applicable.
      */
     ActionFuture<PutWarmerResponse> putWarmer(PutWarmerRequest request);
diff --git a/core/src/main/java/org/elasticsearch/client/node/NodeClient.java b/core/src/main/java/org/elasticsearch/client/node/NodeClient.java
index 1110839..deb3e5b 100644
--- a/core/src/main/java/org/elasticsearch/client/node/NodeClient.java
+++ b/core/src/main/java/org/elasticsearch/client/node/NodeClient.java
@@ -30,19 +30,17 @@ import org.elasticsearch.threadpool.ThreadPool;
 
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  *
  */
 public class NodeClient extends AbstractClient {
 
-    private final Map<GenericAction, TransportAction> actions;
+    private final ImmutableMap<GenericAction, TransportAction> actions;
 
     @Inject
     public NodeClient(Settings settings, ThreadPool threadPool, Headers headers, Map<GenericAction, TransportAction> actions) {
         super(settings, threadPool, headers);
-        this.actions = unmodifiableMap(actions);
+        this.actions = ImmutableMap.copyOf(actions);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/client/support/AbstractClient.java b/core/src/main/java/org/elasticsearch/client/support/AbstractClient.java
index f9abf2f..3fa5d78 100644
--- a/core/src/main/java/org/elasticsearch/client/support/AbstractClient.java
+++ b/core/src/main/java/org/elasticsearch/client/support/AbstractClient.java
@@ -208,10 +208,10 @@ import org.elasticsearch.action.admin.indices.validate.query.ValidateQueryAction
 import org.elasticsearch.action.admin.indices.validate.query.ValidateQueryRequest;
 import org.elasticsearch.action.admin.indices.validate.query.ValidateQueryRequestBuilder;
 import org.elasticsearch.action.admin.indices.validate.query.ValidateQueryResponse;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateAction;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateRequest;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateRequestBuilder;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateResponse;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateAction;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateRequest;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateRequestBuilder;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateResponse;
 import org.elasticsearch.action.admin.indices.warmer.delete.DeleteWarmerAction;
 import org.elasticsearch.action.admin.indices.warmer.delete.DeleteWarmerRequest;
 import org.elasticsearch.action.admin.indices.warmer.delete.DeleteWarmerRequestBuilder;
@@ -1142,6 +1142,21 @@ public abstract class AbstractClient extends AbstractComponent implements Client
         public SnapshotsStatusRequestBuilder prepareSnapshotStatus() {
             return new SnapshotsStatusRequestBuilder(this, SnapshotsStatusAction.INSTANCE);
         }
+
+        @Override
+        public ActionFuture<RenderSearchTemplateResponse> renderSearchTemplate(final RenderSearchTemplateRequest request) {
+            return execute(RenderSearchTemplateAction.INSTANCE, request);
+        }
+
+        @Override
+        public void renderSearchTemplate(final RenderSearchTemplateRequest request, final ActionListener<RenderSearchTemplateResponse> listener) {
+            execute(RenderSearchTemplateAction.INSTANCE, request, listener);
+        }
+
+        @Override
+        public RenderSearchTemplateRequestBuilder prepareRenderSearchTemplate() {
+            return new RenderSearchTemplateRequestBuilder(this, RenderSearchTemplateAction.INSTANCE);
+        }
     }
 
     static class IndicesAdmin implements IndicesAdminClient {
@@ -1618,21 +1633,6 @@ public abstract class AbstractClient extends AbstractComponent implements Client
         }
 
         @Override
-        public ActionFuture<RenderSearchTemplateResponse> renderSearchTemplate(final RenderSearchTemplateRequest request) {
-            return execute(RenderSearchTemplateAction.INSTANCE, request);
-        }
-
-        @Override
-        public void renderSearchTemplate(final RenderSearchTemplateRequest request, final ActionListener<RenderSearchTemplateResponse> listener) {
-            execute(RenderSearchTemplateAction.INSTANCE, request, listener);
-        }
-
-        @Override
-        public RenderSearchTemplateRequestBuilder prepareRenderSearchTemplate() {
-            return new RenderSearchTemplateRequestBuilder(this, RenderSearchTemplateAction.INSTANCE);
-        }
-
-        @Override
         public ActionFuture<PutWarmerResponse> putWarmer(PutWarmerRequest request) {
             return execute(PutWarmerAction.INSTANCE, request);
         }
diff --git a/core/src/main/java/org/elasticsearch/client/transport/support/TransportProxyClient.java b/core/src/main/java/org/elasticsearch/client/transport/support/TransportProxyClient.java
index 9008764..89b3a04 100644
--- a/core/src/main/java/org/elasticsearch/client/transport/support/TransportProxyClient.java
+++ b/core/src/main/java/org/elasticsearch/client/transport/support/TransportProxyClient.java
@@ -19,42 +19,35 @@
 
 package org.elasticsearch.client.transport.support;
 
-import org.elasticsearch.action.Action;
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionRequestBuilder;
-import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.action.GenericAction;
-import org.elasticsearch.action.TransportActionNodeProxy;
+import com.google.common.collect.ImmutableMap;
+import org.elasticsearch.action.*;
 import org.elasticsearch.client.transport.TransportClientNodesService;
 import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.transport.TransportService;
 
-import java.util.HashMap;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  *
  */
 public class TransportProxyClient {
 
     private final TransportClientNodesService nodesService;
-    private final Map<Action, TransportActionNodeProxy> proxies;
+    private final ImmutableMap<Action, TransportActionNodeProxy> proxies;
 
     @Inject
     public TransportProxyClient(Settings settings, TransportService transportService, TransportClientNodesService nodesService, Map<String, GenericAction> actions) {
         this.nodesService = nodesService;
-        Map<Action, TransportActionNodeProxy> proxies = new HashMap<>();
+        MapBuilder<Action, TransportActionNodeProxy> actionsBuilder = new MapBuilder<>();
         for (GenericAction action : actions.values()) {
             if (action instanceof Action) {
-                proxies.put((Action) action, new TransportActionNodeProxy(settings, action, transportService));
+                actionsBuilder.put((Action) action, new TransportActionNodeProxy(settings, action, transportService));
             }
         }
-        this.proxies = unmodifiableMap(proxies);
+        this.proxies = actionsBuilder.immutableMap();
     }
 
     public <Request extends ActionRequest, Response extends ActionResponse, RequestBuilder extends ActionRequestBuilder<Request, Response, RequestBuilder>> void execute(final Action<Request, Response, RequestBuilder> action, final Request request, ActionListener<Response> listener) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterInfo.java b/core/src/main/java/org/elasticsearch/cluster/ClusterInfo.java
index 265ab0f..f10a40b 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterInfo.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterInfo.java
@@ -20,7 +20,9 @@
 package org.elasticsearch.cluster;
 
 import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
+
+import java.util.Collections;
+import java.util.Map;
 
 /**
  * ClusterInfo is an object representing a map of nodes to {@link DiskUsage}
@@ -29,14 +31,15 @@ import org.elasticsearch.common.collect.ImmutableOpenMap;
  * for the key used in the shardSizes map
  */
 public class ClusterInfo {
-    private final ImmutableOpenMap<String, DiskUsage> leastAvailableSpaceUsage;
-    private final ImmutableOpenMap<String, DiskUsage> mostAvailableSpaceUsage;
-    final ImmutableOpenMap<String, Long> shardSizes;
+
+    private final Map<String, DiskUsage> leastAvailableSpaceUsage;
+    private final Map<String, DiskUsage> mostAvailableSpaceUsage;
+    final Map<String, Long> shardSizes;
     public static final ClusterInfo EMPTY = new ClusterInfo();
-    private final ImmutableOpenMap<ShardRouting, String> routingToDataPath;
+    private final Map<ShardRouting, String> routingToDataPath;
 
     protected ClusterInfo() {
-       this(ImmutableOpenMap.of(), ImmutableOpenMap.of(), ImmutableOpenMap.of(), ImmutableOpenMap.of());
+       this(Collections.EMPTY_MAP, Collections.EMPTY_MAP, Collections.EMPTY_MAP, Collections.EMPTY_MAP);
     }
 
     /**
@@ -48,9 +51,7 @@ public class ClusterInfo {
      * @param routingToDataPath the shard routing to datapath mapping
      * @see #shardIdentifierFromRouting
      */
-    public ClusterInfo(ImmutableOpenMap<String, DiskUsage> leastAvailableSpaceUsage,
-            ImmutableOpenMap<String, DiskUsage> mostAvailableSpaceUsage, ImmutableOpenMap<String, Long> shardSizes,
-            ImmutableOpenMap<ShardRouting, String> routingToDataPath) {
+    public ClusterInfo(final Map<String, DiskUsage> leastAvailableSpaceUsage, final Map<String, DiskUsage> mostAvailableSpaceUsage, final Map<String, Long> shardSizes, Map<ShardRouting, String> routingToDataPath) {
         this.leastAvailableSpaceUsage = leastAvailableSpaceUsage;
         this.shardSizes = shardSizes;
         this.mostAvailableSpaceUsage = mostAvailableSpaceUsage;
@@ -60,14 +61,14 @@ public class ClusterInfo {
     /**
      * Returns a node id to disk usage mapping for the path that has the least available space on the node.
      */
-    public ImmutableOpenMap<String, DiskUsage> getNodeLeastAvailableDiskUsages() {
+    public Map<String, DiskUsage> getNodeLeastAvailableDiskUsages() {
         return this.leastAvailableSpaceUsage;
     }
 
     /**
      * Returns a node id to disk usage mapping for the path that has the most available space on the node.
      */
-    public ImmutableOpenMap<String, DiskUsage> getNodeMostAvailableDiskUsages() {
+    public Map<String, DiskUsage> getNodeMostAvailableDiskUsages() {
         return this.mostAvailableSpaceUsage;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterState.java b/core/src/main/java/org/elasticsearch/cluster/ClusterState.java
index b1bdf52..8167ecc 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterState.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterState.java
@@ -389,9 +389,9 @@ public class ClusterState implements ToXContent, Diffable<ClusterState> {
 
             if (!blocks().indices().isEmpty()) {
                 builder.startObject("indices");
-                for (ObjectObjectCursor<String, Set<ClusterBlock>> entry : blocks().indices()) {
-                    builder.startObject(entry.key);
-                    for (ClusterBlock block : entry.value) {
+                for (Map.Entry<String, Set<ClusterBlock>> entry : blocks().indices().entrySet()) {
+                    builder.startObject(entry.getKey());
+                    for (ClusterBlock block : entry.getValue()) {
                         block.toXContent(builder, params);
                     }
                     builder.endObject();
@@ -591,6 +591,10 @@ public class ClusterState implements ToXContent, Diffable<ClusterState> {
             return this;
         }
 
+        public Builder routingTable(RoutingTable.Builder routingTable) {
+            return routingTable(routingTable.build());
+        }
+
         public Builder routingResult(RoutingAllocation.Result routingResult) {
             this.routingTable = routingResult.routingTable();
             return this;
diff --git a/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java b/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java
index 039868d..019e245 100644
--- a/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java
@@ -33,7 +33,6 @@ import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.logging.ESLogger;
@@ -45,7 +44,10 @@ import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.ReceiveTimeoutTransportException;
 
+import java.util.Collections;
+import java.util.HashMap;
 import java.util.List;
+import java.util.Map;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.TimeUnit;
@@ -68,10 +70,10 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
 
     private volatile TimeValue updateFrequency;
 
-    private volatile ImmutableOpenMap<String, DiskUsage> leastAvailableSpaceUsages;
-    private volatile ImmutableOpenMap<String, DiskUsage> mostAvailableSpaceUsages;
-    private volatile ImmutableOpenMap<ShardRouting, String> shardRoutingToDataPath;
-    private volatile ImmutableOpenMap<String, Long> shardSizes;
+    private volatile Map<String, DiskUsage> leastAvailableSpaceUsages;
+    private volatile Map<String, DiskUsage> mostAvailableSpaceUsages;
+    private volatile Map<ShardRouting, String> shardRoutingToDataPath;
+    private volatile Map<String, Long> shardSizes;
     private volatile boolean isMaster = false;
     private volatile boolean enabled;
     private volatile TimeValue fetchTimeout;
@@ -87,10 +89,10 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
                                       TransportIndicesStatsAction transportIndicesStatsAction, ClusterService clusterService,
                                       ThreadPool threadPool) {
         super(settings);
-        this.leastAvailableSpaceUsages = ImmutableOpenMap.of();
-        this.mostAvailableSpaceUsages = ImmutableOpenMap.of();
-        this.shardRoutingToDataPath = ImmutableOpenMap.of();
-        this.shardSizes = ImmutableOpenMap.of();
+        this.leastAvailableSpaceUsages = Collections.emptyMap();
+        this.mostAvailableSpaceUsages = Collections.emptyMap();
+        this.shardRoutingToDataPath = Collections.emptyMap();
+        this.shardSizes = Collections.emptyMap();
         this.transportNodesStatsAction = transportNodesStatsAction;
         this.transportIndicesStatsAction = transportIndicesStatsAction;
         this.clusterService = clusterService;
@@ -196,14 +198,14 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
                         logger.trace("Removing node from cluster info: {}", removedNode.getId());
                     }
                     if (leastAvailableSpaceUsages.containsKey(removedNode.getId())) {
-                        ImmutableOpenMap.Builder<String, DiskUsage> newMaxUsages = ImmutableOpenMap.builder(leastAvailableSpaceUsages);
+                        Map<String, DiskUsage> newMaxUsages = new HashMap<>(leastAvailableSpaceUsages);
                         newMaxUsages.remove(removedNode.getId());
-                        leastAvailableSpaceUsages = newMaxUsages.build();
+                        leastAvailableSpaceUsages = Collections.unmodifiableMap(newMaxUsages);
                     }
                     if (mostAvailableSpaceUsages.containsKey(removedNode.getId())) {
-                        ImmutableOpenMap.Builder<String, DiskUsage> newMinUsages = ImmutableOpenMap.builder(mostAvailableSpaceUsages);
+                        Map<String, DiskUsage> newMinUsages = new HashMap<>(mostAvailableSpaceUsages);
                         newMinUsages.remove(removedNode.getId());
-                        mostAvailableSpaceUsages = newMinUsages.build();
+                        mostAvailableSpaceUsages = Collections.unmodifiableMap(newMinUsages);
                     }
                 }
             }
@@ -307,11 +309,11 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
         final CountDownLatch nodeLatch = updateNodeStats(new ActionListener<NodesStatsResponse>() {
             @Override
             public void onResponse(NodesStatsResponse nodeStatses) {
-                ImmutableOpenMap.Builder<String, DiskUsage> newLeastAvaiableUsages = ImmutableOpenMap.builder();
-                ImmutableOpenMap.Builder<String, DiskUsage> newMostAvaiableUsages = ImmutableOpenMap.builder();
+                Map<String, DiskUsage> newLeastAvaiableUsages = new HashMap<>();
+                Map<String, DiskUsage> newMostAvaiableUsages = new HashMap<>();
                 fillDiskUsagePerNode(logger, nodeStatses.getNodes(), newLeastAvaiableUsages, newMostAvaiableUsages);
-                leastAvailableSpaceUsages = newLeastAvaiableUsages.build();
-                mostAvailableSpaceUsages = newMostAvaiableUsages.build();
+                leastAvailableSpaceUsages = Collections.unmodifiableMap(newLeastAvaiableUsages);
+                mostAvailableSpaceUsages = Collections.unmodifiableMap(newMostAvaiableUsages);
             }
 
             @Override
@@ -327,8 +329,8 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
                         logger.warn("Failed to execute NodeStatsAction for ClusterInfoUpdateJob", e);
                     }
                     // we empty the usages list, to be safe - we don't know what's going on.
-                    leastAvailableSpaceUsages = ImmutableOpenMap.of();
-                    mostAvailableSpaceUsages = ImmutableOpenMap.of();
+                    leastAvailableSpaceUsages = Collections.emptyMap();
+                    mostAvailableSpaceUsages = Collections.emptyMap();
                 }
             }
         });
@@ -337,11 +339,11 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
             @Override
             public void onResponse(IndicesStatsResponse indicesStatsResponse) {
                 ShardStats[] stats = indicesStatsResponse.getShards();
-                ImmutableOpenMap.Builder<String, Long> newShardSizes = ImmutableOpenMap.builder();
-                ImmutableOpenMap.Builder<ShardRouting, String> newShardRoutingToDataPath = ImmutableOpenMap.builder();
+                final HashMap<String, Long> newShardSizes = new HashMap<>();
+                final HashMap<ShardRouting, String> newShardRoutingToDataPath = new HashMap<>();
                 buildShardLevelInfo(logger, stats, newShardSizes, newShardRoutingToDataPath);
-                shardSizes = newShardSizes.build();
-                shardRoutingToDataPath = newShardRoutingToDataPath.build();
+                shardSizes = Collections.unmodifiableMap(newShardSizes);
+                shardRoutingToDataPath = Collections.unmodifiableMap(newShardRoutingToDataPath);
             }
 
             @Override
@@ -357,8 +359,8 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
                         logger.warn("Failed to execute IndicesStatsAction for ClusterInfoUpdateJob", e);
                     }
                     // we empty the usages list, to be safe - we don't know what's going on.
-                    shardSizes = ImmutableOpenMap.of();
-                    shardRoutingToDataPath = ImmutableOpenMap.of();
+                    shardSizes = Collections.emptyMap();
+                    shardRoutingToDataPath = Collections.emptyMap();
                 }
             }
         });
@@ -387,8 +389,7 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
         return clusterInfo;
     }
 
-    static void buildShardLevelInfo(ESLogger logger, ShardStats[] stats, ImmutableOpenMap.Builder<String, Long> newShardSizes,
-            ImmutableOpenMap.Builder<ShardRouting, String> newShardRoutingToDataPath) {
+    static void buildShardLevelInfo(ESLogger logger, ShardStats[] stats, HashMap<String, Long> newShardSizes, HashMap<ShardRouting, String> newShardRoutingToDataPath) {
         for (ShardStats s : stats) {
             newShardRoutingToDataPath.put(s.getShardRouting(), s.getDataPath());
             long size = s.getStats().getStore().sizeInBytes();
@@ -400,9 +401,7 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
         }
     }
 
-    static void fillDiskUsagePerNode(ESLogger logger, NodeStats[] nodeStatsArray,
-            ImmutableOpenMap.Builder<String, DiskUsage> newLeastAvaiableUsages,
-            ImmutableOpenMap.Builder<String, DiskUsage> newMostAvaiableUsages) {
+    static void fillDiskUsagePerNode(ESLogger logger, NodeStats[] nodeStatsArray, Map<String, DiskUsage> newLeastAvaiableUsages, Map<String, DiskUsage> newMostAvaiableUsages) {
         for (NodeStats nodeStats : nodeStatsArray) {
             if (nodeStats.getFs() == null) {
                 logger.warn("Unable to retrieve node FS stats for {}", nodeStats.getNode().name());
diff --git a/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java b/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java
index dd7eb9f..82ba28d 100644
--- a/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java
+++ b/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java
@@ -19,11 +19,9 @@
 
 package org.elasticsearch.cluster;
 
-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.ClusterState.Custom;
 import org.elasticsearch.cluster.metadata.SnapshotId;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.ToXContent;
@@ -35,6 +33,7 @@ import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
+import java.util.Map;
 
 /**
  * Meta data about restore processes that are currently executing
@@ -113,7 +112,7 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
     public static class Entry {
         private final State state;
         private final SnapshotId snapshotId;
-        private final ImmutableOpenMap<ShardId, ShardRestoreStatus> shards;
+        private final Map<ShardId, ShardRestoreStatus> shards;
         private final List<String> indices;
 
         /**
@@ -122,14 +121,14 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
          * @param snapshotId snapshot id
          * @param state      current state of the restore process
          * @param indices    list of indices being restored
-         * @param shards     map of shards being restored to their current restore status
+         * @param shards     list of shards being restored and thier current restore status
          */
-        public Entry(SnapshotId snapshotId, State state, List<String> indices, ImmutableOpenMap<ShardId, ShardRestoreStatus> shards) {
+        public Entry(SnapshotId snapshotId, State state, List<String> indices, ImmutableMap<ShardId, ShardRestoreStatus> shards) {
             this.snapshotId = snapshotId;
             this.state = state;
             this.indices = indices;
             if (shards == null) {
-                this.shards = ImmutableOpenMap.of();
+                this.shards = ImmutableMap.of();
             } else {
                 this.shards = shards;
             }
@@ -149,7 +148,7 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
          *
          * @return list of shards
          */
-        public ImmutableOpenMap<ShardId, ShardRestoreStatus> shards() {
+        public Map<ShardId, ShardRestoreStatus> shards() {
             return this.shards;
         }
 
@@ -417,7 +416,7 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
             for (int j = 0; j < indices; j++) {
                 indexBuilder.add(in.readString());
             }
-            ImmutableOpenMap.Builder<ShardId, ShardRestoreStatus> builder = ImmutableOpenMap.builder();
+            ImmutableMap.Builder<ShardId, ShardRestoreStatus> builder = ImmutableMap.<ShardId, ShardRestoreStatus>builder();
             int shards = in.readVInt();
             for (int j = 0; j < shards; j++) {
                 ShardId shardId = ShardId.readShardId(in);
@@ -443,9 +442,9 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
                 out.writeString(index);
             }
             out.writeVInt(entry.shards().size());
-            for (ObjectObjectCursor<ShardId, ShardRestoreStatus> shardEntry : entry.shards()) {
-                shardEntry.key.writeTo(out);
-                shardEntry.value.writeTo(out);
+            for (Map.Entry<ShardId, ShardRestoreStatus> shardEntry : entry.shards().entrySet()) {
+                shardEntry.getKey().writeTo(out);
+                shardEntry.getValue().writeTo(out);
             }
         }
     }
@@ -484,9 +483,9 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
         builder.endArray();
         builder.startArray("shards");
         {
-            for (ObjectObjectCursor<ShardId, ShardRestoreStatus> shardEntry : entry.shards) {
-                ShardId shardId = shardEntry.key;
-                ShardRestoreStatus status = shardEntry.value;
+            for (Map.Entry<ShardId, ShardRestoreStatus> shardEntry : entry.shards.entrySet()) {
+                ShardId shardId = shardEntry.getKey();
+                ShardRestoreStatus status = shardEntry.getValue();
                 builder.startObject();
                 {
                     builder.field("index", shardId.getIndex());
diff --git a/core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java b/core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java
index 821ab3c..83c663a 100644
--- a/core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java
+++ b/core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java
@@ -19,13 +19,9 @@
 
 package org.elasticsearch.cluster;
 
-import com.carrotsearch.hppc.ObjectContainer;
-import com.carrotsearch.hppc.cursors.ObjectCursor;
-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.ClusterState.Custom;
 import org.elasticsearch.cluster.metadata.SnapshotId;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.ToXContent;
@@ -36,11 +32,14 @@ import org.elasticsearch.index.shard.ShardId;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collection;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * Meta data about snapshots that are currently executing
  */
@@ -70,31 +69,31 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
         private final State state;
         private final SnapshotId snapshotId;
         private final boolean includeGlobalState;
-        private final ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards;
+        private final Map<ShardId, ShardSnapshotStatus> shards;
         private final List<String> indices;
-        private final ImmutableOpenMap<String, List<ShardId>> waitingIndices;
+        private final Map<String, List<ShardId>> waitingIndices;
         private final long startTime;
 
-        public Entry(SnapshotId snapshotId, boolean includeGlobalState, State state, List<String> indices, long startTime, ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards) {
+        public Entry(SnapshotId snapshotId, boolean includeGlobalState, State state, List<String> indices, long startTime, Map<ShardId, ShardSnapshotStatus> shards) {
             this.state = state;
             this.snapshotId = snapshotId;
             this.includeGlobalState = includeGlobalState;
             this.indices = indices;
             this.startTime = startTime;
             if (shards == null) {
-                this.shards = ImmutableOpenMap.of();
-                this.waitingIndices = ImmutableOpenMap.of();
+                this.shards = ImmutableMap.of();
+                this.waitingIndices = ImmutableMap.of();
             } else {
-                this.shards = shards;
+                this.shards = unmodifiableMap(shards);
                 this.waitingIndices = findWaitingIndices(shards);
             }
         }
 
-        public Entry(Entry entry, State state, ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards) {
+        public Entry(Entry entry, State state, Map<ShardId, ShardSnapshotStatus> shards) {
             this(entry.snapshotId, entry.includeGlobalState, state, entry.indices, entry.startTime, shards);
         }
 
-        public Entry(Entry entry, ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards) {
+        public Entry(Entry entry, Map<ShardId, ShardSnapshotStatus> shards) {
             this(entry, entry.state, shards);
         }
 
@@ -102,7 +101,7 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
             return this.snapshotId;
         }
 
-        public ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards() {
+        public Map<ShardId, ShardSnapshotStatus> shards() {
             return this.shards;
         }
 
@@ -114,7 +113,7 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
             return indices;
         }
 
-        public ImmutableOpenMap<String, List<ShardId>> waitingIndices() {
+        public Map<String, List<ShardId>> waitingIndices() {
             return waitingIndices;
         }
 
@@ -156,26 +155,28 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
             return result;
         }
 
-        private ImmutableOpenMap<String, List<ShardId>> findWaitingIndices(ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards) {
+        private ImmutableMap<String, List<ShardId>> findWaitingIndices(Map<ShardId, ShardSnapshotStatus> shards) {
             Map<String, List<ShardId>> waitingIndicesMap = new HashMap<>();
-            for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> entry : shards) {
-                if (entry.value.state() == State.WAITING) {
-                    List<ShardId> waitingShards = waitingIndicesMap.get(entry.key.getIndex());
+            for (ImmutableMap.Entry<ShardId, ShardSnapshotStatus> entry : shards.entrySet()) {
+                if (entry.getValue().state() == State.WAITING) {
+                    List<ShardId> waitingShards = waitingIndicesMap.get(entry.getKey().getIndex());
                     if (waitingShards == null) {
                         waitingShards = new ArrayList<>();
-                        waitingIndicesMap.put(entry.key.getIndex(), waitingShards);
+                        waitingIndicesMap.put(entry.getKey().getIndex(), waitingShards);
                     }
-                    waitingShards.add(entry.key);
+                    waitingShards.add(entry.getKey());
                 }
             }
-            if (waitingIndicesMap.isEmpty()) {
-                return ImmutableOpenMap.of();
-            }
-            ImmutableOpenMap.Builder<String, List<ShardId>> waitingIndicesBuilder = ImmutableOpenMap.builder();
-            for (Map.Entry<String, List<ShardId>> entry : waitingIndicesMap.entrySet()) {
-                waitingIndicesBuilder.put(entry.getKey(), Collections.unmodifiableList(entry.getValue()));
+            if (!waitingIndicesMap.isEmpty()) {
+                ImmutableMap.Builder<String, List<ShardId>> waitingIndicesBuilder = ImmutableMap.builder();
+                for (Map.Entry<String, List<ShardId>> entry : waitingIndicesMap.entrySet()) {
+                    waitingIndicesBuilder.put(entry.getKey(), Collections.unmodifiableList(entry.getValue()));
+                }
+                return waitingIndicesBuilder.build();
+            } else {
+                return ImmutableMap.of();
             }
-            return waitingIndicesBuilder.build();
+
         }
 
     }
@@ -186,9 +187,9 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
      * @param shards list of shard statuses
      * @return true if all shards have completed (either successfully or failed), false otherwise
      */
-    public static boolean completed(ObjectContainer<ShardSnapshotStatus> shards) {
-        for (ObjectCursor<ShardSnapshotStatus> status : shards) {
-            if (status.value.state().completed() == false) {
+    public static boolean completed(Collection<ShardSnapshotStatus> shards) {
+        for (ShardSnapshotStatus status : shards) {
+            if (status.state().completed() == false) {
                 return false;
             }
         }
@@ -368,7 +369,7 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
                 indexBuilder.add(in.readString());
             }
             long startTime = in.readLong();
-            ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> builder = ImmutableOpenMap.builder();
+            ImmutableMap.Builder<ShardId, ShardSnapshotStatus> builder = ImmutableMap.builder();
             int shards = in.readVInt();
             for (int j = 0; j < shards; j++) {
                 ShardId shardId = ShardId.readShardId(in);
@@ -394,10 +395,10 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
             }
             out.writeLong(entry.startTime());
             out.writeVInt(entry.shards().size());
-            for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : entry.shards()) {
-                shardEntry.key.writeTo(out);
-                out.writeOptionalString(shardEntry.value.nodeId());
-                out.writeByte(shardEntry.value.state().value());
+            for (Map.Entry<ShardId, ShardSnapshotStatus> shardEntry : entry.shards().entrySet()) {
+                shardEntry.getKey().writeTo(out);
+                out.writeOptionalString(shardEntry.getValue().nodeId());
+                out.writeByte(shardEntry.getValue().state().value());
             }
         }
     }
@@ -443,9 +444,9 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
         builder.timeValueField(Fields.START_TIME_MILLIS, Fields.START_TIME, entry.startTime());
         builder.startArray(Fields.SHARDS);
         {
-            for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : entry.shards) {
-                ShardId shardId = shardEntry.key;
-                ShardSnapshotStatus status = shardEntry.value;
+            for (Map.Entry<ShardId, ShardSnapshotStatus> shardEntry : entry.shards.entrySet()) {
+                ShardId shardId = shardEntry.getKey();
+                ShardSnapshotStatus status = shardEntry.getValue();
                 builder.startObject();
                 {
                     builder.field(Fields.INDEX, shardId.getIndex());
diff --git a/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java b/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java
index b13c799..e3925aa 100644
--- a/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java
+++ b/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java
@@ -73,7 +73,7 @@ public class MappingUpdatedAction extends AbstractComponent {
             throw new IllegalArgumentException("_default_ mapping should not be updated");
         }
         return client.preparePutMapping(index).setType(type).setSource(mappingUpdate.toString())
-            .setMasterNodeTimeout(timeout).setTimeout(timeout);
+                .setMasterNodeTimeout(timeout).setTimeout(timeout);
     }
 
     public void updateMappingOnMaster(String index, String type, Mapping mappingUpdate, final TimeValue timeout, final MappingUpdateListener listener) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/block/ClusterBlocks.java b/core/src/main/java/org/elasticsearch/cluster/block/ClusterBlocks.java
index cfe88af..ab5609c 100644
--- a/core/src/main/java/org/elasticsearch/cluster/block/ClusterBlocks.java
+++ b/core/src/main/java/org/elasticsearch/cluster/block/ClusterBlocks.java
@@ -19,12 +19,11 @@
 
 package org.elasticsearch.cluster.block;
 
-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+import com.google.common.collect.ImmutableMap;
 
 import org.elasticsearch.cluster.AbstractDiffable;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaDataIndexStateService;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.rest.RestStatus;
@@ -38,6 +37,7 @@ import java.util.function.Function;
 import java.util.function.Predicate;
 import java.util.stream.Stream;
 
+import static java.util.Collections.emptyMap;
 import static java.util.Collections.emptySet;
 import static java.util.Collections.unmodifiableSet;
 import static java.util.stream.Collectors.toSet;
@@ -47,17 +47,17 @@ import static java.util.stream.Stream.concat;
  * Represents current cluster level blocks to block dirty operations done against the cluster.
  */
 public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
-    public static final ClusterBlocks EMPTY_CLUSTER_BLOCK = new ClusterBlocks(emptySet(), ImmutableOpenMap.of());
+    public static final ClusterBlocks EMPTY_CLUSTER_BLOCK = new ClusterBlocks(emptySet(), emptyMap());
 
     public static final ClusterBlocks PROTO = EMPTY_CLUSTER_BLOCK;
 
     private final Set<ClusterBlock> global;
 
-    private final ImmutableOpenMap<String, Set<ClusterBlock>> indicesBlocks;
+    private final Map<String, Set<ClusterBlock>> indicesBlocks;
 
     private final ImmutableLevelHolder[] levelHolders;
 
-    ClusterBlocks(Set<ClusterBlock> global, ImmutableOpenMap<String, Set<ClusterBlock>> indicesBlocks) {
+    ClusterBlocks(Set<ClusterBlock> global, Map<String, Set<ClusterBlock>> indicesBlocks) {
         this.global = global;
         this.indicesBlocks = indicesBlocks;
 
@@ -68,9 +68,9 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
                     .filter(containsLevel)
                     .collect(toSet()));
 
-            ImmutableOpenMap.Builder<String, Set<ClusterBlock>> indicesBuilder = ImmutableOpenMap.builder();
-            for (ObjectObjectCursor<String, Set<ClusterBlock>> entry : indicesBlocks) {
-                indicesBuilder.put(entry.key, unmodifiableSet(entry.value.stream()
+            ImmutableMap.Builder<String, Set<ClusterBlock>> indicesBuilder = ImmutableMap.builder();
+            for (Map.Entry<String, Set<ClusterBlock>> entry : indicesBlocks.entrySet()) {
+                indicesBuilder.put(entry.getKey(), unmodifiableSet(entry.getValue().stream()
                         .filter(containsLevel)
                         .collect(toSet())));
             }
@@ -83,7 +83,7 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
         return global;
     }
 
-    public ImmutableOpenMap<String, Set<ClusterBlock>> indices() {
+    public Map<String, Set<ClusterBlock>> indices() {
         return indicesBlocks;
     }
 
@@ -91,7 +91,7 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
         return levelHolders[level.id()].global();
     }
 
-    public ImmutableOpenMap<String, Set<ClusterBlock>> indices(ClusterBlockLevel level) {
+    public Map<String, Set<ClusterBlock>> indices(ClusterBlockLevel level) {
         return levelHolders[level.id()].indices();
     }
 
@@ -203,9 +203,9 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
     public void writeTo(StreamOutput out) throws IOException {
         writeBlockSet(global, out);
         out.writeVInt(indicesBlocks.size());
-        for (ObjectObjectCursor<String, Set<ClusterBlock>> entry : indicesBlocks) {
-            out.writeString(entry.key);
-            writeBlockSet(entry.value, out);
+        for (Map.Entry<String, Set<ClusterBlock>> entry : indicesBlocks.entrySet()) {
+            out.writeString(entry.getKey());
+            writeBlockSet(entry.getValue(), out);
         }
     }
 
@@ -219,8 +219,8 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
     @Override
     public ClusterBlocks readFrom(StreamInput in) throws IOException {
         Set<ClusterBlock> global = readBlockSet(in);
+        ImmutableMap.Builder<String, Set<ClusterBlock>> indicesBuilder = ImmutableMap.builder();
         int size = in.readVInt();
-        ImmutableOpenMap.Builder<String, Set<ClusterBlock>> indicesBuilder = ImmutableOpenMap.builder(size);
         for (int j = 0; j < size; j++) {
             indicesBuilder.put(in.readString().intern(), readBlockSet(in));
         }
@@ -238,12 +238,12 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
 
     static class ImmutableLevelHolder {
 
-        static final ImmutableLevelHolder EMPTY = new ImmutableLevelHolder(emptySet(), ImmutableOpenMap.of());
+        static final ImmutableLevelHolder EMPTY = new ImmutableLevelHolder(emptySet(), ImmutableMap.of());
 
         private final Set<ClusterBlock> global;
-        private final ImmutableOpenMap<String, Set<ClusterBlock>> indices;
+        private final ImmutableMap<String, Set<ClusterBlock>> indices;
 
-        ImmutableLevelHolder(Set<ClusterBlock> global, ImmutableOpenMap<String, Set<ClusterBlock>> indices) {
+        ImmutableLevelHolder(Set<ClusterBlock> global, ImmutableMap<String, Set<ClusterBlock>> indices) {
             this.global = global;
             this.indices = indices;
         }
@@ -252,7 +252,7 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
             return global;
         }
 
-        public ImmutableOpenMap<String, Set<ClusterBlock>> indices() {
+        public ImmutableMap<String, Set<ClusterBlock>> indices() {
             return indices;
         }
     }
@@ -272,11 +272,11 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
 
         public Builder blocks(ClusterBlocks blocks) {
             global.addAll(blocks.global());
-            for (ObjectObjectCursor<String, Set<ClusterBlock>> entry : blocks.indices()) {
-                if (!indices.containsKey(entry.key)) {
-                    indices.put(entry.key, new HashSet<>());
+            for (Map.Entry<String, Set<ClusterBlock>> entry : blocks.indices().entrySet()) {
+                if (!indices.containsKey(entry.getKey())) {
+                    indices.put(entry.getKey(), new HashSet<>());
                 }
-                indices.get(entry.key).addAll(entry.value);
+                indices.get(entry.getKey()).addAll(entry.getValue());
             }
             return this;
         }
@@ -339,7 +339,7 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
 
         public ClusterBlocks build() {
             // We copy the block sets here in case of the builder is modified after build is called
-            ImmutableOpenMap.Builder<String, Set<ClusterBlock>> indicesBuilder = ImmutableOpenMap.builder(indices.size());
+            ImmutableMap.Builder<String, Set<ClusterBlock>> indicesBuilder = ImmutableMap.builder();
             for (Map.Entry<String, Set<ClusterBlock>> entry : indices.entrySet()) {
                 indicesBuilder.put(entry.getKey(), unmodifiableSet(new HashSet<>(entry.getValue())));
             }
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
index 6ea1d0e..9d11017 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
@@ -21,7 +21,6 @@ package org.elasticsearch.cluster.metadata;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.Diff;
 import org.elasticsearch.cluster.Diffable;
@@ -29,8 +28,6 @@ import org.elasticsearch.cluster.DiffableUtils;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.node.DiscoveryNodeFilters;
-import org.elasticsearch.cluster.routing.HashFunction;
-import org.elasticsearch.cluster.routing.Murmur3HashFunction;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
@@ -167,16 +164,12 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
     public static final String SETTING_PRIORITY = "index.priority";
     public static final String SETTING_CREATION_DATE_STRING = "index.creation_date_string";
     public static final String SETTING_INDEX_UUID = "index.uuid";
-    public static final String SETTING_LEGACY_ROUTING_HASH_FUNCTION = "index.legacy.routing.hash.type";
-    public static final String SETTING_LEGACY_ROUTING_USE_TYPE = "index.legacy.routing.use_type";
     public static final String SETTING_DATA_PATH = "index.data_path";
     public static final String SETTING_SHARED_FS_ALLOW_RECOVERY_ON_ANY_NODE = "index.shared_filesystem.recover_on_any_node";
     public static final String INDEX_UUID_NA_VALUE = "_na_";
 
 
-    // hard-coded hash function as of 2.0
-    // older indices will read which hash function to use in their index settings
-    private static final HashFunction MURMUR3_HASH_FUNCTION = new Murmur3HashFunction();
+
 
     private final String index;
     private final long version;
@@ -200,8 +193,6 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
     private final Version indexCreatedVersion;
     private final Version indexUpgradedVersion;
     private final org.apache.lucene.util.Version minimumCompatibleLuceneVersion;
-    private final HashFunction routingHashFunction;
-    private final boolean useTypeForRouting;
 
     private IndexMetaData(String index, long version, State state, Settings settings, ImmutableOpenMap<String, MappingMetaData> mappings, ImmutableOpenMap<String, AliasMetaData> aliases, ImmutableOpenMap<String, Custom> customs) {
         if (settings.getAsInt(SETTING_NUMBER_OF_SHARDS, null) == null) {
@@ -249,23 +240,6 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
         } else {
             this.minimumCompatibleLuceneVersion = null;
         }
-        final String hashFunction = settings.get(SETTING_LEGACY_ROUTING_HASH_FUNCTION);
-        if (hashFunction == null) {
-            routingHashFunction = MURMUR3_HASH_FUNCTION;
-        } else {
-            final Class<? extends HashFunction> hashFunctionClass;
-            try {
-                hashFunctionClass = Class.forName(hashFunction).asSubclass(HashFunction.class);
-            } catch (ClassNotFoundException|NoClassDefFoundError e) {
-                throw new ElasticsearchException("failed to load custom hash function [" + hashFunction + "]", e);
-            }
-            try {
-                routingHashFunction = hashFunctionClass.newInstance();
-            } catch (InstantiationException | IllegalAccessException e) {
-                throw new IllegalStateException("Cannot instantiate hash function", e);
-            }
-        }
-        useTypeForRouting = settings.getAsBoolean(SETTING_LEGACY_ROUTING_USE_TYPE, false);
     }
 
     public String index() {
@@ -335,29 +309,6 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
         return minimumCompatibleLuceneVersion;
     }
 
-    /**
-     * Return the {@link HashFunction} that should be used for routing.
-     */
-    public HashFunction routingHashFunction() {
-        return routingHashFunction;
-    }
-
-    public HashFunction getRoutingHashFunction() {
-        return routingHashFunction();
-    }
-
-    /**
-     * Return whether routing should use the _type in addition to the _id in
-     * order to decide which shard a document should go to.
-     */
-    public boolean routingUseType() {
-        return useTypeForRouting;
-    }
-
-    public boolean getRoutingUseType() {
-        return routingUseType();
-    }
-
     public long creationDate() {
         return settings.getAsLong(SETTING_CREATION_DATE, -1l);
     }
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
index 049eadc..99ce095 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
@@ -21,7 +21,7 @@ package org.elasticsearch.cluster.metadata;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
+import java.nio.charset.StandardCharsets;
 import org.apache.lucene.util.CollectionUtil;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
@@ -76,7 +76,6 @@ import org.joda.time.DateTimeZone;
 import java.io.BufferedReader;
 import java.io.IOException;
 import java.io.UnsupportedEncodingException;
-import java.nio.charset.StandardCharsets;
 import java.nio.file.DirectoryStream;
 import java.nio.file.Files;
 import java.nio.file.Path;
@@ -463,7 +462,7 @@ public class MetaDataCreateIndexService extends AbstractComponent {
                     if (request.state() == State.OPEN) {
                         RoutingTable.Builder routingTableBuilder = RoutingTable.builder(updatedState.routingTable())
                                 .addAsNew(updatedState.metaData().index(request.index()));
-                        RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(routingTableBuilder.build()).build());
+                        RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(routingTableBuilder).build());
                         updatedState = ClusterState.builder(updatedState).routingResult(routingResult).build();
                     }
                     removalReason = "cleaning up after validating index on master";
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java
index 88e1aad..d7b2e47 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java
@@ -128,7 +128,7 @@ public class MetaDataDeleteIndexService extends AbstractComponent {
                         .build();
 
                 RoutingAllocation.Result routingResult = allocationService.reroute(
-                        ClusterState.builder(currentState).routingTable(routingTableBuilder.build()).metaData(newMetaData).build());
+                        ClusterState.builder(currentState).routingTable(routingTableBuilder).metaData(newMetaData).build());
 
                 ClusterBlocks blocks = ClusterBlocks.builder().blocks(currentState.blocks()).removeIndexBlocks(request.index).build();
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java
index e4452e4..b5b3cb6 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java
@@ -124,7 +124,7 @@ public class MetaDataIndexStateService extends AbstractComponent {
                     rtBuilder.remove(index);
                 }
 
-                RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(rtBuilder.build()).build());
+                RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(rtBuilder).build());
                 //no explicit wait for other nodes needed as we use AckedClusterStateUpdateTask
                 return ClusterState.builder(updatedState).routingResult(routingResult).build();
             }
@@ -181,7 +181,7 @@ public class MetaDataIndexStateService extends AbstractComponent {
                     rtBuilder.addAsFromCloseToOpen(updatedState.metaData().index(index));
                 }
 
-                RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(rtBuilder.build()).build());
+                RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(rtBuilder).build());
                 //no explicit wait for other nodes needed as we use AckedClusterStateUpdateTask
                 return ClusterState.builder(updatedState).routingResult(routingResult).build();
             }
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
index a17fe04..cdde491 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
@@ -21,11 +21,7 @@ package org.elasticsearch.cluster.metadata;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
-import org.elasticsearch.cluster.routing.DjbHashFunction;
-import org.elasticsearch.cluster.routing.HashFunction;
-import org.elasticsearch.cluster.routing.SimpleHashFunction;
 import org.elasticsearch.cluster.routing.UnassignedInfo;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
@@ -34,8 +30,7 @@ import org.elasticsearch.index.Index;
 import org.elasticsearch.index.analysis.AnalysisService;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
 import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.similarity.SimilarityLookupService;
-import org.elasticsearch.index.store.IndexStoreModule;
+import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.script.ScriptService;
 
 import java.util.Locale;
@@ -54,47 +49,12 @@ import static org.elasticsearch.common.util.set.Sets.newHashSet;
  */
 public class MetaDataIndexUpgradeService extends AbstractComponent {
 
-    private static final String DEPRECATED_SETTING_ROUTING_HASH_FUNCTION = "cluster.routing.operation.hash.type";
-    private static final String DEPRECATED_SETTING_ROUTING_USE_TYPE = "cluster.routing.operation.use_type";
-
-    private final Class<? extends HashFunction> pre20HashFunction;
-    private final Boolean pre20UseType;
     private final ScriptService scriptService;
 
     @Inject
     public MetaDataIndexUpgradeService(Settings settings, ScriptService scriptService) {
         super(settings);
         this.scriptService = scriptService;
-        final String pre20HashFunctionName = settings.get(DEPRECATED_SETTING_ROUTING_HASH_FUNCTION, null);
-        final boolean hasCustomPre20HashFunction = pre20HashFunctionName != null;
-        // the hash function package has changed we replace the two hash functions if their fully qualified name is used.
-        if (hasCustomPre20HashFunction) {
-            switch (pre20HashFunctionName) {
-                case "Simple":
-                case "simple":
-                case "org.elasticsearch.cluster.routing.operation.hash.simple.SimpleHashFunction":
-                    pre20HashFunction = SimpleHashFunction.class;
-                    break;
-                case "Djb":
-                case "djb":
-                case "org.elasticsearch.cluster.routing.operation.hash.djb.DjbHashFunction":
-                    pre20HashFunction = DjbHashFunction.class;
-                    break;
-                default:
-                    try {
-                        pre20HashFunction = Class.forName(pre20HashFunctionName).asSubclass(HashFunction.class);
-                    } catch (ClassNotFoundException|NoClassDefFoundError e) {
-                        throw new ElasticsearchException("failed to load custom hash function [" + pre20HashFunctionName + "]", e);
-                    }
-            }
-        } else {
-            pre20HashFunction = DjbHashFunction.class;
-        }
-        pre20UseType = settings.getAsBoolean(DEPRECATED_SETTING_ROUTING_USE_TYPE, null);
-        if (hasCustomPre20HashFunction || pre20UseType != null) {
-            logger.warn("Settings [{}] and [{}] are deprecated. Index settings from your old indices have been updated to record the fact that they "
-                    + "used some custom routing logic, you can now remove these settings from your `elasticsearch.yml` file", DEPRECATED_SETTING_ROUTING_HASH_FUNCTION, DEPRECATED_SETTING_ROUTING_USE_TYPE);
-        }
     }
 
     /**
@@ -110,68 +70,29 @@ public class MetaDataIndexUpgradeService extends AbstractComponent {
             return indexMetaData;
         }
         checkSupportedVersion(indexMetaData);
-        IndexMetaData newMetaData = upgradeLegacyRoutingSettings(indexMetaData);
+        IndexMetaData newMetaData = indexMetaData;
         newMetaData = addDefaultUnitsIfNeeded(newMetaData);
         checkMappingsCompatibility(newMetaData);
-        newMetaData = upgradeSettings(newMetaData);
         newMetaData = markAsUpgraded(newMetaData);
         return newMetaData;
     }
 
-    IndexMetaData upgradeSettings(IndexMetaData indexMetaData) {
-        final String storeType = indexMetaData.getSettings().get(IndexStoreModule.STORE_TYPE);
-        if (storeType != null) {
-            final String upgradeStoreType;
-            switch (storeType.toLowerCase(Locale.ROOT)) {
-                case "nio_fs":
-                case "niofs":
-                    upgradeStoreType = "niofs";
-                    break;
-                case "mmap_fs":
-                case "mmapfs":
-                    upgradeStoreType = "mmapfs";
-                    break;
-                case "simple_fs":
-                case "simplefs":
-                    upgradeStoreType = "simplefs";
-                    break;
-                case "default":
-                    upgradeStoreType = "default";
-                    break;
-                case "fs":
-                    upgradeStoreType = "fs";
-                    break;
-                default:
-                    upgradeStoreType = storeType;
-            }
-            if (storeType.equals(upgradeStoreType) == false) {
-                Settings indexSettings = Settings.builder().put(indexMetaData.settings())
-                        .put(IndexStoreModule.STORE_TYPE, upgradeStoreType)
-                        .build();
-                return IndexMetaData.builder(indexMetaData)
-                        .version(indexMetaData.version())
-                        .settings(indexSettings)
-                        .build();
-            }
-        }
-        return indexMetaData;
-    }
 
     /**
      * Checks if the index was already opened by this version of Elasticsearch and doesn't require any additional checks.
      */
     private boolean isUpgraded(IndexMetaData indexMetaData) {
-        return indexMetaData.upgradeVersion().onOrAfter(Version.V_2_0_0_beta1);
+        return indexMetaData.upgradeVersion().onOrAfter(Version.V_3_0_0);
     }
 
     /**
-     * Elasticsearch 2.0 no longer supports indices with pre Lucene v4.0 (Elasticsearch v 0.90.0) segments. All indices
-     * that were created before Elasticsearch v0.90.0 should be upgraded using upgrade plugin before they can
+     * Elasticsearch 3.0 no longer supports indices with pre Lucene v5.0 (Elasticsearch v2.0.0.beta1) segments. All indices
+     * that were created before Elasticsearch v2.0.0.beta1 should be upgraded using upgrade API before they can
      * be open by this version of elasticsearch.
      */
     private void checkSupportedVersion(IndexMetaData indexMetaData) {
         if (indexMetaData.getState() == IndexMetaData.State.OPEN && isSupportedVersion(indexMetaData) == false) {
-            throw new IllegalStateException("The index [" + indexMetaData.getIndex() + "] was created before v0.90.0 and wasn't upgraded."
+            throw new IllegalStateException("The index [" + indexMetaData.getIndex() + "] was created before v2.0.0.beta1 and wasn't upgraded."
                     + " This index should be open using a version before " + Version.CURRENT.minimumCompatibilityVersion()
                     + " and upgraded using the upgrade API.");
         }
@@ -181,44 +102,18 @@ public class MetaDataIndexUpgradeService extends AbstractComponent {
      * Returns true if this index can be supported by the current version of elasticsearch
      */
     private static boolean isSupportedVersion(IndexMetaData indexMetaData) {
-        if (indexMetaData.creationVersion().onOrAfter(Version.V_0_90_0_Beta1)) {
-            // The index was created with elasticsearch that was using Lucene 4.0
+        if (indexMetaData.creationVersion().onOrAfter(Version.V_2_0_0_beta1)) {
+            // The index was created with elasticsearch that was using Lucene 5.2.1
             return true;
         }
         if (indexMetaData.getMinimumCompatibleVersion() != null &&
-                indexMetaData.getMinimumCompatibleVersion().onOrAfter(org.apache.lucene.util.Version.LUCENE_4_0_0)) {
+                indexMetaData.getMinimumCompatibleVersion().onOrAfter(org.apache.lucene.util.Version.LUCENE_5_0_0)) {
             //The index was upgraded we can work with it
             return true;
         }
         return false;
     }
 
-    /**
-     * Elasticsearch 2.0 deprecated custom routing hash functions. So what we do here is that for old indices, we
-     * move this old and deprecated node setting to an index setting so that we can keep things backward compatible.
-     */
-    private IndexMetaData upgradeLegacyRoutingSettings(IndexMetaData indexMetaData) {
-        if (indexMetaData.settings().get(IndexMetaData.SETTING_LEGACY_ROUTING_HASH_FUNCTION) == null
-                && indexMetaData.getCreationVersion().before(Version.V_2_0_0_beta1)) {
-            // these settings need an upgrade
-            Settings indexSettings = Settings.builder().put(indexMetaData.settings())
-                    .put(IndexMetaData.SETTING_LEGACY_ROUTING_HASH_FUNCTION, pre20HashFunction)
-                    .put(IndexMetaData.SETTING_LEGACY_ROUTING_USE_TYPE, pre20UseType == null ? false : pre20UseType)
-                    .build();
-            return IndexMetaData.builder(indexMetaData)
-                    .version(indexMetaData.version())
-                    .settings(indexSettings)
-                    .build();
-        } else if (indexMetaData.getCreationVersion().onOrAfter(Version.V_2_0_0_beta1)) {
-            if (indexMetaData.getSettings().get(IndexMetaData.SETTING_LEGACY_ROUTING_HASH_FUNCTION) != null
-                    || indexMetaData.getSettings().get(IndexMetaData.SETTING_LEGACY_ROUTING_USE_TYPE) != null) {
-                throw new IllegalStateException("Index [" + indexMetaData.getIndex() + "] created on or after 2.0 should NOT contain [" + IndexMetaData.SETTING_LEGACY_ROUTING_HASH_FUNCTION
-                        + "] + or [" + IndexMetaData.SETTING_LEGACY_ROUTING_USE_TYPE + "] in its index settings");
-            }
-        }
-        return indexMetaData;
-    }
-
     /** All known byte-sized settings for an index. */
     public static final Set<String> INDEX_BYTES_SIZE_SETTINGS = unmodifiableSet(newHashSet(
                                     "index.merge.policy.floor_segment",
@@ -322,11 +217,11 @@ public class MetaDataIndexUpgradeService extends AbstractComponent {
         Index index = new Index(indexMetaData.getIndex());
         Settings settings = indexMetaData.settings();
         try {
-            SimilarityLookupService similarityLookupService = new SimilarityLookupService(index, settings);
+            SimilarityService similarityService = new SimilarityService(index, settings);
             // We cannot instantiate real analysis server at this point because the node might not have
             // been started yet. However, we don't really need real analyzers at this stage - so we can fake it
             try (AnalysisService analysisService = new FakeAnalysisService(index, settings)) {
-                try (MapperService mapperService = new MapperService(index, settings, analysisService, similarityLookupService, scriptService)) {
+                try (MapperService mapperService = new MapperService(index, settings, analysisService, similarityService, scriptService)) {
                     for (ObjectCursor<MappingMetaData> cursor : indexMetaData.getMappings().values()) {
                         MappingMetaData mappingMetaData = cursor.value;
                         mapperService.merge(mappingMetaData.type(), mappingMetaData.source(), false, false);
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataService.java
index 2f21553..ca482ea 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataService.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.cluster.metadata;
 
-import org.elasticsearch.cluster.routing.DjbHashFunction;
+import org.elasticsearch.cluster.routing.Murmur3HashFunction;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.math.MathUtils;
@@ -43,6 +43,6 @@ public class MetaDataService extends AbstractComponent {
     }
 
     public Semaphore indexMetaDataLock(String index) {
-        return indexMdLocks[MathUtils.mod(DjbHashFunction.DJB_HASH(index), indexMdLocks.length)];
+        return indexMdLocks[MathUtils.mod(Murmur3HashFunction.hash(index), indexMdLocks.length)];
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java
index 58dffd8..65d862c 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java
@@ -320,7 +320,7 @@ public class MetaDataUpdateSettingsService extends AbstractComponent implements
                 }
 
 
-                ClusterState updatedState = ClusterState.builder(currentState).metaData(metaDataBuilder).routingTable(routingTableBuilder.build()).blocks(blocks).build();
+                ClusterState updatedState = ClusterState.builder(currentState).metaData(metaDataBuilder).routingTable(routingTableBuilder).blocks(blocks).build();
 
                 // now, reroute in case things change that require it (like number of replicas)
                 RoutingAllocation.Result routingResult = allocationService.reroute(updatedState);
diff --git a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
index 780f511..ebf1bcb 100644
--- a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
+++ b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
@@ -19,15 +19,13 @@
 
 package org.elasticsearch.cluster.node;
 
-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+import com.google.common.collect.ImmutableMap;
 
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Booleans;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Streamable;
+import org.elasticsearch.common.io.stream.*;
+import org.elasticsearch.common.network.NetworkUtils;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.common.transport.TransportAddressSerializers;
@@ -35,6 +33,7 @@ import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
+import java.net.InetAddress;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
@@ -101,7 +100,7 @@ public class DiscoveryNode implements Streamable, ToXContent {
     private String hostName;
     private String hostAddress;
     private TransportAddress address;
-    private ImmutableOpenMap<String, String> attributes;
+    private Map<String, String> attributes;
     private Version version = Version.CURRENT;
 
     DiscoveryNode() {
@@ -144,7 +143,7 @@ public class DiscoveryNode implements Streamable, ToXContent {
     }
 
     /**
-     * Creates a new {@link DiscoveryNode}.
+     * Creates a new {@link DiscoveryNode}
      * <p>
      * <b>Note:</b> if the version of the node is unknown {@link #MINIMUM_DISCOVERY_NODE_VERSION} should be used.
      * it corresponds to the minimum version this elasticsearch version can communicate with. If a higher version is used
@@ -164,7 +163,7 @@ public class DiscoveryNode implements Streamable, ToXContent {
         if (nodeName != null) {
             this.nodeName = nodeName.intern();
         }
-        ImmutableOpenMap.Builder<String, String> builder = ImmutableOpenMap.builder();
+        ImmutableMap.Builder<String, String> builder = ImmutableMap.builder();
         for (Map.Entry<String, String> entry : attributes.entrySet()) {
             builder.put(entry.getKey().intern(), entry.getValue().intern());
         }
@@ -177,39 +176,6 @@ public class DiscoveryNode implements Streamable, ToXContent {
     }
 
     /**
-     * Creates a new {@link DiscoveryNode}.
-     * <p>
-     * <b>Note:</b> if the version of the node is unknown {@link #MINIMUM_DISCOVERY_NODE_VERSION} should be used.
-     * it corresponds to the minimum version this elasticsearch version can communicate with. If a higher version is used
-     * the node might not be able to communicate with the remove node. After initial handshakes node versions will be discovered
-     * and updated.
-     * </p>
-     *
-     * @param nodeName    the nodes name
-     * @param nodeId      the nodes unique id.
-     * @param hostName    the nodes hostname
-     * @param hostAddress the nodes host address
-     * @param address     the nodes transport address
-     * @param attributes  node attributes
-     * @param version     the version of the node.
-     */
-    public DiscoveryNode(String nodeName, String nodeId, String hostName, String hostAddress, TransportAddress address, ImmutableOpenMap<String, String> attributes, Version version) {
-        if (nodeName != null) {
-            this.nodeName = nodeName.intern();
-        }
-        ImmutableOpenMap.Builder<String, String> builder = ImmutableOpenMap.builder();
-        for (ObjectObjectCursor<String, String> entry : attributes) {
-            builder.put(entry.key.intern(), entry.value.intern());
-        }
-        this.attributes = builder.build();
-        this.nodeId = nodeId.intern();
-        this.hostName = hostName.intern();
-        this.hostAddress = hostAddress.intern();
-        this.address = address;
-        this.version = version;
-    }
-
-    /**
      * Should this node form a connection to the provided node.
      */
     public boolean shouldConnectTo(DiscoveryNode otherNode) {
@@ -264,14 +230,14 @@ public class DiscoveryNode implements Streamable, ToXContent {
     /**
      * The node attributes.
      */
-    public ImmutableOpenMap<String, String> attributes() {
+    public Map<String, String> attributes() {
         return this.attributes;
     }
 
     /**
      * The node attributes.
      */
-    public ImmutableOpenMap<String, String> getAttributes() {
+    public Map<String, String> getAttributes() {
         return attributes();
     }
 
@@ -353,11 +319,11 @@ public class DiscoveryNode implements Streamable, ToXContent {
         hostAddress = in.readString().intern();
         address = TransportAddressSerializers.addressFromStream(in);
         int size = in.readVInt();
-        ImmutableOpenMap.Builder<String, String> attributes = ImmutableOpenMap.builder(size);
+        ImmutableMap.Builder<String, String> builder = ImmutableMap.builder();
         for (int i = 0; i < size; i++) {
-            attributes.put(in.readString().intern(), in.readString().intern());
+            builder.put(in.readString().intern(), in.readString().intern());
         }
-        this.attributes = attributes.build();
+        attributes = builder.build();
         version = Version.readVersion(in);
     }
 
@@ -369,9 +335,9 @@ public class DiscoveryNode implements Streamable, ToXContent {
         out.writeString(hostAddress);
         addressToStream(out, address);
         out.writeVInt(attributes.size());
-        for (ObjectObjectCursor<String, String> entry : attributes) {
-            out.writeString(entry.key);
-            out.writeString(entry.value);
+        for (Map.Entry<String, String> entry : attributes.entrySet()) {
+            out.writeString(entry.getKey());
+            out.writeString(entry.getValue());
         }
         Version.writeVersion(version, out);
     }
@@ -419,8 +385,8 @@ public class DiscoveryNode implements Streamable, ToXContent {
         builder.field("transport_address", address().toString());
 
         builder.startObject("attributes");
-        for (ObjectObjectCursor<String, String> attr : attributes) {
-            builder.field(attr.key, attr.value);
+        for (Map.Entry<String, String> attr : attributes().entrySet()) {
+            builder.field(attr.getKey(), attr.getValue());
         }
         builder.endObject();
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java
index 16b7e9e..13b6471 100644
--- a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java
+++ b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java
@@ -22,7 +22,6 @@ package org.elasticsearch.cluster.node;
 import com.carrotsearch.hppc.ObjectHashSet;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.AbstractDiffable;
 import org.elasticsearch.common.Booleans;
@@ -34,12 +33,7 @@ import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.transport.TransportAddress;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
+import java.util.*;
 
 /**
  * This class holds all {@link DiscoveryNode} in the cluster and provides convenience methods to
@@ -380,9 +374,9 @@ public class DiscoveryNodes extends AbstractDiffable<DiscoveryNodes> implements
                             }
                         } else {
                             for (DiscoveryNode node : this) {
-                                for (ObjectObjectCursor<String, String> entry : node.attributes()) {
-                                    String attrName = entry.key;
-                                    String attrValue = entry.value;
+                                for (Map.Entry<String, String> entry : node.attributes().entrySet()) {
+                                    String attrName = entry.getKey();
+                                    String attrValue = entry.getValue();
                                     if (Regex.simpleMatch(matchAttrName, attrName) && Regex.simpleMatch(matchAttrValue, attrValue)) {
                                         resolvedNodesIds.add(node.id());
                                     }
@@ -569,7 +563,6 @@ public class DiscoveryNodes extends AbstractDiffable<DiscoveryNodes> implements
         }
     }
 
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         if (masterNodeId == null) {
             out.writeBoolean(false);
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/DjbHashFunction.java b/core/src/main/java/org/elasticsearch/cluster/routing/DjbHashFunction.java
deleted file mode 100644
index 7616bd3..0000000
--- a/core/src/main/java/org/elasticsearch/cluster/routing/DjbHashFunction.java
+++ /dev/null
@@ -1,70 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cluster.routing;
-
-import org.elasticsearch.cluster.routing.HashFunction;
-
-/**
- * This class implements the efficient hash function
- * developed by <i>Daniel J. Bernstein</i>.
- */
-public class DjbHashFunction implements HashFunction {
-
-    public static int DJB_HASH(String value) {
-        long hash = 5381;
-
-        for (int i = 0; i < value.length(); i++) {
-            hash = ((hash << 5) + hash) + value.charAt(i);
-        }
-
-        return (int) hash;
-    }
-
-    public static int DJB_HASH(byte[] value, int offset, int length) {
-        long hash = 5381;
-
-        final int end = offset + length;
-        for (int i = offset; i < end; i++) {
-            hash = ((hash << 5) + hash) + value[i];
-        }
-
-        return (int) hash;
-    }
-
-    @Override
-    public int hash(String routing) {
-        return DJB_HASH(routing);
-    }
-
-    @Override
-    public int hash(String type, String id) {
-        long hash = 5381;
-
-        for (int i = 0; i < type.length(); i++) {
-            hash = ((hash << 5) + hash) + type.charAt(i);
-        }
-
-        for (int i = 0; i < id.length(); i++) {
-            hash = ((hash << 5) + hash) + id.charAt(i);
-        }
-
-        return (int) hash;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/HashFunction.java b/core/src/main/java/org/elasticsearch/cluster/routing/HashFunction.java
deleted file mode 100644
index 99977ee..0000000
--- a/core/src/main/java/org/elasticsearch/cluster/routing/HashFunction.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cluster.routing;
-
-/**
- * Simple hash function interface used for shard routing.
- */
-public interface HashFunction {
-
-    /**
-     * Calculate a hash value for routing 
-     * @param routing String to calculate the hash value from 
-     * @return hash value of the given routing string
-     */
-    int hash(String routing);
-
-    /**
-     * Calculate a hash value for routing and its type
-     * @param type types name
-     * @param id String to calculate the hash value from 
-     * @return hash value of the given type and routing string
-     */
-    @Deprecated
-    int hash(String type, String id);
-}
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java b/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java
index 6512ee5..e740a4f 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster.routing;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.common.collect.MapBuilder;
@@ -38,8 +39,6 @@ import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.ThreadLocalRandom;
 
-import static java.util.Collections.emptyMap;
-
 /**
  * {@link IndexShardRoutingTable} encapsulates all instances of a single shard.
  * Each Elasticsearch index consists of multiple shards, each shard encapsulates
@@ -61,10 +60,6 @@ public class IndexShardRoutingTable implements Iterable<ShardRouting> {
     final static List<ShardRouting> NO_SHARDS = Collections.emptyList();
     final boolean allShardsStarted;
 
-    private volatile Map<AttributesKey, AttributesRoutings> activeShardsByAttributes = emptyMap();
-    private volatile Map<AttributesKey, AttributesRoutings> initializingShardsByAttributes = emptyMap();
-    private final Object shardsByAttributeMutex = new Object();
-
     /**
      * The initializing list, including ones that are initializing on a target node because of relocation.
      * If we can come up with a better variable name, it would be nice...
@@ -481,6 +476,10 @@ public class IndexShardRoutingTable implements Iterable<ShardRouting> {
         }
     }
 
+    private volatile Map<AttributesKey, AttributesRoutings> activeShardsByAttributes = ImmutableMap.of();
+    private volatile Map<AttributesKey, AttributesRoutings> initializingShardsByAttributes = ImmutableMap.of();
+    private final Object shardsByAttributeMutex = new Object();
+
     private AttributesRoutings getActiveAttribute(AttributesKey key, DiscoveryNodes nodes) {
         AttributesRoutings shardRoutings = activeShardsByAttributes.get(key);
         if (shardRoutings == null) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/Murmur3HashFunction.java b/core/src/main/java/org/elasticsearch/cluster/routing/Murmur3HashFunction.java
index 7ca602a..4752271 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/Murmur3HashFunction.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/Murmur3HashFunction.java
@@ -20,15 +20,17 @@
 package org.elasticsearch.cluster.routing;
 
 import org.apache.lucene.util.StringHelper;
-import org.elasticsearch.cluster.routing.HashFunction;
 
 /**
  * Hash function based on the Murmur3 algorithm, which is the default as of Elasticsearch 2.0.
  */
-public class Murmur3HashFunction implements HashFunction {
+public final class Murmur3HashFunction {
 
-    @Override
-    public int hash(String routing) {
+    private Murmur3HashFunction() {
+        //no instance
+    }
+
+    public static int hash(String routing) {
         final byte[] bytesToHash = new byte[routing.length() * 2];
         for (int i = 0; i < routing.length(); ++i) {
             final char c = routing.charAt(i);
@@ -37,12 +39,10 @@ public class Murmur3HashFunction implements HashFunction {
             bytesToHash[i * 2] = b1;
             bytesToHash[i * 2 + 1] = b2;
         }
-        return StringHelper.murmurhash3_x86_32(bytesToHash, 0, bytesToHash.length, 0);
+        return hash(bytesToHash, 0, bytesToHash.length);
     }
 
-    @Override
-    public int hash(String type, String id) {
-        throw new UnsupportedOperationException();
+    public static int hash(byte[] bytes, int offset, int length) {
+        return StringHelper.murmurhash3_x86_32(bytes, offset, length, 0);
     }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java b/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java
index 411a1ed..c142b75 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.cluster.routing;
 
-import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
@@ -47,7 +46,6 @@ import java.util.Set;
 public class OperationRouting extends AbstractComponent {
 
 
-
     private final AwarenessAllocationDecider awarenessAllocationDecider;
 
     @Inject
@@ -196,9 +194,9 @@ public class OperationRouting extends AbstractComponent {
         // if not, then use it as the index
         String[] awarenessAttributes = awarenessAllocationDecider.awarenessAttributes();
         if (awarenessAttributes.length == 0) {
-            return indexShard.activeInitializingShardsIt(DjbHashFunction.DJB_HASH(preference));
+            return indexShard.activeInitializingShardsIt(Murmur3HashFunction.hash(preference));
         } else {
-            return indexShard.preferAttributesActiveInitializingShardsIt(awarenessAttributes, nodes, DjbHashFunction.DJB_HASH(preference));
+            return indexShard.preferAttributesActiveInitializingShardsIt(awarenessAttributes, nodes, Murmur3HashFunction.hash(preference));
         }
     }
 
@@ -237,37 +235,13 @@ public class OperationRouting extends AbstractComponent {
     @SuppressForbidden(reason = "Math#abs is trappy")
     private int shardId(ClusterState clusterState, String index, String type, String id, @Nullable String routing) {
         final IndexMetaData indexMetaData = indexMetaData(clusterState, index);
-        final Version createdVersion = indexMetaData.getCreationVersion();
-        final HashFunction hashFunction = indexMetaData.getRoutingHashFunction();
-        final boolean useType = indexMetaData.getRoutingUseType();
-
         final int hash;
         if (routing == null) {
-            if (!useType) {
-                hash = hash(hashFunction, id);
-            } else {
-                hash = hash(hashFunction, type, id);
-            }
-        } else {
-            hash = hash(hashFunction, routing);
-        }
-        if (createdVersion.onOrAfter(Version.V_2_0_0_beta1)) {
-            return MathUtils.mod(hash, indexMetaData.numberOfShards());
+            hash = Murmur3HashFunction.hash(id);
         } else {
-            return Math.abs(hash % indexMetaData.numberOfShards());
-        }
-    }
-
-    protected int hash(HashFunction hashFunction, String routing) {
-        return hashFunction.hash(routing);
-    }
-
-    @Deprecated
-    protected int hash(HashFunction hashFunction, String type, String id) {
-        if (type == null || "_all".equals(type)) {
-            throw new IllegalArgumentException("Can't route an operation with no type and having type part of the routing (for backward comp)");
+            hash = Murmur3HashFunction.hash(routing);
         }
-        return hashFunction.hash(type, id);
+        return MathUtils.mod(hash, indexMetaData.numberOfShards());
     }
 
     private void ensureNodeIdExists(DiscoveryNodes nodes, String nodeId) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java
index 43ad6af..596bb97 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java
@@ -19,13 +19,10 @@
 
 package org.elasticsearch.cluster.routing;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.collect.Iterators;
 
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.List;
+import java.util.*;
 
 /**
  * A {@link RoutingNode} represents a cluster node associated with a single {@link DiscoveryNode} including all shards
@@ -51,7 +48,7 @@ public class RoutingNode implements Iterable<ShardRouting> {
 
     @Override
     public Iterator<ShardRouting> iterator() {
-        return Iterators.unmodifiableIterator(shards.iterator());
+        return Collections.unmodifiableCollection(shards).iterator();
     }
 
     Iterator<ShardRouting> mutableIterator() {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
index f632428..39c2d03 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
@@ -21,25 +21,16 @@ package org.elasticsearch.cluster.routing;
 
 import com.carrotsearch.hppc.ObjectIntHashMap;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
-import com.google.common.collect.Iterators;
-
 import org.apache.lucene.util.CollectionUtil;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.block.ClusterBlocks;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
+import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.index.shard.ShardId;
 
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
+import java.util.*;
 import java.util.function.Predicate;
 
 /**
@@ -91,8 +82,8 @@ public class RoutingNodes implements Iterable<RoutingNode> {
 
         // fill in the inverse of node -> shards allocated
         // also fill replicaSet information
-        for (ObjectCursor<IndexRoutingTable> indexRoutingTable : routingTable.indicesRouting().values()) {
-            for (IndexShardRoutingTable indexShard : indexRoutingTable.value) {
+        for (IndexRoutingTable indexRoutingTable : routingTable.indicesRouting().values()) {
+            for (IndexShardRoutingTable indexShard : indexRoutingTable) {
                 for (ShardRouting shard : indexShard) {
                     // to get all the shards belonging to an index, including the replicas,
                     // we define a replica set and keep track of it. A replica set is identified
@@ -153,7 +144,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
 
     @Override
     public Iterator<RoutingNode> iterator() {
-        return Iterators.unmodifiableIterator(nodesToShards.values().iterator());
+        return Collections.unmodifiableCollection(nodesToShards.values()).iterator();
     }
 
     public RoutingTable routingTable() {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
index 10d7ff9..7a8c33e 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
@@ -20,15 +20,13 @@
 package org.elasticsearch.cluster.routing;
 
 import com.carrotsearch.hppc.IntSet;
-import com.carrotsearch.hppc.cursors.ObjectCursor;
-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+import com.google.common.collect.ImmutableMap;
 
 import org.elasticsearch.cluster.Diff;
 import org.elasticsearch.cluster.Diffable;
 import org.elasticsearch.cluster.DiffableUtils;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.util.iterable.Iterables;
@@ -43,6 +41,8 @@ import java.util.List;
 import java.util.Map;
 import java.util.function.Predicate;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * Represents a global cluster-wide routing table for all indices including the
  * version of the current routing state.
@@ -58,11 +58,11 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
     private final long version;
 
     // index to IndexRoutingTable map
-    private final ImmutableOpenMap<String, IndexRoutingTable> indicesRouting;
+    private final Map<String, IndexRoutingTable> indicesRouting;
 
-    RoutingTable(long version, ImmutableOpenMap<String, IndexRoutingTable> indicesRouting) {
+    RoutingTable(long version, Map<String, IndexRoutingTable> indicesRouting) {
         this.version = version;
-        this.indicesRouting = indicesRouting;
+        this.indicesRouting = unmodifiableMap(indicesRouting);
     }
 
     /**
@@ -76,7 +76,7 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
 
     @Override
     public Iterator<IndexRoutingTable> iterator() {
-        return indicesRouting.valuesIt();
+        return indicesRouting.values().iterator();
     }
 
     public boolean hasIndex(String index) {
@@ -87,11 +87,11 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
         return indicesRouting.get(index);
     }
 
-    public ImmutableOpenMap<String, IndexRoutingTable> indicesRouting() {
+    public Map<String, IndexRoutingTable> indicesRouting() {
         return indicesRouting;
     }
 
-    public ImmutableOpenMap<String, IndexRoutingTable> getIndicesRouting() {
+    public Map<String, IndexRoutingTable> getIndicesRouting() {
         return indicesRouting();
     }
 
@@ -126,7 +126,7 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
      */
     public List<ShardRouting> allShards() {
         List<ShardRouting> shards = new ArrayList<>();
-        String[] indices = indicesRouting.keys().toArray(String.class);
+        String[] indices = indicesRouting.keySet().toArray(new String[indicesRouting.keySet().size()]);
         for (String index : indices) {
             List<ShardRouting> allShardsIndex = allShards(index);
             shards.addAll(allShardsIndex);
@@ -303,8 +303,8 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
     public void writeTo(StreamOutput out) throws IOException {
         out.writeLong(version);
         out.writeVInt(indicesRouting.size());
-        for (ObjectCursor<IndexRoutingTable> index : indicesRouting.values()) {
-            index.value.writeTo(out);
+        for (IndexRoutingTable index : indicesRouting.values()) {
+            index.writeTo(out);
         }
     }
 
@@ -312,7 +312,7 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
 
         private final long version;
 
-        private final Diff<ImmutableOpenMap<String, IndexRoutingTable>> indicesRouting;
+        private final Diff<Map<String, IndexRoutingTable>> indicesRouting;
 
         public RoutingTableDiff(RoutingTable before, RoutingTable after) {
             version = after.version;
@@ -321,7 +321,7 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
 
         public RoutingTableDiff(StreamInput in) throws IOException {
             version = in.readLong();
-            indicesRouting = DiffableUtils.readImmutableOpenMapDiff(in, IndexRoutingTable.PROTO);
+            indicesRouting = DiffableUtils.readJdkMapDiff(in, IndexRoutingTable.PROTO);
         }
 
         @Override
@@ -344,13 +344,10 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
         return new Builder(routingTable);
     }
 
-    /**
-     * Builder for the routing table. Note that build can only be called one time.
-     */
     public static class Builder {
 
         private long version;
-        private ImmutableOpenMap.Builder<String, IndexRoutingTable> indicesRouting = ImmutableOpenMap.builder();
+        private final Map<String, IndexRoutingTable> indicesRouting = new HashMap<>();
 
         public Builder() {
 
@@ -406,11 +403,8 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
         }
 
         public Builder updateNumberOfReplicas(int numberOfReplicas, String... indices) {
-            if (indicesRouting == null) {
-                throw new IllegalStateException("once build is called the builder cannot be reused");
-            }
             if (indices == null || indices.length == 0) {
-                indices = indicesRouting.keys().toArray(String.class);
+                indices = indicesRouting.keySet().toArray(new String[indicesRouting.keySet().size()]);
             }
             for (String index : indices) {
                 IndexRoutingTable indexRoutingTable = indicesRouting.get(index);
@@ -495,9 +489,6 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
         }
 
         public Builder add(IndexRoutingTable indexRoutingTable) {
-            if (indicesRouting == null) {
-                throw new IllegalStateException("once build is called the builder cannot be reused");
-            }
             indexRoutingTable.validate();
             indicesRouting.put(indexRoutingTable.index(), indexRoutingTable);
             return this;
@@ -508,18 +499,12 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
             return this;
         }
 
-        public Builder indicesRouting(Map<String, IndexRoutingTable> indicesRouting) {
-            if (indicesRouting == null) {
-                throw new IllegalStateException("once build is called the builder cannot be reused");
-            }
+        public Builder indicesRouting(ImmutableMap<String, IndexRoutingTable> indicesRouting) {
             this.indicesRouting.putAll(indicesRouting);
             return this;
         }
 
         public Builder remove(String index) {
-            if (indicesRouting == null) {
-                throw new IllegalStateException("once build is called the builder cannot be reused");
-            }
             indicesRouting.remove(index);
             return this;
         }
@@ -529,22 +514,12 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
             return this;
         }
 
-        /**
-         * Builds the routing table. Note that once this is called the builder
-         * must be thrown away. If you need to build a new RoutingTable as a
-         * copy of this one you'll need to build a new RoutingTable.Builder.
-         */
         public RoutingTable build() {
-            if (indicesRouting == null) {
-                throw new IllegalStateException("once build is called the builder cannot be reused");
-            }
             // normalize the versions right before we build it...
-            for (ObjectCursor<IndexRoutingTable> indexRoutingTable : indicesRouting.values()) {
-                indicesRouting.put(indexRoutingTable.value.index(), indexRoutingTable.value.normalizeVersions());
+            for (IndexRoutingTable indexRoutingTable : indicesRouting.values()) {
+                indicesRouting.put(indexRoutingTable.index(), indexRoutingTable.normalizeVersions());
             }
-            RoutingTable table = new RoutingTable(version, indicesRouting.build());
-            indicesRouting = null;
-            return table;
+            return new RoutingTable(version, indicesRouting);
         }
 
         public static RoutingTable readFrom(StreamInput in) throws IOException {
@@ -554,8 +529,8 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
 
     public String prettyPrint() {
         StringBuilder sb = new StringBuilder("routing_table (version ").append(version).append("):\n");
-        for (ObjectObjectCursor<String, IndexRoutingTable> entry : indicesRouting) {
-            sb.append(entry.value.prettyPrint()).append('\n');
+        for (Map.Entry<String, IndexRoutingTable> entry : indicesRouting.entrySet()) {
+            sb.append(entry.getValue().prettyPrint()).append('\n');
         }
         return sb.toString();
     }
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTableValidation.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTableValidation.java
index 472e73b..aec8bef 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTableValidation.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTableValidation.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster.routing;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
@@ -30,8 +31,6 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.emptyMap;
-
 /**
  * Encapsulates the result of a routing table validation and provides access to
  * validation failures.
@@ -73,7 +72,7 @@ public class RoutingTableValidation implements Streamable {
 
     public Map<String, List<String>> indicesFailures() {
         if (indicesFailures == null) {
-            return emptyMap();
+            return ImmutableMap.of();
         }
         return indicesFailures;
     }
@@ -129,7 +128,7 @@ public class RoutingTableValidation implements Streamable {
         }
         size = in.readVInt();
         if (size == 0) {
-            indicesFailures = emptyMap();
+            indicesFailures = ImmutableMap.of();
         } else {
             indicesFailures = new HashMap<>();
             for (int i = 0; i < size; i++) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/SimpleHashFunction.java b/core/src/main/java/org/elasticsearch/cluster/routing/SimpleHashFunction.java
deleted file mode 100644
index bbb6a61..0000000
--- a/core/src/main/java/org/elasticsearch/cluster/routing/SimpleHashFunction.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cluster.routing;
-
-/**
- * This class implements a simple hash function based on Java Build-In {@link Object#hashCode()}
- */
-public class SimpleHashFunction implements HashFunction {
-
-    @Override
-    public int hash(String routing) {
-        return routing.hashCode();
-    }
-
-    @Override
-    public int hash(String type, String id) {
-        return type.hashCode() + 31 * id.hashCode();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
index efb5c96..b0ac162 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
@@ -19,14 +19,13 @@
 
 package org.elasticsearch.cluster.routing.allocation.allocator;
 
-import com.carrotsearch.hppc.cursors.ObjectCursor;
-
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.IntroSorter;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.RoutingNode;
 import org.elasticsearch.cluster.routing.RoutingNodes;
-import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingState;
 import org.elasticsearch.cluster.routing.allocation.FailedRerouteAllocation;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
@@ -41,16 +40,7 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.gateway.PriorityComparator;
 import org.elasticsearch.node.settings.NodeSettingsService;
 
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.IdentityHashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.Set;
+import java.util.*;
 import java.util.function.Predicate;
 
 import static org.elasticsearch.cluster.routing.ShardRoutingState.RELOCATING;
@@ -294,9 +284,7 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards
             if (logger.isTraceEnabled()) {
                 logger.trace("Start distributing Shards");
             }
-            for (ObjectCursor<String> index : allocation.routingTable().indicesRouting().keys()) {
-                indices.add(index.value);
-            }
+            indices.addAll(allocation.routingTable().indicesRouting().keySet());
             buildModelFromAssigned(routing.shards(assignedFilter));
             return allocateUnassigned(unassigned);
         }
@@ -440,7 +428,7 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards
                 deltas[i] = sorter.delta();
             }
             new IntroSorter() {
-
+                
                 float pivotWeight;
 
                 @Override
@@ -566,10 +554,10 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards
                 return false;
             }
             boolean changed = false;
-
+          
             /*
              * TODO: We could be smarter here and group the shards by index and then
-             * use the sorter to save some iterations.
+             * use the sorter to save some iterations. 
              */
             final AllocationDeciders deciders = allocation.deciders();
             final PriorityComparator secondaryComparator = PriorityComparator.getAllocationComparator(allocation);
@@ -780,7 +768,7 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards
                 }
 
                 if (candidate != null) {
-
+                  
                     /* allocate on the model even if not throttled */
                     maxNode.removeShard(candidate);
                     minNode.addShard(candidate, decision);
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java
index f21ced8..e1a0b77 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java
@@ -19,10 +19,6 @@
 
 package org.elasticsearch.cluster.routing.allocation.decider;
 
-import com.carrotsearch.hppc.ObjectLookupContainer;
-import com.carrotsearch.hppc.cursors.ObjectCursor;
-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.cluster.ClusterInfo;
@@ -34,7 +30,6 @@ import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingState;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeValue;
@@ -43,6 +38,7 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.set.Sets;
 import org.elasticsearch.node.settings.NodeSettingsService;
 
+import java.util.Map;
 import java.util.Set;
 
 /**
@@ -168,23 +164,23 @@ public class DiskThresholdDecider extends AllocationDecider {
 
         @Override
         public void onNewInfo(ClusterInfo info) {
-            ImmutableOpenMap<String, DiskUsage> usages = info.getNodeLeastAvailableDiskUsages();
+            Map<String, DiskUsage> usages = info.getNodeLeastAvailableDiskUsages();
             if (usages != null) {
                 boolean reroute = false;
                 String explanation = "";
 
                 // Garbage collect nodes that have been removed from the cluster
                 // from the map that tracks watermark crossing
-                ObjectLookupContainer<String> nodes = usages.keys();
+                Set<String> nodes = usages.keySet();
                 for (String node : nodeHasPassedWatermark) {
                     if (nodes.contains(node) == false) {
                         nodeHasPassedWatermark.remove(node);
                     }
                 }
 
-                for (ObjectObjectCursor<String, DiskUsage> entry : usages) {
-                    String node = entry.key;
-                    DiskUsage usage = entry.value;
+                for (Map.Entry<String, DiskUsage> entry : usages.entrySet()) {
+                    String node = entry.getKey();
+                    DiskUsage usage = entry.getValue();
                     warnAboutDiskIfNeeded(usage);
                     if (usage.getFreeBytes() < DiskThresholdDecider.this.freeBytesThresholdHigh.bytes() ||
                             usage.getFreeDiskAsPercentage() < DiskThresholdDecider.this.freeDiskThresholdHigh) {
@@ -340,7 +336,7 @@ public class DiskThresholdDecider extends AllocationDecider {
     @Override
     public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) {
         ClusterInfo clusterInfo = allocation.clusterInfo();
-        ImmutableOpenMap<String, DiskUsage> usages = clusterInfo.getNodeMostAvailableDiskUsages();
+        Map<String, DiskUsage> usages = clusterInfo.getNodeMostAvailableDiskUsages();
         final Decision decision = earlyTerminate(allocation, usages);
         if (decision != null) {
             return decision;
@@ -455,7 +451,7 @@ public class DiskThresholdDecider extends AllocationDecider {
             throw new IllegalArgumentException("Shard [" + shardRouting + "] is not allocated on node: [" + node.nodeId() + "]");
         }
         final ClusterInfo clusterInfo = allocation.clusterInfo();
-        final ImmutableOpenMap<String, DiskUsage> usages = clusterInfo.getNodeLeastAvailableDiskUsages();
+        final Map<String, DiskUsage> usages = clusterInfo.getNodeLeastAvailableDiskUsages();
         final Decision decision = earlyTerminate(allocation, usages);
         if (decision != null) {
             return decision;
@@ -492,7 +488,7 @@ public class DiskThresholdDecider extends AllocationDecider {
         return allocation.decision(Decision.YES, NAME, "enough disk for shard to remain on node, free: [%s]", new ByteSizeValue(freeBytes));
     }
 
-    private DiskUsage getDiskUsage(RoutingNode node, RoutingAllocation allocation, ImmutableOpenMap<String, DiskUsage> usages) {
+    private DiskUsage getDiskUsage(RoutingNode node, RoutingAllocation allocation,  Map<String, DiskUsage> usages) {
         ClusterInfo clusterInfo = allocation.clusterInfo();
         DiskUsage usage = usages.get(node.nodeId());
         if (usage == null) {
@@ -525,15 +521,15 @@ public class DiskThresholdDecider extends AllocationDecider {
      * @param usages Map of nodeId to DiskUsage for all known nodes
      * @return DiskUsage representing given node using the average disk usage
      */
-    public DiskUsage averageUsage(RoutingNode node, ImmutableOpenMap<String, DiskUsage> usages) {
+    public DiskUsage averageUsage(RoutingNode node, Map<String, DiskUsage> usages) {
         if (usages.size() == 0) {
             return new DiskUsage(node.nodeId(), node.node().name(), "_na_", 0, 0);
         }
         long totalBytes = 0;
         long freeBytes = 0;
-        for (ObjectCursor<DiskUsage> du : usages.values()) {
-            totalBytes += du.value.getTotalBytes();
-            freeBytes += du.value.getFreeBytes();
+        for (DiskUsage du : usages.values()) {
+            totalBytes += du.getTotalBytes();
+            freeBytes += du.getFreeBytes();
         }
         return new DiskUsage(node.nodeId(), node.node().name(), "_na_", totalBytes / usages.size(), freeBytes / usages.size());
     }
@@ -596,7 +592,7 @@ public class DiskThresholdDecider extends AllocationDecider {
         }
     }
 
-    private Decision earlyTerminate(RoutingAllocation allocation, ImmutableOpenMap<String, DiskUsage> usages) {
+    private Decision earlyTerminate(RoutingAllocation allocation, final Map<String, DiskUsage> usages) {
         // Always allow allocation if the decider is disabled
         if (!enabled) {
             return allocation.decision(Decision.YES, NAME, "disk threshold decider disabled");
diff --git a/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java b/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
index c230073..74bfac0 100644
--- a/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
@@ -20,16 +20,8 @@
 package org.elasticsearch.cluster.service;
 
 import org.elasticsearch.Version;
-import org.elasticsearch.cluster.AckedClusterStateUpdateTask;
-import org.elasticsearch.cluster.ClusterChangedEvent;
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.*;
 import org.elasticsearch.cluster.ClusterState.Builder;
-import org.elasticsearch.cluster.ClusterStateListener;
-import org.elasticsearch.cluster.ClusterStateUpdateTask;
-import org.elasticsearch.cluster.LocalNodeMasterListener;
-import org.elasticsearch.cluster.TimeoutClusterStateListener;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlocks;
 import org.elasticsearch.cluster.metadata.MetaData;
@@ -49,13 +41,7 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.text.StringText;
 import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
-import org.elasticsearch.common.util.concurrent.CountDown;
-import org.elasticsearch.common.util.concurrent.EsExecutors;
-import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;
-import org.elasticsearch.common.util.concurrent.FutureUtils;
-import org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor;
-import org.elasticsearch.common.util.concurrent.PrioritizedRunnable;
+import org.elasticsearch.common.util.concurrent.*;
 import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.discovery.Discovery;
 import org.elasticsearch.discovery.DiscoveryService;
@@ -63,18 +49,8 @@ import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Queue;
-import java.util.concurrent.ConcurrentMap;
-import java.util.concurrent.CopyOnWriteArrayList;
-import java.util.concurrent.Executor;
-import java.util.concurrent.Future;
-import java.util.concurrent.ScheduledFuture;
-import java.util.concurrent.TimeUnit;
+import java.util.*;
+import java.util.concurrent.*;
 
 import static org.elasticsearch.common.util.concurrent.EsExecutors.daemonThreadFactory;
 
@@ -437,7 +413,7 @@ public class InternalClusterService extends AbstractLifecycleComponent<ClusterSe
                     // only the master controls the version numbers
                     Builder builder = ClusterState.builder(newClusterState).incrementVersion();
                     if (previousClusterState.routingTable() != newClusterState.routingTable()) {
-                        builder.routingTable(RoutingTable.builder(newClusterState.routingTable()).version(newClusterState.routingTable().version() + 1).build());
+                        builder.routingTable(RoutingTable.builder(newClusterState.routingTable()).version(newClusterState.routingTable().version() + 1));
                     }
                     if (previousClusterState.metaData() != newClusterState.metaData()) {
                         builder.metaData(MetaData.builder(newClusterState.metaData()).version(newClusterState.metaData().version() + 1));
diff --git a/core/src/main/java/org/elasticsearch/cluster/settings/DynamicSettings.java b/core/src/main/java/org/elasticsearch/cluster/settings/DynamicSettings.java
index 1935a334..c4137fc 100644
--- a/core/src/main/java/org/elasticsearch/cluster/settings/DynamicSettings.java
+++ b/core/src/main/java/org/elasticsearch/cluster/settings/DynamicSettings.java
@@ -19,21 +19,23 @@
 
 package org.elasticsearch.cluster.settings;
 
-import com.carrotsearch.hppc.cursors.ObjectCursor;
-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
 import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.regex.Regex;
 
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
 /**
  * A container for setting names and validation methods for those settings.
  */
 public class DynamicSettings {
-    private final ImmutableOpenMap<String, Validator> dynamicSettings;
+
+    private final Map<String, Validator> dynamicSettings;
 
     public static class Builder {
-        private ImmutableOpenMap.Builder<String, Validator> settings = ImmutableOpenMap.builder();
+        private Map<String, Validator> settings = new HashMap<>();
 
         public void addSetting(String setting, Validator validator) {
             Validator old = settings.put(setting, validator);
@@ -43,12 +45,12 @@ public class DynamicSettings {
         }
 
         public DynamicSettings build() {
-            return new DynamicSettings(settings.build());
+            return new DynamicSettings(settings);
         }
     }
 
-    private DynamicSettings(ImmutableOpenMap<String, Validator> settings) {
-        this.dynamicSettings = settings;
+    private DynamicSettings(Map<String, Validator> settings) {
+        this.dynamicSettings = Collections.unmodifiableMap(settings);
     }
 
     public boolean isDynamicOrLoggingSetting(String key) {
@@ -56,8 +58,8 @@ public class DynamicSettings {
     }
 
     public boolean hasDynamicSetting(String key) {
-        for (ObjectCursor<String> dynamicSetting : dynamicSettings.keys()) {
-            if (Regex.simpleMatch(dynamicSetting.value, key)) {
+        for (String dynamicSetting : dynamicSettings.keySet()) {
+            if (Regex.simpleMatch(dynamicSetting, key)) {
                 return true;
             }
         }
@@ -65,9 +67,9 @@ public class DynamicSettings {
     }
 
     public String validateDynamicSetting(String dynamicSetting, String value, ClusterState clusterState) {
-        for (ObjectObjectCursor<String, Validator> setting : dynamicSettings) {
-            if (Regex.simpleMatch(setting.key, dynamicSetting)) {
-                return setting.value.validate(dynamicSetting, value, clusterState);
+        for (Map.Entry<String, Validator> setting : dynamicSettings.entrySet()) {
+            if (Regex.simpleMatch(setting.getKey(), dynamicSetting)) {
+                return setting.getValue().validate(dynamicSetting, value, clusterState);
             }
         }
         return null;
diff --git a/core/src/main/java/org/elasticsearch/common/Table.java b/core/src/main/java/org/elasticsearch/common/Table.java
index 6156cc2..fd979cf 100644
--- a/core/src/main/java/org/elasticsearch/common/Table.java
+++ b/core/src/main/java/org/elasticsearch/common/Table.java
@@ -19,13 +19,13 @@
 
 package org.elasticsearch.common;
 
+import com.google.common.collect.ImmutableMap;
+
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.emptyMap;
-
 /**
  */
 public class Table {
@@ -115,7 +115,7 @@ public class Table {
         Map<String, String> mAttr;
         if (attributes.length() == 0) {
             if (inHeaders) {
-                mAttr = emptyMap();
+                mAttr = ImmutableMap.of();
             } else {
                 // get the attributes of the header cell we are going to add to
                 mAttr = headers.get(currentCells.size()).attr;
diff --git a/core/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java b/core/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java
index 7039783..5b9a8bd 100644
--- a/core/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java
+++ b/core/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java
@@ -19,27 +19,22 @@
 
 package org.elasticsearch.common.blobstore.fs;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.common.blobstore.BlobMetaData;
 import org.elasticsearch.common.blobstore.BlobPath;
 import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;
 import org.elasticsearch.common.blobstore.support.PlainBlobMetaData;
 import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.io.Streams;
 
-import java.io.BufferedInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
+import java.io.*;
 import java.nio.file.DirectoryStream;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.StandardCopyOption;
 import java.nio.file.attribute.BasicFileAttributes;
-import java.util.HashMap;
-import java.util.Map;
-
-import static java.util.Collections.unmodifiableMap;
 
 /**
  *
@@ -57,14 +52,14 @@ public class FsBlobContainer extends AbstractBlobContainer {
     }
 
     @Override
-    public Map<String, BlobMetaData> listBlobs() throws IOException {
+    public ImmutableMap<String, BlobMetaData> listBlobs() throws IOException {
         return listBlobsByPrefix(null);
     }
 
     @Override
-    public Map<String, BlobMetaData> listBlobsByPrefix(String blobNamePrefix) throws IOException {
-        // If we get duplicate files we should just take the last entry
-        Map<String, BlobMetaData> builder = new HashMap<>();
+    public ImmutableMap<String, BlobMetaData> listBlobsByPrefix(String blobNamePrefix) throws IOException {
+        // using MapBuilder and not ImmutableMap.Builder as it seems like File#listFiles might return duplicate files!
+        MapBuilder<String, BlobMetaData> builder = MapBuilder.newMapBuilder();
 
         blobNamePrefix = blobNamePrefix == null ? "" : blobNamePrefix;
         try (DirectoryStream<Path> stream = Files.newDirectoryStream(path, blobNamePrefix + "*")) {
@@ -75,7 +70,7 @@ public class FsBlobContainer extends AbstractBlobContainer {
                 }
             }
         }
-        return unmodifiableMap(builder);
+        return builder.immutableMap();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/common/blobstore/url/URLBlobContainer.java b/core/src/main/java/org/elasticsearch/common/blobstore/url/URLBlobContainer.java
index 5bf5521..9dfaa9c 100644
--- a/core/src/main/java/org/elasticsearch/common/blobstore/url/URLBlobContainer.java
+++ b/core/src/main/java/org/elasticsearch/common/blobstore/url/URLBlobContainer.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.common.blobstore.url;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.blobstore.BlobMetaData;
 import org.elasticsearch.common.blobstore.BlobPath;
 import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;
@@ -28,7 +29,6 @@ import java.io.BufferedInputStream;
 import java.io.IOException;
 import java.io.InputStream;
 import java.net.URL;
-import java.util.Map;
 
 /**
  * URL blob implementation of {@link org.elasticsearch.common.blobstore.BlobContainer}
@@ -65,7 +65,7 @@ public class URLBlobContainer extends AbstractBlobContainer {
      * This operation is not supported by URLBlobContainer
      */
     @Override
-    public Map<String, BlobMetaData> listBlobs() throws IOException {
+    public ImmutableMap<String, BlobMetaData> listBlobs() throws IOException {
         throw new UnsupportedOperationException("URL repository doesn't support this operation");
     }
 
@@ -73,7 +73,7 @@ public class URLBlobContainer extends AbstractBlobContainer {
      * This operation is not supported by URLBlobContainer
      */
     @Override
-    public Map<String, BlobMetaData> listBlobsByPrefix(String blobNamePrefix) throws IOException {
+    public ImmutableMap<String, BlobMetaData> listBlobsByPrefix(String blobNamePrefix) throws IOException {
         throw new UnsupportedOperationException("URL repository doesn't support this operation");
     }
 
diff --git a/core/src/main/java/org/elasticsearch/common/cache/Cache.java b/core/src/main/java/org/elasticsearch/common/cache/Cache.java
new file mode 100644
index 0000000..d2d6970
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/cache/Cache.java
@@ -0,0 +1,690 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.cache;
+
+import org.elasticsearch.common.collect.Tuple;
+import org.elasticsearch.common.util.concurrent.ReleasableLock;
+
+import java.util.*;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.atomic.LongAdder;
+import java.util.concurrent.locks.ReadWriteLock;
+import java.util.concurrent.locks.ReentrantLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+import java.util.function.ToLongBiFunction;
+
+/**
+ * A simple concurrent cache.
+ * <p>
+ * Cache is a simple concurrent cache that supports time-based and weight-based evictions, with notifications for all
+ * evictions. The design goals for this cache were simplicity and read performance. This means that we are willing to
+ * accept reduced write performance in exchange for easy-to-understand code. Cache statistics for hits, misses and
+ * evictions are exposed.
+ * <p>
+ * The design of the cache is relatively simple. The cache is segmented into 256 segments which are backed by HashMaps.
+ * Each segment is protected by a re-entrant read/write lock. The read/write locks permit multiple concurrent readers
+ * without contention, and the segments gives us write throughput without impacting readers (so readers are blocked only
+ * if they are reading a segment that a writer is writing to).
+ * <p>
+ * The LRU functionality is backed by a single doubly-linked list chaining the entries in order of insertion. This
+ * LRU list is protected by a lock that serializes all writes to it. There are opportunities for improvements
+ * here if write throughput is a concern.
+ * <ol>
+ * <li>LRU list mutations could be inserted into a blocking queue that a single thread is reading from
+ * and applying to the LRU list.</li>
+ * <li>Promotions could be deferred for entries that were "recently" promoted.</li>
+ * <li>Locks on the list could be taken per node being modified instead of globally.</li>
+ * </ol>
+ * <p>
+ * Evictions only occur after a mutation to the cache (meaning an entry promotion, a cache insertion, or a manual
+ * invalidation) or an explicit call to {@link #refresh()}.
+ *
+ * @param <K> The type of the keys
+ * @param <V> The type of the values
+ */
+public class Cache<K, V> {
+    // positive if entries have an expiration
+    private long expireAfterAccess = -1;
+
+    // true if entries can expire after access
+    private boolean entriesExpireAfterAccess;
+
+    // positive if entries have an expiration after write
+    private long expireAfterWrite = -1;
+
+    // true if entries can expire after initial insertion
+    private boolean entriesExpireAfterWrite;
+
+    // the number of entries in the cache
+    private int count = 0;
+
+    // the weight of the entries in the cache
+    private long weight = 0;
+
+    // the maximum weight that this cache supports
+    private long maximumWeight = -1;
+
+    // the weigher of entries
+    private ToLongBiFunction<K, V> weigher = (k, v) -> 1;
+
+    // the removal callback
+    private RemovalListener<K, V> removalListener = notification -> {
+    };
+
+    // use CacheBuilder to construct
+    Cache() {
+    }
+
+    void setExpireAfterAccess(long expireAfterAccess) {
+        if (expireAfterAccess <= 0) {
+            throw new IllegalArgumentException("expireAfterAccess <= 0");
+        }
+        this.expireAfterAccess = expireAfterAccess;
+        this.entriesExpireAfterAccess = true;
+    }
+
+    void setExpireAfterWrite(long expireAfterWrite) {
+        if (expireAfterWrite <= 0) {
+            throw new IllegalArgumentException("expireAfterWrite <= 0");
+        }
+        this.expireAfterWrite = expireAfterWrite;
+        this.entriesExpireAfterWrite = true;
+    }
+
+    void setMaximumWeight(long maximumWeight) {
+        if (maximumWeight < 0) {
+            throw new IllegalArgumentException("maximumWeight < 0");
+        }
+        this.maximumWeight = maximumWeight;
+    }
+
+    void setWeigher(ToLongBiFunction<K, V> weigher) {
+        Objects.requireNonNull(weigher);
+        this.weigher = weigher;
+    }
+
+    void setRemovalListener(RemovalListener<K, V> removalListener) {
+        Objects.requireNonNull(removalListener);
+        this.removalListener = removalListener;
+    }
+
+    /**
+     * The relative time used to track time-based evictions.
+     *
+     * @return the current relative time
+     */
+    protected long now() {
+        // System.nanoTime takes non-negligible time, so we only use it if we need it
+        // use System.nanoTime because we want relative time, not absolute time
+        return entriesExpireAfterAccess || entriesExpireAfterWrite ? System.nanoTime() : 0;
+    }
+
+    // the state of an entry in the LRU list
+    enum State {
+        NEW, EXISTING, DELETED
+    }
+
+    static class Entry<K, V> {
+        final K key;
+        final V value;
+        long writeTime;
+        volatile long accessTime;
+        Entry<K, V> before;
+        Entry<K, V> after;
+        State state = State.NEW;
+
+        public Entry(K key, V value, long writeTime) {
+            this.key = key;
+            this.value = value;
+            this.writeTime = this.accessTime = writeTime;
+        }
+    }
+
+    /**
+     * A cache segment.
+     * <p>
+     * A CacheSegment is backed by a HashMap and is protected by a read/write lock.
+     *
+     * @param <K> the type of the keys
+     * @param <V> the type of the values
+     */
+    private static class CacheSegment<K, V> {
+        // read/write lock protecting mutations to the segment
+        ReadWriteLock segmentLock = new ReentrantReadWriteLock();
+
+        ReleasableLock readLock = new ReleasableLock(segmentLock.readLock());
+        ReleasableLock writeLock = new ReleasableLock(segmentLock.writeLock());
+
+        Map<K, Entry<K, V>> map = new HashMap<>();
+        SegmentStats segmentStats = new SegmentStats();
+
+        /**
+         * get an entry from the segment
+         *
+         * @param key the key of the entry to get from the cache
+         * @param now the access time of this entry
+         * @return the entry if there was one, otherwise null
+         */
+        Entry<K, V> get(K key, long now) {
+            Entry<K, V> entry;
+            try (ReleasableLock ignored = readLock.acquire()) {
+                entry = map.get(key);
+            }
+            if (entry != null) {
+                segmentStats.hit();
+                entry.accessTime = now;
+            } else {
+                segmentStats.miss();
+            }
+            return entry;
+        }
+
+        /**
+         * put an entry into the segment
+         *
+         * @param key   the key of the entry to add to the cache
+         * @param value the value of the entry to add to the cache
+         * @param now   the access time of this entry
+         * @return a tuple of the new entry and the existing entry, if there was one otherwise null
+         */
+        Tuple<Entry<K, V>, Entry<K, V>> put(K key, V value, long now) {
+            Entry<K, V> entry = new Entry<>(key, value, now);
+            Entry<K, V> existing;
+            try (ReleasableLock ignored = writeLock.acquire()) {
+                existing = map.put(key, entry);
+            }
+            return Tuple.tuple(entry, existing);
+        }
+
+        /**
+         * remove an entry from the segment
+         *
+         * @param key the key of the entry to remove from the cache
+         * @return the removed entry if there was one, otherwise null
+         */
+        Entry<K, V> remove(K key) {
+            Entry<K, V> entry;
+            try (ReleasableLock ignored = writeLock.acquire()) {
+                entry = map.remove(key);
+            }
+            if (entry != null) {
+                segmentStats.eviction();
+            }
+            return entry;
+        }
+
+        private static class SegmentStats {
+            private final LongAdder hits = new LongAdder();
+            private final LongAdder misses = new LongAdder();
+            private final LongAdder evictions = new LongAdder();
+
+            void hit() {
+                hits.increment();
+            }
+
+            void miss() {
+                misses.increment();
+            }
+
+            void eviction() {
+                evictions.increment();
+            }
+        }
+    }
+
+    public static final int NUMBER_OF_SEGMENTS = 256;
+    private final CacheSegment<K, V>[] segments = new CacheSegment[NUMBER_OF_SEGMENTS];
+
+    {
+        for (int i = 0; i < segments.length; i++) {
+            segments[i] = new CacheSegment<>();
+        }
+    }
+
+    Entry<K, V> head;
+    Entry<K, V> tail;
+
+    // lock protecting mutations to the LRU list
+    private ReleasableLock lruLock = new ReleasableLock(new ReentrantLock());
+
+    /**
+     * Returns the value to which the specified key is mapped, or null if this map contains no mapping for the key.
+     *
+     * @param key the key whose associated value is to be returned
+     * @return the value to which the specified key is mapped, or null if this map contains no mapping for the key
+     */
+    public V get(K key) {
+        return get(key, now());
+    }
+
+    private V get(K key, long now) {
+        CacheSegment<K, V> segment = getCacheSegment(key);
+        Entry<K, V> entry = segment.get(key, now);
+        if (entry == null || isExpired(entry, now)) {
+            return null;
+        } else {
+            promote(entry, now);
+            return entry.value;
+        }
+    }
+
+    /**
+     * If the specified key is not already associated with a value (or is mapped to null), attempts to compute its
+     * value using the given mapping function and enters it into this map unless null.
+     *
+     * @param key    the key whose associated value is to be returned or computed for if non-existant
+     * @param loader the function to compute a value given a key
+     * @return the current (existing or computed) value associated with the specified key, or null if the computed
+     * value is null
+     * @throws ExecutionException thrown if loader throws an exception
+     */
+    public V computeIfAbsent(K key, CacheLoader<K, V> loader) throws ExecutionException {
+        long now = now();
+        V value = get(key, now);
+        if (value == null) {
+            CacheSegment<K, V> segment = getCacheSegment(key);
+            // we synchronize against the segment lock; this is to avoid a scenario where another thread is inserting
+            // a value for the same key via put which would not be observed on this thread without a mechanism
+            // synchronizing the two threads; it is possible that the segment lock will be too expensive here (it blocks
+            // readers too!) so consider this as a possible place to optimize should contention be observed
+            try (ReleasableLock ignored = segment.writeLock.acquire()) {
+                value = get(key, now);
+                if (value == null) {
+                    try {
+                        value = loader.load(key);
+                    } catch (Exception e) {
+                        throw new ExecutionException(e);
+                    }
+                    if (value == null) {
+                        throw new ExecutionException(new NullPointerException("loader returned a null value"));
+                    }
+                    put(key, value, now);
+                }
+            }
+        }
+        return value;
+    }
+
+    /**
+     * Associates the specified value with the specified key in this map. If the map previously contained a mapping for
+     * the key, the old value is replaced.
+     *
+     * @param key   key with which the specified value is to be associated
+     * @param value value to be associated with the specified key
+     */
+    public void put(K key, V value) {
+        long now = now();
+        put(key, value, now);
+    }
+
+    private void put(K key, V value, long now) {
+        CacheSegment<K, V> segment = getCacheSegment(key);
+        Tuple<Entry<K, V>, Entry<K, V>> tuple = segment.put(key, value, now);
+        boolean replaced = false;
+        try (ReleasableLock ignored = lruLock.acquire()) {
+            if (tuple.v2() != null && tuple.v2().state == State.EXISTING) {
+                if (unlink(tuple.v2())) {
+                    replaced = true;
+                }
+            }
+            promote(tuple.v1(), now);
+        }
+        if (replaced) {
+            removalListener.onRemoval(new RemovalNotification(tuple.v2().key, tuple.v2().value, RemovalNotification.RemovalReason.REPLACED));
+        }
+    }
+
+    /**
+     * Invalidate the association for the specified key. A removal notification will be issued for invalidated
+     * entries with {@link org.elasticsearch.common.cache.RemovalNotification.RemovalReason} INVALIDATED.
+     *
+     * @param key the key whose mapping is to be invalidated from the cache
+     */
+    public void invalidate(K key) {
+        CacheSegment<K, V> segment = getCacheSegment(key);
+        Entry<K, V> entry = segment.remove(key);
+        if (entry != null) {
+            try (ReleasableLock ignored = lruLock.acquire()) {
+                delete(entry, RemovalNotification.RemovalReason.INVALIDATED);
+            }
+        }
+    }
+
+    /**
+     * Invalidate all cache entries. A removal notification will be issued for invalidated entries with
+     * {@link org.elasticsearch.common.cache.RemovalNotification.RemovalReason} INVALIDATED.
+     */
+    public void invalidateAll() {
+        Entry<K, V> h;
+
+        boolean[] haveSegmentLock = new boolean[NUMBER_OF_SEGMENTS];
+        try {
+            for (int i = 0; i < NUMBER_OF_SEGMENTS; i++) {
+                segments[i].segmentLock.writeLock().lock();
+                haveSegmentLock[i] = true;
+            }
+            try (ReleasableLock ignored = lruLock.acquire()) {
+                h = head;
+                Arrays.stream(segments).forEach(segment -> segment.map = new HashMap<>());
+                Entry<K, V> current = head;
+                while (current != null) {
+                    current.state = State.DELETED;
+                    current = current.after;
+                }
+                head = tail = null;
+                count = 0;
+                weight = 0;
+            }
+        } finally {
+            for (int i = NUMBER_OF_SEGMENTS - 1; i >= 0; i--) {
+                if (haveSegmentLock[i]) {
+                    segments[i].segmentLock.writeLock().unlock();
+                }
+            }
+        }
+        while (h != null) {
+            removalListener.onRemoval(new RemovalNotification<>(h.key, h.value, RemovalNotification.RemovalReason.INVALIDATED));
+            h = h.after;
+        }
+    }
+
+    /**
+     * Force any outstanding size-based and time-based evictions to occur
+     */
+    public void refresh() {
+        long now = now();
+        try (ReleasableLock ignored = lruLock.acquire()) {
+            evict(now);
+        }
+    }
+
+    /**
+     * The number of entries in the cache.
+     *
+     * @return the number of entries in the cache
+     */
+    public int count() {
+        return count;
+    }
+
+    /**
+     * The weight of the entries in the cache.
+     *
+     * @return the weight of the entries in the cache
+     */
+    public long weight() {
+        return weight;
+    }
+
+    /**
+     * An LRU sequencing of the keys in the cache that supports removal. This sequence is not protected from mutations
+     * to the cache (except for {@link Iterator#remove()}. The result of iteration under any other mutation is
+     * undefined.
+     *
+     * @return an LRU-ordered {@link Iterable} over the keys in the cache
+     */
+    public Iterable<K> keys() {
+        return () -> new Iterator<K>() {
+            private CacheIterator iterator = new CacheIterator(head);
+
+            @Override
+            public boolean hasNext() {
+                return iterator.hasNext();
+            }
+
+            @Override
+            public K next() {
+                return iterator.next().key;
+            }
+
+            @Override
+            public void remove() {
+                iterator.remove();
+            }
+        };
+    }
+
+    /**
+     * An LRU sequencing of the values in the cache. This sequence is not protected from mutations
+     * to the cache. The result of iteration under mutation is undefined.
+     *
+     * @return an LRU-ordered {@link Iterable} over the values in the cache
+     */
+    public Iterable<V> values() {
+        return () -> new Iterator<V>() {
+            private CacheIterator iterator = new CacheIterator(head);
+
+            @Override
+            public boolean hasNext() {
+                return iterator.hasNext();
+            }
+
+            @Override
+            public V next() {
+                return iterator.next().value;
+            }
+        };
+    }
+
+    private class CacheIterator implements Iterator<Entry<K, V>> {
+        private Entry<K, V> current;
+        private Entry<K, V> next;
+
+        CacheIterator(Entry<K, V> head) {
+            current = null;
+            next = head;
+        }
+
+        @Override
+        public boolean hasNext() {
+            return next != null;
+        }
+
+        @Override
+        public Entry<K, V> next() {
+            current = next;
+            next = next.after;
+            return current;
+        }
+
+        @Override
+        public void remove() {
+            Entry<K, V> entry = current;
+            if (entry != null) {
+                CacheSegment<K, V> segment = getCacheSegment(entry.key);
+                segment.remove(entry.key);
+                try (ReleasableLock ignored = lruLock.acquire()) {
+                    current = null;
+                    delete(entry, RemovalNotification.RemovalReason.INVALIDATED);
+                }
+            }
+        }
+    }
+
+    /**
+     * The cache statistics tracking hits, misses and evictions. These are taken on a best-effort basis meaning that
+     * they could be out-of-date mid-flight.
+     *
+     * @return the current cache statistics
+     */
+    public CacheStats stats() {
+        long hits = 0;
+        long misses = 0;
+        long evictions = 0;
+        for (int i = 0; i < segments.length; i++) {
+            hits += segments[i].segmentStats.hits.longValue();
+            misses += segments[i].segmentStats.misses.longValue();
+            evictions += segments[i].segmentStats.evictions.longValue();
+        }
+        return new CacheStats(hits, misses, evictions);
+    }
+
+    public static class CacheStats {
+        private long hits;
+        private long misses;
+        private long evictions;
+
+        public CacheStats(long hits, long misses, long evictions) {
+            this.hits = hits;
+            this.misses = misses;
+            this.evictions = evictions;
+        }
+
+        public long getHits() {
+            return hits;
+        }
+
+        public long getMisses() {
+            return misses;
+        }
+
+        public long getEvictions() {
+            return evictions;
+        }
+    }
+
+    private boolean promote(Entry<K, V> entry, long now) {
+        boolean promoted = true;
+        try (ReleasableLock ignored = lruLock.acquire()) {
+            switch (entry.state) {
+                case DELETED:
+                    promoted = false;
+                    break;
+                case EXISTING:
+                    relinkAtHead(entry);
+                    break;
+                case NEW:
+                    linkAtHead(entry);
+                    break;
+            }
+            if (promoted) {
+                evict(now);
+            }
+        }
+        return promoted;
+    }
+
+    private void evict(long now) {
+        assert lruLock.isHeldByCurrentThread();
+
+        while (tail != null && shouldPrune(tail, now)) {
+            CacheSegment<K, V> segment = getCacheSegment(tail.key);
+            Entry<K, V> entry = tail;
+            if (segment != null) {
+                segment.remove(tail.key);
+            }
+            delete(entry, RemovalNotification.RemovalReason.EVICTED);
+        }
+    }
+
+    private void delete(Entry<K, V> entry, RemovalNotification.RemovalReason removalReason) {
+        assert lruLock.isHeldByCurrentThread();
+
+        if (unlink(entry)) {
+            removalListener.onRemoval(new RemovalNotification<>(entry.key, entry.value, removalReason));
+        }
+    }
+
+    private boolean shouldPrune(Entry<K, V> entry, long now) {
+        return exceedsWeight() || isExpired(entry, now);
+    }
+
+    private boolean exceedsWeight() {
+        return maximumWeight != -1 && weight > maximumWeight;
+    }
+
+    private boolean isExpired(Entry<K, V> entry, long now) {
+        return (entriesExpireAfterAccess && now - entry.accessTime > expireAfterAccess) ||
+                (entriesExpireAfterWrite && now - entry.writeTime > expireAfterWrite);
+    }
+
+    private boolean unlink(Entry<K, V> entry) {
+        assert lruLock.isHeldByCurrentThread();
+
+        if (entry.state == State.EXISTING) {
+            final Entry<K, V> before = entry.before;
+            final Entry<K, V> after = entry.after;
+
+            if (before == null) {
+                // removing the head
+                assert head == entry;
+                head = after;
+                if (head != null) {
+                    head.before = null;
+                }
+            } else {
+                // removing inner element
+                before.after = after;
+                entry.before = null;
+            }
+
+            if (after == null) {
+                // removing tail
+                assert tail == entry;
+                tail = before;
+                if (tail != null) {
+                    tail.after = null;
+                }
+            } else {
+                // removing inner element
+                after.before = before;
+                entry.after = null;
+            }
+
+            count--;
+            weight -= weigher.applyAsLong(entry.key, entry.value);
+            entry.state = State.DELETED;
+            return true;
+        } else {
+            return false;
+        }
+    }
+
+    private void linkAtHead(Entry<K, V> entry) {
+        assert lruLock.isHeldByCurrentThread();
+
+        Entry<K, V> h = head;
+        entry.before = null;
+        entry.after = head;
+        head = entry;
+        if (h == null) {
+            tail = entry;
+        } else {
+            h.before = entry;
+        }
+
+        count++;
+        weight += weigher.applyAsLong(entry.key, entry.value);
+        entry.state = State.EXISTING;
+    }
+
+    private void relinkAtHead(Entry<K, V> entry) {
+        assert lruLock.isHeldByCurrentThread();
+
+        if (head != entry) {
+            unlink(entry);
+            linkAtHead(entry);
+        }
+    }
+
+    private CacheSegment<K, V> getCacheSegment(K key) {
+        return segments[key.hashCode() & 0xff];
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/cache/CacheBuilder.java b/core/src/main/java/org/elasticsearch/common/cache/CacheBuilder.java
new file mode 100644
index 0000000..ffb0e59
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/cache/CacheBuilder.java
@@ -0,0 +1,94 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.cache;
+
+import java.util.Objects;
+import java.util.function.ToLongBiFunction;
+
+public class CacheBuilder<K, V> {
+    private long maximumWeight = -1;
+    private long expireAfterAccess = -1;
+    private long expireAfterWrite = -1;
+    private ToLongBiFunction<K, V> weigher;
+    private RemovalListener<K, V> removalListener;
+
+    public static <K, V> CacheBuilder<K, V> builder() {
+        return new CacheBuilder<>();
+    }
+
+    private CacheBuilder() {
+    }
+
+    public CacheBuilder<K, V> setMaximumWeight(long maximumWeight) {
+        if (maximumWeight < 0) {
+            throw new IllegalArgumentException("maximumWeight < 0");
+        }
+        this.maximumWeight = maximumWeight;
+        return this;
+    }
+
+    public CacheBuilder<K, V> setExpireAfterAccess(long expireAfterAccess) {
+        if (expireAfterAccess <= 0) {
+            throw new IllegalArgumentException("expireAfterAccess <= 0");
+        }
+        this.expireAfterAccess = expireAfterAccess;
+        return this;
+    }
+
+    public CacheBuilder<K, V> setExpireAfterWrite(long expireAfterWrite) {
+        if (expireAfterWrite <= 0) {
+            throw new IllegalArgumentException("expireAfterWrite <= 0");
+        }
+        this.expireAfterWrite = expireAfterWrite;
+        return this;
+    }
+
+    public CacheBuilder<K, V> weigher(ToLongBiFunction<K, V> weigher) {
+        Objects.requireNonNull(weigher);
+        this.weigher = weigher;
+        return this;
+    }
+
+    public CacheBuilder<K, V> removalListener(RemovalListener<K, V> removalListener) {
+        Objects.requireNonNull(removalListener);
+        this.removalListener = removalListener;
+        return this;
+    }
+
+    public Cache<K, V> build() {
+        Cache<K, V> cache = new Cache();
+        if (maximumWeight != -1) {
+            cache.setMaximumWeight(maximumWeight);
+        }
+        if (expireAfterAccess != -1) {
+            cache.setExpireAfterAccess(expireAfterAccess);
+        }
+        if (expireAfterWrite != -1) {
+            cache.setExpireAfterWrite(expireAfterWrite);
+        }
+        if (weigher != null) {
+            cache.setWeigher(weigher);
+        }
+        if (removalListener != null) {
+            cache.setRemovalListener(removalListener);
+        }
+        return cache;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/cache/CacheLoader.java b/core/src/main/java/org/elasticsearch/common/cache/CacheLoader.java
new file mode 100644
index 0000000..85636e1
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/cache/CacheLoader.java
@@ -0,0 +1,25 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.cache;
+
+@FunctionalInterface
+public interface CacheLoader<K, V> {
+    V load(K key) throws Exception;
+}
diff --git a/core/src/main/java/org/elasticsearch/common/cache/RemovalListener.java b/core/src/main/java/org/elasticsearch/common/cache/RemovalListener.java
new file mode 100644
index 0000000..ae13300
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/cache/RemovalListener.java
@@ -0,0 +1,25 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.cache;
+
+@FunctionalInterface
+public interface RemovalListener<K, V> {
+    void onRemoval(RemovalNotification<K, V> notification);
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/common/cache/RemovalNotification.java b/core/src/main/java/org/elasticsearch/common/cache/RemovalNotification.java
new file mode 100644
index 0000000..afea5a5
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/cache/RemovalNotification.java
@@ -0,0 +1,46 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.cache;
+
+public class RemovalNotification<K, V> {
+    public enum RemovalReason {REPLACED, INVALIDATED, EVICTED}
+
+    private final K key;
+    private final V value;
+    private final RemovalReason removalReason;
+
+    public RemovalNotification(K key, V value, RemovalReason removalReason) {
+        this.key = key;
+        this.value = value;
+        this.removalReason = removalReason;
+    }
+
+    public K getKey() {
+        return key;
+    }
+
+    public V getValue() {
+        return value;
+    }
+
+    public RemovalReason getRemovalReason() {
+        return removalReason;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/collect/EvictingQueue.java b/core/src/main/java/org/elasticsearch/common/collect/EvictingQueue.java
new file mode 100644
index 0000000..51cc08d
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/collect/EvictingQueue.java
@@ -0,0 +1,176 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.collect;
+
+import java.util.ArrayDeque;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.Queue;
+
+/**
+ * An {@code EvictingQueue} is a non-blocking queue which is limited to a maximum size; when new elements are added to a
+ * full queue, elements are evicted from the head of the queue to accommodate the new elements.
+ *
+ * @param <T> The type of elements in the queue.
+ */
+public class EvictingQueue<T> implements Queue<T> {
+    private final int maximumSize;
+    private final ArrayDeque<T> queue;
+
+    /**
+     * Construct a new {@code EvictingQueue} that holds {@code maximumSize} elements.
+     *
+     * @param maximumSize The maximum number of elements that the queue can hold
+     * @throws IllegalArgumentException if {@code maximumSize} is less than zero
+     */
+    public EvictingQueue(int maximumSize) {
+        if (maximumSize < 0) {
+            throw new IllegalArgumentException("maximumSize < 0");
+        }
+        this.maximumSize = maximumSize;
+        this.queue = new ArrayDeque<>(maximumSize);
+    }
+
+    /**
+     * @return the number of additional elements that the queue can accommodate before evictions occur
+     */
+    public int remainingCapacity() {
+        return this.maximumSize - this.size();
+    }
+
+    /**
+     * Add the given element to the queue, possibly forcing an eviction from the head if {@link #remainingCapacity()} is
+     * zero.
+     *
+     * @param t the element to add
+     * @return true if the element was added (always the case for {@code EvictingQueue}
+     */
+    @Override
+    public boolean add(T t) {
+        if (maximumSize == 0) {
+            return true;
+        }
+        if (queue.size() == maximumSize) {
+            queue.remove();
+        }
+        queue.add(t);
+        return true;
+    }
+
+    /**
+     * @see #add(Object)
+     */
+    @Override
+    public boolean offer(T t) {
+        return add(t);
+    }
+
+    @Override
+    public T remove() {
+        return queue.remove();
+    }
+
+
+    @Override
+    public T poll() {
+        return queue.poll();
+    }
+
+    @Override
+    public T element() {
+        return queue.element();
+    }
+
+    @Override
+    public T peek() {
+        return queue.peek();
+    }
+
+    @Override
+    public int size() {
+        return queue.size();
+    }
+
+    @Override
+    public boolean isEmpty() {
+        return queue.isEmpty();
+    }
+
+    @Override
+    public boolean contains(Object o) {
+        return queue.contains(o);
+    }
+
+    @Override
+    public Iterator<T> iterator() {
+        return queue.iterator();
+    }
+
+    @Override
+    public Object[] toArray() {
+        return queue.toArray();
+    }
+
+    @Override
+    public <T1> T1[] toArray(T1[] a) {
+        return queue.toArray(a);
+    }
+
+    @Override
+    public boolean remove(Object o) {
+        return queue.remove(o);
+    }
+
+    @Override
+    public boolean containsAll(Collection<?> c) {
+        return queue.containsAll(c);
+    }
+
+    /**
+     * Add the given elements to the queue, possibly forcing evictions from the head if {@link #remainingCapacity()} is
+     * zero or becomes zero during the execution of this method.
+     *
+     * @param c the collection of elements to add
+     * @return true if any elements were added to the queue
+     */
+    @Override
+    public boolean addAll(Collection<? extends T> c) {
+        boolean modified = false;
+        for (T e : c)
+            if (add(e))
+                modified = true;
+        return modified;
+    }
+
+    @Override
+    public boolean removeAll(Collection<?> c) {
+        return queue.removeAll(c);
+    }
+
+    @Override
+    public boolean retainAll(Collection<?> c) {
+        return queue.retainAll(c);
+    }
+
+    @Override
+    public void clear() {
+        queue.clear();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/collect/Iterators.java b/core/src/main/java/org/elasticsearch/common/collect/Iterators.java
new file mode 100644
index 0000000..3454612
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/collect/Iterators.java
@@ -0,0 +1,68 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.collect;
+
+import java.util.Iterator;
+import java.util.NoSuchElementException;
+
+public class Iterators {
+    public static <T> Iterator<T> concat(Iterator<? extends T>... iterators) {
+        if (iterators == null) {
+            throw new NullPointerException("iterators");
+        }
+
+        return new ConcatenatedIterator<>(iterators);
+    }
+
+    static class ConcatenatedIterator<T> implements Iterator<T> {
+        private final Iterator<? extends T>[] iterators;
+        private int index = 0;
+
+        public ConcatenatedIterator(Iterator<? extends T>... iterators) {
+            if (iterators == null) {
+                throw new NullPointerException("iterators");
+            }
+            for (int i = 0; i < iterators.length; i++) {
+                if (iterators[i] == null) {
+                    throw new NullPointerException("iterators[" + i  + "]");
+                }
+            }
+            this.iterators = iterators;
+        }
+
+        @Override
+        public boolean hasNext() {
+            boolean hasNext = false;
+            while (index < iterators.length && !(hasNext = iterators[index].hasNext())) {
+                index++;
+            }
+
+            return hasNext;
+        }
+
+        @Override
+        public T next() {
+            if (!hasNext()) {
+                throw new NoSuchElementException();
+            }
+            return iterators[index].next();
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/collect/MapBuilder.java b/core/src/main/java/org/elasticsearch/common/collect/MapBuilder.java
index 2d11198..cafeaad4 100644
--- a/core/src/main/java/org/elasticsearch/common/collect/MapBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/collect/MapBuilder.java
@@ -19,11 +19,11 @@
 
 package org.elasticsearch.common.collect;
 
+import com.google.common.collect.ImmutableMap;
+
 import java.util.HashMap;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  *
  */
@@ -83,15 +83,7 @@ public class MapBuilder<K, V> {
         return this.map;
     }
 
-    /**
-     * Build an immutable copy of the map under construction.
-     *
-     * @deprecated always copies the map under construction. prefer building a
-     *             HashMap by hand and wrapping it in an unmodifiableMap
-     */
-    @Deprecated
-    public Map<K, V> immutableMap() {
-        // Note that this whole method is going to have to go next but we're changing it like this here just to keep the commit smaller.
-        return unmodifiableMap(new HashMap<>(map));
+    public ImmutableMap<K, V> immutableMap() {
+        return ImmutableMap.copyOf(map);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java b/core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java
index c50b85a..0bc9455 100644
--- a/core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java
+++ b/core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java
@@ -19,12 +19,6 @@
 
 package org.elasticsearch.common.geo;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
-
 import org.apache.lucene.util.BitUtil;
 import org.apache.lucene.util.XGeoHashUtils;
 import org.apache.lucene.util.XGeoUtils;
@@ -32,15 +26,12 @@ import org.apache.lucene.util.XGeoUtils;
 /**
  *
  */
-public final class GeoPoint implements Writeable<GeoPoint> {
+public final class GeoPoint {
 
     private double lat;
     private double lon;
     private final static double TOLERANCE = XGeoUtils.TOLERANCE;
     
-    // for serialization purposes
-    private static final GeoPoint PROTOTYPE = new GeoPoint(Double.NaN, Double.NaN);
-
     public GeoPoint() {
     }
 
@@ -179,21 +170,4 @@ public final class GeoPoint implements Writeable<GeoPoint> {
     public static GeoPoint fromIndexLong(long indexLong) {
         return new GeoPoint().resetFromIndexHash(indexLong);
     }
-
-    @Override
-    public GeoPoint readFrom(StreamInput in) throws IOException {
-        double lat = in.readDouble();
-        double lon = in.readDouble();
-        return new GeoPoint(lat, lon);
-    }
-
-    public static GeoPoint readGeoPointFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeDouble(lat);
-        out.writeDouble(lon);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java
index 3837de8..ea5a664 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java
@@ -16,6 +16,8 @@
 
 package org.elasticsearch.common.inject.assistedinject;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.common.inject.ConfigurationException;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.Injector;
@@ -40,7 +42,6 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
-import static java.util.Collections.emptyMap;
 import static java.util.Collections.singleton;
 import static java.util.Collections.unmodifiableSet;
 
@@ -222,7 +223,7 @@ public class FactoryProvider<F> implements Provider<F>, HasDependencies {
         }
 
         if (constructors.isEmpty()) {
-            return emptyMap();
+            return ImmutableMap.of();
         }
 
         Method[] factoryMethods = factoryType.getRawType().getMethods();
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java
index 1f0e05f..0c07e2a 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java
@@ -16,6 +16,8 @@
 
 package org.elasticsearch.common.inject.assistedinject;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.Binder;
 import org.elasticsearch.common.inject.Binding;
@@ -39,11 +41,9 @@ import java.lang.reflect.Proxy;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.inject.internal.Annotations.getKey;
 
 /**
@@ -91,7 +91,7 @@ public final class FactoryProvider2<F> implements InvocationHandler, Provider<F>
      */
     private final Key<?> producedType;
     private final Map<Method, Key<?>> returnTypesByMethod;
-    private final Map<Method, List<Key<?>>> paramTypes;
+    private final ImmutableMap<Method, List<Key<?>>> paramTypes;
 
     /**
      * the hosting injector, or null if we haven't been initialized yet
@@ -117,8 +117,9 @@ public final class FactoryProvider2<F> implements InvocationHandler, Provider<F>
                 Class<F> factoryRawType = (Class) factoryType.getRawType();
 
         try {
-            Map<Method, Key<?>> returnTypesBuilder = new HashMap<>();
-            Map<Method, List<Key<?>>> paramTypesBuilder = new HashMap<>();
+            ImmutableMap.Builder<Method, Key<?>> returnTypesBuilder = ImmutableMap.builder();
+            ImmutableMap.Builder<Method, List<Key<?>>> paramTypesBuilder
+                    = ImmutableMap.builder();
             // TODO: also grab methods from superinterfaces
             for (Method method : factoryRawType.getMethods()) {
                 Key<?> returnType = getKey(
@@ -134,8 +135,8 @@ public final class FactoryProvider2<F> implements InvocationHandler, Provider<F>
                 }
                 paramTypesBuilder.put(method, Collections.unmodifiableList(keys));
             }
-            returnTypesByMethod = unmodifiableMap(returnTypesBuilder);
-            paramTypes = unmodifiableMap(paramTypesBuilder);
+            returnTypesByMethod = returnTypesBuilder.build();
+            paramTypes = paramTypesBuilder.build();
         } catch (ErrorsException e) {
             throw new ConfigurationException(e.getErrors().getMessages());
         }
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/MoreTypes.java b/core/src/main/java/org/elasticsearch/common/inject/internal/MoreTypes.java
index 63d8e40..1b68f2c 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/MoreTypes.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/MoreTypes.java
@@ -17,6 +17,8 @@
 
 package org.elasticsearch.common.inject.internal;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.common.inject.ConfigurationException;
 import org.elasticsearch.common.inject.TypeLiteral;
 import org.elasticsearch.common.inject.spi.Message;
@@ -33,13 +35,11 @@ import java.lang.reflect.Type;
 import java.lang.reflect.TypeVariable;
 import java.lang.reflect.WildcardType;
 import java.util.Arrays;
-import java.util.HashMap;
 import java.util.Map;
 import java.util.NoSuchElementException;
 import java.util.Objects;
 
 import static java.util.Collections.singleton;
-import static java.util.Collections.unmodifiableMap;
 
 /**
  * Static methods for working with types that we aren't publishing in the
@@ -54,20 +54,18 @@ public class MoreTypes {
     private MoreTypes() {
     }
 
-    private static final Map<TypeLiteral<?>, TypeLiteral<?>> PRIMITIVE_TO_WRAPPER;
-    static {
-        Map<TypeLiteral<?>, TypeLiteral<?>> primitiveToWrapper = new HashMap<>();
-        primitiveToWrapper.put(TypeLiteral.get(boolean.class), TypeLiteral.get(Boolean.class));
-        primitiveToWrapper.put(TypeLiteral.get(byte.class), TypeLiteral.get(Byte.class));
-        primitiveToWrapper.put(TypeLiteral.get(short.class), TypeLiteral.get(Short.class));
-        primitiveToWrapper.put(TypeLiteral.get(int.class), TypeLiteral.get(Integer.class));
-        primitiveToWrapper.put(TypeLiteral.get(long.class), TypeLiteral.get(Long.class));
-        primitiveToWrapper.put(TypeLiteral.get(float.class), TypeLiteral.get(Float.class));
-        primitiveToWrapper.put(TypeLiteral.get(double.class), TypeLiteral.get(Double.class));
-        primitiveToWrapper.put(TypeLiteral.get(char.class), TypeLiteral.get(Character.class));
-        primitiveToWrapper.put(TypeLiteral.get(void.class), TypeLiteral.get(Void.class));
-        PRIMITIVE_TO_WRAPPER = unmodifiableMap(primitiveToWrapper);
-    }
+    private static final Map<TypeLiteral<?>, TypeLiteral<?>> PRIMITIVE_TO_WRAPPER
+            = new ImmutableMap.Builder<TypeLiteral<?>, TypeLiteral<?>>()
+            .put(TypeLiteral.get(boolean.class), TypeLiteral.get(Boolean.class))
+            .put(TypeLiteral.get(byte.class), TypeLiteral.get(Byte.class))
+            .put(TypeLiteral.get(short.class), TypeLiteral.get(Short.class))
+            .put(TypeLiteral.get(int.class), TypeLiteral.get(Integer.class))
+            .put(TypeLiteral.get(long.class), TypeLiteral.get(Long.class))
+            .put(TypeLiteral.get(float.class), TypeLiteral.get(Float.class))
+            .put(TypeLiteral.get(double.class), TypeLiteral.get(Double.class))
+            .put(TypeLiteral.get(char.class), TypeLiteral.get(Character.class))
+            .put(TypeLiteral.get(void.class), TypeLiteral.get(Void.class))
+            .build();
 
     /**
      * Returns an equivalent type that's safe for use in a key. The returned type will be free of
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/Nullability.java b/core/src/main/java/org/elasticsearch/common/inject/internal/Nullability.java
index aad0c3a..bb057d6 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/Nullability.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/Nullability.java
@@ -1,3 +1,19 @@
+/*
+ * Copyright (C) 2010 Google Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.elasticsearch.common.inject.internal;
 
 import java.lang.annotation.Annotation;
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java b/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java
index 34cb541..29f7886 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java
@@ -16,6 +16,7 @@
 
 package org.elasticsearch.common.inject.internal;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.inject.Binder;
 import org.elasticsearch.common.inject.Injector;
 import org.elasticsearch.common.inject.Key;
@@ -24,15 +25,7 @@ import org.elasticsearch.common.inject.spi.Element;
 import org.elasticsearch.common.inject.spi.ElementVisitor;
 import org.elasticsearch.common.inject.spi.PrivateElements;
 
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Objects;
-import java.util.Set;
-
-import static java.util.Collections.unmodifiableMap;
+import java.util.*;
 
 /**
  * @author jessewilson@google.com (Jesse Wilson)
@@ -99,7 +92,7 @@ public final class PrivateElementsImpl implements PrivateElements {
             for (ExposureBuilder<?> exposureBuilder : exposureBuilders) {
                 exposedKeysToSourcesMutable.put(exposureBuilder.getKey(), exposureBuilder.getSource());
             }
-            exposedKeysToSources = unmodifiableMap(exposedKeysToSourcesMutable);
+            exposedKeysToSources = ImmutableMap.copyOf(exposedKeysToSourcesMutable);
             exposureBuilders = null;
         }
 
diff --git a/core/src/main/java/org/elasticsearch/common/io/FileSystemUtils.java b/core/src/main/java/org/elasticsearch/common/io/FileSystemUtils.java
index cb5ca5f..e53e7a7 100644
--- a/core/src/main/java/org/elasticsearch/common/io/FileSystemUtils.java
+++ b/core/src/main/java/org/elasticsearch/common/io/FileSystemUtils.java
@@ -19,9 +19,8 @@
 
 package org.elasticsearch.common.io;
 
-import com.google.common.collect.Iterators;
-
 import org.apache.lucene.util.IOUtils;
+import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.common.logging.ESLogger;
 
 import java.io.BufferedReader;
@@ -35,6 +34,7 @@ import java.nio.file.*;
 import java.nio.file.attribute.BasicFileAttributes;
 import java.util.Arrays;
 import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.stream.StreamSupport;
 
 import static java.nio.file.FileVisitResult.CONTINUE;
 import static java.nio.file.FileVisitResult.SKIP_SUBTREE;
@@ -328,7 +328,7 @@ public final class FileSystemUtils {
      */
     public static Path[] files(Path from, DirectoryStream.Filter<Path> filter) throws IOException {
         try (DirectoryStream<Path> stream = Files.newDirectoryStream(from, filter)) {
-            return Iterators.toArray(stream.iterator(), Path.class);
+            return toArray(stream);
         }
     }
 
@@ -337,7 +337,7 @@ public final class FileSystemUtils {
      */
     public static Path[] files(Path directory) throws IOException {
         try (DirectoryStream<Path> stream = Files.newDirectoryStream(directory)) {
-            return Iterators.toArray(stream.iterator(), Path.class);
+            return toArray(stream);
         }
     }
 
@@ -346,8 +346,12 @@ public final class FileSystemUtils {
      */
     public static Path[] files(Path directory, String glob) throws IOException {
         try (DirectoryStream<Path> stream = Files.newDirectoryStream(directory, glob)) {
-            return Iterators.toArray(stream.iterator(), Path.class);
+            return toArray(stream);
         }
     }
 
+    private static Path[] toArray(DirectoryStream<Path> stream) {
+        return StreamSupport.stream(stream.spliterator(), false).toArray(length -> new Path[length]);
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
index 17d9995..a6fc091 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
@@ -31,6 +31,7 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.text.StringAndBytesText;
 import org.elasticsearch.common.text.Text;
 import org.elasticsearch.index.query.QueryBuilder;
@@ -336,19 +337,6 @@ public abstract class StreamInput extends InputStream {
     @Override
     public abstract void close() throws IOException;
 
-//    // IS
-//
-//    @Override public int read() throws IOException {
-//        return readByte();
-//    }
-//
-//    // Here, we assume that we always can read the full byte array
-//
-//    @Override public int read(byte[] b, int off, int len) throws IOException {
-//        readBytes(b, off, len);
-//        return len;
-//    }
-
     public String[] readStringArray() throws IOException {
         int size = readVInt();
         if (size == 0) {
@@ -449,11 +437,20 @@ public abstract class StreamInput extends InputStream {
                 return readDoubleArray();
             case 21:
                 return readBytesRef();
+            case 22:
+                return readGeoPoint();
             default:
                 throw new IOException("Can't read unknown type [" + type + "]");
         }
     }
 
+    /**
+     * Reads a {@link GeoPoint} from this stream input
+     */
+    public GeoPoint readGeoPoint() throws IOException {
+        return new GeoPoint(readDouble(), readDouble());
+    }
+
     public int[] readIntArray() throws IOException {
         int length = readVInt();
         int[] values = new int[length];
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
index 16128e4..3e4aabb 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
@@ -30,6 +30,7 @@ import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.text.Text;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
@@ -422,6 +423,9 @@ public abstract class StreamOutput extends OutputStream {
         } else if (value instanceof BytesRef) {
             writeByte((byte) 21);
             writeBytesRef((BytesRef) value);
+        } else if (type == GeoPoint.class) {
+            writeByte((byte) 22);
+            writeGeoPoint((GeoPoint) value);
         } else {
             throw new IOException("Can't write type [" + type + "]");
         }
@@ -467,14 +471,6 @@ public abstract class StreamOutput extends OutputStream {
         }
     }
 
-    private static int parseIntSafe(String val, int defaultVal) {
-        try {
-            return Integer.parseInt(val);
-        } catch (NumberFormatException ex) {
-            return defaultVal;
-        }
-    }
-
     public void writeThrowable(Throwable throwable) throws IOException {
         if (throwable == null) {
             writeBoolean(false);
@@ -596,4 +592,12 @@ public abstract class StreamOutput extends OutputStream {
     public void writeScoreFunction(ScoreFunctionBuilder<?> scoreFunctionBuilder) throws IOException {
         writeNamedWriteable(scoreFunctionBuilder);
     }
+
+    /**
+     * Writes the given {@link GeoPoint} to the stream
+     */
+    public void writeGeoPoint(GeoPoint geoPoint) throws IOException {
+        writeDouble(geoPoint.lat());
+        writeDouble(geoPoint.lon());
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java b/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java
index 5e7517e..a715d34 100644
--- a/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java
+++ b/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java
@@ -19,8 +19,10 @@
 
 package org.elasticsearch.common.logging.log4j;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.log4j.PropertyConfigurator;
 import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.settings.SettingsException;
 import org.elasticsearch.env.Environment;
@@ -34,12 +36,10 @@ import java.nio.file.SimpleFileVisitor;
 import java.nio.file.attribute.BasicFileAttributes;
 import java.util.Arrays;
 import java.util.EnumSet;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
 
-import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.Strings.cleanPath;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 
@@ -50,43 +50,40 @@ public class LogConfigurator {
 
     static final List<String> ALLOWED_SUFFIXES = Arrays.asList(".yml", ".yaml", ".json", ".properties");
 
-    private static final Map<String, String> REPLACEMENTS;
-    static {
-        Map<String, String> replacements = new HashMap<>();
-        replacements.put("console", "org.elasticsearch.common.logging.log4j.ConsoleAppender");
-        replacements.put("async", "org.apache.log4j.AsyncAppender");
-        replacements.put("dailyRollingFile", "org.apache.log4j.DailyRollingFileAppender");
-        replacements.put("externallyRolledFile", "org.apache.log4j.ExternallyRolledFileAppender");
-        replacements.put("file", "org.apache.log4j.FileAppender");
-        replacements.put("jdbc", "org.apache.log4j.jdbc.JDBCAppender");
-        replacements.put("jms", "org.apache.log4j.net.JMSAppender");
-        replacements.put("lf5", "org.apache.log4j.lf5.LF5Appender");
-        replacements.put("ntevent", "org.apache.log4j.nt.NTEventLogAppender");
-        replacements.put("null", "org.apache.log4j.NullAppender");
-        replacements.put("rollingFile", "org.apache.log4j.RollingFileAppender");
-        replacements.put("extrasRollingFile", "org.apache.log4j.rolling.RollingFileAppender");
-        replacements.put("smtp", "org.apache.log4j.net.SMTPAppender");
-        replacements.put("socket", "org.apache.log4j.net.SocketAppender");
-        replacements.put("socketHub", "org.apache.log4j.net.SocketHubAppender");
-        replacements.put("syslog", "org.apache.log4j.net.SyslogAppender");
-        replacements.put("telnet", "org.apache.log4j.net.TelnetAppender");
-        replacements.put("terminal", "org.elasticsearch.common.logging.log4j.TerminalAppender");
-                // policies
-        replacements.put("timeBased", "org.apache.log4j.rolling.TimeBasedRollingPolicy");
-        replacements.put("sizeBased", "org.apache.log4j.rolling.SizeBasedTriggeringPolicy");
-                // layouts
-        replacements.put("simple", "org.apache.log4j.SimpleLayout");
-        replacements.put("html", "org.apache.log4j.HTMLLayout");
-        replacements.put("pattern", "org.apache.log4j.PatternLayout");
-        replacements.put("consolePattern", "org.apache.log4j.PatternLayout");
-        replacements.put("enhancedPattern", "org.apache.log4j.EnhancedPatternLayout");
-        replacements.put("ttcc", "org.apache.log4j.TTCCLayout");
-        replacements.put("xml", "org.apache.log4j.XMLLayout");
-        REPLACEMENTS = unmodifiableMap(replacements);
-    }
-
     private static boolean loaded;
 
+    private static ImmutableMap<String, String> replacements = new MapBuilder<String, String>()
+            .put("console", "org.elasticsearch.common.logging.log4j.ConsoleAppender")
+            .put("async", "org.apache.log4j.AsyncAppender")
+            .put("dailyRollingFile", "org.apache.log4j.DailyRollingFileAppender")
+            .put("externallyRolledFile", "org.apache.log4j.ExternallyRolledFileAppender")
+            .put("file", "org.apache.log4j.FileAppender")
+            .put("jdbc", "org.apache.log4j.jdbc.JDBCAppender")
+            .put("jms", "org.apache.log4j.net.JMSAppender")
+            .put("lf5", "org.apache.log4j.lf5.LF5Appender")
+            .put("ntevent", "org.apache.log4j.nt.NTEventLogAppender")
+            .put("null", "org.apache.log4j.NullAppender")
+            .put("rollingFile", "org.apache.log4j.RollingFileAppender")
+            .put("extrasRollingFile", "org.apache.log4j.rolling.RollingFileAppender")
+            .put("smtp", "org.apache.log4j.net.SMTPAppender")
+            .put("socket", "org.apache.log4j.net.SocketAppender")
+            .put("socketHub", "org.apache.log4j.net.SocketHubAppender")
+            .put("syslog", "org.apache.log4j.net.SyslogAppender")
+            .put("telnet", "org.apache.log4j.net.TelnetAppender")
+            .put("terminal", "org.elasticsearch.common.logging.log4j.TerminalAppender")
+                    // policies
+            .put("timeBased", "org.apache.log4j.rolling.TimeBasedRollingPolicy")
+            .put("sizeBased", "org.apache.log4j.rolling.SizeBasedTriggeringPolicy")
+                    // layouts
+            .put("simple", "org.apache.log4j.SimpleLayout")
+            .put("html", "org.apache.log4j.HTMLLayout")
+            .put("pattern", "org.apache.log4j.PatternLayout")
+            .put("consolePattern", "org.apache.log4j.PatternLayout")
+            .put("enhancedPattern", "org.apache.log4j.EnhancedPatternLayout")
+            .put("ttcc", "org.apache.log4j.TTCCLayout")
+            .put("xml", "org.apache.log4j.XMLLayout")
+            .immutableMap();
+
     /**
      * Consolidates settings and converts them into actual log4j settings, then initializes loggers and appenders.
      *
@@ -115,7 +112,9 @@ public class LogConfigurator {
         for (Map.Entry<String, String> entry : settingsBuilder.build().getAsMap().entrySet()) {
             String key = "log4j." + entry.getKey();
             String value = entry.getValue();
-            value = REPLACEMENTS.getOrDefault(value, value);
+            if (replacements.containsKey(value)) {
+                value = replacements.get(value);
+            }
             if (key.endsWith(".value")) {
                 props.setProperty(key.substring(0, key.length() - ".value".length()), value);
             } else if (key.endsWith(".type")) {
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java b/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java
index 060482e..3aaaf96 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java
@@ -46,14 +46,11 @@ import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.index.analysis.AnalyzerScope;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
 import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.text.ParseException;
 import java.util.*;
 
-import static org.elasticsearch.common.lucene.search.NoopCollector.NOOP_COLLECTOR;
-
 /**
  *
  */
@@ -229,27 +226,6 @@ public class Lucene {
         }.run();
     }
 
-    public static long count(IndexSearcher searcher, Query query) throws IOException {
-        return searcher.count(query);
-    }
-
-    /**
-     * Performs a count on the <code>searcher</code> for <code>query</code>. Terminates
-     * early when the count has reached <code>terminateAfter</code>
-     */
-    public static long count(IndexSearcher searcher, Query query, int terminateAfterCount) throws IOException {
-        EarlyTerminatingCollector countCollector = createCountBasedEarlyTerminatingCollector(terminateAfterCount);
-        countWithEarlyTermination(searcher, query, countCollector);
-        return countCollector.count();
-    }
-
-    /**
-     * Creates count based early termination collector with a threshold of <code>maxCountHits</code>
-     */
-    public final static EarlyTerminatingCollector createCountBasedEarlyTerminatingCollector(int maxCountHits) {
-        return new EarlyTerminatingCollector(maxCountHits);
-    }
-
     /**
      * Wraps <code>delegate</code> with count based early termination collector with a threshold of <code>maxCountHits</code>
      */
@@ -265,99 +241,27 @@ public class Lucene {
     }
 
     /**
-     * Performs an exists (count &gt; 0) query on the <code>searcher</code> for <code>query</code>
-     * with <code>filter</code> using the given <code>collector</code>
-     *
-     * The <code>collector</code> can be instantiated using <code>Lucene.createExistsCollector()</code>
+     * Check whether there is one or more documents matching the provided query.
      */
-    public static boolean exists(IndexSearcher searcher, Query query, Filter filter,
-                                 EarlyTerminatingCollector collector) throws IOException {
-        collector.reset();
-        countWithEarlyTermination(searcher, filter, query, collector);
-        return collector.exists();
-    }
-
-
-    /**
-     * Performs an exists (count &gt; 0) query on the <code>searcher</code> for <code>query</code>
-     * using the given <code>collector</code>
-     *
-     * The <code>collector</code> can be instantiated using <code>Lucene.createExistsCollector()</code>
-     */
-    public static boolean exists(IndexSearcher searcher, Query query, EarlyTerminatingCollector collector) throws IOException {
-        collector.reset();
-        countWithEarlyTermination(searcher, query, collector);
-        return collector.exists();
-    }
-
-    /**
-     * Calls <code>countWithEarlyTermination(searcher, null, query, collector)</code>
-     */
-    public static boolean countWithEarlyTermination(IndexSearcher searcher, Query query,
-                                                  EarlyTerminatingCollector collector) throws IOException {
-        return countWithEarlyTermination(searcher, null, query, collector);
-    }
-
-    /**
-     * Performs a count on <code>query</code> and <code>filter</code> with early termination using <code>searcher</code>.
-     * The early termination threshold is specified by the provided <code>collector</code>
-     */
-    public static boolean countWithEarlyTermination(IndexSearcher searcher, Filter filter, Query query,
-                                                        EarlyTerminatingCollector collector) throws IOException {
-        try {
-            if (filter == null) {
-                searcher.search(query, collector);
-            } else {
-                searcher.search(query, filter, collector);
+    public static boolean exists(IndexSearcher searcher, Query query) throws IOException {
+        final Weight weight = searcher.createNormalizedWeight(query, false);
+        // the scorer API should be more efficient at stopping after the first
+        // match than the bulk scorer API
+        for (LeafReaderContext context : searcher.getIndexReader().leaves()) {
+            final Scorer scorer = weight.scorer(context);
+            if (scorer == null) {
+                continue;
+            }
+            final Bits liveDocs = context.reader().getLiveDocs();
+            for (int doc = scorer.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = scorer.nextDoc()) {
+                if (liveDocs == null || liveDocs.get(doc)) {
+                    return true;
+                }
             }
-        } catch (EarlyTerminationException e) {
-            // early termination
-            return true;
         }
         return false;
     }
 
-    /**
-     * Performs an exists (count &gt; 0) query on the searcher from the <code>searchContext</code> for <code>query</code>
-     * using the given <code>collector</code>
-     *
-     * The <code>collector</code> can be instantiated using <code>Lucene.createExistsCollector()</code>
-     */
-    public static boolean exists(SearchContext searchContext, Query query, EarlyTerminatingCollector collector) throws IOException {
-        collector.reset();
-        try {
-            searchContext.searcher().search(query, collector);
-        } catch (EarlyTerminationException e) {
-            // ignore, just early termination...
-        } finally {
-            searchContext.clearReleasables(SearchContext.Lifetime.COLLECTION);
-        }
-        return collector.exists();
-    }
-
-    /**
-     * Creates an {@link org.elasticsearch.common.lucene.Lucene.EarlyTerminatingCollector}
-     * with a threshold of <code>1</code>
-     */
-    public final static EarlyTerminatingCollector createExistsCollector() {
-        return createCountBasedEarlyTerminatingCollector(1);
-    }
-
-    /**
-     * Closes the index writer, returning <tt>false</tt> if it failed to close.
-     */
-    public static boolean safeClose(IndexWriter writer) {
-        if (writer == null) {
-            return true;
-        }
-        try {
-            writer.close();
-            return true;
-        } catch (Throwable e) {
-            return false;
-        }
-    }
-
     public static TopDocs readTopDocs(StreamInput in) throws IOException {
         if (in.readBoolean()) {
             int totalHits = in.readVInt();
@@ -612,19 +516,11 @@ public class Lucene {
         private int count = 0;
         private LeafCollector leafCollector;
 
-        EarlyTerminatingCollector(int maxCountHits) {
-            this.maxCountHits = maxCountHits;
-            this.delegate = NOOP_COLLECTOR;
-        }
-
         EarlyTerminatingCollector(final Collector delegate, int maxCountHits) {
             this.maxCountHits = maxCountHits;
-            this.delegate = (delegate == null) ? NOOP_COLLECTOR : delegate;
+            this.delegate = Objects.requireNonNull(delegate);
         }
 
-        public void reset() {
-            count = 0;
-        }
         public int count() {
             return count;
         }
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/NoopCollector.java b/core/src/main/java/org/elasticsearch/common/lucene/search/NoopCollector.java
deleted file mode 100644
index 99845ea..0000000
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/NoopCollector.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.lucene.search;
-
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.SimpleCollector;
-
-import java.io.IOException;
-
-/**
- *
- */
-public class NoopCollector extends SimpleCollector {
-
-    public static final NoopCollector NOOP_COLLECTOR = new NoopCollector();
-
-    @Override
-    public void setScorer(Scorer scorer) throws IOException {
-    }
-
-    @Override
-    public void collect(int doc) throws IOException {
-    }
-
-    @Override
-    protected void doSetNextReader(LeafReaderContext context) throws IOException {
-    }
-
-    @Override
-    public boolean needsScores() {
-        return false;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/uid/Versions.java b/core/src/main/java/org/elasticsearch/common/lucene/uid/Versions.java
index 77eb218..55586d8 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/uid/Versions.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/uid/Versions.java
@@ -33,10 +33,24 @@ import java.util.concurrent.ConcurrentMap;
 /** Utility class to resolve the Lucene doc ID and version for a given uid. */
 public class Versions {
 
-    public static final long MATCH_ANY = -3L; // Version was not specified by the user
+    /** used to indicate the write operation should succeed regardless of current version **/
+    public static final long MATCH_ANY = -3L;
+
+    /** indicates that the current document was not found in lucene and in the version map */
     public static final long NOT_FOUND = -1L;
+
+    /**
+     * used when the document is old and doesn't contain any version information in the index
+     * see {@link PerThreadIDAndVersionLookup#lookup(org.apache.lucene.util.BytesRef)}
+     */
     public static final long NOT_SET = -2L;
 
+    /**
+     * used to indicate that the write operation should be executed if the document is currently deleted
+     * i.e., not found in the index and/or found as deleted (with version) in the version map
+     */
+    public static final long MATCH_DELETED = -4L;
+
     // TODO: is there somewhere else we can store these?
     private static final ConcurrentMap<IndexReader, CloseableThreadLocal<PerThreadIDAndVersionLookup>> lookupStates = ConcurrentCollections.newConcurrentMapWithAggressiveConcurrency();
 
diff --git a/core/src/main/java/org/elasticsearch/common/network/InetAddresses.java b/core/src/main/java/org/elasticsearch/common/network/InetAddresses.java
new file mode 100644
index 0000000..4d3d140
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/network/InetAddresses.java
@@ -0,0 +1,357 @@
+/*
+ * Copyright (C) 2008 The Guava Authors
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.elasticsearch.common.network;
+
+import java.net.Inet4Address;
+import java.net.Inet6Address;
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Locale;
+
+public class InetAddresses {
+    private static int IPV4_PART_COUNT = 4;
+    private static int IPV6_PART_COUNT = 8;
+
+    public static boolean isInetAddress(String ipString) {
+        return ipStringToBytes(ipString) != null;
+    }
+
+    private static byte[] ipStringToBytes(String ipString) {
+        // Make a first pass to categorize the characters in this string.
+        boolean hasColon = false;
+        boolean hasDot = false;
+        for (int i = 0; i < ipString.length(); i++) {
+            char c = ipString.charAt(i);
+            if (c == '.') {
+                hasDot = true;
+            } else if (c == ':') {
+                if (hasDot) {
+                    return null;  // Colons must not appear after dots.
+                }
+                hasColon = true;
+            } else if (Character.digit(c, 16) == -1) {
+                return null;  // Everything else must be a decimal or hex digit.
+            }
+        }
+
+        // Now decide which address family to parse.
+        if (hasColon) {
+            if (hasDot) {
+                ipString = convertDottedQuadToHex(ipString);
+                if (ipString == null) {
+                    return null;
+                }
+            }
+            return textToNumericFormatV6(ipString);
+        } else if (hasDot) {
+            return textToNumericFormatV4(ipString);
+        }
+        return null;
+    }
+
+    private static String convertDottedQuadToHex(String ipString) {
+        int lastColon = ipString.lastIndexOf(':');
+        String initialPart = ipString.substring(0, lastColon + 1);
+        String dottedQuad = ipString.substring(lastColon + 1);
+        byte[] quad = textToNumericFormatV4(dottedQuad);
+        if (quad == null) {
+            return null;
+        }
+        String penultimate = Integer.toHexString(((quad[0] & 0xff) << 8) | (quad[1] & 0xff));
+        String ultimate = Integer.toHexString(((quad[2] & 0xff) << 8) | (quad[3] & 0xff));
+        return initialPart + penultimate + ":" + ultimate;
+    }
+
+    private static byte[] textToNumericFormatV4(String ipString) {
+        String[] address = ipString.split("\\.", IPV4_PART_COUNT + 1);
+        if (address.length != IPV4_PART_COUNT) {
+            return null;
+        }
+
+        byte[] bytes = new byte[IPV4_PART_COUNT];
+        try {
+            for (int i = 0; i < bytes.length; i++) {
+                bytes[i] = parseOctet(address[i]);
+            }
+        } catch (NumberFormatException ex) {
+            return null;
+        }
+
+        return bytes;
+    }
+
+    private static byte parseOctet(String ipPart) {
+        // Note: we already verified that this string contains only hex digits.
+        int octet = Integer.parseInt(ipPart);
+        // Disallow leading zeroes, because no clear standard exists on
+        // whether these should be interpreted as decimal or octal.
+        if (octet > 255 || (ipPart.startsWith("0") && ipPart.length() > 1)) {
+            throw new NumberFormatException();
+        }
+        return (byte) octet;
+    }
+
+    private static byte[] textToNumericFormatV6(String ipString) {
+        // An address can have [2..8] colons, and N colons make N+1 parts.
+        String[] parts = ipString.split(":", IPV6_PART_COUNT + 2);
+        if (parts.length < 3 || parts.length > IPV6_PART_COUNT + 1) {
+            return null;
+        }
+
+        // Disregarding the endpoints, find "::" with nothing in between.
+        // This indicates that a run of zeroes has been skipped.
+        int skipIndex = -1;
+        for (int i = 1; i < parts.length - 1; i++) {
+            if (parts[i].length() == 0) {
+                if (skipIndex >= 0) {
+                    return null;  // Can't have more than one ::
+                }
+                skipIndex = i;
+            }
+        }
+
+        int partsHi;  // Number of parts to copy from above/before the "::"
+        int partsLo;  // Number of parts to copy from below/after the "::"
+        if (skipIndex >= 0) {
+            // If we found a "::", then check if it also covers the endpoints.
+            partsHi = skipIndex;
+            partsLo = parts.length - skipIndex - 1;
+            if (parts[0].length() == 0 && --partsHi != 0) {
+                return null;  // ^: requires ^::
+            }
+            if (parts[parts.length - 1].length() == 0 && --partsLo != 0) {
+                return null;  // :$ requires ::$
+            }
+        } else {
+            // Otherwise, allocate the entire address to partsHi.  The endpoints
+            // could still be empty, but parseHextet() will check for that.
+            partsHi = parts.length;
+            partsLo = 0;
+        }
+
+        // If we found a ::, then we must have skipped at least one part.
+        // Otherwise, we must have exactly the right number of parts.
+        int partsSkipped = IPV6_PART_COUNT - (partsHi + partsLo);
+        if (!(skipIndex >= 0 ? partsSkipped >= 1 : partsSkipped == 0)) {
+            return null;
+        }
+
+        // Now parse the hextets into a byte array.
+        ByteBuffer rawBytes = ByteBuffer.allocate(2 * IPV6_PART_COUNT);
+        try {
+            for (int i = 0; i < partsHi; i++) {
+                rawBytes.putShort(parseHextet(parts[i]));
+            }
+            for (int i = 0; i < partsSkipped; i++) {
+                rawBytes.putShort((short) 0);
+            }
+            for (int i = partsLo; i > 0; i--) {
+                rawBytes.putShort(parseHextet(parts[parts.length - i]));
+            }
+        } catch (NumberFormatException ex) {
+            return null;
+        }
+        return rawBytes.array();
+    }
+
+    private static short parseHextet(String ipPart) {
+        // Note: we already verified that this string contains only hex digits.
+        int hextet = Integer.parseInt(ipPart, 16);
+        if (hextet > 0xffff) {
+            throw new NumberFormatException();
+        }
+        return (short) hextet;
+    }
+
+    /**
+     * Returns the string representation of an {@link InetAddress} suitable
+     * for inclusion in a URI.
+     *
+     * <p>For IPv4 addresses, this is identical to
+     * {@link InetAddress#getHostAddress()}, but for IPv6 addresses it
+     * compresses zeroes and surrounds the text with square brackets; for example
+     * {@code "[2001:db8::1]"}.
+     *
+     * <p>Per section 3.2.2 of
+     * <a target="_parent"
+     *    href="http://tools.ietf.org/html/rfc3986#section-3.2.2"
+     *  >http://tools.ietf.org/html/rfc3986</a>,
+     * a URI containing an IPv6 string literal is of the form
+     * {@code "http://[2001:db8::1]:8888/index.html"}.
+     *
+     * <p>Use of either {@link InetAddresses#toAddrString},
+     * {@link InetAddress#getHostAddress()}, or this method is recommended over
+     * {@link InetAddress#toString()} when an IP address string literal is
+     * desired.  This is because {@link InetAddress#toString()} prints the
+     * hostname and the IP address string joined by a "/".
+     *
+     * @param ip {@link InetAddress} to be converted to URI string literal
+     * @return {@code String} containing URI-safe string literal
+     */
+    public static String toUriString(InetAddress ip) {
+        if (ip instanceof Inet6Address) {
+            return "[" + toAddrString(ip) + "]";
+        }
+        return toAddrString(ip);
+    }
+
+    /**
+     * Returns the string representation of an {@link InetAddress}.
+     *
+     * <p>For IPv4 addresses, this is identical to
+     * {@link InetAddress#getHostAddress()}, but for IPv6 addresses, the output
+     * follows <a href="http://tools.ietf.org/html/rfc5952">RFC 5952</a>
+     * section 4.  The main difference is that this method uses "::" for zero
+     * compression, while Java's version uses the uncompressed form.
+     *
+     * <p>This method uses hexadecimal for all IPv6 addresses, including
+     * IPv4-mapped IPv6 addresses such as "::c000:201".  The output does not
+     * include a Scope ID.
+     *
+     * @param ip {@link InetAddress} to be converted to an address string
+     * @return {@code String} containing the text-formatted IP address
+     * @since 10.0
+     */
+    public static String toAddrString(InetAddress ip) {
+        if (ip == null) {
+            throw new NullPointerException("ip");
+        }
+        if (ip instanceof Inet4Address) {
+            // For IPv4, Java's formatting is good enough.
+            byte[] bytes = ip.getAddress();
+            return (bytes[0] & 0xff) + "." + (bytes[1] & 0xff) + "." + (bytes[2] & 0xff) + "." + (bytes[3] & 0xff);
+        }
+        if (!(ip instanceof Inet6Address)) {
+            throw new IllegalArgumentException("ip");
+        }
+        byte[] bytes = ip.getAddress();
+        int[] hextets = new int[IPV6_PART_COUNT];
+        for (int i = 0; i < hextets.length; i++) {
+            hextets[i] =  (bytes[2 * i] & 255) << 8 | bytes[2 * i + 1] & 255;
+        }
+        compressLongestRunOfZeroes(hextets);
+        return hextetsToIPv6String(hextets);
+    }
+
+    /**
+     * Identify and mark the longest run of zeroes in an IPv6 address.
+     *
+     * <p>Only runs of two or more hextets are considered.  In case of a tie, the
+     * leftmost run wins.  If a qualifying run is found, its hextets are replaced
+     * by the sentinel value -1.
+     *
+     * @param hextets {@code int[]} mutable array of eight 16-bit hextets
+     */
+    private static void compressLongestRunOfZeroes(int[] hextets) {
+        int bestRunStart = -1;
+        int bestRunLength = -1;
+        int runStart = -1;
+        for (int i = 0; i < hextets.length + 1; i++) {
+            if (i < hextets.length && hextets[i] == 0) {
+                if (runStart < 0) {
+                    runStart = i;
+                }
+            } else if (runStart >= 0) {
+                int runLength = i - runStart;
+                if (runLength > bestRunLength) {
+                    bestRunStart = runStart;
+                    bestRunLength = runLength;
+                }
+                runStart = -1;
+            }
+        }
+        if (bestRunLength >= 2) {
+            Arrays.fill(hextets, bestRunStart, bestRunStart + bestRunLength, -1);
+        }
+    }
+
+    /**
+     * Convert a list of hextets into a human-readable IPv6 address.
+     *
+     * <p>In order for "::" compression to work, the input should contain negative
+     * sentinel values in place of the elided zeroes.
+     *
+     * @param hextets {@code int[]} array of eight 16-bit hextets, or -1s
+     */
+    private static String hextetsToIPv6String(int[] hextets) {
+    /*
+     * While scanning the array, handle these state transitions:
+     *   start->num => "num"     start->gap => "::"
+     *   num->num   => ":num"    num->gap   => "::"
+     *   gap->num   => "num"     gap->gap   => ""
+     */
+        StringBuilder buf = new StringBuilder(39);
+        boolean lastWasNumber = false;
+        for (int i = 0; i < hextets.length; i++) {
+            boolean thisIsNumber = hextets[i] >= 0;
+            if (thisIsNumber) {
+                if (lastWasNumber) {
+                    buf.append(':');
+                }
+                buf.append(Integer.toHexString(hextets[i]));
+            } else {
+                if (i == 0 || lastWasNumber) {
+                    buf.append("::");
+                }
+            }
+            lastWasNumber = thisIsNumber;
+        }
+        return buf.toString();
+    }
+
+    /**
+     * Returns the {@link InetAddress} having the given string representation.
+     *
+     * <p>This deliberately avoids all nameservice lookups (e.g. no DNS).
+     *
+     * @param ipString {@code String} containing an IPv4 or IPv6 string literal, e.g.
+     *     {@code "192.168.0.1"} or {@code "2001:db8::1"}
+     * @return {@link InetAddress} representing the argument
+     * @throws IllegalArgumentException if the argument is not a valid IP string literal
+     */
+    public static InetAddress forString(String ipString) {
+        byte[] addr = ipStringToBytes(ipString);
+
+        // The argument was malformed, i.e. not an IP string literal.
+        if (addr == null) {
+            throw new IllegalArgumentException(String.format(Locale.ROOT, "'%s' is not an IP string literal.", ipString));
+        }
+
+        return bytesToInetAddress(addr);
+    }
+
+    /**
+     * Convert a byte array into an InetAddress.
+     *
+     * {@link InetAddress#getByAddress} is documented as throwing a checked
+     * exception "if IP address is of illegal length."  We replace it with
+     * an unchecked exception, for use by callers who already know that addr
+     * is an array of length 4 or 16.
+     *
+     * @param addr the raw 4-byte or 16-byte IP address in big-endian order
+     * @return an InetAddress object created from the raw IP address
+     */
+    private static InetAddress bytesToInetAddress(byte[] addr) {
+        try {
+            return InetAddress.getByAddress(addr);
+        } catch (UnknownHostException e) {
+            throw new AssertionError(e);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/network/NetworkAddress.java b/core/src/main/java/org/elasticsearch/common/network/NetworkAddress.java
index 91eda6b..3dcaeeb 100644
--- a/core/src/main/java/org/elasticsearch/common/network/NetworkAddress.java
+++ b/core/src/main/java/org/elasticsearch/common/network/NetworkAddress.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.common.network;
 
-import com.google.common.net.InetAddresses;
-
 import org.elasticsearch.common.SuppressForbidden;
 
 import java.net.Inet6Address;
diff --git a/core/src/main/java/org/elasticsearch/common/network/NetworkService.java b/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
index 8eff70e..cd46d14 100644
--- a/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
+++ b/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
@@ -27,7 +27,6 @@ import org.elasticsearch.common.unit.TimeValue;
 
 import java.io.IOException;
 import java.net.InetAddress;
-import java.net.UnknownHostException;
 import java.util.List;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.TimeUnit;
@@ -73,7 +72,7 @@ public class NetworkService extends AbstractComponent {
         /**
          * Resolves a custom value handling, return <tt>null</tt> if can't handle it.
          */
-        InetAddress[] resolveIfPossible(String value);
+        InetAddress[] resolveIfPossible(String value) throws IOException;
     }
 
     private final List<CustomNameResolver> customNameResolvers = new CopyOnWriteArrayList<>();
@@ -162,7 +161,7 @@ public class NetworkService extends AbstractComponent {
         return address;
     }
 
-    private InetAddress[] resolveInetAddress(String host) throws UnknownHostException, IOException {
+    private InetAddress[] resolveInetAddress(String host) throws IOException {
         if ((host.startsWith("#") && host.endsWith("#")) || (host.startsWith("_") && host.endsWith("_"))) {
             host = host.substring(1, host.length() - 1);
             // allow custom resolvers to have special names
diff --git a/core/src/main/java/org/elasticsearch/common/path/PathTrie.java b/core/src/main/java/org/elasticsearch/common/path/PathTrie.java
index 2bee827..3bf2a9b 100644
--- a/core/src/main/java/org/elasticsearch/common/path/PathTrie.java
+++ b/core/src/main/java/org/elasticsearch/common/path/PathTrie.java
@@ -19,15 +19,14 @@
 
 package org.elasticsearch.common.path;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.Strings;
 
 import java.util.ArrayList;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
+import static org.elasticsearch.common.collect.MapBuilder.newMapBuilder;
 
 /**
  *
@@ -46,7 +45,7 @@ public class PathTrie<T> {
     };
 
     private final Decoder decoder;
-    private final TrieNode root;
+    private final TrieNode<T> root;
     private final char separator;
     private T rootValue;
 
@@ -61,10 +60,10 @@ public class PathTrie<T> {
     public PathTrie(char separator, String wildcard, Decoder decoder) {
         this.decoder = decoder;
         this.separator = separator;
-        root = new TrieNode(new String(new char[]{separator}), null, wildcard);
+        root = new TrieNode<>(new String(new char[]{separator}), null, null, wildcard);
     }
 
-    public class TrieNode {
+    public class TrieNode<T> {
         private transient String key;
         private transient T value;
         private boolean isWildcard;
@@ -72,14 +71,17 @@ public class PathTrie<T> {
 
         private transient String namedWildcard;
 
-        private Map<String, TrieNode> children;
+        private ImmutableMap<String, TrieNode<T>> children;
 
-        public TrieNode(String key, T value, String wildcard) {
+        private final TrieNode parent;
+
+        public TrieNode(String key, T value, TrieNode parent, String wildcard) {
             this.key = key;
             this.wildcard = wildcard;
             this.isWildcard = (key.equals(wildcard));
+            this.parent = parent;
             this.value = value;
-            this.children = emptyMap();
+            this.children = ImmutableMap.of();
             if (isNamedWildcard(key)) {
                 namedWildcard = key.substring(key.indexOf('{') + 1, key.indexOf('}'));
             } else {
@@ -96,14 +98,8 @@ public class PathTrie<T> {
             return isWildcard;
         }
 
-        public synchronized void addChild(TrieNode child) {
-            addInnerChild(child.key, child);
-        }
-
-        private void addInnerChild(String key, TrieNode child) {
-            Map<String, TrieNode> newChildren = new HashMap<>(children);
-            newChildren.put(key, child);
-            children = unmodifiableMap(newChildren);
+        public synchronized void addChild(TrieNode<T> child) {
+            children = newMapBuilder(children).put(child.key, child).immutableMap();
         }
 
         public TrieNode getChild(String key) {
@@ -119,11 +115,14 @@ public class PathTrie<T> {
             if (isNamedWildcard(token)) {
                 key = wildcard;
             }
-            TrieNode node = children.get(key);
+            TrieNode<T> node = children.get(key);
             if (node == null) {
-                T nodeValue = index == path.length - 1 ? value : null;
-                node = new TrieNode(token, nodeValue, wildcard);
-                addInnerChild(key, node);
+                if (index == (path.length - 1)) {
+                    node = new TrieNode<>(token, value, this, wildcard);
+                } else {
+                    node = new TrieNode<>(token, null, this, wildcard);
+                }
+                children = newMapBuilder(children).put(key, node).immutableMap();
             } else {
                 if (isNamedWildcard(token)) {
                     node.updateKeyWithNamedWildcard(token);
@@ -159,7 +158,7 @@ public class PathTrie<T> {
                 return null;
 
             String token = path[index];
-            TrieNode node = children.get(token);
+            TrieNode<T> node = children.get(token);
             boolean usedWildcard;
             if (node == null) {
                 node = children.get(wildcard);
@@ -196,16 +195,11 @@ public class PathTrie<T> {
             return res;
         }
 
-        private void put(Map<String, String> params, TrieNode node, String value) {
+        private void put(Map<String, String> params, TrieNode<T> node, String value) {
             if (params != null && node.isNamedWildcard()) {
                 params.put(node.namedWildcard(), value);
             }
         }
-
-        @Override
-        public String toString() {
-            return key;
-        }
     }
 
     public void insert(String path, T value) {
diff --git a/core/src/main/java/org/elasticsearch/common/settings/Settings.java b/core/src/main/java/org/elasticsearch/common/settings/Settings.java
index 663abd7..e368430 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/Settings.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/Settings.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.common.settings;
 
+import java.nio.charset.StandardCharsets;
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Booleans;
 import org.elasticsearch.common.Strings;
@@ -28,40 +30,20 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.property.PropertyPlaceholder;
 import org.elasticsearch.common.settings.loader.SettingsLoader;
 import org.elasticsearch.common.settings.loader.SettingsLoaderFactory;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.MemorySizeValue;
-import org.elasticsearch.common.unit.RatioValue;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.unit.*;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.InputStreamReader;
-import java.nio.charset.StandardCharsets;
 import java.nio.file.Files;
 import java.nio.file.Path;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Properties;
-import java.util.Set;
-import java.util.SortedMap;
-import java.util.TreeMap;
+import java.util.*;
 import java.util.concurrent.TimeUnit;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.Strings.toCamelCase;
 import static org.elasticsearch.common.unit.ByteSizeValue.parseBytesSizeValue;
 import static org.elasticsearch.common.unit.SizeValue.parseSizeValue;
@@ -88,8 +70,8 @@ public final class Settings implements ToXContent {
         return settingsRequireUnits;
     }
 
-    private final Map<String, String> forcedUnderscoreSettings;
     private SortedMap<String, String> settings;
+    private final ImmutableMap<String, String> forcedUnderscoreSettings;
 
     Settings(Map<String, String> settings) {
         // we use a sorted map for consistent serialization when using getAsMap()
@@ -104,7 +86,7 @@ public final class Settings implements ToXContent {
                 forcedUnderscoreSettings.put(toUnderscoreCase, entry.getValue());
             }
         }
-        this.forcedUnderscoreSettings = forcedUnderscoreSettings == null ? emptyMap() : unmodifiableMap(forcedUnderscoreSettings);
+        this.forcedUnderscoreSettings = forcedUnderscoreSettings == null ? ImmutableMap.<String, String>of() : ImmutableMap.copyOf(forcedUnderscoreSettings);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java b/core/src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java
index 026f849..d8e121b 100644
--- a/core/src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java
+++ b/core/src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java
@@ -19,16 +19,16 @@
 
 package org.elasticsearch.common.transport;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
 
 import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
+import java.lang.reflect.Constructor;
 
-import static java.util.Collections.unmodifiableMap;
+import static org.elasticsearch.common.collect.MapBuilder.newMapBuilder;
 
 /**
  * A global registry of all different types of {@link org.elasticsearch.common.transport.TransportAddress} allowing
@@ -42,25 +42,23 @@ public abstract class TransportAddressSerializers {
 
     private static final ESLogger logger = Loggers.getLogger(TransportAddressSerializers.class);
 
-    private static final Map<Short, TransportAddress> ADDRESS_REGISTRY;
+    private static ImmutableMap<Short, TransportAddress> ADDRESS_REGISTRY = ImmutableMap.of();
 
     static {
-        Map<Short, TransportAddress> registry = new HashMap<>();
         try {
-            addAddressType(registry, DummyTransportAddress.INSTANCE);
-            addAddressType(registry, InetSocketTransportAddress.PROTO);
-            addAddressType(registry, LocalTransportAddress.PROTO);
+            addAddressType(DummyTransportAddress.INSTANCE);
+            addAddressType(InetSocketTransportAddress.PROTO);
+            addAddressType(LocalTransportAddress.PROTO);
         } catch (Exception e) {
-            logger.warn("Failed to setup TransportAddresses", e);
+            logger.warn("Failed to add InetSocketTransportAddress", e);
         }
-        ADDRESS_REGISTRY = unmodifiableMap(registry);
     }
 
-    public static synchronized void addAddressType(Map<Short, TransportAddress> registry, TransportAddress address) throws Exception {
-        if (registry.containsKey(address.uniqueAddressTypeId())) {
+    public static synchronized void addAddressType(TransportAddress address) throws Exception {
+        if (ADDRESS_REGISTRY.containsKey(address.uniqueAddressTypeId())) {
             throw new IllegalStateException("Address [" + address.uniqueAddressTypeId() + "] already bound");
         }
-        registry.put(address.uniqueAddressTypeId(), address);
+        ADDRESS_REGISTRY = newMapBuilder(ADDRESS_REGISTRY).put(address.uniqueAddressTypeId(), address).immutableMap();
     }
 
     public static TransportAddress addressFromStream(StreamInput input) throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java b/core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java
index 243c944..fd0f64d 100644
--- a/core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java
+++ b/core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java
@@ -23,16 +23,12 @@ import com.carrotsearch.hppc.DoubleArrayList;
 import com.carrotsearch.hppc.FloatArrayList;
 import com.carrotsearch.hppc.LongArrayList;
 import com.carrotsearch.hppc.ObjectArrayList;
-import com.google.common.collect.Iterators;
-
 import org.apache.lucene.util.*;
 
 import java.util.*;
 
 /** Collections-related utility methods. */
-public enum CollectionUtils {
-    CollectionUtils;
-
+public class CollectionUtils {
     public static void sort(LongArrayList list) {
         sort(list.buffer, list.size());
     }
@@ -367,13 +363,6 @@ public enum CollectionUtils {
 
     }
 
-    /**
-     * Combines multiple iterators into a single iterator.
-     */
-    public static <T> Iterator<T> concat(Iterator<? extends T>... iterators) {
-        return Iterators.<T>concat(iterators);
-    }
-
     public static <E> ArrayList<E> iterableAsArrayList(Iterable<? extends E> elements) {
         if (elements == null) {
             throw new NullPointerException("elements");
@@ -458,4 +447,5 @@ public enum CollectionUtils {
 
         return result;
     }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java b/core/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java
index 0ebba04..e4285d2 100644
--- a/core/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java
+++ b/core/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.gateway;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.component.AbstractComponent;
@@ -33,9 +34,6 @@ import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
 
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * The dangling indices state is responsible for finding new dangling indices (indices that have
  * their state written on disk, but don't exists in the metadata of the cluster), and importing
@@ -75,8 +73,7 @@ public class DanglingIndicesState extends AbstractComponent {
      * The current set of dangling indices.
      */
     Map<String, IndexMetaData> getDanglingIndices() {
-        // This might be a good use case for CopyOnWriteHashMap
-        return unmodifiableMap(new HashMap<>(danglingIndices));
+        return ImmutableMap.copyOf(danglingIndices);
     }
 
     /**
@@ -110,7 +107,7 @@ public class DanglingIndicesState extends AbstractComponent {
             indices = nodeEnv.findAllIndices();
         } catch (Throwable e) {
             logger.warn("failed to list dangling indices", e);
-            return emptyMap();
+            return ImmutableMap.of();
         }
 
         Map<String, IndexMetaData>  newIndices = new HashMap<>();
diff --git a/core/src/main/java/org/elasticsearch/gateway/GatewayService.java b/core/src/main/java/org/elasticsearch/gateway/GatewayService.java
index 855b6ce..742f789 100644
--- a/core/src/main/java/org/elasticsearch/gateway/GatewayService.java
+++ b/core/src/main/java/org/elasticsearch/gateway/GatewayService.java
@@ -20,12 +20,7 @@
 package org.elasticsearch.gateway;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
-
-import org.elasticsearch.cluster.ClusterChangedEvent;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ClusterStateListener;
-import org.elasticsearch.cluster.ClusterStateUpdateTask;
+import org.elasticsearch.cluster.*;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.block.ClusterBlocks;
@@ -258,7 +253,7 @@ public class GatewayService extends AbstractLifecycleComponent<GatewayService> i
                     routingTableBuilder.version(0);
 
                     // now, reroute
-                    RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(routingTableBuilder.build()).build());
+                    RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(routingTableBuilder).build());
 
                     return ClusterState.builder(updatedState).routingResult(routingResult).build();
                 }
diff --git a/core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java b/core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java
index b8491b9..1ab7a56 100644
--- a/core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java
+++ b/core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java
@@ -37,13 +37,7 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.discovery.MasterNotDiscoveredException;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportChannel;
-import org.elasticsearch.transport.TransportException;
-import org.elasticsearch.transport.TransportRequest;
-import org.elasticsearch.transport.TransportRequestHandler;
-import org.elasticsearch.transport.TransportResponse;
-import org.elasticsearch.transport.TransportResponseHandler;
-import org.elasticsearch.transport.TransportService;
+import org.elasticsearch.transport.*;
 
 import java.io.IOException;
 import java.util.Arrays;
@@ -164,11 +158,10 @@ public class LocalAllocateDangledIndices extends AbstractComponent {
                     }
                     logger.info("auto importing dangled indices {} from [{}]", sb.toString(), request.fromNode);
 
-                    RoutingTable routingTable = routingTableBuilder.build();
-                    ClusterState updatedState = ClusterState.builder(currentState).metaData(metaData).blocks(blocks).routingTable(routingTable).build();
+                    ClusterState updatedState = ClusterState.builder(currentState).metaData(metaData).blocks(blocks).routingTable(routingTableBuilder).build();
 
                     // now, reroute
-                    RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(routingTable).build());
+                    RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(routingTableBuilder).build());
 
                     return ClusterState.builder(updatedState).routingResult(routingResult).build();
                 }
diff --git a/core/src/main/java/org/elasticsearch/http/HttpServer.java b/core/src/main/java/org/elasticsearch/http/HttpServer.java
index f3b8c3f..168ed06 100644
--- a/core/src/main/java/org/elasticsearch/http/HttpServer.java
+++ b/core/src/main/java/org/elasticsearch/http/HttpServer.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.http;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.FileSystemUtils;
@@ -26,29 +28,18 @@ import org.elasticsearch.common.io.Streams;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.node.service.NodeService;
-import org.elasticsearch.rest.BytesRestResponse;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestFilter;
-import org.elasticsearch.rest.RestFilterChain;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.RestStatus;
+import org.elasticsearch.rest.*;
 
 import java.io.ByteArrayOutputStream;
 import java.io.IOException;
 import java.io.InputStream;
-import java.nio.file.Files;
-import java.nio.file.Path;
+import java.nio.file.*;
 import java.nio.file.attribute.BasicFileAttributes;
 import java.util.HashMap;
 import java.util.Locale;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
-import static org.elasticsearch.rest.RestStatus.FORBIDDEN;
-import static org.elasticsearch.rest.RestStatus.INTERNAL_SERVER_ERROR;
-import static org.elasticsearch.rest.RestStatus.NOT_FOUND;
-import static org.elasticsearch.rest.RestStatus.OK;
+import static org.elasticsearch.rest.RestStatus.*;
 
 /**
  *
@@ -213,7 +204,7 @@ public class HttpServer extends AbstractLifecycleComponent<HttpServer> {
         final String separator = siteFile.getFileSystem().getSeparator();
         // Convert file separators.
         sitePath = sitePath.replace("/", separator);
-
+        
         Path file = siteFile.resolve(sitePath);
 
         // return not found instead of forbidden to prevent malicious requests to find out if files exist or dont exist
@@ -284,7 +275,7 @@ public class HttpServer extends AbstractLifecycleComponent<HttpServer> {
         mimeTypes.put("svg", "image/svg+xml");
         mimeTypes.put("ico", "image/vnd.microsoft.icon");
         mimeTypes.put("mp3", "audio/mpeg");
-        DEFAULT_MIME_TYPES = unmodifiableMap(mimeTypes);
+        DEFAULT_MIME_TYPES = ImmutableMap.copyOf(mimeTypes);
     }
 
     public static final Map<String, String> DEFAULT_MIME_TYPES;
diff --git a/core/src/main/java/org/elasticsearch/index/IndexService.java b/core/src/main/java/org/elasticsearch/index/IndexService.java
index 602ccc3..2fc7a24 100644
--- a/core/src/main/java/org/elasticsearch/index/IndexService.java
+++ b/core/src/main/java/org/elasticsearch/index/IndexService.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.ElasticsearchException;
@@ -41,11 +42,7 @@ import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.query.IndexQueryParserService;
 import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.settings.IndexSettingsService;
-import org.elasticsearch.index.shard.IndexShard;
-import org.elasticsearch.index.shard.ShadowIndexShard;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.shard.ShardNotFoundException;
-import org.elasticsearch.index.shard.ShardPath;
+import org.elasticsearch.index.shard.*;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.index.store.IndexStore;
 import org.elasticsearch.index.store.Store;
@@ -62,8 +59,6 @@ import java.util.Set;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.collect.MapBuilder.newMapBuilder;
 
 /**
@@ -81,7 +76,7 @@ public class IndexService extends AbstractIndexComponent implements IndexCompone
     private final IndicesService indicesServices;
     private final IndexServicesProvider indexServicesProvider;
     private final IndexStore indexStore;
-    private volatile Map<Integer, IndexShard> shards = emptyMap();
+    private volatile ImmutableMap<Integer, IndexShard> shards = ImmutableMap.of();
     private final AtomicBoolean closed = new AtomicBoolean(false);
     private final AtomicBoolean deleted = new AtomicBoolean(false);
 
@@ -318,9 +313,9 @@ public class IndexService extends AbstractIndexComponent implements IndexCompone
             return;
         }
         logger.debug("[{}] closing... (reason: [{}])", shardId, reason);
-        HashMap<Integer, IndexShard> newShards = new HashMap<>(shards);
-        indexShard = newShards.remove(shardId);
-        shards = unmodifiableMap(newShards);
+        HashMap<Integer, IndexShard> tmpShardsMap = new HashMap<>(shards);
+        indexShard = tmpShardsMap.remove(shardId);
+        shards = ImmutableMap.copyOf(tmpShardsMap);
         closeShard(reason, sId, indexShard, indexShard.store());
         logger.debug("[{}] closed (reason: [{}])", shardId, reason);
     }
diff --git a/core/src/main/java/org/elasticsearch/index/IndexServicesProvider.java b/core/src/main/java/org/elasticsearch/index/IndexServicesProvider.java
index fe84284..0d5c3cb 100644
--- a/core/src/main/java/org/elasticsearch/index/IndexServicesProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/IndexServicesProvider.java
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.elasticsearch.index;
 
 import org.elasticsearch.common.Nullable;
@@ -34,6 +35,7 @@ import org.elasticsearch.index.termvectors.TermVectorsService;
 import org.elasticsearch.indices.IndicesLifecycle;
 import org.elasticsearch.indices.IndicesWarmer;
 import org.elasticsearch.indices.cache.query.IndicesQueryCache;
+import org.elasticsearch.indices.memory.IndexingMemoryController;
 import org.elasticsearch.threadpool.ThreadPool;
 
 /**
@@ -58,9 +60,10 @@ public final class IndexServicesProvider {
     private final EngineFactory factory;
     private final BigArrays bigArrays;
     private final IndexSearcherWrapper indexSearcherWrapper;
+    private final IndexingMemoryController indexingMemoryController;
 
     @Inject
-    public IndexServicesProvider(IndicesLifecycle indicesLifecycle, ThreadPool threadPool, MapperService mapperService, IndexQueryParserService queryParserService, IndexCache indexCache, IndexAliasesService indexAliasesService, IndicesQueryCache indicesQueryCache, CodecService codecService, TermVectorsService termVectorsService, IndexFieldDataService indexFieldDataService, @Nullable IndicesWarmer warmer, SimilarityService similarityService, EngineFactory factory, BigArrays bigArrays, @Nullable IndexSearcherWrapper indexSearcherWrapper) {
+    public IndexServicesProvider(IndicesLifecycle indicesLifecycle, ThreadPool threadPool, MapperService mapperService, IndexQueryParserService queryParserService, IndexCache indexCache, IndexAliasesService indexAliasesService, IndicesQueryCache indicesQueryCache, CodecService codecService, TermVectorsService termVectorsService, IndexFieldDataService indexFieldDataService, @Nullable IndicesWarmer warmer, SimilarityService similarityService, EngineFactory factory, BigArrays bigArrays, @Nullable IndexSearcherWrapper indexSearcherWrapper, IndexingMemoryController indexingMemoryController) {
         this.indicesLifecycle = indicesLifecycle;
         this.threadPool = threadPool;
         this.mapperService = mapperService;
@@ -76,6 +79,7 @@ public final class IndexServicesProvider {
         this.factory = factory;
         this.bigArrays = bigArrays;
         this.indexSearcherWrapper = indexSearcherWrapper;
+        this.indexingMemoryController = indexingMemoryController;
     }
 
     public IndicesLifecycle getIndicesLifecycle() {
@@ -134,5 +138,11 @@ public final class IndexServicesProvider {
         return bigArrays;
     }
 
-    public IndexSearcherWrapper getIndexSearcherWrapper() { return indexSearcherWrapper; }
+    public IndexSearcherWrapper getIndexSearcherWrapper() {
+        return indexSearcherWrapper;
+    }
+
+    public IndexingMemoryController getIndexingMemoryController() {
+        return indexingMemoryController;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/LocalNodeId.java b/core/src/main/java/org/elasticsearch/index/LocalNodeId.java
deleted file mode 100644
index a045636..0000000
--- a/core/src/main/java/org/elasticsearch/index/LocalNodeId.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index;
-
-import org.elasticsearch.common.inject.BindingAnnotation;
-
-import java.lang.annotation.Documented;
-import java.lang.annotation.Retention;
-import java.lang.annotation.Target;
-
-import static java.lang.annotation.ElementType.FIELD;
-import static java.lang.annotation.ElementType.PARAMETER;
-import static java.lang.annotation.RetentionPolicy.RUNTIME;
-
-/**
- *
- */
-@BindingAnnotation
-@Target({FIELD, PARAMETER})
-@Retention(RUNTIME)
-@Documented
-public @interface LocalNodeId {
-}
diff --git a/core/src/main/java/org/elasticsearch/index/LocalNodeIdModule.java b/core/src/main/java/org/elasticsearch/index/LocalNodeIdModule.java
deleted file mode 100644
index 82e36cd..0000000
--- a/core/src/main/java/org/elasticsearch/index/LocalNodeIdModule.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index;
-
-import org.elasticsearch.common.inject.AbstractModule;
-
-/**
- *
- */
-public class LocalNodeIdModule extends AbstractModule {
-
-    private final String localNodeId;
-
-    public LocalNodeIdModule(String localNodeId) {
-        this.localNodeId = localNodeId;
-    }
-
-    @Override
-    protected void configure() {
-        bind(String.class).annotatedWith(LocalNodeId.class).toInstance(localNodeId);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/VersionType.java b/core/src/main/java/org/elasticsearch/index/VersionType.java
index a5d8cae..b8f998b 100644
--- a/core/src/main/java/org/elasticsearch/index/VersionType.java
+++ b/core/src/main/java/org/elasticsearch/index/VersionType.java
@@ -31,24 +31,37 @@ import java.io.IOException;
 public enum VersionType implements Writeable<VersionType> {
     INTERNAL((byte) 0) {
         @Override
-        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion) {
-            return isVersionConflict(currentVersion, expectedVersion);
+        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
+            return isVersionConflict(currentVersion, expectedVersion, deleted);
+        }
+
+        @Override
+        public String explainConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
+            if (expectedVersion == Versions.MATCH_DELETED) {
+                return "document already exists (current version [" + currentVersion + "])";
+            }
+            return "current version [" + currentVersion + "] is different than the one provided [" + expectedVersion + "]";
         }
 
         @Override
         public boolean isVersionConflictForReads(long currentVersion, long expectedVersion) {
-            return isVersionConflict(currentVersion, expectedVersion);
+            return isVersionConflict(currentVersion, expectedVersion, false);
         }
 
-        private boolean isVersionConflict(long currentVersion, long expectedVersion) {
+        @Override
+        public String explainConflictForReads(long currentVersion, long expectedVersion) {
+            return "current version [" + currentVersion + "] is different than the one provided [" + expectedVersion + "]";
+        }
+
+        private boolean isVersionConflict(long currentVersion, long expectedVersion, boolean deleted) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
             }
             if (expectedVersion == Versions.MATCH_ANY) {
                 return false;
             }
-            if (currentVersion == Versions.NOT_FOUND) {
-                return true;
+            if (expectedVersion == Versions.MATCH_DELETED) {
+                return deleted == false;
             }
             if (currentVersion != expectedVersion) {
                 return true;
@@ -63,8 +76,7 @@ public enum VersionType implements Writeable<VersionType> {
 
         @Override
         public boolean validateVersionForWrites(long version) {
-            // not allowing Versions.NOT_FOUND as it is not a valid input value.
-            return version > 0L || version == Versions.MATCH_ANY;
+            return version > 0L || version == Versions.MATCH_ANY || version == Versions.MATCH_DELETED;
         }
 
         @Override
@@ -82,7 +94,7 @@ public enum VersionType implements Writeable<VersionType> {
     },
     EXTERNAL((byte) 1) {
         @Override
-        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion) {
+        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
             }
@@ -99,6 +111,11 @@ public enum VersionType implements Writeable<VersionType> {
         }
 
         @Override
+        public String explainConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
+            return "current version [" + currentVersion + "] is higher or equal to the one provided [" + expectedVersion + "]";
+        }
+
+        @Override
         public boolean isVersionConflictForReads(long currentVersion, long expectedVersion) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
@@ -116,6 +133,11 @@ public enum VersionType implements Writeable<VersionType> {
         }
 
         @Override
+        public String explainConflictForReads(long currentVersion, long expectedVersion) {
+            return "current version [" + currentVersion + "] is different than the one provided [" + expectedVersion + "]";
+        }
+
+        @Override
         public long updateVersion(long currentVersion, long expectedVersion) {
             return expectedVersion;
         }
@@ -133,7 +155,7 @@ public enum VersionType implements Writeable<VersionType> {
     },
     EXTERNAL_GTE((byte) 2) {
         @Override
-        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion) {
+        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
             }
@@ -150,6 +172,11 @@ public enum VersionType implements Writeable<VersionType> {
         }
 
         @Override
+        public String explainConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
+            return "current version [" + currentVersion + "] is higher than the one provided [" + expectedVersion + "]";
+        }
+
+        @Override
         public boolean isVersionConflictForReads(long currentVersion, long expectedVersion) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
@@ -167,6 +194,11 @@ public enum VersionType implements Writeable<VersionType> {
         }
 
         @Override
+        public String explainConflictForReads(long currentVersion, long expectedVersion) {
+            return "current version [" + currentVersion + "] is different than the one provided [" + expectedVersion + "]";
+        }
+
+        @Override
         public long updateVersion(long currentVersion, long expectedVersion) {
             return expectedVersion;
         }
@@ -187,7 +219,7 @@ public enum VersionType implements Writeable<VersionType> {
      */
     FORCE((byte) 3) {
         @Override
-        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion) {
+        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
             }
@@ -195,17 +227,27 @@ public enum VersionType implements Writeable<VersionType> {
                 return false;
             }
             if (expectedVersion == Versions.MATCH_ANY) {
-                return true;
+                throw new IllegalStateException("you must specify a version when use VersionType.FORCE");
             }
             return false;
         }
 
         @Override
+        public String explainConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
+            throw new AssertionError("VersionType.FORCE should never result in a write conflict");
+        }
+
+        @Override
         public boolean isVersionConflictForReads(long currentVersion, long expectedVersion) {
             return false;
         }
 
         @Override
+        public String explainConflictForReads(long currentVersion, long expectedVersion) {
+            throw new AssertionError("VersionType.FORCE should never result in a read conflict");
+        }
+
+        @Override
         public long updateVersion(long currentVersion, long expectedVersion) {
             return expectedVersion;
         }
@@ -237,18 +279,47 @@ public enum VersionType implements Writeable<VersionType> {
     /**
      * Checks whether the current version conflicts with the expected version, based on the current version type.
      *
+     * @param currentVersion  the current version for the document
+     * @param expectedVersion the version specified for the write operation
+     * @param deleted         true if the document is currently deleted (note that #currentVersion will typically be
+     *                        {@link Versions#NOT_FOUND}, but may be something else if the document was recently deleted
      * @return true if versions conflict false o.w.
      */
-    public abstract boolean isVersionConflictForWrites(long currentVersion, long expectedVersion);
+    public abstract boolean isVersionConflictForWrites(long currentVersion, long expectedVersion, boolean deleted);
+
+
+    /**
+     * Returns a human readable explanation for a version conflict on write.
+     *
+     * Note that this method is only called if {@link #isVersionConflictForWrites(long, long, boolean)} returns true;
+     *
+     * @param currentVersion  the current version for the document
+     * @param expectedVersion the version specified for the write operation
+     * @param deleted         true if the document is currently deleted (note that #currentVersion will typically be
+     *                        {@link Versions#NOT_FOUND}, but may be something else if the document was recently deleted
+     */
+    public abstract String explainConflictForWrites(long currentVersion, long expectedVersion, boolean deleted);
 
     /**
      * Checks whether the current version conflicts with the expected version, based on the current version type.
      *
+     * @param currentVersion  the current version for the document
+     * @param expectedVersion the version specified for the read operation
      * @return true if versions conflict false o.w.
      */
     public abstract boolean isVersionConflictForReads(long currentVersion, long expectedVersion);
 
     /**
+     * Returns a human readable explanation for a version conflict on read.
+     *
+     * Note that this method is only called if {@link #isVersionConflictForReads(long, long)} returns true;
+     *
+     * @param currentVersion  the current version for the document
+     * @param expectedVersion the version specified for the read operation
+     */
+    public abstract String explainConflictForReads(long currentVersion, long expectedVersion);
+
+    /**
      * Returns the new version for a document, based on its current one and the specified in the request
      *
      * @return new version
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/Analysis.java b/core/src/main/java/org/elasticsearch/index/analysis/Analysis.java
index 861f070..1040a27 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/Analysis.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/Analysis.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.index.analysis;
 
+import java.nio.charset.StandardCharsets;
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.NumericTokenStream;
 import org.apache.lucene.analysis.TokenStream;
@@ -58,6 +60,7 @@ import org.apache.lucene.analysis.tr.TurkishAnalyzer;
 import org.apache.lucene.analysis.util.CharArraySet;
 import org.apache.lucene.util.Version;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.io.FileSystemUtils;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.lucene.Lucene;
@@ -68,19 +71,15 @@ import org.elasticsearch.index.settings.IndexSettings;
 import java.io.BufferedReader;
 import java.io.IOException;
 import java.io.Reader;
-import java.nio.charset.StandardCharsets;
 import java.nio.file.Path;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  *
  */
@@ -125,44 +124,40 @@ public class Analysis {
         }
     }
 
-    public static final Map<String, Set<?>> NAMED_STOP_WORDS;
-    static {
-        Map<String, Set<?>> namedStopWords = new HashMap<>();
-        namedStopWords.put("_arabic_", ArabicAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_armenian_", ArmenianAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_basque_", BasqueAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_brazilian_", BrazilianAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_bulgarian_", BulgarianAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_catalan_", CatalanAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_czech_", CzechAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_danish_", DanishAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_dutch_", DutchAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_english_", EnglishAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_finnish_", FinnishAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_french_", FrenchAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_galician_", GalicianAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_german_", GermanAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_greek_", GreekAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_hindi_", HindiAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_hungarian_", HungarianAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_indonesian_", IndonesianAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_irish_", IrishAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_italian_", ItalianAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_latvian_", LatvianAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_lithuanian_", LithuanianAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_norwegian_", NorwegianAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_persian_", PersianAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_portuguese_", PortugueseAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_romanian_", RomanianAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_russian_", RussianAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_sorani_", SoraniAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_spanish_", SpanishAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_swedish_", SwedishAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_thai_", ThaiAnalyzer.getDefaultStopSet());
-        namedStopWords.put("_turkish_", TurkishAnalyzer.getDefaultStopSet());
-
-        NAMED_STOP_WORDS = unmodifiableMap(namedStopWords);
-    }
+    public static final ImmutableMap<String, Set<?>> namedStopWords = MapBuilder.<String, Set<?>>newMapBuilder()
+            .put("_arabic_", ArabicAnalyzer.getDefaultStopSet())
+            .put("_armenian_", ArmenianAnalyzer.getDefaultStopSet())
+            .put("_basque_", BasqueAnalyzer.getDefaultStopSet())
+            .put("_brazilian_", BrazilianAnalyzer.getDefaultStopSet())
+            .put("_bulgarian_", BulgarianAnalyzer.getDefaultStopSet())
+            .put("_catalan_", CatalanAnalyzer.getDefaultStopSet())
+            .put("_czech_", CzechAnalyzer.getDefaultStopSet())
+            .put("_danish_", DanishAnalyzer.getDefaultStopSet())
+            .put("_dutch_", DutchAnalyzer.getDefaultStopSet())
+            .put("_english_", EnglishAnalyzer.getDefaultStopSet())
+            .put("_finnish_", FinnishAnalyzer.getDefaultStopSet())
+            .put("_french_", FrenchAnalyzer.getDefaultStopSet())
+            .put("_galician_", GalicianAnalyzer.getDefaultStopSet())
+            .put("_german_", GermanAnalyzer.getDefaultStopSet())
+            .put("_greek_", GreekAnalyzer.getDefaultStopSet())
+            .put("_hindi_", HindiAnalyzer.getDefaultStopSet())
+            .put("_hungarian_", HungarianAnalyzer.getDefaultStopSet())
+            .put("_indonesian_", IndonesianAnalyzer.getDefaultStopSet())
+            .put("_irish_", IrishAnalyzer.getDefaultStopSet())
+            .put("_italian_", ItalianAnalyzer.getDefaultStopSet())
+            .put("_latvian_", LatvianAnalyzer.getDefaultStopSet())
+            .put("_lithuanian_", LithuanianAnalyzer.getDefaultStopSet())
+            .put("_norwegian_", NorwegianAnalyzer.getDefaultStopSet())
+            .put("_persian_", PersianAnalyzer.getDefaultStopSet())
+            .put("_portuguese_", PortugueseAnalyzer.getDefaultStopSet())
+            .put("_romanian_", RomanianAnalyzer.getDefaultStopSet())
+            .put("_russian_", RussianAnalyzer.getDefaultStopSet())
+            .put("_sorani_", SoraniAnalyzer.getDefaultStopSet())
+            .put("_spanish_", SpanishAnalyzer.getDefaultStopSet())
+            .put("_swedish_", SwedishAnalyzer.getDefaultStopSet())
+            .put("_thai_", ThaiAnalyzer.getDefaultStopSet())
+            .put("_turkish_", TurkishAnalyzer.getDefaultStopSet())
+            .immutableMap();
 
     public static CharArraySet parseWords(Environment env, Settings settings, String name, CharArraySet defaultWords, Map<String, Set<?>> namedWords, boolean ignoreCase) {
         String value = settings.get(name);
@@ -181,7 +176,7 @@ public class Analysis {
     }
 
     public static CharArraySet parseCommonWords(Environment env, Settings settings, CharArraySet defaultCommonWords, boolean ignoreCase) {
-        return parseWords(env, settings, "common_words", defaultCommonWords, NAMED_STOP_WORDS, ignoreCase);
+        return parseWords(env, settings, "common_words", defaultCommonWords, namedStopWords, ignoreCase);
     }
 
     public static CharArraySet parseArticles(Environment env, Settings settings) {
@@ -193,7 +188,7 @@ public class Analysis {
     }
 
     public static CharArraySet parseStopWords(Environment env, Settings settings, CharArraySet defaultStopWords, boolean ignoreCase) {
-        return parseWords(env, settings, "stopwords", defaultStopWords, NAMED_STOP_WORDS, ignoreCase);
+        return parseWords(env, settings, "stopwords", defaultStopWords, namedStopWords, ignoreCase);
     }
 
     private static CharArraySet resolveNamedWords(Collection<String> words, Map<String, Set<?>> namedWords, boolean ignoreCase) {
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/AnalysisService.java b/core/src/main/java/org/elasticsearch/index/analysis/AnalysisService.java
index c76446c..829f9db 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/AnalysisService.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/AnalysisService.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.analysis;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.analysis.Analyzer;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -36,17 +37,15 @@ import java.io.Closeable;
 import java.util.HashMap;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  *
  */
 public class AnalysisService extends AbstractIndexComponent implements Closeable {
 
-    private final Map<String, NamedAnalyzer> analyzers;
-    private final Map<String, TokenizerFactory> tokenizers;
-    private final Map<String, CharFilterFactory> charFilters;
-    private final Map<String, TokenFilterFactory> tokenFilters;
+    private final ImmutableMap<String, NamedAnalyzer> analyzers;
+    private final ImmutableMap<String, TokenizerFactory> tokenizers;
+    private final ImmutableMap<String, CharFilterFactory> charFilters;
+    private final ImmutableMap<String, TokenFilterFactory> tokenFilters;
 
     private final NamedAnalyzer defaultAnalyzer;
     private final NamedAnalyzer defaultIndexAnalyzer;
@@ -99,7 +98,7 @@ public class AnalysisService extends AbstractIndexComponent implements Closeable
             }
         }
 
-        this.tokenizers = unmodifiableMap(tokenizers);
+        this.tokenizers = ImmutableMap.copyOf(tokenizers);
 
         Map<String, CharFilterFactory> charFilters = new HashMap<>();
         if (charFilterFactoryFactories != null) {
@@ -134,7 +133,7 @@ public class AnalysisService extends AbstractIndexComponent implements Closeable
             }
         }
 
-        this.charFilters = unmodifiableMap(charFilters);
+        this.charFilters = ImmutableMap.copyOf(charFilters);
 
         Map<String, TokenFilterFactory> tokenFilters = new HashMap<>();
         if (tokenFilterFactoryFactories != null) {
@@ -169,7 +168,7 @@ public class AnalysisService extends AbstractIndexComponent implements Closeable
                 }
             }
         }
-        this.tokenFilters = unmodifiableMap(tokenFilters);
+        this.tokenFilters = ImmutableMap.copyOf(tokenFilters);
 
         Map<String, AnalyzerProvider> analyzerProviders = new HashMap<>();
         if (analyzerFactoryFactories != null) {
@@ -276,7 +275,7 @@ public class AnalysisService extends AbstractIndexComponent implements Closeable
                 throw new IllegalArgumentException("analyzer name must not start with '_'. got \"" + analyzer.getKey() + "\"");
             }
         }
-        this.analyzers = unmodifiableMap(analyzers);
+        this.analyzers = ImmutableMap.copyOf(analyzers);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java b/core/src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java
index f1ad1d5..b884095 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java
@@ -19,10 +19,12 @@
 
 package org.elasticsearch.index.analysis;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.ngram.Lucene43NGramTokenizer;
 import org.apache.lucene.analysis.ngram.NGramTokenizer;
 import org.apache.lucene.util.Version;
+
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.assistedinject.Assisted;
 import org.elasticsearch.common.settings.Settings;
@@ -31,12 +33,9 @@ import org.elasticsearch.index.settings.IndexSettings;
 
 import java.lang.reflect.Field;
 import java.lang.reflect.Modifier;
-import java.util.HashMap;
 import java.util.Locale;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  *
  */
@@ -50,12 +49,12 @@ public class NGramTokenizerFactory extends AbstractTokenizerFactory {
     static final Map<String, CharMatcher> MATCHERS;
 
     static {
-        Map<String, CharMatcher> matchers = new HashMap<>();
-        matchers.put("letter", CharMatcher.Basic.LETTER);
-        matchers.put("digit", CharMatcher.Basic.DIGIT);
-        matchers.put("whitespace", CharMatcher.Basic.WHITESPACE);
-        matchers.put("punctuation", CharMatcher.Basic.PUNCTUATION);
-        matchers.put("symbol", CharMatcher.Basic.SYMBOL);
+        ImmutableMap.Builder<String, CharMatcher> builder = ImmutableMap.builder();
+        builder.put("letter", CharMatcher.Basic.LETTER);
+        builder.put("digit", CharMatcher.Basic.DIGIT);
+        builder.put("whitespace", CharMatcher.Basic.WHITESPACE);
+        builder.put("punctuation", CharMatcher.Basic.PUNCTUATION);
+        builder.put("symbol", CharMatcher.Basic.SYMBOL);
         // Populate with unicode categories from java.lang.Character
         for (Field field : Character.class.getFields()) {
             if (!field.getName().startsWith("DIRECTIONALITY")
@@ -63,14 +62,14 @@ public class NGramTokenizerFactory extends AbstractTokenizerFactory {
                     && Modifier.isStatic(field.getModifiers())
                     && field.getType() == byte.class) {
                 try {
-                    matchers.put(field.getName().toLowerCase(Locale.ROOT), CharMatcher.ByUnicodeCategory.of(field.getByte(null)));
+                    builder.put(field.getName().toLowerCase(Locale.ROOT), CharMatcher.ByUnicodeCategory.of(field.getByte(null)));
                 } catch (Exception e) {
                     // just ignore
                     continue;
                 }
             }
         }
-        MATCHERS = unmodifiableMap(matchers);
+        MATCHERS = builder.build();
     }
 
     static CharMatcher parseTokenChars(String[] characterClasses) {
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzerProvider.java b/core/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzerProvider.java
index a3f9004..39cf56b 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzerProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzerProvider.java
@@ -18,11 +18,13 @@
  */
 package org.elasticsearch.index.analysis;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.analysis.core.StopAnalyzer;
 import org.apache.lucene.analysis.de.GermanAnalyzer;
 import org.apache.lucene.analysis.fr.FrenchAnalyzer;
 import org.apache.lucene.analysis.nl.DutchAnalyzer;
 import org.apache.lucene.analysis.util.CharArraySet;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.assistedinject.Assisted;
 import org.elasticsearch.common.settings.Settings;
@@ -30,11 +32,6 @@ import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.Index;
 import org.elasticsearch.index.settings.IndexSettings;
 
-import java.util.HashMap;
-import java.util.Map;
-
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * Creates a SnowballAnalyzer initialized with stopwords and Snowball filter. Only
  * supports Dutch, English (default), French, German and German2 where stopwords
@@ -49,17 +46,14 @@ import static java.util.Collections.unmodifiableMap;
  *
  */
 public class SnowballAnalyzerProvider extends AbstractIndexAnalyzerProvider<SnowballAnalyzer> {
-    private static final Map<String, CharArraySet> DEFAULT_LANGUAGE_STOPWORDS;
 
-    static {
-        Map<String, CharArraySet> defaultLanguageStopwords = new HashMap<>();
-        defaultLanguageStopwords.put("English", StopAnalyzer.ENGLISH_STOP_WORDS_SET);
-        defaultLanguageStopwords.put("Dutch", DutchAnalyzer.getDefaultStopSet());
-        defaultLanguageStopwords.put("German", GermanAnalyzer.getDefaultStopSet());
-        defaultLanguageStopwords.put("German2", GermanAnalyzer.getDefaultStopSet());
-        defaultLanguageStopwords.put("French", FrenchAnalyzer.getDefaultStopSet());
-        DEFAULT_LANGUAGE_STOPWORDS = unmodifiableMap(defaultLanguageStopwords);
-    }
+    private static final ImmutableMap<String, CharArraySet> defaultLanguageStopwords = MapBuilder.<String, CharArraySet>newMapBuilder()
+            .put("English", StopAnalyzer.ENGLISH_STOP_WORDS_SET)
+            .put("Dutch", DutchAnalyzer.getDefaultStopSet())
+            .put("German", GermanAnalyzer.getDefaultStopSet())
+            .put("German2", GermanAnalyzer.getDefaultStopSet())
+            .put("French", FrenchAnalyzer.getDefaultStopSet())
+            .immutableMap();
 
     private final SnowballAnalyzer analyzer;
 
@@ -68,7 +62,7 @@ public class SnowballAnalyzerProvider extends AbstractIndexAnalyzerProvider<Snow
         super(index, indexSettings, name, settings);
 
         String language = settings.get("language", settings.get("name", "English"));
-        CharArraySet defaultStopwords = DEFAULT_LANGUAGE_STOPWORDS.getOrDefault(language, CharArraySet.EMPTY_SET);
+        CharArraySet defaultStopwords = defaultLanguageStopwords.containsKey(language) ? defaultLanguageStopwords.get(language) : CharArraySet.EMPTY_SET;
         CharArraySet stopWords = Analysis.parseStopWords(env, settings, defaultStopwords);
 
         analyzer = new SnowballAnalyzer(language, stopWords);
diff --git a/core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java b/core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java
index 30c0905..f2b7ba8 100644
--- a/core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java
+++ b/core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java
@@ -19,11 +19,6 @@
 
 package org.elasticsearch.index.cache.bitset;
 
-import com.google.common.cache.Cache;
-import com.google.common.cache.CacheBuilder;
-import com.google.common.cache.RemovalListener;
-import com.google.common.cache.RemovalNotification;
-
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.LeafReaderContext;
@@ -38,6 +33,10 @@ import org.apache.lucene.util.BitDocIdSet;
 import org.apache.lucene.util.BitSet;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.cache.Cache;
+import org.elasticsearch.common.cache.CacheBuilder;
+import org.elasticsearch.common.cache.RemovalListener;
+import org.elasticsearch.common.cache.RemovalNotification;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.settings.Settings;
@@ -58,10 +57,11 @@ import org.elasticsearch.threadpool.ThreadPool;
 import java.io.Closeable;
 import java.io.IOException;
 import java.util.HashSet;
-import java.util.Map;
 import java.util.Objects;
 import java.util.Set;
-import java.util.concurrent.*;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Executor;
 
 /**
  * This is a cache for {@link BitDocIdSet} based filters and is unbounded by size or time.
@@ -94,10 +94,11 @@ public class BitsetFilterCache extends AbstractIndexComponent implements LeafRea
     public BitsetFilterCache(Index index, @IndexSettings Settings indexSettings) {
         super(index, indexSettings);
         this.loadRandomAccessFiltersEagerly = indexSettings.getAsBoolean(LOAD_RANDOM_ACCESS_FILTERS_EAGERLY, true);
-        this.loadedFilters = CacheBuilder.newBuilder().removalListener(this).build();
+        this.loadedFilters = CacheBuilder.<Object, Cache<Query, Value>>builder().removalListener(this).build();
         this.warmer = new BitSetProducerWarmer();
     }
 
+
     @Inject(optional = true)
     public void setIndicesWarmer(IndicesWarmer indicesWarmer) {
         this.indicesWarmer = indicesWarmer;
@@ -144,14 +145,12 @@ public class BitsetFilterCache extends AbstractIndexComponent implements LeafRea
     private BitSet getAndLoadIfNotPresent(final Query query, final LeafReaderContext context) throws IOException, ExecutionException {
         final Object coreCacheReader = context.reader().getCoreCacheKey();
         final ShardId shardId = ShardUtils.extractShardId(context.reader());
-        Cache<Query, Value> filterToFbs = loadedFilters.get(coreCacheReader, new Callable<Cache<Query, Value>>() {
-            @Override
-            public Cache<Query, Value> call() throws Exception {
-                context.reader().addCoreClosedListener(BitsetFilterCache.this);
-                return CacheBuilder.newBuilder().build();
-            }
+        Cache<Query, Value> filterToFbs = loadedFilters.computeIfAbsent(coreCacheReader, key -> {
+            context.reader().addCoreClosedListener(BitsetFilterCache.this);
+            return CacheBuilder.<Query, Value>builder().build();
         });
-        return filterToFbs.get(query, () -> {
+
+        return filterToFbs.computeIfAbsent(query, key -> {
             final IndexReaderContext topLevelContext = ReaderUtil.getTopLevelContext(context);
             final IndexSearcher searcher = new IndexSearcher(topLevelContext);
             searcher.setQueryCache(null);
@@ -172,8 +171,7 @@ public class BitsetFilterCache extends AbstractIndexComponent implements LeafRea
 
     @Override
     public void onRemoval(RemovalNotification<Object, Cache<Query, Value>> notification) {
-        Object key = notification.getKey();
-        if (key == null) {
+        if (notification.getKey() == null) {
             return;
         }
 
@@ -182,7 +180,7 @@ public class BitsetFilterCache extends AbstractIndexComponent implements LeafRea
             return;
         }
 
-        for (Value value : valueCache.asMap().values()) {
+        for (Value value : valueCache.values()) {
             listener.onRemoval(value.shardId, value.bitset);
             // if null then this means the shard has already been removed and the stats are 0 anyway for the shard this key belongs to
         }
diff --git a/core/src/main/java/org/elasticsearch/index/cache/request/ShardRequestCache.java b/core/src/main/java/org/elasticsearch/index/cache/request/ShardRequestCache.java
index ef82e73..0f594d2 100644
--- a/core/src/main/java/org/elasticsearch/index/cache/request/ShardRequestCache.java
+++ b/core/src/main/java/org/elasticsearch/index/cache/request/ShardRequestCache.java
@@ -19,10 +19,8 @@
 
 package org.elasticsearch.index.cache.request;
 
-import com.google.common.cache.RemovalListener;
-import com.google.common.cache.RemovalNotification;
-
-import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.cache.RemovalListener;
+import org.elasticsearch.common.cache.RemovalNotification;
 import org.elasticsearch.common.metrics.CounterMetric;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.settings.IndexSettings;
@@ -61,7 +59,7 @@ public class ShardRequestCache extends AbstractIndexShardComponent implements Re
 
     @Override
     public void onRemoval(RemovalNotification<IndicesRequestCache.Key, IndicesRequestCache.Value> removalNotification) {
-        if (removalNotification.wasEvicted()) {
+        if (removalNotification.getRemovalReason() == RemovalNotification.RemovalReason.EVICTED) {
             evictionsMetric.inc();
         }
         long dec = 0;
diff --git a/core/src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormat.java b/core/src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormat.java
deleted file mode 100644
index 71a52a7..0000000
--- a/core/src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormat.java
+++ /dev/null
@@ -1,440 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.codec.postingsformat;
-
-import org.apache.lucene.codecs.*;
-import org.apache.lucene.index.*;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.store.*;
-import org.apache.lucene.util.*;
-import org.elasticsearch.common.util.BloomFilter;
-
-import java.io.IOException;
-import java.util.*;
-import java.util.Map.Entry;
-
-/**
- * <p>
- * A {@link PostingsFormat} useful for low doc-frequency fields such as primary
- * keys. Bloom filters are maintained in a ".blm" file which offers "fast-fail"
- * for reads in segments known to have no record of the key. A choice of
- * delegate PostingsFormat is used to record all other Postings data.
- * </p>
- * <p>
- * This is a special bloom filter version, based on {@link org.elasticsearch.common.util.BloomFilter} and inspired
- * by Lucene {@code org.apache.lucene.codecs.bloom.BloomFilteringPostingsFormat}.
- * @deprecated only for reading old segments
- */
-@Deprecated
-public class BloomFilterPostingsFormat extends PostingsFormat {
-
-    public static final String BLOOM_CODEC_NAME = "XBloomFilter"; // the Lucene one is named BloomFilter
-    public static final int BLOOM_CODEC_VERSION = 1;
-    public static final int BLOOM_CODEC_VERSION_CHECKSUM = 2;
-    public static final int BLOOM_CODEC_VERSION_CURRENT = BLOOM_CODEC_VERSION_CHECKSUM;
-
-    /**
-     * Extension of Bloom Filters file
-     */
-    static final String BLOOM_EXTENSION = "blm";
-
-    private BloomFilter.Factory bloomFilterFactory = BloomFilter.Factory.DEFAULT;
-    private PostingsFormat delegatePostingsFormat;
-
-    /**
-     * Creates Bloom filters for a selection of fields created in the index. This
-     * is recorded as a set of Bitsets held as a segment summary in an additional
-     * "blm" file. This PostingsFormat delegates to a choice of delegate
-     * PostingsFormat for encoding all other postings data.
-     *
-     * @param delegatePostingsFormat The PostingsFormat that records all the non-bloom filter data i.e.
-     *                               postings info.
-     * @param bloomFilterFactory     The {@link org.elasticsearch.common.util.BloomFilter.Factory} responsible for sizing BloomFilters
-     *                               appropriately
-     */
-    public BloomFilterPostingsFormat(PostingsFormat delegatePostingsFormat,
-                                     BloomFilter.Factory bloomFilterFactory) {
-        super(BLOOM_CODEC_NAME);
-        this.delegatePostingsFormat = delegatePostingsFormat;
-        this.bloomFilterFactory = bloomFilterFactory;
-    }
-
-    // Used only by core Lucene at read-time via Service Provider instantiation -
-    // do not use at Write-time in application code.
-    public BloomFilterPostingsFormat() {
-        super(BLOOM_CODEC_NAME);
-    }
-
-    @Override
-    public BloomFilteredFieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-        throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-
-    @Override
-    public BloomFilteredFieldsProducer fieldsProducer(SegmentReadState state)
-            throws IOException {
-        return new BloomFilteredFieldsProducer(state);
-    }
-
-    public PostingsFormat getDelegate() {
-        return delegatePostingsFormat;
-    }
-
-    private final class LazyBloomLoader implements Accountable {
-        private final long offset;
-        private final IndexInput indexInput;
-        private BloomFilter filter;
-
-        private LazyBloomLoader(long offset, IndexInput origial) {
-            this.offset = offset;
-            this.indexInput = origial.clone();
-        }
-
-        synchronized BloomFilter get() throws IOException {
-            if (filter == null) {
-                try (final IndexInput input = indexInput) {
-                    input.seek(offset);
-                    this.filter = BloomFilter.deserialize(input);
-                }
-            }
-            return filter;
-        }
-
-        @Override
-        public long ramBytesUsed() {
-            return filter == null ? 0l : filter.getSizeInBytes();
-        }
-
-        @Override
-        public Collection<Accountable> getChildResources() {
-            return Collections.singleton(Accountables.namedAccountable("bloom", ramBytesUsed()));
-        }
-    }
-
-    public final class BloomFilteredFieldsProducer extends FieldsProducer {
-        private FieldsProducer delegateFieldsProducer;
-        HashMap<String, LazyBloomLoader> bloomsByFieldName = new HashMap<>();
-        private final int version;
-        private final IndexInput data;
-
-        // for internal use only
-        FieldsProducer getDelegate() {
-            return delegateFieldsProducer;
-        }
-
-        public BloomFilteredFieldsProducer(SegmentReadState state)
-                throws IOException {
-
-            final String bloomFileName = IndexFileNames.segmentFileName(
-                    state.segmentInfo.name, state.segmentSuffix, BLOOM_EXTENSION);
-            final Directory directory = state.directory;
-            IndexInput dataInput = directory.openInput(bloomFileName, state.context);
-            try {
-                ChecksumIndexInput bloomIn = new BufferedChecksumIndexInput(dataInput.clone());
-                version = CodecUtil.checkHeader(bloomIn, BLOOM_CODEC_NAME, BLOOM_CODEC_VERSION,
-                        BLOOM_CODEC_VERSION_CURRENT);
-                // // Load the hash function used in the BloomFilter
-                // hashFunction = HashFunction.forName(bloomIn.readString());
-                // Load the delegate postings format
-               final String delegatePostings = bloomIn.readString();
-                this.delegateFieldsProducer = PostingsFormat.forName(delegatePostings)
-                        .fieldsProducer(state);
-                this.data = dataInput;
-                dataInput = null; // null it out such that we don't close it
-            } finally {
-                IOUtils.closeWhileHandlingException(dataInput);
-            }
-        }
-
-        @Override
-        public Iterator<String> iterator() {
-            return delegateFieldsProducer.iterator();
-        }
-
-        @Override
-        public void close() throws IOException {
-            IOUtils.close(data, delegateFieldsProducer);
-        }
-
-        @Override
-        public Terms terms(String field) throws IOException {
-            LazyBloomLoader filter = bloomsByFieldName.get(field);
-            if (filter == null) {
-                return delegateFieldsProducer.terms(field);
-            } else {
-                Terms result = delegateFieldsProducer.terms(field);
-                if (result == null) {
-                    return null;
-                }
-                return new BloomFilteredTerms(result, filter.get());
-            }
-        }
-
-        @Override
-        public int size() {
-            return delegateFieldsProducer.size();
-        }
-
-        @Override
-        public long ramBytesUsed() {
-            long size = delegateFieldsProducer.ramBytesUsed();
-            for (LazyBloomLoader bloomFilter : bloomsByFieldName.values()) {
-                size += bloomFilter.ramBytesUsed();
-            }
-            return size;
-        }
-
-        @Override
-        public Collection<Accountable> getChildResources() {
-            List<Accountable> resources = new ArrayList<>();
-            resources.addAll(Accountables.namedAccountables("field", bloomsByFieldName));
-            if (delegateFieldsProducer != null) {
-                resources.add(Accountables.namedAccountable("delegate", delegateFieldsProducer));
-            }
-            return Collections.unmodifiableList(resources);
-        }
-
-        @Override
-        public void checkIntegrity() throws IOException {
-            delegateFieldsProducer.checkIntegrity();
-            if (version >= BLOOM_CODEC_VERSION_CHECKSUM) {
-                CodecUtil.checksumEntireFile(data);
-            }
-        }
-
-        @Override
-        public FieldsProducer getMergeInstance() throws IOException {
-            return delegateFieldsProducer.getMergeInstance();
-        }
-    }
-
-    public static final class BloomFilteredTerms extends FilterLeafReader.FilterTerms {
-        private BloomFilter filter;
-
-        public BloomFilteredTerms(Terms terms, BloomFilter filter) {
-            super(terms);
-            this.filter = filter;
-        }
-
-        public BloomFilter getFilter() {
-            return filter;
-        }
-
-        @Override
-        public TermsEnum iterator() throws IOException {
-            return new BloomFilteredTermsEnum(this.in, filter);
-        }
-    }
-
-    static final class BloomFilteredTermsEnum extends TermsEnum {
-
-        private Terms delegateTerms;
-        private TermsEnum delegateTermsEnum;
-        private BloomFilter filter;
-
-        public BloomFilteredTermsEnum(Terms other, BloomFilter filter) {
-            this.delegateTerms = other;
-            this.filter = filter;
-        }
-
-        void reset(Terms others) {
-            this.delegateTermsEnum = null;
-            this.delegateTerms = others;
-        }
-
-        private TermsEnum getDelegate() throws IOException {
-            if (delegateTermsEnum == null) {
-                /* pull the iterator only if we really need it -
-                 * this can be a relatively heavy operation depending on the 
-                 * delegate postings format and they underlying directory
-                 * (clone IndexInput) */
-                delegateTermsEnum = delegateTerms.iterator();
-            }
-            return delegateTermsEnum;
-        }
-
-        @Override
-        public final BytesRef next() throws IOException {
-            return getDelegate().next();
-        }
-
-        @Override
-        public final boolean seekExact(BytesRef text)
-                throws IOException {
-            // The magical fail-fast speed up that is the entire point of all of
-            // this code - save a disk seek if there is a match on an in-memory
-            // structure
-            // that may occasionally give a false positive but guaranteed no false
-            // negatives
-            if (!filter.mightContain(text)) {
-                return false;
-            }
-            return getDelegate().seekExact(text);
-        }
-
-        @Override
-        public final SeekStatus seekCeil(BytesRef text)
-                throws IOException {
-            return getDelegate().seekCeil(text);
-        }
-
-        @Override
-        public final void seekExact(long ord) throws IOException {
-            getDelegate().seekExact(ord);
-        }
-
-        @Override
-        public final BytesRef term() throws IOException {
-            return getDelegate().term();
-        }
-
-        @Override
-        public final long ord() throws IOException {
-            return getDelegate().ord();
-        }
-
-        @Override
-        public final int docFreq() throws IOException {
-            return getDelegate().docFreq();
-        }
-
-        @Override
-        public final long totalTermFreq() throws IOException {
-            return getDelegate().totalTermFreq();
-        }
-
-
-        @Override
-        public PostingsEnum postings(PostingsEnum reuse, int flags) throws IOException {
-            return getDelegate().postings(reuse, flags);
-        }
-    }
-
-    // TODO: would be great to move this out to test code, but the interaction between es090 and bloom is complex
-    // at least it is not accessible via SPI
-    public final class BloomFilteredFieldsConsumer extends FieldsConsumer {
-        private final FieldsConsumer delegateFieldsConsumer;
-        private final Map<FieldInfo, BloomFilter> bloomFilters = new HashMap<>();
-        private final SegmentWriteState state;
-        private boolean closed = false;
-
-        // private PostingsFormat delegatePostingsFormat;
-
-        public BloomFilteredFieldsConsumer(FieldsConsumer fieldsConsumer,
-                                           SegmentWriteState state, PostingsFormat delegatePostingsFormat) {
-            this.delegateFieldsConsumer = fieldsConsumer;
-            // this.delegatePostingsFormat=delegatePostingsFormat;
-            this.state = state;
-        }
-
-        // for internal use only
-        public FieldsConsumer getDelegate() {
-            return delegateFieldsConsumer;
-        }
-
-
-        @Override
-        public void write(Fields fields) throws IOException {
-
-            // Delegate must write first: it may have opened files
-            // on creating the class
-            // (e.g. Lucene41PostingsConsumer), and write() will
-            // close them; alternatively, if we delayed pulling
-            // the fields consumer until here, we could do it
-            // afterwards:
-            delegateFieldsConsumer.write(fields);
-
-            for(String field : fields) {
-                Terms terms = fields.terms(field);
-                if (terms == null) {
-                    continue;
-                }
-                FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);
-                TermsEnum termsEnum = terms.iterator();
-
-                BloomFilter bloomFilter = null;
-
-                PostingsEnum postings = null;
-                while (true) {
-                    BytesRef term = termsEnum.next();
-                    if (term == null) {
-                        break;
-                    }
-                    if (bloomFilter == null) {
-                        bloomFilter = bloomFilterFactory.createFilter(state.segmentInfo.maxDoc());
-                        assert bloomFilters.containsKey(field) == false;
-                        bloomFilters.put(fieldInfo, bloomFilter);
-                    }
-                    // Make sure there's at least one doc for this term:
-                    postings = termsEnum.postings(postings, 0);
-                    if (postings.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
-                        bloomFilter.put(term);
-                    }
-                }
-            }
-        }
-
-        @Override
-        public void close() throws IOException {
-            if (closed) {
-                return;
-            }
-            closed = true;
-            delegateFieldsConsumer.close();
-            // Now we are done accumulating values for these fields
-            List<Entry<FieldInfo, BloomFilter>> nonSaturatedBlooms = new ArrayList<>();
-
-            for (Entry<FieldInfo, BloomFilter> entry : bloomFilters.entrySet()) {
-                nonSaturatedBlooms.add(entry);
-            }
-            String bloomFileName = IndexFileNames.segmentFileName(
-                    state.segmentInfo.name, state.segmentSuffix, BLOOM_EXTENSION);
-            IndexOutput bloomOutput = null;
-            try {
-                bloomOutput = state.directory
-                        .createOutput(bloomFileName, state.context);
-                CodecUtil.writeHeader(bloomOutput, BLOOM_CODEC_NAME,
-                        BLOOM_CODEC_VERSION_CURRENT);
-                // remember the name of the postings format we will delegate to
-                bloomOutput.writeString(delegatePostingsFormat.getName());
-
-                // First field in the output file is the number of fields+blooms saved
-                bloomOutput.writeInt(nonSaturatedBlooms.size());
-                for (Entry<FieldInfo, BloomFilter> entry : nonSaturatedBlooms) {
-                    FieldInfo fieldInfo = entry.getKey();
-                    BloomFilter bloomFilter = entry.getValue();
-                    bloomOutput.writeInt(fieldInfo.number);
-                    saveAppropriatelySizedBloomFilter(bloomOutput, bloomFilter, fieldInfo);
-                }
-                CodecUtil.writeFooter(bloomOutput);
-            } finally {
-                IOUtils.close(bloomOutput);
-            }
-            //We are done with large bitsets so no need to keep them hanging around
-            bloomFilters.clear();
-        }
-
-        private void saveAppropriatelySizedBloomFilter(IndexOutput bloomOutput,
-                                                       BloomFilter bloomFilter, FieldInfo fieldInfo) throws IOException {
-            BloomFilter.serilaize(bloomFilter, bloomOutput);
-        }
-
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/codec/postingsformat/Elasticsearch090PostingsFormat.java b/core/src/main/java/org/elasticsearch/index/codec/postingsformat/Elasticsearch090PostingsFormat.java
deleted file mode 100644
index b6ceba8..0000000
--- a/core/src/main/java/org/elasticsearch/index/codec/postingsformat/Elasticsearch090PostingsFormat.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.codec.postingsformat;
-
-import com.google.common.collect.Iterators;
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FilterLeafReader;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.util.BloomFilter;
-import org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat.BloomFilteredFieldsConsumer;
-import org.elasticsearch.index.mapper.internal.UidFieldMapper;
-
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.function.Predicate;
-
-/**
- * This is the old default postings format for Elasticsearch that special cases
- * the <tt>_uid</tt> field to use a bloom filter while all other fields
- * will use a {@link Lucene50PostingsFormat}. This format will reuse the underlying
- * {@link Lucene50PostingsFormat} and its files also for the <tt>_uid</tt> saving up to
- * 5 files per segment in the default case.
- * <p>
- * @deprecated only for reading old segments
- */
-@Deprecated
-public class Elasticsearch090PostingsFormat extends PostingsFormat {
-    protected final BloomFilterPostingsFormat bloomPostings;
-
-    public Elasticsearch090PostingsFormat() {
-        super("es090");
-        Lucene50PostingsFormat delegate = new Lucene50PostingsFormat();
-        assert delegate.getName().equals(Lucene.LATEST_POSTINGS_FORMAT);
-        bloomPostings = new BloomFilterPostingsFormat(delegate, BloomFilter.Factory.DEFAULT);
-    }
-
-    public PostingsFormat getDefaultWrapped() {
-        return bloomPostings.getDelegate();
-    }
-
-    protected static final Predicate<String> UID_FIELD_FILTER = field -> UidFieldMapper.NAME.equals(field);
-
-    @Override
-    public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-        throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-
-    @Override
-    public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-        // we can just return the delegate here since we didn't record bloom filters for 
-        // the other fields. 
-        return bloomPostings.fieldsProducer(state);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/index/engine/CreateFailedEngineException.java b/core/src/main/java/org/elasticsearch/index/engine/CreateFailedEngineException.java
deleted file mode 100644
index 32d9ee6..0000000
--- a/core/src/main/java/org/elasticsearch/index/engine/CreateFailedEngineException.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.engine;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.index.shard.ShardId;
-
-import java.io.IOException;
-import java.util.Objects;
-
-/**
- *
- */
-public class CreateFailedEngineException extends EngineException {
-
-    private final String type;
-
-    private final String id;
-
-    public CreateFailedEngineException(ShardId shardId, String type, String id, Throwable cause) {
-        super(shardId, "Create failed for [" + type + "#" + id + "]", cause);
-        Objects.requireNonNull(type, "type must not be null");
-        Objects.requireNonNull(id, "id must not be null");
-        this.type = type;
-        this.id = id;
-    }
-
-    public CreateFailedEngineException(StreamInput in) throws IOException{
-        super(in);
-        type = in.readString();
-        id = in.readString();
-    }
-
-    public String type() {
-        return this.type;
-    }
-
-    public String id() {
-        return this.id;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeString(type);
-        out.writeString(id);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/engine/DeleteByQueryFailedEngineException.java b/core/src/main/java/org/elasticsearch/index/engine/DeleteByQueryFailedEngineException.java
deleted file mode 100644
index 95d57c5..0000000
--- a/core/src/main/java/org/elasticsearch/index/engine/DeleteByQueryFailedEngineException.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.engine;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.index.shard.ShardId;
-
-import java.io.IOException;
-
-/** @deprecated Delete-by-query is removed in 2.0, but we keep this so translog can replay on upgrade. */
-@Deprecated
-public class DeleteByQueryFailedEngineException extends EngineException {
-
-    public DeleteByQueryFailedEngineException(ShardId shardId, Engine.DeleteByQuery deleteByQuery, Throwable cause) {
-        super(shardId, "Delete by query failed for [" + deleteByQuery.query() + "]", cause);
-    }
-
-    public DeleteByQueryFailedEngineException(StreamInput in) throws IOException{
-        super(in);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/engine/DocumentAlreadyExistsException.java b/core/src/main/java/org/elasticsearch/index/engine/DocumentAlreadyExistsException.java
deleted file mode 100644
index 467dd8c..0000000
--- a/core/src/main/java/org/elasticsearch/index/engine/DocumentAlreadyExistsException.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.engine;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.rest.RestStatus;
-
-import java.io.IOException;
-
-/**
- *
- */
-public class DocumentAlreadyExistsException extends EngineException {
-
-    public DocumentAlreadyExistsException(ShardId shardId, String type, String id) {
-        super(shardId, "[" + type + "][" + id + "]: document already exists");
-    }
-
-    public DocumentAlreadyExistsException(StreamInput in) throws IOException{
-        super(in);
-    }
-
-    @Override
-    public RestStatus status() {
-        return RestStatus.CONFLICT;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/engine/Engine.java b/core/src/main/java/org/elasticsearch/index/engine/Engine.java
index 1330ef0..c07be06 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/Engine.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/Engine.java
@@ -45,7 +45,6 @@ import org.elasticsearch.index.mapper.ParseContext.Document;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.Uid;
 import org.elasticsearch.index.merge.MergeStats;
-import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.index.store.Store;
 import org.elasticsearch.index.translog.Translog;
@@ -60,7 +59,6 @@ import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 import java.util.function.Function;
-import java.util.function.Supplier;
 
 /**
  *
@@ -144,7 +142,8 @@ public abstract class Engine implements Closeable {
         return new MergeStats();
     }
 
-    /** A throttling class that can be activated, causing the
+    /**
+     * A throttling class that can be activated, causing the
      * {@code acquireThrottle} method to block on a lock when throttling
      * is enabled
      */
@@ -203,20 +202,15 @@ public abstract class Engine implements Closeable {
         }
     }
 
-    public abstract void create(Create create) throws EngineException;
-
-    public abstract boolean index(Index index) throws EngineException;
+    public abstract boolean index(Index operation) throws EngineException;
 
     public abstract void delete(Delete delete) throws EngineException;
 
-    /** @deprecated This was removed, but we keep this API so translog can replay any DBQs on upgrade. */
-    @Deprecated
-    public abstract void delete(DeleteByQuery delete) throws EngineException;
-
     /**
      * Attempts to do a special commit where the given syncID is put into the commit data. The attempt
      * succeeds if there are not pending writes in lucene and the current point is equal to the expected one.
-     * @param syncId id of this sync
+     *
+     * @param syncId           id of this sync
      * @param expectedCommitId the expected value of
      * @return true if the sync commit was made, false o.w.
      */
@@ -243,7 +237,8 @@ public abstract class Engine implements Closeable {
             if (get.versionType().isVersionConflictForReads(docIdAndVersion.version, get.version())) {
                 Releasables.close(searcher);
                 Uid uid = Uid.createUid(get.uid().text());
-                throw new VersionConflictEngineException(shardId, uid.type(), uid.id(), docIdAndVersion.version, get.version());
+                throw new VersionConflictEngineException(shardId, uid.type(), uid.id(),
+                        get.versionType().explainConflictForReads(docIdAndVersion.version, get.version()));
             }
         }
 
@@ -328,7 +323,7 @@ public abstract class Engine implements Closeable {
         } catch (IOException e) {
             // Fall back to reading from the store if reading from the commit fails
             try {
-                return store. readLastCommittedSegmentsInfo();
+                return store.readLastCommittedSegmentsInfo();
             } catch (IOException e2) {
                 e2.addSuppressed(e);
                 throw e2;
@@ -366,6 +361,9 @@ public abstract class Engine implements Closeable {
         stats.addIndexWriterMaxMemoryInBytes(0);
     }
 
+    /** How much heap Lucene's IndexWriter is using */
+    abstract public long indexWriterRAMBytesUsed();
+
     protected Segment[] getSegmentInfo(SegmentInfos lastCommittedSegmentInfos, boolean verbose) {
         ensureOpen();
         Map<String, Segment> segments = new HashMap<>();
@@ -469,7 +467,8 @@ public abstract class Engine implements Closeable {
 
     /**
      * Flushes the state of the engine including the transaction log, clearing memory.
-     * @param force if <code>true</code> a lucene commit is executed even if no changes need to be committed.
+     *
+     * @param force         if <code>true</code> a lucene commit is executed even if no changes need to be committed.
      * @param waitIfOngoing if <code>true</code> this call will block until all currently running flushes have finished.
      *                      Otherwise this call will return without blocking.
      * @return the commit Id for the resulting commit
@@ -607,89 +606,43 @@ public abstract class Engine implements Closeable {
         }
     }
 
-    public static interface Operation {
-        static enum Type {
-            CREATE,
-            INDEX,
-            DELETE
-        }
-
-        static enum Origin {
-            PRIMARY,
-            REPLICA,
-            RECOVERY
-        }
-
-        Type opType();
-
-        Origin origin();
-    }
-
-    public static abstract class IndexingOperation implements Operation {
-
+    public static abstract class Operation {
         private final Term uid;
-        private final ParsedDocument doc;
         private long version;
         private final VersionType versionType;
         private final Origin origin;
         private Translog.Location location;
-
         private final long startTime;
         private long endTime;
 
-        public IndexingOperation(Term uid, ParsedDocument doc, long version, VersionType versionType, Origin origin, long startTime) {
+        public Operation(Term uid, long version, VersionType versionType, Origin origin, long startTime) {
             this.uid = uid;
-            this.doc = doc;
             this.version = version;
             this.versionType = versionType;
             this.origin = origin;
             this.startTime = startTime;
         }
 
-        public IndexingOperation(Term uid, ParsedDocument doc) {
-            this(uid, doc, Versions.MATCH_ANY, VersionType.INTERNAL, Origin.PRIMARY, System.nanoTime());
+        public static enum Origin {
+            PRIMARY,
+            REPLICA,
+            RECOVERY
         }
 
-        @Override
         public Origin origin() {
             return this.origin;
         }
 
-        public ParsedDocument parsedDoc() {
-            return this.doc;
-        }
-
         public Term uid() {
             return this.uid;
         }
 
-        public String type() {
-            return this.doc.type();
-        }
-
-        public String id() {
-            return this.doc.id();
-        }
-
-        public String routing() {
-            return this.doc.routing();
-        }
-
-        public long timestamp() {
-            return this.doc.timestamp();
-        }
-
-        public long ttl() {
-            return this.doc.ttl();
-        }
-
         public long version() {
             return this.version;
         }
 
         public void updateVersion(long version) {
             this.version = version;
-            this.doc.version().setLongValue(version);
         }
 
         public void setTranslogLocation(Translog.Location location) {
@@ -704,18 +657,6 @@ public abstract class Engine implements Closeable {
             return this.versionType;
         }
 
-        public String parent() {
-            return this.doc.parent();
-        }
-
-        public List<Document> docs() {
-            return this.doc.docs();
-        }
-
-        public BytesReference source() {
-            return this.doc.source();
-        }
-
         /**
          * Returns operation start time in nanoseconds.
          */
@@ -733,78 +674,77 @@ public abstract class Engine implements Closeable {
         public long endTime() {
             return this.endTime;
         }
-
-        /**
-         * Execute this operation against the provided {@link IndexShard} and
-         * return whether the document was created.
-         */
-        public abstract boolean execute(IndexShard shard);
     }
 
-    public static final class Create extends IndexingOperation {
+    public static class Index extends Operation {
 
-        public Create(Term uid, ParsedDocument doc, long version, VersionType versionType, Origin origin, long startTime) {
-            super(uid, doc, version, versionType, origin, startTime);
+        private final ParsedDocument doc;
+
+        public Index(Term uid, ParsedDocument doc, long version, VersionType versionType, Origin origin, long startTime) {
+            super(uid, version, versionType, origin, startTime);
+            this.doc = doc;
         }
 
-        public Create(Term uid, ParsedDocument doc) {
-            super(uid, doc);
+        public Index(Term uid, ParsedDocument doc) {
+            this(uid, doc, Versions.MATCH_ANY);
         }
 
-        @Override
-        public Type opType() {
-            return Type.CREATE;
+        public Index(Term uid, ParsedDocument doc, long version) {
+            this(uid, doc, version, VersionType.INTERNAL, Origin.PRIMARY, System.nanoTime());
         }
 
-        @Override
-        public boolean execute(IndexShard shard) {
-            shard.create(this);
-            return true;
+        public ParsedDocument parsedDoc() {
+            return this.doc;
         }
-    }
 
-    public static final class Index extends IndexingOperation {
+        public String type() {
+            return this.doc.type();
+        }
 
-        public Index(Term uid, ParsedDocument doc, long version, VersionType versionType, Origin origin, long startTime) {
-            super(uid, doc, version, versionType, origin, startTime);
+        public String id() {
+            return this.doc.id();
         }
 
-        public Index(Term uid, ParsedDocument doc) {
-            super(uid, doc);
+        public String routing() {
+            return this.doc.routing();
         }
 
-        @Override
-        public Type opType() {
-            return Type.INDEX;
+        public long timestamp() {
+            return this.doc.timestamp();
+        }
+
+        public long ttl() {
+            return this.doc.ttl();
         }
 
         @Override
-        public boolean execute(IndexShard shard) {
-            return shard.index(this);
+        public void updateVersion(long version) {
+            super.updateVersion(version);
+            this.doc.version().setLongValue(version);
+        }
+
+        public String parent() {
+            return this.doc.parent();
+        }
+
+        public List<Document> docs() {
+            return this.doc.docs();
+        }
+
+        public BytesReference source() {
+            return this.doc.source();
         }
     }
 
-    public static class Delete implements Operation {
+    public static class Delete extends Operation {
         private final String type;
         private final String id;
-        private final Term uid;
-        private long version;
-        private final VersionType versionType;
-        private final Origin origin;
         private boolean found;
 
-        private final long startTime;
-        private long endTime;
-        private Translog.Location location;
-
         public Delete(String type, String id, Term uid, long version, VersionType versionType, Origin origin, long startTime, boolean found) {
+            super(uid, version, versionType, origin, startTime);
             this.type = type;
             this.id = id;
-            this.uid = uid;
-            this.version = version;
-            this.versionType = versionType;
-            this.origin = origin;
-            this.startTime = startTime;
             this.found = found;
         }
 
@@ -816,16 +756,6 @@ public abstract class Engine implements Closeable {
             this(template.type(), template.id(), template.uid(), template.version(), versionType, template.origin(), template.startTime(), template.found());
         }
 
-        @Override
-        public Type opType() {
-            return Type.DELETE;
-        }
-
-        @Override
-        public Origin origin() {
-            return this.origin;
-        }
-
         public String type() {
             return this.type;
         }
@@ -834,55 +764,14 @@ public abstract class Engine implements Closeable {
             return this.id;
         }
 
-        public Term uid() {
-            return this.uid;
-        }
-
         public void updateVersion(long version, boolean found) {
-            this.version = version;
+            updateVersion(version);
             this.found = found;
         }
 
-        /**
-         * before delete execution this is the version to be deleted. After this is the version of the "delete" transaction record.
-         */
-        public long version() {
-            return this.version;
-        }
-
-        public VersionType versionType() {
-            return this.versionType;
-        }
-
         public boolean found() {
             return this.found;
         }
-
-        /**
-         * Returns operation start time in nanoseconds.
-         */
-        public long startTime() {
-            return this.startTime;
-        }
-
-        public void endTime(long endTime) {
-            this.endTime = endTime;
-        }
-
-        /**
-         * Returns operation end time in nanoseconds.
-         */
-        public long endTime() {
-            return this.endTime;
-        }
-
-        public void setTranslogLocation(Translog.Location location) {
-            this.location = location;
-        }
-
-        public Translog.Location getTranslogLocation() {
-            return this.location;
-        }
     }
 
     public static class DeleteByQuery {
@@ -1135,12 +1024,18 @@ public abstract class Engine implements Closeable {
 
         @Override
         public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
+            if (this == o) {
+                return true;
+            }
+            if (o == null || getClass() != o.getClass()) {
+                return false;
+            }
 
             CommitId commitId = (CommitId) o;
 
-            if (!Arrays.equals(id, commitId.id)) return false;
+            if (!Arrays.equals(id, commitId.id)) {
+                return false;
+            }
 
             return true;
         }
@@ -1151,5 +1046,6 @@ public abstract class Engine implements Closeable {
         }
     }
 
-    public void onSettingsChanged() {}
+    public void onSettingsChanged() {
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java b/core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java
index a79587e..fd4b5da 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java
@@ -40,6 +40,7 @@ import org.elasticsearch.index.shard.TranslogRecoveryPerformer;
 import org.elasticsearch.index.store.Store;
 import org.elasticsearch.index.translog.TranslogConfig;
 import org.elasticsearch.indices.IndicesWarmer;
+import org.elasticsearch.indices.memory.IndexingMemoryController;
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.util.concurrent.TimeUnit;
@@ -107,8 +108,6 @@ public final class EngineConfig {
 
     public static final TimeValue DEFAULT_REFRESH_INTERVAL = new TimeValue(1, TimeUnit.SECONDS);
     public static final TimeValue DEFAULT_GC_DELETES = TimeValue.timeValueSeconds(60);
-    public static final ByteSizeValue DEFAULT_INDEX_BUFFER_SIZE = new ByteSizeValue(64, ByteSizeUnit.MB);
-    public static final ByteSizeValue INACTIVE_SHARD_INDEXING_BUFFER = ByteSizeValue.parseBytesSizeValue("500kb", "INACTIVE_SHARD_INDEXING_BUFFER");
 
     public static final String DEFAULT_VERSION_MAP_SIZE = "25%";
 
@@ -139,7 +138,8 @@ public final class EngineConfig {
         this.failedEngineListener = failedEngineListener;
         this.compoundOnFlush = indexSettings.getAsBoolean(EngineConfig.INDEX_COMPOUND_ON_FLUSH, compoundOnFlush);
         codecName = indexSettings.get(EngineConfig.INDEX_CODEC_SETTING, EngineConfig.DEFAULT_CODEC_NAME);
-        indexingBufferSize = DEFAULT_INDEX_BUFFER_SIZE;
+        // We start up inactive and rely on IndexingMemoryController to give us our fair share once we start indexing:
+        indexingBufferSize = IndexingMemoryController.INACTIVE_SHARD_INDEXING_BUFFER;
         gcDeletesInMillis = indexSettings.getAsTime(INDEX_GC_DELETES_SETTING, EngineConfig.DEFAULT_GC_DELETES).millis();
         versionMapSizeSetting = indexSettings.get(INDEX_VERSION_MAP_SIZE, DEFAULT_VERSION_MAP_SIZE);
         updateVersionMapSize();
@@ -258,10 +258,10 @@ public final class EngineConfig {
 
     /**
      * Returns a {@link org.elasticsearch.index.indexing.ShardIndexingService} used inside the engine to inform about
-     * pre and post index and create operations. The operations are used for statistic purposes etc.
+     * pre and post index. The operations are used for statistic purposes etc.
      *
-     * @see org.elasticsearch.index.indexing.ShardIndexingService#postCreate(org.elasticsearch.index.engine.Engine.Create)
-     * @see org.elasticsearch.index.indexing.ShardIndexingService#preCreate(org.elasticsearch.index.engine.Engine.Create)
+     * @see org.elasticsearch.index.indexing.ShardIndexingService#postIndex(Engine.Index)
+     * @see org.elasticsearch.index.indexing.ShardIndexingService#preIndex(Engine.Index)
      *
      */
     public ShardIndexingService getIndexingService() {
diff --git a/core/src/main/java/org/elasticsearch/index/engine/EngineException.java b/core/src/main/java/org/elasticsearch/index/engine/EngineException.java
index d7487ef..23f6be7 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/EngineException.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/EngineException.java
@@ -30,16 +30,16 @@ import java.io.IOException;
  */
 public class EngineException extends ElasticsearchException {
 
-    public EngineException(ShardId shardId, String msg) {
-        this(shardId, msg, null);
+    public EngineException(ShardId shardId, String msg, Object... params) {
+        this(shardId, msg, null, params);
     }
 
-    public EngineException(ShardId shardId, String msg, Throwable cause) {
-        super(msg, cause);
+    public EngineException(ShardId shardId, String msg, Throwable cause, Object... params) {
+        super(msg, cause, params);
         setShard(shardId);
     }
 
-    public EngineException(StreamInput in) throws IOException{
+    public EngineException(StreamInput in) throws IOException {
         super(in);
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java b/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
index 227212d..3973b47 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
@@ -21,8 +21,9 @@ package org.elasticsearch.index.engine;
 
 import org.apache.lucene.index.*;
 import org.apache.lucene.index.IndexWriter.IndexReaderWarmer;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.*;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.SearcherFactory;
+import org.apache.lucene.search.SearcherManager;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.LockObtainFailedException;
@@ -31,7 +32,7 @@ import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.InfoStream;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.cluster.routing.DjbHashFunction;
+import org.elasticsearch.cluster.routing.Murmur3HashFunction;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.logging.ESLogger;
@@ -48,7 +49,6 @@ import org.elasticsearch.index.indexing.ShardIndexingService;
 import org.elasticsearch.index.mapper.Uid;
 import org.elasticsearch.index.merge.MergeStats;
 import org.elasticsearch.index.merge.OnGoingMerge;
-import org.elasticsearch.index.search.nested.IncludeNestedDocsQuery;
 import org.elasticsearch.index.shard.ElasticsearchMergePolicy;
 import org.elasticsearch.index.shard.MergeSchedulerConfig;
 import org.elasticsearch.index.shard.ShardId;
@@ -67,7 +67,6 @@ import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
 import java.util.function.Function;
-import java.util.function.Supplier;
 
 /**
  *
@@ -182,8 +181,7 @@ public class InternalEngine extends Engine {
             }
             translogConfig.setTranslogGeneration(generation);
             if (generation != null && generation.translogUUID == null) {
-                // only upgrade on pre-2.0 indices...
-                Translog.upgradeLegacyTranslog(logger, translogConfig);
+                throw new IndexFormatTooOldException("trasnlog", "translog has no generation nor a UUID - this might be an index from a previous version consider upgrading to N-1 first");
             }
         }
         final Translog translog = new Translog(translogConfig);
@@ -316,7 +314,8 @@ public class InternalEngine extends Engine {
                     }
                     if (get.versionType().isVersionConflictForReads(versionValue.version(), get.version())) {
                         Uid uid = Uid.createUid(get.uid().text());
-                        throw new VersionConflictEngineException(shardId, uid.type(), uid.id(), versionValue.version(), get.version());
+                        throw new VersionConflictEngineException(shardId, uid.type(), uid.id(),
+                                get.versionType().explainConflictForReads(versionValue.version(), get.version()));
                     }
                     Translog.Operation op = translog.read(versionValue.translogLocation());
                     if (op != null) {
@@ -331,96 +330,7 @@ public class InternalEngine extends Engine {
     }
 
     @Override
-    public void create(Create create) throws EngineException {
-        try (ReleasableLock lock = readLock.acquire()) {
-            ensureOpen();
-            if (create.origin() == Operation.Origin.RECOVERY) {
-                // Don't throttle recovery operations
-                innerCreate(create);
-            } else {
-                try (Releasable r = throttle.acquireThrottle()) {
-                    innerCreate(create);
-                }
-            }
-        } catch (OutOfMemoryError | IllegalStateException | IOException t) {
-            maybeFailEngine("create", t);
-            throw new CreateFailedEngineException(shardId, create.type(), create.id(), t);
-        }
-        checkVersionMapRefresh();
-    }
-
-    private void innerCreate(Create create) throws IOException {
-        synchronized (dirtyLock(create.uid())) {
-            final long currentVersion;
-            final VersionValue versionValue;
-            versionValue = versionMap.getUnderLock(create.uid().bytes());
-            if (versionValue == null) {
-                currentVersion = loadCurrentVersionFromIndex(create.uid());
-            } else {
-                if (engineConfig.isEnableGcDeletes() && versionValue.delete() && (engineConfig.getThreadPool().estimatedTimeInMillis() - versionValue.time()) > engineConfig.getGcDeletesInMillis()) {
-                    currentVersion = Versions.NOT_FOUND; // deleted, and GC
-                } else {
-                    currentVersion = versionValue.version();
-                }
-            }
-            innerCreateUnderLock(create, currentVersion, versionValue);
-        }
-    }
-
-    private void innerCreateUnderLock(Create create, long currentVersion, VersionValue versionValue) throws IOException {
-
-        // same logic as index
-        long updatedVersion;
-        long expectedVersion = create.version();
-        if (create.versionType().isVersionConflictForWrites(currentVersion, expectedVersion)) {
-            if (create.origin() == Operation.Origin.RECOVERY) {
-                return;
-            } else {
-                throw new VersionConflictEngineException(shardId, create.type(), create.id(), currentVersion, expectedVersion);
-            }
-        }
-        updatedVersion = create.versionType().updateVersion(currentVersion, expectedVersion);
-
-        // if the doc exists
-        boolean doUpdate = false;
-        if ((versionValue != null && versionValue.delete() == false) || (versionValue == null && currentVersion != Versions.NOT_FOUND)) {
-            if (create.origin() == Operation.Origin.RECOVERY) {
-                return;
-            } else if (create.origin() == Operation.Origin.REPLICA) {
-                // #7142: the primary already determined it's OK to index this document, and we confirmed above that the version doesn't
-                // conflict, so we must also update here on the replica to remain consistent:
-                doUpdate = true;
-            } else {
-                // On primary, we throw DAEE if the _uid is already in the index with an older version:
-                assert create.origin() == Operation.Origin.PRIMARY;
-                throw new DocumentAlreadyExistsException(shardId, create.type(), create.id());
-            }
-        }
-
-        create.updateVersion(updatedVersion);
-
-        if (doUpdate) {
-            if (create.docs().size() > 1) {
-                indexWriter.updateDocuments(create.uid(), create.docs());
-            } else {
-                indexWriter.updateDocument(create.uid(), create.docs().get(0));
-            }
-        } else {
-            if (create.docs().size() > 1) {
-                indexWriter.addDocuments(create.docs());
-            } else {
-                indexWriter.addDocument(create.docs().get(0));
-            }
-        }
-        Translog.Location translogLocation = translog.add(new Translog.Create(create));
-
-        versionMap.putUnderLock(create.uid().bytes(), new VersionValue(updatedVersion, translogLocation));
-        create.setTranslogLocation(translogLocation);
-        indexingService.postCreateUnderLock(create);
-    }
-
-    @Override
-    public boolean index(Index index) throws EngineException {
+    public boolean index(Index index) {
         final boolean created;
         try (ReleasableLock lock = readLock.acquire()) {
             ensureOpen();
@@ -440,40 +350,16 @@ public class InternalEngine extends Engine {
         return created;
     }
 
-    /**
-     * Forces a refresh if the versionMap is using too much RAM
-     */
-    private void checkVersionMapRefresh() {
-        if (versionMap.ramBytesUsedForRefresh() > config().getVersionMapSize().bytes() && versionMapRefreshPending.getAndSet(true) == false) {
-            try {
-                if (isClosed.get()) {
-                    // no point...
-                    return;
-                }
-                // Now refresh to clear versionMap:
-                engineConfig.getThreadPool().executor(ThreadPool.Names.REFRESH).execute(new Runnable() {
-                    @Override
-                    public void run() {
-                        try {
-                            refresh("version_table_full");
-                        } catch (EngineClosedException ex) {
-                            // ignore
-                        }
-                    }
-                });
-            } catch (EsRejectedExecutionException ex) {
-                // that is fine too.. we might be shutting down
-            }
-        }
-    }
-
     private boolean innerIndex(Index index) throws IOException {
         synchronized (dirtyLock(index.uid())) {
             final long currentVersion;
+            final boolean deleted;
             VersionValue versionValue = versionMap.getUnderLock(index.uid().bytes());
             if (versionValue == null) {
                 currentVersion = loadCurrentVersionFromIndex(index.uid());
+                deleted = currentVersion == Versions.NOT_FOUND;
             } else {
+                deleted = versionValue.delete();
                 if (engineConfig.isEnableGcDeletes() && versionValue.delete() && (engineConfig.getThreadPool().estimatedTimeInMillis() - versionValue.time()) > engineConfig.getGcDeletesInMillis()) {
                     currentVersion = Versions.NOT_FOUND; // deleted, and GC
                 } else {
@@ -481,19 +367,20 @@ public class InternalEngine extends Engine {
                 }
             }
 
-            long updatedVersion;
             long expectedVersion = index.version();
-            if (index.versionType().isVersionConflictForWrites(currentVersion, expectedVersion)) {
+            if (index.versionType().isVersionConflictForWrites(currentVersion, expectedVersion, deleted)) {
                 if (index.origin() == Operation.Origin.RECOVERY) {
                     return false;
                 } else {
-                    throw new VersionConflictEngineException(shardId, index.type(), index.id(), currentVersion, expectedVersion);
+                    throw new VersionConflictEngineException(shardId, index.type(), index.id(),
+                            index.versionType().explainConflictForWrites(currentVersion, expectedVersion, deleted));
                 }
             }
-            updatedVersion = index.versionType().updateVersion(currentVersion, expectedVersion);
+            long updatedVersion = index.versionType().updateVersion(currentVersion, expectedVersion);
 
             final boolean created;
             index.updateVersion(updatedVersion);
+
             if (currentVersion == Versions.NOT_FOUND) {
                 // document does not exists, we can optimize for create
                 created = true;
@@ -518,11 +405,39 @@ public class InternalEngine extends Engine {
 
             versionMap.putUnderLock(index.uid().bytes(), new VersionValue(updatedVersion, translogLocation));
             index.setTranslogLocation(translogLocation);
+
             indexingService.postIndexUnderLock(index);
             return created;
         }
     }
 
+    /**
+     * Forces a refresh if the versionMap is using too much RAM
+     */
+    private void checkVersionMapRefresh() {
+        if (versionMap.ramBytesUsedForRefresh() > config().getVersionMapSize().bytes() && versionMapRefreshPending.getAndSet(true) == false) {
+            try {
+                if (isClosed.get()) {
+                    // no point...
+                    return;
+                }
+                // Now refresh to clear versionMap:
+                engineConfig.getThreadPool().executor(ThreadPool.Names.REFRESH).execute(new Runnable() {
+                    @Override
+                    public void run() {
+                        try {
+                            refresh("version_table_full");
+                        } catch (EngineClosedException ex) {
+                            // ignore
+                        }
+                    }
+                });
+            } catch (EsRejectedExecutionException ex) {
+                // that is fine too.. we might be shutting down
+            }
+        }
+    }
+
     @Override
     public void delete(Delete delete) throws EngineException {
         try (ReleasableLock lock = readLock.acquire()) {
@@ -549,10 +464,13 @@ public class InternalEngine extends Engine {
     private void innerDelete(Delete delete) throws IOException {
         synchronized (dirtyLock(delete.uid())) {
             final long currentVersion;
+            final boolean deleted;
             VersionValue versionValue = versionMap.getUnderLock(delete.uid().bytes());
             if (versionValue == null) {
                 currentVersion = loadCurrentVersionFromIndex(delete.uid());
+                deleted = currentVersion == Versions.NOT_FOUND;
             } else {
+                deleted = versionValue.delete();
                 if (engineConfig.isEnableGcDeletes() && versionValue.delete() && (engineConfig.getThreadPool().estimatedTimeInMillis() - versionValue.time()) > engineConfig.getGcDeletesInMillis()) {
                     currentVersion = Versions.NOT_FOUND; // deleted, and GC
                 } else {
@@ -562,11 +480,12 @@ public class InternalEngine extends Engine {
 
             long updatedVersion;
             long expectedVersion = delete.version();
-            if (delete.versionType().isVersionConflictForWrites(currentVersion, expectedVersion)) {
+            if (delete.versionType().isVersionConflictForWrites(currentVersion, expectedVersion, deleted)) {
                 if (delete.origin() == Operation.Origin.RECOVERY) {
                     return;
                 } else {
-                    throw new VersionConflictEngineException(shardId, delete.type(), delete.id(), currentVersion, expectedVersion);
+                    throw new VersionConflictEngineException(shardId, delete.type(), delete.id(),
+                            delete.versionType().explainConflictForWrites(currentVersion, expectedVersion, deleted));
                 }
             }
             updatedVersion = delete.versionType().updateVersion(currentVersion, expectedVersion);
@@ -591,48 +510,6 @@ public class InternalEngine extends Engine {
         }
     }
 
-    /** @deprecated This was removed, but we keep this API so translog can replay any DBQs on upgrade. */
-    @Deprecated
-    @Override
-    public void delete(DeleteByQuery delete) throws EngineException {
-        try (ReleasableLock lock = readLock.acquire()) {
-            ensureOpen();
-            if (delete.origin() == Operation.Origin.RECOVERY) {
-                // Don't throttle recovery operations
-                innerDelete(delete);
-            } else {
-                try (Releasable r = throttle.acquireThrottle()) {
-                    innerDelete(delete);
-                }
-            }
-        }
-    }
-
-    private void innerDelete(DeleteByQuery delete) throws EngineException {
-        try {
-            Query query = delete.query();
-            if (delete.aliasFilter() != null) {
-                query = new BooleanQuery.Builder()
-                        .add(query, Occur.MUST)
-                        .add(delete.aliasFilter(), Occur.FILTER)
-                        .build();
-            }
-            if (delete.nested()) {
-                query = new IncludeNestedDocsQuery(query, delete.parentFilter());
-            }
-
-            indexWriter.deleteDocuments(query);
-            translog.add(new Translog.DeleteByQuery(delete));
-        } catch (Throwable t) {
-            maybeFailEngine("delete_by_query", t);
-            throw new DeleteByQueryFailedEngineException(shardId, delete, t);
-        }
-
-        // TODO: This is heavy, since we refresh, but we must do this because we don't know which documents were in fact deleted (i.e., our
-        // versionMap isn't updated), so we must force a cutover to a new reader to "see" the deletions:
-        refresh("delete_by_query");
-    }
-
     @Override
     public void refresh(String source) throws EngineException {
         // we obtain a read lock here, since we don't want a flush to happen while we are refreshing
@@ -905,6 +782,11 @@ public class InternalEngine extends Engine {
     }
 
     @Override
+    public long indexWriterRAMBytesUsed() {
+        return indexWriter.ramBytesUsed();
+    }
+
+    @Override
     public List<Segment> segments(boolean verbose) {
         try (ReleasableLock lock = readLock.acquire()) {
             Segment[] segmentsArr = getSegmentInfo(lastCommittedSegmentInfos, verbose);
@@ -974,7 +856,7 @@ public class InternalEngine extends Engine {
     }
 
     private Object dirtyLock(BytesRef uid) {
-        int hash = DjbHashFunction.DJB_HASH(uid.bytes, uid.offset, uid.length);
+        int hash = Murmur3HashFunction.hash(uid.bytes, uid.offset, uid.length);
         return dirtyLocks[MathUtils.mod(hash, dirtyLocks.length)];
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/engine/Segment.java b/core/src/main/java/org/elasticsearch/index/engine/Segment.java
index cbccaa1..7d3882f 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/Segment.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/Segment.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.engine;
 
-import com.google.common.collect.Iterators;
 import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.Accountables;
 import org.elasticsearch.common.Nullable;
@@ -32,7 +31,6 @@ import org.elasticsearch.common.unit.ByteSizeValue;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Iterator;
 import java.util.List;
 
 public class Segment implements Streamable {
diff --git a/core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java b/core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java
index 7588ffa..921f116 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java
@@ -103,11 +103,6 @@ public class ShadowEngine extends Engine {
 
 
     @Override
-    public void create(Create create) throws EngineException {
-        throw new UnsupportedOperationException(shardId + " create operation not allowed on shadow engine");
-    }
-
-    @Override
     public boolean index(Index index) throws EngineException {
         throw new UnsupportedOperationException(shardId + " index operation not allowed on shadow engine");
     }
@@ -117,13 +112,6 @@ public class ShadowEngine extends Engine {
         throw new UnsupportedOperationException(shardId + " delete operation not allowed on shadow engine");
     }
 
-    /** @deprecated This was removed, but we keep this API so translog can replay any DBQs on upgrade. */
-    @Deprecated
-    @Override
-    public void delete(DeleteByQuery delete) throws EngineException {
-        throw new UnsupportedOperationException(shardId + " delete-by-query operation not allowed on shadow engine");
-    }
-
     @Override
     public SyncedFlushResult syncFlush(String syncId, CommitId expectedCommitId) {
         throw new UnsupportedOperationException(shardId + " sync commit operation not allowed on shadow engine");
@@ -245,4 +233,9 @@ public class ShadowEngine extends Engine {
         return lastCommittedSegmentInfos;
     }
 
+    @Override
+    public long indexWriterRAMBytesUsed() {
+        // No IndexWriter
+        throw new UnsupportedOperationException("ShadowEngine has no IndexWriter");
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/engine/VersionConflictEngineException.java b/core/src/main/java/org/elasticsearch/index/engine/VersionConflictEngineException.java
index 8c2d352..9b038c6 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/VersionConflictEngineException.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/VersionConflictEngineException.java
@@ -29,8 +29,16 @@ import java.io.IOException;
  */
 public class VersionConflictEngineException extends EngineException {
 
-    public VersionConflictEngineException(ShardId shardId, String type, String id, long current, long provided) {
-        super(shardId, "[" + type + "][" + id + "]: version conflict, current [" + current + "], provided [" + provided + "]");
+    public VersionConflictEngineException(ShardId shardId, String type, String id, String explanation) {
+        this(shardId, null, type, id, explanation);
+    }
+
+    public VersionConflictEngineException(ShardId shardId, Throwable cause, String type, String id, String explanation) {
+        this(shardId, "[{}][{}]: version conflict, {}", cause, type, id, explanation);
+    }
+
+    public VersionConflictEngineException(ShardId shardId, String msg, Throwable cause, Object... params) {
+        super(shardId, msg, cause, params);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java b/core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java
index de3adbc..a9713ce 100644
--- a/core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java
+++ b/core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java
@@ -19,8 +19,10 @@
 
 package org.elasticsearch.index.fielddata;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.Accountable;
 import org.elasticsearch.ExceptionsHelper;
+import org.elasticsearch.Version;
 import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.Inject;
@@ -39,7 +41,6 @@ import org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData;
 import org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData;
 import org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.MappedFieldType.Names;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.core.BooleanFieldMapper;
 import org.elasticsearch.index.mapper.internal.IndexFieldMapper;
@@ -55,7 +56,7 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
+import static org.elasticsearch.index.mapper.MappedFieldType.Names;
 
 /**
  */
@@ -69,28 +70,26 @@ public class IndexFieldDataService extends AbstractIndexComponent {
     private static final String ARRAY_FORMAT = "array";
     private static final String PAGED_BYTES_FORMAT = "paged_bytes";
 
-    private final static Map<String, IndexFieldData.Builder> buildersByType;
-    private final static Map<String, IndexFieldData.Builder> docValuesBuildersByType;
-    private final static Map<Tuple<String, String>, IndexFieldData.Builder> buildersByTypeAndFormat;
+    private final static ImmutableMap<String, IndexFieldData.Builder> buildersByType;
+    private final static ImmutableMap<String, IndexFieldData.Builder> docValuesBuildersByType;
+    private final static ImmutableMap<Tuple<String, String>, IndexFieldData.Builder> buildersByTypeAndFormat;
     private final CircuitBreakerService circuitBreakerService;
 
     static {
-        Map<String, IndexFieldData.Builder> buildersByTypeBuilder = new HashMap<>();
-        buildersByTypeBuilder.put("string", new PagedBytesIndexFieldData.Builder());
-        buildersByTypeBuilder.put("float", new FloatArrayIndexFieldData.Builder());
-        buildersByTypeBuilder.put("double", new DoubleArrayIndexFieldData.Builder());
-        buildersByTypeBuilder.put("byte", new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.BYTE));
-        buildersByTypeBuilder.put("short", new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.SHORT));
-        buildersByTypeBuilder.put("int", new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.INT));
-        buildersByTypeBuilder.put("long", new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.LONG));
-        buildersByTypeBuilder.put("geo_point", new GeoPointDoubleArrayIndexFieldData.Builder());
-        buildersByTypeBuilder.put(ParentFieldMapper.NAME, new ParentChildIndexFieldData.Builder());
-        buildersByTypeBuilder.put(IndexFieldMapper.NAME, new IndexIndexFieldData.Builder());
-        buildersByTypeBuilder.put("binary", new DisabledIndexFieldData.Builder());
-        buildersByTypeBuilder.put(BooleanFieldMapper.CONTENT_TYPE,
-                new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.BOOLEAN));
-         buildersByType = unmodifiableMap(buildersByTypeBuilder);
-
+        buildersByType = MapBuilder.<String, IndexFieldData.Builder>newMapBuilder()
+                .put("string", new PagedBytesIndexFieldData.Builder())
+                .put("float", new FloatArrayIndexFieldData.Builder())
+                .put("double", new DoubleArrayIndexFieldData.Builder())
+                .put("byte", new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.BYTE))
+                .put("short", new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.SHORT))
+                .put("int", new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.INT))
+                .put("long", new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.LONG))
+                .put("geo_point", new GeoPointDoubleArrayIndexFieldData.Builder())
+                .put(ParentFieldMapper.NAME, new ParentChildIndexFieldData.Builder())
+                .put(IndexFieldMapper.NAME, new IndexIndexFieldData.Builder())
+                .put("binary", new DisabledIndexFieldData.Builder())
+                .put(BooleanFieldMapper.CONTENT_TYPE, new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.BOOLEAN))
+                .immutableMap();
 
         docValuesBuildersByType = MapBuilder.<String, IndexFieldData.Builder>newMapBuilder()
                 .put("string", new DocValuesIndexFieldData.Builder())
diff --git a/core/src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java b/core/src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java
index 899da8f..fa4b587 100644
--- a/core/src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java
+++ b/core/src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java
@@ -18,6 +18,8 @@
  */
 package org.elasticsearch.index.fieldvisitor;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.StoredFieldVisitor;
 import org.apache.lucene.index.StoredFieldVisitor;
@@ -45,7 +47,6 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
-import static java.util.Collections.emptyMap;
 import static java.util.Collections.unmodifiableSet;
 import static org.elasticsearch.common.util.set.Sets.newHashSet;
 
@@ -190,7 +191,9 @@ public class FieldsVisitor extends StoredFieldVisitor {
     }
 
     public Map<String, List<Object>> fields() {
-        return fieldsValues != null ? fieldsValues : emptyMap();
+        return fieldsValues != null
+                ? fieldsValues
+                : ImmutableMap.<String, List<Object>>of();
     }
 
     public void reset() {
diff --git a/core/src/main/java/org/elasticsearch/index/get/GetResult.java b/core/src/main/java/org/elasticsearch/index/get/GetResult.java
index d243694..c788fcf 100644
--- a/core/src/main/java/org/elasticsearch/index/get/GetResult.java
+++ b/core/src/main/java/org/elasticsearch/index/get/GetResult.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.get;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.compress.CompressorFactory;
@@ -39,7 +40,6 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.emptyMap;
 import static org.elasticsearch.index.get.GetField.readGetField;
 
 /**
@@ -68,7 +68,7 @@ public class GetResult implements Streamable, Iterable<GetField>, ToXContent {
         this.source = source;
         this.fields = fields;
         if (this.fields == null) {
-            this.fields = emptyMap();
+            this.fields = ImmutableMap.of();
         }
     }
 
@@ -286,7 +286,7 @@ public class GetResult implements Streamable, Iterable<GetField>, ToXContent {
             }
             int size = in.readVInt();
             if (size == 0) {
-                fields = emptyMap();
+                fields = ImmutableMap.of();
             } else {
                 fields = new HashMap<>(size);
                 for (int i = 0; i < size; i++) {
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java b/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
index 858453f..651bc40 100644
--- a/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
+++ b/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
@@ -28,39 +28,8 @@ public abstract class IndexingOperationListener {
     /**
      * Called before the indexing occurs.
      */
-    public Engine.Create preCreate(Engine.Create create) {
-        return create;
-    }
-
-    /**
-     * Called after the indexing occurs, under a locking scheme to maintain
-     * concurrent updates to the same doc.
-     * <p>
-     * Note, long operations should not occur under this callback.
-     */
-    public void postCreateUnderLock(Engine.Create create) {
-
-    }
-
-    /**
-     * Called after create index operation occurred.
-     */
-    public void postCreate(Engine.Create create) {
-
-    }
-
-    /**
-     * Called after create index operation occurred with exception.
-     */
-    public void postCreate(Engine.Create create, Throwable ex) {
-
-    }
-
-    /**
-     * Called before the indexing occurs.
-     */
-    public Engine.Index preIndex(Engine.Index index) {
-        return index;
+    public Engine.Index preIndex(Engine.Index operation) {
+        return operation;
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java b/core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java
index ea45db2..292c2a1 100644
--- a/core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java
+++ b/core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java
@@ -128,10 +128,6 @@ public final class IndexingSlowLog {
         postIndexing(index.parsedDoc(), tookInNanos);
     }
 
-    void postCreate(Engine.Create create, long tookInNanos) {
-        postIndexing(create.parsedDoc(), tookInNanos);
-    }
-
     /**
      * Reads how much of the source to log. The user can specify any value they
      * like and numbers are interpreted the maximum number of characters to log
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java b/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
index b61ba7f..d1abbf1 100644
--- a/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
+++ b/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.indexing;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.metrics.CounterMetric;
 import org.elasticsearch.common.metrics.MeanMetric;
@@ -34,8 +35,6 @@ import java.util.Map;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.TimeUnit;
 
-import static java.util.Collections.emptyMap;
-
 /**
  */
 public class ShardIndexingService extends AbstractIndexShardComponent {
@@ -46,7 +45,7 @@ public class ShardIndexingService extends AbstractIndexShardComponent {
 
     private final CopyOnWriteArrayList<IndexingOperationListener> listeners = new CopyOnWriteArrayList<>();
 
-    private volatile Map<String, StatsHolder> typesStats = emptyMap();
+    private volatile Map<String, StatsHolder> typesStats = ImmutableMap.of();
 
     public ShardIndexingService(ShardId shardId, Settings indexSettings) {
         super(shardId, indexSettings);
@@ -86,25 +85,6 @@ public class ShardIndexingService extends AbstractIndexShardComponent {
         listeners.remove(listener);
     }
 
-    public Engine.Create preCreate(Engine.Create create) {
-        totalStats.indexCurrent.inc();
-        typeStats(create.type()).indexCurrent.inc();
-        for (IndexingOperationListener listener : listeners) {
-            create = listener.preCreate(create);
-        }
-        return create;
-    }
-
-    public void postCreateUnderLock(Engine.Create create) {
-        for (IndexingOperationListener listener : listeners) {
-            try {
-                listener.postCreateUnderLock(create);
-            } catch (Exception e) {
-                logger.warn("postCreateUnderLock listener [{}] failed", e, listener);
-            }
-        }
-    }
-
     public void throttlingActivated() {
         totalStats.setThrottled(true);
     }
@@ -113,40 +93,13 @@ public class ShardIndexingService extends AbstractIndexShardComponent {
         totalStats.setThrottled(false);
     }
 
-    public void postCreate(Engine.Create create) {
-        long took = create.endTime() - create.startTime();
-        totalStats.indexMetric.inc(took);
-        totalStats.indexCurrent.dec();
-        StatsHolder typeStats = typeStats(create.type());
-        typeStats.indexMetric.inc(took);
-        typeStats.indexCurrent.dec();
-        slowLog.postCreate(create, took);
-        for (IndexingOperationListener listener : listeners) {
-            try {
-                listener.postCreate(create);
-            } catch (Exception e) {
-                logger.warn("postCreate listener [{}] failed", e, listener);
-            }
-        }
-    }
-
-    public void postCreate(Engine.Create create, Throwable ex) {
-        for (IndexingOperationListener listener : listeners) {
-            try {
-                listener.postCreate(create, ex);
-            } catch (Throwable t) {
-                logger.warn("postCreate listener [{}] failed", t, listener);
-            }
-        }
-    }
-
-    public Engine.Index preIndex(Engine.Index index) {
+    public Engine.Index preIndex(Engine.Index operation) {
         totalStats.indexCurrent.inc();
-        typeStats(index.type()).indexCurrent.inc();
+        typeStats(operation.type()).indexCurrent.inc();
         for (IndexingOperationListener listener : listeners) {
-            index = listener.preIndex(index);
+            operation = listener.preIndex(operation);
         }
-        return index;
+        return operation;
     }
 
     public void postIndexUnderLock(Engine.Index index) {
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java
index 0314f8f..54b2c98 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.mapper;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -69,8 +70,6 @@ import java.util.Map;
 import java.util.Objects;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 
-import static java.util.Collections.emptyMap;
-
 /**
  *
  */
@@ -86,7 +85,7 @@ public class DocumentMapper implements ToXContent {
 
         private final RootObjectMapper rootObjectMapper;
 
-        private Map<String, Object> meta = emptyMap();
+        private ImmutableMap<String, Object> meta = ImmutableMap.of();
 
         private final Mapper.BuilderContext builderContext;
 
@@ -116,7 +115,7 @@ public class DocumentMapper implements ToXContent {
             this.rootMappers.put(FieldNamesFieldMapper.class, new FieldNamesFieldMapper(indexSettings, mapperService.fullName(FieldNamesFieldMapper.NAME)));
         }
 
-        public Builder meta(Map<String, Object> meta) {
+        public Builder meta(ImmutableMap<String, Object> meta) {
             this.meta = meta;
             return this;
         }
@@ -170,7 +169,7 @@ public class DocumentMapper implements ToXContent {
 
     public DocumentMapper(MapperService mapperService, @Nullable Settings indexSettings, DocumentMapperParser docMapperParser,
                           RootObjectMapper rootObjectMapper,
-                          Map<String, Object> meta,
+                          ImmutableMap<String, Object> meta,
                           Map<Class<? extends MetadataFieldMapper>, MetadataFieldMapper> rootMappers,
                           List<SourceTransform> sourceTransforms,
                           ReentrantReadWriteLock mappingLock) {
@@ -235,7 +234,7 @@ public class DocumentMapper implements ToXContent {
         return this.typeText;
     }
 
-    public Map<String, Object> meta() {
+    public ImmutableMap<String, Object> meta() {
         return mapping.meta;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
index 65a6d3a..82ff5fb 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
@@ -19,10 +19,12 @@
 
 package org.elasticsearch.index.mapper;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.compress.CompressedXContent;
 import org.elasticsearch.common.geo.ShapesAvailability;
@@ -33,51 +35,20 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.analysis.AnalysisService;
-import org.elasticsearch.index.mapper.core.BinaryFieldMapper;
-import org.elasticsearch.index.mapper.core.BooleanFieldMapper;
-import org.elasticsearch.index.mapper.core.ByteFieldMapper;
-import org.elasticsearch.index.mapper.core.CompletionFieldMapper;
-import org.elasticsearch.index.mapper.core.DateFieldMapper;
-import org.elasticsearch.index.mapper.core.DoubleFieldMapper;
-import org.elasticsearch.index.mapper.core.FloatFieldMapper;
-import org.elasticsearch.index.mapper.core.IntegerFieldMapper;
-import org.elasticsearch.index.mapper.core.LongFieldMapper;
-import org.elasticsearch.index.mapper.core.ShortFieldMapper;
-import org.elasticsearch.index.mapper.core.StringFieldMapper;
-import org.elasticsearch.index.mapper.core.TokenCountFieldMapper;
-import org.elasticsearch.index.mapper.core.TypeParsers;
+import org.elasticsearch.index.mapper.core.*;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 import org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper;
-import org.elasticsearch.index.mapper.internal.AllFieldMapper;
-import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
-import org.elasticsearch.index.mapper.internal.IdFieldMapper;
-import org.elasticsearch.index.mapper.internal.IndexFieldMapper;
-import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
-import org.elasticsearch.index.mapper.internal.RoutingFieldMapper;
-import org.elasticsearch.index.mapper.internal.SourceFieldMapper;
-import org.elasticsearch.index.mapper.internal.TTLFieldMapper;
-import org.elasticsearch.index.mapper.internal.TimestampFieldMapper;
-import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
-import org.elasticsearch.index.mapper.internal.UidFieldMapper;
-import org.elasticsearch.index.mapper.internal.VersionFieldMapper;
+import org.elasticsearch.index.mapper.internal.*;
 import org.elasticsearch.index.mapper.ip.IpFieldMapper;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.mapper.object.RootObjectMapper;
 import org.elasticsearch.index.settings.IndexSettings;
-import org.elasticsearch.index.similarity.SimilarityLookupService;
+import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService;
 
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.SortedMap;
-import java.util.TreeMap;
+import java.util.*;
 
-import static java.util.Collections.unmodifiableMap;
-import static java.util.Collections.unmodifiableSortedMap;
 import static org.elasticsearch.index.mapper.MapperBuilders.doc;
 
 public class DocumentMapperParser {
@@ -86,7 +57,7 @@ public class DocumentMapperParser {
     final MapperService mapperService;
     final AnalysisService analysisService;
     private static final ESLogger logger = Loggers.getLogger(DocumentMapperParser.class);
-    private final SimilarityLookupService similarityLookupService;
+    private final SimilarityService similarityService;
     private final ScriptService scriptService;
 
     private final RootObjectMapper.TypeParser rootObjectTypeParser = new RootObjectMapper.TypeParser();
@@ -95,82 +66,83 @@ public class DocumentMapperParser {
     private final Version indexVersionCreated;
     private final ParseFieldMatcher parseFieldMatcher;
 
-    private volatile Map<String, Mapper.TypeParser> typeParsers;
-    private volatile Map<String, Mapper.TypeParser> rootTypeParsers;
+    private volatile ImmutableMap<String, Mapper.TypeParser> typeParsers;
+    private volatile ImmutableMap<String, Mapper.TypeParser> rootTypeParsers;
     private volatile SortedMap<String, Mapper.TypeParser> additionalRootMappers;
 
     public DocumentMapperParser(@IndexSettings Settings indexSettings, MapperService mapperService, AnalysisService analysisService,
-                                SimilarityLookupService similarityLookupService, ScriptService scriptService) {
+                                SimilarityService similarityService, ScriptService scriptService) {
         this.indexSettings = indexSettings;
         this.parseFieldMatcher = new ParseFieldMatcher(indexSettings);
         this.mapperService = mapperService;
         this.analysisService = analysisService;
-        this.similarityLookupService = similarityLookupService;
+        this.similarityService = similarityService;
         this.scriptService = scriptService;
-        Map<String, Mapper.TypeParser> typeParsers = new HashMap<>();
-        typeParsers.put(ByteFieldMapper.CONTENT_TYPE, new ByteFieldMapper.TypeParser());
-        typeParsers.put(ShortFieldMapper.CONTENT_TYPE, new ShortFieldMapper.TypeParser());
-        typeParsers.put(IntegerFieldMapper.CONTENT_TYPE, new IntegerFieldMapper.TypeParser());
-        typeParsers.put(LongFieldMapper.CONTENT_TYPE, new LongFieldMapper.TypeParser());
-        typeParsers.put(FloatFieldMapper.CONTENT_TYPE, new FloatFieldMapper.TypeParser());
-        typeParsers.put(DoubleFieldMapper.CONTENT_TYPE, new DoubleFieldMapper.TypeParser());
-        typeParsers.put(BooleanFieldMapper.CONTENT_TYPE, new BooleanFieldMapper.TypeParser());
-        typeParsers.put(BinaryFieldMapper.CONTENT_TYPE, new BinaryFieldMapper.TypeParser());
-        typeParsers.put(DateFieldMapper.CONTENT_TYPE, new DateFieldMapper.TypeParser());
-        typeParsers.put(IpFieldMapper.CONTENT_TYPE, new IpFieldMapper.TypeParser());
-        typeParsers.put(StringFieldMapper.CONTENT_TYPE, new StringFieldMapper.TypeParser());
-        typeParsers.put(TokenCountFieldMapper.CONTENT_TYPE, new TokenCountFieldMapper.TypeParser());
-        typeParsers.put(ObjectMapper.CONTENT_TYPE, new ObjectMapper.TypeParser());
-        typeParsers.put(ObjectMapper.NESTED_CONTENT_TYPE, new ObjectMapper.TypeParser());
-        typeParsers.put(TypeParsers.MULTI_FIELD_CONTENT_TYPE, TypeParsers.multiFieldConverterTypeParser);
-        typeParsers.put(CompletionFieldMapper.CONTENT_TYPE, new CompletionFieldMapper.TypeParser());
-        typeParsers.put(GeoPointFieldMapper.CONTENT_TYPE, new GeoPointFieldMapper.TypeParser());
+        MapBuilder<String, Mapper.TypeParser> typeParsersBuilder = new MapBuilder<String, Mapper.TypeParser>()
+                .put(ByteFieldMapper.CONTENT_TYPE, new ByteFieldMapper.TypeParser())
+                .put(ShortFieldMapper.CONTENT_TYPE, new ShortFieldMapper.TypeParser())
+                .put(IntegerFieldMapper.CONTENT_TYPE, new IntegerFieldMapper.TypeParser())
+                .put(LongFieldMapper.CONTENT_TYPE, new LongFieldMapper.TypeParser())
+                .put(FloatFieldMapper.CONTENT_TYPE, new FloatFieldMapper.TypeParser())
+                .put(DoubleFieldMapper.CONTENT_TYPE, new DoubleFieldMapper.TypeParser())
+                .put(BooleanFieldMapper.CONTENT_TYPE, new BooleanFieldMapper.TypeParser())
+                .put(BinaryFieldMapper.CONTENT_TYPE, new BinaryFieldMapper.TypeParser())
+                .put(DateFieldMapper.CONTENT_TYPE, new DateFieldMapper.TypeParser())
+                .put(IpFieldMapper.CONTENT_TYPE, new IpFieldMapper.TypeParser())
+                .put(StringFieldMapper.CONTENT_TYPE, new StringFieldMapper.TypeParser())
+                .put(TokenCountFieldMapper.CONTENT_TYPE, new TokenCountFieldMapper.TypeParser())
+                .put(ObjectMapper.CONTENT_TYPE, new ObjectMapper.TypeParser())
+                .put(ObjectMapper.NESTED_CONTENT_TYPE, new ObjectMapper.TypeParser())
+                .put(TypeParsers.MULTI_FIELD_CONTENT_TYPE, TypeParsers.multiFieldConverterTypeParser)
+                .put(CompletionFieldMapper.CONTENT_TYPE, new CompletionFieldMapper.TypeParser())
+                .put(GeoPointFieldMapper.CONTENT_TYPE, new GeoPointFieldMapper.TypeParser());
 
         if (ShapesAvailability.JTS_AVAILABLE) {
-            typeParsers.put(GeoShapeFieldMapper.CONTENT_TYPE, new GeoShapeFieldMapper.TypeParser());
+            typeParsersBuilder.put(GeoShapeFieldMapper.CONTENT_TYPE, new GeoShapeFieldMapper.TypeParser());
         }
 
-        this.typeParsers = unmodifiableMap(typeParsers);
-
-        Map<String, Mapper.TypeParser> rootTypeParsers = new HashMap<>();
-        rootTypeParsers.put(IndexFieldMapper.NAME, new IndexFieldMapper.TypeParser());
-        rootTypeParsers.put(SourceFieldMapper.NAME, new SourceFieldMapper.TypeParser());
-        rootTypeParsers.put(TypeFieldMapper.NAME, new TypeFieldMapper.TypeParser());
-        rootTypeParsers.put(AllFieldMapper.NAME, new AllFieldMapper.TypeParser());
-        rootTypeParsers.put(ParentFieldMapper.NAME, new ParentFieldMapper.TypeParser());
-        rootTypeParsers.put(RoutingFieldMapper.NAME, new RoutingFieldMapper.TypeParser());
-        rootTypeParsers.put(TimestampFieldMapper.NAME, new TimestampFieldMapper.TypeParser());
-        rootTypeParsers.put(TTLFieldMapper.NAME, new TTLFieldMapper.TypeParser());
-        rootTypeParsers.put(UidFieldMapper.NAME, new UidFieldMapper.TypeParser());
-        rootTypeParsers.put(VersionFieldMapper.NAME, new VersionFieldMapper.TypeParser());
-        rootTypeParsers.put(IdFieldMapper.NAME, new IdFieldMapper.TypeParser());
-        rootTypeParsers.put(FieldNamesFieldMapper.NAME, new FieldNamesFieldMapper.TypeParser());
-        this.rootTypeParsers = unmodifiableMap(rootTypeParsers);
+        typeParsers = typeParsersBuilder.immutableMap();
+
+        rootTypeParsers = new MapBuilder<String, Mapper.TypeParser>()
+                .put(IndexFieldMapper.NAME, new IndexFieldMapper.TypeParser())
+                .put(SourceFieldMapper.NAME, new SourceFieldMapper.TypeParser())
+                .put(TypeFieldMapper.NAME, new TypeFieldMapper.TypeParser())
+                .put(AllFieldMapper.NAME, new AllFieldMapper.TypeParser())
+                .put(ParentFieldMapper.NAME, new ParentFieldMapper.TypeParser())
+                .put(RoutingFieldMapper.NAME, new RoutingFieldMapper.TypeParser())
+                .put(TimestampFieldMapper.NAME, new TimestampFieldMapper.TypeParser())
+                .put(TTLFieldMapper.NAME, new TTLFieldMapper.TypeParser())
+                .put(UidFieldMapper.NAME, new UidFieldMapper.TypeParser())
+                .put(VersionFieldMapper.NAME, new VersionFieldMapper.TypeParser())
+                .put(IdFieldMapper.NAME, new IdFieldMapper.TypeParser())
+                .put(FieldNamesFieldMapper.NAME, new FieldNamesFieldMapper.TypeParser())
+                .immutableMap();
         additionalRootMappers = Collections.emptySortedMap();
         indexVersionCreated = Version.indexCreated(indexSettings);
     }
 
     public void putTypeParser(String type, Mapper.TypeParser typeParser) {
         synchronized (typeParsersMutex) {
-            Map<String, Mapper.TypeParser> typeParsers = new HashMap<>(this.typeParsers);
-            typeParsers.put(type, typeParser);
-            this.typeParsers = unmodifiableMap(typeParsers);
+            typeParsers = new MapBuilder<>(typeParsers)
+                    .put(type, typeParser)
+                    .immutableMap();
         }
     }
 
     public void putRootTypeParser(String type, Mapper.TypeParser typeParser) {
         synchronized (typeParsersMutex) {
-            Map<String, Mapper.TypeParser> rootTypeParsers = new HashMap<>(this.rootTypeParsers);
-            rootTypeParsers.put(type, typeParser);
-            this.rootTypeParsers = rootTypeParsers;
-            SortedMap<String, Mapper.TypeParser> additionalRootMappers = new TreeMap<>(this.additionalRootMappers);
-            additionalRootMappers.put(type, typeParser);
-            this.additionalRootMappers = unmodifiableSortedMap(additionalRootMappers);
+            rootTypeParsers = new MapBuilder<>(rootTypeParsers)
+                    .put(type, typeParser)
+                    .immutableMap();
+            SortedMap<String, Mapper.TypeParser> newAdditionalRootMappers = new TreeMap<>();
+            newAdditionalRootMappers.putAll(additionalRootMappers);
+            newAdditionalRootMappers.put(type, typeParser);
+            additionalRootMappers = Collections.unmodifiableSortedMap(newAdditionalRootMappers);
         }
     }
 
     public Mapper.TypeParser.ParserContext parserContext(String type) {
-        return new Mapper.TypeParser.ParserContext(type, analysisService, similarityLookupService, mapperService, typeParsers, indexVersionCreated, parseFieldMatcher);
+        return new Mapper.TypeParser.ParserContext(type, analysisService, similarityService::getSimilarity, mapperService, typeParsers::get, indexVersionCreated, parseFieldMatcher);
     }
 
     public DocumentMapper parse(String source) throws MapperParsingException {
@@ -268,12 +240,11 @@ public class DocumentMapperParser {
             }
         }
 
-        Map<String, Object> meta = (Map<String, Object>) mapping.remove("_meta");
-        if (meta != null) {
-            // It may not be required to copy meta here to maintain immutability
-            // but the cost is pretty low here.
-            docBuilder.meta(unmodifiableMap(new HashMap<>(meta)));
+        ImmutableMap<String, Object> attributes = ImmutableMap.of();
+        if (mapping.containsKey("_meta")) {
+            attributes = ImmutableMap.copyOf((Map<String, Object>) mapping.remove("_meta"));
         }
+        docBuilder.meta(attributes);
 
         checkNoRemainingFields(mapping, parserContext.indexVersionCreated(), "Root mapping definition has unsupported parameters: ");
 
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java b/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java
index db2919e..97435e0 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java
@@ -122,7 +122,7 @@ class DocumentParser implements Closeable {
                 // entire type is disabled
                 parser.skipChildren();
             } else if (emptyDoc == false) {
-                Mapper update = parseObject(context, mapping.root);
+                Mapper update = parseObject(context, mapping.root, true);
                 if (update != null) {
                     context.addDynamicMappingsUpdate(update);
                 }
@@ -194,7 +194,7 @@ class DocumentParser implements Closeable {
         return doc;
     }
 
-    static ObjectMapper parseObject(ParseContext context, ObjectMapper mapper) throws IOException {
+    static ObjectMapper parseObject(ParseContext context, ObjectMapper mapper, boolean atRoot) throws IOException {
         if (mapper.isEnabled() == false) {
             context.parser().skipChildren();
             return null;
@@ -202,6 +202,10 @@ class DocumentParser implements Closeable {
         XContentParser parser = context.parser();
 
         String currentFieldName = parser.currentName();
+        if (atRoot && MapperService.isMetadataField(currentFieldName) &&
+            Version.indexCreated(context.indexSettings()).onOrAfter(Version.V_2_0_0_beta1)) {
+            throw new MapperParsingException("Field [" + currentFieldName + "] is a metadata field and cannot be added inside a document. Use the index API request parameters.");
+        }
         XContentParser.Token token = parser.currentToken();
         if (token == XContentParser.Token.VALUE_NULL) {
             // the object is null ("obj1" : null), simply bail
@@ -302,7 +306,7 @@ class DocumentParser implements Closeable {
 
     private static Mapper parseObjectOrField(ParseContext context, Mapper mapper) throws IOException {
         if (mapper instanceof ObjectMapper) {
-            return parseObject(context, (ObjectMapper) mapper);
+            return parseObject(context, (ObjectMapper) mapper, false);
         } else {
             FieldMapper fieldMapper = (FieldMapper)mapper;
             Mapper update = fieldMapper.parse(context);
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java
index ec53fba..45bef68 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java
@@ -34,8 +34,8 @@ import org.elasticsearch.index.analysis.NamedAnalyzer;
 import org.elasticsearch.index.fielddata.FieldDataType;
 import org.elasticsearch.index.mapper.core.TypeParsers;
 import org.elasticsearch.index.mapper.internal.AllFieldMapper;
-import org.elasticsearch.index.similarity.SimilarityLookupService;
 import org.elasticsearch.index.similarity.SimilarityProvider;
+import org.elasticsearch.index.similarity.SimilarityService;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -447,7 +447,7 @@ public abstract class FieldMapper extends Mapper {
         if (fieldType().similarity() != null) {
             builder.field("similarity", fieldType().similarity().name());
         } else if (includeDefaults) {
-            builder.field("similarity", SimilarityLookupService.DEFAULT_SIMILARITY);
+            builder.field("similarity", SimilarityService.DEFAULT_SIMILARITY);
         }
 
         if (includeDefaults || hasCustomFieldDataSettings()) {
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/Mapper.java b/core/src/main/java/org/elasticsearch/index/mapper/Mapper.java
index f55ca93c..9ca34e1 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/Mapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/Mapper.java
@@ -26,9 +26,10 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.index.analysis.AnalysisService;
-import org.elasticsearch.index.similarity.SimilarityLookupService;
+import org.elasticsearch.index.similarity.SimilarityProvider;
 
 import java.util.Map;
+import java.util.function.Function;
 
 public abstract class Mapper implements ToXContent, Iterable<Mapper> {
 
@@ -84,18 +85,18 @@ public abstract class Mapper implements ToXContent, Iterable<Mapper> {
 
             private final AnalysisService analysisService;
 
-            private final SimilarityLookupService similarityLookupService;
+            private final Function<String, SimilarityProvider> similarityLookupService;
 
             private final MapperService mapperService;
 
-            private final Map<String, TypeParser> typeParsers;
+            private final Function<String, TypeParser> typeParsers;
 
             private final Version indexVersionCreated;
 
             private final ParseFieldMatcher parseFieldMatcher;
 
-            public ParserContext(String type, AnalysisService analysisService, SimilarityLookupService similarityLookupService,
-                                 MapperService mapperService, Map<String, TypeParser> typeParsers,
+            public ParserContext(String type, AnalysisService analysisService,  Function<String, SimilarityProvider> similarityLookupService,
+                                 MapperService mapperService, Function<String, TypeParser> typeParsers,
                                  Version indexVersionCreated, ParseFieldMatcher parseFieldMatcher) {
                 this.type = type;
                 this.analysisService = analysisService;
@@ -114,8 +115,8 @@ public abstract class Mapper implements ToXContent, Iterable<Mapper> {
                 return analysisService;
             }
 
-            public SimilarityLookupService similarityLookupService() {
-                return similarityLookupService;
+            public SimilarityProvider getSimilarity(String name) {
+                return similarityLookupService.apply(name);
             }
 
             public MapperService mapperService() {
@@ -123,7 +124,7 @@ public abstract class Mapper implements ToXContent, Iterable<Mapper> {
             }
 
             public TypeParser typeParser(String type) {
-                return typeParsers.get(Strings.toUnderscoreCase(type));
+                return typeParsers.apply(Strings.toUnderscoreCase(type));
             }
 
             public Version indexVersionCreated() {
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java b/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java
index 0357ef2..256a673 100755
--- a/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java
@@ -20,7 +20,7 @@
 package org.elasticsearch.index.mapper;
 
 import com.carrotsearch.hppc.ObjectHashSet;
-import com.google.common.collect.Iterators;
+import com.google.common.collect.ImmutableMap;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.DelegatingAnalyzerWrapper;
@@ -38,6 +38,7 @@ import org.elasticsearch.ElasticsearchGenerationException;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
+import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.common.compress.CompressedXContent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.Queries;
@@ -51,7 +52,7 @@ import org.elasticsearch.index.mapper.Mapper.BuilderContext;
 import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.settings.IndexSettings;
-import org.elasticsearch.index.similarity.SimilarityLookupService;
+import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.indices.InvalidTypeNameException;
 import org.elasticsearch.indices.TypeMissingException;
 import org.elasticsearch.percolator.PercolatorService;
@@ -63,7 +64,6 @@ import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
-import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
@@ -72,10 +72,9 @@ import java.util.Set;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 import java.util.function.Function;
+import java.util.stream.Collectors;
 
-import static java.util.Collections.emptyMap;
 import static java.util.Collections.emptySet;
-import static java.util.Collections.unmodifiableMap;
 import static java.util.Collections.unmodifiableSet;
 import static org.elasticsearch.common.collect.MapBuilder.newMapBuilder;
 
@@ -100,7 +99,7 @@ public class MapperService extends AbstractIndexComponent implements Closeable {
     private volatile String defaultMappingSource;
     private volatile String defaultPercolatorMappingSource;
 
-    private volatile Map<String, DocumentMapper> mappers = emptyMap();
+    private volatile Map<String, DocumentMapper> mappers = ImmutableMap.of();
 
     // A lock for mappings: modifications (put mapping) need to be performed
     // under the write lock and read operations (document parsing) need to be
@@ -120,18 +119,18 @@ public class MapperService extends AbstractIndexComponent implements Closeable {
 
     private final List<DocumentTypeListener> typeListeners = new CopyOnWriteArrayList<>();
 
-    private volatile Map<String, MappedFieldType> unmappedFieldTypes = emptyMap();
+    private volatile ImmutableMap<String, MappedFieldType> unmappedFieldTypes = ImmutableMap.of();
 
     private volatile Set<String> parentTypes = emptySet();
 
     @Inject
     public MapperService(Index index, @IndexSettings Settings indexSettings, AnalysisService analysisService,
-                         SimilarityLookupService similarityLookupService,
+                         SimilarityService similarityService,
                          ScriptService scriptService) {
         super(index, indexSettings);
         this.analysisService = analysisService;
         this.fieldTypes = new FieldTypeLookup();
-        this.documentParser = new DocumentMapperParser(indexSettings, this, analysisService, similarityLookupService, scriptService);
+        this.documentParser = new DocumentMapperParser(indexSettings, this, analysisService, similarityService, scriptService);
         this.indexAnalyzer = new MapperAnalyzerWrapper(analysisService.defaultIndexAnalyzer(), p -> p.indexAnalyzer());
         this.searchAnalyzer = new MapperAnalyzerWrapper(analysisService.defaultSearchAnalyzer(), p -> p.searchAnalyzer());
         this.searchQuoteAnalyzer = new MapperAnalyzerWrapper(analysisService.defaultSearchQuoteAnalyzer(), p -> p.searchQuoteAnalyzer());
@@ -186,13 +185,13 @@ public class MapperService extends AbstractIndexComponent implements Closeable {
      */
     public Iterable<DocumentMapper> docMappers(final boolean includingDefaultMapping) {
         return () -> {
-            final Iterator<DocumentMapper> iterator;
+            final Collection<DocumentMapper> documentMappers;
             if (includingDefaultMapping) {
-                iterator = mappers.values().iterator();
+                documentMappers = mappers.values();
             } else {
-                iterator = mappers.values().stream().filter(mapper -> !DEFAULT_MAPPING.equals(mapper.type())).iterator();
+                documentMappers = mappers.values().stream().filter(mapper -> !DEFAULT_MAPPING.equals(mapper.type())).collect(Collectors.toList());
             }
-            return Iterators.unmodifiableIterator(iterator);
+            return Collections.unmodifiableCollection(documentMappers).iterator();
         };
     }
 
@@ -540,23 +539,24 @@ public class MapperService extends AbstractIndexComponent implements Closeable {
      * Given a type (eg. long, string, ...), return an anonymous field mapper that can be used for search operations.
      */
     public MappedFieldType unmappedFieldType(String type) {
-        MappedFieldType fieldType = unmappedFieldTypes.get(type);
+        final ImmutableMap<String, MappedFieldType> unmappedFieldMappers = this.unmappedFieldTypes;
+        MappedFieldType fieldType = unmappedFieldMappers.get(type);
         if (fieldType == null) {
             final Mapper.TypeParser.ParserContext parserContext = documentMapperParser().parserContext(type);
             Mapper.TypeParser typeParser = parserContext.typeParser(type);
             if (typeParser == null) {
                 throw new IllegalArgumentException("No mapper found for type [" + type + "]");
             }
-            final Mapper.Builder<?, ?> builder = typeParser.parse("__anonymous_" + type, emptyMap(), parserContext);
+            final Mapper.Builder<?, ?> builder = typeParser.parse("__anonymous_" + type, ImmutableMap.<String, Object>of(), parserContext);
             final BuilderContext builderContext = new BuilderContext(indexSettings, new ContentPath(1));
             fieldType = ((FieldMapper)builder.build(builderContext)).fieldType();
 
             // There is no need to synchronize writes here. In the case of concurrent access, we could just
             // compute some mappers several times, which is not a big deal
-            Map<String, MappedFieldType> newUnmappedFieldTypes = new HashMap<>();
-            newUnmappedFieldTypes.putAll(unmappedFieldTypes);
-            newUnmappedFieldTypes.put(type, fieldType);
-            unmappedFieldTypes = unmodifiableMap(newUnmappedFieldTypes);
+            this.unmappedFieldTypes = ImmutableMap.<String, MappedFieldType>builder()
+                    .putAll(unmappedFieldMappers)
+                    .put(type, fieldType)
+                    .build();
         }
         return fieldType;
     }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/Mapping.java b/core/src/main/java/org/elasticsearch/index/mapper/Mapping.java
index 6eeb520..c3b22c6 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/Mapping.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/Mapping.java
@@ -19,7 +19,10 @@
 
 package org.elasticsearch.index.mapper;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.Version;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
@@ -28,13 +31,9 @@ import org.elasticsearch.index.mapper.object.RootObjectMapper;
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.Comparator;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * Wrapper around everything that defines a mapping, without references to
  * utility classes like MapperService, ...
@@ -58,20 +57,20 @@ public final class Mapping implements ToXContent {
     final Version indexCreated;
     final RootObjectMapper root;
     final MetadataFieldMapper[] metadataMappers;
-    final Map<Class<? extends MetadataFieldMapper>, MetadataFieldMapper> rootMappersMap;
+    final ImmutableMap<Class<? extends MetadataFieldMapper>, MetadataFieldMapper> rootMappersMap;
     final SourceTransform[] sourceTransforms;
-    volatile Map<String, Object> meta;
+    volatile ImmutableMap<String, Object> meta;
 
-    public Mapping(Version indexCreated, RootObjectMapper rootObjectMapper, MetadataFieldMapper[] metadataMappers, SourceTransform[] sourceTransforms, Map<String, Object> meta) {
+    public Mapping(Version indexCreated, RootObjectMapper rootObjectMapper, MetadataFieldMapper[] metadataMappers, SourceTransform[] sourceTransforms, ImmutableMap<String, Object> meta) {
         this.indexCreated = indexCreated;
         this.root = rootObjectMapper;
         this.metadataMappers = metadataMappers;
-        Map<Class<? extends MetadataFieldMapper>, MetadataFieldMapper> rootMappersMap = new HashMap<>();
+        ImmutableMap.Builder<Class<? extends MetadataFieldMapper>, MetadataFieldMapper> builder = ImmutableMap.builder();
         for (MetadataFieldMapper metadataMapper : metadataMappers) {
             if (indexCreated.before(Version.V_2_0_0_beta1) && LEGACY_INCLUDE_IN_OBJECT.contains(metadataMapper.name())) {
                 root.putMapper(metadataMapper);
             }
-            rootMappersMap.put(metadataMapper.getClass(), metadataMapper);
+            builder.put(metadataMapper.getClass(), metadataMapper);
         }
         // keep root mappers sorted for consistent serialization
         Arrays.sort(metadataMappers, new Comparator<Mapper>() {
@@ -80,7 +79,7 @@ public final class Mapping implements ToXContent {
                 return o1.name().compareTo(o2.name());
             }
         });
-        this.rootMappersMap = unmodifiableMap(rootMappersMap);
+        this.rootMappersMap = builder.build();
         this.sourceTransforms = sourceTransforms;
         this.meta = meta;
     }
@@ -120,7 +119,7 @@ public final class Mapping implements ToXContent {
             meta = mergeWith.meta;
         }
     }
-
+    
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         root.toXContent(builder, params, new ToXContent() {
@@ -151,11 +150,22 @@ public final class Mapping implements ToXContent {
         return builder;
     }
 
+    /** Serialize to a {@link BytesReference}. */
+    public BytesReference toBytes() {
+        try {
+            XContentBuilder builder = XContentFactory.jsonBuilder().startObject();
+            toXContent(builder, new ToXContent.MapParams(ImmutableMap.<String, String>of()));
+            return builder.endObject().bytes();
+        } catch (IOException bogus) {
+            throw new AssertionError(bogus);
+        }
+    }
+
     @Override
     public String toString() {
         try {
             XContentBuilder builder = XContentFactory.jsonBuilder().startObject();
-            toXContent(builder, new ToXContent.MapParams(emptyMap()));
+            toXContent(builder, new ToXContent.MapParams(ImmutableMap.<String, String>of()));
             return builder.endObject().string();
         } catch (IOException bogus) {
             throw new AssertionError(bogus);
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java
index 78d0385..7468f4f 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java
@@ -79,7 +79,6 @@ public class BinaryFieldMapper extends FieldMapper {
         @Override
         public BinaryFieldMapper build(BuilderContext context) {
             setupFieldType(context);
-            ((BinaryFieldType)fieldType).setTryUncompressing(context.indexCreatedVersion().before(Version.V_2_0_0_beta1));
             return new BinaryFieldMapper(name, fieldType, defaultFieldType,
                     context.indexSettings(), multiFieldsBuilder.build(this, context), copyTo);
         }
@@ -103,13 +102,11 @@ public class BinaryFieldMapper extends FieldMapper {
     }
 
     static final class BinaryFieldType extends MappedFieldType {
-        private boolean tryUncompressing = false;
 
         public BinaryFieldType() {}
 
         protected BinaryFieldType(BinaryFieldType ref) {
             super(ref);
-            this.tryUncompressing = ref.tryUncompressing;
         }
 
         @Override
@@ -117,40 +114,12 @@ public class BinaryFieldMapper extends FieldMapper {
             return new BinaryFieldType(this);
         }
 
-        @Override
-        public boolean equals(Object o) {
-            if (!super.equals(o)) return false;
-            BinaryFieldType that = (BinaryFieldType) o;
-            return Objects.equals(tryUncompressing, that.tryUncompressing);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(super.hashCode(), tryUncompressing);
-        }
 
         @Override
         public String typeName() {
             return CONTENT_TYPE;
         }
 
-        @Override
-        public void checkCompatibility(MappedFieldType fieldType, List<String> conflicts, boolean strict) {
-            super.checkCompatibility(fieldType, conflicts, strict);
-            BinaryFieldType other = (BinaryFieldType)fieldType;
-            if (tryUncompressing() != other.tryUncompressing()) {
-                conflicts.add("mapper [" + names().fullName() + "] has different [try_uncompressing] (IMPOSSIBLE)");
-            }
-        }
-
-        public boolean tryUncompressing() {
-            return tryUncompressing;
-        }
-
-        public void setTryUncompressing(boolean tryUncompressing) {
-            checkIfFrozen();
-            this.tryUncompressing = tryUncompressing;
-        }
 
         @Override
         public BytesReference value(Object value) {
@@ -172,15 +141,7 @@ public class BinaryFieldMapper extends FieldMapper {
                     throw new ElasticsearchParseException("failed to convert bytes", e);
                 }
             }
-            try {
-                if (tryUncompressing) { // backcompat behavior
-                    return CompressorFactory.uncompressIfNeeded(bytes);
-                } else {
-                    return bytes;
-                }
-            } catch (IOException e) {
-                throw new ElasticsearchParseException("failed to decompress source", e);
-            }
+            return bytes;
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java
index 7f06c22..0e512bf 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.index.mapper.core;
 
-import com.carrotsearch.hppc.DoubleArrayList;
-
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.document.Field;
@@ -36,8 +34,6 @@ import org.elasticsearch.common.Explicit;
 import org.elasticsearch.common.Numbers;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.Fuzziness;
-import org.elasticsearch.common.util.ByteUtils;
-import org.elasticsearch.common.util.CollectionUtils;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
@@ -286,17 +282,7 @@ public class DoubleFieldMapper extends NumberFieldMapper {
             fields.add(field);
         }
         if (fieldType().hasDocValues()) {
-            if (useSortedNumericDocValues) {
-                addDocValue(context, fields, doubleToSortableLong(value));
-            } else {
-                CustomDoubleNumericDocValuesField field = (CustomDoubleNumericDocValuesField) context.doc().getByKey(fieldType().names().indexName());
-                if (field != null) {
-                    field.add(value);
-                } else {
-                    field = new CustomDoubleNumericDocValuesField(fieldType().names().indexName(), value);
-                    context.doc().addWithKey(fieldType().names().indexName(), field);
-                }
-            }
+            addDocValue(context, fields, doubleToSortableLong(value));
         }
     }
 
@@ -346,30 +332,4 @@ public class DoubleFieldMapper extends NumberFieldMapper {
         }
     }
 
-    public static class CustomDoubleNumericDocValuesField extends CustomNumericDocValuesField {
-
-        private final DoubleArrayList values;
-
-        public CustomDoubleNumericDocValuesField(String  name, double value) {
-            super(name);
-            values = new DoubleArrayList();
-            add(value);
-        }
-
-        public void add(double value) {
-            values.add(value);
-        }
-
-        @Override
-        public BytesRef binaryValue() {
-            CollectionUtils.sortAndDedup(values);
-
-            final byte[] bytes = new byte[values.size() * 8];
-            for (int i = 0; i < values.size(); ++i) {
-                ByteUtils.writeDoubleLE(values.get(i), bytes, i * 8);
-            }
-            return new BytesRef(bytes);
-        }
-
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java
index caeb2d7..9a607ff 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.index.mapper.core;
 
-import com.carrotsearch.hppc.FloatArrayList;
-
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.document.Field;
@@ -37,8 +35,6 @@ import org.elasticsearch.common.Numbers;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.Fuzziness;
-import org.elasticsearch.common.util.ByteUtils;
-import org.elasticsearch.common.util.CollectionUtils;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
@@ -298,17 +294,7 @@ public class FloatFieldMapper extends NumberFieldMapper {
             fields.add(field);
         }
         if (fieldType().hasDocValues()) {
-            if (useSortedNumericDocValues) {
-                addDocValue(context, fields, floatToSortableInt(value));
-            } else {
-                CustomFloatNumericDocValuesField field = (CustomFloatNumericDocValuesField) context.doc().getByKey(fieldType().names().indexName());
-                if (field != null) {
-                    field.add(value);
-                } else {
-                    field = new CustomFloatNumericDocValuesField(fieldType().names().indexName(), value);
-                    context.doc().addWithKey(fieldType().names().indexName(), field);
-                }
-            }
+            addDocValue(context, fields, floatToSortableInt(value));
         }
     }
 
@@ -357,31 +343,4 @@ public class FloatFieldMapper extends NumberFieldMapper {
             return Float.toString(number);
         }
     }
-
-    public static class CustomFloatNumericDocValuesField extends CustomNumericDocValuesField {
-
-        private final FloatArrayList values;
-
-        public CustomFloatNumericDocValuesField(String  name, float value) {
-            super(name);
-            values = new FloatArrayList();
-            add(value);
-        }
-
-        public void add(float value) {
-            values.add(value);
-        }
-
-        @Override
-        public BytesRef binaryValue() {
-            CollectionUtils.sortAndDedup(values);
-
-            final byte[] bytes = new byte[values.size() * 4];
-            for (int i = 0; i < values.size(); ++i) {
-                ByteUtils.writeFloatLE(values.get(i), bytes, i * 4);
-            }
-            return new BytesRef(bytes);
-        }
-
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java
index 78406c2..3fba511 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.mapper.core;
 
-import com.carrotsearch.hppc.LongArrayList;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.NumericTokenStream;
 import org.apache.lucene.analysis.TokenStream;
@@ -31,14 +30,10 @@ import org.apache.lucene.index.IndexOptions;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.IndexableFieldType;
 import org.apache.lucene.search.Query;
-import org.apache.lucene.store.ByteArrayDataOutput;
 import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.Version;
 import org.elasticsearch.common.Explicit;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.Fuzziness;
-import org.elasticsearch.common.util.ByteUtils;
-import org.elasticsearch.common.util.CollectionUtils;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
 import org.elasticsearch.index.mapper.*;
@@ -170,21 +165,12 @@ public abstract class NumberFieldMapper extends FieldMapper implements AllFieldM
 
     protected Explicit<Boolean> coerce;
     
-    /** 
-     * True if index version is 1.4+
-     * <p>
-     * In this case numerics are encoded with SORTED_NUMERIC docvalues,
-     * otherwise for older indexes we must continue to write BINARY (for now)
-     */
-    protected final boolean useSortedNumericDocValues;
-
     protected NumberFieldMapper(String simpleName, MappedFieldType fieldType, MappedFieldType defaultFieldType,
                                 Explicit<Boolean> ignoreMalformed, Explicit<Boolean> coerce, Settings indexSettings,
                                 MultiFields multiFields, CopyTo copyTo) {
         super(simpleName, fieldType, defaultFieldType, indexSettings, multiFields, copyTo);
         this.ignoreMalformed = ignoreMalformed;
         this.coerce = coerce;
-        this.useSortedNumericDocValues = Version.indexCreated(indexSettings).onOrAfter(Version.V_1_4_0_Beta1);
     }
 
     @Override
@@ -225,17 +211,7 @@ public abstract class NumberFieldMapper extends FieldMapper implements AllFieldM
     protected abstract void innerParseCreateField(ParseContext context, List<Field> fields) throws IOException;
 
     protected final void addDocValue(ParseContext context, List<Field> fields, long value) {
-        if (useSortedNumericDocValues) {
-            fields.add(new SortedNumericDocValuesField(fieldType().names().indexName(), value));
-        } else {
-            CustomLongNumericDocValuesField field = (CustomLongNumericDocValuesField) context.doc().getByKey(fieldType().names().indexName());
-            if (field != null) {
-                field.add(value);
-            } else {
-                field = new CustomLongNumericDocValuesField(fieldType().names().indexName(), value);
-                context.doc().addWithKey(fieldType().names().indexName(), field);
-            }
-        }
+        fields.add(new SortedNumericDocValuesField(fieldType().names().indexName(), value));
     }
 
     /**
@@ -414,40 +390,6 @@ public abstract class NumberFieldMapper extends FieldMapper implements AllFieldM
 
     }
 
-
-    public static class CustomLongNumericDocValuesField extends CustomNumericDocValuesField {
-
-        private final LongArrayList values;
-
-        public CustomLongNumericDocValuesField(String  name, long value) {
-            super(name);
-            values = new LongArrayList();
-            add(value);
-        }
-
-        public void add(long value) {
-            values.add(value);
-        }
-
-        @Override
-        public BytesRef binaryValue() {
-            CollectionUtils.sortAndDedup(values);
-
-            // here is the trick:
-            //  - the first value is zig-zag encoded so that eg. -5 would become positive and would be better compressed by vLong
-            //  - for other values, we only encode deltas using vLong
-            final byte[] bytes = new byte[values.size() * ByteUtils.MAX_BYTES_VLONG];
-            final ByteArrayDataOutput out = new ByteArrayDataOutput(bytes);
-            ByteUtils.writeVLong(out, ByteUtils.zigZagEncode(values.get(0)));
-            for (int i = 1; i < values.size(); ++i) {
-                final long delta = values.get(i) - values.get(i - 1);
-                ByteUtils.writeVLong(out, delta);
-            }
-            return new BytesRef(bytes, 0, out.getPosition());
-        }
-
-    }
-
     @Override
     protected void doXContentBody(XContentBuilder builder, boolean includeDefaults, Params params) throws IOException {
         super.doXContentBody(builder, includeDefaults, params);
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java b/core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java
index 0588bd1..3f142cc 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java
@@ -173,7 +173,7 @@ public class TypeParsers {
                 builder.omitNorms(nodeBooleanValue(propNode));
                 iterator.remove();
             } else if (propName.equals("similarity")) {
-                builder.similarity(parserContext.similarityLookupService().similarity(propNode.toString()));
+                builder.similarity(parserContext.getSimilarity(propNode.toString()));
                 iterator.remove();
             } else if (parseMultiField(builder, name, parserContext, propName, propNode)) {
                 iterator.remove();
@@ -277,7 +277,7 @@ public class TypeParsers {
                 // ignore for old indexes
                 iterator.remove();
             } else if (propName.equals("similarity")) {
-                builder.similarity(parserContext.similarityLookupService().similarity(propNode.toString()));
+                builder.similarity(parserContext.getSimilarity(propNode.toString()));
                 iterator.remove();
             } else if (propName.equals("fielddata")) {
                 final Settings settings = Settings.builder().put(SettingsLoader.Helper.loadNestedFromMap(nodeMapValue(propNode, "fielddata"))).build();
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java
index 4111786..b264bfa 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java
@@ -21,7 +21,6 @@ package org.elasticsearch.index.mapper.geo;
 
 import com.carrotsearch.hppc.ObjectHashSet;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
-import com.google.common.collect.Iterators;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexOptions;
 import org.apache.lucene.util.BytesRef;
@@ -30,6 +29,7 @@ import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Explicit;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.common.geo.GeoDistance;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
@@ -39,14 +39,7 @@ import org.elasticsearch.common.util.ByteUtils;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.support.XContentMapValues;
-import org.elasticsearch.index.mapper.ContentPath;
-import org.elasticsearch.index.mapper.FieldMapper;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.Mapper;
-import org.elasticsearch.index.mapper.MapperParsingException;
-import org.elasticsearch.index.mapper.MergeMappingException;
-import org.elasticsearch.index.mapper.MergeResult;
-import org.elasticsearch.index.mapper.ParseContext;
+import org.elasticsearch.index.mapper.*;
 import org.elasticsearch.index.mapper.core.DoubleFieldMapper;
 import org.elasticsearch.index.mapper.core.NumberFieldMapper;
 import org.elasticsearch.index.mapper.core.NumberFieldMapper.CustomNumericDocValuesField;
@@ -54,18 +47,10 @@ import org.elasticsearch.index.mapper.core.StringFieldMapper;
 import org.elasticsearch.index.mapper.object.ArrayValueMapperParser;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-
-import static org.elasticsearch.index.mapper.MapperBuilders.doubleField;
-import static org.elasticsearch.index.mapper.MapperBuilders.geoPointField;
-import static org.elasticsearch.index.mapper.MapperBuilders.stringField;
-import static org.elasticsearch.index.mapper.core.TypeParsers.parseField;
-import static org.elasticsearch.index.mapper.core.TypeParsers.parseMultiField;
-import static org.elasticsearch.index.mapper.core.TypeParsers.parsePathType;
+import java.util.*;
+
+import static org.elasticsearch.index.mapper.MapperBuilders.*;
+import static org.elasticsearch.index.mapper.core.TypeParsers.*;
 
 /**
  * Parsing: We handle:
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
index e538a00..59b664d 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
@@ -41,7 +41,7 @@ import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.query.QueryShardContext;
-import org.elasticsearch.index.similarity.SimilarityLookupService;
+import org.elasticsearch.index.similarity.SimilarityService;
 
 import java.io.IOException;
 import java.util.Iterator;
@@ -300,7 +300,7 @@ public class AllFieldMapper extends MetadataFieldMapper {
         if (fieldType().similarity() != null) {
             builder.field("similarity", fieldType().similarity().name());
         } else if (includeDefaults) {
-            builder.field("similarity", SimilarityLookupService.DEFAULT_SIMILARITY);
+            builder.field("similarity", SimilarityService.DEFAULT_SIMILARITY);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java
index 1ac34df..1d73139 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.mapper.ip;
 
-import com.google.common.net.InetAddresses;
 import org.apache.lucene.analysis.NumericTokenStream;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexOptions;
@@ -29,6 +28,7 @@ import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefBuilder;
 import org.apache.lucene.util.NumericUtils;
 import org.elasticsearch.common.Explicit;
+import org.elasticsearch.common.network.InetAddresses;
 import org.elasticsearch.common.Numbers;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
diff --git a/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java b/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
index d811f1f..1f8a4c6 100644
--- a/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
+++ b/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
@@ -242,29 +242,12 @@ public final class PercolatorQueriesRegistry extends AbstractIndexShardComponent
     private class RealTimePercolatorOperationListener extends IndexingOperationListener {
 
         @Override
-        public Engine.Create preCreate(Engine.Create create) {
+        public Engine.Index preIndex(Engine.Index operation) {
             // validate the query here, before we index
-            if (PercolatorService.TYPE_NAME.equals(create.type())) {
-                parsePercolatorDocument(create.id(), create.source());
+            if (PercolatorService.TYPE_NAME.equals(operation.type())) {
+                parsePercolatorDocument(operation.id(), operation.source());
             }
-            return create;
-        }
-
-        @Override
-        public void postCreateUnderLock(Engine.Create create) {
-            // add the query under a doc lock
-            if (PercolatorService.TYPE_NAME.equals(create.type())) {
-                addPercolateQuery(create.id(), create.source());
-            }
-        }
-
-        @Override
-        public Engine.Index preIndex(Engine.Index index) {
-            // validate the query here, before we index
-            if (PercolatorService.TYPE_NAME.equals(index.type())) {
-                parsePercolatorDocument(index.id(), index.source());
-            }
-            return index;
+            return operation;
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
index 4ad63eb..59d20ce 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
@@ -314,8 +314,8 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
     public GeoBoundingBoxQueryBuilder doReadFrom(StreamInput in) throws IOException {
         String fieldName = in.readString();
         GeoBoundingBoxQueryBuilder geo = new GeoBoundingBoxQueryBuilder(fieldName);
-        geo.topLeft = geo.topLeft.readFrom(in);
-        geo.bottomRight = geo.bottomRight.readFrom(in);
+        geo.topLeft = in.readGeoPoint();
+        geo.bottomRight = in.readGeoPoint();
         geo.type = GeoExecType.readTypeFrom(in);
         geo.validationMethod = GeoValidationMethod.readGeoValidationMethodFrom(in);
         return geo;
@@ -324,8 +324,8 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
     @Override
     public void doWriteTo(StreamOutput out) throws IOException {
         out.writeString(fieldName);
-        topLeft.writeTo(out);
-        bottomRight.writeTo(out);
+        out.writeGeoPoint(topLeft);
+        out.writeGeoPoint(bottomRight);
         type.writeTo(out);
         validationMethod.writeTo(out);
     }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
index 7d6066e..0560272 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
@@ -261,7 +261,7 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
         GeoDistanceQueryBuilder result = new GeoDistanceQueryBuilder(fieldName);
         result.distance = in.readDouble();
         result.validationMethod = GeoValidationMethod.readGeoValidationMethodFrom(in);
-        result.center = GeoPoint.readGeoPointFrom(in);
+        result.center = in.readGeoPoint();
         result.optimizeBbox = in.readString();
         result.geoDistance = GeoDistance.readGeoDistanceFrom(in);
         return result;
@@ -272,7 +272,7 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
         out.writeString(fieldName);
         out.writeDouble(distance);
         validationMethod.writeTo(out);
-        center.writeTo(out);
+        out.writeGeoPoint(center);
         out.writeString(optimizeBbox);
         geoDistance.writeTo(out);
     }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
index 9a224fa..ee3d8bc 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
@@ -202,7 +202,7 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
     }
     
     /** Returns validation method for coordinates. */
-    public GeoValidationMethod getValidationMethod(GeoValidationMethod method) {
+    public GeoValidationMethod getValidationMethod() {
         return this.validationMethod;
     }
 
@@ -221,6 +221,7 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
             }
         }
 
+        GeoPoint point = new GeoPoint(this.point);
         if (GeoValidationMethod.isCoerce(validationMethod)) {
             GeoUtils.normalizePoint(point, true, true);
         }
@@ -276,7 +277,7 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
 
     @Override
     protected GeoDistanceRangeQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        GeoDistanceRangeQueryBuilder queryBuilder = new GeoDistanceRangeQueryBuilder(in.readString(), GeoPoint.readGeoPointFrom(in));
+        GeoDistanceRangeQueryBuilder queryBuilder = new GeoDistanceRangeQueryBuilder(in.readString(), in.readGeoPoint());
         queryBuilder.from = in.readGenericValue();
         queryBuilder.to = in.readGenericValue();
         queryBuilder.includeLower = in.readBoolean();
@@ -291,7 +292,7 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
     @Override
     protected void doWriteTo(StreamOutput out) throws IOException {
         out.writeString(fieldName);
-        point.writeTo(out);
+        out.writeGeoPoint(point);
         out.writeGenericValue(from);
         out.writeGenericValue(to);
         out.writeBoolean(includeLower);
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
index e832be5..192fd86 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
@@ -164,7 +164,7 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
         List<GeoPoint> shell = new ArrayList<>();
         int size = in.readVInt();
         for (int i = 0; i < size; i++) {
-            shell.add(GeoPoint.readGeoPointFrom(in));
+            shell.add(in.readGeoPoint());
         }
         GeoPolygonQueryBuilder builder = new GeoPolygonQueryBuilder(fieldName, shell);
         builder.validationMethod = GeoValidationMethod.readGeoValidationMethodFrom(in);
@@ -176,7 +176,7 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
         out.writeString(fieldName);
         out.writeVInt(shell.size());
         for (GeoPoint point : shell) {
-            point.writeTo(out);
+            out.writeGeoPoint(point);
         }
         validationMethod.writeTo(out);
     }
diff --git a/core/src/main/java/org/elasticsearch/index/query/ParsedQuery.java b/core/src/main/java/org/elasticsearch/index/query/ParsedQuery.java
index 1c21926..d1a4ca5 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ParsedQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ParsedQuery.java
@@ -19,31 +19,22 @@
 
 package org.elasticsearch.index.query;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.lucene.search.Queries;
 
-import java.util.Map;
-
-import static java.util.Collections.emptyMap;
-
 /**
  * The result of parsing a query.
+ *
+ *
  */
 public class ParsedQuery {
+
     private final Query query;
-    private final Map<String, Query> namedFilters;
+    private final ImmutableMap<String, Query> namedFilters;
 
-    /**
-     * Store the query and filters.
-     *
-     * @param query
-     *            the query
-     * @param namedFilters
-     *            an immutable Map containing the named filters. Good callers
-     *            use emptyMap or unmodifiableMap and copy the source to make
-     *            sure this is immutable.
-     */
-    public ParsedQuery(Query query, Map<String, Query> namedFilters) {
+    public ParsedQuery(Query query, ImmutableMap<String, Query> namedFilters) {
         this.query = query;
         this.namedFilters = namedFilters;
     }
@@ -55,7 +46,7 @@ public class ParsedQuery {
 
     public ParsedQuery(Query query) {
         this.query = query;
-        this.namedFilters = emptyMap();
+        this.namedFilters = ImmutableMap.of();
     }
 
     /**
@@ -65,11 +56,11 @@ public class ParsedQuery {
         return this.query;
     }
 
-    public Map<String, Query> namedFilters() {
-        return namedFilters;
+    public ImmutableMap<String, Query> namedFilters() {
+        return this.namedFilters;
     }
 
     public static ParsedQuery parsedMatchAllQuery() {
-        return new ParsedQuery(Queries.newMatchAllQuery(), emptyMap());
+        return new ParsedQuery(Queries.newMatchAllQuery(), ImmutableMap.<String, Query>of());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java b/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
index 177ae9b..5b12b2d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.query;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.queryparser.classic.MapperQueryParser;
 import org.apache.lucene.queryparser.classic.QueryParserSettings;
@@ -36,11 +37,7 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.Index;
 import org.elasticsearch.index.analysis.AnalysisService;
 import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.index.mapper.ContentPath;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.Mapper;
-import org.elasticsearch.index.mapper.MapperBuilders;
-import org.elasticsearch.index.mapper.MapperService;
+import org.elasticsearch.index.mapper.*;
 import org.elasticsearch.index.mapper.core.StringFieldMapper;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.support.NestedScope;
@@ -57,8 +54,6 @@ import java.util.Collection;
 import java.util.HashMap;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * Context object used to create lucene queries on the shard level.
  */
@@ -153,7 +148,7 @@ public class QueryShardContext {
     }
 
     public Similarity searchSimilarity() {
-        return indexQueryParser.similarityService != null ? indexQueryParser.similarityService.similarity() : null;
+        return indexQueryParser.similarityService != null ? indexQueryParser.similarityService.similarity(indexQueryParser.mapperService) : null;
     }
 
     public String defaultField() {
@@ -191,9 +186,8 @@ public class QueryShardContext {
         }
     }
 
-    public Map<String, Query> copyNamedQueries() {
-        // This might be a good use case for CopyOnWriteHashMap
-        return unmodifiableMap(new HashMap<>(namedQueries));
+    public ImmutableMap<String, Query> copyNamedQueries() {
+        return ImmutableMap.copyOf(namedQueries);
     }
 
     public void combineNamedQueries(QueryShardContext context) {
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/exp/ExponentialDecayFunctionBuilder.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/exp/ExponentialDecayFunctionBuilder.java
index 3c81393..e133abd 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/exp/ExponentialDecayFunctionBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/exp/ExponentialDecayFunctionBuilder.java
@@ -27,7 +27,7 @@ import org.elasticsearch.index.query.functionscore.DecayFunctionBuilder;
 
 public class ExponentialDecayFunctionBuilder extends DecayFunctionBuilder<ExponentialDecayFunctionBuilder> {
 
-    private static final DecayFunction EXP_DECAY_FUNCTION = new ExponentialDecayScoreFunction();
+    public static final DecayFunction EXP_DECAY_FUNCTION = new ExponentialDecayScoreFunction();
 
     public ExponentialDecayFunctionBuilder(String fieldName, Object origin, Object scale, Object offset) {
         super(fieldName, origin, scale, offset);
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/gauss/GaussDecayFunctionBuilder.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/gauss/GaussDecayFunctionBuilder.java
index 621b22a..618503a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/gauss/GaussDecayFunctionBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/gauss/GaussDecayFunctionBuilder.java
@@ -27,7 +27,7 @@ import org.elasticsearch.index.query.functionscore.DecayFunctionBuilder;
 
 public class GaussDecayFunctionBuilder extends DecayFunctionBuilder<GaussDecayFunctionBuilder> {
 
-    private static final DecayFunction GAUSS_DECAY_FUNCTION = new GaussScoreFunction();
+    public static final DecayFunction GAUSS_DECAY_FUNCTION = new GaussScoreFunction();
 
     public GaussDecayFunctionBuilder(String fieldName, Object origin, Object scale, Object offset) {
         super(fieldName, origin, scale, offset);
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/lin/LinearDecayFunctionBuilder.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/lin/LinearDecayFunctionBuilder.java
index 2e63aed..f321ee1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/lin/LinearDecayFunctionBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/lin/LinearDecayFunctionBuilder.java
@@ -26,7 +26,7 @@ import org.elasticsearch.index.query.functionscore.DecayFunctionBuilder;
 
 public class LinearDecayFunctionBuilder extends DecayFunctionBuilder<LinearDecayFunctionBuilder> {
 
-    private static final DecayFunction LINEAR_DECAY_FUNCTION = new LinearDecayScoreFunction();
+    public static final DecayFunction LINEAR_DECAY_FUNCTION = new LinearDecayScoreFunction();
 
     public LinearDecayFunctionBuilder(String fieldName, Object origin, Object scale, Object offset) {
         super(fieldName, origin, scale, offset);
diff --git a/core/src/main/java/org/elasticsearch/index/search/stats/ShardSearchStats.java b/core/src/main/java/org/elasticsearch/index/search/stats/ShardSearchStats.java
index 829db9d..3ef5652 100644
--- a/core/src/main/java/org/elasticsearch/index/search/stats/ShardSearchStats.java
+++ b/core/src/main/java/org/elasticsearch/index/search/stats/ShardSearchStats.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.search.stats;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.metrics.CounterMetric;
 import org.elasticsearch.common.metrics.MeanMetric;
@@ -30,8 +31,6 @@ import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.TimeUnit;
 
-import static java.util.Collections.emptyMap;
-
 /**
  */
 public final class ShardSearchStats {
@@ -39,7 +38,7 @@ public final class ShardSearchStats {
     private final SearchSlowLog slowLogSearchService;
     private final StatsHolder totalStats = new StatsHolder();
     private final CounterMetric openContexts = new CounterMetric();
-    private volatile Map<String, StatsHolder> groupsStats = emptyMap();
+    private volatile Map<String, StatsHolder> groupsStats = ImmutableMap.of();
 
     public ShardSearchStats(Settings indexSettings) {
         this.slowLogSearchService = new SearchSlowLog(indexSettings);
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
index ea2d555..86e53b4 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -33,7 +33,6 @@ import org.elasticsearch.action.admin.indices.optimize.OptimizeRequest;
 import org.elasticsearch.action.admin.indices.upgrade.post.UpgradeRequest;
 import org.elasticsearch.action.termvectors.TermVectorsRequest;
 import org.elasticsearch.action.termvectors.TermVectorsResponse;
-import org.elasticsearch.bootstrap.Elasticsearch;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.ShardRouting;
@@ -43,6 +42,7 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.support.LoggerMessageFormat;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.metrics.MeanMetric;
 import org.elasticsearch.common.settings.Settings;
@@ -84,8 +84,8 @@ import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.settings.IndexSettingsService;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.index.snapshots.IndexShardRepository;
-import org.elasticsearch.index.store.Store;
 import org.elasticsearch.index.store.Store.MetadataSnapshot;
+import org.elasticsearch.index.store.Store;
 import org.elasticsearch.index.store.StoreFileMetaData;
 import org.elasticsearch.index.store.StoreStats;
 import org.elasticsearch.index.suggest.stats.ShardSuggestMetric;
@@ -100,6 +100,7 @@ import org.elasticsearch.index.warmer.WarmerStats;
 import org.elasticsearch.indices.IndicesWarmer;
 import org.elasticsearch.indices.InternalIndicesLifecycle;
 import org.elasticsearch.indices.cache.query.IndicesQueryCache;
+import org.elasticsearch.indices.memory.IndexingMemoryController;
 import org.elasticsearch.indices.recovery.RecoveryFailedException;
 import org.elasticsearch.indices.recovery.RecoveryState;
 import org.elasticsearch.percolator.PercolatorService;
@@ -118,16 +119,15 @@ import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicReference;
 
+
 public class IndexShard extends AbstractIndexShardComponent implements IndexSettingsService.Listener {
 
     private final ThreadPool threadPool;
     private final MapperService mapperService;
-    private final IndexQueryParserService queryParserService;
     private final IndexCache indexCache;
     private final InternalIndicesLifecycle indicesLifecycle;
     private final Store store;
     private final MergeSchedulerConfig mergeSchedulerConfig;
-    private final IndexAliasesService indexAliasesService;
     private final ShardIndexingService indexingService;
     private final ShardSearchStats searchService;
     private final ShardGetService getService;
@@ -190,6 +190,13 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
 
     private final IndexSearcherWrapper searcherWrapper;
 
+    /** True if this shard is still indexing (recently) and false if we've been idle for long enough (as periodically checked by {@link
+     *  IndexingMemoryController}). */
+    private final AtomicBoolean active = new AtomicBoolean();
+
+    private volatile long lastWriteNS;
+    private final IndexingMemoryController indexingMemoryController;
+
     @Inject
     public IndexShard(ShardId shardId, @IndexSettings Settings indexSettings, ShardPath path, Store store, IndexServicesProvider provider) {
         super(shardId, indexSettings);
@@ -202,11 +209,9 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         this.indicesLifecycle = (InternalIndicesLifecycle) provider.getIndicesLifecycle();
         this.store = store;
         this.mergeSchedulerConfig = new MergeSchedulerConfig(indexSettings);
-        this.threadPool =  provider.getThreadPool();
-        this.mapperService =  provider.getMapperService();
-        this.queryParserService =  provider.getQueryParserService();
-        this.indexCache =  provider.getIndexCache();
-        this.indexAliasesService =  provider.getIndexAliasesService();
+        this.threadPool = provider.getThreadPool();
+        this.mapperService = provider.getMapperService();
+        this.indexCache = provider.getIndexCache();
         this.indexingService = new ShardIndexingService(shardId, indexSettings);
         this.getService = new ShardGetService(this, mapperService);
         this.termVectorsService =  provider.getTermVectorsService();
@@ -242,11 +247,16 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         this.flushThresholdSize = indexSettings.getAsBytesSize(INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE, new ByteSizeValue(512, ByteSizeUnit.MB));
         this.disableFlush = indexSettings.getAsBoolean(INDEX_TRANSLOG_DISABLE_FLUSH, false);
         this.indexShardOperationCounter = new IndexShardOperationCounter(logger, shardId);
+        this.indexingMemoryController = provider.getIndexingMemoryController();
+
         this.searcherWrapper = provider.getIndexSearcherWrapper();
-        this.percolatorQueriesRegistry = new PercolatorQueriesRegistry(shardId, indexSettings, queryParserService, indexingService, mapperService, indexFieldDataService);
+        this.percolatorQueriesRegistry = new PercolatorQueriesRegistry(shardId, indexSettings, provider.getQueryParserService(), indexingService, mapperService, indexFieldDataService);
         if (mapperService.hasMapping(PercolatorService.TYPE_NAME)) {
             percolatorQueriesRegistry.enableRealTimePercolator();
         }
+
+        // We start up inactive
+        active.set(false);
     }
 
     public Store store() {
@@ -278,7 +288,9 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         return indexFieldDataService;
     }
 
-    public MapperService mapperService() { return mapperService;}
+    public MapperService mapperService() {
+        return mapperService;
+    }
 
     public ShardSearchStats searchService() {
         return this.searchService;
@@ -423,40 +435,6 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         return previousState;
     }
 
-    public Engine.Create prepareCreate(SourceToParse source, long version, VersionType versionType, Engine.Operation.Origin origin) {
-        try {
-            return prepareCreate(docMapper(source.type()), source, version, versionType, origin);
-        } catch (Throwable t) {
-            verifyNotClosed(t);
-            throw t;
-        }
-    }
-
-    static Engine.Create prepareCreate(DocumentMapperForType docMapper, SourceToParse source, long version, VersionType versionType, Engine.Operation.Origin origin) {
-        long startTime = System.nanoTime();
-        ParsedDocument doc = docMapper.getDocumentMapper().parse(source);
-        if (docMapper.getMapping() != null) {
-            doc.addDynamicMappingsUpdate(docMapper.getMapping());
-        }
-        return new Engine.Create(docMapper.getDocumentMapper().uidMapper().term(doc.uid().stringValue()), doc, version, versionType, origin, startTime);
-    }
-
-    public void create(Engine.Create create) {
-        writeAllowed(create.origin());
-        create = indexingService.preCreate(create);
-        try {
-            if (logger.isTraceEnabled()) {
-                logger.trace("index [{}][{}]{}", create.type(), create.id(), create.docs());
-            }
-            getEngine().create(create);
-            create.endTime(System.nanoTime());
-        } catch (Throwable ex) {
-            indexingService.postCreate(create, ex);
-            throw ex;
-        }
-        indexingService.postCreate(create);
-    }
-
     public Engine.Index prepareIndex(SourceToParse source, long version, VersionType versionType, Engine.Operation.Origin origin) {
         try {
             return prepareIndex(docMapper(source.type()), source, version, versionType, origin);
@@ -480,7 +458,8 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
      * updated.
      */
     public boolean index(Engine.Index index) {
-        writeAllowed(index.origin());
+        ensureWriteAllowed(index);
+        markLastWrite(index);
         index = indexingService.preIndex(index);
         final boolean created;
         try {
@@ -504,7 +483,8 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
     }
 
     public void delete(Engine.Delete delete) {
-        writeAllowed(delete.origin());
+        ensureWriteAllowed(delete);
+        markLastWrite(delete);
         delete = indexingService.preDelete(delete);
         try {
             if (logger.isTraceEnabled()) {
@@ -914,7 +894,24 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         }
     }
 
-    private void writeAllowed(Engine.Operation.Origin origin) throws IllegalIndexShardStateException {
+    /** Returns timestamp of last indexing operation */
+    public long getLastWriteNS() {
+        return lastWriteNS;
+    }
+
+    /** Records timestamp of the last write operation, possibly switching {@code active} to true if we were inactive. */
+    private void markLastWrite(Engine.Operation op) {
+        lastWriteNS = op.startTime();
+        if (active.getAndSet(true) == false) {
+            // We are currently inactive, but a new write operation just showed up, so we now notify IMC
+            // to wake up and fix our indexing buffer.  We could do this async instead, but cost should
+            // be low, and it's rare this happens.
+            indexingMemoryController.forceCheck();
+        }
+    }
+
+    private void ensureWriteAllowed(Engine.Operation op) throws IllegalIndexShardStateException {
+        Engine.Operation.Origin origin = op.origin();
         IndexShardState state = this.state; // one time volatile read
 
         if (origin == Engine.Operation.Origin.PRIMARY) {
@@ -976,6 +973,8 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         this.failedEngineListener.delegates.add(failedEngineListener);
     }
 
+    /** Change the indexing and translog buffer sizes.  If {@code IndexWriter} is currently using more than
+     *  the new buffering indexing size then we do a refresh to free up the heap. */
     public void updateBufferSize(ByteSizeValue shardIndexingBufferSize, ByteSizeValue shardTranslogBufferSize) {
 
         final EngineConfig config = engineConfig;
@@ -994,27 +993,50 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
             // so we push changes these changes down to IndexWriter:
             engine.onSettingsChanged();
 
-            if (shardIndexingBufferSize == EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER) {
-                // it's inactive: make sure we do a refresh / full IW flush in this case, since the memory
-                // changes only after a "data" change has happened to the writer
-                // the index writer lazily allocates memory and a refresh will clean it all up.
-                logger.debug("updating index_buffer_size from [{}] to (inactive) [{}]", preValue, shardIndexingBufferSize);
+            long iwBytesUsed = engine.indexWriterRAMBytesUsed();
+
+            String message = LoggerMessageFormat.format("updating index_buffer_size from [{}] to [{}]; IndexWriter now using [{}] bytes",
+                                                        preValue, shardIndexingBufferSize, iwBytesUsed);
+
+            if (iwBytesUsed > shardIndexingBufferSize.bytes()) {
+                // our allowed buffer was changed to less than we are currently using; we ask IW to refresh
+                // so it clears its buffers (otherwise it won't clear until the next indexing/delete op)
+                logger.debug(message + "; now refresh to clear IndexWriter memory");
+
+                // TODO: should IW have an API to move segments to disk, but not refresh?  Its flush method is protected...
                 try {
                     refresh("update index buffer");
                 } catch (Throwable e) {
-                    logger.warn("failed to refresh after setting shard to inactive", e);
+                    logger.warn("failed to refresh after decreasing index buffer", e);
                 }
             } else {
-                logger.debug("updating index_buffer_size from [{}] to [{}]", preValue, shardIndexingBufferSize);
+                logger.debug(message);
             }
         }
 
         engine.getTranslog().updateBuffer(shardTranslogBufferSize);
     }
 
-    public void markAsInactive() {
-        updateBufferSize(EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER, TranslogConfig.INACTIVE_SHARD_TRANSLOG_BUFFER);
-        indicesLifecycle.onShardInactive(this);
+    /** Called by {@link IndexingMemoryController} to check whether more than {@code inactiveTimeNS} has passed since the last
+     *  indexing operation, and become inactive (reducing indexing and translog buffers to tiny values) if so.  This returns true
+     *  if the shard is inactive. */
+    public boolean checkIdle(long inactiveTimeNS) {
+        if (System.nanoTime() - lastWriteNS >= inactiveTimeNS) {
+            boolean wasActive = active.getAndSet(false);
+            if (wasActive) {
+                updateBufferSize(IndexingMemoryController.INACTIVE_SHARD_INDEXING_BUFFER, IndexingMemoryController.INACTIVE_SHARD_TRANSLOG_BUFFER);
+                logger.debug("shard is now inactive");
+                indicesLifecycle.onShardInactive(this);
+            }
+        }
+
+        return active.get() == false;
+    }
+
+    /** Returns {@code true} if this shard is active (has seen indexing ops in the last {@link
+     *  IndexingMemoryController#SHARD_INACTIVE_TIME_SETTING} (default 5 minutes), else {@code false}. */
+    public boolean getActive() {
+        return active.get();
     }
 
     public final boolean isFlushOnClose() {
@@ -1416,8 +1438,7 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
     }
 
     private final EngineConfig newEngineConfig(TranslogConfig translogConfig, QueryCachingPolicy cachingPolicy) {
-        final TranslogRecoveryPerformer translogRecoveryPerformer = new TranslogRecoveryPerformer(shardId, mapperService, queryParserService,
-                indexAliasesService, indexCache, logger) {
+        final TranslogRecoveryPerformer translogRecoveryPerformer = new TranslogRecoveryPerformer(shardId, mapperService, logger) {
             @Override
             protected void operationProcessed() {
                 assert recoveryState != null;
@@ -1426,7 +1447,7 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         };
         return new EngineConfig(shardId,
                 threadPool, indexingService, indexSettings, warmer, store, deletionPolicy, mergePolicyConfig.getMergePolicy(), mergeSchedulerConfig,
-                mapperService.indexAnalyzer(), similarityService.similarity(), codecService, failedEngineListener, translogRecoveryPerformer, indexCache.query(), cachingPolicy, translogConfig);
+                mapperService.indexAnalyzer(), similarityService.similarity(mapperService), codecService, failedEngineListener, translogRecoveryPerformer, indexCache.query(), cachingPolicy, translogConfig);
     }
 
     private static class IndexShardOperationCounter extends AbstractRefCounted {
@@ -1499,6 +1520,7 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
     /**
      * Schedules a flush if needed but won't schedule more than one flush concurrently. The flush will be executed on the
      * Flush thread-pool asynchronously.
+     *
      * @return <code>true</code> if a new flush is scheduled otherwise <code>false</code>.
      */
     public boolean maybeFlush() {
diff --git a/core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java
index c81b9e5..8bdf1fb 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java
@@ -18,6 +18,8 @@
  */
 package org.elasticsearch.index.shard;
 
+import java.io.IOException;
+
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.IndexServicesProvider;
@@ -26,8 +28,7 @@ import org.elasticsearch.index.engine.EngineConfig;
 import org.elasticsearch.index.merge.MergeStats;
 import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.store.Store;
-
-import java.io.IOException;
+import org.elasticsearch.index.translog.TranslogStats;
 
 /**
  * ShadowIndexShard extends {@link IndexShard} to add file synchronization
@@ -82,4 +83,9 @@ public final class ShadowIndexShard extends IndexShard {
     public boolean allowsPrimaryPromotion() {
         return false;
     }
+
+    @Override
+    public TranslogStats translogStats() {
+        return null; // shadow engine has no translog
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java b/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
index f893ec4..68c552d 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
@@ -18,27 +18,13 @@
  */
 package org.elasticsearch.index.shard;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.join.BitSetProducer;
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.Version;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.aliases.IndexAliasesService;
-import org.elasticsearch.index.cache.IndexCache;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.engine.IgnoreOnRecoveryEngineException;
 import org.elasticsearch.index.mapper.*;
-import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.index.translog.Translog;
 
 import java.io.IOException;
@@ -53,20 +39,13 @@ import static org.elasticsearch.index.mapper.SourceToParse.source;
  */
 public class TranslogRecoveryPerformer {
     private final MapperService mapperService;
-    private final IndexQueryParserService queryParserService;
-    private final IndexAliasesService indexAliasesService;
-    private final IndexCache indexCache;
     private final ESLogger logger;
     private final Map<String, Mapping> recoveredTypes = new HashMap<>();
     private final ShardId shardId;
 
-    protected TranslogRecoveryPerformer(ShardId shardId, MapperService mapperService, IndexQueryParserService queryParserService,
-                                        IndexAliasesService indexAliasesService, IndexCache indexCache, ESLogger logger) {
+    protected TranslogRecoveryPerformer(ShardId shardId, MapperService mapperService, ESLogger logger) {
         this.shardId = shardId;
         this.mapperService = mapperService;
-        this.queryParserService = queryParserService;
-        this.indexAliasesService = indexAliasesService;
-        this.indexCache = indexCache;
         this.logger = logger;
     }
 
@@ -145,19 +124,7 @@ public class TranslogRecoveryPerformer {
     public void performRecoveryOperation(Engine engine, Translog.Operation operation, boolean allowMappingUpdates) {
         try {
             switch (operation.opType()) {
-                case CREATE:
-                    Translog.Create create = (Translog.Create) operation;
-                    Engine.Create engineCreate = IndexShard.prepareCreate(docMapper(create.type()),
-                            source(create.source()).index(shardId.getIndex()).type(create.type()).id(create.id())
-                                    .routing(create.routing()).parent(create.parent()).timestamp(create.timestamp()).ttl(create.ttl()),
-                            create.version(), create.versionType().versionTypeForReplicationAndRecovery(), Engine.Operation.Origin.RECOVERY);
-                    maybeAddMappingUpdate(engineCreate.type(), engineCreate.parsedDoc().dynamicMappingsUpdate(), engineCreate.id(), allowMappingUpdates);
-                    if (logger.isTraceEnabled()) {
-                        logger.trace("[translog] recover [create] op of [{}][{}]", create.type(), create.id());
-                    }
-                    engine.create(engineCreate);
-                    break;
-                case SAVE:
+                case INDEX:
                     Translog.Index index = (Translog.Index) operation;
                     Engine.Index engineIndex = IndexShard.prepareIndex(docMapper(index.type()), source(index.source()).type(index.type()).id(index.id())
                                     .routing(index.routing()).parent(index.parent()).timestamp(index.timestamp()).ttl(index.ttl()),
@@ -177,11 +144,6 @@ public class TranslogRecoveryPerformer {
                     engine.delete(new Engine.Delete(uid.type(), uid.id(), delete.uid(), delete.version(),
                             delete.versionType().versionTypeForReplicationAndRecovery(), Engine.Operation.Origin.RECOVERY, System.nanoTime(), false));
                     break;
-                case DELETE_BY_QUERY:
-                    Translog.DeleteByQuery deleteByQuery = (Translog.DeleteByQuery) operation;
-                    engine.delete(prepareDeleteByQuery(queryParserService, mapperService, indexAliasesService, indexCache,
-                            deleteByQuery.source(), deleteByQuery.filteringAliases(), Engine.Operation.Origin.RECOVERY, deleteByQuery.types()));
-                    break;
                 default:
                     throw new IllegalStateException("No operation defined for [" + operation + "]");
             }
@@ -206,38 +168,6 @@ public class TranslogRecoveryPerformer {
         operationProcessed();
     }
 
-    private static Engine.DeleteByQuery prepareDeleteByQuery(IndexQueryParserService queryParserService, MapperService mapperService, IndexAliasesService indexAliasesService, IndexCache indexCache, BytesReference source, @Nullable String[] filteringAliases, Engine.Operation.Origin origin, String... types) {
-        long startTime = System.nanoTime();
-        if (types == null) {
-            types = Strings.EMPTY_ARRAY;
-        }
-        Query query;
-        try {
-            query = queryParserService.parseQuery(source).query();
-        } catch (ParsingException ex) {
-            // for BWC we try to parse directly the query since pre 1.0.0.Beta2 we didn't require a top level query field
-            if (queryParserService.getIndexCreatedVersion().onOrBefore(Version.V_1_0_0_Beta2)) {
-                try {
-                    XContentParser parser = XContentHelper.createParser(source);
-                    ParsedQuery parse = queryParserService.parse(parser);
-                    query = parse.query();
-                } catch (Throwable t) {
-                    ex.addSuppressed(t);
-                    throw ex;
-                }
-            } else {
-                throw ex;
-            }
-        }
-        Query searchFilter = mapperService.searchFilter(types);
-        if (searchFilter != null) {
-            query = Queries.filtered(query, searchFilter);
-        }
-
-        Query aliasFilter = indexAliasesService.aliasFilter(filteringAliases);
-        BitSetProducer parentFilter = mapperService.hasNested() ? indexCache.bitsetFilterCache().getBitSetProducer(Queries.newNonNestedFilter()) : null;
-        return new Engine.DeleteByQuery(query, source, filteringAliases, aliasFilter, parentFilter, origin, startTime, types);
-    }
 
     /**
      * Called once for every processed operation by this recovery performer.
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/BM25SimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/BM25SimilarityProvider.java
index 1983c4e..68e50da 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/BM25SimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/BM25SimilarityProvider.java
@@ -21,8 +21,6 @@ package org.elasticsearch.index.similarity;
 
 import org.apache.lucene.search.similarities.BM25Similarity;
 import org.apache.lucene.search.similarities.Similarity;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.inject.assistedinject.Assisted;
 import org.elasticsearch.common.settings.Settings;
 
 /**
@@ -40,8 +38,7 @@ public class BM25SimilarityProvider extends AbstractSimilarityProvider {
 
     private final BM25Similarity similarity;
 
-    @Inject
-    public BM25SimilarityProvider(@Assisted String name, @Assisted Settings settings) {
+    public BM25SimilarityProvider(String name, Settings settings) {
         super(name);
         float k1 = settings.getAsFloat("k1", 1.2f);
         float b = settings.getAsFloat("b", 0.75f);
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java
index 7858cb1..10ba1d4 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java
@@ -19,29 +19,13 @@
 
 package org.elasticsearch.index.similarity;
 
-import org.apache.lucene.search.similarities.AfterEffect;
-import org.apache.lucene.search.similarities.AfterEffectB;
-import org.apache.lucene.search.similarities.AfterEffectL;
-import org.apache.lucene.search.similarities.BasicModel;
-import org.apache.lucene.search.similarities.BasicModelBE;
-import org.apache.lucene.search.similarities.BasicModelD;
-import org.apache.lucene.search.similarities.BasicModelG;
-import org.apache.lucene.search.similarities.BasicModelIF;
-import org.apache.lucene.search.similarities.BasicModelIn;
-import org.apache.lucene.search.similarities.BasicModelIne;
-import org.apache.lucene.search.similarities.BasicModelP;
-import org.apache.lucene.search.similarities.DFRSimilarity;
-import org.apache.lucene.search.similarities.Normalization;
-import org.apache.lucene.search.similarities.Similarity;
+import com.google.common.collect.ImmutableMap;
+import org.apache.lucene.search.similarities.*;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.assistedinject.Assisted;
 import org.elasticsearch.common.settings.Settings;
 
-import java.util.HashMap;
-import java.util.Map;
-
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * {@link SimilarityProvider} for {@link DFRSimilarity}.
  * <p>
@@ -54,11 +38,12 @@ import static java.util.Collections.unmodifiableMap;
  * @see DFRSimilarity For more information about configuration
  */
 public class DFRSimilarityProvider extends AbstractSimilarityProvider {
-    private static final Map<String, BasicModel> MODEL_CACHE;
-    private static final Map<String, AfterEffect> EFFECT_CACHE;
+
+    private static final ImmutableMap<String, BasicModel> MODEL_CACHE;
+    private static final ImmutableMap<String, AfterEffect> EFFECT_CACHE;
 
     static {
-        Map<String, BasicModel> models = new HashMap<>();
+        MapBuilder<String, BasicModel> models = MapBuilder.newMapBuilder();
         models.put("be", new BasicModelBE());
         models.put("d", new BasicModelD());
         models.put("g", new BasicModelG());
@@ -66,19 +51,18 @@ public class DFRSimilarityProvider extends AbstractSimilarityProvider {
         models.put("in", new BasicModelIn());
         models.put("ine", new BasicModelIne());
         models.put("p", new BasicModelP());
-        MODEL_CACHE = unmodifiableMap(models);
+        MODEL_CACHE = models.immutableMap();
 
-        Map<String, AfterEffect> effects = new HashMap<>();
+        MapBuilder<String, AfterEffect> effects = MapBuilder.newMapBuilder();
         effects.put("no", new AfterEffect.NoAfterEffect());
         effects.put("b", new AfterEffectB());
         effects.put("l", new AfterEffectL());
-        EFFECT_CACHE = unmodifiableMap(effects);
+        EFFECT_CACHE = effects.immutableMap();
     }
 
     private final DFRSimilarity similarity;
 
-    @Inject
-    public DFRSimilarityProvider(@Assisted String name, @Assisted Settings settings) {
+    public DFRSimilarityProvider(String name, Settings settings) {
         super(name);
         BasicModel basicModel = parseBasicModel(settings);
         AfterEffect afterEffect = parseAfterEffect(settings);
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/DefaultSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/DefaultSimilarityProvider.java
index 0f9feba..3acbd98 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/DefaultSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/DefaultSimilarityProvider.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.index.similarity;
 
 import org.apache.lucene.search.similarities.DefaultSimilarity;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.inject.assistedinject.Assisted;
 import org.elasticsearch.common.settings.Settings;
 
 /**
@@ -37,8 +35,7 @@ public class DefaultSimilarityProvider extends AbstractSimilarityProvider {
 
     private final DefaultSimilarity similarity = new DefaultSimilarity();
 
-    @Inject
-    public DefaultSimilarityProvider(@Assisted String name, @Assisted Settings settings) {
+    public DefaultSimilarityProvider(String name, Settings settings) {
         super(name);
         boolean discountOverlaps = settings.getAsBoolean("discount_overlaps", true);
         this.similarity.setDiscountOverlaps(discountOverlaps);
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java
index 2f619c5..eb8d20a 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java
@@ -19,24 +19,11 @@
 
 package org.elasticsearch.index.similarity;
 
-import org.apache.lucene.search.similarities.Distribution;
-import org.apache.lucene.search.similarities.DistributionLL;
-import org.apache.lucene.search.similarities.DistributionSPL;
-import org.apache.lucene.search.similarities.IBSimilarity;
-import org.apache.lucene.search.similarities.Lambda;
-import org.apache.lucene.search.similarities.LambdaDF;
-import org.apache.lucene.search.similarities.LambdaTTF;
-import org.apache.lucene.search.similarities.Normalization;
-import org.apache.lucene.search.similarities.Similarity;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.inject.assistedinject.Assisted;
+import com.google.common.collect.ImmutableMap;
+import org.apache.lucene.search.similarities.*;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.settings.Settings;
 
-import java.util.HashMap;
-import java.util.Map;
-
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * {@link SimilarityProvider} for {@link IBSimilarity}.
  * <p>
@@ -50,25 +37,24 @@ import static java.util.Collections.unmodifiableMap;
  */
 public class IBSimilarityProvider extends AbstractSimilarityProvider {
 
-    private static final Map<String, Distribution> DISTRIBUTIONS;
-    private static final Map<String, Lambda> LAMBDAS;
+    private static final ImmutableMap<String, Distribution> DISTRIBUTION_CACHE;
+    private static final ImmutableMap<String, Lambda> LAMBDA_CACHE;
 
     static {
-        Map<String, Distribution> distributions = new HashMap<>();
+        MapBuilder<String, Distribution> distributions = MapBuilder.newMapBuilder();
         distributions.put("ll", new DistributionLL());
         distributions.put("spl", new DistributionSPL());
-        DISTRIBUTIONS = unmodifiableMap(distributions);
+        DISTRIBUTION_CACHE = distributions.immutableMap();
 
-        Map<String, Lambda> lamdas = new HashMap<>();
+        MapBuilder<String, Lambda> lamdas = MapBuilder.newMapBuilder();
         lamdas.put("df", new LambdaDF());
         lamdas.put("ttf", new LambdaTTF());
-        LAMBDAS = unmodifiableMap(lamdas);
+        LAMBDA_CACHE = lamdas.immutableMap();
     }
 
     private final IBSimilarity similarity;
 
-    @Inject
-    public IBSimilarityProvider(@Assisted String name, @Assisted Settings settings) {
+    public IBSimilarityProvider(String name, Settings settings) {
         super(name);
         Distribution distribution = parseDistribution(settings);
         Lambda lambda = parseLambda(settings);
@@ -84,7 +70,7 @@ public class IBSimilarityProvider extends AbstractSimilarityProvider {
      */
     protected Distribution parseDistribution(Settings settings) {
         String rawDistribution = settings.get("distribution");
-        Distribution distribution = DISTRIBUTIONS.get(rawDistribution);
+        Distribution distribution = DISTRIBUTION_CACHE.get(rawDistribution);
         if (distribution == null) {
             throw new IllegalArgumentException("Unsupported Distribution [" + rawDistribution + "]");
         }
@@ -99,7 +85,7 @@ public class IBSimilarityProvider extends AbstractSimilarityProvider {
      */
     protected Lambda parseLambda(Settings settings) {
         String rawLambda = settings.get("lambda");
-        Lambda lambda = LAMBDAS.get(rawLambda);
+        Lambda lambda = LAMBDA_CACHE.get(rawLambda);
         if (lambda == null) {
             throw new IllegalArgumentException("Unsupported Lambda [" + rawLambda + "]");
         }
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/LMDirichletSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/LMDirichletSimilarityProvider.java
index efea285..24494dc 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/LMDirichletSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/LMDirichletSimilarityProvider.java
@@ -21,8 +21,6 @@ package org.elasticsearch.index.similarity;
 
 import org.apache.lucene.search.similarities.LMDirichletSimilarity;
 import org.apache.lucene.search.similarities.Similarity;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.inject.assistedinject.Assisted;
 import org.elasticsearch.common.settings.Settings;
 
 /**
@@ -38,8 +36,7 @@ public class LMDirichletSimilarityProvider extends AbstractSimilarityProvider {
 
     private final LMDirichletSimilarity similarity;
 
-    @Inject
-    public LMDirichletSimilarityProvider(@Assisted String name, @Assisted Settings settings) {
+    public LMDirichletSimilarityProvider(String name, Settings settings) {
         super(name);
         float mu = settings.getAsFloat("mu", 2000f);
         this.similarity = new LMDirichletSimilarity(mu);
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/LMJelinekMercerSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/LMJelinekMercerSimilarityProvider.java
index 5d30b30..3d5a40f 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/LMJelinekMercerSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/LMJelinekMercerSimilarityProvider.java
@@ -38,8 +38,7 @@ public class LMJelinekMercerSimilarityProvider extends AbstractSimilarityProvide
 
     private final LMJelinekMercerSimilarity similarity;
 
-    @Inject
-    public LMJelinekMercerSimilarityProvider(@Assisted String name, @Assisted Settings settings) {
+    public LMJelinekMercerSimilarityProvider(String name, Settings settings) {
         super(name);
         float lambda = settings.getAsFloat("lambda", 0.1f);
         this.similarity = new LMJelinekMercerSimilarity(lambda);
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/PreBuiltSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/PreBuiltSimilarityProvider.java
deleted file mode 100644
index 4b3f0cc..0000000
--- a/core/src/main/java/org/elasticsearch/index/similarity/PreBuiltSimilarityProvider.java
+++ /dev/null
@@ -1,73 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.similarity;
-
-import org.apache.lucene.search.similarities.Similarity;
-import org.elasticsearch.common.settings.Settings;
-
-/**
- * {@link SimilarityProvider} for pre-built Similarities
- */
-public class PreBuiltSimilarityProvider extends AbstractSimilarityProvider {
-
-    public static class Factory implements SimilarityProvider.Factory {
-
-        private final PreBuiltSimilarityProvider similarity;
-
-        public Factory(String name, Similarity similarity) {
-            this.similarity = new PreBuiltSimilarityProvider(name, similarity);
-        }
-
-        @Override
-        public SimilarityProvider create(String name, Settings settings) {
-            return similarity;
-        }
-
-        public String name() {
-            return similarity.name();
-        }
-
-        public SimilarityProvider get() {
-            return similarity;
-        }
-    }
-
-    private final Similarity similarity;
-
-    /**
-     * Creates a new {@link PreBuiltSimilarityProvider} with the given name and given
-     * pre-built Similarity
-     *
-     * @param name Name of the Provider
-     * @param similarity Pre-built Similarity
-     */
-    public PreBuiltSimilarityProvider(String name, Similarity similarity) {
-        super(name);
-        this.similarity = similarity;
-    }
-
-    /**
-     * {@inheritDoc}
-     */
-    @Override
-    public Similarity get() {
-        return similarity;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/Similarities.java b/core/src/main/java/org/elasticsearch/index/similarity/Similarities.java
deleted file mode 100644
index 3bbbc85..0000000
--- a/core/src/main/java/org/elasticsearch/index/similarity/Similarities.java
+++ /dev/null
@@ -1,58 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.similarity;
-
-import org.apache.lucene.search.similarities.BM25Similarity;
-import org.apache.lucene.search.similarities.DefaultSimilarity;
-
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.Map;
-
-import static java.util.Collections.unmodifiableMap;
-
-/**
- * Cache of pre-defined Similarities
- */
-public class Similarities {
-
-    private static final Map<String, PreBuiltSimilarityProvider.Factory> PRE_BUILT_SIMILARITIES;
-
-    static {
-        Map<String, PreBuiltSimilarityProvider.Factory> similarities = new HashMap<>();
-        similarities.put(SimilarityLookupService.DEFAULT_SIMILARITY,
-                new PreBuiltSimilarityProvider.Factory(SimilarityLookupService.DEFAULT_SIMILARITY, new DefaultSimilarity()));
-        similarities.put("BM25", new PreBuiltSimilarityProvider.Factory("BM25", new BM25Similarity()));
-
-        PRE_BUILT_SIMILARITIES = unmodifiableMap(similarities);
-    }
-
-    private Similarities() {
-    }
-
-    /**
-     * Returns the list of pre-defined SimilarityProvider Factories
-     *
-     * @return Pre-defined SimilarityProvider Factories
-     */
-    public static Collection<PreBuiltSimilarityProvider.Factory> listFactories() {
-        return PRE_BUILT_SIMILARITIES.values();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityLookupService.java b/core/src/main/java/org/elasticsearch/index/similarity/SimilarityLookupService.java
deleted file mode 100644
index 903f0ea..0000000
--- a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityLookupService.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.similarity;
-
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.AbstractIndexComponent;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.settings.IndexSettings;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
-
-/**
- * Service for looking up configured {@link SimilarityProvider} implementations by name.
- * <p>
- * The service instantiates the Providers through their Factories using configuration
- * values found with the {@link SimilarityModule#SIMILARITY_SETTINGS_PREFIX} prefix.
- */
-public class SimilarityLookupService extends AbstractIndexComponent {
-
-    public final static String DEFAULT_SIMILARITY = "default";
-
-    private final Map<String, SimilarityProvider> similarities;
-
-    public SimilarityLookupService(Index index, Settings indexSettings) {
-        this(index, indexSettings, emptyMap());
-    }
-
-    @Inject
-    public SimilarityLookupService(Index index, @IndexSettings Settings indexSettings, Map<String, SimilarityProvider.Factory> similarities) {
-        super(index, indexSettings);
-
-        Map<String, SimilarityProvider> providers = new HashMap<>();
-
-        Map<String, Settings> similaritySettings = indexSettings.getGroups(SimilarityModule.SIMILARITY_SETTINGS_PREFIX);
-        for (Map.Entry<String, SimilarityProvider.Factory> entry : similarities.entrySet()) {
-            String name = entry.getKey();
-            SimilarityProvider.Factory factory = entry.getValue();
-
-            Settings settings = similaritySettings.get(name);
-            if (settings == null) {
-                settings = Settings.Builder.EMPTY_SETTINGS;
-            }
-            providers.put(name, factory.create(name, settings));
-        }
-
-        // For testing
-        for (PreBuiltSimilarityProvider.Factory factory : Similarities.listFactories()) {
-            if (!providers.containsKey(factory.name())) {
-                providers.put(factory.name(), factory.get());
-            }
-        }
-
-        this.similarities = unmodifiableMap(providers);
-    }
-
-    /**
-     * Returns the {@link SimilarityProvider} with the given name
-     *
-     * @param name Name of the SimilarityProvider to find
-     * @return {@link SimilarityProvider} with the given name, or {@code null} if no Provider exists
-     */
-    public SimilarityProvider similarity(String name) {
-        return similarities.get(name);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityModule.java b/core/src/main/java/org/elasticsearch/index/similarity/SimilarityModule.java
index 6e03bcf..29312f2 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityModule.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/SimilarityModule.java
@@ -20,19 +20,18 @@
 package org.elasticsearch.index.similarity;
 
 import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Scopes;
-import org.elasticsearch.common.inject.assistedinject.FactoryProvider;
-import org.elasticsearch.common.inject.multibindings.MapBinder;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.Index;
 
 import java.util.HashMap;
 import java.util.Map;
+import java.util.function.BiFunction;
 
 /**
  * {@link SimilarityModule} is responsible gathering registered and configured {@link SimilarityProvider}
- * implementations and making them available through the {@link SimilarityLookupService} and {@link SimilarityService}.
+ * implementations and making them available through the {@link SimilarityService}.
  *
- * New {@link SimilarityProvider} implementations can be registered through {@link #addSimilarity(String, Class)}
+ * New {@link SimilarityProvider} implementations can be registered through {@link #addSimilarity(String, BiFunction)}
  * while existing Providers can be referenced through Settings under the {@link #SIMILARITY_SETTINGS_PREFIX} prefix
  * along with the "type" value.  For example, to reference the {@link BM25SimilarityProvider}, the configuration
  * <tt>"index.similarity.my_similarity.type : "BM25"</tt> can be used.
@@ -42,16 +41,12 @@ public class SimilarityModule extends AbstractModule {
     public static final String SIMILARITY_SETTINGS_PREFIX = "index.similarity";
 
     private final Settings settings;
-    private final Map<String, Class<? extends SimilarityProvider>> similarities = new HashMap<>();
+    private final Map<String, BiFunction<String, Settings, SimilarityProvider>> similarities = new HashMap<>();
+    private final Index index;
 
-    public SimilarityModule(Settings settings) {
+    public SimilarityModule(Index index, Settings settings) {
         this.settings = settings;
-        addSimilarity("default", DefaultSimilarityProvider.class);
-        addSimilarity("BM25", BM25SimilarityProvider.class);
-        addSimilarity("DFR", DFRSimilarityProvider.class);
-        addSimilarity("IB", IBSimilarityProvider.class);
-        addSimilarity("LMDirichlet", LMDirichletSimilarityProvider.class);
-        addSimilarity("LMJelinekMercer", LMJelinekMercerSimilarityProvider.class);
+        this.index = index;
     }
 
     /**
@@ -60,36 +55,16 @@ public class SimilarityModule extends AbstractModule {
      * @param name Name of the SimilarityProvider
      * @param similarity SimilarityProvider to register
      */
-    public void addSimilarity(String name, Class<? extends SimilarityProvider> similarity) {
+    public void addSimilarity(String name, BiFunction<String, Settings, SimilarityProvider> similarity) {
+        if (similarities.containsKey(name) || SimilarityService.BUILT_IN.containsKey(name)) {
+            throw new IllegalArgumentException("similarity for name: [" + name + " is already registered");
+        }
         similarities.put(name, similarity);
     }
 
     @Override
     protected void configure() {
-        MapBinder<String, SimilarityProvider.Factory> similarityBinder =
-            MapBinder.newMapBinder(binder(), String.class, SimilarityProvider.Factory.class);
-
-        Map<String, Settings> similaritySettings = settings.getGroups(SIMILARITY_SETTINGS_PREFIX);
-        for (Map.Entry<String, Settings> entry : similaritySettings.entrySet()) {
-            String name = entry.getKey();
-            Settings settings = entry.getValue();
-
-            String typeName = settings.get("type");
-            if (typeName == null) {
-                throw new IllegalArgumentException("Similarity [" + name + "] must have an associated type");
-            } else if (similarities.containsKey(typeName) == false) {
-                throw new IllegalArgumentException("Unknown Similarity type [" + typeName + "] for [" + name + "]");
-            }
-            similarityBinder.addBinding(entry.getKey()).toProvider(FactoryProvider.newFactory(SimilarityProvider.Factory.class, similarities.get(typeName))).in(Scopes.SINGLETON);
-        }
-
-        for (PreBuiltSimilarityProvider.Factory factory : Similarities.listFactories()) {
-            if (!similarities.containsKey(factory.name())) {
-                similarityBinder.addBinding(factory.name()).toInstance(factory);
-            }
-        }
-
-        bind(SimilarityLookupService.class).asEagerSingleton();
-        bind(SimilarityService.class).asEagerSingleton();
+        SimilarityService service = new SimilarityService(index, settings, new HashMap<>(similarities));
+        bind(SimilarityService.class).toInstance(service);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/SimilarityProvider.java
index 38f56af..6433181 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/SimilarityProvider.java
@@ -40,19 +40,4 @@ public interface SimilarityProvider {
      * @return Provided {@link Similarity}
      */
     Similarity get();
-
-    /**
-     * Factory for creating {@link SimilarityProvider} instances
-     */
-    public static interface Factory {
-
-        /**
-         * Creates a new {@link SimilarityProvider} instance
-         *
-         * @param name Name of the provider
-         * @param settings Settings to be used by the Provider
-         * @return {@link SimilarityProvider} instance created by the Factory
-         */
-        SimilarityProvider create(String name, Settings settings);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java b/core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java
index 98faa87..a77a2de 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java
@@ -25,55 +25,96 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.AbstractIndexComponent;
 import org.elasticsearch.index.Index;
-import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.settings.IndexSettings;
 
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.function.BiFunction;
+
 /**
  *
  */
 public class SimilarityService extends AbstractIndexComponent {
 
-    private final SimilarityLookupService similarityLookupService;
-    private final MapperService mapperService;
-
-    private final Similarity perFieldSimilarity;
-
+    public final static String DEFAULT_SIMILARITY = "default";
+    private final Similarity defaultSimilarity;
+    private final Similarity baseSimilarity;
+    private final Map<String, SimilarityProvider> similarities;
+    static final Map<String, BiFunction<String, Settings, SimilarityProvider>> DEFAULTS;
+    static final Map<String, BiFunction<String, Settings, SimilarityProvider>> BUILT_IN;
+    static {
+        Map<String, BiFunction<String, Settings, SimilarityProvider>> defaults = new HashMap<>();
+        Map<String, BiFunction<String, Settings, SimilarityProvider>> buildIn = new HashMap<>();
+        defaults.put("default", DefaultSimilarityProvider::new);
+        defaults.put("BM25", BM25SimilarityProvider::new);
+        buildIn.put("default", DefaultSimilarityProvider::new);
+        buildIn.put("BM25", BM25SimilarityProvider::new);
+        buildIn.put("DFR", DFRSimilarityProvider::new);
+        buildIn.put("IB", IBSimilarityProvider::new);
+        buildIn.put("LMDirichlet", LMDirichletSimilarityProvider::new);
+        buildIn.put("LMJelinekMercer", LMJelinekMercerSimilarityProvider::new);
+        DEFAULTS = Collections.unmodifiableMap(defaults);
+        BUILT_IN = Collections.unmodifiableMap(buildIn);
+    }
     public SimilarityService(Index index) {
         this(index, Settings.Builder.EMPTY_SETTINGS);
     }
 
     public SimilarityService(Index index, Settings settings) {
-        this(index, settings, new SimilarityLookupService(index, settings), null);
+        this(index, settings, Collections.EMPTY_MAP);
     }
 
     @Inject
-    public SimilarityService(Index index, @IndexSettings Settings indexSettings,
-                             final SimilarityLookupService similarityLookupService, final MapperService mapperService) {
+    public SimilarityService(Index index, @IndexSettings Settings indexSettings, Map<String, BiFunction<String, Settings, SimilarityProvider>> similarities) {
         super(index, indexSettings);
-        this.similarityLookupService = similarityLookupService;
-        this.mapperService = mapperService;
-
-        Similarity defaultSimilarity = similarityLookupService.similarity(SimilarityLookupService.DEFAULT_SIMILARITY).get();
+        Map<String, SimilarityProvider> providers = new HashMap<>(similarities.size());
+        Map<String, Settings> similaritySettings = indexSettings.getGroups(SimilarityModule.SIMILARITY_SETTINGS_PREFIX);
+        for (Map.Entry<String, Settings> entry : similaritySettings.entrySet()) {
+            String name = entry.getKey();
+            Settings settings = entry.getValue();
+            String typeName = settings.get("type");
+            if (typeName == null) {
+                throw new IllegalArgumentException("Similarity [" + name + "] must have an associated type");
+            } else if ((similarities.containsKey(typeName) || BUILT_IN.containsKey(typeName)) == false) {
+                throw new IllegalArgumentException("Unknown Similarity type [" + typeName + "] for [" + name + "]");
+            }
+            BiFunction<String, Settings, SimilarityProvider> factory = similarities.getOrDefault(typeName, BUILT_IN.get(typeName));
+            if (settings == null) {
+                settings = Settings.Builder.EMPTY_SETTINGS;
+            }
+            providers.put(name, factory.apply(name, settings));
+        }
+        addSimilarities(similaritySettings, providers, DEFAULTS);
+        this.similarities = providers;
+        defaultSimilarity = providers.get(SimilarityService.DEFAULT_SIMILARITY).get();
         // Expert users can configure the base type as being different to default, but out-of-box we use default.
-        Similarity baseSimilarity = (similarityLookupService.similarity("base") != null) ? similarityLookupService.similarity("base").get() :
-                defaultSimilarity;
-
-        this.perFieldSimilarity = (mapperService != null) ? new PerFieldSimilarity(defaultSimilarity, baseSimilarity, mapperService) :
+        baseSimilarity = (providers.get("base") != null) ? providers.get("base").get() :
                 defaultSimilarity;
     }
 
-    public Similarity similarity() {
-        return perFieldSimilarity;
+    public Similarity similarity(MapperService mapperService) {
+        // TODO we can maybe factor out MapperService here entirely by introducing an interface for the lookup?
+        return (mapperService != null) ? new PerFieldSimilarity(defaultSimilarity, baseSimilarity, mapperService) :
+                defaultSimilarity;
     }
 
-    public SimilarityLookupService similarityLookupService() {
-        return similarityLookupService;
+    private void addSimilarities(Map<String, Settings>  similaritySettings, Map<String, SimilarityProvider> providers, Map<String, BiFunction<String, Settings, SimilarityProvider>> similarities)  {
+        for (Map.Entry<String, BiFunction<String, Settings, SimilarityProvider>> entry : similarities.entrySet()) {
+            String name = entry.getKey();
+            BiFunction<String, Settings, SimilarityProvider> factory = entry.getValue();
+            Settings settings = similaritySettings.get(name);
+            if (settings == null) {
+                settings = Settings.Builder.EMPTY_SETTINGS;
+            }
+            providers.put(name, factory.apply(name, settings));
+        }
     }
 
-    public MapperService mapperService() {
-        return mapperService;
+    public SimilarityProvider getSimilarity(String name) {
+        return similarities.get(name);
     }
 
     static class PerFieldSimilarity extends PerFieldSimilarityWrapper {
diff --git a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
index d90a869..091985e 100644
--- a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
+++ b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
@@ -19,11 +19,7 @@
 
 package org.elasticsearch.index.snapshots.blobstore;
 
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexCommit;
-import org.apache.lucene.index.IndexFormatTooNewException;
-import org.apache.lucene.index.IndexFormatTooOldException;
-import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.index.*;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
@@ -52,11 +48,7 @@ import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.snapshots.IndexShardRepository;
-import org.elasticsearch.index.snapshots.IndexShardRestoreFailedException;
-import org.elasticsearch.index.snapshots.IndexShardSnapshotException;
-import org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException;
-import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;
+import org.elasticsearch.index.snapshots.*;
 import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot.FileInfo;
 import org.elasticsearch.index.store.Store;
 import org.elasticsearch.index.store.StoreFileMetaData;
@@ -72,15 +64,8 @@ import org.elasticsearch.repositories.blobstore.LegacyBlobStoreFormat;
 import java.io.FilterInputStream;
 import java.io.IOException;
 import java.io.InputStream;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
+import java.util.*;
+
 import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.testBlobPrefix;
 
 /**
@@ -827,7 +812,7 @@ public class BlobStoreIndexShardRepository extends AbstractComponent implements
                     snapshotMetaData.put(fileInfo.metadata().name(), fileInfo.metadata());
                     fileInfos.put(fileInfo.metadata().name(), fileInfo);
                 }
-                final Store.MetadataSnapshot sourceMetaData = new Store.MetadataSnapshot(unmodifiableMap(snapshotMetaData), emptyMap(), 0);
+                final Store.MetadataSnapshot sourceMetaData = new Store.MetadataSnapshot(snapshotMetaData, Collections.EMPTY_MAP, 0);
                 final Store.RecoveryDiff diff = sourceMetaData.recoveryDiff(recoveryTargetMetadata);
                 for (StoreFileMetaData md : diff.identical) {
                     FileInfo fileInfo = fileInfos.get(md.name());
diff --git a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
index 8f05572..ad3b9c9 100644
--- a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
+++ b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.snapshots.blobstore;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
@@ -37,8 +38,6 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * Contains information about all snapshot for the given shard in repository
  * <p>
@@ -78,15 +77,15 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
                 physicalFileList.add(newFiles.get(fileInfo.name()));
             }
         }
-        Map<String, List<FileInfo>> mapBuilder = new HashMap<>();
+        ImmutableMap.Builder<String, List<FileInfo>> mapBuilder = ImmutableMap.builder();
         for (Map.Entry<String, List<FileInfo>> entry : physicalFiles.entrySet()) {
             mapBuilder.put(entry.getKey(), Collections.unmodifiableList(new ArrayList<>(entry.getValue())));
         }
-        this.physicalFiles = unmodifiableMap(mapBuilder);
-        this.files = unmodifiableMap(newFiles);
+        this.physicalFiles = mapBuilder.build();
+        this.files = ImmutableMap.copyOf(newFiles);
     }
 
-    private BlobStoreIndexShardSnapshots(Map<String, FileInfo> files, List<SnapshotFiles> shardSnapshots) {
+    private BlobStoreIndexShardSnapshots(ImmutableMap<String, FileInfo> files, List<SnapshotFiles> shardSnapshots) {
         this.shardSnapshots = shardSnapshots;
         this.files = files;
         Map<String, List<FileInfo>> physicalFiles = new HashMap<>();
@@ -100,11 +99,11 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
                 physicalFileList.add(files.get(fileInfo.name()));
             }
         }
-        Map<String, List<FileInfo>> mapBuilder = new HashMap<>();
+        ImmutableMap.Builder<String, List<FileInfo>> mapBuilder = ImmutableMap.builder();
         for (Map.Entry<String, List<FileInfo>> entry : physicalFiles.entrySet()) {
             mapBuilder.put(entry.getKey(), Collections.unmodifiableList(new ArrayList<>(entry.getValue())));
         }
-        this.physicalFiles = unmodifiableMap(mapBuilder);
+        this.physicalFiles = mapBuilder.build();
     }
 
     private BlobStoreIndexShardSnapshots() {
@@ -233,14 +232,13 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
         return builder;
     }
 
-    @Override
     public BlobStoreIndexShardSnapshots fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
         XContentParser.Token token = parser.currentToken();
         if (token == null) { // New parser
             token = parser.nextToken();
         }
         Map<String, List<String>> snapshotsMap = new HashMap<>();
-        Map<String, FileInfo> files = new HashMap<>();
+        ImmutableMap.Builder<String, FileInfo> filesBuilder = ImmutableMap.builder();
         if (token == XContentParser.Token.START_OBJECT) {
             while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                 if (token != XContentParser.Token.FIELD_NAME) {
@@ -254,7 +252,7 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
                     }
                     while (parser.nextToken() != XContentParser.Token.END_ARRAY) {
                         FileInfo fileInfo = FileInfo.fromXContent(parser);
-                        files.put(fileInfo.name(), fileInfo);
+                        filesBuilder.put(fileInfo.name(), fileInfo);
                     }
                 } else if (token == XContentParser.Token.START_OBJECT) {
                     if (parseFieldMatcher.match(currentFieldName, ParseFields.SNAPSHOTS) == false) {
@@ -290,6 +288,7 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
             }
         }
 
+        ImmutableMap<String, FileInfo> files = filesBuilder.build();
         List<SnapshotFiles> snapshots = new ArrayList<>();
         for (Map.Entry<String, List<String>> entry : snapshotsMap.entrySet()) {
             List<FileInfo> fileInfosBuilder = new ArrayList<>();
diff --git a/core/src/main/java/org/elasticsearch/index/store/Store.java b/core/src/main/java/org/elasticsearch/index/store/Store.java
index c2b55ac..7fb1b40 100644
--- a/core/src/main/java/org/elasticsearch/index/store/Store.java
+++ b/core/src/main/java/org/elasticsearch/index/store/Store.java
@@ -19,34 +19,14 @@
 
 package org.elasticsearch.index.store;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexCommit;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexFormatTooNewException;
-import org.apache.lucene.index.IndexFormatTooOldException;
-import org.apache.lucene.index.IndexNotFoundException;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.SegmentCommitInfo;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.BufferedChecksum;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FilterDirectory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.Lock;
-import org.apache.lucene.store.SimpleFSDirectory;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.Version;
+import org.apache.lucene.index.*;
+import org.apache.lucene.store.*;
+import org.apache.lucene.util.*;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
+import org.apache.lucene.util.Version;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.collect.Tuple;
@@ -64,39 +44,26 @@ import org.elasticsearch.common.lucene.store.InputStreamIndexInput;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.Callback;
+import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.common.util.SingleObjectCache;
 import org.elasticsearch.common.util.concurrent.AbstractRefCounted;
 import org.elasticsearch.common.util.concurrent.RefCounted;
-import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.env.ShardLock;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.shard.AbstractIndexShardComponent;
 import org.elasticsearch.index.shard.ShardId;
 
-import java.io.Closeable;
-import java.io.EOFException;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.io.InputStream;
+import java.io.*;
 import java.nio.file.NoSuchFileException;
 import java.nio.file.Path;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 import java.util.zip.Adler32;
 import java.util.zip.CRC32;
 import java.util.zip.Checksum;
 
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * A Store provides plain access to files written by an elasticsearch index shard. Each shard
  * has a dedicated store that is uses to access Lucene's Directory which represents the lowest level
@@ -767,7 +734,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
         private static final ESLogger logger = Loggers.getLogger(MetadataSnapshot.class);
         private static final Version FIRST_LUCENE_CHECKSUM_VERSION = Version.LUCENE_4_8;
 
-        private final Map<String, StoreFileMetaData> metadata;
+        private final ImmutableMap<String, StoreFileMetaData> metadata;
 
         public static final MetadataSnapshot EMPTY = new MetadataSnapshot();
 
@@ -776,14 +743,16 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
         private final long numDocs;
 
         public MetadataSnapshot(Map<String, StoreFileMetaData> metadata, Map<String, String> commitUserData, long numDocs) {
-            this.metadata = metadata;
-            this.commitUserData = commitUserData;
+            ImmutableMap.Builder<String, StoreFileMetaData> metaDataBuilder = ImmutableMap.builder();
+            this.metadata = metaDataBuilder.putAll(metadata).build();
+            ImmutableMap.Builder<String, String> commitUserDataBuilder = ImmutableMap.builder();
+            this.commitUserData = commitUserDataBuilder.putAll(commitUserData).build();
             this.numDocs = numDocs;
         }
 
         MetadataSnapshot() {
-            metadata = emptyMap();
-            commitUserData = emptyMap();
+            metadata = ImmutableMap.of();
+            commitUserData = ImmutableMap.of();
             numDocs = 0;
         }
 
@@ -797,19 +766,19 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
 
         public MetadataSnapshot(StreamInput in) throws IOException {
             final int size = in.readVInt();
-            Map<String, StoreFileMetaData> metadata = new HashMap<>();
+            final ImmutableMap.Builder<String, StoreFileMetaData> metadataBuilder = ImmutableMap.builder();
             for (int i = 0; i < size; i++) {
                 StoreFileMetaData meta = StoreFileMetaData.readStoreFileMetaData(in);
-                metadata.put(meta.name(), meta);
+                metadataBuilder.put(meta.name(), meta);
             }
-            Map<String, String> commitUserData = new HashMap<>();
+            final ImmutableMap.Builder<String, String> commitUserDataBuilder = ImmutableMap.builder();
             int num = in.readVInt();
             for (int i = num; i > 0; i--) {
-                commitUserData.put(in.readString(), in.readString());
+                commitUserDataBuilder.put(in.readString(), in.readString());
             }
 
-            this.metadata = unmodifiableMap(metadata);
-            this.commitUserData = unmodifiableMap(commitUserData);
+            this.commitUserData = commitUserDataBuilder.build();
+            this.metadata = metadataBuilder.build();
             this.numDocs = in.readLong();
             assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles();
         }
@@ -822,11 +791,11 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
         }
 
         static class LoadedMetadata {
-            final Map<String, StoreFileMetaData> fileMetadata;
-            final Map<String, String> userData;
+            final ImmutableMap<String, StoreFileMetaData> fileMetadata;
+            final ImmutableMap<String, String> userData;
             final long numDocs;
 
-            LoadedMetadata(Map<String, StoreFileMetaData> fileMetadata, Map<String, String> userData, long numDocs) {
+            LoadedMetadata(ImmutableMap<String, StoreFileMetaData> fileMetadata, ImmutableMap<String, String> userData, long numDocs) {
                 this.fileMetadata = fileMetadata;
                 this.userData = userData;
                 this.numDocs = numDocs;
@@ -835,9 +804,9 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
 
         static LoadedMetadata loadMetadata(IndexCommit commit, Directory directory, ESLogger logger) throws IOException {
             long numDocs;
-            Map<String, StoreFileMetaData> builder = new HashMap<>();
+            ImmutableMap.Builder<String, StoreFileMetaData> builder = ImmutableMap.builder();
             Map<String, String> checksumMap = readLegacyChecksums(directory).v1();
-            Map<String, String> commitUserDataBuilder = new HashMap<>();
+            ImmutableMap.Builder<String, String> commitUserDataBuilder = ImmutableMap.builder();
             try {
                 final SegmentInfos segmentCommitInfos = Store.readSegmentsInfo(commit, directory);
                 numDocs = Lucene.getNumDocs(segmentCommitInfos);
@@ -894,7 +863,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
 
                 throw ex;
             }
-            return new LoadedMetadata(unmodifiableMap(builder), unmodifiableMap(commitUserDataBuilder), numDocs);
+            return new LoadedMetadata(builder.build(), commitUserDataBuilder.build(), numDocs);
         }
 
         /**
@@ -951,8 +920,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
             }
         }
 
-        private static void checksumFromLuceneFile(Directory directory, String file, Map<String, StoreFileMetaData> builder,
-                ESLogger logger, Version version, boolean readFileAsHash) throws IOException {
+        private static void checksumFromLuceneFile(Directory directory, String file, ImmutableMap.Builder<String, StoreFileMetaData> builder, ESLogger logger, Version version, boolean readFileAsHash) throws IOException {
             final String checksum;
             final BytesRefBuilder fileHash = new BytesRefBuilder();
             try (final IndexInput in = directory.openInput(file, IOContext.READONCE)) {
diff --git a/core/src/main/java/org/elasticsearch/index/translog/Translog.java b/core/src/main/java/org/elasticsearch/index/translog/Translog.java
index 5084895..4265d61 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/Translog.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/Translog.java
@@ -23,11 +23,9 @@ import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TwoPhaseCommit;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.CollectionUtil;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.RamUsageEstimator;
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
@@ -38,7 +36,6 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
 import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.lease.Releasables;
-import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.lucene.uid.Versions;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.util.BigArrays;
@@ -54,7 +51,6 @@ import org.elasticsearch.threadpool.ThreadPool;
 
 import java.io.Closeable;
 import java.io.EOFException;
-import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.nio.channels.FileChannel;
 import java.nio.file.*;
@@ -189,99 +185,6 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
         }
     }
 
-    /**
-     * This method is used to upgarde a pre 2.0 translog structure to the new checkpoint based structure.
-     * The {@link org.elasticsearch.index.translog.Translog.TranslogGeneration} in the given config is
-     * used to determine the smallest file generation to upgrade. The procedure will travers the translog
-     * directory to find all files that have a generation greater or equal to the translog generation and
-     * renames the files to the new <tt>.tlog</tt> file format.
-     * <p>
-     * For each of the files a <tt>${filename}.ckp</tt>
-     * file is written containing the size of the translog in bytes, it's ID and the number of operations. Since
-     * these files are all relying on the pre 2.0 truncation feature where we read operations until hitting an {@link EOFException}
-     * the number of operations are recoreded as <tt>-1</tt>. Later once these files are opened for reading legacy readers will
-     * allow for unknown number of operations and mimic the old behavior.
-     * </p>
-     */
-    public static void upgradeLegacyTranslog(ESLogger logger, TranslogConfig config) throws IOException {
-        Path translogPath = config.getTranslogPath();
-        TranslogGeneration translogGeneration = config.getTranslogGeneration();
-        if (translogGeneration == null) {
-            throw new IllegalArgumentException("TranslogGeneration must be set in order to upgrade");
-        }
-        if (translogGeneration.translogUUID != null) {
-            throw new IllegalArgumentException("TranslogGeneration has a non-null UUID - index must have already been upgraded");
-        }
-        try {
-            if (Checkpoint.read(translogPath.resolve(CHECKPOINT_FILE_NAME)) != null) {
-                throw new IllegalStateException(CHECKPOINT_FILE_NAME + " file already present, translog is already upgraded");
-            }
-        } catch (NoSuchFileException | FileNotFoundException ex) {
-            logger.debug("upgrading translog - no checkpoint found");
-        }
-        final Pattern parseLegacyIdPattern = Pattern.compile("^" + TRANSLOG_FILE_PREFIX + "(\\d+)((\\.recovering))?$"); // here we have to be lenient - nowhere else!
-        try (DirectoryStream<Path> stream = Files.newDirectoryStream(translogPath, new DirectoryStream.Filter<Path>() {
-            @Override
-            public boolean accept(Path entry) throws IOException {
-                Matcher matcher = parseLegacyIdPattern.matcher(entry.getFileName().toString());
-                if (matcher.matches() == false) {
-                    Matcher newIdMatcher = PARSE_STRICT_ID_PATTERN.matcher(entry.getFileName().toString());
-                    return newIdMatcher.matches();
-                } else {
-                    return true;
-                }
-            }
-        })) {
-            long latestGeneration = -1;
-            List<PathWithGeneration> filesToUpgrade = new ArrayList<>();
-            for (Path path : stream) {
-                Matcher matcher = parseLegacyIdPattern.matcher(path.getFileName().toString());
-                if (matcher.matches()) {
-                    long generation = Long.parseLong(matcher.group(1));
-                    if (generation >= translogGeneration.translogFileGeneration) {
-                        latestGeneration = Math.max(translogGeneration.translogFileGeneration, generation);
-                    }
-                    filesToUpgrade.add(new PathWithGeneration(path, generation));
-                } else {
-                    Matcher strict_matcher = PARSE_STRICT_ID_PATTERN.matcher(path.getFileName().toString());
-                    if (strict_matcher.matches()) {
-                        throw new IllegalStateException("non-legacy translog file [" + path.getFileName().toString() + "] found on a translog that wasn't upgraded yet");
-                    }
-                }
-            }
-            if (latestGeneration < translogGeneration.translogFileGeneration) {
-                throw new IllegalStateException("latest found translog has a lower generation that the excepcted uncommitted " + translogGeneration.translogFileGeneration + " > " + latestGeneration);
-            }
-            CollectionUtil.timSort(filesToUpgrade, new Comparator<PathWithGeneration>() {
-                @Override
-                public int compare(PathWithGeneration o1, PathWithGeneration o2) {
-                    long gen1 = o1.getGeneration();
-                    long gen2 = o2.getGeneration();
-                    return Long.compare(gen1, gen2);
-                }
-            });
-            for (PathWithGeneration pathAndGeneration : filesToUpgrade) {
-                final Path path = pathAndGeneration.getPath();
-                final long generation = pathAndGeneration.getGeneration();
-                final Path target = path.resolveSibling(getFilename(generation));
-                logger.debug("upgrading translog copy file from {} to {}", path, target);
-                Files.move(path, target, StandardCopyOption.ATOMIC_MOVE);
-                logger.debug("write commit point for {}", target);
-                if (generation == latestGeneration) {
-                    // for the last one we only write a checkpoint not a real commit
-                    Checkpoint checkpoint = new Checkpoint(Files.size(translogPath.resolve(getFilename(latestGeneration))), -1, latestGeneration);
-                    Checkpoint.write(translogPath.resolve(CHECKPOINT_FILE_NAME), checkpoint, StandardOpenOption.WRITE, StandardOpenOption.CREATE_NEW);
-                } else {
-                    Checkpoint checkpoint = new Checkpoint(Files.size(target), -1, generation);
-                    Checkpoint.write(translogPath.resolve(getCommitCheckpointFileName(generation)), checkpoint, StandardOpenOption.WRITE, StandardOpenOption.CREATE_NEW);
-                }
-            }
-
-            IOUtils.fsync(translogPath, true);
-
-        }
-    }
-
     /** recover all translog files found on disk */
     private ArrayList<ImmutableTranslogReader> recoverFromFiles(TranslogGeneration translogGeneration, Checkpoint checkpoint) throws IOException {
         boolean success = false;
@@ -465,11 +368,10 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
     }
 
     /**
-     * Adds a created / delete / index operations to the transaction log.
+     * Adds a delete / index operations to the transaction log.
      *
      * @see org.elasticsearch.index.translog.Translog.Operation
-     * @see org.elasticsearch.index.translog.Translog.Create
-     * @see org.elasticsearch.index.translog.Translog.Index
+     * @see Index
      * @see org.elasticsearch.index.translog.Translog.Delete
      */
     public Location add(Operation operation) throws TranslogException {
@@ -874,10 +776,10 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
      */
     public interface Operation extends Streamable {
         enum Type {
+            @Deprecated
             CREATE((byte) 1),
-            SAVE((byte) 2),
-            DELETE((byte) 3),
-            DELETE_BY_QUERY((byte) 4);
+            INDEX((byte) 2),
+            DELETE((byte) 3);
 
             private final byte id;
 
@@ -894,11 +796,9 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
                     case 1:
                         return CREATE;
                     case 2:
-                        return SAVE;
+                        return INDEX;
                     case 3:
                         return DELETE;
-                    case 4:
-                        return DELETE_BY_QUERY;
                     default:
                         throw new IllegalArgumentException("No type mapped for [" + id + "]");
                 }
@@ -929,199 +829,6 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
         }
     }
 
-    public static class Create implements Operation {
-        public static final int SERIALIZATION_FORMAT = 6;
-
-        private String id;
-        private String type;
-        private BytesReference source;
-        private String routing;
-        private String parent;
-        private long timestamp;
-        private long ttl;
-        private long version = Versions.MATCH_ANY;
-        private VersionType versionType = VersionType.INTERNAL;
-
-        public Create() {
-        }
-
-        public Create(Engine.Create create) {
-            this.id = create.id();
-            this.type = create.type();
-            this.source = create.source();
-            this.routing = create.routing();
-            this.parent = create.parent();
-            this.timestamp = create.timestamp();
-            this.ttl = create.ttl();
-            this.version = create.version();
-            this.versionType = create.versionType();
-        }
-
-        public Create(String type, String id, byte[] source) {
-            this.id = id;
-            this.type = type;
-            this.source = new BytesArray(source);
-        }
-
-        @Override
-        public Type opType() {
-            return Type.CREATE;
-        }
-
-        @Override
-        public long estimateSize() {
-            return ((id.length() + type.length()) * 2) + source.length() + 12;
-        }
-
-        public String id() {
-            return this.id;
-        }
-
-        public BytesReference source() {
-            return this.source;
-        }
-
-        public String type() {
-            return this.type;
-        }
-
-        public String routing() {
-            return this.routing;
-        }
-
-        public String parent() {
-            return this.parent;
-        }
-
-        public long timestamp() {
-            return this.timestamp;
-        }
-
-        public long ttl() {
-            return this.ttl;
-        }
-
-        public long version() {
-            return this.version;
-        }
-
-        public VersionType versionType() {
-            return versionType;
-        }
-
-        @Override
-        public Source getSource() {
-            return new Source(source, routing, parent, timestamp, ttl);
-        }
-
-        @Override
-        public void readFrom(StreamInput in) throws IOException {
-            int version = in.readVInt(); // version
-            id = in.readString();
-            type = in.readString();
-            source = in.readBytesReference();
-            if (version >= 1) {
-                if (in.readBoolean()) {
-                    routing = in.readString();
-                }
-            }
-            if (version >= 2) {
-                if (in.readBoolean()) {
-                    parent = in.readString();
-                }
-            }
-            if (version >= 3) {
-                this.version = in.readLong();
-            }
-            if (version >= 4) {
-                this.timestamp = in.readLong();
-            }
-            if (version >= 5) {
-                this.ttl = in.readLong();
-            }
-            if (version >= 6) {
-                this.versionType = VersionType.fromValue(in.readByte());
-            }
-
-            assert versionType.validateVersionForWrites(version);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVInt(SERIALIZATION_FORMAT);
-            out.writeString(id);
-            out.writeString(type);
-            out.writeBytesReference(source);
-            if (routing == null) {
-                out.writeBoolean(false);
-            } else {
-                out.writeBoolean(true);
-                out.writeString(routing);
-            }
-            if (parent == null) {
-                out.writeBoolean(false);
-            } else {
-                out.writeBoolean(true);
-                out.writeString(parent);
-            }
-            out.writeLong(version);
-            out.writeLong(timestamp);
-            out.writeLong(ttl);
-            out.writeByte(versionType.getValue());
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) {
-                return true;
-            }
-            if (o == null || getClass() != o.getClass()) {
-                return false;
-            }
-
-            Create create = (Create) o;
-
-            if (timestamp != create.timestamp ||
-                    ttl != create.ttl ||
-                    version != create.version ||
-                    id.equals(create.id) == false ||
-                    type.equals(create.type) == false ||
-                    source.equals(create.source) == false) {
-                return false;
-            }
-            if (routing != null ? !routing.equals(create.routing) : create.routing != null) {
-                return false;
-            }
-            if (parent != null ? !parent.equals(create.parent) : create.parent != null) {
-                return false;
-            }
-            return versionType == create.versionType;
-
-        }
-
-        @Override
-        public int hashCode() {
-            int result = id.hashCode();
-            result = 31 * result + type.hashCode();
-            result = 31 * result + source.hashCode();
-            result = 31 * result + (routing != null ? routing.hashCode() : 0);
-            result = 31 * result + (parent != null ? parent.hashCode() : 0);
-            result = 31 * result + (int) (timestamp ^ (timestamp >>> 32));
-            result = 31 * result + (int) (ttl ^ (ttl >>> 32));
-            result = 31 * result + (int) (version ^ (version >>> 32));
-            result = 31 * result + versionType.hashCode();
-            return result;
-        }
-
-        @Override
-        public String toString() {
-            return "Create{" +
-                    "id='" + id + '\'' +
-                    ", type='" + type + '\'' +
-                    '}';
-        }
-    }
-
     public static class Index implements Operation {
         public static final int SERIALIZATION_FORMAT = 6;
 
@@ -1158,7 +865,7 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
 
         @Override
         public Type opType() {
-            return Type.SAVE;
+            return Type.INDEX;
         }
 
         @Override
@@ -1425,137 +1132,6 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
         }
     }
 
-    /** @deprecated Delete-by-query is removed in 2.0, but we keep this so translog can replay on upgrade. */
-    @Deprecated
-    public static class DeleteByQuery implements Operation {
-
-        public static final int SERIALIZATION_FORMAT = 2;
-        private BytesReference source;
-        @Nullable
-        private String[] filteringAliases;
-        private String[] types = Strings.EMPTY_ARRAY;
-
-        public DeleteByQuery() {
-        }
-
-        public DeleteByQuery(Engine.DeleteByQuery deleteByQuery) {
-            this(deleteByQuery.source(), deleteByQuery.filteringAliases(), deleteByQuery.types());
-        }
-
-        public DeleteByQuery(BytesReference source, String[] filteringAliases, String... types) {
-            this.source = source;
-            this.types = types == null ? Strings.EMPTY_ARRAY : types;
-            this.filteringAliases = filteringAliases;
-        }
-
-        @Override
-        public Type opType() {
-            return Type.DELETE_BY_QUERY;
-        }
-
-        @Override
-        public long estimateSize() {
-            return source.length() + 8;
-        }
-
-        public BytesReference source() {
-            return this.source;
-        }
-
-        public String[] filteringAliases() {
-            return filteringAliases;
-        }
-
-        public String[] types() {
-            return this.types;
-        }
-
-        @Override
-        public Source getSource() {
-            throw new IllegalStateException("trying to read doc source from delete_by_query operation");
-        }
-
-        @Override
-        public void readFrom(StreamInput in) throws IOException {
-            int version = in.readVInt(); // version
-            source = in.readBytesReference();
-            if (version < 2) {
-                // for query_parser_name, which was removed
-                if (in.readBoolean()) {
-                    in.readString();
-                }
-            }
-            int typesSize = in.readVInt();
-            if (typesSize > 0) {
-                types = new String[typesSize];
-                for (int i = 0; i < typesSize; i++) {
-                    types[i] = in.readString();
-                }
-            }
-            if (version >= 1) {
-                int aliasesSize = in.readVInt();
-                if (aliasesSize > 0) {
-                    filteringAliases = new String[aliasesSize];
-                    for (int i = 0; i < aliasesSize; i++) {
-                        filteringAliases[i] = in.readString();
-                    }
-                }
-            }
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVInt(SERIALIZATION_FORMAT);
-            out.writeBytesReference(source);
-            out.writeVInt(types.length);
-            for (String type : types) {
-                out.writeString(type);
-            }
-            if (filteringAliases != null) {
-                out.writeVInt(filteringAliases.length);
-                for (String alias : filteringAliases) {
-                    out.writeString(alias);
-                }
-            } else {
-                out.writeVInt(0);
-            }
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) {
-                return true;
-            }
-            if (o == null || getClass() != o.getClass()) {
-                return false;
-            }
-
-            DeleteByQuery that = (DeleteByQuery) o;
-
-            if (!Arrays.equals(filteringAliases, that.filteringAliases)) {
-                return false;
-            }
-            if (!Arrays.equals(types, that.types)) {
-                return false;
-            }
-            return source.equals(that.source);
-        }
-
-        @Override
-        public int hashCode() {
-            int result = source.hashCode();
-            result = 31 * result + (filteringAliases != null ? Arrays.hashCode(filteringAliases) : 0);
-            result = 31 * result + Arrays.hashCode(types);
-            return result;
-        }
-
-        @Override
-        public String toString() {
-            return "DeleteByQuery{" +
-                    "types=" + Arrays.toString(types) +
-                    '}';
-        }
-    }
 
     public enum Durabilty {
         /**
@@ -1667,13 +1243,12 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
     static Translog.Operation newOperationFromType(Translog.Operation.Type type) throws IOException {
         switch (type) {
             case CREATE:
-                return new Translog.Create();
+                // the deserialization logic in Index was identical to that of Create when create was deprecated
+                return new Index();
             case DELETE:
                 return new Translog.Delete();
-            case DELETE_BY_QUERY:
-                return new Translog.DeleteByQuery();
-            case SAVE:
-                return new Translog.Index();
+            case INDEX:
+                return new Index();
             default:
                 throw new IOException("No type for [" + type + "]");
         }
@@ -1781,10 +1356,6 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
         return current.getFirstOperationOffset();
     }
 
-    List<ImmutableTranslogReader> getRecoveredReaders() { // for testing
-        return this.recoveredTranslogs;
-    }
-
     private void ensureOpen() {
         if (closed.get()) {
             throw new AlreadyClosedException("translog is already closed");
@@ -1798,21 +1369,4 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
         return outstandingViews.size();
     }
 
-    private static class PathWithGeneration {
-        private final Path path;
-        private final long generation;
-
-        public PathWithGeneration(Path path, long generation) {
-            this.path = path;
-            this.generation = generation;
-        }
-
-        public Path getPath() {
-            return path;
-        }
-
-        public long getGeneration() {
-            return generation;
-        }
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/translog/TranslogConfig.java b/core/src/main/java/org/elasticsearch/index/translog/TranslogConfig.java
index 4d74961..30ab814 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/TranslogConfig.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/TranslogConfig.java
@@ -27,6 +27,7 @@ import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.index.translog.Translog.TranslogGeneration;
+import org.elasticsearch.indices.memory.IndexingMemoryController;
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.nio.file.Path;
@@ -42,7 +43,6 @@ public final class TranslogConfig {
     public static final String INDEX_TRANSLOG_FS_TYPE = "index.translog.fs.type";
     public static final String INDEX_TRANSLOG_BUFFER_SIZE = "index.translog.fs.buffer_size";
     public static final String INDEX_TRANSLOG_SYNC_INTERVAL = "index.translog.sync_interval";
-    public static final ByteSizeValue INACTIVE_SHARD_TRANSLOG_BUFFER = ByteSizeValue.parseBytesSizeValue("1kb", "INACTIVE_SHARD_TRANSLOG_BUFFER");
 
     private final TimeValue syncInterval;
     private final BigArrays bigArrays;
@@ -73,7 +73,7 @@ public final class TranslogConfig {
         this.threadPool = threadPool;
         this.bigArrays = bigArrays;
         this.type = TranslogWriter.Type.fromString(indexSettings.get(INDEX_TRANSLOG_FS_TYPE, TranslogWriter.Type.BUFFERED.name()));
-        this.bufferSize = (int) indexSettings.getAsBytesSize(INDEX_TRANSLOG_BUFFER_SIZE, ByteSizeValue.parseBytesSizeValue("64k", INDEX_TRANSLOG_BUFFER_SIZE)).bytes(); // Not really interesting, updated by IndexingMemoryController...
+        this.bufferSize = (int) indexSettings.getAsBytesSize(INDEX_TRANSLOG_BUFFER_SIZE, IndexingMemoryController.INACTIVE_SHARD_TRANSLOG_BUFFER).bytes(); // Not really interesting, updated by IndexingMemoryController...
 
         syncInterval = indexSettings.getAsTime(INDEX_TRANSLOG_SYNC_INTERVAL, TimeValue.timeValueSeconds(5));
         if (syncInterval.millis() > 0 && threadPool != null) {
diff --git a/core/src/main/java/org/elasticsearch/index/translog/TranslogStats.java b/core/src/main/java/org/elasticsearch/index/translog/TranslogStats.java
index 1af0a74..a4431b5 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/TranslogStats.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/TranslogStats.java
@@ -18,11 +18,10 @@
  */
 package org.elasticsearch.index.translog;
 
+import org.elasticsearch.action.support.ToXContentToBytes;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 
@@ -31,17 +30,23 @@ import java.io.IOException;
 /**
  *
  */
-public class TranslogStats implements ToXContent, Streamable {
+public class TranslogStats extends ToXContentToBytes implements Streamable {
 
-    private long translogSizeInBytes = 0;
-    private int estimatedNumberOfOperations = -1;
+    private long translogSizeInBytes;
+    private int numberOfOperations;
 
     public TranslogStats() {
     }
 
-    public TranslogStats(int estimatedNumberOfOperations, long translogSizeInBytes) {
+    public TranslogStats(int numberOfOperations, long translogSizeInBytes) {
+        if (numberOfOperations < 0) {
+            throw new IllegalArgumentException("numberOfOperations must be >= 0");
+        }
+        if (translogSizeInBytes < 0) {
+            throw new IllegalArgumentException("translogSizeInBytes must be >= 0");
+        }
         assert translogSizeInBytes >= 0 : "translogSizeInBytes must be >= 0, got [" + translogSizeInBytes + "]";
-        this.estimatedNumberOfOperations = estimatedNumberOfOperations;
+        this.numberOfOperations = numberOfOperations;
         this.translogSizeInBytes = translogSizeInBytes;
     }
 
@@ -50,22 +55,22 @@ public class TranslogStats implements ToXContent, Streamable {
             return;
         }
 
-        this.estimatedNumberOfOperations += translogStats.estimatedNumberOfOperations;
-        this.translogSizeInBytes = +translogStats.translogSizeInBytes;
+        this.numberOfOperations += translogStats.numberOfOperations;
+        this.translogSizeInBytes += translogStats.translogSizeInBytes;
     }
 
-    public ByteSizeValue translogSizeInBytes() {
-        return new ByteSizeValue(translogSizeInBytes);
+    public long getTranslogSizeInBytes() {
+        return translogSizeInBytes;
     }
 
     public long estimatedNumberOfOperations() {
-        return estimatedNumberOfOperations;
+        return numberOfOperations;
     }
 
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject(Fields.TRANSLOG);
-        builder.field(Fields.OPERATIONS, estimatedNumberOfOperations);
+        builder.field(Fields.OPERATIONS, numberOfOperations);
         builder.byteSizeField(Fields.SIZE_IN_BYTES, Fields.SIZE, translogSizeInBytes);
         builder.endObject();
         return builder;
@@ -80,13 +85,13 @@ public class TranslogStats implements ToXContent, Streamable {
 
     @Override
     public void readFrom(StreamInput in) throws IOException {
-        estimatedNumberOfOperations = in.readVInt();
+        numberOfOperations = in.readVInt();
         translogSizeInBytes = in.readVLong();
     }
 
     @Override
     public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(estimatedNumberOfOperations);
+        out.writeVInt(numberOfOperations);
         out.writeVLong(translogSizeInBytes);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/indices/IndicesService.java b/core/src/main/java/org/elasticsearch/indices/IndicesService.java
index ba160fc..8601d76 100644
--- a/core/src/main/java/org/elasticsearch/indices/IndicesService.java
+++ b/core/src/main/java/org/elasticsearch/indices/IndicesService.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.indices;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.util.CollectionUtil;
 import org.apache.lucene.util.IOUtils;
@@ -50,7 +52,6 @@ import org.elasticsearch.index.IndexModule;
 import org.elasticsearch.index.IndexNameModule;
 import org.elasticsearch.index.IndexNotFoundException;
 import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.LocalNodeIdModule;
 import org.elasticsearch.index.analysis.AnalysisModule;
 import org.elasticsearch.index.analysis.AnalysisService;
 import org.elasticsearch.index.cache.IndexCache;
@@ -93,8 +94,6 @@ import java.util.concurrent.Executors;
 import java.util.concurrent.TimeUnit;
 import java.util.stream.Stream;
 
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.common.collect.MapBuilder.newMapBuilder;
@@ -118,8 +117,8 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
     private final NodeEnvironment nodeEnv;
     private final TimeValue shardsClosedTimeout;
 
-    private volatile Map<String, IndexServiceInjectorPair> indices = emptyMap();
-
+    private volatile Map<String, IndexServiceInjectorPair> indices = ImmutableMap.of();
+    
     static class IndexServiceInjectorPair {
         private final IndexService indexService;
         private final Injector injector;
@@ -137,7 +136,7 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
             return injector;
         }
     }
-
+    
     private final Map<Index, List<PendingDelete>> pendingDeletes = new HashMap<>();
 
     private final OldShardsStats oldShardsStats = new OldShardsStats();
@@ -330,7 +329,6 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
 
         ModulesBuilder modules = new ModulesBuilder();
         modules.add(new IndexNameModule(index));
-        modules.add(new LocalNodeIdModule(localNodeId));
         modules.add(new IndexSettingsModule(index, indexSettings));
         // plugin modules must be added here, before others or we can get crazy injection errors...
         for (Module pluginModule : pluginsService.indexModules(indexSettings)) {
@@ -338,9 +336,10 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
         }
         modules.add(new IndexStoreModule(indexSettings));
         modules.add(new AnalysisModule(indexSettings, indicesAnalysisService));
-        modules.add(new SimilarityModule(indexSettings));
+        modules.add(new SimilarityModule(index, indexSettings));
         modules.add(new IndexCacheModule(indexSettings));
         modules.add(new IndexModule());
+        
         pluginsService.processModules(modules);
 
         Injector indexInjector;
@@ -381,11 +380,11 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
                 }
 
                 logger.debug("[{}] closing ... (reason [{}])", index, reason);
-                Map<String, IndexServiceInjectorPair> newIndices = new HashMap<>(indices);
-                IndexServiceInjectorPair remove = newIndices.remove(index);
+                Map<String, IndexServiceInjectorPair> tmpMap = new HashMap<>(indices);
+                IndexServiceInjectorPair remove = tmpMap.remove(index);
                 indexService = remove.getIndexService();
                 indexInjector = remove.getInjector();
-                indices = unmodifiableMap(newIndices);
+                indices = ImmutableMap.copyOf(tmpMap);
             }
 
             indicesLifecycle.beforeIndexClosed(indexService);
diff --git a/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java b/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
index 4ab4691..5fb70b6 100644
--- a/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
+++ b/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
@@ -21,12 +21,6 @@ package org.elasticsearch.indices.cache.request;
 
 import com.carrotsearch.hppc.ObjectHashSet;
 import com.carrotsearch.hppc.ObjectSet;
-import com.google.common.cache.Cache;
-import com.google.common.cache.CacheBuilder;
-import com.google.common.cache.RemovalListener;
-import com.google.common.cache.RemovalNotification;
-import com.google.common.cache.Weigher;
-
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.util.Accountable;
@@ -35,6 +29,7 @@ import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.cache.*;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
@@ -51,14 +46,11 @@ import org.elasticsearch.search.query.QueryPhase;
 import org.elasticsearch.search.query.QuerySearchResult;
 import org.elasticsearch.threadpool.ThreadPool;
 
-import java.util.Collection;
-import java.util.Collections;
-import java.util.EnumSet;
-import java.util.Iterator;
-import java.util.Set;
-import java.util.concurrent.Callable;
+import java.io.IOException;
+import java.util.*;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.TimeUnit;
+import java.util.function.Function;
 
 import static org.elasticsearch.common.Strings.hasLength;
 
@@ -162,25 +154,17 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
     private void buildCache() {
         long sizeInBytes = MemorySizeValue.parseBytesSizeValueOrHeapRatio(size, INDICES_CACHE_QUERY_SIZE).bytes();
 
-        CacheBuilder<Key, Value> cacheBuilder = CacheBuilder.newBuilder()
-                .maximumWeight(sizeInBytes).weigher(new QueryCacheWeigher()).removalListener(this);
-        cacheBuilder.concurrencyLevel(concurrencyLevel);
+        CacheBuilder<Key, Value> cacheBuilder = CacheBuilder.<Key, Value>builder()
+                .setMaximumWeight(sizeInBytes).weigher((k, v) -> k.ramBytesUsed() + v.ramBytesUsed()).removalListener(this);
+        // cacheBuilder.concurrencyLevel(concurrencyLevel);
 
         if (expire != null) {
-            cacheBuilder.expireAfterAccess(expire.millis(), TimeUnit.MILLISECONDS);
+            cacheBuilder.setExpireAfterAccess(TimeUnit.MILLISECONDS.toNanos(expire.millis()));
         }
 
         cache = cacheBuilder.build();
     }
 
-    private static class QueryCacheWeigher implements Weigher<Key, Value> {
-
-        @Override
-        public int weigh(Key key, Value value) {
-            return (int) (key.ramBytesUsed() + value.ramBytesUsed());
-        }
-    }
-
     public void close() {
         reaper.close();
         cache.invalidateAll();
@@ -197,9 +181,6 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
 
     @Override
     public void onRemoval(RemovalNotification<Key, Value> notification) {
-        if (notification.getKey() == null) {
-            return;
-        }
         notification.getKey().shard.requestCache().onRemoval(notification);
     }
 
@@ -258,8 +239,8 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
     public void loadIntoContext(final ShardSearchRequest request, final SearchContext context, final QueryPhase queryPhase) throws Exception {
         assert canCache(request, context);
         Key key = buildKey(request, context);
-        Loader loader = new Loader(queryPhase, context, key);
-        Value value = cache.get(key, loader);
+        Loader loader = new Loader(queryPhase, context);
+        Value value = cache.computeIfAbsent(key, loader);
         if (loader.isLoaded()) {
             key.shard.requestCache().onMiss();
             // see if its the first time we see this reader, and make sure to register a cleanup key
@@ -279,17 +260,15 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
         }
     }
 
-    private static class Loader implements Callable<Value> {
+    private static class Loader implements CacheLoader<Key, Value> {
 
         private final QueryPhase queryPhase;
         private final SearchContext context;
-        private final IndicesRequestCache.Key key;
         private boolean loaded;
 
-        Loader(QueryPhase queryPhase, SearchContext context, IndicesRequestCache.Key key) {
+        Loader(QueryPhase queryPhase, SearchContext context) {
             this.queryPhase = queryPhase;
             this.context = context;
-            this.key = key;
         }
 
         public boolean isLoaded() {
@@ -297,7 +276,7 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
         }
 
         @Override
-        public Value call() throws Exception {
+        public Value load(Key key) throws Exception {
             queryPhase.execute(context);
 
             /* BytesStreamOutput allows to pass the expected size but by default uses
@@ -473,7 +452,7 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
 
             if (!currentKeysToClean.isEmpty() || !currentFullClean.isEmpty()) {
                 CleanupKey lookupKey = new CleanupKey(null, -1);
-                for (Iterator<Key> iterator = cache.asMap().keySet().iterator(); iterator.hasNext(); ) {
+                for (Iterator<Key> iterator = cache.keys().iterator(); iterator.hasNext(); ) {
                     Key key = iterator.next();
                     if (currentFullClean.contains(key.shard)) {
                         iterator.remove();
@@ -487,7 +466,7 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
                 }
             }
 
-            cache.cleanUp();
+            cache.refresh();
             currentKeysToClean.clear();
             currentFullClean.clear();
         }
diff --git a/core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java b/core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java
index 2a2aef4..6612b9f 100644
--- a/core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java
+++ b/core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java
@@ -19,12 +19,15 @@
 
 package org.elasticsearch.indices.fielddata.cache;
 
-import com.google.common.cache.*;
-import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.SegmentReader;
 import org.apache.lucene.util.Accountable;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.cache.Cache;
+import org.elasticsearch.common.cache.CacheBuilder;
+import org.elasticsearch.common.cache.RemovalListener;
+import org.elasticsearch.common.cache.RemovalNotification;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.logging.ESLogger;
@@ -43,6 +46,7 @@ import org.elasticsearch.threadpool.ThreadPool;
 
 import java.util.ArrayList;
 import java.util.List;
+import java.util.function.ToLongBiFunction;
 
 /**
  */
@@ -66,17 +70,11 @@ public class IndicesFieldDataCache extends AbstractComponent implements RemovalL
         this.indicesFieldDataCacheListener = indicesFieldDataCacheListener;
         final String size = settings.get(INDICES_FIELDDATA_CACHE_SIZE_KEY, "-1");
         final long sizeInBytes = settings.getAsMemory(INDICES_FIELDDATA_CACHE_SIZE_KEY, "-1").bytes();
-        CacheBuilder<Key, Accountable> cacheBuilder = CacheBuilder.newBuilder()
+        CacheBuilder<Key, Accountable> cacheBuilder = CacheBuilder.<Key, Accountable>builder()
                 .removalListener(this);
         if (sizeInBytes > 0) {
-            cacheBuilder.maximumWeight(sizeInBytes).weigher(new FieldDataWeigher());
+            cacheBuilder.setMaximumWeight(sizeInBytes).weigher(new FieldDataWeigher());
         }
-        // defaults to 4, but this is a busy map for all indices, increase it a bit by default
-        final int concurrencyLevel =  settings.getAsInt(FIELDDATA_CACHE_CONCURRENCY_LEVEL, 16);
-        if (concurrencyLevel <= 0) {
-            throw new IllegalArgumentException("concurrency_level must be > 0 but was: " + concurrencyLevel);
-        }
-        cacheBuilder.concurrencyLevel(concurrencyLevel);
 
         logger.debug("using size [{}] [{}]", size, new ByteSizeValue(sizeInBytes));
         cache = cacheBuilder.build();
@@ -108,7 +106,7 @@ public class IndicesFieldDataCache extends AbstractComponent implements RemovalL
         final Accountable value = notification.getValue();
         for (IndexFieldDataCache.Listener listener : key.listeners) {
             try {
-                listener.onRemoval(key.shardId, indexCache.fieldNames, indexCache.fieldDataType, notification.wasEvicted(), value.ramBytesUsed());
+                listener.onRemoval(key.shardId, indexCache.fieldNames, indexCache.fieldDataType, notification.getRemovalReason() == RemovalNotification.RemovalReason.EVICTED, value.ramBytesUsed());
             } catch (Throwable e) {
                 // load anyway since listeners should not throw exceptions
                 logger.error("Failed to call listener on field data cache unloading", e);
@@ -116,10 +114,9 @@ public class IndicesFieldDataCache extends AbstractComponent implements RemovalL
         }
     }
 
-    public static class FieldDataWeigher implements Weigher<Key, Accountable> {
-
+    public static class FieldDataWeigher implements ToLongBiFunction<Key, Accountable> {
         @Override
-        public int weigh(Key key, Accountable ramUsage) {
+        public long applyAsLong(Key key, Accountable ramUsage) {
             int weight = (int) Math.min(ramUsage.ramBytesUsed(), Integer.MAX_VALUE);
             return weight == 0 ? 1 : weight;
         }
@@ -150,13 +147,13 @@ public class IndicesFieldDataCache extends AbstractComponent implements RemovalL
             final ShardId shardId = ShardUtils.extractShardId(context.reader());
             final Key key = new Key(this, context.reader().getCoreCacheKey(), shardId);
             //noinspection unchecked
-            final Accountable accountable = cache.get(key, () -> {
+            final Accountable accountable = cache.computeIfAbsent(key, k -> {
                 context.reader().addCoreClosedListener(IndexFieldCache.this);
                 for (Listener listener : this.listeners) {
-                    key.listeners.add(listener);
+                    k.listeners.add(listener);
                 }
                 final AtomicFieldData fieldData = indexFieldData.loadDirect(context);
-                for (Listener listener : key.listeners) {
+                for (Listener listener : k.listeners) {
                     try {
                         listener.onCache(shardId, fieldNames, fieldDataType, fieldData);
                     } catch (Throwable e) {
@@ -174,13 +171,13 @@ public class IndicesFieldDataCache extends AbstractComponent implements RemovalL
             final ShardId shardId = ShardUtils.extractShardId(indexReader);
             final Key key = new Key(this, indexReader.getCoreCacheKey(), shardId);
             //noinspection unchecked
-            final Accountable accountable = cache.get(key, () -> {
+            final Accountable accountable = cache.computeIfAbsent(key, k -> {
                 indexReader.addReaderClosedListener(IndexFieldCache.this);
                 for (Listener listener : this.listeners) {
-                    key.listeners.add(listener);
+                    k.listeners.add(listener);
                 }
                 final Accountable ifd = (Accountable) indexFieldData.localGlobalDirect(indexReader);
-                for (Listener listener : key.listeners) {
+                for (Listener listener : k.listeners) {
                     try {
                         listener.onCache(shardId, fieldNames, fieldDataType, ifd);
                     } catch (Throwable e) {
@@ -207,38 +204,28 @@ public class IndicesFieldDataCache extends AbstractComponent implements RemovalL
 
         @Override
         public void clear() {
-            for (Key key : cache.asMap().keySet()) {
+            for (Key key : cache.keys()) {
                 if (key.indexCache.index.equals(index)) {
                     cache.invalidate(key);
                 }
             }
-            // Note that cache invalidation in Guava does not immediately remove
-            // values from the cache. In the case of a cache with a rare write or
-            // read rate, it's possible for values to persist longer than desired.
-            //
-            // Note this is intended by the Guava developers, see:
-            // https://code.google.com/p/guava-libraries/wiki/CachesExplained#Eviction
-            // (the "When Does Cleanup Happen" section)
-
-            // We call it explicitly here since it should be a "rare" operation, and
-            // if a user runs it he probably wants to see memory returned as soon as
-            // possible
-            cache.cleanUp();
+            // force eviction
+            cache.refresh();
         }
 
         @Override
         public void clear(String fieldName) {
-            for (Key key : cache.asMap().keySet()) {
+            for (Key key : cache.keys()) {
                 if (key.indexCache.index.equals(index)) {
                     if (key.indexCache.fieldNames.fullName().equals(fieldName)) {
                         cache.invalidate(key);
                     }
                 }
             }
-            // we call cleanUp() because this is a manual operation, should happen
+            // we call refresh because this is a manual operation, should happen
             // rarely and probably means the user wants to see memory returned as
             // soon as possible
-            cache.cleanUp();
+            cache.refresh();
         }
 
         @Override
@@ -305,7 +292,7 @@ public class IndicesFieldDataCache extends AbstractComponent implements RemovalL
                 logger.trace("running periodic field data cache cleanup");
             }
             try {
-                this.cache.cleanUp();
+                this.cache.refresh();
             } catch (Exception e) {
                 logger.warn("Exception during periodic field data cache cleanup:", e);
             }
diff --git a/core/src/main/java/org/elasticsearch/indices/flush/IndicesSyncedFlushResult.java b/core/src/main/java/org/elasticsearch/indices/flush/IndicesSyncedFlushResult.java
index 435c0d1..54ec76e 100644
--- a/core/src/main/java/org/elasticsearch/indices/flush/IndicesSyncedFlushResult.java
+++ b/core/src/main/java/org/elasticsearch/indices/flush/IndicesSyncedFlushResult.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.indices.flush;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.common.xcontent.ToXContent;
@@ -29,8 +30,6 @@ import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * The result of performing a sync flush operation on all shards of multiple indices
  */
@@ -41,10 +40,7 @@ public class IndicesSyncedFlushResult implements ToXContent {
 
 
     public IndicesSyncedFlushResult(Map<String, List<ShardsSyncedFlushResult>> shardsResultPerIndex) {
-        // shardsResultPerIndex is never modified after it is passed to this
-        // constructor so this is safe even though shardsResultPerIndex is a
-        // ConcurrentHashMap
-        this.shardsResultPerIndex = unmodifiableMap(shardsResultPerIndex);
+        this.shardsResultPerIndex = ImmutableMap.copyOf(shardsResultPerIndex);
         this.shardCounts = calculateShardCounts(Iterables.flatten(shardsResultPerIndex.values()));
     }
 
diff --git a/core/src/main/java/org/elasticsearch/indices/flush/ShardsSyncedFlushResult.java b/core/src/main/java/org/elasticsearch/indices/flush/ShardsSyncedFlushResult.java
index f7ae5f9..1388373 100644
--- a/core/src/main/java/org/elasticsearch/indices/flush/ShardsSyncedFlushResult.java
+++ b/core/src/main/java/org/elasticsearch/indices/flush/ShardsSyncedFlushResult.java
@@ -18,15 +18,13 @@
  */
 package org.elasticsearch.indices.flush;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.index.shard.ShardId;
 
 import java.util.HashMap;
 import java.util.Map;
 
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * Result for all copies of a shard
  */
@@ -51,7 +49,7 @@ public class ShardsSyncedFlushResult {
     public ShardsSyncedFlushResult(ShardId shardId, int totalShards, String failureReason) {
         this.syncId = null;
         this.failureReason = failureReason;
-        this.shardResponses = emptyMap();
+        this.shardResponses = ImmutableMap.of();
         this.shardId = shardId;
         this.totalShards = totalShards;
     }
@@ -61,7 +59,8 @@ public class ShardsSyncedFlushResult {
      */
     public ShardsSyncedFlushResult(ShardId shardId, String syncId, int totalShards, Map<ShardRouting, SyncedFlushService.SyncedFlushResponse> shardResponses) {
         this.failureReason = null;
-        this.shardResponses = unmodifiableMap(new HashMap<>(shardResponses));
+        ImmutableMap.Builder<ShardRouting, SyncedFlushService.SyncedFlushResponse> builder = ImmutableMap.builder();
+        this.shardResponses = builder.putAll(shardResponses).build();
         this.syncId = syncId;
         this.totalShards = totalShards;
         this.shardId = shardId;
diff --git a/core/src/main/java/org/elasticsearch/indices/flush/SyncedFlushService.java b/core/src/main/java/org/elasticsearch/indices/flush/SyncedFlushService.java
index c0e5dcd..b6fc3cd 100644
--- a/core/src/main/java/org/elasticsearch/indices/flush/SyncedFlushService.java
+++ b/core/src/main/java/org/elasticsearch/indices/flush/SyncedFlushService.java
@@ -48,13 +48,7 @@ import org.elasticsearch.indices.IndexClosedException;
 import org.elasticsearch.indices.IndicesLifecycle;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.BaseTransportResponseHandler;
-import org.elasticsearch.transport.TransportChannel;
-import org.elasticsearch.transport.TransportException;
-import org.elasticsearch.transport.TransportRequest;
-import org.elasticsearch.transport.TransportRequestHandler;
-import org.elasticsearch.transport.TransportResponse;
-import org.elasticsearch.transport.TransportService;
+import org.elasticsearch.transport.*;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -180,7 +174,7 @@ public class SyncedFlushService extends AbstractComponent {
     * be written on a primary if no write operation was executed between step 1 and step 3 and sync id will only be written on
     * the replica if it contains the same changes that the primary contains.
     *
-    * Synced flush is a best effort operation. The sync id may be written on all, some or none of the copies.
+    * Synced flush is a best effort operation. The sync id may be written on all, some or none of the copies. 
     **/
     public void attemptSyncedFlush(final ShardId shardId, final ActionListener<ShardsSyncedFlushResult> actionListener) {
         try {
@@ -347,7 +341,8 @@ public class SyncedFlushService extends AbstractComponent {
     }
 
     private void contDownAndSendResponseIfDone(String syncId, List<ShardRouting> shards, ShardId shardId, int totalShards,
-            ActionListener<ShardsSyncedFlushResult> listener, CountDown countDown, Map<ShardRouting, SyncedFlushResponse> results) {
+                                               ActionListener<ShardsSyncedFlushResult> listener, CountDown countDown, Map<ShardRouting,
+            SyncedFlushResponse> results) {
         if (countDown.countDown()) {
             assert results.size() == shards.size();
             listener.onResponse(new ShardsSyncedFlushResult(shardId, syncId, totalShards, results));
diff --git a/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java b/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java
index a84fff3..90bb4c4 100644
--- a/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java
+++ b/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java
@@ -29,12 +29,10 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.FutureUtils;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.engine.EngineClosedException;
-import org.elasticsearch.index.engine.EngineConfig;
 import org.elasticsearch.index.engine.FlushNotAllowedEngineException;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.IndexShardState;
 import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.translog.Translog;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.monitor.jvm.JvmInfo;
 import org.elasticsearch.threadpool.ThreadPool;
@@ -42,9 +40,6 @@ import org.elasticsearch.threadpool.ThreadPool;
 import java.util.*;
 import java.util.concurrent.ScheduledFuture;
 
-/**
- *
- */
 public class IndexingMemoryController extends AbstractLifecycleComponent<IndexingMemoryController> {
 
     /** How much heap (% or bytes) we will share across all actively indexing shards on this node (default: 10%). */
@@ -83,6 +78,12 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
     /** How frequently we check shards to find inactive ones (default: 30 seconds). */
     public static final String SHARD_INACTIVE_INTERVAL_TIME_SETTING = "indices.memory.interval";
 
+    /** Once a shard becomes inactive, we reduce the {@code IndexWriter} buffer to this value (500 KB) to let active shards use the heap instead. */
+    public static final ByteSizeValue INACTIVE_SHARD_INDEXING_BUFFER = ByteSizeValue.parseBytesSizeValue("500kb", "INACTIVE_SHARD_INDEXING_BUFFER");
+
+    /** Once a shard becomes inactive, we reduce the {@code Translog} buffer to this value (1 KB) to let active shards use the heap instead. */
+    public static final ByteSizeValue INACTIVE_SHARD_TRANSLOG_BUFFER = ByteSizeValue.parseBytesSizeValue("1kb", "INACTIVE_SHARD_TRANSLOG_BUFFER");
+
     private final ThreadPool threadPool;
     private final IndicesService indicesService;
 
@@ -164,7 +165,6 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
 
         this.statusChecker = new ShardsIndicesStatusChecker();
 
-
         logger.debug("using indexing buffer size [{}], with {} [{}], {} [{}], {} [{}], {} [{}]",
                 this.indexingBuffer,
                 MIN_SHARD_INDEX_BUFFER_SIZE_SETTING, this.minShardIndexBufferSize,
@@ -175,7 +175,7 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
 
     @Override
     protected void doStart() {
-        // its fine to run it on the scheduler thread, no busy work
+        // it's fine to run it on the scheduler thread, no busy work
         this.scheduler = threadPool.scheduleWithFixedDelay(statusChecker, interval);
     }
 
@@ -240,6 +240,7 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
         return null;
     }
 
+    /** set new indexing and translog buffers on this shard.  this may cause the shard to refresh to free up heap. */
     protected void updateShardBuffers(ShardId shardId, ByteSizeValue shardIndexingBufferSize, ByteSizeValue shardTranslogBufferSize) {
         final IndexShard shard = getShard(shardId);
         if (shard != null) {
@@ -255,105 +256,86 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
         }
     }
 
-
-    /** returns the current translog status (generation id + ops) for the given shard id. Returns null if unavailable. */
-    protected ShardIndexingStatus getTranslogStatus(ShardId shardId) {
+    /** returns {@link IndexShard#getActive} if the shard exists, else null */
+    protected Boolean getShardActive(ShardId shardId) {
         final IndexShard indexShard = getShard(shardId);
         if (indexShard == null) {
             return null;
         }
-        final Translog translog;
-        try {
-            translog = indexShard.getTranslog();
-        } catch (EngineClosedException e) {
-            // not ready yet to be checked for activity
-            return null;
-        }
-
-        ShardIndexingStatus status = new ShardIndexingStatus();
-        status.translogId = translog.currentFileGeneration();
-        status.translogNumberOfOperations = translog.totalOperations();
-        return status;
+        return indexShard.getActive();
     }
 
-    // used for tests
-    void forceCheck() {
+    /** check if any shards active status changed, now. */
+    public void forceCheck() {
         statusChecker.run();
     }
 
     class ShardsIndicesStatusChecker implements Runnable {
 
-        private final Map<ShardId, ShardIndexingStatus> shardsIndicesStatus = new HashMap<>();
+        // True if the shard was active last time we checked
+        private final Map<ShardId,Boolean> shardWasActive = new HashMap<>();
 
         @Override
-        public void run() {
+        public synchronized void run() {
             EnumSet<ShardStatusChangeType> changes = purgeDeletedAndClosedShards();
 
-            final List<ShardId> activeToInactiveIndexingShards = new ArrayList<>();
-            final int activeShards = updateShardStatuses(changes, activeToInactiveIndexingShards);
-            for (ShardId indexShard : activeToInactiveIndexingShards) {
-                markShardAsInactive(indexShard);
-            }
+            updateShardStatuses(changes);
 
             if (changes.isEmpty() == false) {
                 // Something changed: recompute indexing buffers:
-                calcAndSetShardBuffers(activeShards, "[" + changes + "]");
+                calcAndSetShardBuffers("[" + changes + "]");
             }
         }
 
         /**
-         * goes through all existing shards and check whether the changes their active status
-         *
-         * @return the current count of active shards
+         * goes through all existing shards and check whether there are changes in their active status
          */
-        private int updateShardStatuses(EnumSet<ShardStatusChangeType> changes, List<ShardId> activeToInactiveIndexingShards) {
-            int activeShards = 0;
+        private void updateShardStatuses(EnumSet<ShardStatusChangeType> changes) {
             for (ShardId shardId : availableShards()) {
 
-                final ShardIndexingStatus currentStatus = getTranslogStatus(shardId);
+                // Is the shard active now?
+                Boolean isActive = getShardActive(shardId);
 
-                if (currentStatus == null) {
+                if (isActive == null) {
                     // shard was closed..
                     continue;
                 }
 
-                ShardIndexingStatus status = shardsIndicesStatus.get(shardId);
-                if (status == null) {
-                    status = currentStatus;
-                    shardsIndicesStatus.put(shardId, status);
+                // Was the shard active last time we checked?
+                Boolean wasActive = shardWasActive.get(shardId);
+
+                if (wasActive == null) {
+                    // First time we are seeing this shard
+                    shardWasActive.put(shardId, isActive);
                     changes.add(ShardStatusChangeType.ADDED);
-                } else {
-                    final boolean lastActiveIndexing = status.activeIndexing;
-                    status.updateWith(currentTimeInNanos(), currentStatus, inactiveTime.nanos());
-                    if (lastActiveIndexing && (status.activeIndexing == false)) {
-                        activeToInactiveIndexingShards.add(shardId);
-                        changes.add(ShardStatusChangeType.BECAME_INACTIVE);
-                        logger.debug("marking shard {} as inactive (inactive_time[{}]) indexing wise, setting size to [{}]",
-                                shardId,
-                                inactiveTime, EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER);
-                    } else if ((lastActiveIndexing == false) && status.activeIndexing) {
+                } else if (isActive) {
+                    // Shard is active now
+                    if (wasActive == false) {
+                        // Shard became active itself, since we last checked (due to new indexing op arriving)
                         changes.add(ShardStatusChangeType.BECAME_ACTIVE);
                         logger.debug("marking shard {} as active indexing wise", shardId);
+                        shardWasActive.put(shardId, true);
+                    } else if (checkIdle(shardId, inactiveTime.nanos()) == Boolean.TRUE) {
+                        // Make shard inactive now
+                        changes.add(ShardStatusChangeType.BECAME_INACTIVE);
+                        logger.debug("marking shard {} as inactive (inactive_time[{}]) indexing wise",
+                                     shardId,
+                                     inactiveTime);
+                        shardWasActive.put(shardId, false);
                     }
                 }
-
-                if (status.activeIndexing) {
-                    activeShards++;
-                }
             }
-
-            return activeShards;
         }
 
         /**
          * purge any existing statuses that are no longer updated
          *
-         * @return true if any change
+         * @return the changes applied
          */
         private EnumSet<ShardStatusChangeType> purgeDeletedAndClosedShards() {
             EnumSet<ShardStatusChangeType> changes = EnumSet.noneOf(ShardStatusChangeType.class);
 
-            Iterator<ShardId> statusShardIdIterator = shardsIndicesStatus.keySet().iterator();
+            Iterator<ShardId> statusShardIdIterator = shardWasActive.keySet().iterator();
             while (statusShardIdIterator.hasNext()) {
                 ShardId shardId = statusShardIdIterator.next();
                 if (shardAvailable(shardId) == false) {
@@ -364,12 +346,25 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
             return changes;
         }
 
-        private void calcAndSetShardBuffers(int activeShards, String reason) {
-            if (activeShards == 0) {
+        private void calcAndSetShardBuffers(String reason) {
+
+            // Count how many shards are now active:
+            int activeShardCount = 0;
+            for (Map.Entry<ShardId,Boolean> ent : shardWasActive.entrySet()) {
+                if (ent.getValue()) {
+                    activeShardCount++;
+                }
+            }
+
+            // TODO: we could be smarter here by taking into account how RAM the IndexWriter on each shard
+            // is actually using (using IW.ramBytesUsed), so that small indices (e.g. Marvel) would not
+            // get the same indexing buffer as large indices.  But it quickly gets tricky...
+            if (activeShardCount == 0) {
                 logger.debug("no active shards (reason={})", reason);
                 return;
             }
-            ByteSizeValue shardIndexingBufferSize = new ByteSizeValue(indexingBuffer.bytes() / activeShards);
+
+            ByteSizeValue shardIndexingBufferSize = new ByteSizeValue(indexingBuffer.bytes() / activeShardCount);
             if (shardIndexingBufferSize.bytes() < minShardIndexBufferSize.bytes()) {
                 shardIndexingBufferSize = minShardIndexBufferSize;
             }
@@ -377,7 +372,7 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
                 shardIndexingBufferSize = maxShardIndexBufferSize;
             }
 
-            ByteSizeValue shardTranslogBufferSize = new ByteSizeValue(translogBuffer.bytes() / activeShards);
+            ByteSizeValue shardTranslogBufferSize = new ByteSizeValue(translogBuffer.bytes() / activeShardCount);
             if (shardTranslogBufferSize.bytes() < minShardTranslogBufferSize.bytes()) {
                 shardTranslogBufferSize = minShardTranslogBufferSize;
             }
@@ -385,11 +380,12 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
                 shardTranslogBufferSize = maxShardTranslogBufferSize;
             }
 
-            logger.debug("recalculating shard indexing buffer (reason={}), total is [{}] with [{}] active shards, each shard set to indexing=[{}], translog=[{}]", reason, indexingBuffer, activeShards, shardIndexingBufferSize, shardTranslogBufferSize);
-            for (ShardId shardId : availableShards()) {
-                ShardIndexingStatus status = shardsIndicesStatus.get(shardId);
-                if (status == null || status.activeIndexing) {
-                    updateShardBuffers(shardId, shardIndexingBufferSize, shardTranslogBufferSize);
+            logger.debug("recalculating shard indexing buffer (reason={}), total is [{}] with [{}] active shards, each shard set to indexing=[{}], translog=[{}]", reason, indexingBuffer, activeShardCount, shardIndexingBufferSize, shardTranslogBufferSize);
+
+            for (Map.Entry<ShardId,Boolean> ent : shardWasActive.entrySet()) {
+                if (ent.getValue()) {
+                    // This shard is active
+                    updateShardBuffers(ent.getKey(), shardIndexingBufferSize, shardTranslogBufferSize);
                 }
             }
         }
@@ -399,13 +395,14 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
         return System.nanoTime();
     }
 
-    // update inactive indexing buffer size
-    protected void markShardAsInactive(ShardId shardId) {
+    /** ask this shard to check now whether it is inactive, and reduces its indexing and translog buffers if so.  returns Boolean.TRUE if
+     *  it did deactive, Boolean.FALSE if it did not, and null if the shard is unknown */
+    protected Boolean checkIdle(ShardId shardId, long inactiveTimeNS) {
         String ignoreReason = null;
         final IndexShard shard = getShard(shardId);
         if (shard != null) {
             try {
-                shard.markAsInactive();
+                return shard.checkIdle(inactiveTimeNS);
             } catch (EngineClosedException e) {
                 // ignore
                 ignoreReason = "EngineClosedException";
@@ -419,47 +416,10 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
         if (ignoreReason != null) {
             logger.trace("ignore [{}] while marking shard {} as inactive", ignoreReason, shardId);
         }
+        return null;
     }
 
     private static enum ShardStatusChangeType {
         ADDED, DELETED, BECAME_ACTIVE, BECAME_INACTIVE
     }
-
-    static class ShardIndexingStatus {
-        long translogId = -1;
-        long translogNumberOfOperations = -1;
-        boolean activeIndexing = true;
-        long idleSinceNanoTime = -1; // contains the first time we saw this shard with no operations done on it
-
-
-        /** update status based on a new sample. updates all internal variables */
-        public void updateWith(long currentNanoTime, ShardIndexingStatus current, long inactiveNanoInterval) {
-            final boolean idle = (translogId == current.translogId && translogNumberOfOperations == current.translogNumberOfOperations);
-            if (activeIndexing && idle) {
-                // no indexing activity detected.
-                if (idleSinceNanoTime < 0) {
-                    // first time we see this, start the clock.
-                    idleSinceNanoTime = currentNanoTime;
-                } else if ((currentNanoTime - idleSinceNanoTime) > inactiveNanoInterval) {
-                    // shard is inactive. mark it as such.
-                    activeIndexing = false;
-                }
-            } else if (activeIndexing == false  // we weren't indexing before
-                    && idle == false // but we do now
-                    && current.translogNumberOfOperations > 0 // but only if we're really sure - see note bellow
-                    ) {
-                // since we sync flush once a shard becomes inactive, the translog id can change, however that
-                // doesn't mean the an indexing operation has happened. Note that if we're really unlucky and a flush happens
-                // immediately after an indexing operation we may not become active immediately. The following
-                // indexing operation will mark the shard as active, so it's OK. If that one doesn't come, we might as well stay
-                // inactive
-
-                activeIndexing = true;
-                idleSinceNanoTime = -1;
-            }
-
-            translogId = current.translogId;
-            translogNumberOfOperations = current.translogNumberOfOperations;
-        }
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java b/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
index 0cec415..b453503 100644
--- a/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
+++ b/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.indices.query;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
@@ -31,10 +32,9 @@ import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
 
-import static java.util.Collections.unmodifiableMap;
-
 public class IndicesQueriesRegistry extends AbstractComponent {
-    private Map<String, QueryParser<?>> queryParsers;
+
+    private ImmutableMap<String, QueryParser<?>> queryParsers;
 
     @Inject
     public IndicesQueriesRegistry(Settings settings, Set<QueryParser> injectedQueryParsers, NamedWriteableRegistry namedWriteableRegistry) {
@@ -49,13 +49,13 @@ public class IndicesQueriesRegistry extends AbstractComponent {
         // EmptyQueryBuilder is not registered as query parser but used internally.
         // We need to register it with the NamedWriteableRegistry in order to serialize it
         namedWriteableRegistry.registerPrototype(QueryBuilder.class, EmptyQueryBuilder.PROTOTYPE);
-        this.queryParsers = unmodifiableMap(queryParsers);
+        this.queryParsers = ImmutableMap.copyOf(queryParsers);
     }
 
     /**
      * Returns all the registered query parsers
      */
-    public Map<String, QueryParser<?>> queryParsers() {
+    public ImmutableMap<String, QueryParser<?>> queryParsers() {
         return queryParsers;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/monitor/fs/FsInfo.java b/core/src/main/java/org/elasticsearch/monitor/fs/FsInfo.java
index 6fcbec5..b97457a 100644
--- a/core/src/main/java/org/elasticsearch/monitor/fs/FsInfo.java
+++ b/core/src/main/java/org/elasticsearch/monitor/fs/FsInfo.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.monitor.fs;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -30,6 +29,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.Set;
@@ -235,7 +235,7 @@ public class FsInfo implements Iterable<FsInfo.Path>, Streamable, ToXContent {
 
     @Override
     public Iterator<Path> iterator() {
-        return Iterators.forArray(paths);
+        return Arrays.stream(paths).iterator();
     }
 
     public static FsInfo readFsInfo(StreamInput in) throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/monitor/jvm/DeadlockAnalyzer.java b/core/src/main/java/org/elasticsearch/monitor/jvm/DeadlockAnalyzer.java
index 6956273..97b9730 100644
--- a/core/src/main/java/org/elasticsearch/monitor/jvm/DeadlockAnalyzer.java
+++ b/core/src/main/java/org/elasticsearch/monitor/jvm/DeadlockAnalyzer.java
@@ -19,17 +19,17 @@
 
 package org.elasticsearch.monitor.jvm;
 
+import com.google.common.collect.ImmutableMap;
+
 import java.lang.management.ManagementFactory;
 import java.lang.management.ThreadInfo;
 import java.lang.management.ThreadMXBean;
 import java.util.Arrays;
-import java.util.HashMap;
 import java.util.HashSet;
 import java.util.LinkedHashSet;
 import java.util.Map;
 import java.util.Set;
 
-import static java.util.Collections.unmodifiableMap;
 import static java.util.Collections.unmodifiableSet;
 
 /**
@@ -55,7 +55,7 @@ public class DeadlockAnalyzer {
         if (deadlockedThreads == null || deadlockedThreads.length == 0) {
             return NULL_RESULT;
         }
-        Map<Long, ThreadInfo> threadInfoMap = createThreadInfoMap(deadlockedThreads);
+        ImmutableMap<Long, ThreadInfo> threadInfoMap = createThreadInfoMap(deadlockedThreads);
         Set<LinkedHashSet<ThreadInfo>> cycles = calculateCycles(threadInfoMap);
         Set<LinkedHashSet<ThreadInfo>> chains = calculateCycleDeadlockChains(threadInfoMap, cycles);
         cycles.addAll(chains);
@@ -89,7 +89,7 @@ public class DeadlockAnalyzer {
     }
 
 
-    private Set<LinkedHashSet<ThreadInfo>> calculateCycleDeadlockChains(Map<Long, ThreadInfo> threadInfoMap, Set<LinkedHashSet<ThreadInfo>> cycles) {
+    private Set<LinkedHashSet<ThreadInfo>> calculateCycleDeadlockChains(ImmutableMap<Long, ThreadInfo> threadInfoMap, Set<LinkedHashSet<ThreadInfo>> cycles) {
         ThreadInfo allThreads[] = threadBean.getThreadInfo(threadBean.getAllThreadIds());
         Set<LinkedHashSet<ThreadInfo>> deadlockChain = new HashSet<>();
         Set<Long> knownDeadlockedThreads = threadInfoMap.keySet();
@@ -113,13 +113,13 @@ public class DeadlockAnalyzer {
     }
 
 
-    private Map<Long, ThreadInfo> createThreadInfoMap(long threadIds[]) {
+    private ImmutableMap<Long, ThreadInfo> createThreadInfoMap(long threadIds[]) {
         ThreadInfo threadInfos[] = threadBean.getThreadInfo(threadIds);
-        Map<Long, ThreadInfo> threadInfoMap = new HashMap<>();
+        ImmutableMap.Builder<Long, ThreadInfo> threadInfoMap = ImmutableMap.builder();
         for (ThreadInfo threadInfo : threadInfos) {
             threadInfoMap.put(threadInfo.getThreadId(), threadInfo);
         }
-        return unmodifiableMap(threadInfoMap);
+        return threadInfoMap.build();
     }
 
     public static class Deadlock {
diff --git a/core/src/main/java/org/elasticsearch/monitor/jvm/JvmMonitorService.java b/core/src/main/java/org/elasticsearch/monitor/jvm/JvmMonitorService.java
index a11fc29..1efd3c7 100644
--- a/core/src/main/java/org/elasticsearch/monitor/jvm/JvmMonitorService.java
+++ b/core/src/main/java/org/elasticsearch/monitor/jvm/JvmMonitorService.java
@@ -19,20 +19,20 @@
 
 package org.elasticsearch.monitor.jvm;
 
+import com.google.common.collect.ImmutableMap;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.FutureUtils;
-import org.elasticsearch.monitor.jvm.JvmStats.GarbageCollector;
 import org.elasticsearch.threadpool.ThreadPool;
 
-import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.ScheduledFuture;
 
-import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.unit.TimeValue.timeValueSeconds;
+import static org.elasticsearch.monitor.jvm.JvmStats.GarbageCollector;
 import static org.elasticsearch.monitor.jvm.JvmStats.jvmStats;
 
 /**
@@ -43,7 +43,7 @@ public class JvmMonitorService extends AbstractLifecycleComponent<JvmMonitorServ
     private final ThreadPool threadPool;
     private final boolean enabled;
     private final TimeValue interval;
-    private final Map<String, GcThreshold> gcThresholds;
+    private final ImmutableMap<String, GcThreshold> gcThresholds;
 
     private volatile ScheduledFuture scheduledFuture;
 
@@ -79,7 +79,7 @@ public class JvmMonitorService extends AbstractLifecycleComponent<JvmMonitorServ
         this.enabled = this.settings.getAsBoolean("monitor.jvm.enabled", true);
         this.interval = this.settings.getAsTime("monitor.jvm.interval", timeValueSeconds(1));
 
-        Map<String, GcThreshold> gcThresholds = new HashMap<>();
+        MapBuilder<String, GcThreshold> gcThresholds = MapBuilder.newMapBuilder();
         Map<String, Settings> gcThresholdGroups = this.settings.getGroups("monitor.jvm.gc");
         for (Map.Entry<String, Settings> entry : gcThresholdGroups.entrySet()) {
             String name = entry.getKey();
@@ -92,10 +92,17 @@ public class JvmMonitorService extends AbstractLifecycleComponent<JvmMonitorServ
                 gcThresholds.put(name, new GcThreshold(name, warn.millis(), info.millis(), debug.millis()));
             }
         }
-        gcThresholds.putIfAbsent(GcNames.YOUNG, new GcThreshold(GcNames.YOUNG, 1000, 700, 400));
-        gcThresholds.putIfAbsent(GcNames.OLD, new GcThreshold(GcNames.OLD, 10000, 5000, 2000));
-        gcThresholds.putIfAbsent("default", new GcThreshold("default", 10000, 5000, 2000));
-        this.gcThresholds = unmodifiableMap(gcThresholds);
+        if (!gcThresholds.containsKey(GcNames.YOUNG)) {
+            gcThresholds.put(GcNames.YOUNG, new GcThreshold(GcNames.YOUNG, 1000, 700, 400));
+        }
+        if (!gcThresholds.containsKey(GcNames.OLD)) {
+            gcThresholds.put(GcNames.OLD, new GcThreshold(GcNames.OLD, 10000, 5000, 2000));
+        }
+        if (!gcThresholds.containsKey("default")) {
+            gcThresholds.put("default", new GcThreshold("default", 10000, 5000, 2000));
+        }
+
+        this.gcThresholds = gcThresholds.immutableMap();
 
         logger.debug("enabled [{}], interval [{}], gc_threshold [{}]", enabled, interval, this.gcThresholds);
     }
diff --git a/core/src/main/java/org/elasticsearch/monitor/jvm/JvmStats.java b/core/src/main/java/org/elasticsearch/monitor/jvm/JvmStats.java
index d219488..c695e26 100644
--- a/core/src/main/java/org/elasticsearch/monitor/jvm/JvmStats.java
+++ b/core/src/main/java/org/elasticsearch/monitor/jvm/JvmStats.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.monitor.jvm;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
@@ -32,6 +31,7 @@ import org.elasticsearch.common.xcontent.XContentBuilderString;
 import java.io.IOException;
 import java.lang.management.*;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.Iterator;
 import java.util.List;
 import java.util.concurrent.TimeUnit;
@@ -378,7 +378,7 @@ public class JvmStats implements Streamable, ToXContent {
 
         @Override
         public Iterator<GarbageCollector> iterator() {
-            return Iterators.forArray(collectors);
+            return Arrays.stream(collectors).iterator();
         }
     }
 
@@ -546,7 +546,7 @@ public class JvmStats implements Streamable, ToXContent {
 
         @Override
         public Iterator<MemoryPool> iterator() {
-            return Iterators.forArray(pools);
+            return Arrays.stream(pools).iterator();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java b/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
index 7aacde5..3f35ddf 100644
--- a/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
+++ b/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
@@ -83,42 +83,20 @@ public class InternalSettingsPreparer {
         initializeSettings(output, input, true);
         Environment environment = new Environment(output.build());
 
-        // TODO: can we simplify all of this and have a single filename, which is looked up in the config dir?
-        boolean loadFromEnv = true;
-        if (useSystemProperties(input)) {
-            // if its default, then load it, but also load form env
-            if (Strings.hasText(System.getProperty("es.default.config"))) {
-                // TODO: we don't allow multiple config files, but having loadFromEnv true here allows just that
-                loadFromEnv = true;
-                output.loadFromPath(environment.configFile().resolve(System.getProperty("es.default.config")));
-            }
-            // TODO: these should be elseifs so that multiple files cannot be loaded
-            // if explicit, just load it and don't load from env
-            if (Strings.hasText(System.getProperty("es.config"))) {
-                loadFromEnv = false;
-                output.loadFromPath(environment.configFile().resolve(System.getProperty("es.config")));
-            }
-            if (Strings.hasText(System.getProperty("elasticsearch.config"))) {
-                loadFromEnv = false;
-                output.loadFromPath(environment.configFile().resolve(System.getProperty("elasticsearch.config")));
-            }
-        }
-        if (loadFromEnv) {
-            boolean settingsFileFound = false;
-            Set<String> foundSuffixes = new HashSet<>();
-            for (String allowedSuffix : ALLOWED_SUFFIXES) {
-                Path path = environment.configFile().resolve("elasticsearch" + allowedSuffix);
-                if (Files.exists(path)) {
-                    if (!settingsFileFound) {
-                        output.loadFromPath(path);
-                    }
-                    settingsFileFound = true;
-                    foundSuffixes.add(allowedSuffix);
+        boolean settingsFileFound = false;
+        Set<String> foundSuffixes = new HashSet<>();
+        for (String allowedSuffix : ALLOWED_SUFFIXES) {
+            Path path = environment.configFile().resolve("elasticsearch" + allowedSuffix);
+            if (Files.exists(path)) {
+                if (!settingsFileFound) {
+                    output.loadFromPath(path);
                 }
+                settingsFileFound = true;
+                foundSuffixes.add(allowedSuffix);
             }
-            if (foundSuffixes.size() > 1) {
-                throw new SettingsException("multiple settings files found with suffixes: " + Strings.collectionToDelimitedString(foundSuffixes, ","));
-            }
+        }
+        if (foundSuffixes.size() > 1) {
+            throw new SettingsException("multiple settings files found with suffixes: " + Strings.collectionToDelimitedString(foundSuffixes, ","));
         }
 
         // re-initialize settings now that the config file has been loaded
diff --git a/core/src/main/java/org/elasticsearch/node/service/NodeService.java b/core/src/main/java/org/elasticsearch/node/service/NodeService.java
index fe57800..369f7e0 100644
--- a/core/src/main/java/org/elasticsearch/node/service/NodeService.java
+++ b/core/src/main/java/org/elasticsearch/node/service/NodeService.java
@@ -19,12 +19,15 @@
 
 package org.elasticsearch.node.service;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.Build;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;
 import org.elasticsearch.action.admin.cluster.node.stats.NodeStats;
 import org.elasticsearch.action.admin.indices.stats.CommonStatsFlags;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
@@ -39,12 +42,8 @@ import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
 import java.io.IOException;
-import java.util.HashMap;
 import java.util.Map;
 
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
-
 /**
  */
 public class NodeService extends AbstractComponent {
@@ -60,7 +59,7 @@ public class NodeService extends AbstractComponent {
     @Nullable
     private HttpServer httpServer;
 
-    private volatile Map<String, String> serviceAttributes = emptyMap();
+    private volatile ImmutableMap<String, String> serviceAttributes = ImmutableMap.of();
 
     private final Version version;
 
@@ -94,15 +93,11 @@ public class NodeService extends AbstractComponent {
     }
 
     public synchronized void putAttribute(String key, String value) {
-        Map<String, String> newServiceAttributes = new HashMap<>(serviceAttributes);
-        newServiceAttributes.put(key, value);
-        serviceAttributes = unmodifiableMap(newServiceAttributes);
+        serviceAttributes = new MapBuilder<>(serviceAttributes).put(key, value).immutableMap();
     }
 
     public synchronized void removeAttribute(String key) {
-        Map<String, String> newServiceAttributes = new HashMap<>(serviceAttributes);
-        newServiceAttributes.remove(key);
-        serviceAttributes = unmodifiableMap(newServiceAttributes);
+        serviceAttributes = new MapBuilder<>(serviceAttributes).remove(key).immutableMap();
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java b/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
index b20a54f..1ec1d34 100644
--- a/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
+++ b/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
@@ -25,6 +25,7 @@ import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.memory.ExtendedMemoryIndex;
 import org.apache.lucene.index.memory.MemoryIndex;
 import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.MatchAllDocsQuery;
@@ -457,22 +458,22 @@ public class PercolatorService extends AbstractComponent {
         @Override
         public PercolateShardResponse doPercolate(PercolateShardRequest request, PercolateContext context, boolean isNested) {
             long count = 0;
-            Lucene.EarlyTerminatingCollector collector = Lucene.createExistsCollector();
             for (Map.Entry<BytesRef, Query> entry : context.percolateQueries().entrySet()) {
                 try {
+                    Query existsQuery = entry.getValue();
                     if (isNested) {
-                        Lucene.exists(context.docSearcher(), entry.getValue(), Queries.newNonNestedFilter(), collector);
-                    } else {
-                        Lucene.exists(context.docSearcher(), entry.getValue(), collector);
+                        existsQuery = new BooleanQuery.Builder()
+                            .add(existsQuery, Occur.MUST)
+                            .add(Queries.newNonNestedFilter(), Occur.FILTER)
+                            .build();
+                    }
+                    if (Lucene.exists(context.docSearcher(), existsQuery)) {
+                        count ++;
                     }
                 } catch (Throwable e) {
                     logger.debug("[" + entry.getKey() + "] failed to execute query", e);
                     throw new PercolateException(context.indexShard().shardId(), "failed to execute", e);
                 }
-
-                if (collector.exists()) {
-                    count++;
-                }
             }
             return new PercolateShardResponse(count, context, request.shardId());
         }
@@ -552,7 +553,6 @@ public class PercolatorService extends AbstractComponent {
             long count = 0;
             List<BytesRef> matches = new ArrayList<>();
             List<Map<String, HighlightField>> hls = new ArrayList<>();
-            Lucene.EarlyTerminatingCollector collector = Lucene.createExistsCollector();
 
             for (Map.Entry<BytesRef, Query> entry : context.percolateQueries().entrySet()) {
                 if (context.highlight() != null) {
@@ -560,26 +560,27 @@ public class PercolatorService extends AbstractComponent {
                     context.hitContext().cache().clear();
                 }
                 try {
+                    Query existsQuery = entry.getValue();
                     if (isNested) {
-                        Lucene.exists(context.docSearcher(), entry.getValue(), Queries.newNonNestedFilter(), collector);
-                    } else {
-                        Lucene.exists(context.docSearcher(), entry.getValue(), collector);
+                        existsQuery = new BooleanQuery.Builder()
+                            .add(existsQuery, Occur.MUST)
+                            .add(Queries.newNonNestedFilter(), Occur.FILTER)
+                            .build();
+                    }
+                    if (Lucene.exists(context.docSearcher(), existsQuery)) {
+                        if (!context.limit || count < context.size()) {
+                            matches.add(entry.getKey());
+                            if (context.highlight() != null) {
+                                highlightPhase.hitExecute(context, context.hitContext());
+                                hls.add(context.hitContext().hit().getHighlightFields());
+                            }
+                        }
+                        count++;
                     }
                 } catch (Throwable e) {
                     logger.debug("[" + entry.getKey() + "] failed to execute query", e);
                     throw new PercolateException(context.indexShard().shardId(), "failed to execute", e);
                 }
-
-                if (collector.exists()) {
-                    if (!context.limit || count < context.size()) {
-                        matches.add(entry.getKey());
-                        if (context.highlight() != null) {
-                            highlightPhase.hitExecute(context, context.hitContext());
-                            hls.add(context.hitContext().hit().getHighlightFields());
-                        }
-                    }
-                    count++;
-                }
             }
 
             BytesRef[] finalMatches = matches.toArray(new BytesRef[matches.size()]);
diff --git a/core/src/main/java/org/elasticsearch/percolator/QueryCollector.java b/core/src/main/java/org/elasticsearch/percolator/QueryCollector.java
index dfa9f4b..094201c 100644
--- a/core/src/main/java/org/elasticsearch/percolator/QueryCollector.java
+++ b/core/src/main/java/org/elasticsearch/percolator/QueryCollector.java
@@ -19,8 +19,10 @@
 package org.elasticsearch.percolator;
 
 import com.carrotsearch.hppc.FloatArrayList;
+
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.*;
+import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.lucene.Lucene;
@@ -54,7 +56,6 @@ abstract class QueryCollector extends SimpleCollector {
     final ESLogger logger;
     boolean isNestedDoc = false;
 
-    final Lucene.EarlyTerminatingCollector collector = Lucene.createExistsCollector();
     BytesRef current;
 
     SortedBinaryDocValues values;
@@ -166,6 +167,13 @@ abstract class QueryCollector extends SimpleCollector {
                 // log???
                 return;
             }
+            Query existsQuery = query;
+            if (isNestedDoc) {
+                existsQuery = new BooleanQuery.Builder()
+                    .add(existsQuery, Occur.MUST)
+                    .add(Queries.newNonNestedFilter(), Occur.FILTER)
+                    .build();
+            }
             // run the query
             try {
                 if (context.highlight() != null) {
@@ -173,12 +181,7 @@ abstract class QueryCollector extends SimpleCollector {
                     context.hitContext().cache().clear();
                 }
 
-                if (isNestedDoc) {
-                    Lucene.exists(searcher, query, Queries.newNonNestedFilter(), collector);
-                } else {
-                    Lucene.exists(searcher, query, collector);
-                }
-                if (collector.exists()) {
+                if (Lucene.exists(searcher, existsQuery)) {
                     if (!limit || counter < size) {
                         matches.add(BytesRef.deepCopyOf(current));
                         if (context.highlight() != null) {
@@ -230,14 +233,16 @@ abstract class QueryCollector extends SimpleCollector {
                 // log???
                 return;
             }
+            Query existsQuery = query;
+            if (isNestedDoc) {
+                existsQuery = new BooleanQuery.Builder()
+                    .add(existsQuery, Occur.MUST)
+                    .add(Queries.newNonNestedFilter(), Occur.FILTER)
+                    .build();
+            }
             // run the query
             try {
-                if (isNestedDoc) {
-                    Lucene.exists(searcher, query, Queries.newNonNestedFilter(), collector);
-                } else {
-                    Lucene.exists(searcher, query, collector);
-                }
-                if (collector.exists()) {
+                if (Lucene.exists(searcher, existsQuery)) {
                     topDocsLeafCollector.collect(doc);
                     postMatch(doc);
                 }
@@ -298,18 +303,20 @@ abstract class QueryCollector extends SimpleCollector {
                 // log???
                 return;
             }
+            Query existsQuery = query;
+            if (isNestedDoc) {
+                existsQuery = new BooleanQuery.Builder()
+                    .add(existsQuery, Occur.MUST)
+                    .add(Queries.newNonNestedFilter(), Occur.FILTER)
+                    .build();
+            }
             // run the query
             try {
                 if (context.highlight() != null) {
                     context.parsedQuery(new ParsedQuery(query));
                     context.hitContext().cache().clear();
                 }
-                if (isNestedDoc) {
-                    Lucene.exists(searcher, query, Queries.newNonNestedFilter(), collector);
-                } else {
-                    Lucene.exists(searcher, query, collector);
-                }
-                if (collector.exists()) {
+                if (Lucene.exists(searcher, existsQuery)) {
                     if (!limit || counter < size) {
                         matches.add(BytesRef.deepCopyOf(current));
                         scores.add(scorer.score());
@@ -363,14 +370,16 @@ abstract class QueryCollector extends SimpleCollector {
                 // log???
                 return;
             }
+            Query existsQuery = query;
+            if (isNestedDoc) {
+                existsQuery = new BooleanQuery.Builder()
+                    .add(existsQuery, Occur.MUST)
+                    .add(Queries.newNonNestedFilter(), Occur.FILTER)
+                    .build();
+            }
             // run the query
             try {
-                if (isNestedDoc) {
-                    Lucene.exists(searcher, query, Queries.newNonNestedFilter(), collector);
-                } else {
-                    Lucene.exists(searcher, query, collector);
-                }
-                if (collector.exists()) {
+                if (Lucene.exists(searcher, existsQuery)) {
                     counter++;
                     postMatch(doc);
                 }
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
index 9e7a6fc..2545deb 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
@@ -19,14 +19,8 @@
 
 package org.elasticsearch.plugins;
 
-import com.google.common.collect.Iterators;
-
 import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.Build;
-import org.elasticsearch.ElasticsearchCorruptionException;
-import org.elasticsearch.ElasticsearchTimeoutException;
-import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.Version;
+import org.elasticsearch.*;
 import org.elasticsearch.bootstrap.JarHell;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.cli.Terminal;
@@ -41,21 +35,12 @@ import java.io.IOException;
 import java.io.OutputStream;
 import java.net.MalformedURLException;
 import java.net.URL;
-import java.nio.file.DirectoryStream;
-import java.nio.file.FileVisitResult;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.nio.file.SimpleFileVisitor;
+import java.nio.file.*;
 import java.nio.file.attribute.BasicFileAttributes;
 import java.nio.file.attribute.PosixFileAttributeView;
 import java.nio.file.attribute.PosixFilePermission;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Locale;
-import java.util.Random;
-import java.util.Set;
+import java.util.*;
+import java.util.stream.StreamSupport;
 import java.util.zip.ZipEntry;
 import java.util.zip.ZipInputStream;
 
@@ -90,10 +75,10 @@ public class PluginManager {
             "analysis-phonetic",
             "analysis-smartcn",
             "analysis-stempel",
-            "cloud-gce",
             "delete-by-query",
             "discovery-azure",
             "discovery-ec2",
+            "discovery-gce",
             "discovery-multicast",
             "lang-expression",
             "lang-groovy",
@@ -225,7 +210,6 @@ public class PluginManager {
     }
 
     private void extract(PluginHandle pluginHandle, Terminal terminal, Path pluginFile) throws IOException {
-
         // unzip plugin to a staging temp dir, named for the plugin
         Path tmp = Files.createTempDirectory(environment.tmpFile(), null);
         Path root = tmp.resolve(pluginHandle.name);
@@ -255,22 +239,74 @@ public class PluginManager {
         terminal.println("Installed %s into %s", pluginHandle.name, extractLocation.toAbsolutePath());
 
         // cleanup
-        IOUtils.rm(tmp, pluginFile);
+        tryToDeletePath(terminal, tmp, pluginFile);
 
         // take care of bin/ by moving and applying permissions if needed
-        Path binFile = extractLocation.resolve("bin");
-        if (Files.isDirectory(binFile)) {
-            Path toLocation = pluginHandle.binDir(environment);
-            terminal.println(VERBOSE, "Found bin, moving to %s", toLocation.toAbsolutePath());
-            if (Files.exists(toLocation)) {
-                IOUtils.rm(toLocation);
+        Path sourcePluginBinDirectory = extractLocation.resolve("bin");
+        Path destPluginBinDirectory = pluginHandle.binDir(environment);
+        boolean needToCopyBinDirectory = Files.exists(sourcePluginBinDirectory);
+        if (needToCopyBinDirectory) {
+            if (Files.exists(destPluginBinDirectory) && !Files.isDirectory(destPluginBinDirectory)) {
+                tryToDeletePath(terminal, extractLocation);
+                throw new IOException("plugin bin directory " + destPluginBinDirectory + " is not a directory");
+            }
+
+            try {
+                copyBinDirectory(sourcePluginBinDirectory, destPluginBinDirectory, pluginHandle.name, terminal);
+            } catch (IOException e) {
+                // rollback and remove potentially before installed leftovers
+                terminal.printError("Error copying bin directory [%s] to [%s], cleaning up, reason: %s", sourcePluginBinDirectory, destPluginBinDirectory, e.getMessage());
+                tryToDeletePath(terminal, extractLocation, pluginHandle.binDir(environment));
+                throw e;
+            }
+
+        }
+
+        Path sourceConfigDirectory = extractLocation.resolve("config");
+        Path destConfigDirectory = pluginHandle.configDir(environment);
+        boolean needToCopyConfigDirectory = Files.exists(sourceConfigDirectory);
+        if (needToCopyConfigDirectory) {
+            if (Files.exists(destConfigDirectory) && !Files.isDirectory(destConfigDirectory)) {
+                tryToDeletePath(terminal, extractLocation, destPluginBinDirectory);
+                throw new IOException("plugin config directory " + destConfigDirectory + " is not a directory");
+            }
+
+            try {
+                terminal.println(VERBOSE, "Found config, moving to %s", destConfigDirectory.toAbsolutePath());
+                moveFilesWithoutOverwriting(sourceConfigDirectory, destConfigDirectory, ".new");
+                terminal.println(VERBOSE, "Installed %s into %s", pluginHandle.name, destConfigDirectory.toAbsolutePath());
+            } catch (IOException e) {
+                terminal.printError("Error copying config directory [%s] to [%s], cleaning up, reason: %s", sourceConfigDirectory, destConfigDirectory, e.getMessage());
+                tryToDeletePath(terminal, extractLocation, destPluginBinDirectory, destConfigDirectory);
+                throw e;
+            }
+        }
+    }
+
+    private void tryToDeletePath(Terminal terminal, Path ... paths) {
+        for (Path path : paths) {
+            try {
+                IOUtils.rm(path);
+            } catch (IOException e) {
+                terminal.printError(e);
+            }
+        }
+    }
+
+    private void copyBinDirectory(Path sourcePluginBinDirectory, Path destPluginBinDirectory, String pluginName, Terminal terminal) throws IOException {
+        boolean canCopyFromSource = Files.exists(sourcePluginBinDirectory) && Files.isReadable(sourcePluginBinDirectory) && Files.isDirectory(sourcePluginBinDirectory);
+        if (canCopyFromSource) {
+            terminal.println(VERBOSE, "Found bin, moving to %s", destPluginBinDirectory.toAbsolutePath());
+            if (Files.exists(destPluginBinDirectory)) {
+                IOUtils.rm(destPluginBinDirectory);
             }
             try {
-                FileSystemUtils.move(binFile, toLocation);
+                Files.createDirectories(destPluginBinDirectory.getParent());
+                FileSystemUtils.move(sourcePluginBinDirectory, destPluginBinDirectory);
             } catch (IOException e) {
-                throw new IOException("Could not move [" + binFile + "] to [" + toLocation + "]", e);
+                throw new IOException("Could not move [" + sourcePluginBinDirectory + "] to [" + destPluginBinDirectory + "]", e);
             }
-            if (Environment.getFileStore(toLocation).supportsFileAttributeView(PosixFileAttributeView.class)) {
+            if (Environment.getFileStore(destPluginBinDirectory).supportsFileAttributeView(PosixFileAttributeView.class)) {
                 // add read and execute permissions to existing perms, so execution will work.
                 // read should generally be set already, but set it anyway: don't rely on umask...
                 final Set<PosixFilePermission> executePerms = new HashSet<>();
@@ -280,7 +316,7 @@ public class PluginManager {
                 executePerms.add(PosixFilePermission.OWNER_EXECUTE);
                 executePerms.add(PosixFilePermission.GROUP_EXECUTE);
                 executePerms.add(PosixFilePermission.OTHERS_EXECUTE);
-                Files.walkFileTree(toLocation, new SimpleFileVisitor<Path>() {
+                Files.walkFileTree(destPluginBinDirectory, new SimpleFileVisitor<Path>() {
                     @Override
                     public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {
                         if (attrs.isRegularFile()) {
@@ -294,15 +330,7 @@ public class PluginManager {
             } else {
                 terminal.println(VERBOSE, "Skipping posix permissions - filestore doesn't support posix permission");
             }
-            terminal.println(VERBOSE, "Installed %s into %s", pluginHandle.name, toLocation.toAbsolutePath());
-        }
-
-        Path configFile = extractLocation.resolve("config");
-        if (Files.isDirectory(configFile)) {
-            Path configDestLocation = pluginHandle.configDir(environment);
-            terminal.println(VERBOSE, "Found config, moving to %s", configDestLocation.toAbsolutePath());
-            moveFilesWithoutOverwriting(configFile, configDestLocation, ".new");
-            terminal.println(VERBOSE, "Installed %s into %s", pluginHandle.name, configDestLocation.toAbsolutePath());
+            terminal.println(VERBOSE, "Installed %s into %s", pluginName, destPluginBinDirectory.toAbsolutePath());
         }
     }
 
@@ -437,7 +465,7 @@ public class PluginManager {
         }
 
         try (DirectoryStream<Path> stream = Files.newDirectoryStream(environment.pluginsFile())) {
-            return Iterators.toArray(stream.iterator(), Path.class);
+            return StreamSupport.stream(stream.spliterator(), false).toArray(length -> new Path[length]);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java b/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java
index 6eb32cf..d485e47 100644
--- a/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java
+++ b/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java
@@ -19,12 +19,9 @@
 
 package org.elasticsearch.repositories;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.cluster.AckedClusterStateUpdateTask;
-import org.elasticsearch.cluster.ClusterChangedEvent;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ClusterStateListener;
+import org.elasticsearch.cluster.*;
 import org.elasticsearch.cluster.ack.ClusterStateUpdateRequest;
 import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;
 import org.elasticsearch.cluster.metadata.MetaData;
@@ -43,15 +40,9 @@ import org.elasticsearch.snapshots.SnapshotsService;
 import org.elasticsearch.transport.TransportService;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 import java.util.stream.Collectors;
 
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
 
 /**
@@ -67,7 +58,7 @@ public class RepositoriesService extends AbstractComponent implements ClusterSta
 
     private final VerifyNodeRepositoryAction verifyAction;
 
-    private volatile Map<String, RepositoryHolder> repositories = emptyMap();
+    private volatile Map<String, RepositoryHolder> repositories = ImmutableMap.of();
 
     @Inject
     public RepositoriesService(Settings settings, ClusterService clusterService, TransportService transportService, RepositoryTypesRegistry typesRegistry, Injector injector) {
@@ -281,7 +272,7 @@ public class RepositoriesService extends AbstractComponent implements ClusterSta
                 }
             }
 
-            Map<String, RepositoryHolder> builder = new HashMap<>();
+            ImmutableMap.Builder<String, RepositoryHolder> builder = ImmutableMap.builder();
             if (newMetaData != null) {
                 // Now go through all repositories and update existing or create missing
                 for (RepositoryMetaData repositoryMetaData : newMetaData.repositories()) {
@@ -312,7 +303,7 @@ public class RepositoriesService extends AbstractComponent implements ClusterSta
                     }
                 }
             }
-            repositories = unmodifiableMap(builder);
+            repositories = builder.build();
         } catch (Throwable ex) {
             logger.warn("failure updating cluster state ", ex);
         }
@@ -377,6 +368,7 @@ public class RepositoriesService extends AbstractComponent implements ClusterSta
         }
         Map<String, RepositoryHolder> newRepositories = new HashMap<>(repositories);
         newRepositories.put(repositoryMetaData.name(), holder);
+        repositories = ImmutableMap.copyOf(newRepositories);
         return true;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/hotthreads/RestNodesHotThreadsAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/hotthreads/RestNodesHotThreadsAction.java
index 5e37407..c1a39cc 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/hotthreads/RestNodesHotThreadsAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/hotthreads/RestNodesHotThreadsAction.java
@@ -58,6 +58,7 @@ public class RestNodesHotThreadsAction extends BaseRestHandler {
         nodesHotThreadsRequest.type(request.param("type", nodesHotThreadsRequest.type()));
         nodesHotThreadsRequest.interval(TimeValue.parseTimeValue(request.param("interval"), nodesHotThreadsRequest.interval(), "interval"));
         nodesHotThreadsRequest.snapshots(request.paramAsInt("snapshots", nodesHotThreadsRequest.snapshots()));
+        nodesHotThreadsRequest.timeout(request.param("timeout"));
         client.admin().cluster().nodesHotThreads(nodesHotThreadsRequest, new RestResponseListener<NodesHotThreadsResponse>(channel) {
             @Override
             public RestResponse buildResponse(NodesHotThreadsResponse response) throws Exception {
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java
index aed9514..f2c5185 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java
@@ -87,6 +87,7 @@ public class RestNodesInfoAction extends BaseRestHandler {
         }
 
         final NodesInfoRequest nodesInfoRequest = new NodesInfoRequest(nodeIds);
+        nodesInfoRequest.timeout(request.param("timeout"));
         // shortcut, dont do checks if only all is specified
         if (metrics.size() == 1 && metrics.contains("_all")) {
             nodesInfoRequest.all();
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java
index fa146b5..2e3927e 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java
@@ -60,6 +60,7 @@ public class RestNodesStatsAction extends BaseRestHandler {
         Set<String> metrics = Strings.splitStringByCommaToSet(request.param("metric", "_all"));
 
         NodesStatsRequest nodesStatsRequest = new NodesStatsRequest(nodesIds);
+        nodesStatsRequest.timeout(request.param("timeout"));
 
         if (metrics.size() == 1 && metrics.contains("_all")) {
             nodesStatsRequest.all();
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/stats/RestClusterStatsAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/stats/RestClusterStatsAction.java
index 572a48d..975c460 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/stats/RestClusterStatsAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/stats/RestClusterStatsAction.java
@@ -43,6 +43,7 @@ public class RestClusterStatsAction extends BaseRestHandler {
     @Override
     public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) {
         ClusterStatsRequest clusterStatsRequest = new ClusterStatsRequest().nodesIds(request.paramAsStringArray("nodeId", null));
+        clusterStatsRequest.timeout(request.param("timeout"));
         client.admin().cluster().clusterStats(clusterStatsRequest, new RestToXContentListener<ClusterStatsResponse>(channel));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java
index 7e75dc1..a25754d 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java
@@ -20,8 +20,8 @@
 package org.elasticsearch.rest.action.admin.indices.validate.template;
 
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateRequest;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateResponse;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateRequest;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
@@ -93,7 +93,7 @@ public class RestRenderSearchTemplateAction extends BaseRestHandler {
         }
         renderSearchTemplateRequest = new RenderSearchTemplateRequest();
         renderSearchTemplateRequest.template(template);
-        client.admin().indices().renderSearchTemplate(renderSearchTemplateRequest, new RestBuilderListener<RenderSearchTemplateResponse>(channel) {
+        client.admin().cluster().renderSearchTemplate(renderSearchTemplateRequest, new RestBuilderListener<RenderSearchTemplateResponse>(channel) {
 
             @Override
             public RestResponse buildResponse(RenderSearchTemplateResponse response, XContentBuilder builder) throws Exception {
diff --git a/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodeAttrsAction.java b/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodeAttrsAction.java
index 2ac08fd..4193208 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodeAttrsAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodeAttrsAction.java
@@ -18,8 +18,6 @@
  */
 
 package org.elasticsearch.rest.action.cat;
-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
 import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;
 import org.elasticsearch.action.admin.cluster.node.info.NodesInfoRequest;
 import org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse;
@@ -43,6 +41,8 @@ import org.elasticsearch.rest.action.support.RestActionListener;
 import org.elasticsearch.rest.action.support.RestResponseListener;
 import org.elasticsearch.rest.action.support.RestTable;
 
+import java.util.Map;
+
 import static org.elasticsearch.rest.RestRequest.Method.GET;
 
 public class RestNodeAttrsAction extends AbstractCatAction {
@@ -111,7 +111,8 @@ public class RestNodeAttrsAction extends AbstractCatAction {
 
         for (DiscoveryNode node : nodes) {
             NodeInfo info = nodesInfo.getNodesMap().get(node.id());
-            for(ObjectObjectCursor<String, String> att : node.attributes()) {
+            Map<String, String> attrs = node.getAttributes();
+            for(String att : attrs.keySet()) {
                 table.startRow();
                 table.addCell(node.name());
                 table.addCell(fullId ? node.id() : Strings.substring(node.getId(), 0, 4));
@@ -123,8 +124,8 @@ public class RestNodeAttrsAction extends AbstractCatAction {
                 } else {
                     table.addCell("-");
                 }
-                table.addCell(att.key);
-                table.addCell(att.value);
+                table.addCell(att);
+                table.addCell(attrs.containsKey(att) ? attrs.get(att) : null);
                 table.endRow();
             }
         }
diff --git a/core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java b/core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java
index 073a4eb..71154a5 100644
--- a/core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java
+++ b/core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.script;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.apache.lucene.index.LeafReaderContext;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.component.AbstractComponent;
@@ -29,8 +31,6 @@ import org.elasticsearch.search.lookup.SearchLookup;
 import java.io.IOException;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * A native script engine service.
  */
@@ -38,12 +38,12 @@ public class NativeScriptEngineService extends AbstractComponent implements Scri
 
     public static final String NAME = "native";
 
-    private final Map<String, NativeScriptFactory> scripts;
+    private final ImmutableMap<String, NativeScriptFactory> scripts;
 
     @Inject
     public NativeScriptEngineService(Settings settings, Map<String, NativeScriptFactory> scripts) {
         super(settings);
-        this.scripts = unmodifiableMap(scripts);
+        this.scripts = ImmutableMap.copyOf(scripts);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java b/core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java
index 929575c..10a1c42 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java
@@ -19,13 +19,10 @@
 
 package org.elasticsearch.script;
 
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Map;
-import java.util.Set;
+import com.google.common.collect.ImmutableMap;
+
+import java.util.*;
 
-import static java.util.Collections.unmodifiableMap;
 import static java.util.Collections.unmodifiableSet;
 
 /**
@@ -36,7 +33,7 @@ import static java.util.Collections.unmodifiableSet;
 public final class ScriptContextRegistry {
     static final Set<String> RESERVED_SCRIPT_CONTEXTS = reservedScriptContexts();
 
-    private final Map<String, ScriptContext> scriptContexts;
+    private final ImmutableMap<String, ScriptContext> scriptContexts;
 
     public ScriptContextRegistry(Collection<ScriptContext.Plugin> customScriptContexts) {
         Map<String, ScriptContext> scriptContexts = new HashMap<>();
@@ -50,7 +47,7 @@ public final class ScriptContextRegistry {
                 throw new IllegalArgumentException("script context [" + customScriptContext.getKey() + "] cannot be registered twice");
             }
         }
-        this.scriptContexts = unmodifiableMap(scriptContexts);
+        this.scriptContexts = ImmutableMap.copyOf(scriptContexts);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptModes.java b/core/src/main/java/org/elasticsearch/script/ScriptModes.java
index cfa3a59..897e69b 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptModes.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptModes.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.script;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.script.ScriptService.ScriptType;
@@ -28,8 +29,6 @@ import java.util.Map;
 import java.util.Set;
 import java.util.TreeMap;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * Holds the {@link org.elasticsearch.script.ScriptMode}s for each of the different scripting languages available,
  * each script source and each scripted operation.
@@ -39,7 +38,7 @@ public class ScriptModes {
     static final String SCRIPT_SETTINGS_PREFIX = "script.";
     static final String ENGINE_SETTINGS_PREFIX = "script.engine";
 
-    final Map<String, ScriptMode> scriptModes;
+    final ImmutableMap<String, ScriptMode> scriptModes;
 
     ScriptModes(Map<String, ScriptEngineService> scriptEngines, ScriptContextRegistry scriptContextRegistry, Settings settings) {
         //filter out the native engine as we don't want to apply fine grained settings to it.
@@ -49,7 +48,7 @@ public class ScriptModes {
         this.scriptModes = buildScriptModeSettingsMap(settings, filteredEngines, scriptContextRegistry);
     }
 
-    private static Map<String, ScriptMode> buildScriptModeSettingsMap(Settings settings, Map<String, ScriptEngineService> scriptEngines, ScriptContextRegistry scriptContextRegistry) {
+    private static ImmutableMap<String, ScriptMode> buildScriptModeSettingsMap(Settings settings, Map<String, ScriptEngineService> scriptEngines, ScriptContextRegistry scriptContextRegistry) {
         HashMap<String, ScriptMode> scriptModesMap = new HashMap<>();
 
         //file scripts are enabled by default, for any language
@@ -62,7 +61,7 @@ public class ScriptModes {
         processSourceBasedGlobalSettings(settings, scriptEngines, scriptContextRegistry, scriptModesMap);
         processOperationBasedGlobalSettings(settings, scriptEngines, scriptContextRegistry, scriptModesMap);
         processEngineSpecificSettings(settings, scriptEngines, scriptContextRegistry, scriptModesMap);
-        return unmodifiableMap(scriptModesMap);
+        return ImmutableMap.copyOf(scriptModesMap);
     }
 
     private static void processSourceBasedGlobalSettings(Settings settings, Map<String, ScriptEngineService> scriptEngines, ScriptContextRegistry scriptContextRegistry, Map<String, ScriptMode> scriptModes) {
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptService.java b/core/src/main/java/org/elasticsearch/script/ScriptService.java
index 1095377..70f0dad 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptService.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptService.java
@@ -19,11 +19,7 @@
 
 package org.elasticsearch.script;
 
-import com.google.common.cache.Cache;
-import com.google.common.cache.CacheBuilder;
-import com.google.common.cache.RemovalListener;
-import com.google.common.cache.RemovalNotification;
-
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.delete.DeleteRequest;
@@ -41,6 +37,10 @@ import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.cache.Cache;
+import org.elasticsearch.common.cache.CacheBuilder;
+import org.elasticsearch.common.cache.RemovalListener;
+import org.elasticsearch.common.cache.RemovalNotification;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
@@ -68,14 +68,10 @@ import java.io.InputStreamReader;
 import java.nio.charset.StandardCharsets;
 import java.nio.file.Files;
 import java.nio.file.Path;
-import java.util.HashMap;
 import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.ConcurrentMap;
-import java.util.concurrent.TimeUnit;
-
-import static java.util.Collections.unmodifiableMap;
 
 /**
  *
@@ -95,8 +91,8 @@ public class ScriptService extends AbstractComponent implements Closeable {
     private final String defaultLang;
 
     private final Set<ScriptEngineService> scriptEngines;
-    private final Map<String, ScriptEngineService> scriptEnginesByLang;
-    private final Map<String, ScriptEngineService> scriptEnginesByExt;
+    private final ImmutableMap<String, ScriptEngineService> scriptEnginesByLang;
+    private final ImmutableMap<String, ScriptEngineService> scriptEnginesByExt;
 
     private final ConcurrentMap<String, CompiledScript> staticCache = ConcurrentCollections.newConcurrentMap();
 
@@ -155,17 +151,17 @@ public class ScriptService extends AbstractComponent implements Closeable {
 
         this.defaultLang = settings.get(DEFAULT_SCRIPTING_LANGUAGE_SETTING, DEFAULT_LANG);
 
-        CacheBuilder cacheBuilder = CacheBuilder.newBuilder();
+        CacheBuilder<String, CompiledScript> cacheBuilder = CacheBuilder.builder();
         if (cacheMaxSize >= 0) {
-            cacheBuilder.maximumSize(cacheMaxSize);
+            cacheBuilder.setMaximumWeight(cacheMaxSize);
         }
         if (cacheExpire != null) {
-            cacheBuilder.expireAfterAccess(cacheExpire.nanos(), TimeUnit.NANOSECONDS);
+            cacheBuilder.setExpireAfterAccess(cacheExpire.nanos());
         }
         this.cache = cacheBuilder.removalListener(new ScriptCacheRemovalListener()).build();
 
-        Map<String, ScriptEngineService> enginesByLangBuilder = new HashMap<>();
-        Map<String, ScriptEngineService> enginesByExtBuilder = new HashMap<>();
+        ImmutableMap.Builder<String, ScriptEngineService> enginesByLangBuilder = ImmutableMap.builder();
+        ImmutableMap.Builder<String, ScriptEngineService> enginesByExtBuilder = ImmutableMap.builder();
         for (ScriptEngineService scriptEngine : scriptEngines) {
             for (String type : scriptEngine.types()) {
                 enginesByLangBuilder.put(type, scriptEngine);
@@ -174,8 +170,8 @@ public class ScriptService extends AbstractComponent implements Closeable {
                 enginesByExtBuilder.put(ext, scriptEngine);
             }
         }
-        this.scriptEnginesByLang = unmodifiableMap(enginesByLangBuilder);
-        this.scriptEnginesByExt = unmodifiableMap(enginesByExtBuilder);
+        this.scriptEnginesByLang = enginesByLangBuilder.build();
+        this.scriptEnginesByExt = enginesByExtBuilder.build();
 
         this.scriptModes = new ScriptModes(this.scriptEnginesByLang, scriptContextRegistry, settings);
 
@@ -303,7 +299,7 @@ public class ScriptService extends AbstractComponent implements Closeable {
         }
 
         String cacheKey = getCacheKey(scriptEngineService, type == ScriptType.INLINE ? null : name, code);
-        CompiledScript compiledScript = cache.getIfPresent(cacheKey);
+        CompiledScript compiledScript = cache.get(cacheKey);
 
         if (compiledScript == null) {
             //Either an un-cached inline script or indexed script
@@ -495,12 +491,8 @@ public class ScriptService extends AbstractComponent implements Closeable {
      * script has been removed from the cache
      */
     private class ScriptCacheRemovalListener implements RemovalListener<String, CompiledScript> {
-
         @Override
         public void onRemoval(RemovalNotification<String, CompiledScript> notification) {
-            if (logger.isDebugEnabled()) {
-                logger.debug("notifying script services of script removal due to: [{}]", notification.getCause());
-            }
             scriptMetrics.onCacheEviction();
             for (ScriptEngineService service : scriptEngines) {
                 try {
diff --git a/core/src/main/java/org/elasticsearch/search/SearchService.java b/core/src/main/java/org/elasticsearch/search/SearchService.java
index 57cecfc..403f4a5 100644
--- a/core/src/main/java/org/elasticsearch/search/SearchService.java
+++ b/core/src/main/java/org/elasticsearch/search/SearchService.java
@@ -22,6 +22,7 @@ package org.elasticsearch.search;
 import com.carrotsearch.hppc.ObjectHashSet;
 import com.carrotsearch.hppc.ObjectSet;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
+import com.google.common.collect.ImmutableMap;
 
 import org.apache.lucene.index.IndexOptions;
 import org.apache.lucene.index.LeafReaderContext;
@@ -111,7 +112,6 @@ import java.util.concurrent.Executor;
 import java.util.concurrent.ScheduledFuture;
 import java.util.concurrent.atomic.AtomicLong;
 
-import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.Strings.hasLength;
 import static org.elasticsearch.common.unit.TimeValue.timeValueMillis;
 import static org.elasticsearch.common.unit.TimeValue.timeValueMinutes;
@@ -160,7 +160,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
 
     private final ConcurrentMapLong<SearchContext> activeContexts = ConcurrentCollections.newConcurrentMapLongWithAggressiveConcurrency();
 
-    private final Map<String, SearchParseElement> elementParsers;
+    private final ImmutableMap<String, SearchParseElement> elementParsers;
 
     private final ParseFieldMatcher parseFieldMatcher;
 
@@ -212,7 +212,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
         elementParsers.putAll(queryPhase.parseElements());
         elementParsers.putAll(fetchPhase.parseElements());
         elementParsers.put("stats", new StatsGroupsParseElement());
-        this.elementParsers = unmodifiableMap(elementParsers);
+        this.elementParsers = ImmutableMap.copyOf(elementParsers);
 
         this.keepAliveReaper = threadPool.scheduleWithFixedDelay(new Reaper(), keepAliveInterval);
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
index 742f678..123da5a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.search.aggregations;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.Query;
@@ -34,33 +35,34 @@ import org.elasticsearch.search.query.QueryPhaseExecutionException;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  *
  */
 public class AggregationPhase implements SearchPhase {
-    private final Map<String, SearchParseElement> parseElements;
+
+    private final AggregationParseElement parseElement;
+
+    private final AggregationBinaryParseElement binaryParseElement;
 
     @Inject
     public AggregationPhase(AggregationParseElement parseElement, AggregationBinaryParseElement binaryParseElement) {
-        Map<String, SearchParseElement> parseElements = new HashMap<>();
-        parseElements.put("aggregations", parseElement);
-        parseElements.put("aggs", parseElement);
-        parseElements.put("aggregations_binary", binaryParseElement);
-        parseElements.put("aggregationsBinary", binaryParseElement);
-        parseElements.put("aggs_binary", binaryParseElement);
-        parseElements.put("aggsBinary", binaryParseElement);
-        this.parseElements = unmodifiableMap(parseElements);
+        this.parseElement = parseElement;
+        this.binaryParseElement = binaryParseElement;
     }
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return parseElements;
+        return ImmutableMap.<String, SearchParseElement>builder()
+                .put("aggregations", parseElement)
+                .put("aggs", parseElement)
+                .put("aggregations_binary", binaryParseElement)
+                .put("aggregationsBinary", binaryParseElement)
+                .put("aggs_binary", binaryParseElement)
+                .put("aggsBinary", binaryParseElement)
+                .build();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationStreams.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationStreams.java
index 2ebe2dd..97985f3 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationStreams.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationStreams.java
@@ -18,22 +18,20 @@
  */
 package org.elasticsearch.search.aggregations;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.io.stream.StreamInput;
 
 import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
 
 /**
  * A registry for all the dedicated streams in the aggregation module. This is to support dynamic addAggregation that
  * know how to stream themselves.
  */
 public class AggregationStreams {
-    private static Map<BytesReference, Stream> streams = emptyMap();
+
+    private static ImmutableMap<BytesReference, Stream> streams = ImmutableMap.of();
 
     /**
      * A stream that knows how to read an aggregation from the input.
@@ -49,11 +47,11 @@ public class AggregationStreams {
      * @param types     The types associated with the streams
      */
     public static synchronized void registerStream(Stream stream, BytesReference... types) {
-        Map<BytesReference, Stream> newStreams = new HashMap<>(streams);
+        MapBuilder<BytesReference, Stream> uStreams = MapBuilder.newMapBuilder(streams);
         for (BytesReference type : types) {
-            newStreams.put(type, stream);
+            uStreams.put(type, stream);
         }
-        streams = unmodifiableMap(newStreams);
+        streams = uStreams.immutableMap();
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
index f38138f..257fef8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
@@ -18,6 +18,9 @@
  */
 package org.elasticsearch.search.aggregations;
 
+import com.google.common.collect.ImmutableMap;
+
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -27,22 +30,19 @@ import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * A registry for all the aggregator parser, also servers as the main parser for the aggregations module
  */
 public class AggregatorParsers {
-    public static final Pattern VALID_AGG_NAME = Pattern.compile("[^\\[\\]>]+");
 
-    private final Map<String, Aggregator.Parser> aggParsers;
-    private final Map<String, PipelineAggregator.Parser> pipelineAggregatorParsers;
+    public static final Pattern VALID_AGG_NAME = Pattern.compile("[^\\[\\]>]+");
+    private final ImmutableMap<String, Aggregator.Parser> aggParsers;
+    private final ImmutableMap<String, PipelineAggregator.Parser> pipelineAggregatorParsers;
 
 
     /**
@@ -55,16 +55,16 @@ public class AggregatorParsers {
      */
     @Inject
     public AggregatorParsers(Set<Aggregator.Parser> aggParsers, Set<PipelineAggregator.Parser> pipelineAggregatorParsers) {
-        Map<String, Aggregator.Parser> aggParsersBuilder = new HashMap<>(aggParsers.size());
+        MapBuilder<String, Aggregator.Parser> aggParsersBuilder = MapBuilder.newMapBuilder();
         for (Aggregator.Parser parser : aggParsers) {
             aggParsersBuilder.put(parser.type(), parser);
         }
-        this.aggParsers = unmodifiableMap(aggParsersBuilder);
-        Map<String, PipelineAggregator.Parser> pipelineAggregatorParsersBuilder = new HashMap<>(pipelineAggregatorParsers.size());
+        this.aggParsers = aggParsersBuilder.immutableMap();
+        MapBuilder<String, PipelineAggregator.Parser> pipelineAggregatorParsersBuilder = MapBuilder.newMapBuilder();
         for (PipelineAggregator.Parser parser : pipelineAggregatorParsers) {
             pipelineAggregatorParsersBuilder.put(parser.type(), parser);
         }
-        this.pipelineAggregatorParsers = unmodifiableMap(pipelineAggregatorParsersBuilder);
+        this.pipelineAggregatorParsers = pipelineAggregatorParsersBuilder.immutableMap();
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java b/core/src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java
index 3841030..4297680 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.search.aggregations;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -36,9 +37,6 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.stream.Collectors;
-
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
 /**
  * An internal implementation of {@link Aggregations}.
  */
@@ -48,7 +46,7 @@ public class InternalAggregations implements Aggregations, ToXContent, Streamabl
 
     private List<InternalAggregation> aggregations = Collections.emptyList();
 
-    private Map<String, Aggregation> aggregationsAsMap;
+    private Map<String, InternalAggregation> aggregationsAsMap;
 
     private InternalAggregations() {
     }
@@ -90,13 +88,13 @@ public class InternalAggregations implements Aggregations, ToXContent, Streamabl
     @Override
     public Map<String, Aggregation> getAsMap() {
         if (aggregationsAsMap == null) {
-            Map<String, InternalAggregation> newAggregationsAsMap = new HashMap<>();
+            Map<String, InternalAggregation> aggregationsAsMap = new HashMap<>();
             for (InternalAggregation aggregation : aggregations) {
-                newAggregationsAsMap.put(aggregation.getName(), aggregation);
+                aggregationsAsMap.put(aggregation.getName(), aggregation);
             }
-            this.aggregationsAsMap = unmodifiableMap(newAggregationsAsMap);
+            this.aggregationsAsMap = aggregationsAsMap;
         }
-        return aggregationsAsMap;
+        return new HashMap<>(aggregationsAsMap);
     }
 
     /**
@@ -202,7 +200,7 @@ public class InternalAggregations implements Aggregations, ToXContent, Streamabl
         int size = in.readVInt();
         if (size == 0) {
             aggregations = Collections.emptyList();
-            aggregationsAsMap = emptyMap();
+            aggregationsAsMap = ImmutableMap.of();
         } else {
             aggregations = new ArrayList<>(size);
             for (int i = 0; i < size; i++) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketStreams.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketStreams.java
index ffbf826..aa489e0 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketStreams.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketStreams.java
@@ -19,18 +19,16 @@
 
 package org.elasticsearch.search.aggregations.bucket;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.io.stream.StreamInput;
 
 import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
 
 public class BucketStreams {
-    private static Map<BytesReference, Stream> streams = emptyMap();
+
+    private static ImmutableMap<BytesReference, Stream> STREAMS = ImmutableMap.of();
 
     /**
      * A stream that knows how to read a bucket from the input.
@@ -47,11 +45,11 @@ public class BucketStreams {
      * @param types     The types associated with the streams
      */
     public static synchronized void registerStream(Stream stream, BytesReference... types) {
-        Map<BytesReference, Stream> newStreams = new HashMap<>(streams);
+        MapBuilder<BytesReference, Stream> uStreams = MapBuilder.newMapBuilder(STREAMS);
         for (BytesReference type : types) {
-            newStreams.put(type, stream);
+            uStreams.put(type, stream);
         }
-        streams = unmodifiableMap(newStreams);
+        STREAMS = uStreams.immutableMap();
     }
 
     /**
@@ -61,7 +59,7 @@ public class BucketStreams {
      * @return  The associated stream
      */
     public static Stream stream(BytesReference type) {
-        return streams.get(type);
+        return STREAMS.get(type);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
index 694abf2..ae2ab8a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
@@ -18,7 +18,10 @@
  */
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.rounding.DateTimeUnit;
 import org.elasticsearch.common.rounding.Rounding;
 import org.elasticsearch.common.rounding.TimeZoneRounding;
@@ -31,12 +34,7 @@ import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
 import org.elasticsearch.search.internal.SearchContext;
-
 import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static java.util.Collections.unmodifiableMap;
 
 /**
  *
@@ -47,27 +45,27 @@ public class DateHistogramParser implements Aggregator.Parser {
     static final ParseField OFFSET = new ParseField("offset");
     static final ParseField INTERVAL = new ParseField("interval");
 
-    public static final Map<String, DateTimeUnit> DATE_FIELD_UNITS;
+    public static final ImmutableMap<String, DateTimeUnit> DATE_FIELD_UNITS;
 
     static {
-        Map<String, DateTimeUnit> dateFieldUnits = new HashMap<>();
-        dateFieldUnits.put("year", DateTimeUnit.YEAR_OF_CENTURY);
-        dateFieldUnits.put("1y", DateTimeUnit.YEAR_OF_CENTURY);
-        dateFieldUnits.put("quarter", DateTimeUnit.QUARTER);
-        dateFieldUnits.put("1q", DateTimeUnit.QUARTER);
-        dateFieldUnits.put("month", DateTimeUnit.MONTH_OF_YEAR);
-        dateFieldUnits.put("1M", DateTimeUnit.MONTH_OF_YEAR);
-        dateFieldUnits.put("week", DateTimeUnit.WEEK_OF_WEEKYEAR);
-        dateFieldUnits.put("1w", DateTimeUnit.WEEK_OF_WEEKYEAR);
-        dateFieldUnits.put("day", DateTimeUnit.DAY_OF_MONTH);
-        dateFieldUnits.put("1d", DateTimeUnit.DAY_OF_MONTH);
-        dateFieldUnits.put("hour", DateTimeUnit.HOUR_OF_DAY);
-        dateFieldUnits.put("1h", DateTimeUnit.HOUR_OF_DAY);
-        dateFieldUnits.put("minute", DateTimeUnit.MINUTES_OF_HOUR);
-        dateFieldUnits.put("1m", DateTimeUnit.MINUTES_OF_HOUR);
-        dateFieldUnits.put("second", DateTimeUnit.SECOND_OF_MINUTE);
-        dateFieldUnits.put("1s", DateTimeUnit.SECOND_OF_MINUTE);
-        DATE_FIELD_UNITS = unmodifiableMap(dateFieldUnits);
+        DATE_FIELD_UNITS = MapBuilder.<String, DateTimeUnit>newMapBuilder()
+                .put("year", DateTimeUnit.YEAR_OF_CENTURY)
+                .put("1y", DateTimeUnit.YEAR_OF_CENTURY)
+                .put("quarter", DateTimeUnit.QUARTER)
+                .put("1q", DateTimeUnit.QUARTER)
+                .put("month", DateTimeUnit.MONTH_OF_YEAR)
+                .put("1M", DateTimeUnit.MONTH_OF_YEAR)
+                .put("week", DateTimeUnit.WEEK_OF_WEEKYEAR)
+                .put("1w", DateTimeUnit.WEEK_OF_WEEKYEAR)
+                .put("day", DateTimeUnit.DAY_OF_MONTH)
+                .put("1d", DateTimeUnit.DAY_OF_MONTH)
+                .put("hour", DateTimeUnit.HOUR_OF_DAY)
+                .put("1h", DateTimeUnit.HOUR_OF_DAY)
+                .put("minute", DateTimeUnit.MINUTES_OF_HOUR)
+                .put("1m", DateTimeUnit.MINUTES_OF_HOUR)
+                .put("second", DateTimeUnit.SECOND_OF_MINUTE)
+                .put("1s", DateTimeUnit.SECOND_OF_MINUTE)
+                .immutableMap();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestState.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestState.java
index 25c875b..5b3182d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestState.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestState.java
@@ -1,20 +1,21 @@
 /*
-* Licensed to the Apache Software Foundation (ASF) under one or more
-* contributor license agreements. See the NOTICE file distributed with
-* this work for additional information regarding copyright ownership.
-* The ASF licenses this file to You under the Apache License, Version 2.0
-* (the "License"); you may not use this file except in compliance with
-* the License. You may obtain a copy of the License at
-*
-* http://www.apache.org/licenses/LICENSE-2.0
-*
-* Unless required by applicable law or agreed to in writing, software
-* distributed under the License is distributed on an "AS IS" BASIS,
-* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-* See the License for the specific language governing permissions and
-* limitations under the License.
-*/
-
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
 package org.elasticsearch.search.aggregations.metrics.percentiles.tdigest;
 
 import com.tdunning.math.stats.AVLTreeDigest;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java
index 0614cd7..4bbb407 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java
@@ -30,7 +30,7 @@ import java.util.Map;
 /**
  * Builder for the {@link ScriptedMetric} aggregation.
  */
-public class ScriptedMetricBuilder extends MetricsAggregationBuilder {
+public class ScriptedMetricBuilder extends MetricsAggregationBuilder<ScriptedMetricBuilder> {
 
     private Script initScript = null;
     private Script mapScript = null;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorStreams.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorStreams.java
index 7104609..a633a3c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorStreams.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorStreams.java
@@ -18,22 +18,21 @@
  */
 package org.elasticsearch.search.aggregations.pipeline;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.io.stream.StreamInput;
 
 import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
 
 /**
  * A registry for all the dedicated streams in the aggregation module. This is to support dynamic addAggregation that
  * know how to stream themselves.
  */
 public class PipelineAggregatorStreams {
-    private static Map<BytesReference, Stream> streams = emptyMap();
+
+    private static ImmutableMap<BytesReference, Stream> streams = ImmutableMap.of();
 
     /**
      * A stream that knows how to read an aggregation from the input.
@@ -49,11 +48,11 @@ public class PipelineAggregatorStreams {
      * @param types     The types associated with the streams
      */
     public static synchronized void registerStream(Stream stream, BytesReference... types) {
-        Map<BytesReference, Stream> newStreams = new HashMap<>(streams);
+        MapBuilder<BytesReference, Stream> uStreams = MapBuilder.newMapBuilder(streams);
         for (BytesReference type : types) {
-            newStreams.put(type, stream);
+            uStreams.put(type, stream);
         }
-        streams = unmodifiableMap(newStreams);
+        streams = uStreams.immutableMap();
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
index a681bc7..48686b9 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.search.aggregations.pipeline.movavg;
 
-import com.google.common.collect.EvictingQueue;
+import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.search.aggregations.Aggregation;
@@ -102,7 +102,7 @@ public class MovAvgPipelineAggregator extends PipelineAggregator {
         InternalHistogram.Factory<? extends InternalHistogram.Bucket> factory = histo.getFactory();
 
         List newBuckets = new ArrayList<>();
-        EvictingQueue<Double> values = EvictingQueue.create(this.window);
+        EvictingQueue<Double> values = new EvictingQueue<>(this.window);
 
         long lastValidKey = 0;
         int lastValidPosition = 0;
@@ -202,7 +202,7 @@ public class MovAvgPipelineAggregator extends PipelineAggregator {
     private MovAvgModel minimize(List<? extends InternalHistogram.Bucket> buckets, InternalHistogram histo, MovAvgModel model) {
 
         int counter = 0;
-        EvictingQueue<Double> values = EvictingQueue.create(window);
+        EvictingQueue<Double> values = new EvictingQueue<>(this.window);
 
         double[] test = new double[window];
         ListIterator<? extends InternalHistogram.Bucket> iter = buckets.listIterator(buckets.size());
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/SimulatedAnealingMinimizer.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/SimulatedAnealingMinimizer.java
index bb04502..711ee22 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/SimulatedAnealingMinimizer.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/SimulatedAnealingMinimizer.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.search.aggregations.pipeline.movavg;
 
-import com.google.common.collect.EvictingQueue;
+import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModel;
 
 /**
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
index d00f064..5df97d3 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
@@ -19,8 +19,8 @@
 
 package org.elasticsearch.search.aggregations.pipeline.serialdiff;
 
-import com.google.common.collect.EvictingQueue;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -86,7 +86,7 @@ public class SerialDiffPipelineAggregator extends PipelineAggregator {
         InternalHistogram.Factory<? extends InternalHistogram.Bucket> factory = histo.getFactory();
 
         List newBuckets = new ArrayList<>();
-        EvictingQueue<Double> lagWindow = EvictingQueue.create(lag);
+        EvictingQueue<Double> lagWindow = new EvictingQueue<>(lag);
         int counter = 0;
 
         for (InternalHistogram.Bucket bucket : buckets) {
diff --git a/core/src/main/java/org/elasticsearch/search/dfs/DfsPhase.java b/core/src/main/java/org/elasticsearch/search/dfs/DfsPhase.java
index e1b98c4..f552292 100644
--- a/core/src/main/java/org/elasticsearch/search/dfs/DfsPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/dfs/DfsPhase.java
@@ -19,10 +19,10 @@
 
 package org.elasticsearch.search.dfs;
 
-import com.carrotsearch.hppc.ObjectHashSet;
 import com.carrotsearch.hppc.ObjectObjectHashMap;
+import com.carrotsearch.hppc.ObjectHashSet;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
-
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
@@ -39,8 +39,6 @@ import java.util.Collection;
 import java.util.Iterator;
 import java.util.Map;
 
-import static java.util.Collections.emptyMap;
-
 /**
  *
  */
@@ -48,7 +46,7 @@ public class DfsPhase implements SearchPhase {
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return emptyMap();
+        return ImmutableMap.of();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java
index be3798e..e8e2e0e 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.search.fetch;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.search.DocIdSet;
@@ -63,7 +64,6 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
-import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.xcontent.XContentFactory.contentBuilder;
 
 /**
@@ -82,12 +82,12 @@ public class FetchPhase implements SearchPhase {
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        Map<String, SearchParseElement> parseElements = new HashMap<>();
+        ImmutableMap.Builder<String, SearchParseElement> parseElements = ImmutableMap.builder();
         parseElements.put("fields", new FieldsParseElement());
         for (FetchSubPhase fetchSubPhase : fetchSubPhases) {
             parseElements.putAll(fetchSubPhase.parseElements());
         }
-        return unmodifiableMap(parseElements);
+        return parseElements.build();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/explain/ExplainFetchSubPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/explain/ExplainFetchSubPhase.java
index 42eecb5..1c0eeaa 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/explain/ExplainFetchSubPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/explain/ExplainFetchSubPhase.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.search.fetch.explain;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.search.Explanation;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.fetch.FetchPhaseExecutionException;
@@ -29,8 +30,6 @@ import org.elasticsearch.search.rescore.RescoreSearchContext;
 import java.io.IOException;
 import java.util.Map;
 
-import static java.util.Collections.singletonMap;
-
 /**
  *
  */
@@ -38,7 +37,7 @@ public class ExplainFetchSubPhase implements FetchSubPhase {
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return singletonMap("explain", new ExplainParseElement());
+        return ImmutableMap.of("explain", new ExplainParseElement());
     }
 
     @Override
@@ -60,7 +59,7 @@ public class ExplainFetchSubPhase implements FetchSubPhase {
         try {
             final int topLevelDocId = hitContext.hit().docId();
             Explanation explanation = context.searcher().explain(context.query(), topLevelDocId);
-
+            
             for (RescoreSearchContext rescore : context.rescore()) {
                 explanation = rescore.rescorer().explain(topLevelDocId, context, rescore, explanation);
             }
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsFetchSubPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsFetchSubPhase.java
index c74ef7b..1ec0a98 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsFetchSubPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsFetchSubPhase.java
@@ -18,13 +18,16 @@
  */
 package org.elasticsearch.search.fetch.fielddata;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.index.fielddata.AtomicFieldData;
 import org.elasticsearch.index.fielddata.ScriptDocValues;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.search.SearchHitField;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.fetch.FetchSubPhase;
+import org.elasticsearch.search.fetch.FetchSubPhaseContext;
 import org.elasticsearch.search.internal.InternalSearchHit;
 import org.elasticsearch.search.internal.InternalSearchHitField;
 import org.elasticsearch.search.internal.SearchContext;
@@ -33,8 +36,6 @@ import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * Query sub phase which pulls data from field data (using the cache if
  * available, building it if not).
@@ -63,10 +64,10 @@ public class FieldDataFieldsFetchSubPhase implements FetchSubPhase {
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        Map<String, SearchParseElement> parseElements = new HashMap<>();
-        parseElements.put("fielddata_fields", new FieldDataFieldsParseElement());
-        parseElements.put("fielddataFields", new FieldDataFieldsParseElement());
-        return unmodifiableMap(parseElements);
+        ImmutableMap.Builder<String, SearchParseElement> parseElements = ImmutableMap.builder();
+        parseElements.put("fielddata_fields", new FieldDataFieldsParseElement())
+                .put("fielddataFields", new FieldDataFieldsParseElement());
+        return parseElements.build();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsFetchSubPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsFetchSubPhase.java
index 3557e55..2a36797 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsFetchSubPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsFetchSubPhase.java
@@ -19,9 +19,11 @@
 
 package org.elasticsearch.search.fetch.innerhits;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.search.FieldDoc;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TopDocs;
+import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.search.SearchParseElement;
@@ -41,24 +43,32 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
-import static java.util.Collections.singletonMap;
-
 /**
  */
 public class InnerHitsFetchSubPhase implements FetchSubPhase {
-    private final Map<String, ? extends SearchParseElement> parseElements;
+
+    private final SortParseElement sortParseElement;
+    private final FetchSourceParseElement sourceParseElement;
+    private final HighlighterParseElement highlighterParseElement;
+    private final FieldDataFieldsParseElement fieldDataFieldsParseElement;
+    private final ScriptFieldsParseElement scriptFieldsParseElement;
 
     private FetchPhase fetchPhase;
 
     @Inject
     public InnerHitsFetchSubPhase(SortParseElement sortParseElement, FetchSourceParseElement sourceParseElement, HighlighterParseElement highlighterParseElement, FieldDataFieldsParseElement fieldDataFieldsParseElement, ScriptFieldsParseElement scriptFieldsParseElement) {
-        parseElements = singletonMap("inner_hits", new InnerHitsParseElement(sortParseElement, sourceParseElement, highlighterParseElement,
-                fieldDataFieldsParseElement, scriptFieldsParseElement));
+        this.sortParseElement = sortParseElement;
+        this.sourceParseElement = sourceParseElement;
+        this.highlighterParseElement = highlighterParseElement;
+        this.fieldDataFieldsParseElement = fieldDataFieldsParseElement;
+        this.scriptFieldsParseElement = scriptFieldsParseElement;
     }
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return parseElements;
+        return ImmutableMap.of("inner_hits", new InnerHitsParseElement(
+                sortParseElement, sourceParseElement, highlighterParseElement, fieldDataFieldsParseElement, scriptFieldsParseElement
+        ));
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java
index de5294f..2824bc1 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.search.fetch.matchedqueries;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.TwoPhaseIterator;
@@ -34,8 +35,6 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.emptyMap;
-
 /**
  *
  */
@@ -43,7 +42,7 @@ public class MatchedQueriesFetchSubPhase implements FetchSubPhase {
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return emptyMap();
+        return ImmutableMap.of();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/script/ScriptFieldsFetchSubPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/script/ScriptFieldsFetchSubPhase.java
index 8abcdfb..05ec51e 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/script/ScriptFieldsFetchSubPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/script/ScriptFieldsFetchSubPhase.java
@@ -18,6 +18,10 @@
  */
 package org.elasticsearch.search.fetch.script;
 
+import com.google.common.collect.ImmutableMap;
+
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.script.LeafSearchScript;
 import org.elasticsearch.search.SearchHitField;
 import org.elasticsearch.search.SearchParseElement;
@@ -34,23 +38,21 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  *
  */
 public class ScriptFieldsFetchSubPhase implements FetchSubPhase {
-    private static final Map<String, SearchParseElement> PARSE_ELEMENTS;
-    static {
-        Map<String, SearchParseElement> parseElements = new HashMap<>();
-        parseElements.put("script_fields", new ScriptFieldsParseElement());
-        parseElements.put("scriptFields", new ScriptFieldsParseElement());
-        PARSE_ELEMENTS = unmodifiableMap(parseElements);
+
+    @Inject
+    public ScriptFieldsFetchSubPhase() {
     }
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return PARSE_ELEMENTS;
+        ImmutableMap.Builder<String, SearchParseElement> parseElements = ImmutableMap.builder();
+        parseElements.put("script_fields", new ScriptFieldsParseElement())
+                .put("scriptFields", new ScriptFieldsParseElement());
+        return parseElements.build();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceSubPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceSubPhase.java
index 1ed4738..445d680 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceSubPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceSubPhase.java
@@ -19,9 +19,12 @@
 
 package org.elasticsearch.search.fetch.source;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.internal.InternalSearchHit;
@@ -31,16 +34,20 @@ import org.elasticsearch.search.lookup.SourceLookup;
 import java.io.IOException;
 import java.util.Map;
 
-import static java.util.Collections.singletonMap;
-
 /**
  */
 public class FetchSourceSubPhase implements FetchSubPhase {
-    private static final Map<String, SearchParseElement> PARSE_ELEMENTS = singletonMap("_source", new FetchSourceParseElement());
+
+    @Inject
+    public FetchSourceSubPhase() {
+
+    }
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return PARSE_ELEMENTS;
+        ImmutableMap.Builder<String, SearchParseElement> parseElements = ImmutableMap.builder();
+        parseElements.put("_source", new FetchSourceParseElement());
+        return parseElements.build();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/version/VersionFetchSubPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/version/VersionFetchSubPhase.java
index ec36b78..6a5264d 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/version/VersionFetchSubPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/version/VersionFetchSubPhase.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.search.fetch.version;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.ElasticsearchException;
@@ -32,17 +33,14 @@ import org.elasticsearch.search.internal.SearchContext;
 import java.io.IOException;
 import java.util.Map;
 
-import static java.util.Collections.singletonMap;
-
 /**
  *
  */
 public class VersionFetchSubPhase implements FetchSubPhase {
-    private static final Map<String, ? extends SearchParseElement> PARSE_ELEMENTS = singletonMap("version", new VersionParseElement());
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return PARSE_ELEMENTS;
+        return ImmutableMap.of("version", new VersionParseElement());
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java b/core/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java
index 5352af7..96e1988 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.search.highlight;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
@@ -39,15 +40,12 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.singletonMap;
-
 /**
  *
  */
 public class HighlightPhase extends AbstractComponent implements FetchSubPhase {
+
     private static final List<String> STANDARD_HIGHLIGHTERS_BY_PRECEDENCE = Arrays.asList("fvh", "postings", "plain");
-    private static final Map<String, ? extends SearchParseElement> PARSE_ELEMENTS = singletonMap("highlight",
-            new HighlighterParseElement());
 
     private final Highlighters highlighters;
 
@@ -59,7 +57,7 @@ public class HighlightPhase extends AbstractComponent implements FetchSubPhase {
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return PARSE_ELEMENTS;
+        return ImmutableMap.of("highlight", new HighlighterParseElement());
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java b/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java
index c1194f1..7c334a9 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.search.internal;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.search.Explanation;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.ElasticsearchParseException;
@@ -51,9 +52,6 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.singletonMap;
-import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.lucene.Lucene.readExplanation;
 import static org.elasticsearch.common.lucene.Lucene.writeExplanation;
 import static org.elasticsearch.search.SearchShardTarget.readSearchShardTarget;
@@ -80,7 +78,7 @@ public class InternalSearchHit implements SearchHit {
 
     private BytesReference source;
 
-    private Map<String, SearchHitField> fields = emptyMap();
+    private Map<String, SearchHitField> fields = ImmutableMap.of();
 
     private Map<String, HighlightField> highlightFields = null;
 
@@ -294,12 +292,15 @@ public class InternalSearchHit implements SearchHit {
 
     @Override
     public Map<String, SearchHitField> fields() {
-        return fields == null ? emptyMap() : fields;
+        if (fields == null) {
+            return ImmutableMap.of();
+        }
+        return fields;
     }
 
     // returns the fields without handling null cases
     public Map<String, SearchHitField> fieldsOrNull() {
-        return fields;
+        return this.fields;
     }
 
     @Override
@@ -317,7 +318,10 @@ public class InternalSearchHit implements SearchHit {
 
     @Override
     public Map<String, HighlightField> highlightFields() {
-        return highlightFields == null ? emptyMap() : highlightFields;
+        if (highlightFields == null) {
+            return ImmutableMap.of();
+        }
+        return this.highlightFields;
     }
 
     @Override
@@ -570,32 +574,69 @@ public class InternalSearchHit implements SearchHit {
         }
         int size = in.readVInt();
         if (size == 0) {
-            fields = emptyMap();
+            fields = ImmutableMap.of();
         } else if (size == 1) {
             SearchHitField hitField = readSearchHitField(in);
-            fields = singletonMap(hitField.name(), hitField);
+            fields = ImmutableMap.of(hitField.name(), hitField);
+        } else if (size == 2) {
+            SearchHitField hitField1 = readSearchHitField(in);
+            SearchHitField hitField2 = readSearchHitField(in);
+            fields = ImmutableMap.of(hitField1.name(), hitField1, hitField2.name(), hitField2);
+        } else if (size == 3) {
+            SearchHitField hitField1 = readSearchHitField(in);
+            SearchHitField hitField2 = readSearchHitField(in);
+            SearchHitField hitField3 = readSearchHitField(in);
+            fields = ImmutableMap.of(hitField1.name(), hitField1, hitField2.name(), hitField2, hitField3.name(), hitField3);
+        } else if (size == 4) {
+            SearchHitField hitField1 = readSearchHitField(in);
+            SearchHitField hitField2 = readSearchHitField(in);
+            SearchHitField hitField3 = readSearchHitField(in);
+            SearchHitField hitField4 = readSearchHitField(in);
+            fields = ImmutableMap.of(hitField1.name(), hitField1, hitField2.name(), hitField2, hitField3.name(), hitField3, hitField4.name(), hitField4);
+        } else if (size == 5) {
+            SearchHitField hitField1 = readSearchHitField(in);
+            SearchHitField hitField2 = readSearchHitField(in);
+            SearchHitField hitField3 = readSearchHitField(in);
+            SearchHitField hitField4 = readSearchHitField(in);
+            SearchHitField hitField5 = readSearchHitField(in);
+            fields = ImmutableMap.of(hitField1.name(), hitField1, hitField2.name(), hitField2, hitField3.name(), hitField3, hitField4.name(), hitField4, hitField5.name(), hitField5);
         } else {
-            Map<String, SearchHitField> fields = new HashMap<>();
+            ImmutableMap.Builder<String, SearchHitField> builder = ImmutableMap.builder();
             for (int i = 0; i < size; i++) {
                 SearchHitField hitField = readSearchHitField(in);
-                fields.put(hitField.name(), hitField);
+                builder.put(hitField.name(), hitField);
             }
-            this.fields = unmodifiableMap(fields);
+            fields = builder.build();
         }
 
         size = in.readVInt();
         if (size == 0) {
-            highlightFields = emptyMap();
+            highlightFields = ImmutableMap.of();
         } else if (size == 1) {
             HighlightField field = readHighlightField(in);
-            highlightFields = singletonMap(field.name(), field);
+            highlightFields = ImmutableMap.of(field.name(), field);
+        } else if (size == 2) {
+            HighlightField field1 = readHighlightField(in);
+            HighlightField field2 = readHighlightField(in);
+            highlightFields = ImmutableMap.of(field1.name(), field1, field2.name(), field2);
+        } else if (size == 3) {
+            HighlightField field1 = readHighlightField(in);
+            HighlightField field2 = readHighlightField(in);
+            HighlightField field3 = readHighlightField(in);
+            highlightFields = ImmutableMap.of(field1.name(), field1, field2.name(), field2, field3.name(), field3);
+        } else if (size == 4) {
+            HighlightField field1 = readHighlightField(in);
+            HighlightField field2 = readHighlightField(in);
+            HighlightField field3 = readHighlightField(in);
+            HighlightField field4 = readHighlightField(in);
+            highlightFields = ImmutableMap.of(field1.name(), field1, field2.name(), field2, field3.name(), field3, field4.name(), field4);
         } else {
-            Map<String, HighlightField> highlightFields = new HashMap<>();
+            ImmutableMap.Builder<String, HighlightField> builder = ImmutableMap.builder();
             for (int i = 0; i < size; i++) {
                 HighlightField field = readHighlightField(in);
-                highlightFields.put(field.name(), field);
+                builder.put(field.name(), field);
             }
-            this.highlightFields = unmodifiableMap(highlightFields);
+            highlightFields = builder.build();
         }
 
         size = in.readVInt();
diff --git a/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHits.java b/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHits.java
index 393b7b6..9e787cf 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHits.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHits.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.search.internal;
 
 import com.carrotsearch.hppc.IntObjectHashMap;
-import com.google.common.collect.Iterators;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -30,6 +29,7 @@ import org.elasticsearch.search.SearchHits;
 import org.elasticsearch.search.SearchShardTarget;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.IdentityHashMap;
 import java.util.Iterator;
 import java.util.Map;
@@ -156,7 +156,7 @@ public class InternalSearchHits implements SearchHits {
 
     @Override
     public Iterator<SearchHit> iterator() {
-        return Iterators.forArray(hits());
+        return Arrays.stream(hits()).iterator();
     }
 
     public InternalSearchHit[] internalHits() {
diff --git a/core/src/main/java/org/elasticsearch/search/lookup/IndexLookup.java b/core/src/main/java/org/elasticsearch/search/lookup/IndexLookup.java
index 485c690..0150ef7 100644
--- a/core/src/main/java/org/elasticsearch/search/lookup/IndexLookup.java
+++ b/core/src/main/java/org/elasticsearch/search/lookup/IndexLookup.java
@@ -18,24 +18,12 @@
  */
 package org.elasticsearch.search.lookup;
 
-import org.apache.lucene.index.LeafReaderContext;
-
-import java.util.HashMap;
-import java.util.Map;
+import com.google.common.collect.ImmutableMap.Builder;
 
-import static java.util.Collections.unmodifiableMap;
+import org.apache.lucene.index.LeafReaderContext;
 
 public class IndexLookup {
-    public static final Map<String, Object> NAMES;
-    static {
-        Map<String, Object> names = new HashMap<>();
-        names.put("_FREQUENCIES", IndexLookup.FLAG_FREQUENCIES);
-        names.put("_POSITIONS", IndexLookup.FLAG_POSITIONS);
-        names.put("_OFFSETS", IndexLookup.FLAG_OFFSETS);
-        names.put("_PAYLOADS", IndexLookup.FLAG_PAYLOADS);
-        names.put("_CACHE", IndexLookup.FLAG_CACHE);
-        NAMES = unmodifiableMap(names);
-    }
+
     /**
      * Flag to pass to {@link IndexField#get(Object, int)} if you require
      * offsets in the returned {@link IndexFieldTerm}.
@@ -67,7 +55,15 @@ public class IndexLookup {
      */
     public static final int FLAG_CACHE = 32;
 
-    public static LeafIndexLookup getLeafIndexLookup(LeafReaderContext context) {
+    public IndexLookup(Builder<String, Object> builder) {
+        builder.put("_FREQUENCIES", IndexLookup.FLAG_FREQUENCIES);
+        builder.put("_POSITIONS", IndexLookup.FLAG_POSITIONS);
+        builder.put("_OFFSETS", IndexLookup.FLAG_OFFSETS);
+        builder.put("_PAYLOADS", IndexLookup.FLAG_PAYLOADS);
+        builder.put("_CACHE", IndexLookup.FLAG_CACHE);
+    }
+
+    public LeafIndexLookup getLeafIndexLookup(LeafReaderContext context) {
         return new LeafIndexLookup(context);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/lookup/LeafFieldsLookup.java b/core/src/main/java/org/elasticsearch/search/lookup/LeafFieldsLookup.java
index e5295e8..d45067f 100644
--- a/core/src/main/java/org/elasticsearch/search/lookup/LeafFieldsLookup.java
+++ b/core/src/main/java/org/elasticsearch/search/lookup/LeafFieldsLookup.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.search.lookup;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.LeafReader;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.Nullable;
@@ -32,8 +33,6 @@ import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
 
-import static java.util.Collections.singletonMap;
-
 /**
  *
  */
@@ -149,7 +148,7 @@ public class LeafFieldsLookup implements Map {
             try {
                 reader.document(docId, fieldVisitor);
                 fieldVisitor.postProcess(data.fieldType());
-                data.fields(singletonMap(name, fieldVisitor.fields().get(data.fieldType().names().indexName())));
+                data.fields(ImmutableMap.of(name, fieldVisitor.fields().get(data.fieldType().names().indexName())));
             } catch (IOException e) {
                 throw new ElasticsearchParseException("failed to load field [{}]", e, name);
             }
diff --git a/core/src/main/java/org/elasticsearch/search/lookup/SearchLookup.java b/core/src/main/java/org/elasticsearch/search/lookup/SearchLookup.java
index c9438fd..091a368 100644
--- a/core/src/main/java/org/elasticsearch/search/lookup/SearchLookup.java
+++ b/core/src/main/java/org/elasticsearch/search/lookup/SearchLookup.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.search.lookup;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.LeafReaderContext;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.index.fielddata.IndexFieldDataService;
@@ -35,10 +36,17 @@ public class SearchLookup {
 
     final FieldsLookup fieldsLookup;
 
+    final IndexLookup indexLookup;
+
+    final ImmutableMap<String, Object> asMap;
+
     public SearchLookup(MapperService mapperService, IndexFieldDataService fieldDataService, @Nullable String[] types) {
+        ImmutableMap.Builder<String, Object> builder = ImmutableMap.builder();
         docMap = new DocLookup(mapperService, fieldDataService, types);
         sourceLookup = new SourceLookup();
         fieldsLookup = new FieldsLookup(mapperService, types);
+        indexLookup = new IndexLookup(builder);
+        asMap = builder.build();
     }
 
     public LeafSearchLookup getLeafSearchLookup(LeafReaderContext context) {
@@ -46,8 +54,8 @@ public class SearchLookup {
                 docMap.getLeafDocLookup(context),
                 sourceLookup,
                 fieldsLookup.getLeafFieldsLookup(context),
-                IndexLookup.getLeafIndexLookup(context),
-                IndexLookup.NAMES);
+                indexLookup.getLeafIndexLookup(context),
+                asMap);
     }
 
     public DocLookup doc() {
diff --git a/core/src/main/java/org/elasticsearch/search/lookup/SourceLookup.java b/core/src/main/java/org/elasticsearch/search/lookup/SourceLookup.java
index 910f5da..c0ca2eb 100644
--- a/core/src/main/java/org/elasticsearch/search/lookup/SourceLookup.java
+++ b/core/src/main/java/org/elasticsearch/search/lookup/SourceLookup.java
@@ -18,6 +18,8 @@
  */
 package org.elasticsearch.search.lookup;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.LeafReaderContext;
 import org.elasticsearch.ElasticsearchParseException;
@@ -33,8 +35,6 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
-import static java.util.Collections.emptyMap;
-
 /**
  *
  */
@@ -71,7 +71,7 @@ public class SourceLookup implements Map {
             reader.document(docId, sourceFieldVisitor);
             BytesReference source = sourceFieldVisitor.source();
             if (source == null) {
-                this.source = emptyMap();
+                this.source = ImmutableMap.of();
                 this.sourceContentType = null;
             } else {
                 Tuple<XContentType, Map<String, Object>> tuple = sourceAsMapAndType(source);
diff --git a/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java b/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java
index c4aa23f..d347897 100644
--- a/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java
@@ -19,28 +19,12 @@
 
 package org.elasticsearch.search.query;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.MinDocQuery;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.FieldDoc;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.MultiCollector;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TimeLimitingCollector;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.search.TopDocsCollector;
-import org.apache.lucene.search.TopFieldCollector;
-import org.apache.lucene.search.TopScoreDocCollector;
-import org.apache.lucene.search.TotalHitCountCollector;
-import org.apache.lucene.search.Weight;
+import org.apache.lucene.search.*;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.Lucene;
@@ -59,13 +43,10 @@ import org.elasticsearch.search.sort.TrackScoresParseElement;
 import org.elasticsearch.search.suggest.SuggestPhase;
 
 import java.util.ArrayList;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.Callable;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  *
  */
@@ -84,30 +65,29 @@ public class QueryPhase implements SearchPhase {
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        Map<String, SearchParseElement> parseElements = new HashMap<>();
-        parseElements.put("from", new FromParseElement());
-        parseElements.put("size", new SizeParseElement());
-        parseElements.put("indices_boost", new IndicesBoostParseElement());
-        parseElements.put("indicesBoost", new IndicesBoostParseElement());
-        parseElements.put("query", new QueryParseElement());
-        parseElements.put("queryBinary", new QueryBinaryParseElement());
-        parseElements.put("query_binary", new QueryBinaryParseElement());
-        parseElements.put("filter", new PostFilterParseElement()); // For bw comp reason, should be removed in version 1.1
-        parseElements.put("post_filter", new PostFilterParseElement());
-        parseElements.put("postFilter", new PostFilterParseElement());
-        parseElements.put("filterBinary", new FilterBinaryParseElement());
-        parseElements.put("filter_binary", new FilterBinaryParseElement());
-        parseElements.put("sort", new SortParseElement());
-        parseElements.put("trackScores", new TrackScoresParseElement());
-        parseElements.put("track_scores", new TrackScoresParseElement());
-        parseElements.put("min_score", new MinScoreParseElement());
-        parseElements.put("minScore", new MinScoreParseElement());
-        parseElements.put("timeout", new TimeoutParseElement());
-        parseElements.put("terminate_after", new TerminateAfterParseElement());
-        parseElements.putAll(aggregationPhase.parseElements());
-        parseElements.putAll(suggestPhase.parseElements());
-        parseElements.putAll(rescorePhase.parseElements());
-        return unmodifiableMap(parseElements);
+        ImmutableMap.Builder<String, SearchParseElement> parseElements = ImmutableMap.builder();
+        parseElements.put("from", new FromParseElement()).put("size", new SizeParseElement())
+                .put("indices_boost", new IndicesBoostParseElement())
+                .put("indicesBoost", new IndicesBoostParseElement())
+                .put("query", new QueryParseElement())
+                .put("queryBinary", new QueryBinaryParseElement())
+                .put("query_binary", new QueryBinaryParseElement())
+                .put("filter", new PostFilterParseElement()) // For bw comp reason, should be removed in version 1.1
+                .put("post_filter", new PostFilterParseElement())
+                .put("postFilter", new PostFilterParseElement())
+                .put("filterBinary", new FilterBinaryParseElement())
+                .put("filter_binary", new FilterBinaryParseElement())
+                .put("sort", new SortParseElement())
+                .put("trackScores", new TrackScoresParseElement())
+                .put("track_scores", new TrackScoresParseElement())
+                .put("min_score", new MinScoreParseElement())
+                .put("minScore", new MinScoreParseElement())
+                .put("timeout", new TimeoutParseElement())
+                .put("terminate_after", new TerminateAfterParseElement())
+                .putAll(aggregationPhase.parseElements())
+                .putAll(suggestPhase.parseElements())
+                .putAll(rescorePhase.parseElements());
+        return parseElements.build();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/rescore/RescorePhase.java b/core/src/main/java/org/elasticsearch/search/rescore/RescorePhase.java
index d1592aa..48d8407 100644
--- a/core/src/main/java/org/elasticsearch/search/rescore/RescorePhase.java
+++ b/core/src/main/java/org/elasticsearch/search/rescore/RescorePhase.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.search.rescore;
 
+import com.google.common.collect.ImmutableMap;
+import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TopDocs;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.component.AbstractComponent;
@@ -31,13 +33,10 @@ import org.elasticsearch.search.internal.SearchContext;
 import java.io.IOException;
 import java.util.Map;
 
-import static java.util.Collections.singletonMap;
-
 /**
  */
 public class RescorePhase extends AbstractComponent implements SearchPhase {
-    private static final Map<String, SearchParseElement> PARSE_ELEMENTS = singletonMap("rescore", new RescoreParseElement());
-
+    
     @Inject
     public RescorePhase(Settings settings) {
         super(settings);
@@ -45,7 +44,9 @@ public class RescorePhase extends AbstractComponent implements SearchPhase {
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return PARSE_ELEMENTS;
+        ImmutableMap.Builder<String, SearchParseElement> parseElements = ImmutableMap.builder();
+        parseElements.put("rescore", new RescoreParseElement());
+        return parseElements.build();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java b/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java
index 8454537..f03ca7c 100644
--- a/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.search.sort;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.QueryWrapperFilter;
 import org.apache.lucene.search.Sort;
@@ -32,19 +34,17 @@ import org.elasticsearch.index.fielddata.IndexFieldData;
 import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource.Nested;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.core.LongFieldMapper;
+import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.support.NestedInnerQueryParseSupport;
 import org.elasticsearch.search.MultiValueMode;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.internal.SearchContext;
+import org.elasticsearch.search.internal.SubSearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
-
-import static java.util.Collections.unmodifiableMap;
 
 /**
  *
@@ -62,16 +62,16 @@ public class SortParseElement implements SearchParseElement {
     public static final String SCORE_FIELD_NAME = "_score";
     public static final String DOC_FIELD_NAME = "_doc";
 
-    private static final Map<String, SortParser> PARSERS;
+    private final ImmutableMap<String, SortParser> parsers;
 
-    static {
-        Map<String, SortParser> parsers = new HashMap<>();
-        addParser(parsers, new ScriptSortParser());
-        addParser(parsers, new GeoDistanceSortParser());
-        PARSERS = unmodifiableMap(parsers);
+    public SortParseElement() {
+        ImmutableMap.Builder<String, SortParser> builder = ImmutableMap.builder();
+        addParser(builder, new ScriptSortParser());
+        addParser(builder, new GeoDistanceSortParser());
+        this.parsers = builder.build();
     }
 
-    private static void addParser(Map<String, SortParser> parsers, SortParser parser) {
+    private void addParser(ImmutableMap.Builder<String, SortParser> parsers, SortParser parser) {
         for (String name : parser.names()) {
             parsers.put(name, parser);
         }
@@ -140,8 +140,8 @@ public class SortParseElement implements SearchParseElement {
                     }
                     addSortField(context, sortFields, fieldName, reverse, unmappedType, missing, sortMode, nestedFilterParseHelper);
                 } else {
-                    if (PARSERS.containsKey(fieldName)) {
-                        sortFields.add(PARSERS.get(fieldName).parse(parser, context));
+                    if (parsers.containsKey(fieldName)) {
+                        sortFields.add(parsers.get(fieldName).parse(parser, context));
                     } else {
                         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                             if (token == XContentParser.Token.FIELD_NAME) {
diff --git a/core/src/main/java/org/elasticsearch/search/suggest/SuggestPhase.java b/core/src/main/java/org/elasticsearch/search/suggest/SuggestPhase.java
index 541efa7..58a4502 100644
--- a/core/src/main/java/org/elasticsearch/search/suggest/SuggestPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/suggest/SuggestPhase.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.search.suggest;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.util.CharsRefBuilder;
 import org.elasticsearch.ElasticsearchException;
@@ -37,24 +38,23 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.singletonMap;
-
 /**
  */
 public class SuggestPhase extends AbstractComponent implements SearchPhase {
-    private final Map<String, SearchParseElement> parseElements;
+
     private final SuggestParseElement parseElement;
 
     @Inject
     public SuggestPhase(Settings settings, SuggestParseElement suggestParseElement) {
         super(settings);
         this.parseElement = suggestParseElement;
-        parseElements = singletonMap("suggest", parseElement);
     }
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return parseElements;
+        ImmutableMap.Builder<String, SearchParseElement> parseElements = ImmutableMap.builder();
+        parseElements.put("suggest", parseElement);
+        return parseElements.build();
     }
 
     public SuggestParseElement parseElement() {
diff --git a/core/src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java b/core/src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java
index 447b3fd..bbb3340 100644
--- a/core/src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java
+++ b/core/src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java
@@ -18,6 +18,8 @@
  */
 package org.elasticsearch.search.suggest.completion;
 
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.ImmutableMap.Builder;
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
@@ -57,8 +59,6 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.singletonMap;
-
 /**
  * This {@link PostingsFormat} is basically a T-Sink for a default postings
  * format that is used to store postings on disk fitting the lucene APIs and
@@ -75,12 +75,18 @@ public class Completion090PostingsFormat extends PostingsFormat {
     public static final int SUGGEST_VERSION_CURRENT = SUGGEST_CODEC_VERSION;
     public static final String EXTENSION = "cmp";
 
-    private static final ESLogger logger = Loggers.getLogger(Completion090PostingsFormat.class);
-    private static final CompletionLookupProvider LOOKUP_PROVIDER = new AnalyzingCompletionLookupProvider(true, false, true, false);
-    private static final Map<String, CompletionLookupProvider> PROVIDERS = singletonMap(LOOKUP_PROVIDER.getName(), LOOKUP_PROVIDER);
+    private final static ESLogger logger = Loggers.getLogger(Completion090PostingsFormat.class);
     private PostingsFormat delegatePostingsFormat;
+    private final static Map<String, CompletionLookupProvider> providers;
     private CompletionLookupProvider writeProvider;
 
+
+    static {
+        final CompletionLookupProvider provider = new AnalyzingCompletionLookupProvider(true, false, true, false);
+        final Builder<String, CompletionLookupProvider> builder = ImmutableMap.builder();
+        providers = builder.put(provider.getName(), provider).build();
+    }
+
     public Completion090PostingsFormat(PostingsFormat delegatePostingsFormat, CompletionLookupProvider provider) {
         super(CODEC_NAME);
         this.delegatePostingsFormat = delegatePostingsFormat;
@@ -167,11 +173,11 @@ public class Completion090PostingsFormat extends PostingsFormat {
             try {
                 PostingsFormat delegatePostingsFormat = PostingsFormat.forName(input.readString());
                 String providerName = input.readString();
-                CompletionLookupProvider completionLookupProvider = PROVIDERS.get(providerName);
+                CompletionLookupProvider completionLookupProvider = providers.get(providerName);
                 if (completionLookupProvider == null) {
                     throw new IllegalStateException("no provider with name [" + providerName + "] registered");
                 }
-                // TODO: we could clone the ReadState and make it always forward IOContext.MERGE to prevent unecessary heap usage?
+                // TODO: we could clone the ReadState and make it always forward IOContext.MERGE to prevent unecessary heap usage? 
                 delegateProducer = delegatePostingsFormat.fieldsProducer(state);
                 /*
                  * If we are merging we don't load the FSTs at all such that we
diff --git a/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java b/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java
index 724e3d4..6e7a91d 100644
--- a/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java
+++ b/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java
@@ -104,7 +104,6 @@ public final class PhraseSuggester extends Suggester<PhraseSuggestionContext> {
             response.addTerm(resultEntry);
 
             final BytesRefBuilder byteSpare = new BytesRefBuilder();
-            final EarlyTerminatingCollector collector = Lucene.createExistsCollector();
             final CompiledScript collateScript = suggestion.getCollateQueryScript();
             final boolean collatePrune = (collateScript != null) && suggestion.collatePrune();
             for (int i = 0; i < checkerResult.corrections.length; i++) {
@@ -119,7 +118,7 @@ public final class PhraseSuggester extends Suggester<PhraseSuggestionContext> {
                     final ExecutableScript executable = scriptService.executable(collateScript, vars);
                     final BytesReference querySource = (BytesReference) executable.run();
                     final ParsedQuery parsedQuery = suggestion.getQueryParserService().parse(querySource);
-                    collateMatch = Lucene.exists(searcher, parsedQuery.query(), collector);
+                    collateMatch = Lucene.exists(searcher, parsedQuery.query());
                 }
                 if (!collateMatch && !collatePrune) {
                     continue;
diff --git a/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java b/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
index c709daa..723ca2c 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
@@ -22,6 +22,7 @@ import com.carrotsearch.hppc.IntHashSet;
 import com.carrotsearch.hppc.IntSet;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+import com.google.common.collect.ImmutableMap;
 
 import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionListener;
@@ -54,7 +55,6 @@ import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
 import org.elasticsearch.cluster.settings.DynamicSettings;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
@@ -94,8 +94,6 @@ import static java.util.Collections.unmodifiableSet;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_AUTO_EXPAND_REPLICAS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_CREATION_DATE;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_INDEX_UUID;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_LEGACY_ROUTING_HASH_FUNCTION;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_LEGACY_ROUTING_USE_TYPE;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_VERSION_CREATED;
@@ -131,8 +129,6 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
     private static final Set<String> UNMODIFIABLE_SETTINGS = unmodifiableSet(newHashSet(
             SETTING_NUMBER_OF_SHARDS,
             SETTING_VERSION_CREATED,
-            SETTING_LEGACY_ROUTING_HASH_FUNCTION,
-            SETTING_LEGACY_ROUTING_USE_TYPE,
             SETTING_INDEX_UUID,
             SETTING_CREATION_DATE));
 
@@ -232,11 +228,11 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
                     MetaData.Builder mdBuilder = MetaData.builder(currentState.metaData());
                     ClusterBlocks.Builder blocks = ClusterBlocks.builder().blocks(currentState.blocks());
                     RoutingTable.Builder rtBuilder = RoutingTable.builder(currentState.routingTable());
-                    ImmutableOpenMap<ShardId, RestoreInProgress.ShardRestoreStatus> shards;
+                    final ImmutableMap<ShardId, RestoreInProgress.ShardRestoreStatus> shards;
                     Set<String> aliases = new HashSet<>();
                     if (!renamedIndices.isEmpty()) {
                         // We have some indices to restore
-                        ImmutableOpenMap.Builder<ShardId, RestoreInProgress.ShardRestoreStatus> shardsBuilder = ImmutableOpenMap.builder();
+                        ImmutableMap.Builder<ShardId, RestoreInProgress.ShardRestoreStatus> shardsBuilder = ImmutableMap.builder();
                         for (Map.Entry<String, String> indexEntry : renamedIndices.entrySet()) {
                             String index = indexEntry.getValue();
                             boolean partial = checkPartial(index);
@@ -311,7 +307,7 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
                         RestoreInProgress.Entry restoreEntry = new RestoreInProgress.Entry(snapshotId, RestoreInProgress.State.INIT, Collections.unmodifiableList(new ArrayList<>(renamedIndices.keySet())), shards);
                         builder.putCustom(RestoreInProgress.TYPE, new RestoreInProgress(restoreEntry));
                     } else {
-                        shards = ImmutableOpenMap.of();
+                        shards = ImmutableMap.of();
                     }
 
                     checkAliasNameConflicts(renamedIndices, aliases);
@@ -325,9 +321,8 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
                                 shards.size(), shards.size() - failedShards(shards));
                     }
 
-                    RoutingTable rt = rtBuilder.build();
-                    ClusterState updatedState = builder.metaData(mdBuilder).blocks(blocks).routingTable(rt).build();
-                    RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(rt).build());
+                    ClusterState updatedState = builder.metaData(mdBuilder).blocks(blocks).routingTable(rtBuilder).build();
+                    RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(rtBuilder).build());
                     return ClusterState.builder(updatedState).routingResult(routingResult).build();
                 }
 
@@ -532,7 +527,7 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
 
         clusterService.submitStateUpdateTask("update snapshot state", new ClusterStateUpdateTask() {
             private final List<UpdateIndexShardRestoreStatusRequest> drainedRequests = new ArrayList<>();
-            private Map<SnapshotId, Tuple<RestoreInfo, ImmutableOpenMap<ShardId, ShardRestoreStatus>>> batchedRestoreInfo = null;
+            private Map<SnapshotId, Tuple<RestoreInfo, Map<ShardId, ShardRestoreStatus>>> batchedRestoreInfo = null;
 
             @Override
             public ClusterState execute(ClusterState currentState) {
@@ -555,7 +550,7 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
                     int changedCount = 0;
                     final List<RestoreInProgress.Entry> entries = new ArrayList<>();
                     for (RestoreInProgress.Entry entry : restore.entries()) {
-                        ImmutableOpenMap.Builder<ShardId, ShardRestoreStatus> shardsBuilder = null;
+                        Map<ShardId, ShardRestoreStatus> shards = null;
 
                         for (int i = 0; i < batchSize; i++) {
                             final UpdateIndexShardRestoreStatusRequest updateSnapshotState = drainedRequests.get(i);
@@ -563,18 +558,17 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
 
                             if (entry.snapshotId().equals(updateSnapshotState.snapshotId())) {
                                 logger.trace("[{}] Updating shard [{}] with status [{}]", updateSnapshotState.snapshotId(), updateSnapshotState.shardId(), updateSnapshotState.status().state());
-                                if (shardsBuilder == null) {
-                                    shardsBuilder = ImmutableOpenMap.builder(entry.shards());
+                                if (shards == null) {
+                                    shards = new HashMap<>(entry.shards());
                                 }
-                                shardsBuilder.put(updateSnapshotState.shardId(), updateSnapshotState.status());
+                                shards.put(updateSnapshotState.shardId(), updateSnapshotState.status());
                                 changedCount++;
                             }
                         }
 
-                        if (shardsBuilder != null) {
-                            ImmutableOpenMap<ShardId, ShardRestoreStatus> shards = shardsBuilder.build();
+                        if (shards != null) {
                             if (!completed(shards)) {
-                                entries.add(new RestoreInProgress.Entry(entry.snapshotId(), RestoreInProgress.State.STARTED, entry.indices(), shards));
+                                entries.add(new RestoreInProgress.Entry(entry.snapshotId(), RestoreInProgress.State.STARTED, entry.indices(), ImmutableMap.copyOf(shards)));
                             } else {
                                 logger.info("restore [{}] is done", entry.snapshotId());
                                 if (batchedRestoreInfo == null) {
@@ -611,15 +605,15 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
             @Override
             public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {
                 if (batchedRestoreInfo != null) {
-                    for (final Entry<SnapshotId, Tuple<RestoreInfo, ImmutableOpenMap<ShardId, ShardRestoreStatus>>> entry : batchedRestoreInfo.entrySet()) {
+                    for (final Entry<SnapshotId, Tuple<RestoreInfo, Map<ShardId, ShardRestoreStatus>>> entry : batchedRestoreInfo.entrySet()) {
                         final SnapshotId snapshotId = entry.getKey();
                         final RestoreInfo restoreInfo = entry.getValue().v1();
-                        final ImmutableOpenMap<ShardId, ShardRestoreStatus> shards = entry.getValue().v2();
+                        final Map<ShardId, ShardRestoreStatus> shards = entry.getValue().v2();
                         RoutingTable routingTable = newState.getRoutingTable();
                         final List<ShardId> waitForStarted = new ArrayList<>();
-                        for (ObjectObjectCursor<ShardId, ShardRestoreStatus> shard : shards) {
-                            if (shard.value.state() == RestoreInProgress.State.SUCCESS ) {
-                                ShardId shardId = shard.key;
+                        for (Map.Entry<ShardId, ShardRestoreStatus> shard : shards.entrySet()) {
+                            if (shard.getValue().state() == RestoreInProgress.State.SUCCESS ) {
+                                ShardId shardId = shard.getKey();
                                 ShardRouting shardRouting = findPrimaryShard(routingTable, shardId);
                                 if (shardRouting != null && !shardRouting.active()) {
                                     logger.trace("[{}][{}] waiting for the shard to start", snapshotId, shardId);
@@ -679,19 +673,19 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
         });
     }
 
-    private boolean completed(ImmutableOpenMap<ShardId, RestoreInProgress.ShardRestoreStatus> shards) {
-        for (ObjectCursor<RestoreInProgress.ShardRestoreStatus> status : shards.values()) {
-            if (!status.value.state().completed()) {
+    private boolean completed(Map<ShardId, RestoreInProgress.ShardRestoreStatus> shards) {
+        for (RestoreInProgress.ShardRestoreStatus status : shards.values()) {
+            if (!status.state().completed()) {
                 return false;
             }
         }
         return true;
     }
 
-    private int failedShards(ImmutableOpenMap<ShardId, RestoreInProgress.ShardRestoreStatus> shards) {
+    private int failedShards(Map<ShardId, RestoreInProgress.ShardRestoreStatus> shards) {
         int failedShards = 0;
-        for (ObjectCursor<RestoreInProgress.ShardRestoreStatus> status : shards.values()) {
-            if (status.value.state() == RestoreInProgress.State.FAILURE) {
+        for (RestoreInProgress.ShardRestoreStatus status : shards.values()) {
+            if (status.state() == RestoreInProgress.State.FAILURE) {
                 failedShards++;
             }
         }
@@ -746,13 +740,13 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
             // Some indices were deleted, let's make sure all indices that we are restoring still exist
             for (RestoreInProgress.Entry entry : restore.entries()) {
                 List<ShardId> shardsToFail = null;
-                for (ObjectObjectCursor<ShardId, ShardRestoreStatus> shard : entry.shards()) {
-                    if (!shard.value.state().completed()) {
-                        if (!event.state().metaData().hasIndex(shard.key.getIndex())) {
+                for (ImmutableMap.Entry<ShardId, ShardRestoreStatus> shard : entry.shards().entrySet()) {
+                    if (!shard.getValue().state().completed()) {
+                        if (!event.state().metaData().hasIndex(shard.getKey().getIndex())) {
                             if (shardsToFail == null) {
                                 shardsToFail = new ArrayList<>();
                             }
-                            shardsToFail.add(shard.key);
+                            shardsToFail.add(shard.getKey());
                         }
                     }
                 }
diff --git a/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java b/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java
index 91cf2af..c751895 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java
@@ -19,8 +19,7 @@
 
 package org.elasticsearch.snapshots;
 
-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.IndexCommit;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.cluster.ClusterChangedEvent;
@@ -31,7 +30,6 @@ import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.cluster.SnapshotsInProgress;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -69,8 +67,6 @@ import java.util.concurrent.locks.Condition;
 import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
 
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.cluster.SnapshotsInProgress.completed;
 
 /**
@@ -95,7 +91,7 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
 
     private final Condition shutdownCondition = shutdownLock.newCondition();
 
-    private volatile Map<SnapshotId, SnapshotShards> shardSnapshots = emptyMap();
+    private volatile Map<SnapshotId, SnapshotShards> shardSnapshots = ImmutableMap.of();
 
     private final BlockingQueue<UpdateIndexShardSnapshotStatusRequest> updatedSnapshotStateQueue = ConcurrentCollections.newBlockingQueue();
 
@@ -214,12 +210,12 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
                 if (entry.state() == SnapshotsInProgress.State.STARTED) {
                     Map<ShardId, IndexShardSnapshotStatus> startedShards = new HashMap<>();
                     SnapshotShards snapshotShards = shardSnapshots.get(entry.snapshotId());
-                    for (ObjectObjectCursor<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shard : entry.shards()) {
+                    for (Map.Entry<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shard : entry.shards().entrySet()) {
                         // Add all new shards to start processing on
-                        if (localNodeId.equals(shard.value.nodeId())) {
-                            if (shard.value.state() == SnapshotsInProgress.State.INIT && (snapshotShards == null || !snapshotShards.shards.containsKey(shard.key))) {
-                                logger.trace("[{}] - Adding shard to the queue", shard.key);
-                                startedShards.put(shard.key, new IndexShardSnapshotStatus());
+                        if (localNodeId.equals(shard.getValue().nodeId())) {
+                            if (shard.getValue().state() == SnapshotsInProgress.State.INIT && (snapshotShards == null || !snapshotShards.shards.containsKey(shard.getKey()))) {
+                                logger.trace("[{}] - Adding shard to the queue", shard.getKey());
+                                startedShards.put(shard.getKey(), new IndexShardSnapshotStatus());
                             }
                         }
                     }
@@ -227,23 +223,23 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
                         newSnapshots.put(entry.snapshotId(), startedShards);
                         if (snapshotShards != null) {
                             // We already saw this snapshot but we need to add more started shards
-                            Map<ShardId, IndexShardSnapshotStatus> shards = new HashMap<>();
+                            ImmutableMap.Builder<ShardId, IndexShardSnapshotStatus> shards = ImmutableMap.builder();
                             // Put all shards that were already running on this node
                             shards.putAll(snapshotShards.shards);
                             // Put all newly started shards
                             shards.putAll(startedShards);
-                            survivors.put(entry.snapshotId(), new SnapshotShards(unmodifiableMap(shards)));
+                            survivors.put(entry.snapshotId(), new SnapshotShards(shards.build()));
                         } else {
                             // Brand new snapshot that we haven't seen before
-                            survivors.put(entry.snapshotId(), new SnapshotShards(unmodifiableMap(startedShards)));
+                            survivors.put(entry.snapshotId(), new SnapshotShards(ImmutableMap.copyOf(startedShards)));
                         }
                     }
                 } else if (entry.state() == SnapshotsInProgress.State.ABORTED) {
                     // Abort all running shards for this snapshot
                     SnapshotShards snapshotShards = shardSnapshots.get(entry.snapshotId());
                     if (snapshotShards != null) {
-                        for (ObjectObjectCursor<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shard : entry.shards()) {
-                            IndexShardSnapshotStatus snapshotStatus = snapshotShards.shards.get(shard.key);
+                        for (Map.Entry<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shard : entry.shards().entrySet()) {
+                            IndexShardSnapshotStatus snapshotStatus = snapshotShards.shards.get(shard.getKey());
                             if (snapshotStatus != null) {
                                 switch (snapshotStatus.stage()) {
                                     case INIT:
@@ -251,16 +247,16 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
                                         snapshotStatus.abort();
                                         break;
                                     case FINALIZE:
-                                        logger.debug("[{}] trying to cancel snapshot on shard [{}] that is finalizing, letting it finish", entry.snapshotId(), shard.key);
+                                        logger.debug("[{}] trying to cancel snapshot on shard [{}] that is finalizing, letting it finish", entry.snapshotId(), shard.getKey());
                                         break;
                                     case DONE:
-                                        logger.debug("[{}] trying to cancel snapshot on the shard [{}] that is already done, updating status on the master", entry.snapshotId(), shard.key);
-                                        updateIndexShardSnapshotStatus(entry.snapshotId(), shard.key,
+                                        logger.debug("[{}] trying to cancel snapshot on the shard [{}] that is already done, updating status on the master", entry.snapshotId(), shard.getKey());
+                                        updateIndexShardSnapshotStatus(entry.snapshotId(), shard.getKey(),
                                                 new SnapshotsInProgress.ShardSnapshotStatus(event.state().nodes().localNodeId(), SnapshotsInProgress.State.SUCCESS));
                                         break;
                                     case FAILURE:
-                                        logger.debug("[{}] trying to cancel snapshot on the shard [{}] that has already failed, updating status on the master", entry.snapshotId(), shard.key);
-                                        updateIndexShardSnapshotStatus(entry.snapshotId(), shard.key,
+                                        logger.debug("[{}] trying to cancel snapshot on the shard [{}] that has already failed, updating status on the master", entry.snapshotId(), shard.getKey());
+                                        updateIndexShardSnapshotStatus(entry.snapshotId(), shard.getKey(),
                                                 new SnapshotsInProgress.ShardSnapshotStatus(event.state().nodes().localNodeId(), SnapshotsInProgress.State.FAILED, snapshotStatus.failure()));
                                         break;
                                     default:
@@ -277,7 +273,7 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
         // If startup of these shards fails later, we don't want to try starting these shards again
         shutdownLock.lock();
         try {
-            shardSnapshots = unmodifiableMap(survivors);
+            shardSnapshots = ImmutableMap.copyOf(survivors);
             if (shardSnapshots.isEmpty()) {
                 // Notify all waiting threads that no more snapshots
                 shutdownCondition.signalAll();
@@ -372,7 +368,7 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
             if (snapshot.state() == SnapshotsInProgress.State.STARTED || snapshot.state() == SnapshotsInProgress.State.ABORTED) {
                 Map<ShardId, IndexShardSnapshotStatus> localShards = currentSnapshotShards(snapshot.snapshotId());
                 if (localShards != null) {
-                    ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> masterShards = snapshot.shards();
+                    Map<ShardId, SnapshotsInProgress.ShardSnapshotStatus> masterShards = snapshot.shards();
                     for(Map.Entry<ShardId, IndexShardSnapshotStatus> localShard : localShards.entrySet()) {
                         ShardId shardId = localShard.getKey();
                         IndexShardSnapshotStatus localShardStatus = localShard.getValue();
@@ -404,7 +400,7 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
     private static class SnapshotShards {
         private final Map<ShardId, IndexShardSnapshotStatus> shards;
 
-        private SnapshotShards(Map<ShardId, IndexShardSnapshotStatus> shards) {
+        private SnapshotShards(ImmutableMap<ShardId, IndexShardSnapshotStatus> shards) {
             this.shards = shards;
         }
     }
@@ -522,7 +518,7 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
                     int changedCount = 0;
                     final List<SnapshotsInProgress.Entry> entries = new ArrayList<>();
                     for (SnapshotsInProgress.Entry entry : snapshots.entries()) {
-                        ImmutableOpenMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards = ImmutableOpenMap.builder();
+                        final Map<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards = new HashMap<>();
                         boolean updated = false;
 
                         for (int i = 0; i < batchSize; i++) {
@@ -542,11 +538,11 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
 
                         if (updated) {
                             if (completed(shards.values()) == false) {
-                                entries.add(new SnapshotsInProgress.Entry(entry, shards.build()));
+                                entries.add(new SnapshotsInProgress.Entry(entry, ImmutableMap.copyOf(shards)));
                             } else {
                                 // Snapshot is finished - mark it as done
                                 // TODO: Add PARTIAL_SUCCESS status?
-                                SnapshotsInProgress.Entry updatedEntry = new SnapshotsInProgress.Entry(entry, SnapshotsInProgress.State.SUCCESS, shards.build());
+                                SnapshotsInProgress.Entry updatedEntry = new SnapshotsInProgress.Entry(entry, SnapshotsInProgress.State.SUCCESS, ImmutableMap.copyOf(shards));
                                 entries.add(updatedEntry);
                                 // Finalize snapshot in the repository
                                 snapshotsService.endSnapshot(updatedEntry);
diff --git a/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java b/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java
index 0b4d041..d89d260 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java
@@ -19,26 +19,15 @@
 
 package org.elasticsearch.snapshots;
 
-import com.carrotsearch.hppc.cursors.ObjectCursor;
-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.CollectionUtil;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.search.ShardSearchFailure;
 import org.elasticsearch.action.support.IndicesOptions;
-import org.elasticsearch.cluster.ClusterChangedEvent;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ClusterStateListener;
-import org.elasticsearch.cluster.ClusterStateUpdateTask;
-import org.elasticsearch.cluster.SnapshotsInProgress;
+import org.elasticsearch.cluster.*;
 import org.elasticsearch.cluster.SnapshotsInProgress.ShardSnapshotStatus;
 import org.elasticsearch.cluster.SnapshotsInProgress.State;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.metadata.RepositoriesMetaData;
-import org.elasticsearch.cluster.metadata.SnapshotId;
+import org.elasticsearch.cluster.metadata.*;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.IndexRoutingTable;
@@ -46,7 +35,6 @@ import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
 import org.elasticsearch.cluster.routing.RoutingTable;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
@@ -62,18 +50,9 @@ import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Set;
+import java.util.*;
 import java.util.concurrent.CopyOnWriteArrayList;
 
-import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.cluster.SnapshotsInProgress.completed;
 
 /**
@@ -247,7 +226,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
         }
         validate(new SnapshotId(request.repository(), request.name()));
     }
-
+    
     private static void validate(SnapshotId snapshotId) {
         String name = snapshotId.getSnapshot();
         if (!Strings.hasLength(name)) {
@@ -318,7 +297,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                     for (SnapshotsInProgress.Entry entry : snapshots.entries()) {
                         if (entry.snapshotId().equals(snapshot.snapshotId())) {
                             // Replace the snapshot that was just created
-                            ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards = shards(currentState, entry.indices());
+                            ImmutableMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards = shards(currentState, entry.indices());
                             if (!partial) {
                                 Tuple<Set<String>, Set<String>> indicesWithMissingShards = indicesWithMissingShards(shards, currentState.metaData());
                                 Set<String> missing = indicesWithMissingShards.v1();
@@ -468,9 +447,9 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
      * @param snapshotId snapshot id
      * @return map of shard id to snapshot status
      */
-    public Map<ShardId, IndexShardSnapshotStatus> snapshotShards(SnapshotId snapshotId) throws IOException {
+    public ImmutableMap<ShardId, IndexShardSnapshotStatus> snapshotShards(SnapshotId snapshotId) throws IOException {
         validate(snapshotId);
-        Map<ShardId, IndexShardSnapshotStatus> shardStatus = new HashMap<>();
+        ImmutableMap.Builder<ShardId, IndexShardSnapshotStatus> shardStatusBuilder = ImmutableMap.builder();
         Repository repository = repositoriesService.repository(snapshotId.getRepository());
         IndexShardRepository indexShardRepository = repositoriesService.indexShardRepository(snapshotId.getRepository());
         Snapshot snapshot = repository.readSnapshot(snapshotId);
@@ -486,15 +465,15 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                         IndexShardSnapshotStatus shardSnapshotStatus = new IndexShardSnapshotStatus();
                         shardSnapshotStatus.updateStage(IndexShardSnapshotStatus.Stage.FAILURE);
                         shardSnapshotStatus.failure(shardFailure.reason());
-                        shardStatus.put(shardId, shardSnapshotStatus);
+                        shardStatusBuilder.put(shardId, shardSnapshotStatus);
                     } else {
                         IndexShardSnapshotStatus shardSnapshotStatus = indexShardRepository.snapshotStatus(snapshotId, snapshot.version(), shardId);
-                        shardStatus.put(shardId, shardSnapshotStatus);
+                        shardStatusBuilder.put(shardId, shardSnapshotStatus);
                     }
                 }
             }
         }
-        return unmodifiableMap(shardStatus);
+        return shardStatusBuilder.build();
     }
 
 
@@ -546,23 +525,23 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                         SnapshotsInProgress.Entry updatedSnapshot = snapshot;
                         boolean snapshotChanged = false;
                         if (snapshot.state() == State.STARTED || snapshot.state() == State.ABORTED) {
-                            ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableOpenMap.builder();
-                            for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : snapshot.shards()) {
-                                ShardSnapshotStatus shardStatus = shardEntry.value;
+                            ImmutableMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableMap.builder();
+                            for (ImmutableMap.Entry<ShardId, ShardSnapshotStatus> shardEntry : snapshot.shards().entrySet()) {
+                                ShardSnapshotStatus shardStatus = shardEntry.getValue();
                                 if (!shardStatus.state().completed() && shardStatus.nodeId() != null) {
                                     if (nodes.nodeExists(shardStatus.nodeId())) {
-                                        shards.put(shardEntry.key, shardEntry.value);
+                                        shards.put(shardEntry);
                                     } else {
                                         // TODO: Restart snapshot on another node?
                                         snapshotChanged = true;
-                                        logger.warn("failing snapshot of shard [{}] on closed node [{}]", shardEntry.key, shardStatus.nodeId());
-                                        shards.put(shardEntry.key, new ShardSnapshotStatus(shardStatus.nodeId(), State.FAILED, "node shutdown"));
+                                        logger.warn("failing snapshot of shard [{}] on closed node [{}]", shardEntry.getKey(), shardStatus.nodeId());
+                                        shards.put(shardEntry.getKey(), new ShardSnapshotStatus(shardStatus.nodeId(), State.FAILED, "node shutdown"));
                                     }
                                 }
                             }
                             if (snapshotChanged) {
                                 changed = true;
-                                ImmutableOpenMap<ShardId, ShardSnapshotStatus> shardsMap = shards.build();
+                                ImmutableMap<ShardId, ShardSnapshotStatus> shardsMap = shards.build();
                                 if (!snapshot.state().completed() && completed(shardsMap.values())) {
                                     updatedSnapshot = new SnapshotsInProgress.Entry(snapshot, State.SUCCESS, shardsMap);
                                     endSnapshot(updatedSnapshot);
@@ -617,7 +596,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                         for (final SnapshotsInProgress.Entry snapshot : snapshots.entries()) {
                             SnapshotsInProgress.Entry updatedSnapshot = snapshot;
                             if (snapshot.state() == State.STARTED) {
-                                ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards = processWaitingShards(snapshot.shards(), routingTable);
+                                Map<ShardId, ShardSnapshotStatus> shards = processWaitingShards(snapshot.shards(), routingTable);
                                 if (shards != null) {
                                     changed = true;
                                     if (!snapshot.state().completed() && completed(shards.values())) {
@@ -646,14 +625,13 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
         }
     }
 
-    private ImmutableOpenMap<ShardId, ShardSnapshotStatus> processWaitingShards(
-            ImmutableOpenMap<ShardId, ShardSnapshotStatus> snapshotShards, RoutingTable routingTable) {
+    private Map<ShardId, ShardSnapshotStatus> processWaitingShards(Map<ShardId, ShardSnapshotStatus> snapshotShards, RoutingTable routingTable) {
         boolean snapshotChanged = false;
-        ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableOpenMap.builder();
-        for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : snapshotShards) {
-            ShardSnapshotStatus shardStatus = shardEntry.value;
-            ShardId shardId = shardEntry.key;
+        ImmutableMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableMap.builder();
+        for (ImmutableMap.Entry<ShardId, ShardSnapshotStatus> shardEntry : snapshotShards.entrySet()) {
+            ShardSnapshotStatus shardStatus = shardEntry.getValue();
             if (shardStatus.state() == State.WAITING) {
+                ShardId shardId = shardEntry.getKey();
                 IndexRoutingTable indexShardRoutingTable = routingTable.index(shardId.getIndex());
                 if (indexShardRoutingTable != null) {
                     IndexShardRoutingTable shardRouting = indexShardRoutingTable.shard(shardId.id());
@@ -661,22 +639,22 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                         if (shardRouting.primaryShard().started()) {
                             // Shard that we were waiting for has started on a node, let's process it
                             snapshotChanged = true;
-                            logger.trace("starting shard that we were waiting for [{}] on node [{}]", shardId, shardStatus.nodeId());
-                            shards.put(shardId, new ShardSnapshotStatus(shardRouting.primaryShard().currentNodeId()));
+                            logger.trace("starting shard that we were waiting for [{}] on node [{}]", shardEntry.getKey(), shardStatus.nodeId());
+                            shards.put(shardEntry.getKey(), new ShardSnapshotStatus(shardRouting.primaryShard().currentNodeId()));
                             continue;
                         } else if (shardRouting.primaryShard().initializing() || shardRouting.primaryShard().relocating()) {
                             // Shard that we were waiting for hasn't started yet or still relocating - will continue to wait
-                            shards.put(shardId, shardStatus);
+                            shards.put(shardEntry);
                             continue;
                         }
                     }
                 }
                 // Shard that we were waiting for went into unassigned state or disappeared - giving up
                 snapshotChanged = true;
-                logger.warn("failing snapshot of shard [{}] on unassigned shard [{}]", shardId, shardStatus.nodeId());
-                shards.put(shardId, new ShardSnapshotStatus(shardStatus.nodeId(), State.FAILED, "shard is unassigned"));
+                logger.warn("failing snapshot of shard [{}] on unassigned shard [{}]", shardEntry.getKey(), shardStatus.nodeId());
+                shards.put(shardEntry.getKey(), new ShardSnapshotStatus(shardStatus.nodeId(), State.FAILED, "shard is unassigned"));
             } else {
-                shards.put(shardId, shardStatus);
+                shards.put(shardEntry);
             }
         }
         if (snapshotChanged) {
@@ -691,10 +669,10 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
         if (curr != null) {
             for (SnapshotsInProgress.Entry entry : curr.entries()) {
                 if (entry.state() == State.STARTED && !entry.waitingIndices().isEmpty()) {
-                    for (ObjectCursor<String> index : entry.waitingIndices().keys()) {
-                        if (event.indexRoutingTableChanged(index.value)) {
-                            IndexRoutingTable indexShardRoutingTable = event.state().getRoutingTable().index(index.value);
-                            for (ShardId shardId : entry.waitingIndices().get(index.value)) {
+                    for (String index : entry.waitingIndices().keySet()) {
+                        if (event.indexRoutingTableChanged(index)) {
+                            IndexRoutingTable indexShardRoutingTable = event.state().getRoutingTable().index(index);
+                            for (ShardId shardId : entry.waitingIndices().get(index)) {
                                 ShardRouting shardRouting = indexShardRoutingTable.shard(shardId.id()).primaryShard();
                                 if (shardRouting != null && (shardRouting.started() || shardRouting.unassigned())) {
                                     return true;
@@ -721,8 +699,8 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                 return true;
             }
             for (DiscoveryNode node : event.nodesDelta().removedNodes()) {
-                for (ObjectCursor<ShardSnapshotStatus> shardStatus : snapshot.shards().values()) {
-                    if (!shardStatus.value.state().completed() && node.getId().equals(shardStatus.value.nodeId())) {
+                for (ShardSnapshotStatus shardStatus : snapshot.shards().values()) {
+                    if (!shardStatus.state().completed() && node.getId().equals(shardStatus.nodeId())) {
                         // At least one shard was running on the removed node - we need to fail it
                         return true;
                     }
@@ -738,15 +716,15 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
      * @param shards list of shard statuses
      * @return list of failed and closed indices
      */
-    private Tuple<Set<String>, Set<String>> indicesWithMissingShards(ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards, MetaData metaData) {
+    private Tuple<Set<String>, Set<String>> indicesWithMissingShards(Map<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards, MetaData metaData) {
         Set<String> missing = new HashSet<>();
         Set<String> closed = new HashSet<>();
-        for (ObjectObjectCursor<ShardId, SnapshotsInProgress.ShardSnapshotStatus> entry : shards) {
-            if (entry.value.state() == State.MISSING) {
-                if (metaData.hasIndex(entry.key.getIndex()) && metaData.index(entry.key.getIndex()).getState() == IndexMetaData.State.CLOSE) {
-                    closed.add(entry.key.getIndex());
+        for (Map.Entry<ShardId, SnapshotsInProgress.ShardSnapshotStatus> entry : shards.entrySet()) {
+            if (entry.getValue().state() == State.MISSING) {
+                if (metaData.hasIndex(entry.getKey().getIndex()) && metaData.index(entry.getKey().getIndex()).getState() == IndexMetaData.State.CLOSE) {
+                    closed.add(entry.getKey().getIndex());
                 } else {
-                    missing.add(entry.key.getIndex());
+                    missing.add(entry.getKey().getIndex());
                 }
             }
         }
@@ -783,9 +761,9 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                     logger.trace("[{}] finalizing snapshot in repository, state: [{}], failure[{}]", snapshotId, entry.state(), failure);
                     ArrayList<ShardSearchFailure> failures = new ArrayList<>();
                     ArrayList<SnapshotShardFailure> shardFailures = new ArrayList<>();
-                    for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardStatus : entry.shards()) {
-                        ShardId shardId = shardStatus.key;
-                        ShardSnapshotStatus status = shardStatus.value;
+                    for (Map.Entry<ShardId, ShardSnapshotStatus> shardStatus : entry.shards().entrySet()) {
+                        ShardId shardId = shardStatus.getKey();
+                        ShardSnapshotStatus status = shardStatus.getValue();
                         if (status.state().failed()) {
                             failures.add(new ShardSearchFailure(status.reason(), new SearchShardTarget(status.nodeId(), shardId.getIndex(), shardId.id())));
                             shardFailures.add(new SnapshotShardFailure(status.nodeId(), shardId.getIndex(), shardId.id(), status.reason()));
@@ -886,16 +864,16 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                 } else {
                     // This snapshot is currently running - stopping shards first
                     waitForSnapshot = true;
-                    ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards;
+                    Map<ShardId, ShardSnapshotStatus> shards;
                     if (snapshot.state() == State.STARTED && snapshot.shards() != null) {
                         // snapshot is currently running - stop started shards
-                        ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shardsBuilder = ImmutableOpenMap.builder();
-                        for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : snapshot.shards()) {
-                            ShardSnapshotStatus status = shardEntry.value;
+                        ImmutableMap.Builder<ShardId, ShardSnapshotStatus> shardsBuilder = ImmutableMap.builder();
+                        for (ImmutableMap.Entry<ShardId, ShardSnapshotStatus> shardEntry : snapshot.shards().entrySet()) {
+                            ShardSnapshotStatus status = shardEntry.getValue();
                             if (!status.state().completed()) {
-                                shardsBuilder.put(shardEntry.key, new ShardSnapshotStatus(status.nodeId(), State.ABORTED));
+                                shardsBuilder.put(shardEntry.getKey(), new ShardSnapshotStatus(status.nodeId(), State.ABORTED));
                             } else {
-                                shardsBuilder.put(shardEntry.key, status);
+                                shardsBuilder.put(shardEntry.getKey(), status);
                             }
                         }
                         shards = shardsBuilder.build();
@@ -906,10 +884,9 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                     } else {
                         boolean hasUncompletedShards = false;
                         // Cleanup in case a node gone missing and snapshot wasn't updated for some reason
-                        for (ObjectCursor<ShardSnapshotStatus> shardStatus : snapshot.shards().values()) {
+                        for (ShardSnapshotStatus shardStatus : snapshot.shards().values()) {
                             // Check if we still have shard running on existing nodes
-                            if (shardStatus.value.state().completed() == false && shardStatus.value.nodeId() != null
-                                    && currentState.nodes().get(shardStatus.value.nodeId()) != null) {
+                            if (shardStatus.state().completed() == false && shardStatus.nodeId() != null && currentState.nodes().get(shardStatus.nodeId()) != null) {
                                 hasUncompletedShards = true;
                                 break;
                             }
@@ -1014,8 +991,8 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
      * @param indices      list of indices to be snapshotted
      * @return list of shard to be included into current snapshot
      */
-    private ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards(ClusterState clusterState, List<String> indices) {
-        ImmutableOpenMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus> builder = ImmutableOpenMap.builder();
+    private ImmutableMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards(ClusterState clusterState, List<String> indices) {
+        ImmutableMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus> builder = ImmutableMap.builder();
         MetaData metaData = clusterState.metaData();
         for (String index : indices) {
             IndexMetaData indexMetaData = metaData.index(index);
diff --git a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
index d20f600..85de5f6 100644
--- a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
+++ b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.threadpool;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.Counter;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.component.AbstractComponent;
@@ -56,7 +57,7 @@ import java.util.concurrent.ThreadFactory;
 import java.util.concurrent.ThreadPoolExecutor;
 import java.util.concurrent.TimeUnit;
 
-import static java.util.Collections.unmodifiableMap;
+import static org.elasticsearch.common.collect.MapBuilder.newMapBuilder;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.unit.SizeValue.parseSizeValue;
 import static org.elasticsearch.common.unit.TimeValue.timeValueMinutes;
@@ -88,7 +89,7 @@ public class ThreadPool extends AbstractComponent {
 
     public static final String THREADPOOL_GROUP = "threadpool.";
 
-    private volatile Map<String, ExecutorHolder> executors;
+    private volatile ImmutableMap<String, ExecutorHolder> executors;
 
     private final Map<String, Settings> defaultExecutorTypeSettings;
 
@@ -117,38 +118,26 @@ public class ThreadPool extends AbstractComponent {
         int availableProcessors = EsExecutors.boundedNumberOfProcessors(settings);
         int halfProcMaxAt5 = Math.min(((availableProcessors + 1) / 2), 5);
         int halfProcMaxAt10 = Math.min(((availableProcessors + 1) / 2), 10);
-        Map<String, Settings> defaultExecutorTypeSettings = new HashMap<>();
-        defaultExecutorTypeSettings.put(Names.GENERIC, settingsBuilder().put("type", "cached").put("keep_alive", "30s").build());
-        defaultExecutorTypeSettings.put(Names.INDEX,
-                settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 200).build());
-        defaultExecutorTypeSettings.put(Names.BULK,
-                settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 50).build());
-        defaultExecutorTypeSettings.put(Names.GET,
-                settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 1000).build());
-        defaultExecutorTypeSettings.put(Names.SEARCH,
-                settingsBuilder().put("type", "fixed").put("size", ((availableProcessors * 3) / 2) + 1).put("queue_size", 1000).build());
-        defaultExecutorTypeSettings.put(Names.SUGGEST,
-                settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 1000).build());
-        defaultExecutorTypeSettings.put(Names.PERCOLATE,
-                settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 1000).build());
-        defaultExecutorTypeSettings  .put(Names.MANAGEMENT, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", 5).build());
-        // no queue as this means clients will need to handle rejections on listener queue even if the operation succeeded
-        // the assumption here is that the listeners should be very lightweight on the listeners side
-        defaultExecutorTypeSettings.put(Names.LISTENER, settingsBuilder().put("type", "fixed").put("size", halfProcMaxAt10).build());
-        defaultExecutorTypeSettings.put(Names.FLUSH,
-                settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt5).build());
-        defaultExecutorTypeSettings.put(Names.REFRESH,
-                settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt10).build());
-        defaultExecutorTypeSettings.put(Names.WARMER,
-                settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt5).build());
-        defaultExecutorTypeSettings.put(Names.SNAPSHOT,
-                settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt5).build());
-        defaultExecutorTypeSettings.put(Names.OPTIMIZE, settingsBuilder().put("type", "fixed").put("size", 1).build());
-        defaultExecutorTypeSettings.put(Names.FETCH_SHARD_STARTED,
-                settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", availableProcessors * 2).build());
-        defaultExecutorTypeSettings.put(Names.FETCH_SHARD_STORE,
-                settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", availableProcessors * 2).build());
-        this.defaultExecutorTypeSettings = unmodifiableMap(defaultExecutorTypeSettings);
+        defaultExecutorTypeSettings = ImmutableMap.<String, Settings>builder()
+                .put(Names.GENERIC, settingsBuilder().put("type", "cached").put("keep_alive", "30s").build())
+                .put(Names.INDEX, settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 200).build())
+                .put(Names.BULK, settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 50).build())
+                .put(Names.GET, settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 1000).build())
+                .put(Names.SEARCH, settingsBuilder().put("type", "fixed").put("size", ((availableProcessors * 3) / 2) + 1).put("queue_size", 1000).build())
+                .put(Names.SUGGEST, settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 1000).build())
+                .put(Names.PERCOLATE, settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 1000).build())
+                .put(Names.MANAGEMENT, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", 5).build())
+                // no queue as this means clients will need to handle rejections on listener queue even if the operation succeeded
+                // the assumption here is that the listeners should be very lightweight on the listeners side
+                .put(Names.LISTENER, settingsBuilder().put("type", "fixed").put("size", halfProcMaxAt10).build())
+                .put(Names.FLUSH, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt5).build())
+                .put(Names.REFRESH, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt10).build())
+                .put(Names.WARMER, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt5).build())
+                .put(Names.SNAPSHOT, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt5).build())
+                .put(Names.OPTIMIZE, settingsBuilder().put("type", "fixed").put("size", 1).build())
+                .put(Names.FETCH_SHARD_STARTED, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", availableProcessors * 2).build())
+                .put(Names.FETCH_SHARD_STORE, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", availableProcessors * 2).build())
+                .build();
 
         Map<String, ExecutorHolder> executors = new HashMap<>();
         for (Map.Entry<String, Settings> executor : defaultExecutorTypeSettings.entrySet()) {
@@ -167,7 +156,7 @@ public class ThreadPool extends AbstractComponent {
         if (!executors.get(Names.GENERIC).info.getType().equals("cached")) {
             throw new IllegalArgumentException("generic thread pool must be of type cached");
         }
-        this.executors = unmodifiableMap(executors);
+        this.executors = ImmutableMap.copyOf(executors);
         this.scheduler = new ScheduledThreadPoolExecutor(1, EsExecutors.daemonThreadFactory(settings, "scheduler"), new EsAbortPolicy());
         this.scheduler.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);
         this.scheduler.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);
@@ -457,9 +446,7 @@ public class ThreadPool extends AbstractComponent {
             ExecutorHolder oldExecutorHolder = executors.get(executor.getKey());
             ExecutorHolder newExecutorHolder = rebuild(executor.getKey(), oldExecutorHolder, updatedSettings, executor.getValue());
             if (!oldExecutorHolder.equals(newExecutorHolder)) {
-                Map<String, ExecutorHolder> newExecutors = new HashMap<>(executors);
-                newExecutors.put(executor.getKey(), newExecutorHolder);
-                executors = unmodifiableMap(newExecutors);
+                executors = newMapBuilder(executors).put(executor.getKey(), newExecutorHolder).immutableMap();
                 if (!oldExecutorHolder.executor().equals(newExecutorHolder.executor()) && oldExecutorHolder.executor() instanceof EsThreadPoolExecutor) {
                     retiredExecutors.add(oldExecutorHolder);
                     ((EsThreadPoolExecutor) oldExecutorHolder.executor()).shutdown(new ExecutorShutdownListener(oldExecutorHolder));
@@ -479,9 +466,7 @@ public class ThreadPool extends AbstractComponent {
             // case the settings contains a thread pool not defined in the initial settings in the constructor. The if
             // statement will then fail and so this prevents the addition of new thread groups at runtime, which is desired.
             if (!newExecutorHolder.equals(oldExecutorHolder)) {
-                Map<String, ExecutorHolder> newExecutors = new HashMap<>(executors);
-                newExecutors.put(entry.getKey(), newExecutorHolder);
-                executors = unmodifiableMap(newExecutors);
+                executors = newMapBuilder(executors).put(entry.getKey(), newExecutorHolder).immutableMap();
                 if (!oldExecutorHolder.executor().equals(newExecutorHolder.executor()) && oldExecutorHolder.executor() instanceof EsThreadPoolExecutor) {
                     retiredExecutors.add(oldExecutorHolder);
                     ((EsThreadPoolExecutor) oldExecutorHolder.executor()).shutdown(new ExecutorShutdownListener(oldExecutorHolder));
diff --git a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
index 853497d..5cf9ed3 100644
--- a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
+++ b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.transport.netty;
 
+import java.nio.charset.StandardCharsets;
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.node.DiscoveryNode;
@@ -91,7 +93,6 @@ import java.net.InetSocketAddress;
 import java.net.SocketAddress;
 import java.net.UnknownHostException;
 import java.nio.channels.CancelledKeyException;
-import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
@@ -114,7 +115,6 @@ import java.util.concurrent.locks.ReentrantReadWriteLock;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
-import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_BLOCKING;
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_BLOCKING_CLIENT;
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_BLOCKING_SERVER;
@@ -340,7 +340,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
 
     @Override
     public Map<String, BoundTransportAddress> profileBoundAddresses() {
-        return unmodifiableMap(new HashMap<>(profileBoundAddresses));
+        return ImmutableMap.copyOf(profileBoundAddresses);
     }
 
     private InetSocketAddress createPublishAddress(String publishHost, int publishPort) {
@@ -453,7 +453,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
             bindServerBootstrap(name, hostAddress, settings);
         }
     }
-
+        
     private void bindServerBootstrap(final String name, final InetAddress hostAddress, Settings profileSettings) {
 
         String port = profileSettings.get("port");
@@ -657,15 +657,15 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
 
     @Override
     public TransportAddress[] addressesFromString(String address, int perAddressLimit) throws Exception {
-        return parse(address, settings.get("transport.profiles.default.port",
-                              settings.get("transport.netty.port",
-                              settings.get("transport.tcp.port",
+        return parse(address, settings.get("transport.profiles.default.port", 
+                              settings.get("transport.netty.port", 
+                              settings.get("transport.tcp.port", 
                               DEFAULT_PORT_RANGE))), perAddressLimit);
     }
-
+    
     // this code is a take on guava's HostAndPort, like a HostAndPortRange
-
-    // pattern for validating ipv6 bracked addresses.
+    
+    // pattern for validating ipv6 bracked addresses. 
     // not perfect, but PortsRange should take care of any port range validation, not a regex
     private static final Pattern BRACKET_PATTERN = Pattern.compile("^\\[(.*:.*)\\](?::([\\d\\-]*))?$");
 
@@ -698,12 +698,12 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
             }
           }
         }
-
+        
         // if port isn't specified, fill with the default
         if (portString == null || portString.isEmpty()) {
             portString = defaultPortRange;
         }
-
+        
         // generate address for each port in the range
         Set<InetAddress> addresses = new HashSet<>(Arrays.asList(InetAddress.getAllByName(host)));
         List<TransportAddress> transportAddresses = new ArrayList<>();
diff --git a/core/src/main/java/org/elasticsearch/tribe/TribeService.java b/core/src/main/java/org/elasticsearch/tribe/TribeService.java
index 75b8176..36ea7fc 100644
--- a/core/src/main/java/org/elasticsearch/tribe/TribeService.java
+++ b/core/src/main/java/org/elasticsearch/tribe/TribeService.java
@@ -19,15 +19,10 @@
 
 package org.elasticsearch.tribe;
 
-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.support.master.TransportMasterNodeReadAction;
-import org.elasticsearch.cluster.ClusterChangedEvent;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ClusterStateListener;
-import org.elasticsearch.cluster.ClusterStateUpdateTask;
+import org.elasticsearch.cluster.*;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.block.ClusterBlocks;
@@ -38,6 +33,7 @@ import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.IndexRoutingTable;
 import org.elasticsearch.cluster.routing.RoutingTable;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.regex.Regex;
@@ -50,15 +46,9 @@ import org.elasticsearch.node.NodeBuilder;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.elasticsearch.rest.RestStatus;
 
-import java.util.EnumSet;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
+import java.util.*;
 import java.util.concurrent.CopyOnWriteArrayList;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * The tribe service holds a list of node clients connected to a list of tribe members, and uses their
  * cluster state events to update this local node cluster state with the merged view of it.
@@ -244,12 +234,8 @@ public class TribeService extends AbstractLifecycleComponent<TribeService> {
                     for (DiscoveryNode tribe : tribeState.nodes()) {
                         if (currentState.nodes().get(tribe.id()) == null) {
                             // a new node, add it, but also add the tribe name to the attributes
-                            Map<String, String> tribeAttr = new HashMap<>();
-                            for (ObjectObjectCursor<String, String> attr : tribe.attributes()) {
-                                tribeAttr.put(attr.key, attr.value);
-                            }
-                            tribeAttr.put(TRIBE_NAME, tribeName);
-                            DiscoveryNode discoNode = new DiscoveryNode(tribe.name(), tribe.id(), tribe.getHostName(), tribe.getHostAddress(), tribe.address(), unmodifiableMap(tribeAttr), tribe.version());
+                            ImmutableMap<String, String> tribeAttr = MapBuilder.newMapBuilder(tribe.attributes()).put(TRIBE_NAME, tribeName).immutableMap();
+                            DiscoveryNode discoNode = new DiscoveryNode(tribe.name(), tribe.id(), tribe.getHostName(), tribe.getHostAddress(), tribe.address(), tribeAttr, tribe.version());
                             logger.info("[{}] adding node [{}]", tribeName, discoNode);
                             nodes.put(discoNode);
                         }
@@ -315,7 +301,7 @@ public class TribeService extends AbstractLifecycleComponent<TribeService> {
                         }
                     }
 
-                    return ClusterState.builder(currentState).incrementVersion().blocks(blocks).nodes(nodes).metaData(metaData).routingTable(routingTable.build()).build();
+                    return ClusterState.builder(currentState).incrementVersion().blocks(blocks).nodes(nodes).metaData(metaData).routingTable(routingTable).build();
                 }
 
                 private void removeIndex(ClusterBlocks.Builder blocks, MetaData.Builder metaData, RoutingTable.Builder routingTable, IndexMetaData index) {
diff --git a/core/src/main/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/core/src/main/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
index 52134e4..06b50d3 100644
--- a/core/src/main/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
+++ b/core/src/main/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
@@ -1,3 +1 @@
-org.elasticsearch.index.codec.postingsformat.Elasticsearch090PostingsFormat
-org.elasticsearch.search.suggest.completion.Completion090PostingsFormat
-org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat
+org.elasticsearch.search.suggest.completion.Completion090PostingsFormat
\ No newline at end of file
diff --git a/core/src/main/resources/org/elasticsearch/bootstrap/security.policy b/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
index 1126824..76b5a58 100644
--- a/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
+++ b/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
@@ -52,8 +52,8 @@ grant codeBase "${es.security.plugin.discovery-ec2}" {
   permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
 };
 
-grant codeBase "${es.security.plugin.cloud-gce}" {
-  // needed because of problems in cloud-gce
+grant codeBase "${es.security.plugin.discovery-gce}" {
+  // needed because of problems in discovery-gce
   permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
 };
 
diff --git a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
index 0811ec7..3577215 100644
--- a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
+++ b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
@@ -38,10 +38,10 @@ OFFICIAL PLUGINS
     - analysis-phonetic
     - analysis-smartcn
     - analysis-stempel
-    - cloud-gce
     - delete-by-query
     - discovery-azure
     - discovery-ec2
+    - discovery-gce
     - discovery-multicast
     - lang-expression
     - lang-groovy
diff --git a/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatterTests.java b/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatterTests.java
index ef7a9ac..dc176ae 100644
--- a/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatterTests.java
+++ b/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatterTests.java
@@ -1,19 +1,20 @@
 /*
- * Licensed to Elasticsearch under one
- * or more contributor license agreements. See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership. Elasticsearch licenses this
- * file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
  *
- *     http://www.apache.org/licenses/LICENSE-2.0
+ *    http://www.apache.org/licenses/LICENSE-2.0
  *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
  */
 
 package org.apache.lucene.search.postingshighlight;
diff --git a/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighterTests.java b/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighterTests.java
index 450382c..58728d8 100644
--- a/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighterTests.java
+++ b/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighterTests.java
@@ -1,19 +1,20 @@
 /*
- * Licensed to Elasticsearch under one
- * or more contributor license agreements. See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership. Elasticsearch licenses this
- * file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
  *
- *     http://www.apache.org/licenses/LICENSE-2.0
+ *    http://www.apache.org/licenses/LICENSE-2.0
  *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
  */
 
 package org.apache.lucene.search.postingshighlight;
diff --git a/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomSeparatorBreakIteratorTests.java b/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomSeparatorBreakIteratorTests.java
index 3b63f76..1be578f 100644
--- a/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomSeparatorBreakIteratorTests.java
+++ b/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomSeparatorBreakIteratorTests.java
@@ -1,20 +1,20 @@
 /*
-Licensed to Elasticsearch under one or more contributor
-license agreements. See the NOTICE file distributed with
-this work for additional information regarding copyright
-ownership. Elasticsearch licenses this file to you under
-the Apache License, Version 2.0 (the "License"); you may
-not use this file except in compliance with the License.
-You may obtain a copy of the License at
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
  *
-   http://www.apache.org/licenses/LICENSE-2.0
+ *    http://www.apache.org/licenses/LICENSE-2.0
  *
-Unless required by applicable law or agreed to in writing,
-software distributed under the License is distributed on an
-"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-KIND, either express or implied.  See the License for the
-specific language governing permissions and limitations
-under the License.
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
  */
 
 package org.apache.lucene.search.postingshighlight;
diff --git a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
index 9a260f0..55dc2e4 100644
--- a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
+++ b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
@@ -20,7 +20,6 @@ package org.elasticsearch;
 
 import com.fasterxml.jackson.core.JsonLocation;
 import com.fasterxml.jackson.core.JsonParseException;
-
 import org.apache.lucene.util.Constants;
 import org.elasticsearch.action.FailedNodeException;
 import org.elasticsearch.action.RoutingMissingException;
@@ -31,12 +30,7 @@ import org.elasticsearch.client.AbstractClientHeadersTestCase;
 import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.routing.IllegalShardRoutingStateException;
-import org.elasticsearch.cluster.routing.RoutingTableValidation;
-import org.elasticsearch.cluster.routing.RoutingValidationException;
-import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.cluster.routing.ShardRoutingState;
-import org.elasticsearch.cluster.routing.TestShardRouting;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.breaker.CircuitBreakingException;
 import org.elasticsearch.common.io.PathUtils;
@@ -55,7 +49,6 @@ import org.elasticsearch.common.xcontent.XContentLocation;
 import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.index.AlreadyExpiredException;
 import org.elasticsearch.index.Index;
-import org.elasticsearch.index.engine.CreateFailedEngineException;
 import org.elasticsearch.index.engine.IndexFailedEngineException;
 import org.elasticsearch.index.engine.RecoveryEngineException;
 import org.elasticsearch.index.mapper.MergeMappingException;
@@ -139,9 +132,9 @@ public class ExceptionSerializationTests extends ESTestCase {
                         Class<?> clazz = loadClass(filename);
                         if (ignore.contains(clazz) == false) {
                             if (Modifier.isAbstract(clazz.getModifiers()) == false && Modifier.isInterface(clazz.getModifiers()) == false && isEsException(clazz)) {
-                                if (ElasticsearchException.isRegistered((Class<? extends Throwable>)clazz) == false && ElasticsearchException.class.equals(clazz.getEnclosingClass()) == false) {
+                                if (ElasticsearchException.isRegistered((Class<? extends Throwable>) clazz) == false && ElasticsearchException.class.equals(clazz.getEnclosingClass()) == false) {
                                     notRegistered.add(clazz);
-                                } else if (ElasticsearchException.isRegistered((Class<? extends Throwable>)clazz)) {
+                                } else if (ElasticsearchException.isRegistered((Class<? extends Throwable>) clazz)) {
                                     registered.add(clazz);
                                     try {
                                         if (clazz.getDeclaredMethod("writeTo", StreamOutput.class) != null) {
@@ -199,7 +192,7 @@ public class ExceptionSerializationTests extends ESTestCase {
     }
 
     public static final class TestException extends ElasticsearchException {
-        public TestException(StreamInput in) throws IOException{
+        public TestException(StreamInput in) throws IOException {
             super(in);
         }
     }
@@ -247,7 +240,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals(ex.getIndex(), "foo");
         assertEquals(ex.getMessage(), "fobar");
 
-        ex = serialize(new QueryShardException((Index)null, null, null));
+        ex = serialize(new QueryShardException((Index) null, null, null));
         assertNull(ex.getIndex());
         assertNull(ex.getMessage());
     }
@@ -282,22 +275,8 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals(-3, alreadyExpiredException.now());
     }
 
-    public void testCreateFailedEngineException() throws IOException {
-        CreateFailedEngineException ex = serialize(new CreateFailedEngineException(new ShardId("idx", 2), "type", "id", null));
-        assertEquals(ex.getShardId(), new ShardId("idx", 2));
-        assertEquals("type", ex.type());
-        assertEquals("id", ex.id());
-        assertNull(ex.getCause());
-
-        ex = serialize(new CreateFailedEngineException(null, "type", "id", new NullPointerException()));
-        assertNull(ex.getShardId());
-        assertEquals("type", ex.type());
-        assertEquals("id", ex.id());
-        assertTrue(ex.getCause() instanceof NullPointerException);
-    }
-
     public void testMergeMappingException() throws IOException {
-        MergeMappingException ex = serialize(new MergeMappingException(new String[] {"one", "two"}));
+        MergeMappingException ex = serialize(new MergeMappingException(new String[]{"one", "two"}));
         assertArrayEquals(ex.failures(), new String[]{"one", "two"});
     }
 
@@ -342,7 +321,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals("the dude abides!", ex.name());
         assertEquals("index_template [the dude abides!] already exists", ex.getMessage());
 
-        ex = serialize(new IndexTemplateAlreadyExistsException((String)null));
+        ex = serialize(new IndexTemplateAlreadyExistsException((String) null));
         assertNull(ex.name());
         assertEquals("index_template [null] already exists", ex.getMessage());
     }
@@ -449,7 +428,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals(ctx.shardTarget(), ex.shard());
     }
 
-    public void testIllegalIndexShardStateException()throws IOException {
+    public void testIllegalIndexShardStateException() throws IOException {
         ShardId id = new ShardId("foo", 1);
         IndexShardState state = randomFrom(IndexShardState.values());
         IllegalIndexShardStateException ex = serialize(new IllegalIndexShardStateException(id, state, "come back later buddy"));
@@ -480,7 +459,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals("baam", ex.getMessage());
         assertTrue(ex.getCause() instanceof NullPointerException);
         assertEquals(empty.length, ex.shardFailures().length);
-        ShardSearchFailure[] one = new ShardSearchFailure[] {
+        ShardSearchFailure[] one = new ShardSearchFailure[]{
                 new ShardSearchFailure(new IllegalArgumentException("nono!"))
         };
 
@@ -521,7 +500,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals("index_template [name] missing", ex.getMessage());
         assertEquals("name", ex.name());
 
-        ex = serialize(new IndexTemplateMissingException((String)null));
+        ex = serialize(new IndexTemplateMissingException((String) null));
         assertEquals("index_template [null] missing", ex.getMessage());
         assertNull(ex.name());
     }
@@ -570,8 +549,8 @@ public class ExceptionSerializationTests extends ESTestCase {
         ex = serialize(new NotSerializableExceptionWrapper(new IllegalArgumentException("nono!")));
         assertEquals("{\"type\":\"illegal_argument_exception\",\"reason\":\"nono!\"}", toXContent(ex));
 
-        Throwable[] unknowns = new Throwable[] {
-                new JsonParseException("foobar", new JsonLocation(new Object(), 1,2,3,4)),
+        Throwable[] unknowns = new Throwable[]{
+                new JsonParseException("foobar", new JsonLocation(new Object(), 1, 2, 3, 4)),
                 new ClassCastException("boom boom boom"),
                 new IOException("booom")
         };
@@ -609,7 +588,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         UnknownHeaderException uhe = new UnknownHeaderException("msg", status);
         uhe.addHeader("foo", "foo", "bar");
 
-        ElasticsearchException serialize = serialize((ElasticsearchException)uhe);
+        ElasticsearchException serialize = serialize((ElasticsearchException) uhe);
         assertTrue(serialize instanceof NotSerializableExceptionWrapper);
         NotSerializableExceptionWrapper e = (NotSerializableExceptionWrapper) serialize;
         assertEquals("msg", e.getMessage());
@@ -684,7 +663,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         ids.put(19, org.elasticsearch.ResourceNotFoundException.class);
         ids.put(20, org.elasticsearch.transport.ActionTransportException.class);
         ids.put(21, org.elasticsearch.ElasticsearchGenerationException.class);
-        ids.put(22, org.elasticsearch.index.engine.CreateFailedEngineException.class);
+        ids.put(22, null); // was CreateFailedEngineException
         ids.put(23, org.elasticsearch.index.shard.IndexShardStartedException.class);
         ids.put(24, org.elasticsearch.search.SearchContextMissingException.class);
         ids.put(25, org.elasticsearch.script.ScriptException.class);
@@ -716,7 +695,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         ids.put(51, org.elasticsearch.index.IndexShardAlreadyExistsException.class);
         ids.put(52, org.elasticsearch.index.engine.VersionConflictEngineException.class);
         ids.put(53, org.elasticsearch.index.engine.EngineException.class);
-        ids.put(54, org.elasticsearch.index.engine.DocumentAlreadyExistsException.class);
+        ids.put(54, null); // was DocumentAlreadyExistsException, which is superseded with VersionConflictEngineException
         ids.put(55, org.elasticsearch.action.NoSuchNodeException.class);
         ids.put(56, org.elasticsearch.common.settings.SettingsException.class);
         ids.put(57, org.elasticsearch.indices.IndexTemplateMissingException.class);
@@ -726,7 +705,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         ids.put(61, org.elasticsearch.cluster.routing.RoutingValidationException.class);
         ids.put(62, org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper.class);
         ids.put(63, org.elasticsearch.indices.AliasFilterParsingException.class);
-        ids.put(64, org.elasticsearch.index.engine.DeleteByQueryFailedEngineException.class);
+        ids.put(64, null); // DeleteByQueryFailedEngineException was removed in 3.0
         ids.put(65, org.elasticsearch.gateway.GatewayException.class);
         ids.put(66, org.elasticsearch.index.shard.IndexShardNotRecoveringException.class);
         ids.put(67, org.elasticsearch.http.HttpException.class);
@@ -813,7 +792,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         }
 
         for (ElasticsearchException.ElasticsearchExceptionHandle handle : ElasticsearchException.ElasticsearchExceptionHandle.values()) {
-            assertEquals((int)reverse.get(handle.exceptionClass), handle.id);
+            assertEquals((int) reverse.get(handle.exceptionClass), handle.id);
         }
 
         for (Map.Entry<Integer, Class<? extends ElasticsearchException>> entry : ids.entrySet()) {
diff --git a/core/src/test/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponsesTests.java b/core/src/test/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponsesTests.java
index 86ead20..d66c1bc 100644
--- a/core/src/test/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponsesTests.java
+++ b/core/src/test/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponsesTests.java
@@ -21,18 +21,17 @@ package org.elasticsearch.action.admin.cluster.health;
 
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
+import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
+import org.elasticsearch.action.admin.cluster.health.ClusterHealthStatus;
+import org.elasticsearch.action.admin.cluster.health.ClusterIndexHealth;
+import org.elasticsearch.action.admin.cluster.health.ClusterShardHealth;
 import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.routing.IndexRoutingTable;
-import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
-import org.elasticsearch.cluster.routing.RoutingTable;
-import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.cluster.routing.ShardRoutingState;
-import org.elasticsearch.cluster.routing.TestShardRouting;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.settings.Settings;
@@ -47,10 +46,7 @@ import java.io.IOException;
 
 import static org.hamcrest.CoreMatchers.allOf;
 import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.Matchers.empty;
-import static org.hamcrest.Matchers.greaterThanOrEqualTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.lessThanOrEqualTo;
+import static org.hamcrest.Matchers.*;
 
 public class ClusterHealthResponsesTests extends ESTestCase {
 
@@ -213,7 +209,7 @@ public class ClusterHealthResponsesTests extends ESTestCase {
             metaData.put(indexMetaData, true);
             routingTable.add(indexRoutingTable);
         }
-        ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable.build()).build();
+        ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
         int pendingTasks = randomIntBetween(0, 200);
         int inFlight = randomIntBetween(0, 200);
         int delayedUnassigned = randomIntBetween(0, 200);
@@ -253,7 +249,7 @@ public class ClusterHealthResponsesTests extends ESTestCase {
         MetaData.Builder metaData = MetaData.builder();
         metaData.put(indexMetaData, true);
         routingTable.add(indexRoutingTable);
-        ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable.build()).build();
+        ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
         ClusterHealthResponse clusterHealth = new ClusterHealthResponse("bla", indexNameExpressionResolver.concreteIndices(clusterState, IndicesOptions.strictExpand(), (String[]) null), clusterState, 0, 0, 0, TimeValue.timeValueMillis(0));
         clusterHealth = maybeSerialize(clusterHealth);
         // currently we have no cluster level validation failures as index validation issues are reported per index.
diff --git a/core/src/test/java/org/elasticsearch/action/admin/indices/upgrade/UpgradeReallyOldIndexIT.java b/core/src/test/java/org/elasticsearch/action/admin/indices/upgrade/UpgradeReallyOldIndexIT.java
deleted file mode 100644
index d365f5b..0000000
--- a/core/src/test/java/org/elasticsearch/action/admin/indices/upgrade/UpgradeReallyOldIndexIT.java
+++ /dev/null
@@ -1,74 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.admin.indices.upgrade;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.bwcompat.StaticIndexBackwardCompatibilityIT;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.IndexService;
-import org.elasticsearch.indices.IndicesService;
-
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.hamcrest.Matchers.containsString;
-
-public class UpgradeReallyOldIndexIT extends StaticIndexBackwardCompatibilityIT {
-
-    public void testUpgrade_0_90_6() throws Exception {
-        String indexName = "index-0.90.6";
-
-        loadIndex(indexName);
-        assertMinVersion(indexName, org.apache.lucene.util.Version.parse("4.5.1"));
-        UpgradeIT.assertNotUpgraded(client(), indexName);
-        assertTrue(UpgradeIT.hasAncientSegments(client(), indexName));
-        assertNoFailures(client().admin().indices().prepareUpgrade(indexName).setUpgradeOnlyAncientSegments(true).get());
-
-        assertFalse(UpgradeIT.hasAncientSegments(client(), indexName));
-        // This index has only ancient segments, so it should now be fully upgraded:
-        UpgradeIT.assertUpgraded(client(), indexName);
-        assertEquals(Version.CURRENT.luceneVersion.toString(), client().admin().indices().prepareGetSettings(indexName).get().getSetting(indexName, IndexMetaData.SETTING_VERSION_MINIMUM_COMPATIBLE));
-        assertMinVersion(indexName, Version.CURRENT.luceneVersion);
-
-        assertEquals(client().admin().indices().prepareGetSettings(indexName).get().getSetting(indexName, IndexMetaData.SETTING_VERSION_UPGRADED), Integer.toString(Version.CURRENT.id));
-    }
-
-    public void testUpgradeConflictingMapping() throws Exception {
-        String indexName = "index-conflicting-mappings-1.7.0";
-        logger.info("Checking static index " + indexName);
-        Settings nodeSettings = prepareBackwardsDataDir(getDataPath(indexName + ".zip"));
-        try {
-            internalCluster().startNode(nodeSettings);
-            fail("Should have failed to start the node");
-        } catch (Exception ex) {
-            assertThat(ex.getMessage(), containsString("conflicts with existing mapping in other types"));
-        }
-    }
-
-    private void assertMinVersion(String index, org.apache.lucene.util.Version version) {
-        for (IndicesService services : internalCluster().getInstances(IndicesService.class)) {
-            IndexService indexService = services.indexService(index);
-            if (indexService != null) {
-                assertEquals(version, indexService.getShardOrNull(0).minimumCompatibleVersion());
-            }
-        }
-
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/action/index/IndexRequestTests.java b/core/src/test/java/org/elasticsearch/action/index/IndexRequestTests.java
index 1cdea96..7c08a0d 100644
--- a/core/src/test/java/org/elasticsearch/action/index/IndexRequestTests.java
+++ b/core/src/test/java/org/elasticsearch/action/index/IndexRequestTests.java
@@ -18,12 +18,18 @@
  */
 package org.elasticsearch.action.index;
 
+import org.elasticsearch.index.VersionType;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
-import static org.hamcrest.Matchers.equalTo;
+
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.Set;
+
+import static org.hamcrest.Matchers.*;
 
 /**
-  */
+ */
 public class IndexRequestTests extends ESTestCase {
 
     @Test
@@ -39,9 +45,23 @@ public class IndexRequestTests extends ESTestCase {
         assertThat(IndexRequest.OpType.fromString(indexUpper), equalTo(IndexRequest.OpType.INDEX));
     }
 
-    @Test(expected= IllegalArgumentException.class)
-    public void testReadBogusString(){
+    @Test(expected = IllegalArgumentException.class)
+    public void testReadBogusString() {
         String foobar = "foobar";
         IndexRequest.OpType.fromString(foobar);
     }
+
+    public void testCreateOperationRejectsVersions() {
+        Set<VersionType> allButInternalSet = new HashSet<>(Arrays.asList(VersionType.values()));
+        allButInternalSet.remove(VersionType.INTERNAL);
+        VersionType[] allButInternal = allButInternalSet.toArray(new VersionType[]{});
+        IndexRequest request = new IndexRequest("index", "type", "1");
+        request.opType(IndexRequest.OpType.CREATE);
+        request.versionType(randomFrom(allButInternal));
+        assertThat(request.validate().validationErrors(), not(empty()));
+
+        request.versionType(VersionType.INTERNAL);
+        request.version(randomIntBetween(0, Integer.MAX_VALUE));
+        assertThat(request.validate().validationErrors(), not(empty()));
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/action/support/replication/ClusterStateCreationUtils.java b/core/src/test/java/org/elasticsearch/action/support/replication/ClusterStateCreationUtils.java
index 100f68f..e5143a3 100644
--- a/core/src/test/java/org/elasticsearch/action/support/replication/ClusterStateCreationUtils.java
+++ b/core/src/test/java/org/elasticsearch/action/support/replication/ClusterStateCreationUtils.java
@@ -27,12 +27,7 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.IndexRoutingTable;
-import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
-import org.elasticsearch.cluster.routing.RoutingTable;
-import org.elasticsearch.cluster.routing.ShardRoutingState;
-import org.elasticsearch.cluster.routing.TestShardRouting;
-import org.elasticsearch.cluster.routing.UnassignedInfo;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.DummyTransportAddress;
 import org.elasticsearch.index.shard.ShardId;
@@ -40,10 +35,8 @@ import org.elasticsearch.index.shard.ShardId;
 import java.util.HashSet;
 import java.util.Set;
 
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_CREATION_DATE;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_VERSION_CREATED;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.*;
+import static org.elasticsearch.test.ESTestCase.randomBoolean;
 import static org.elasticsearch.test.ESTestCase.randomFrom;
 import static org.elasticsearch.test.ESTestCase.randomIntBetween;
 
@@ -131,7 +124,7 @@ public class ClusterStateCreationUtils {
         ClusterState.Builder state = ClusterState.builder(new ClusterName("test"));
         state.nodes(discoBuilder);
         state.metaData(MetaData.builder().put(indexMetaData, false).generateClusterUuidIfNeeded());
-        state.routingTable(RoutingTable.builder().add(IndexRoutingTable.builder(index).addIndexShard(indexShardRoutingBuilder.build())).build());
+        state.routingTable(RoutingTable.builder().add(IndexRoutingTable.builder(index).addIndexShard(indexShardRoutingBuilder.build())));
         return state.build();
     }
 
@@ -165,7 +158,7 @@ public class ClusterStateCreationUtils {
             indexShardRoutingBuilder.addShard(TestShardRouting.newShardRouting(index, i, newNode(1).id(), null, null, false, ShardRoutingState.STARTED, 0, null));
             indexRoutingTableBuilder.addIndexShard(indexShardRoutingBuilder.build());
         }
-        state.routingTable(RoutingTable.builder().add(indexRoutingTableBuilder.build()).build());
+        state.routingTable(RoutingTable.builder().add(indexRoutingTableBuilder));
         return state.build();
     }
 
@@ -221,7 +214,7 @@ public class ClusterStateCreationUtils {
         ClusterState.Builder state = ClusterState.builder(new ClusterName("test"));
         state.nodes(discoBuilder);
         state.metaData(MetaData.builder().generateClusterUuidIfNeeded());
-        state.routingTable(RoutingTable.builder().build());
+        state.routingTable(RoutingTable.builder());
         return state.build();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationActionTests.java b/core/src/test/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationActionTests.java
new file mode 100644
index 0000000..fce4312
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationActionTests.java
@@ -0,0 +1,316 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.support.single.instance;
+
+import org.elasticsearch.ExceptionsHelper;
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.ActionResponse;
+import org.elasticsearch.action.IndicesRequest;
+import org.elasticsearch.action.support.ActionFilter;
+import org.elasticsearch.action.support.ActionFilters;
+import org.elasticsearch.action.support.PlainActionFuture;
+import org.elasticsearch.action.support.replication.ClusterStateCreationUtils;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.block.ClusterBlock;
+import org.elasticsearch.cluster.block.ClusterBlockException;
+import org.elasticsearch.cluster.block.ClusterBlockLevel;
+import org.elasticsearch.cluster.block.ClusterBlocks;
+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.cluster.routing.ShardIterator;
+import org.elasticsearch.cluster.routing.ShardRoutingState;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.rest.RestStatus;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.cluster.TestClusterService;
+import org.elasticsearch.test.transport.CapturingTransport;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.ConnectTransportException;
+import org.elasticsearch.transport.TransportException;
+import org.elasticsearch.transport.TransportService;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.function.Supplier;
+
+import static org.hamcrest.core.IsEqual.equalTo;
+
+public class TransportInstanceSingleOperationActionTests extends ESTestCase {
+
+    private static ThreadPool THREAD_POOL;
+
+    private TestClusterService clusterService;
+    private CapturingTransport transport;
+    private TransportService transportService;
+
+    private TestTransportInstanceSingleOperationAction action;
+
+    public static class Request extends InstanceShardOperationRequest<Request> {
+        public Request() {
+        }
+    }
+
+    public static class Response extends ActionResponse {
+        public Response() {
+        }
+    }
+
+    class TestTransportInstanceSingleOperationAction extends TransportInstanceSingleOperationAction<Request, Response> {
+        private final Map<ShardId, Object> shards = new HashMap<>();
+
+        public TestTransportInstanceSingleOperationAction(Settings settings, String actionName, TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver, Supplier<Request> request) {
+            super(settings, actionName, THREAD_POOL, TransportInstanceSingleOperationActionTests.this.clusterService, transportService, actionFilters, indexNameExpressionResolver, request);
+        }
+
+        public Map<ShardId, Object> getResults() {
+            return shards;
+        }
+
+        @Override
+        protected String executor() {
+            return ThreadPool.Names.SAME;
+        }
+
+        @Override
+        protected void shardOperation(Request request, ActionListener<Response> listener) {
+            throw new UnsupportedOperationException("Not implemented in test class");
+        }
+
+        @Override
+        protected Response newResponse() {
+            return new Response();
+        }
+
+        @Override
+        protected boolean resolveRequest(ClusterState state, Request request, ActionListener<Response> listener) {
+            return true;
+        }
+
+        @Override
+        protected ShardIterator shards(ClusterState clusterState, Request request) {
+            return clusterState.routingTable().index(request.concreteIndex()).shard(request.shardId).primaryShardIt();
+        }
+    }
+
+    class MyResolver extends IndexNameExpressionResolver {
+        public MyResolver() {
+            super(Settings.EMPTY);
+        }
+
+        @Override
+        public String[] concreteIndices(ClusterState state, IndicesRequest request) {
+            return request.indices();
+        }
+    }
+
+    @BeforeClass
+    public static void startThreadPool() {
+        THREAD_POOL = new ThreadPool(TransportInstanceSingleOperationActionTests.class.getSimpleName());
+    }
+
+    @Before
+    public void setUp() throws Exception {
+        super.setUp();
+        transport = new CapturingTransport();
+        clusterService = new TestClusterService(THREAD_POOL);
+        transportService = new TransportService(transport, THREAD_POOL);
+        transportService.start();
+        action = new TestTransportInstanceSingleOperationAction(
+                Settings.EMPTY,
+                "indices:admin/test",
+                transportService,
+                new ActionFilters(new HashSet<ActionFilter>()),
+                new MyResolver(),
+                Request::new
+        );
+    }
+
+    @AfterClass
+    public static void destroyThreadPool() {
+        ThreadPool.terminate(THREAD_POOL, 30, TimeUnit.SECONDS);
+        // since static must set to null to be eligible for collection
+        THREAD_POOL = null;
+    }
+
+    public void testGlobalBlock() {
+        Request request = new Request();
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        ClusterBlocks.Builder block = ClusterBlocks.builder()
+                .addGlobalBlock(new ClusterBlock(1, "", false, true, RestStatus.SERVICE_UNAVAILABLE, ClusterBlockLevel.ALL));
+        clusterService.setState(ClusterState.builder(clusterService.state()).blocks(block));
+        try {
+            action.new AsyncSingleAction(request, listener).start();
+            listener.get();
+            fail("expected ClusterBlockException");
+        } catch (Throwable t) {
+            if (ExceptionsHelper.unwrap(t, ClusterBlockException.class) == null) {
+                logger.info("expected ClusterBlockException  but got ", t);
+                fail("expected ClusterBlockException");
+            }
+        }
+    }
+
+    public void testBasicRequestWorks() throws InterruptedException, ExecutionException, TimeoutException {
+        Request request = new Request().index("test");
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        clusterService.setState(ClusterStateCreationUtils.state("test", randomBoolean(), ShardRoutingState.STARTED));
+        action.new AsyncSingleAction(request, listener).start();
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        transport.handleResponse(transport.capturedRequests()[0].requestId, new Response());
+        listener.get();
+    }
+
+    public void testFailureWithoutRetry() throws Exception {
+        Request request = new Request().index("test");
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        clusterService.setState(ClusterStateCreationUtils.state("test", randomBoolean(), ShardRoutingState.STARTED));
+
+        action.new AsyncSingleAction(request, listener).start();
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        long requestId = transport.capturedRequests()[0].requestId;
+        transport.clear();
+        // this should not trigger retry or anything and the listener should report exception immediately
+        transport.handleResponse(requestId, new TransportException("a generic transport exception", new Exception("generic test exception")));
+
+        try {
+            // result should return immediately
+            assertTrue(listener.isDone());
+            listener.get();
+            fail("this should fail with a transport exception");
+        } catch (ExecutionException t) {
+            if (ExceptionsHelper.unwrap(t, TransportException.class) == null) {
+                logger.info("expected TransportException  but got ", t);
+                fail("expected and TransportException");
+            }
+        }
+    }
+
+    public void testSuccessAfterRetryWithClusterStateUpdate() throws Exception {
+        Request request = new Request().index("test");
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        boolean local = randomBoolean();
+        clusterService.setState(ClusterStateCreationUtils.state("test", local, ShardRoutingState.INITIALIZING));
+        action.new AsyncSingleAction(request, listener).start();
+        // this should fail because primary not initialized
+        assertThat(transport.capturedRequests().length, equalTo(0));
+        clusterService.setState(ClusterStateCreationUtils.state("test", local, ShardRoutingState.STARTED));
+        // this time it should work
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        transport.handleResponse(transport.capturedRequests()[0].requestId, new Response());
+        listener.get();
+    }
+
+    public void testSuccessAfterRetryWithExcpetionFromTransport() throws Exception {
+        Request request = new Request().index("test");
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        boolean local = randomBoolean();
+        clusterService.setState(ClusterStateCreationUtils.state("test", local, ShardRoutingState.STARTED));
+        action.new AsyncSingleAction(request, listener).start();
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        long requestId = transport.capturedRequests()[0].requestId;
+        transport.clear();
+        DiscoveryNode node = clusterService.state().getNodes().getLocalNode();
+        transport.handleResponse(requestId, new ConnectTransportException(node, "test exception"));
+        // trigger cluster state observer
+        clusterService.setState(ClusterStateCreationUtils.state("test", local, ShardRoutingState.STARTED));
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        transport.handleResponse(transport.capturedRequests()[0].requestId, new Response());
+        listener.get();
+    }
+
+    public void testRetryOfAnAlreadyTimedOutRequest() throws Exception {
+        Request request = new Request().index("test").timeout(new TimeValue(0, TimeUnit.MILLISECONDS));
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        clusterService.setState(ClusterStateCreationUtils.state("test", randomBoolean(), ShardRoutingState.STARTED));
+        action.new AsyncSingleAction(request, listener).start();
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        long requestId = transport.capturedRequests()[0].requestId;
+        transport.clear();
+        DiscoveryNode node = clusterService.state().getNodes().getLocalNode();
+        transport.handleResponse(requestId, new ConnectTransportException(node, "test exception"));
+
+        // wait until the timeout was triggered and we actually tried to send for the second time
+        assertBusy(new Runnable() {
+            @Override
+            public void run() {
+                assertThat(transport.capturedRequests().length, equalTo(1));
+            }
+        });
+
+        // let it fail the second time too
+        requestId = transport.capturedRequests()[0].requestId;
+        transport.handleResponse(requestId, new ConnectTransportException(node, "test exception"));
+        try {
+            // result should return immediately
+            assertTrue(listener.isDone());
+            listener.get();
+            fail("this should fail with a transport exception");
+        } catch (ExecutionException t) {
+            if (ExceptionsHelper.unwrap(t, ConnectTransportException.class) == null) {
+                logger.info("expected ConnectTransportException  but got ", t);
+                fail("expected and ConnectTransportException");
+            }
+        }
+    }
+
+    public void testUnresolvableRequestDoesNotHang() throws InterruptedException, ExecutionException, TimeoutException {
+        action = new TestTransportInstanceSingleOperationAction(
+                Settings.EMPTY,
+                "indices:admin/test_unresolvable",
+                transportService,
+                new ActionFilters(new HashSet<ActionFilter>()),
+                new MyResolver(),
+                Request::new
+        ) {
+            @Override
+            protected boolean resolveRequest(ClusterState state, Request request, ActionListener<Response> listener) {
+                return false;
+            }
+        };
+        Request request = new Request().index("test");
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        clusterService.setState(ClusterStateCreationUtils.state("test", randomBoolean(), ShardRoutingState.STARTED));
+        action.new AsyncSingleAction(request, listener).start();
+        assertThat(transport.capturedRequests().length, equalTo(0));
+        try {
+            listener.get();
+        } catch (Throwable t) {
+            if (ExceptionsHelper.unwrap(t, IllegalStateException.class) == null) {
+                logger.info("expected IllegalStateException  but got ", t);
+                fail("expected and IllegalStateException");
+            }
+        }
+    }
+}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java
index 6b2608c..9ec74c9 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.benchmark.cluster;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -34,7 +35,6 @@ import org.elasticsearch.test.ESAllocationTestCase;
 
 import java.util.Random;
 
-import static java.util.Collections.singletonMap;
 import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
 
 public class ClusterAllocationRerouteBenchmark {
@@ -64,7 +64,7 @@ public class ClusterAllocationRerouteBenchmark {
         RoutingTable routingTable = rb.build();
         DiscoveryNodes.Builder nb = DiscoveryNodes.builder();
         for (int i = 1; i <= numberOfNodes; i++) {
-            nb.put(ESAllocationTestCase.newNode("node" + i, singletonMap("tag", "tag_" + (i % numberOfTags))));
+            nb.put(ESAllocationTestCase.newNode("node" + i, numberOfTags == 0 ? ImmutableMap.<String, String>of() : ImmutableMap.of("tag", "tag_" + (i % numberOfTags))));
         }
         ClusterState initialClusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).nodes(nb).build();
 
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
index 8e71f3d..990c399 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
@@ -246,7 +246,7 @@ public class OldIndexBackwardsCompatibilityIT extends ESIntegTestCase {
         SortedSet<String> expectedVersions = new TreeSet<>();
         for (Version v : VersionUtils.allVersions()) {
             if (v.snapshot()) continue;  // snapshots are unreleased, so there is no backcompat yet
-            if (v.onOrBefore(Version.V_0_20_6)) continue; // we can only test back one major lucene version
+            if (v.onOrBefore(Version.V_2_0_0_beta1)) continue; // we can only test back one major lucene version
             if (v.equals(Version.CURRENT)) continue; // the current version is always compatible with itself
             expectedVersions.add("index-" + v.toString() + ".zip");
         }
@@ -312,7 +312,7 @@ public class OldIndexBackwardsCompatibilityIT extends ESIntegTestCase {
             client().admin().indices().prepareOpen(indexName).get();
             fail("Shouldn't be able to open an old index");
         } catch (IllegalStateException ex) {
-            assertThat(ex.getMessage(), containsString("was created before v0.90.0 and wasn't upgraded"));
+            assertThat(ex.getMessage(), containsString("was created before v2.0.0.beta1 and wasn't upgraded"));
         }
         unloadIndex(indexName);
         logger.info("--> Done testing " + index + ", took " + ((System.currentTimeMillis() - startTime) / 1000.0) + " seconds");
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/RecoveryWithUnsupportedIndicesIT.java b/core/src/test/java/org/elasticsearch/bwcompat/RecoveryWithUnsupportedIndicesIT.java
index 486267b..8957485 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/RecoveryWithUnsupportedIndicesIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/RecoveryWithUnsupportedIndicesIT.java
@@ -36,7 +36,7 @@ public class RecoveryWithUnsupportedIndicesIT extends StaticIndexBackwardCompati
             internalCluster().startNode(nodeSettings);
             fail();
         } catch (Exception ex) {
-            assertThat(ex.getMessage(), containsString(" was created before v0.90.0 and wasn't upgraded"));
+            assertThat(ex.getMessage(), containsString(" was created before v2.0.0.beta1 and wasn't upgraded"));
         }
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java b/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java
index 9ef4238..740b185 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java
@@ -95,9 +95,8 @@ public class RestoreBackwardsCompatIT extends AbstractSnapshotIntegTestCase {
             if (Modifier.isStatic(field.getModifiers()) && field.getType() == Version.class) {
                 Version v = (Version) field.get(Version.class);
                 if (v.snapshot()) continue;
-                if (v.onOrBefore(Version.V_1_0_0_Beta1)) continue;
+                if (v.onOrBefore(Version.V_2_0_0_beta1)) continue;
                 if (v.equals(Version.CURRENT)) continue;
-
                 expectedVersions.add(v.toString());
             }
         }
diff --git a/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java b/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java
index b00b677..0cf16b4 100644
--- a/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java
+++ b/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.client;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.GenericAction;
@@ -137,9 +138,10 @@ public abstract class AbstractClientHeadersTestCase extends ESTestCase {
     @Test
     public void testOverideHeader() throws Exception {
         String key1Val = randomAsciiOfLength(5);
-        Map<String, Object> expected = new HashMap<>();
-        expected.put("key1", key1Val);
-        expected.put("key2", "val 2");
+        Map<String, Object> expected = ImmutableMap.<String, Object>builder()
+                .put("key1", key1Val)
+                .put("key2", "val 2")
+                .build();
 
         client.prepareGet("idx", "type", "id")
                 .putHeader("key1", key1Val)
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterInfoServiceIT.java b/core/src/test/java/org/elasticsearch/cluster/ClusterInfoServiceIT.java
index f938a78..f672b26 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ClusterInfoServiceIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ClusterInfoServiceIT.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.cluster;
 
-import com.carrotsearch.hppc.cursors.ObjectCursor;
-
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.ActionModule;
@@ -36,7 +34,6 @@ import org.elasticsearch.cluster.routing.RoutingNodes;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.IndexService;
@@ -56,6 +53,7 @@ import org.junit.Test;
 
 import java.io.IOException;
 import java.util.Collection;
+import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.atomic.AtomicBoolean;
@@ -151,24 +149,24 @@ public class ClusterInfoServiceIT extends ESIntegTestCase {
         final InternalClusterInfoService infoService = (InternalClusterInfoService) internalTestCluster.getInstance(ClusterInfoService.class, internalTestCluster.getMasterName());
         ClusterInfo info = infoService.refresh();
         assertNotNull("info should not be null", info);
-        ImmutableOpenMap<String, DiskUsage> leastUsages = info.getNodeLeastAvailableDiskUsages();
-        ImmutableOpenMap<String, DiskUsage> mostUsages = info.getNodeMostAvailableDiskUsages();
-        ImmutableOpenMap<String, Long> shardSizes = info.shardSizes;
+        final Map<String, DiskUsage> leastUsages = info.getNodeLeastAvailableDiskUsages();
+        final Map<String, DiskUsage> mostUsages = info.getNodeMostAvailableDiskUsages();
+        final Map<String, Long> shardSizes = info.shardSizes;
         assertNotNull(leastUsages);
         assertNotNull(shardSizes);
         assertThat("some usages are populated", leastUsages.values().size(), Matchers.equalTo(2));
         assertThat("some shard sizes are populated", shardSizes.values().size(), greaterThan(0));
-        for (ObjectCursor<DiskUsage> usage : leastUsages.values()) {
-            logger.info("--> usage: {}", usage.value);
-            assertThat("usage has be retrieved", usage.value.getFreeBytes(), greaterThan(0L));
+        for (DiskUsage usage : leastUsages.values()) {
+            logger.info("--> usage: {}", usage);
+            assertThat("usage has be retrieved", usage.getFreeBytes(), greaterThan(0L));
         }
-        for (ObjectCursor<DiskUsage> usage : mostUsages.values()) {
-            logger.info("--> usage: {}", usage.value);
-            assertThat("usage has be retrieved", usage.value.getFreeBytes(), greaterThan(0L));
+        for (DiskUsage usage : mostUsages.values()) {
+            logger.info("--> usage: {}", usage);
+            assertThat("usage has be retrieved", usage.getFreeBytes(), greaterThan(0L));
         }
-        for (ObjectCursor<Long> size : shardSizes.values()) {
-            logger.info("--> shard size: {}", size.value);
-            assertThat("shard size is greater than 0", size.value, greaterThanOrEqualTo(0L));
+        for (Long size : shardSizes.values()) {
+            logger.info("--> shard size: {}", size);
+            assertThat("shard size is greater than 0", size, greaterThanOrEqualTo(0L));
         }
         ClusterService clusterService = internalTestCluster.getInstance(ClusterService.class, internalTestCluster.getMasterName());
         ClusterState state = clusterService.state();
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java b/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java
index ac2182c..1aa1602 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java
@@ -20,25 +20,14 @@
 package org.elasticsearch.cluster;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
-
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlocks;
-import org.elasticsearch.cluster.metadata.AliasMetaData;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.metadata.RepositoriesMetaData;
-import org.elasticsearch.cluster.metadata.SnapshotId;
+import org.elasticsearch.cluster.metadata.*;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.IndexRoutingTable;
-import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
-import org.elasticsearch.cluster.routing.RoutingTable;
-import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.cluster.routing.ShardRoutingState;
-import org.elasticsearch.cluster.routing.TestShardRouting;
-import org.elasticsearch.cluster.routing.UnassignedInfo;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
@@ -57,13 +46,12 @@ import org.junit.Test;
 import java.util.Collections;
 import java.util.List;
 
-import static java.util.Collections.emptyList;
 import static org.elasticsearch.cluster.metadata.AliasMetaData.newAliasMetaDataBuilder;
 import static org.elasticsearch.cluster.routing.RandomShardRoutingMutator.randomChange;
 import static org.elasticsearch.cluster.routing.RandomShardRoutingMutator.randomReason;
-import static org.elasticsearch.test.VersionUtils.randomVersion;
 import static org.elasticsearch.test.XContentTestUtils.convertToMap;
 import static org.elasticsearch.test.XContentTestUtils.differenceBetweenMapsIgnoringArrayOrder;
+import static org.elasticsearch.test.VersionUtils.randomVersion;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.is;
 
@@ -209,7 +197,7 @@ public class ClusterStateDiffIT extends ESIntegTestCase {
         RoutingTable.Builder builder = RoutingTable.builder(clusterState.routingTable());
         int numberOfIndices = clusterState.routingTable().indicesRouting().size();
         if (numberOfIndices > 0) {
-            List<String> randomIndices = randomSubsetOf(randomInt(numberOfIndices - 1), clusterState.routingTable().indicesRouting().keys().toArray(String.class));
+            List<String> randomIndices = randomSubsetOf(randomInt(numberOfIndices - 1), clusterState.routingTable().indicesRouting().keySet().toArray(new String[numberOfIndices]));
             for (String index : randomIndices) {
                 if (randomBoolean()) {
                     builder.remove(index);
@@ -673,13 +661,13 @@ public class ClusterStateDiffIT extends ESIntegTestCase {
                                 SnapshotsInProgress.State.fromValue((byte) randomIntBetween(0, 6)),
                                 Collections.<String>emptyList(),
                                 Math.abs(randomLong()),
-                                ImmutableOpenMap.of()));
+                                ImmutableMap.<ShardId, SnapshotsInProgress.ShardSnapshotStatus>of()));
                     case 1:
                         return new RestoreInProgress(new RestoreInProgress.Entry(
                                 new SnapshotId(randomName("repo"), randomName("snap")),
                                 RestoreInProgress.State.fromValue((byte) randomIntBetween(0, 3)),
-                                emptyList(),
-                                ImmutableOpenMap.of()));
+                                Collections.<String>emptyList(),
+                                ImmutableMap.<ShardId, RestoreInProgress.ShardRestoreStatus>of()));
                     default:
                         throw new IllegalArgumentException("Shouldn't be here");
                 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/DiskUsageTests.java b/core/src/test/java/org/elasticsearch/cluster/DiskUsageTests.java
index 595dbc9..a427829 100644
--- a/core/src/test/java/org/elasticsearch/cluster/DiskUsageTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/DiskUsageTests.java
@@ -21,13 +21,13 @@ package org.elasticsearch.cluster;
 
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.cluster.node.stats.NodeStats;
+import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
 import org.elasticsearch.action.admin.indices.stats.CommonStats;
 import org.elasticsearch.action.admin.indices.stats.ShardStats;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingHelper;
 import org.elasticsearch.cluster.routing.UnassignedInfo;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.transport.DummyTransportAddress;
 import org.elasticsearch.index.shard.ShardPath;
 import org.elasticsearch.index.store.StoreStats;
@@ -36,6 +36,8 @@ import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
 import java.nio.file.Path;
+import java.util.HashMap;
+import java.util.Map;
 
 import static org.hamcrest.Matchers.equalTo;
 
@@ -93,7 +95,7 @@ public class DiskUsageTests extends ESTestCase {
             }
         }
     }
-
+    
     public void testFillShardLevelInfo() {
         ShardRouting test_0 = ShardRouting.newUnassigned("test", 0, null, false, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo"));
         ShardRoutingHelper.initialize(test_0, "node1");
@@ -111,8 +113,8 @@ public class DiskUsageTests extends ESTestCase {
                 new ShardStats(test_0, new ShardPath(false, test0Path, test0Path, "0xdeadbeef", test_0.shardId()), commonStats0 , null),
                 new ShardStats(test_1, new ShardPath(false, test1Path, test1Path, "0xdeadbeef", test_1.shardId()), commonStats1 , null)
         };
-        ImmutableOpenMap.Builder<String, Long> shardSizes = ImmutableOpenMap.builder();
-        ImmutableOpenMap.Builder<ShardRouting, String> routingToPath = ImmutableOpenMap.builder();
+        HashMap<String, Long> shardSizes = new HashMap<>();
+        HashMap<ShardRouting, String> routingToPath = new HashMap<>();
         InternalClusterInfoService.buildShardLevelInfo(logger, stats, shardSizes, routingToPath);
         assertEquals(2, shardSizes.size());
         assertTrue(shardSizes.containsKey(ClusterInfo.shardIdentifierFromRouting(test_0)));
@@ -128,8 +130,8 @@ public class DiskUsageTests extends ESTestCase {
     }
 
     public void testFillDiskUsage() {
-        ImmutableOpenMap.Builder<String, DiskUsage> newLeastAvaiableUsages = ImmutableOpenMap.builder();
-        ImmutableOpenMap.Builder<String, DiskUsage> newMostAvaiableUsages = ImmutableOpenMap.builder();
+        Map<String, DiskUsage> newLeastAvaiableUsages = new HashMap<>();
+        Map<String, DiskUsage> newMostAvaiableUsages = new HashMap<>();
         FsInfo.Path[] node1FSInfo =  new FsInfo.Path[] {
                 new FsInfo.Path("/middle", "/dev/sda", 100, 90, 80),
                 new FsInfo.Path("/least", "/dev/sdb", 200, 190, 70),
diff --git a/core/src/test/java/org/elasticsearch/cluster/MockInternalClusterInfoService.java b/core/src/test/java/org/elasticsearch/cluster/MockInternalClusterInfoService.java
index dd1cb0b..eff22c8 100644
--- a/core/src/test/java/org/elasticsearch/cluster/MockInternalClusterInfoService.java
+++ b/core/src/test/java/org/elasticsearch/cluster/MockInternalClusterInfoService.java
@@ -27,7 +27,6 @@ import org.elasticsearch.action.admin.indices.stats.IndicesStatsResponse;
 import org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.DummyTransportAddress;
@@ -36,6 +35,10 @@ import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.threadpool.ThreadPool;
 
+import java.util.AbstractMap;
+import java.util.Collections;
+import java.util.Map;
+import java.util.Set;
 import java.util.concurrent.CountDownLatch;
 
 /**
@@ -113,24 +116,30 @@ public class MockInternalClusterInfoService extends InternalClusterInfoService {
         return new CountDownLatch(0);
     }
 
-    @Override
     public ClusterInfo getClusterInfo() {
         ClusterInfo clusterInfo = super.getClusterInfo();
-        return new DevNullClusterInfo(clusterInfo.getNodeLeastAvailableDiskUsages(), clusterInfo.getNodeMostAvailableDiskUsages(), clusterInfo.shardSizes);
+        return new ClusterInfo(clusterInfo.getNodeLeastAvailableDiskUsages(), clusterInfo.getNodeMostAvailableDiskUsages(), clusterInfo.shardSizes, DEV_NULL_MAP);
     }
 
-    /**
-     * ClusterInfo that always points to DevNull.
-     */
-    public static class DevNullClusterInfo extends ClusterInfo {
-        public DevNullClusterInfo(ImmutableOpenMap<String, DiskUsage> leastAvailableSpaceUsage,
-            ImmutableOpenMap<String, DiskUsage> mostAvailableSpaceUsage, ImmutableOpenMap<String, Long> shardSizes) {
-            super(leastAvailableSpaceUsage, mostAvailableSpaceUsage, shardSizes, null);
+    public static final Map<ShardRouting, String> DEV_NULL_MAP = Collections.unmodifiableMap(new StaticValueMap("/dev/null"));
+
+    // a test only map that always returns the same value no matter what key is passed
+    private static final class StaticValueMap extends AbstractMap<ShardRouting, String> {
+
+        private final String value;
+
+        private StaticValueMap(String value) {
+            this.value = value;
+        }
+
+        @Override
+        public String get(Object key) {
+            return value;
         }
 
         @Override
-        public String getDataPath(ShardRouting shardRouting) {
-            return "/dev/null";
+        public Set<Entry<ShardRouting, String>> entrySet() {
+            throw new UnsupportedOperationException("this is a test-only map that only supports #get(Object key)");
         }
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeServiceTests.java b/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeServiceTests.java
deleted file mode 100644
index 88f27bc..0000000
--- a/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeServiceTests.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.cluster.metadata;
-
-import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-import org.elasticsearch.Version;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.store.IndexStoreModule;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Arrays;
-import java.util.Locale;
-
-public class MetaDataIndexUpgradeServiceTests extends ESTestCase {
-
-    public void testUpgradeStoreSettings() {
-        final String type = RandomPicks.randomFrom(random(), Arrays.asList("nio_fs", "mmap_fs", "simple_fs", "default", "fs"));
-        MetaDataIndexUpgradeService metaDataIndexUpgradeService = new MetaDataIndexUpgradeService(Settings.EMPTY, null);
-        Settings indexSettings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                .put(IndexStoreModule.STORE_TYPE, randomBoolean() ? type : type.toUpperCase(Locale.ROOT))
-                .build();
-        IndexMetaData test = IndexMetaData.builder("test")
-                .settings(indexSettings)
-                .numberOfShards(1)
-                .numberOfReplicas(1)
-                .build();
-        IndexMetaData indexMetaData = metaDataIndexUpgradeService.upgradeSettings(test);
-        assertEquals(type.replace("_", ""), indexMetaData.getSettings().get(IndexStoreModule.STORE_TYPE));
-    }
-
-    public void testNoStoreSetting() {
-        Settings indexSettings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                .build();
-        IndexMetaData test = IndexMetaData.builder("test")
-                .settings(indexSettings)
-                .numberOfShards(1)
-                .numberOfReplicas(1)
-                .build();
-        MetaDataIndexUpgradeService metaDataIndexUpgradeService = new MetaDataIndexUpgradeService(Settings.EMPTY, null);
-        IndexMetaData indexMetaData = metaDataIndexUpgradeService.upgradeSettings(test);
-        assertSame(indexMetaData, test);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeFiltersTests.java b/core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeFiltersTests.java
index 2136d1e..b37495e 100644
--- a/core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeFiltersTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeFiltersTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster.node;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.DummyTransportAddress;
@@ -32,12 +33,8 @@ import java.net.InetAddress;
 import java.net.UnknownHostException;
 import java.util.ArrayList;
 import java.util.Collections;
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
 
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.singletonMap;
 import static org.elasticsearch.cluster.node.DiscoveryNodeFilters.OpType.AND;
 import static org.elasticsearch.cluster.node.DiscoveryNodeFilters.OpType.OR;
 import static org.hamcrest.Matchers.equalTo;
@@ -65,10 +62,10 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build();
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(OR, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
+        DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(true));
 
-        node = new DiscoveryNode("name2", "id2", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
+        node = new DiscoveryNode("name2", "id2", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(false));
     }
 
@@ -79,10 +76,10 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build();
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(OR, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
+        DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(true));
 
-        node = new DiscoveryNode("name2", "id2", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
+        node = new DiscoveryNode("name2", "id2", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(false));
     }
 
@@ -94,13 +91,13 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(OR, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
+        DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(true));
 
-        node = new DiscoveryNode("name2", "id2", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
+        node = new DiscoveryNode("name2", "id2", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(true));
 
-        node = new DiscoveryNode("name3", "id3", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
+        node = new DiscoveryNode("name3", "id3", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(false));
     }
 
@@ -112,30 +109,19 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(AND, "xxx.", settings);
 
-        Map<String, String> attributes = new HashMap<>();
-        attributes.put("tag", "A");
-        attributes.put("group", "B");
         DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE,
-                attributes, Version.CURRENT);
+                ImmutableMap.of("tag", "A", "group", "B"), Version.CURRENT);
         assertThat(filters.match(node), equalTo(true));
 
-        attributes = new HashMap<>();
-        attributes.put("tag", "A");
-        attributes.put("group", "B");
-        attributes.put("name", "X");
         node = new DiscoveryNode("name2", "id2", DummyTransportAddress.INSTANCE,
-                attributes, Version.CURRENT);
+                ImmutableMap.of("tag", "A", "group", "B", "name", "X"), Version.CURRENT);
         assertThat(filters.match(node), equalTo(true));
 
-        attributes = new HashMap<>();
-        attributes.put("tag", "A");
-        attributes.put("group", "F");
-        attributes.put("name", "X");
         node = new DiscoveryNode("name3", "id3", DummyTransportAddress.INSTANCE,
-                attributes, Version.CURRENT);
+                ImmutableMap.of("tag", "A", "group", "F", "name", "X"), Version.CURRENT);
         assertThat(filters.match(node), equalTo(false));
 
-        node = new DiscoveryNode("name4", "id4", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
+        node = new DiscoveryNode("name4", "id4", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(false));
     }
 
@@ -146,7 +132,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build();
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(OR, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
+        DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(true));
     }
 
@@ -158,7 +144,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(AND, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
         assertThat(filters.match(node), equalTo(true));
     }
 
@@ -170,7 +156,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(AND, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
         assertThat(filters.match(node), equalTo(false));
     }
 
@@ -182,7 +168,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(AND, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
         assertThat(filters.match(node), equalTo(false));
     }
 
@@ -194,7 +180,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(OR, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
         assertThat(filters.match(node), equalTo(true));
     }
 
@@ -206,7 +192,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(OR, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
         assertThat(filters.match(node), equalTo(true));
     }
 
@@ -218,7 +204,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(AND, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
         assertThat(filters.match(node), equalTo(true));
     }
 
@@ -230,7 +216,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(AND, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
         assertThat(filters.match(node), equalTo(false));
     }
 
@@ -242,7 +228,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(OR, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
         assertThat(filters.match(node), equalTo(true));
     }
 
@@ -254,7 +240,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(OR, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
         assertThat(filters.match(node), equalTo(true));
     }
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java
index a2dbf78..29281e2 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java
@@ -38,43 +38,32 @@ import java.util.Arrays;
 public class RoutingBackwardCompatibilityTests extends ESTestCase {
 
     public void testBackwardCompatibility() throws Exception {
-        Path baseDir = createTempDir();
-        Node node = new Node(Settings.builder().put("path.home", baseDir.toString()).build());
-        try {
-            try (BufferedReader reader = new BufferedReader(new InputStreamReader(RoutingBackwardCompatibilityTests.class.getResourceAsStream("/org/elasticsearch/cluster/routing/shard_routes.txt"), "UTF-8"))) {
-                for (String line = reader.readLine(); line != null; line = reader.readLine()) {
-                    if (line.startsWith("#")) { // comment
-                        continue;
-                    }
-                    String[] parts = line.split("\t");
-                    assertEquals(Arrays.toString(parts), 7, parts.length);
-                    final String index = parts[0];
-                    final int numberOfShards = Integer.parseInt(parts[1]);
-                    final String type = parts[2];
-                    final String id = parts[3];
-                    final String routing = "null".equals(parts[4]) ? null : parts[4];
-                    final int pre20ExpectedShardId = Integer.parseInt(parts[5]);
-                    final int currentExpectedShard = Integer.parseInt(parts[6]);
+        try (BufferedReader reader = new BufferedReader(new InputStreamReader(RoutingBackwardCompatibilityTests.class.getResourceAsStream("/org/elasticsearch/cluster/routing/shard_routes.txt"), "UTF-8"))) {
+            for (String line = reader.readLine(); line != null; line = reader.readLine()) {
+                if (line.startsWith("#")) { // comment
+                    continue;
+                }
+                String[] parts = line.split("\t");
+                assertEquals(Arrays.toString(parts), 7, parts.length);
+                final String index = parts[0];
+                final int numberOfShards = Integer.parseInt(parts[1]);
+                final String type = parts[2];
+                final String id = parts[3];
+                final String routing = "null".equals(parts[4]) ? null : parts[4];
+                final int pre20ExpectedShardId = Integer.parseInt(parts[5]); // not needed anymore - old hashing is gone
+                final int currentExpectedShard = Integer.parseInt(parts[6]);
 
-                    OperationRouting operationRouting = node.injector().getInstance(OperationRouting.class);
-                    for (Version version : VersionUtils.allVersions()) {
-                        final Settings settings = settings(version).build();
-                        IndexMetaData indexMetaData = IndexMetaData.builder(index).settings(settings).numberOfShards(numberOfShards).numberOfReplicas(randomInt(3)).build();
-                        MetaData.Builder metaData = MetaData.builder().put(indexMetaData, false);
-                        RoutingTable routingTable = RoutingTable.builder().addAsNew(indexMetaData).build();
-                        ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
-                        final int shardId = operationRouting.indexShards(clusterState, index, type, id, routing).shardId().getId();
-                        if (version.before(Version.V_2_0_0_beta1)) {
-                            assertEquals(pre20ExpectedShardId, shardId);
-                        } else {
-                            assertEquals(currentExpectedShard, shardId);
-                        }
-                    }
+                OperationRouting operationRouting = new OperationRouting(Settings.EMPTY, null);
+                for (Version version : VersionUtils.allVersions()) {
+                    final Settings settings = settings(version).build();
+                    IndexMetaData indexMetaData = IndexMetaData.builder(index).settings(settings).numberOfShards(numberOfShards).numberOfReplicas(randomInt(3)).build();
+                    MetaData.Builder metaData = MetaData.builder().put(indexMetaData, false);
+                    RoutingTable routingTable = RoutingTable.builder().addAsNew(indexMetaData).build();
+                    ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
+                    final int shardId = operationRouting.indexShards(clusterState, index, type, id, routing).shardId().getId();
+                    assertEquals(currentExpectedShard, shardId);
                 }
             }
-        } finally {
-            node.close();
         }
     }
-
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityUponUpgradeIT.java b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityUponUpgradeIT.java
deleted file mode 100644
index bff1977..0000000
--- a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityUponUpgradeIT.java
+++ /dev/null
@@ -1,73 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cluster.routing;
-
-import org.apache.lucene.util.LuceneTestCase;
-import org.elasticsearch.action.admin.indices.get.GetIndexResponse;
-import org.elasticsearch.action.admin.indices.settings.get.GetSettingsResponse;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.SearchHit;
-import org.elasticsearch.test.ESIntegTestCase;
-
-import java.nio.file.Path;
-
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-
-@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0, minNumDataNodes = 0, maxNumDataNodes = 0)
-@LuceneTestCase.SuppressFileSystems("*") // extra files break the single data cluster expectation when unzipping the static index
-public class RoutingBackwardCompatibilityUponUpgradeIT extends ESIntegTestCase {
-
-    public void testDefaultRouting() throws Exception {
-        test("default_routing_1_x", DjbHashFunction.class, false);
-    }
-
-    public void testCustomRouting() throws Exception {
-        test("custom_routing_1_x", SimpleHashFunction.class, true);
-    }
-
-    private void test(String name, Class<? extends HashFunction> expectedHashFunction, boolean expectedUseType) throws Exception {
-        Path zippedIndexDir = getDataPath("/org/elasticsearch/cluster/routing/" + name + ".zip");
-        Settings baseSettings = prepareBackwardsDataDir(zippedIndexDir);
-        internalCluster().startNode(Settings.builder()
-                .put(baseSettings)
-                .put(Node.HTTP_ENABLED, true)
-                .build());
-        ensureYellow("test");
-        GetIndexResponse getIndexResponse = client().admin().indices().prepareGetIndex().get();
-        assertArrayEquals(new String[] {"test"}, getIndexResponse.indices());
-        GetSettingsResponse getSettingsResponse = client().admin().indices().prepareGetSettings("test").get();
-        assertEquals(expectedHashFunction.getName(), getSettingsResponse.getSetting("test", IndexMetaData.SETTING_LEGACY_ROUTING_HASH_FUNCTION));
-        assertEquals(Boolean.valueOf(expectedUseType).toString(), getSettingsResponse.getSetting("test", IndexMetaData.SETTING_LEGACY_ROUTING_USE_TYPE));
-        SearchResponse allDocs = client().prepareSearch("test").get();
-        assertSearchResponse(allDocs);
-        assertHitCount(allDocs, 4);
-        // Make sure routing works
-        for (SearchHit hit : allDocs.getHits().hits()) {
-            GetResponse get = client().prepareGet(hit.index(), hit.type(), hit.id()).get();
-            assertTrue(get.isExists());
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java
index 9309fa7..c2ba1cb 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java
@@ -36,6 +36,8 @@ import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
 
+import java.util.Collections;
+import java.util.Comparator;
 import java.util.List;
 import java.util.concurrent.atomic.AtomicBoolean;
 
@@ -74,7 +76,7 @@ public class RoutingServiceTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test")).build()).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test"))).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2")).localNodeId("node1").masterNodeId("node1")).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
@@ -104,7 +106,7 @@ public class RoutingServiceTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test")).build()).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test"))).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2")).localNodeId("node1").masterNodeId("node1")).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
@@ -151,7 +153,7 @@ public class RoutingServiceTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test")).build()).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test"))).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2")).localNodeId("node1").masterNodeId("node1")).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTests.java
index 2a7ed6a..ea5dda0 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTests.java
@@ -35,7 +35,6 @@ import org.junit.Test;
 
 import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.hamcrest.Matchers.containsString;
 import static org.hamcrest.Matchers.is;
 import static org.hamcrest.Matchers.nullValue;
 
@@ -56,7 +55,6 @@ public class RoutingTableTests extends ESAllocationTestCase {
             .build());
     private ClusterState clusterState;
 
-    @Override
     @Before
     public void setUp() throws Exception {
         super.setUp();
@@ -266,34 +264,4 @@ public class RoutingTableTests extends ESAllocationTestCase {
             fail("Calling with non-existing index should be ignored at the moment");
         }
     }
-
-    public void testRoutingTableBuiltMoreThanOnce() {
-        RoutingTable.Builder b = RoutingTable.builder();
-        b.build(); // Ok the first time
-        try {
-            b.build();
-            fail("expected exception");
-        } catch (IllegalStateException e) {
-            assertThat(e.getMessage(), containsString("cannot be reused"));
-        }
-        try {
-            b.add((IndexRoutingTable) null);
-            fail("expected exception");
-        } catch (IllegalStateException e) {
-            assertThat(e.getMessage(), containsString("cannot be reused"));
-        }
-        try {
-            b.updateNumberOfReplicas(1, "foo");
-            fail("expected exception");
-        } catch (IllegalStateException e) {
-            assertThat(e.getMessage(), containsString("cannot be reused"));
-        }
-        try {
-            b.remove("foo");
-            fail("expected exception");
-        } catch (IllegalStateException e) {
-            assertThat(e.getMessage(), containsString("cannot be reused"));
-        }
-
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java
index fc7ce64..fdc1c52 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java
@@ -21,7 +21,6 @@ package org.elasticsearch.cluster.routing;
 
 import com.carrotsearch.hppc.IntHashSet;
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterState;
@@ -41,15 +40,8 @@ import org.junit.Test;
 import java.util.Collections;
 import java.util.EnumSet;
 
-import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.STARTED;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.UNASSIGNED;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.lessThan;
-import static org.hamcrest.Matchers.lessThanOrEqualTo;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.*;
+import static org.hamcrest.Matchers.*;
 
 /**
  */
@@ -97,7 +89,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test")).build()).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test"))).build();
         for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.INDEX_CREATED));
         }
@@ -110,7 +102,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsRecovery(metaData.index("test")).build()).build();
+                .routingTable(RoutingTable.builder().addAsRecovery(metaData.index("test"))).build();
         for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.CLUSTER_RECOVERED));
         }
@@ -123,7 +115,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsFromCloseToOpen(metaData.index("test")).build()).build();
+                .routingTable(RoutingTable.builder().addAsFromCloseToOpen(metaData.index("test"))).build();
         for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.INDEX_REOPENED));
         }
@@ -136,7 +128,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNewRestore(metaData.index("test"), new RestoreSource(new SnapshotId("rep1", "snp1"), Version.CURRENT, "test"), new IntHashSet()).build()).build();
+                .routingTable(RoutingTable.builder().addAsNewRestore(metaData.index("test"), new RestoreSource(new SnapshotId("rep1", "snp1"), Version.CURRENT, "test"), new IntHashSet())).build();
         for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.NEW_INDEX_RESTORED));
         }
@@ -149,7 +141,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsRestore(metaData.index("test"), new RestoreSource(new SnapshotId("rep1", "snp1"), Version.CURRENT, "test")).build()).build();
+                .routingTable(RoutingTable.builder().addAsRestore(metaData.index("test"), new RestoreSource(new SnapshotId("rep1", "snp1"), Version.CURRENT, "test"))).build();
         for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.EXISTING_INDEX_RESTORED));
         }
@@ -162,7 +154,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsFromDangling(metaData.index("test")).build()).build();
+                .routingTable(RoutingTable.builder().addAsFromDangling(metaData.index("test"))).build();
         for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.DANGLING_INDEX_IMPORTED));
         }
@@ -176,7 +168,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test")).build()).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test"))).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1"))).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
@@ -186,7 +178,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
             builder.addIndexShard(indexShardRoutingTable);
         }
         builder.addReplica();
-        clusterState = ClusterState.builder(clusterState).routingTable(RoutingTable.builder(clusterState.routingTable()).add(builder).build()).build();
+        clusterState = ClusterState.builder(clusterState).routingTable(RoutingTable.builder(clusterState.routingTable()).add(builder)).build();
         assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1));
         assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo(), notNullValue());
         assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.REPLICA_ADDED));
@@ -219,7 +211,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test")).build()).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test"))).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2"))).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
@@ -249,7 +241,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test")).build()).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test"))).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2"))).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
@@ -313,7 +305,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test1")).addAsNew(metaData.index("test2")).build()).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test1")).addAsNew(metaData.index("test2"))).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2"))).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         assertThat(UnassignedInfo.getNumberOfDelayedUnassigned(System.currentTimeMillis(),
@@ -339,7 +331,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test1")).addAsNew(metaData.index("test2")).build()).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test1")).addAsNew(metaData.index("test2"))).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2"))).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         assertThat(UnassignedInfo.getNumberOfDelayedUnassigned(System.currentTimeMillis(),
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java
index 836422f..a35e9f4 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.cluster.routing.allocation;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
-
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -236,8 +235,8 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
 
 
     private void assertNumIndexShardsPerNode(ClusterState state, Matcher<Integer> matcher) {
-        for (ObjectCursor<String> index : state.routingTable().indicesRouting().keys()) {
-            assertNumIndexShardsPerNode(state, index.value, matcher);
+        for (String index : state.routingTable().indicesRouting().keySet()) {
+            assertNumIndexShardsPerNode(state, index, matcher);
         }
     }
 
@@ -249,10 +248,10 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
 
 
     private void assertAtLeastOneIndexShardPerNode(ClusterState state) {
-        for (ObjectCursor<String> index : state.routingTable().indicesRouting().keys()) {
+        for (String index : state.routingTable().indicesRouting().keySet()) {
 
             for (RoutingNode node : state.getRoutingNodes()) {
-                assertThat(node.shardsWithState(index.value, STARTED).size(), Matchers.greaterThanOrEqualTo(1));
+                assertThat(node.shardsWithState(index, STARTED).size(), Matchers.greaterThanOrEqualTo(1));
             }
         }
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java
index 96cba27..a983d88 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster.routing.allocation;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -42,10 +43,7 @@ import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
-import static java.util.Collections.singletonMap;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.RELOCATING;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.STARTED;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.*;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.hamcrest.Matchers.equalTo;
 
@@ -120,7 +118,7 @@ public class AllocationCommandsTests extends ESAllocationTestCase {
                 .put(newNode("node1"))
                 .put(newNode("node2"))
                 .put(newNode("node3"))
-                .put(newNode("node4", singletonMap("data", Boolean.FALSE.toString())))
+                .put(newNode("node4", ImmutableMap.of("data", Boolean.FALSE.toString())))
         ).build();
         RoutingAllocation.Result rerouteResult = allocation.reroute(clusterState);
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java
index e17ac30..7f050f3 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster.routing.allocation;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -37,15 +38,9 @@ import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
-import static java.util.Collections.singletonMap;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.RELOCATING;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.STARTED;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.UNASSIGNED;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.*;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.sameInstance;
+import static org.hamcrest.Matchers.*;
 
 /**
  */
@@ -75,8 +70,8 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", singletonMap("rack_id", "1")))
-                .put(newNode("node2", singletonMap("rack_id", "1")))
+                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -94,7 +89,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node3", singletonMap("rack_id", "2")))
+                .put(newNode("node3", ImmutableMap.of("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -114,7 +109,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, make sure nothing moves");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node4", singletonMap("rack_id", "3")))
+                .put(newNode("node4", ImmutableMap.of("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         assertThat(routingTable, sameInstance(clusterState.routingTable()));
@@ -144,9 +139,9 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", singletonMap("rack_id", "1")))
-                .put(newNode("node2", singletonMap("rack_id", "1")))
-                .put(newNode("node3", singletonMap("rack_id", "1")))
+                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node3", ImmutableMap.of("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -164,7 +159,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node4", singletonMap("rack_id", "2")))
+                .put(newNode("node4", ImmutableMap.of("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -184,7 +179,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, make sure nothing moves");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node5", singletonMap("rack_id", "3")))
+                .put(newNode("node5", ImmutableMap.of("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         assertThat(routingTable, sameInstance(clusterState.routingTable()));
@@ -219,8 +214,8 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", singletonMap("rack_id", "1")))
-                .put(newNode("node2", singletonMap("rack_id", "1")))
+                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -252,7 +247,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node3", singletonMap("rack_id", "2")))
+                .put(newNode("node3", ImmutableMap.of("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -277,7 +272,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, some more relocation should happen");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node4", singletonMap("rack_id", "3")))
+                .put(newNode("node4", ImmutableMap.of("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -319,8 +314,8 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", singletonMap("rack_id", "1")))
-                .put(newNode("node2", singletonMap("rack_id", "1")))
+                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -338,7 +333,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node3", singletonMap("rack_id", "2")))
+                .put(newNode("node3", ImmutableMap.of("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -367,7 +362,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, some more relocation should happen");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node4", singletonMap("rack_id", "3")))
+                .put(newNode("node4", ImmutableMap.of("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -411,8 +406,8 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", singletonMap("rack_id", "1")))
-                .put(newNode("node2", singletonMap("rack_id", "1")))
+                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -430,7 +425,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node3", singletonMap("rack_id", "2")))
+                .put(newNode("node3", ImmutableMap.of("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -450,7 +445,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, we will have another relocation");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node4", singletonMap("rack_id", "3")))
+                .put(newNode("node4", ImmutableMap.of("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -490,10 +485,10 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", singletonMap("rack_id", "1")))
-                .put(newNode("node2", singletonMap("rack_id", "1")))
-                .put(newNode("node3", singletonMap("rack_id", "1")))
-                .put(newNode("node4", singletonMap("rack_id", "1")))
+                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node3", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node4", ImmutableMap.of("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -511,7 +506,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node5", singletonMap("rack_id", "2")))
+                .put(newNode("node5", ImmutableMap.of("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -531,7 +526,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, we will have another relocation");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node6", singletonMap("rack_id", "3")))
+                .put(newNode("node6", ImmutableMap.of("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -572,8 +567,8 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", singletonMap("rack_id", "1")))
-                .put(newNode("node2", singletonMap("rack_id", "1")))
+                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -589,7 +584,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node3", singletonMap("rack_id", "2")))
+                .put(newNode("node3", ImmutableMap.of("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -609,7 +604,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, make sure nothing moves");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node4", singletonMap("rack_id", "3")))
+                .put(newNode("node4", ImmutableMap.of("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         assertThat(routingTable, sameInstance(clusterState.routingTable()));
@@ -640,9 +635,9 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", singletonMap("rack_id", "1")))
-                .put(newNode("node2", singletonMap("rack_id", "1")))
-                .put(newNode("node3", singletonMap("rack_id", "1")))
+                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node3", ImmutableMap.of("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -658,7 +653,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node4", singletonMap("rack_id", "2")))
+                .put(newNode("node4", ImmutableMap.of("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -678,7 +673,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, make sure nothing moves");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node5", singletonMap("rack_id", "3")))
+                .put(newNode("node5", ImmutableMap.of("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         assertThat(routingTable, sameInstance(clusterState.routingTable()));
@@ -716,8 +711,8 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", singletonMap("rack_id", "1")))
-                .put(newNode("node2", singletonMap("rack_id", "1")))
+                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -731,7 +726,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node3", singletonMap("rack_id", "2")))
+                .put(newNode("node3", ImmutableMap.of("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -755,7 +750,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, some more relocation should happen");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node4", singletonMap("rack_id", "3")))
+                .put(newNode("node4", ImmutableMap.of("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -796,8 +791,8 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes in different zones and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("A-0", singletonMap("zone", "a")))
-                .put(newNode("B-0", singletonMap("zone", "b")))
+                .put(newNode("A-0", ImmutableMap.of("zone", "a")))
+                .put(newNode("B-0", ImmutableMap.of("zone", "b")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -818,7 +813,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node in zone 'a' and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("A-1", singletonMap("zone", "a")))
+                .put(newNode("A-1", ImmutableMap.of("zone", "a")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -858,12 +853,12 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding 5 nodes in different zones and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                        .put(newNode("A-0", singletonMap("zone", "a")))
-                        .put(newNode("A-1", singletonMap("zone", "a")))
-                        .put(newNode("A-2", singletonMap("zone", "a")))
-                        .put(newNode("A-3", singletonMap("zone", "a")))
-                        .put(newNode("A-4", singletonMap("zone", "a")))
-                        .put(newNode("B-0", singletonMap("zone", "b")))
+                        .put(newNode("A-0", ImmutableMap.of("zone", "a")))
+                        .put(newNode("A-1", ImmutableMap.of("zone", "a")))
+                        .put(newNode("A-2", ImmutableMap.of("zone", "a")))
+                        .put(newNode("A-3", ImmutableMap.of("zone", "a")))
+                        .put(newNode("A-4", ImmutableMap.of("zone", "a")))
+                        .put(newNode("B-0", ImmutableMap.of("zone", "b")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
index 2bd18f8..e17fe47 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.cluster.routing.allocation;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
-
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.EmptyClusterInfoService;
@@ -28,11 +27,7 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.RoutingNode;
-import org.elasticsearch.cluster.routing.RoutingNodes;
-import org.elasticsearch.cluster.routing.RoutingTable;
-import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.cluster.routing.ShardRoutingState;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator;
 import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocator;
 import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators;
@@ -40,9 +35,9 @@ import org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllo
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.elasticsearch.test.gateway.NoopGatewayAllocator;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.hamcrest.Matchers;
 import org.junit.Test;
 
@@ -251,11 +246,11 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
         final int minAvgNumberOfShards = Math.round(Math.round(Math.floor(avgNumShards - treshold)));
         final int maxAvgNumberOfShards = Math.round(Math.round(Math.ceil(avgNumShards + treshold)));
 
-        for (ObjectCursor<String> index : nodes.getRoutingTable().indicesRouting().keys()) {
+        for (String index : nodes.getRoutingTable().indicesRouting().keySet()) {
             for (RoutingNode node : nodes) {
 //              logger.info(node.nodeId() +":"+index+ ": " + node.shardsWithState(index, INITIALIZING, STARTED).size() + " shards ("+minAvgNumberOfShards+" to "+maxAvgNumberOfShards+")");
-                assertThat(node.shardsWithState(index.value, STARTED).size(), Matchers.greaterThanOrEqualTo(minAvgNumberOfShards));
-                assertThat(node.shardsWithState(index.value, STARTED).size(), Matchers.lessThanOrEqualTo(maxAvgNumberOfShards));
+                assertThat(node.shardsWithState(index, STARTED).size(), Matchers.greaterThanOrEqualTo(minAvgNumberOfShards));
+                assertThat(node.shardsWithState(index, STARTED).size(), Matchers.lessThanOrEqualTo(maxAvgNumberOfShards));
             }
         }
     }
@@ -267,10 +262,10 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
         final int minAvgNumberOfShards = Math.round(Math.round(Math.floor(avgNumShards - treshold)));
         final int maxAvgNumberOfShards = Math.round(Math.round(Math.ceil(avgNumShards + treshold)));
 
-        for (ObjectCursor<String> index : nodes.getRoutingTable().indicesRouting().keys()) {
+        for (String index : nodes.getRoutingTable().indicesRouting().keySet()) {
             for (RoutingNode node : nodes) {
                 int primaries = 0;
-                for (ShardRouting shard : node.shardsWithState(index.value, STARTED)) {
+                for (ShardRouting shard : node.shardsWithState(index, STARTED)) {
                     primaries += shard.primary() ? 1 : 0;
                 }
 //                logger.info(node.nodeId() + ": " + primaries + " primaries ("+minAvgNumberOfShards+" to "+maxAvgNumberOfShards+")");
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java
index eb4d62a..e9a905d 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java
@@ -19,13 +19,14 @@
 
 package org.elasticsearch.cluster.routing.allocation;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.RoutingTable;
 import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.routing.RoutingTable;
 import org.elasticsearch.cluster.routing.ShardRoutingState;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
@@ -35,7 +36,6 @@ import org.junit.Test;
 
 import java.util.List;
 
-import static java.util.Collections.singletonMap;
 import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.hamcrest.Matchers.equalTo;
@@ -67,10 +67,10 @@ public class FilterRoutingTests extends ESAllocationTestCase {
 
         logger.info("--> adding four nodes and performing rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", singletonMap("tag1", "value1")))
-                .put(newNode("node2", singletonMap("tag1", "value2")))
-                .put(newNode("node3", singletonMap("tag1", "value3")))
-                .put(newNode("node4", singletonMap("tag1", "value4")))
+                .put(newNode("node1", ImmutableMap.of("tag1", "value1")))
+                .put(newNode("node2", ImmutableMap.of("tag1", "value2")))
+                .put(newNode("node3", ImmutableMap.of("tag1", "value3")))
+                .put(newNode("node4", ImmutableMap.of("tag1", "value4")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -116,10 +116,10 @@ public class FilterRoutingTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes and performing rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", singletonMap("tag1", "value1")))
-                .put(newNode("node2", singletonMap("tag1", "value2")))
-                .put(newNode("node3", singletonMap("tag1", "value3")))
-                .put(newNode("node4", singletonMap("tag1", "value4")))
+                .put(newNode("node1", ImmutableMap.of("tag1", "value1")))
+                .put(newNode("node2", ImmutableMap.of("tag1", "value2")))
+                .put(newNode("node3", ImmutableMap.of("tag1", "value3")))
+                .put(newNode("node4", ImmutableMap.of("tag1", "value4")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java
index 616949e..f307b63 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java
@@ -18,21 +18,18 @@
  */
 package org.elasticsearch.cluster.routing.allocation;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.RoutingTable;
 import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.routing.RoutingTable;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
-import static java.util.Collections.singletonMap;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.RELOCATING;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.STARTED;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.UNASSIGNED;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.*;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.hamcrest.Matchers.equalTo;
 
@@ -68,8 +65,8 @@ public class PreferLocalPrimariesToRelocatingPrimariesTests extends ESAllocation
 
         logger.info("adding two nodes and performing rerouting till all are allocated");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", singletonMap("tag1", "value1")))
-                .put(newNode("node2", singletonMap("tag1", "value2")))).build();
+                .put(newNode("node1", ImmutableMap.of("tag1", "value1")))
+                .put(newNode("node2", ImmutableMap.of("tag1", "value2")))).build();
 
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -104,7 +101,7 @@ public class PreferLocalPrimariesToRelocatingPrimariesTests extends ESAllocation
 
         logger.info("start node back up");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node1", singletonMap("tag1", "value1")))).build();
+                .put(newNode("node1", ImmutableMap.of("tag1", "value1")))).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java
index 86369b9..5213797 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java
@@ -19,14 +19,15 @@
 
 package org.elasticsearch.cluster.routing.allocation;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.RoutingTable;
 import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.routing.RoutingTable;
 import org.elasticsearch.cluster.routing.ShardRoutingState;
 import org.elasticsearch.cluster.routing.allocation.decider.SameShardAllocationDecider;
 import org.elasticsearch.common.logging.ESLogger;
@@ -35,7 +36,6 @@ import org.elasticsearch.common.transport.DummyTransportAddress;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
-import static java.util.Collections.emptyMap;
 import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
 import static org.elasticsearch.cluster.routing.allocation.RoutingNodesUtils.numberOfShardsOfType;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
@@ -62,8 +62,8 @@ public class SameShardRoutingTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes with the same host");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(new DiscoveryNode("node1", "node1", "test1", "test1", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT))
-                .put(new DiscoveryNode("node2", "node2", "test1", "test1", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT))).build();
+                .put(new DiscoveryNode("node1", "node1", "test1", "test1", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT))
+                .put(new DiscoveryNode("node2", "node2", "test1", "test1", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT))).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
@@ -78,7 +78,7 @@ public class SameShardRoutingTests extends ESAllocationTestCase {
 
         logger.info("--> add another node, with a different host, replicas will be allocating");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(new DiscoveryNode("node3", "node3", "test2", "test2", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT))).build();
+                .put(new DiscoveryNode("node3", "node3", "test2", "test2", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT))).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/StartedShardsRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/StartedShardsRoutingTests.java
index 1e8a5fb..dc23085 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/StartedShardsRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/StartedShardsRoutingTests.java
@@ -24,13 +24,7 @@ import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.AllocationId;
-import org.elasticsearch.cluster.routing.IndexRoutingTable;
-import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
-import org.elasticsearch.cluster.routing.RoutingTable;
-import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.cluster.routing.ShardRoutingState;
-import org.elasticsearch.cluster.routing.TestShardRouting;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
@@ -60,7 +54,7 @@ public class StartedShardsRoutingTests extends ESAllocationTestCase {
         stateBuilder.routingTable(RoutingTable.builder().add(IndexRoutingTable.builder("test")
                 .addIndexShard(new IndexShardRoutingTable.Builder(initShard.shardId()).addShard(initShard).build())
                 .addIndexShard(new IndexShardRoutingTable.Builder(startedShard.shardId()).addShard(startedShard).build())
-                .addIndexShard(new IndexShardRoutingTable.Builder(relocatingShard.shardId()).addShard(relocatingShard).build())).build());
+                .addIndexShard(new IndexShardRoutingTable.Builder(relocatingShard.shardId()).addShard(relocatingShard).build())));
 
         ClusterState state = stateBuilder.build();
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java
index 525c446..5852faf 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java
@@ -24,7 +24,7 @@ import org.elasticsearch.cluster.ClusterInfo;
 import org.elasticsearch.cluster.ClusterInfoService;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.DiskUsage;
-import org.elasticsearch.cluster.MockInternalClusterInfoService.DevNullClusterInfo;
+import org.elasticsearch.cluster.MockInternalClusterInfoService;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
@@ -43,7 +43,6 @@ import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators;
 import org.elasticsearch.cluster.routing.allocation.command.AllocationCommand;
 import org.elasticsearch.cluster.routing.allocation.command.AllocationCommands;
 import org.elasticsearch.cluster.routing.allocation.command.MoveAllocationCommand;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.LocalTransportAddress;
 import org.elasticsearch.index.shard.ShardId;
@@ -52,6 +51,7 @@ import org.elasticsearch.test.gateway.NoopGatewayAllocator;
 import org.junit.Test;
 
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Map;
@@ -78,18 +78,16 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, 0.7)
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, 0.8).build();
 
-        ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
-        usagesBuilder.put("node1", new DiskUsage("node1", "node1", "/dev/null", 100, 10)); // 90% used
-        usagesBuilder.put("node2", new DiskUsage("node2", "node2", "/dev/null", 100, 35)); // 65% used
-        usagesBuilder.put("node3", new DiskUsage("node3", "node3", "/dev/null", 100, 60)); // 40% used
-        usagesBuilder.put("node4", new DiskUsage("node4", "node4", "/dev/null", 100, 80)); // 20% used
-        ImmutableOpenMap<String, DiskUsage> usages = usagesBuilder.build();
+        Map<String, DiskUsage> usages = new HashMap<>();
+        usages.put("node1", new DiskUsage("node1", "node1", "/dev/null", 100, 10)); // 90% used
+        usages.put("node2", new DiskUsage("node2", "node2", "/dev/null", 100, 35)); // 65% used
+        usages.put("node3", new DiskUsage("node3", "node3", "/dev/null", 100, 60)); // 40% used
+        usages.put("node4", new DiskUsage("node4", "node4", "/dev/null", 100, 80)); // 20% used
 
-        ImmutableOpenMap.Builder<String, Long> shardSizesBuilder = ImmutableOpenMap.builder();
-        shardSizesBuilder.put("[test][0][p]", 10L); // 10 bytes
-        shardSizesBuilder.put("[test][0][r]", 10L);
-        ImmutableOpenMap<String, Long> shardSizes = shardSizesBuilder.build();
-        final ClusterInfo clusterInfo = new DevNullClusterInfo(usages, usages, shardSizes);
+        Map<String, Long> shardSizes = new HashMap<>();
+        shardSizes.put("[test][0][p]", 10L); // 10 bytes
+        shardSizes.put("[test][0][r]", 10L);
+        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(usages), Collections.unmodifiableMap(usages), Collections.unmodifiableMap(shardSizes), MockInternalClusterInfoService.DEV_NULL_MAP);
 
         AllocationDeciders deciders = new AllocationDeciders(Settings.EMPTY,
                 new HashSet<>(Arrays.asList(
@@ -273,19 +271,17 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, "30b")
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "9b").build();
 
-        ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
-        usagesBuilder.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 10)); // 90% used
-        usagesBuilder.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 10)); // 90% used
-        usagesBuilder.put("node3", new DiskUsage("node3", "n3", "/dev/null", 100, 60)); // 40% used
-        usagesBuilder.put("node4", new DiskUsage("node4", "n4", "/dev/null", 100, 80)); // 20% used
-        usagesBuilder.put("node5", new DiskUsage("node5", "n5", "/dev/null", 100, 85)); // 15% used
-        ImmutableOpenMap<String, DiskUsage> usages = usagesBuilder.build();
+        Map<String, DiskUsage> usages = new HashMap<>();
+        usages.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 10)); // 90% used
+        usages.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 10)); // 90% used
+        usages.put("node3", new DiskUsage("node3", "n3", "/dev/null", 100, 60)); // 40% used
+        usages.put("node4", new DiskUsage("node4", "n4", "/dev/null", 100, 80)); // 20% used
+        usages.put("node5", new DiskUsage("node5", "n5", "/dev/null", 100, 85)); // 15% used
 
-        ImmutableOpenMap.Builder<String, Long> shardSizesBuilder = ImmutableOpenMap.builder();
-        shardSizesBuilder.put("[test][0][p]", 10L); // 10 bytes
-        shardSizesBuilder.put("[test][0][r]", 10L);
-        ImmutableOpenMap<String, Long> shardSizes = shardSizesBuilder.build();
-        final ClusterInfo clusterInfo = new DevNullClusterInfo(usages, usages, shardSizes);
+        Map<String, Long> shardSizes = new HashMap<>();
+        shardSizes.put("[test][0][p]", 10L); // 10 bytes
+        shardSizes.put("[test][0][r]", 10L);
+        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(usages), Collections.unmodifiableMap(usages), Collections.unmodifiableMap(shardSizes), MockInternalClusterInfoService.DEV_NULL_MAP);
 
         AllocationDeciders deciders = new AllocationDeciders(Settings.EMPTY,
                 new HashSet<>(Arrays.asList(
@@ -346,10 +342,8 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         logger.info("--> nodeWithoutPrimary: {}", nodeWithoutPrimary);
 
         // Make node without the primary now habitable to replicas
-        usagesBuilder = ImmutableOpenMap.builder(usages);
-        usagesBuilder.put(nodeWithoutPrimary, new DiskUsage(nodeWithoutPrimary, "", "/dev/null", 100, 35)); // 65% used
-        usages = usagesBuilder.build();
-        final ClusterInfo clusterInfo2 = new DevNullClusterInfo(usages, usages, shardSizes);
+        usages.put(nodeWithoutPrimary, new DiskUsage(nodeWithoutPrimary, "", "/dev/null", 100, 35)); // 65% used
+        final ClusterInfo clusterInfo2 = new ClusterInfo(Collections.unmodifiableMap(usages), Collections.unmodifiableMap(usages), Collections.unmodifiableMap(shardSizes), MockInternalClusterInfoService.DEV_NULL_MAP);
         cis = new ClusterInfoService() {
             @Override
             public ClusterInfo getClusterInfo() {
@@ -542,15 +536,13 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, 0.7)
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "71%").build();
 
-        ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
-        usagesBuilder.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 31)); // 69% used
-        usagesBuilder.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 1));  // 99% used
-        ImmutableOpenMap<String, DiskUsage> usages = usagesBuilder.build();
+        Map<String, DiskUsage> usages = new HashMap<>();
+        usages.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 31)); // 69% used
+        usages.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 1));  // 99% used
 
-        ImmutableOpenMap.Builder<String, Long> shardSizesBuilder = ImmutableOpenMap.builder();
-        shardSizesBuilder.put("[test][0][p]", 10L); // 10 bytes
-        ImmutableOpenMap<String, Long> shardSizes = shardSizesBuilder.build();
-        final ClusterInfo clusterInfo = new DevNullClusterInfo(usages, usages, shardSizes);
+        Map<String, Long> shardSizes = new HashMap<>();
+        shardSizes.put("[test][0][p]", 10L); // 10 bytes
+        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(usages), Collections.unmodifiableMap(usages), Collections.unmodifiableMap(shardSizes), MockInternalClusterInfoService.DEV_NULL_MAP);
 
         AllocationDeciders deciders = new AllocationDeciders(Settings.EMPTY,
                 new HashSet<>(Arrays.asList(
@@ -610,16 +602,14 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, 0.7)
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, 0.85).build();
 
-        ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
-        usagesBuilder.put("node2", new DiskUsage("node2", "node2", "/dev/null", 100, 50)); // 50% used
-        usagesBuilder.put("node3", new DiskUsage("node3", "node3", "/dev/null", 100, 0));  // 100% used
-        ImmutableOpenMap<String, DiskUsage> usages = usagesBuilder.build();
+        Map<String, DiskUsage> usages = new HashMap<>();
+        usages.put("node2", new DiskUsage("node2", "node2", "/dev/null", 100, 50)); // 50% used
+        usages.put("node3", new DiskUsage("node3", "node3", "/dev/null", 100, 0));  // 100% used
 
-        ImmutableOpenMap.Builder<String, Long> shardSizesBuilder = ImmutableOpenMap.builder();
-        shardSizesBuilder.put("[test][0][p]", 10L); // 10 bytes
-        shardSizesBuilder.put("[test][0][r]", 10L); // 10 bytes
-        ImmutableOpenMap<String, Long> shardSizes = shardSizesBuilder.build();
-        final ClusterInfo clusterInfo = new DevNullClusterInfo(usages, usages, shardSizes);
+        Map<String, Long> shardSizes = new HashMap<>();
+        shardSizes.put("[test][0][p]", 10L); // 10 bytes
+        shardSizes.put("[test][0][r]", 10L); // 10 bytes
+        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(usages), Collections.unmodifiableMap(usages), Collections.unmodifiableMap(shardSizes), MockInternalClusterInfoService.DEV_NULL_MAP);
 
         AllocationDeciders deciders = new AllocationDeciders(Settings.EMPTY,
                 new HashSet<>(Arrays.asList(
@@ -683,11 +673,11 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         RoutingNode rn = new RoutingNode("node1", newNode("node1"));
         DiskThresholdDecider decider = new DiskThresholdDecider(Settings.EMPTY);
 
-        ImmutableOpenMap.Builder<String, DiskUsage> usages = ImmutableOpenMap.builder();
+        Map<String, DiskUsage> usages = new HashMap<>();
         usages.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 50)); // 50% used
         usages.put("node3", new DiskUsage("node3", "n3", "/dev/null", 100, 0));  // 100% used
 
-        DiskUsage node1Usage = decider.averageUsage(rn, usages.build());
+        DiskUsage node1Usage = decider.averageUsage(rn, usages);
         assertThat(node1Usage.getTotalBytes(), equalTo(100L));
         assertThat(node1Usage.getFreeBytes(), equalTo(25L));
     }
@@ -713,19 +703,17 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, 0.7)
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, 0.8).build();
 
-        ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
-        usagesBuilder.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 40)); // 60% used
-        usagesBuilder.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 40)); // 60% used
-        usagesBuilder.put("node3", new DiskUsage("node3", "n3", "/dev/null", 100, 40)); // 60% used
-        ImmutableOpenMap<String, DiskUsage> usages = usagesBuilder.build();
+        Map<String, DiskUsage> usages = new HashMap<>();
+        usages.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 40)); // 60% used
+        usages.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 40)); // 60% used
+        usages.put("node3", new DiskUsage("node3", "n3", "/dev/null", 100, 40)); // 60% used
 
-        ImmutableOpenMap.Builder<String, Long> shardSizesBuilder = ImmutableOpenMap.builder();
-        shardSizesBuilder.put("[test][0][p]", 14L); // 14 bytes
-        shardSizesBuilder.put("[test][0][r]", 14L);
-        shardSizesBuilder.put("[test2][0][p]", 1L); // 1 bytes
-        shardSizesBuilder.put("[test2][0][r]", 1L);
-        ImmutableOpenMap<String, Long> shardSizes = shardSizesBuilder.build();
-        final ClusterInfo clusterInfo = new DevNullClusterInfo(usages, usages, shardSizes);
+        Map<String, Long> shardSizes = new HashMap<>();
+        shardSizes.put("[test][0][p]", 14L); // 14 bytes
+        shardSizes.put("[test][0][r]", 14L);
+        shardSizes.put("[test2][0][p]", 1L); // 1 bytes
+        shardSizes.put("[test2][0][r]", 1L);
+        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(usages), Collections.unmodifiableMap(usages), Collections.unmodifiableMap(shardSizes), MockInternalClusterInfoService.DEV_NULL_MAP);
 
         AllocationDeciders deciders = new AllocationDeciders(Settings.EMPTY,
                 new HashSet<>(Arrays.asList(
@@ -821,17 +809,14 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "70%").build();
 
         // We have an index with 2 primary shards each taking 40 bytes. Each node has 100 bytes available
-        ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
-        usagesBuilder.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 20)); // 80% used
-        usagesBuilder.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 100)); // 0% used
-        ImmutableOpenMap<String, DiskUsage> usages = usagesBuilder.build();
-
-        ImmutableOpenMap.Builder<String, Long> shardSizesBuilder = ImmutableOpenMap.builder();
-        shardSizesBuilder.put("[test][0][p]", 40L);
-        shardSizesBuilder.put("[test][1][p]", 40L);
-        ImmutableOpenMap<String, Long> shardSizes = shardSizesBuilder.build();
+        Map<String, DiskUsage> usages = new HashMap<>();
+        usages.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 20)); // 80% used
+        usages.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 100)); // 0% used
 
-        final ClusterInfo clusterInfo = new DevNullClusterInfo(usages, usages, shardSizes);
+        Map<String, Long> shardSizes = new HashMap<>();
+        shardSizes.put("[test][0][p]", 40L);
+        shardSizes.put("[test][1][p]", 40L);
+        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(usages), Collections.unmodifiableMap(usages), Collections.unmodifiableMap(shardSizes), MockInternalClusterInfoService.DEV_NULL_MAP);
 
         DiskThresholdDecider diskThresholdDecider = new DiskThresholdDecider(diskSettings);
         MetaData metaData = MetaData.builder()
@@ -867,7 +852,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                                         .build()
                         )
         );
-        ClusterState clusterState = ClusterState.builder(baseClusterState).routingTable(builder.build()).build();
+        ClusterState clusterState = ClusterState.builder(baseClusterState).routingTable(builder).build();
         RoutingAllocation routingAllocation = new RoutingAllocation(null, new RoutingNodes(clusterState), discoveryNodes, clusterInfo);
         Decision decision = diskThresholdDecider.canRemain(firstRouting, firstRoutingNode, routingAllocation);
         assertThat(decision.type(), equalTo(Decision.Type.NO));
@@ -887,7 +872,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                                         .build()
                         )
         );
-        clusterState = ClusterState.builder(baseClusterState).routingTable(builder.build()).build();
+        clusterState = ClusterState.builder(baseClusterState).routingTable(builder).build();
         routingAllocation = new RoutingAllocation(null, new RoutingNodes(clusterState), discoveryNodes, clusterInfo);
         decision = diskThresholdDecider.canRemain(firstRouting, firstRoutingNode, routingAllocation);
         assertThat(decision.type(), equalTo(Decision.Type.YES));
@@ -932,17 +917,16 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, "60%")
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "70%").build();
 
-        ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
-        usagesBuilder.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 100)); // 0% used
-        usagesBuilder.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 20));  // 80% used
-        usagesBuilder.put("node3", new DiskUsage("node3", "n3", "/dev/null", 100, 100)); // 0% used
-        ImmutableOpenMap<String, DiskUsage> usages = usagesBuilder.build();
+        Map<String, DiskUsage> usages = new HashMap<>();
+        usages.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 100)); // 0% used
+        usages.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 20));  // 80% used
+        usages.put("node3", new DiskUsage("node3", "n3", "/dev/null", 100, 100)); // 0% used
 
         // We have an index with 1 primary shards each taking 40 bytes. Each node has 100 bytes available
-        ImmutableOpenMap.Builder<String, Long> shardSizes = ImmutableOpenMap.builder();
+        Map<String, Long> shardSizes = new HashMap<>();
         shardSizes.put("[test][0][p]", 40L);
         shardSizes.put("[test][1][p]", 40L);
-        final ClusterInfo clusterInfo = new DevNullClusterInfo(usages, usages, shardSizes.build());
+        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(usages), Collections.unmodifiableMap(usages), Collections.unmodifiableMap(shardSizes), MockInternalClusterInfoService.DEV_NULL_MAP);
 
         DiskThresholdDecider diskThresholdDecider = new DiskThresholdDecider(diskSettings);
         MetaData metaData = MetaData.builder()
@@ -986,7 +970,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                                         .build()
                         )
         );
-        ClusterState clusterState = ClusterState.builder(baseClusterState).routingTable(builder.build()).build();
+        ClusterState clusterState = ClusterState.builder(baseClusterState).routingTable(builder).build();
         RoutingAllocation routingAllocation = new RoutingAllocation(null, new RoutingNodes(clusterState), discoveryNodes, clusterInfo);
         Decision decision = diskThresholdDecider.canRemain(firstRouting, firstRoutingNode, routingAllocation);
 
@@ -1043,7 +1027,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                         )
         );
 
-        clusterState = ClusterState.builder(updateClusterState).routingTable(builder.build()).build();
+        clusterState = ClusterState.builder(updateClusterState).routingTable(builder).build();
         routingAllocation = new RoutingAllocation(null, new RoutingNodes(clusterState), discoveryNodes, clusterInfo);
         decision = diskThresholdDecider.canRemain(firstRouting, firstRoutingNode, routingAllocation);
         assertThat(decision.type(), equalTo(Decision.Type.YES));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java
index 5417a9b..6460664 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java
@@ -25,7 +25,7 @@ import org.elasticsearch.cluster.ClusterInfoService;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.DiskUsage;
 import org.elasticsearch.cluster.EmptyClusterInfoService;
-import org.elasticsearch.cluster.MockInternalClusterInfoService.DevNullClusterInfo;
+import org.elasticsearch.cluster.MockInternalClusterInfoService;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
@@ -36,7 +36,6 @@ import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingHelper;
 import org.elasticsearch.cluster.routing.UnassignedInfo;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.DummyTransportAddress;
 import org.elasticsearch.common.transport.LocalTransportAddress;
@@ -46,6 +45,9 @@ import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
 import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
 
 import static org.hamcrest.CoreMatchers.equalTo;
 
@@ -121,17 +123,17 @@ public class DiskThresholdDeciderUnitTests extends ESTestCase {
         ).build();
 
         // actual test -- after all that bloat :)
-        ImmutableOpenMap.Builder<String, DiskUsage> leastAvailableUsages = ImmutableOpenMap.builder();
+        Map<String, DiskUsage> leastAvailableUsages = new HashMap<>();
         leastAvailableUsages.put("node_0", new DiskUsage("node_0", "node_0", "_na_", 100, 0)); // all full
         leastAvailableUsages.put("node_1", new DiskUsage("node_1", "node_1", "_na_", 100, 0)); // all full
 
-        ImmutableOpenMap.Builder<String, DiskUsage> mostAvailableUsage = ImmutableOpenMap.builder();
+        Map<String, DiskUsage> mostAvailableUsage = new HashMap<>();
         mostAvailableUsage.put("node_0", new DiskUsage("node_0", "node_0", "_na_", 100, randomIntBetween(20, 100))); // 20 - 99 percent since after allocation there must be at least 10% left and shard is 10byte
         mostAvailableUsage.put("node_1", new DiskUsage("node_1", "node_1", "_na_", 100, randomIntBetween(0, 10))); // this is weird and smells like a bug! it should be up to 20%?
 
-        ImmutableOpenMap.Builder<String, Long> shardSizes = ImmutableOpenMap.builder();
+        Map<String, Long> shardSizes = new HashMap<>();
         shardSizes.put("[test][0][p]", 10L); // 10 bytes
-        final ClusterInfo clusterInfo = new ClusterInfo(leastAvailableUsages.build(), mostAvailableUsage.build(), shardSizes.build(), ImmutableOpenMap.of());
+        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(leastAvailableUsages), Collections.unmodifiableMap(mostAvailableUsage), Collections.unmodifiableMap(shardSizes), Collections.EMPTY_MAP);
         RoutingAllocation allocation = new RoutingAllocation(new AllocationDeciders(Settings.EMPTY, new AllocationDecider[]{decider}), clusterState.getRoutingNodes(), clusterState.nodes(), clusterInfo);
         assertEquals(mostAvailableUsage.toString(), Decision.YES, decider.canAllocate(test_0, new RoutingNode("node_0", node_0), allocation));
         assertEquals(mostAvailableUsage.toString(), Decision.NO, decider.canAllocate(test_0, new RoutingNode("node_1", node_1), allocation));
@@ -141,7 +143,7 @@ public class DiskThresholdDeciderUnitTests extends ESTestCase {
         NodeSettingsService nss = new NodeSettingsService(Settings.EMPTY);
         ClusterInfoService cis = EmptyClusterInfoService.INSTANCE;
         DiskThresholdDecider decider = new DiskThresholdDecider(Settings.EMPTY, nss, cis, null);
-        ImmutableOpenMap.Builder<ShardRouting, String> shardRoutingMap = ImmutableOpenMap.builder();
+        Map<ShardRouting, String> shardRoutingMap = new HashMap<>();
 
         DiscoveryNode node_0 = new DiscoveryNode("node_0", DummyTransportAddress.INSTANCE, Version.CURRENT);
         DiscoveryNode node_1 = new DiscoveryNode("node_1", DummyTransportAddress.INSTANCE, Version.CURRENT);
@@ -156,16 +158,6 @@ public class DiskThresholdDeciderUnitTests extends ESTestCase {
         ShardRoutingHelper.moveToStarted(test_1);
         shardRoutingMap.put(test_1, "/node1/least");
 
-        ShardRouting test_2 = ShardRouting.newUnassigned("test", 2, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo"));
-        ShardRoutingHelper.initialize(test_2, node_1.getId());
-        ShardRoutingHelper.moveToStarted(test_2);
-        shardRoutingMap.put(test_2, "/node1/most");
-
-        ShardRouting test_3 = ShardRouting.newUnassigned("test", 3, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo"));
-        ShardRoutingHelper.initialize(test_3, node_1.getId());
-        ShardRoutingHelper.moveToStarted(test_3);
-        // Intentionally not in the shardRoutingMap. We want to test what happens when we don't know where it is.
-
         MetaData metaData = MetaData.builder()
                 .put(IndexMetaData.builder("test").settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1))
                 .build();
@@ -183,20 +175,20 @@ public class DiskThresholdDeciderUnitTests extends ESTestCase {
         ).build();
 
         // actual test -- after all that bloat :)
-        ImmutableOpenMap.Builder<String, DiskUsage> leastAvailableUsages = ImmutableOpenMap.builder();
+        Map<String, DiskUsage> leastAvailableUsages = new HashMap<>();
         leastAvailableUsages.put("node_0", new DiskUsage("node_0", "node_0", "/node0/least", 100, 10)); // 90% used
         leastAvailableUsages.put("node_1", new DiskUsage("node_1", "node_1", "/node1/least", 100, 9)); // 91% used
 
-        ImmutableOpenMap.Builder<String, DiskUsage> mostAvailableUsage = ImmutableOpenMap.builder();
+        Map<String, DiskUsage> mostAvailableUsage = new HashMap<>();
         mostAvailableUsage.put("node_0", new DiskUsage("node_0", "node_0", "/node0/most", 100, 90)); // 10% used
         mostAvailableUsage.put("node_1", new DiskUsage("node_1", "node_1", "/node1/most", 100, 90)); // 10% used
 
-        ImmutableOpenMap.Builder<String, Long> shardSizes = ImmutableOpenMap.builder();
+        Map<String, Long> shardSizes = new HashMap<>();
         shardSizes.put("[test][0][p]", 10L); // 10 bytes
         shardSizes.put("[test][1][p]", 10L);
         shardSizes.put("[test][2][p]", 10L);
 
-        final ClusterInfo clusterInfo = new ClusterInfo(leastAvailableUsages.build(), mostAvailableUsage.build(), shardSizes.build(), shardRoutingMap.build());
+        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(leastAvailableUsages), Collections.unmodifiableMap(mostAvailableUsage), Collections.unmodifiableMap(shardSizes), shardRoutingMap);
         RoutingAllocation allocation = new RoutingAllocation(new AllocationDeciders(Settings.EMPTY, new AllocationDecider[]{decider}), clusterState.getRoutingNodes(), clusterState.nodes(), clusterInfo);
         assertEquals(Decision.YES, decider.canRemain(test_0, new RoutingNode("node_0", node_0), allocation));
         assertEquals(Decision.NO, decider.canRemain(test_1, new RoutingNode("node_1", node_1), allocation));
@@ -213,19 +205,26 @@ public class DiskThresholdDeciderUnitTests extends ESTestCase {
             // not allocated on that node
         }
 
+        ShardRouting test_2 = ShardRouting.newUnassigned("test", 2, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo"));
+        ShardRoutingHelper.initialize(test_2, node_1.getId());
+        ShardRoutingHelper.moveToStarted(test_2);
+        shardRoutingMap.put(test_2, "/node1/most");
         assertEquals("can stay since allocated on a different path with enough space", Decision.YES, decider.canRemain(test_2, new RoutingNode("node_1", node_1), allocation));
 
+        ShardRouting test_3 = ShardRouting.newUnassigned("test", 3, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo"));
+        ShardRoutingHelper.initialize(test_3, node_1.getId());
+        ShardRoutingHelper.moveToStarted(test_3);
         assertEquals("can stay since we don't have information about this shard", Decision.YES, decider.canRemain(test_2, new RoutingNode("node_1", node_1), allocation));
     }
 
 
     public void testShardSizeAndRelocatingSize() {
-        ImmutableOpenMap.Builder<String, Long> shardSizes = ImmutableOpenMap.builder();
+        Map<String, Long> shardSizes = new HashMap<>();
         shardSizes.put("[test][0][r]", 10L);
         shardSizes.put("[test][1][r]", 100L);
         shardSizes.put("[test][2][r]", 1000L);
         shardSizes.put("[other][0][p]", 10000L);
-        ClusterInfo info = new DevNullClusterInfo(ImmutableOpenMap.of(), ImmutableOpenMap.of(), shardSizes.build());
+        ClusterInfo info = new ClusterInfo(Collections.EMPTY_MAP, Collections.EMPTY_MAP, shardSizes, MockInternalClusterInfoService.DEV_NULL_MAP);
         ShardRouting test_0 = ShardRouting.newUnassigned("test", 0, null, false, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo"));
         ShardRoutingHelper.initialize(test_0, "node1");
         ShardRoutingHelper.moveToStarted(test_0);
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/operation/hash/murmur3/Murmur3HashFunctionTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/operation/hash/murmur3/Murmur3HashFunctionTests.java
index ed454ae..4dcc5ac 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/operation/hash/murmur3/Murmur3HashFunctionTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/operation/hash/murmur3/Murmur3HashFunctionTests.java
@@ -24,8 +24,6 @@ import org.elasticsearch.test.ESTestCase;
 
 public class Murmur3HashFunctionTests extends ESTestCase {
 
-    private static Murmur3HashFunction HASH = new Murmur3HashFunction();
-
     public void testKnownValues() {
         assertHash(0x5a0cb7c3, "hell");
         assertHash(0xd7c31989, "hello");
@@ -37,6 +35,6 @@ public class Murmur3HashFunctionTests extends ESTestCase {
     }
 
     private static void assertHash(int expected, String stringInput) {
-        assertEquals(expected, HASH.hash(stringInput));
+        assertEquals(expected, Murmur3HashFunction.hash(stringInput));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java b/core/src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java
index 87280f6..fe782f1 100644
--- a/core/src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster.serialization;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.AbstractDiffable;
 import org.elasticsearch.cluster.Diff;
 import org.elasticsearch.cluster.DiffableUtils;
@@ -35,24 +36,23 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
 import static org.hamcrest.CoreMatchers.equalTo;
 
 public class DiffableTests extends ESTestCase {
 
     @Test
     public void testJdkMapDiff() throws IOException {
-        Map<String, TestDiffable> before = new HashMap<>();
-        before.put("foo", new TestDiffable("1"));
-        before.put("bar", new TestDiffable("2"));
-        before.put("baz", new TestDiffable("3"));
-        before = unmodifiableMap(before);
+        ImmutableMap.Builder<String, TestDiffable> builder = ImmutableMap.builder();
+        builder.put("foo", new TestDiffable("1"));
+        builder.put("bar", new TestDiffable("2"));
+        builder.put("baz", new TestDiffable("3"));
+        ImmutableMap<String, TestDiffable> before = builder.build();
         Map<String, TestDiffable> map = new HashMap<>();
         map.putAll(before);
         map.remove("bar");
         map.put("baz", new TestDiffable("4"));
         map.put("new", new TestDiffable("5"));
-        Map<String, TestDiffable> after = unmodifiableMap(new HashMap<>(map));
+        ImmutableMap<String, TestDiffable> after = ImmutableMap.copyOf(map);
         Diff diff = DiffableUtils.diff(before, after);
         BytesStreamOutput out = new BytesStreamOutput();
         diff.writeTo(out);
diff --git a/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java b/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java
index 236378e..d405fb1 100644
--- a/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java
@@ -19,43 +19,28 @@
 
 package org.elasticsearch.cluster.structure;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.GroupShardsIterator;
-import org.elasticsearch.cluster.routing.OperationRouting;
-import org.elasticsearch.cluster.routing.PlainShardIterator;
-import org.elasticsearch.cluster.routing.RotationShardShuffler;
-import org.elasticsearch.cluster.routing.RoutingTable;
-import org.elasticsearch.cluster.routing.ShardIterator;
-import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.cluster.routing.ShardShuffler;
-import org.elasticsearch.cluster.routing.ShardsIterator;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.cluster.routing.allocation.AllocationService;
 import org.elasticsearch.cluster.routing.allocation.decider.AwarenessAllocationDecider;
 import org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider;
+import org.elasticsearch.cluster.routing.OperationRouting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
 import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
 
-import static java.util.Collections.singletonMap;
-import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
-import static org.hamcrest.Matchers.sameInstance;
+import static org.hamcrest.Matchers.*;
 
 public class RoutingIteratorTests extends ESAllocationTestCase {
 
@@ -246,15 +231,9 @@ public class RoutingIteratorTests extends ESAllocationTestCase {
 
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
 
-        Map<String, String> node1Attributes = new HashMap<>();
-        node1Attributes.put("rack_id", "rack_1");
-        node1Attributes.put("zone", "zone1");
-        Map<String, String> node2Attributes = new HashMap<>();
-        node2Attributes.put("rack_id", "rack_2");
-        node2Attributes.put("zone", "zone2");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", unmodifiableMap(node1Attributes)))
-                .put(newNode("node2", unmodifiableMap(node2Attributes)))
+                .put(newNode("node1", ImmutableMap.of("rack_id", "rack_1", "zone", "zone1")))
+                .put(newNode("node2", ImmutableMap.of("rack_id", "rack_2", "zone", "zone2")))
                 .localNodeId("node1")
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
@@ -302,8 +281,8 @@ public class RoutingIteratorTests extends ESAllocationTestCase {
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
 
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                        .put(newNode("fred","node1", singletonMap("disk", "ebs")))
-                        .put(newNode("barney","node2", singletonMap("disk", "ephemeral")))
+                        .put(newNode("fred","node1", ImmutableMap.of("disk", "ebs")))
+                        .put(newNode("barney","node2", ImmutableMap.of("disk", "ephemeral")))
                         .localNodeId("node1")
         ).build();
 
@@ -335,7 +314,7 @@ public class RoutingIteratorTests extends ESAllocationTestCase {
         } catch (IllegalArgumentException illegal) {
             //expected exception
         }
-
+        
         shardsIterator = clusterState.routingTable().index("test").shard(0).onlyNodeSelectorActiveInitializingShardsIt("fred",clusterState.nodes());
         assertThat(shardsIterator.size(), equalTo(1));
         assertThat(shardsIterator.nextOrNull().currentNodeId(),equalTo("node1"));
diff --git a/core/src/test/java/org/elasticsearch/common/cache/CacheTests.java b/core/src/test/java/org/elasticsearch/common/cache/CacheTests.java
new file mode 100644
index 0000000..d1481a5
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/cache/CacheTests.java
@@ -0,0 +1,536 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.cache;
+
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.util.*;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.concurrent.atomic.AtomicReferenceArray;
+
+import static org.hamcrest.CoreMatchers.instanceOf;
+
+public class CacheTests extends ESTestCase {
+    private int numberOfEntries;
+
+    @Before
+    public void setUp() throws Exception {
+        super.setUp();
+        numberOfEntries = randomIntBetween(1000, 10000);
+        logger.debug("numberOfEntries: " + numberOfEntries);
+    }
+
+    // cache some entries, then randomly lookup keys that do not exist, then check the stats
+    public void testCacheStats() {
+        AtomicLong evictions = new AtomicLong();
+        Set<Integer> keys = new HashSet<>();
+        Cache<Integer, String> cache =
+                CacheBuilder.<Integer, String>builder()
+                        .setMaximumWeight(numberOfEntries / 2)
+                        .removalListener(notification -> {
+                            keys.remove(notification.getKey());
+                            evictions.incrementAndGet();
+                        })
+                        .build();
+
+        for (int i = 0; i < numberOfEntries; i++) {
+            // track the keys, which will be removed upon eviction (see the RemovalListener)
+            keys.add(i);
+            cache.put(i, Integer.toString(i));
+        }
+        long hits = 0;
+        long misses = 0;
+        Integer missingKey = 0;
+        for (Integer key : keys) {
+            --missingKey;
+            if (rarely()) {
+                misses++;
+                cache.get(missingKey);
+            } else {
+                hits++;
+                cache.get(key);
+            }
+        }
+        assertEquals(hits, cache.stats().getHits());
+        assertEquals(misses, cache.stats().getMisses());
+        assertEquals((long) Math.ceil(numberOfEntries / 2.0), evictions.get());
+        assertEquals(evictions.get(), cache.stats().getEvictions());
+    }
+
+    // cache some entries in batches of size maximumWeight; for each batch, touch the even entries to affect the
+    // ordering; upon the next caching of entries, the entries from the previous batch will be evicted; we can then
+    // check that the evicted entries were evicted in LRU order (first the odds in a batch, then the evens in a batch)
+    // for each batch
+    public void testCacheEvictions() {
+        int maximumWeight = randomIntBetween(1, numberOfEntries);
+        AtomicLong evictions = new AtomicLong();
+        List<Integer> evictedKeys = new ArrayList<>();
+        Cache<Integer, String> cache =
+                CacheBuilder.<Integer, String>builder()
+                        .setMaximumWeight(maximumWeight)
+                        .removalListener(notification -> {
+                            evictions.incrementAndGet();
+                            evictedKeys.add(notification.getKey());
+                        })
+                        .build();
+        // cache entries up to numberOfEntries - maximumWeight; all of these entries will ultimately be evicted in
+        // batches of size maximumWeight, first the odds in the batch, then the evens in the batch
+        List<Integer> expectedEvictions = new ArrayList<>();
+        int iterations = (int)Math.ceil((numberOfEntries - maximumWeight) / (1.0 * maximumWeight));
+        for (int i = 0; i < iterations; i++) {
+            for (int j = i * maximumWeight; j < (i + 1) * maximumWeight && j < numberOfEntries - maximumWeight; j++) {
+                cache.put(j, Integer.toString(j));
+                if (j % 2 == 1) {
+                    expectedEvictions.add(j);
+                }
+            }
+            for (int j = i * maximumWeight; j < (i + 1) * maximumWeight && j < numberOfEntries - maximumWeight; j++) {
+                if (j % 2 == 0) {
+                    cache.get(j);
+                    expectedEvictions.add(j);
+                }
+            }
+        }
+        // finish filling the cache
+        for (int i = numberOfEntries - maximumWeight; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        assertEquals(numberOfEntries - maximumWeight, evictions.get());
+        assertEquals(evictions.get(), cache.stats().getEvictions());
+
+        // assert that the keys were evicted in LRU order
+        Set<Integer> keys = new HashSet<>();
+        List<Integer> remainingKeys = new ArrayList<>();
+        for (Integer key : cache.keys()) {
+            keys.add(key);
+            remainingKeys.add(key);
+        }
+        assertEquals(expectedEvictions.size(), evictedKeys.size());
+        for (int i = 0; i < expectedEvictions.size(); i++) {
+            assertFalse(keys.contains(expectedEvictions.get(i)));
+            assertEquals(expectedEvictions.get(i), evictedKeys.get(i));
+        }
+        for (int i = numberOfEntries - maximumWeight; i < numberOfEntries; i++) {
+            assertTrue(keys.contains(i));
+            assertEquals(
+                    numberOfEntries - i + (numberOfEntries - maximumWeight) - 1,
+                    (int) remainingKeys.get(i - (numberOfEntries - maximumWeight))
+            );
+        }
+    }
+
+    // cache some entries and exceed the maximum weight, then check that the cache has the expected weight and the
+    // expected evictions occurred
+    public void testWeigher() {
+        int maximumWeight = 2 * numberOfEntries;
+        int weight = randomIntBetween(2, 10);
+        AtomicLong evictions = new AtomicLong();
+        Cache<Integer, String> cache =
+                CacheBuilder.<Integer, String>builder()
+                        .setMaximumWeight(maximumWeight)
+                        .weigher((k, v) -> weight)
+                        .removalListener(notification -> evictions.incrementAndGet())
+                        .build();
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        // cache weight should be the largest multiple of weight less than maximumWeight
+        assertEquals(weight * (maximumWeight / weight), cache.weight());
+
+        // the number of evicted entries should be the number of entries that fit in the excess weight
+        assertEquals((int) Math.ceil((weight - 2) * numberOfEntries / (1.0 * weight)), evictions.get());
+
+        assertEquals(evictions.get(), cache.stats().getEvictions());
+    }
+
+    // cache some entries, randomly invalidate some of them, then check that the weight of the cache is correct
+    public void testWeight() {
+        Cache<Integer, String> cache =
+                CacheBuilder.<Integer, String>builder()
+                        .weigher((k, v) -> k)
+                        .build();
+        int weight = 0;
+        for (int i = 0; i < numberOfEntries; i++) {
+            weight += i;
+            cache.put(i, Integer.toString(i));
+        }
+        for (int i = 0; i < numberOfEntries; i++) {
+            if (rarely()) {
+                weight -= i;
+                cache.invalidate(i);
+            }
+        }
+        assertEquals(weight, cache.weight());
+    }
+
+    // cache some entries, randomly invalidate some of them, then check that the number of cached entries is correct
+    public void testCount() {
+        Cache<Integer, String> cache = CacheBuilder.<Integer, String>builder().build();
+        int count = 0;
+        for (int i = 0; i < numberOfEntries; i++) {
+            count++;
+            cache.put(i, Integer.toString(i));
+        }
+        for (int i = 0; i < numberOfEntries; i++) {
+            if (rarely()) {
+                count--;
+                cache.invalidate(i);
+            }
+        }
+        assertEquals(count, cache.count());
+    }
+
+    // cache some entries, step the clock forward, cache some more entries, step the clock forward and then check that
+    // the first batch of cached entries expired and were removed
+    public void testExpirationAfterAccess() {
+        AtomicLong now = new AtomicLong();
+        Cache<Integer, String> cache = new Cache<Integer, String>() {
+            @Override
+            protected long now() {
+                return now.get();
+            }
+        };
+        cache.setExpireAfterAccess(1);
+        List<Integer> evictedKeys = new ArrayList<>();
+        cache.setRemovalListener(notification -> {
+            assertEquals(RemovalNotification.RemovalReason.EVICTED, notification.getRemovalReason());
+            evictedKeys.add(notification.getKey());
+        });
+        now.set(0);
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        now.set(1);
+        for (int i = numberOfEntries; i < 2 * numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        now.set(2);
+        cache.refresh();
+        assertEquals(numberOfEntries, cache.count());
+        for (int i = 0; i < evictedKeys.size(); i++) {
+            assertEquals(i, (int) evictedKeys.get(i));
+        }
+        Set<Integer> remainingKeys = new HashSet<>();
+        for (Integer key : cache.keys()) {
+            remainingKeys.add(key);
+        }
+        for (int i = numberOfEntries; i < 2 * numberOfEntries; i++) {
+            assertTrue(remainingKeys.contains(i));
+        }
+    }
+
+    public void testExpirationAfterWrite() {
+        AtomicLong now = new AtomicLong();
+        Cache<Integer, String> cache = new Cache<Integer, String>() {
+            @Override
+            protected long now() {
+                return now.get();
+            }
+        };
+        cache.setExpireAfterWrite(1);
+        List<Integer> evictedKeys = new ArrayList<>();
+        cache.setRemovalListener(notification -> {
+            assertEquals(RemovalNotification.RemovalReason.EVICTED, notification.getRemovalReason());
+            evictedKeys.add(notification.getKey());
+        });
+        now.set(0);
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        now.set(1);
+        for (int i = numberOfEntries; i < 2 * numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        now.set(2);
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.get(i);
+        }
+        cache.refresh();
+        assertEquals(numberOfEntries, cache.count());
+        for (int i = 0; i < evictedKeys.size(); i++) {
+            assertEquals(i, (int) evictedKeys.get(i));
+        }
+        Set<Integer> remainingKeys = new HashSet<>();
+        for (Integer key : cache.keys()) {
+            remainingKeys.add(key);
+        }
+        for (int i = numberOfEntries; i < 2 * numberOfEntries; i++) {
+            assertTrue(remainingKeys.contains(i));
+        }
+    }
+
+    // randomly promote some entries, step the clock forward, then check that the promoted entries remain and the
+    // non-promoted entries were removed
+    public void testPromotion() {
+        AtomicLong now = new AtomicLong();
+        Cache<Integer, String> cache = new Cache<Integer, String>() {
+            @Override
+            protected long now() {
+                return now.get();
+            }
+        };
+        cache.setExpireAfterAccess(1);
+        now.set(0);
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        now.set(1);
+        Set<Integer> promotedKeys = new HashSet<>();
+        for (int i = 0; i < numberOfEntries; i++) {
+            if (rarely()) {
+                cache.get(i);
+                promotedKeys.add(i);
+            }
+        }
+        now.set(2);
+        cache.refresh();
+        assertEquals(promotedKeys.size(), cache.count());
+        for (int i = 0; i < numberOfEntries; i++) {
+            if (promotedKeys.contains(i)) {
+                assertNotNull(cache.get(i));
+            } else {
+                assertNull(cache.get(i));
+            }
+        }
+    }
+
+
+    // randomly invalidate some cached entries, then check that a lookup for each of those and only those keys is null
+    public void testInvalidate() {
+        Cache<Integer, String> cache = CacheBuilder.<Integer, String>builder().build();
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        Set<Integer> keys = new HashSet<>();
+        for (Integer key : cache.keys()) {
+            if (rarely()) {
+                cache.invalidate(key);
+                keys.add(key);
+            }
+        }
+        for (int i = 0; i < numberOfEntries; i++) {
+            if (keys.contains(i)) {
+                assertNull(cache.get(i));
+            } else {
+                assertNotNull(cache.get(i));
+            }
+        }
+    }
+
+    // randomly invalidate some cached entries, then check that we receive invalidate notifications for those and only
+    // those entries
+    public void testNotificationOnInvalidate() {
+        Set<Integer> notifications = new HashSet<>();
+        Cache<Integer, String> cache =
+                CacheBuilder.<Integer, String>builder()
+                        .removalListener(notification -> {
+                            assertEquals(RemovalNotification.RemovalReason.INVALIDATED, notification.getRemovalReason());
+                            notifications.add(notification.getKey());
+                        })
+                        .build();
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        Set<Integer> invalidated = new HashSet<>();
+        for (int i = 0; i < numberOfEntries; i++) {
+            if (rarely()) {
+                cache.invalidate(i);
+                invalidated.add(i);
+            }
+        }
+        assertEquals(notifications, invalidated);
+    }
+
+    // invalidate all cached entries, then check that the cache is empty
+    public void testInvalidateAll() {
+        Cache<Integer, String> cache = CacheBuilder.<Integer, String>builder().build();
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        cache.invalidateAll();
+        assertEquals(0, cache.count());
+        assertEquals(0, cache.weight());
+    }
+
+    // invalidate all cached entries, then check that we receive invalidate notifications for all entries
+    public void testNotificationOnInvalidateAll() {
+        Set<Integer> notifications = new HashSet<>();
+        Cache<Integer, String> cache =
+                CacheBuilder.<Integer, String>builder()
+                        .removalListener(notification -> {
+                            assertEquals(RemovalNotification.RemovalReason.INVALIDATED, notification.getRemovalReason());
+                            notifications.add(notification.getKey());
+                        })
+                        .build();
+        Set<Integer> invalidated = new HashSet<>();
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+            invalidated.add(i);
+        }
+        cache.invalidateAll();
+        assertEquals(invalidated, notifications);
+    }
+
+    // randomly replace some entries, increasing the weight by 1 for each replacement, then count that the cache size
+    // is correct
+    public void testReplaceRecomputesSize() {
+        class Key {
+            private int key;
+            private long weight;
+
+            public Key(int key, long weight) {
+                this.key = key;
+                this.weight = weight;
+            }
+
+            @Override
+            public boolean equals(Object o) {
+                if (this == o) return true;
+                if (o == null || getClass() != o.getClass()) return false;
+
+                Key key1 = (Key) o;
+
+                return key == key1.key;
+
+            }
+
+            @Override
+            public int hashCode() {
+                return key;
+            }
+        }
+        Cache<Key, String> cache = CacheBuilder.<Key, String>builder().weigher((k, s) -> k.weight).build();
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(new Key(i, 1), Integer.toString(i));
+        }
+        assertEquals(numberOfEntries, cache.count());
+        assertEquals(numberOfEntries, cache.weight());
+        int replaced = 0;
+        for (int i = 0; i < numberOfEntries; i++) {
+            if (rarely()) {
+                replaced++;
+                cache.put(new Key(i, 2), Integer.toString(i));
+            }
+        }
+        assertEquals(numberOfEntries, cache.count());
+        assertEquals(numberOfEntries + replaced, cache.weight());
+    }
+
+    // randomly replace some entries, then check that we received replacement notifications for those and only those
+    // entries
+    public void testNotificationOnReplace() {
+        Set<Integer> notifications = new HashSet<>();
+        Cache<Integer, String> cache =
+                CacheBuilder.<Integer, String>builder()
+                        .removalListener(notification -> {
+                            assertEquals(RemovalNotification.RemovalReason.REPLACED, notification.getRemovalReason());
+                            notifications.add(notification.getKey());
+                        })
+                        .build();
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        Set<Integer> replacements = new HashSet<>();
+        for (int i = 0; i < numberOfEntries; i++) {
+            if (rarely()) {
+                cache.put(i, Integer.toString(i) + Integer.toString(i));
+                replacements.add(i);
+            }
+        }
+        assertEquals(replacements, notifications);
+    }
+
+    public void testComputeIfAbsentCallsOnce() throws InterruptedException {
+        int numberOfThreads = randomIntBetween(2, 200);
+        final Cache<Integer, String> cache = CacheBuilder.<Integer, String>builder().build();
+        List<Thread> threads = new ArrayList<>();
+        AtomicReferenceArray flags = new AtomicReferenceArray(numberOfEntries);
+        for (int j = 0; j < numberOfEntries; j++) {
+            flags.set(j, false);
+        }
+        CountDownLatch latch = new CountDownLatch(1 + numberOfThreads);
+        for (int i = 0; i < numberOfThreads; i++) {
+            Thread thread = new Thread(() -> {
+                latch.countDown();
+                for (int j = 0; j < numberOfEntries; j++) {
+                    try {
+                        cache.computeIfAbsent(j, key -> {
+                            assertTrue(flags.compareAndSet(key, false, true));
+                            return Integer.toString(key);
+                        });
+                    } catch (ExecutionException e) {
+                        throw new RuntimeException(e);
+                    }
+                }
+            });
+            threads.add(thread);
+            thread.start();
+        }
+        latch.countDown();
+        for (Thread thread : threads) {
+            thread.join();
+        }
+    }
+
+    public void testComputeIfAbsentThrowsExceptionIfLoaderReturnsANullValue() {
+        final Cache<Integer, String> cache = CacheBuilder.<Integer, String>builder().build();
+        try {
+            cache.computeIfAbsent(1, k -> null);
+            fail("expected ExecutionException");
+        } catch (ExecutionException e) {
+            assertThat(e.getCause(), instanceOf(NullPointerException.class));
+        }
+    }
+
+    // test that the cache is not corrupted under lots of concurrent modifications, even hitting the same key
+    // here be dragons: this test did catch one subtle bug during development; do not remove lightly
+    public void testTorture() throws InterruptedException {
+        int numberOfThreads = randomIntBetween(2, 200);
+        final Cache<Integer, String> cache =
+                CacheBuilder.<Integer, String>builder()
+                        .setMaximumWeight(1000)
+                        .weigher((k, v) -> 2)
+                        .build();
+
+        CountDownLatch latch = new CountDownLatch(1 + numberOfThreads);
+        List<Thread> threads = new ArrayList<>();
+        for (int i = 0; i < numberOfThreads; i++) {
+            Thread thread = new Thread(() -> {
+                Random random = new Random(random().nextLong());
+                latch.countDown();
+                for (int j = 0; j < numberOfEntries; j++) {
+                    Integer key = random.nextInt(numberOfEntries);
+                    cache.put(key, Integer.toString(j));
+                }
+            });
+            threads.add(thread);
+            thread.start();
+        }
+        latch.countDown();
+        for (Thread thread : threads) {
+            thread.join();
+        }
+        cache.refresh();
+        assertEquals(500, cache.count());
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/cli/CliToolTests.java b/core/src/test/java/org/elasticsearch/common/cli/CliToolTests.java
index f275d1d..748e417 100644
--- a/core/src/test/java/org/elasticsearch/common/cli/CliToolTests.java
+++ b/core/src/test/java/org/elasticsearch/common/cli/CliToolTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.common.cli;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.commons.cli.CommandLine;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.Strings;
@@ -28,20 +29,14 @@ import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReference;
 
-import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.cli.CliTool.ExitStatus.OK;
 import static org.elasticsearch.common.cli.CliTool.ExitStatus.USAGE;
 import static org.elasticsearch.common.cli.CliToolConfig.Builder.cmd;
-import static org.hamcrest.Matchers.arrayContaining;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.hasItem;
-import static org.hamcrest.Matchers.hasSize;
-import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.*;
 
 /**
  *
@@ -392,11 +387,11 @@ public class CliToolTests extends CliToolTestCase {
             super(CliToolConfig.config(name, MultiCmdTool.class)
                     .cmds(cmds(commands))
                     .build(), terminal);
-            Map<String, Command> commandByName = new HashMap<>();
+            ImmutableMap.Builder<String, Command> commandByName = ImmutableMap.builder();
             for (int i = 0; i < commands.length; i++) {
                 commandByName.put(commands[i].name, commands[i]);
             }
-            this.commands = unmodifiableMap(commandByName);
+            this.commands = commandByName.build();
         }
 
         @Override
diff --git a/core/src/test/java/org/elasticsearch/common/collect/CopyOnWriteHashMapTests.java b/core/src/test/java/org/elasticsearch/common/collect/CopyOnWriteHashMapTests.java
index d35b540..f6372e5 100644
--- a/core/src/test/java/org/elasticsearch/common/collect/CopyOnWriteHashMapTests.java
+++ b/core/src/test/java/org/elasticsearch/common/collect/CopyOnWriteHashMapTests.java
@@ -19,13 +19,12 @@
 
 package org.elasticsearch.common.collect;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.test.ESTestCase;
 
 import java.util.HashMap;
 import java.util.Map;
 
-import static java.util.Collections.emptyMap;
-
 public class CopyOnWriteHashMapTests extends ESTestCase {
 
     private static class O {
@@ -95,7 +94,7 @@ public class CopyOnWriteHashMapTests extends ESTestCase {
                 map = newMap;
             }
             assertEquals(ref, CopyOnWriteHashMap.copyOf(ref));
-            assertEquals(emptyMap(), CopyOnWriteHashMap.copyOf(ref).copyAndRemoveAll(ref.keySet()));
+            assertEquals(ImmutableMap.of(), CopyOnWriteHashMap.copyOf(ref).copyAndRemoveAll(ref.keySet()));
         }
     }
 
@@ -141,7 +140,7 @@ public class CopyOnWriteHashMapTests extends ESTestCase {
         } catch (IllegalArgumentException e) {
             // expected
         }
-
+        
         try {
             new CopyOnWriteHashMap<>().copyAndPut(null, "b");
             fail();
diff --git a/core/src/test/java/org/elasticsearch/common/collect/EvictingQueueTests.java b/core/src/test/java/org/elasticsearch/common/collect/EvictingQueueTests.java
new file mode 100644
index 0000000..de822b8
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/collect/EvictingQueueTests.java
@@ -0,0 +1,150 @@
+/*
+ * Copyright (C) 2012 The Guava Authors
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.elasticsearch.common.collect;
+
+import org.elasticsearch.common.util.CollectionUtils;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.Collections;
+import java.util.NoSuchElementException;
+
+public class EvictingQueueTests extends ESTestCase {
+    public void testCreateWithNegativeSize() throws Exception {
+        try {
+            new EvictingQueue<>(-1);
+            fail();
+        } catch (IllegalArgumentException expected) {
+        }
+    }
+
+    public void testCreateWithZeroSize() throws Exception {
+        EvictingQueue<String> queue = new EvictingQueue<>(0);
+        assertEquals(0, queue.size());
+
+        assertTrue(queue.add("hi"));
+        assertEquals(0, queue.size());
+
+        assertTrue(queue.offer("hi"));
+        assertEquals(0, queue.size());
+
+        assertFalse(queue.remove("hi"));
+        assertEquals(0, queue.size());
+
+        try {
+            queue.element();
+            fail();
+        } catch (NoSuchElementException expected) {}
+
+        assertNull(queue.peek());
+        assertNull(queue.poll());
+        try {
+            queue.remove();
+            fail();
+        } catch (NoSuchElementException expected) {}
+    }
+
+    public void testRemainingCapacityMaximumSizeZero() {
+        EvictingQueue<String> queue = new EvictingQueue<>(0);
+        assertEquals(0, queue.remainingCapacity());
+    }
+
+    public void testRemainingCapacityMaximumSizeOne() {
+        EvictingQueue<String> queue = new EvictingQueue<>(1);
+        assertEquals(1, queue.remainingCapacity());
+        queue.add("hi");
+        assertEquals(0, queue.remainingCapacity());
+    }
+
+    public void testRemainingCapacityMaximumSizeThree() {
+        EvictingQueue<String> queue = new EvictingQueue<>(3);
+        assertEquals(3, queue.remainingCapacity());
+        queue.add("hi");
+        assertEquals(2, queue.remainingCapacity());
+        queue.add("hi");
+        assertEquals(1, queue.remainingCapacity());
+        queue.add("hi");
+        assertEquals(0, queue.remainingCapacity());
+    }
+
+    public void testEvictingAfterOne() throws Exception {
+        EvictingQueue<String> queue = new EvictingQueue<>(1);
+        assertEquals(0, queue.size());
+        assertEquals(1, queue.remainingCapacity());
+
+        assertTrue(queue.add("hi"));
+        assertEquals("hi", queue.element());
+        assertEquals("hi", queue.peek());
+        assertEquals(1, queue.size());
+        assertEquals(0, queue.remainingCapacity());
+
+        assertTrue(queue.add("there"));
+        assertEquals("there", queue.element());
+        assertEquals("there", queue.peek());
+        assertEquals(1, queue.size());
+        assertEquals(0, queue.remainingCapacity());
+
+        assertEquals("there", queue.remove());
+        assertEquals(0, queue.size());
+        assertEquals(1, queue.remainingCapacity());
+    }
+
+    public void testEvictingAfterThree() throws Exception {
+        EvictingQueue<String> queue = new EvictingQueue<>(3);
+        assertEquals(0, queue.size());
+        assertEquals(3, queue.remainingCapacity());
+
+        assertTrue(queue.add("one"));
+        assertTrue(queue.add("two"));
+        assertTrue(queue.add("three"));
+        assertEquals("one", queue.element());
+        assertEquals("one", queue.peek());
+        assertEquals(3, queue.size());
+        assertEquals(0, queue.remainingCapacity());
+
+        assertTrue(queue.add("four"));
+        assertEquals("two", queue.element());
+        assertEquals("two", queue.peek());
+        assertEquals(3, queue.size());
+        assertEquals(0, queue.remainingCapacity());
+
+        assertEquals("two", queue.remove());
+        assertEquals(2, queue.size());
+        assertEquals(1, queue.remainingCapacity());
+    }
+
+    public void testAddAll() throws Exception {
+        EvictingQueue<String> queue = new EvictingQueue<>(3);
+        assertEquals(0, queue.size());
+        assertEquals(3, queue.remainingCapacity());
+
+        assertTrue(queue.addAll(CollectionUtils.arrayAsArrayList("one", "two", "three")));
+        assertEquals("one", queue.element());
+        assertEquals("one", queue.peek());
+        assertEquals(3, queue.size());
+        assertEquals(0, queue.remainingCapacity());
+
+        assertTrue(queue.addAll(Collections.singletonList("four")));
+        assertEquals("two", queue.element());
+        assertEquals("two", queue.peek());
+        assertEquals(3, queue.size());
+        assertEquals(0, queue.remainingCapacity());
+
+        assertEquals("two", queue.remove());
+        assertEquals(2, queue.size());
+        assertEquals(1, queue.remainingCapacity());
+    }
+}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/common/collect/IteratorsTests.java b/core/src/test/java/org/elasticsearch/common/collect/IteratorsTests.java
new file mode 100644
index 0000000..9097218
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/collect/IteratorsTests.java
@@ -0,0 +1,162 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.collect;
+
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.*;
+
+public class IteratorsTests extends ESTestCase {
+    public void testConcatentation() {
+        List<Integer> threeTwoOne = Arrays.asList(3, 2, 1);
+        List<Integer> fourFiveSix = Arrays.asList(4, 5, 6);
+        Iterator<Integer> concat = Iterators.concat(threeTwoOne.iterator(), fourFiveSix.iterator());
+        assertContainsInOrder(concat, 3, 2, 1, 4, 5, 6);
+    }
+
+    public void testNoConcatenation() {
+        Iterator<Integer> iterator = Iterators.<Integer>concat();
+        assertEmptyIterator(iterator);
+    }
+
+    public void testEmptyConcatenation() {
+        Iterator<Integer> iterator = Iterators.<Integer>concat(empty());
+        assertEmptyIterator(iterator);
+    }
+
+    public void testMultipleEmptyConcatenation() {
+        Iterator<Integer> iterator = Iterators.concat(empty(), empty());
+        assertEmptyIterator(iterator);
+    }
+
+    public void testSingleton() {
+        int value = randomInt();
+        assertSingleton(value, singletonIterator(value));
+    }
+
+    public void testEmptyBeforeSingleton() {
+        int value = randomInt();
+        assertSingleton(value, empty(), singletonIterator(value));
+    }
+
+
+    public void testEmptyAfterSingleton() {
+        int value = randomInt();
+        assertSingleton(value, singletonIterator(value), empty());
+    }
+
+    public void testRandomSingleton() {
+        int numberOfIterators = randomIntBetween(1, 1000);
+        int singletonIndex = randomIntBetween(0, numberOfIterators - 1);
+        int value = randomInt();
+        Iterator<Integer>[] iterators = new Iterator[numberOfIterators];
+        for (int i = 0; i < numberOfIterators; i++) {
+            iterators[i] = i != singletonIndex ? empty() : singletonIterator(value);
+        }
+        assertSingleton(value, iterators);
+    }
+
+    public void testRandomIterators() {
+        int numberOfIterators = randomIntBetween(1, 1000);
+        Iterator<Integer>[] iterators = new Iterator[numberOfIterators];
+        List<Integer> values = new ArrayList<>();
+        for (int i = 0; i < numberOfIterators; i++) {
+            int numberOfValues = randomIntBetween(0, 256);
+            List<Integer> theseValues = new ArrayList<>();
+            for (int j = 0; j < numberOfValues; j++) {
+                int value = randomInt();
+                values.add(value);
+                theseValues.add(value);
+            }
+            iterators[i] = theseValues.iterator();
+        }
+        assertContainsInOrder(Iterators.concat(iterators), values.toArray(new Integer[values.size()]));
+    }
+
+    public void testTwoEntries() {
+        int first = randomInt();
+        int second = randomInt();
+        Iterator<Integer> concat = Iterators.concat(singletonIterator(first), empty(), empty(), singletonIterator(second));
+        assertContainsInOrder(concat, first, second);
+    }
+
+    public void testNull() {
+        try {
+            Iterators.concat((Iterator<?>)null);
+            fail("expected " + NullPointerException.class.getSimpleName());
+        } catch (NullPointerException e) {
+
+        }
+    }
+
+    public void testNullIterator() {
+        try {
+            Iterators.concat(singletonIterator(1), empty(), null, empty(), singletonIterator(2));
+            fail("expected " + NullPointerException.class.getSimpleName());
+        } catch (NullPointerException e) {
+
+        }
+    }
+
+    private <T> Iterator<T> singletonIterator(T value) {
+        return Collections.singleton(value).iterator();
+    }
+
+    private <T> void assertSingleton(T value, Iterator<T>... iterators) {
+        Iterator<T> concat = Iterators.concat(iterators);
+        assertContainsInOrder(concat, value);
+    }
+
+    private <T> Iterator<T> empty() {
+        return new Iterator<T>() {
+            @Override
+            public boolean hasNext() {
+                return false;
+            }
+
+            @Override
+            public T next() {
+                throw new NoSuchElementException();
+            }
+        };
+    }
+
+    private <T> void assertContainsInOrder(Iterator<T> iterator, T... values) {
+        for (T value : values) {
+            assertTrue(iterator.hasNext());
+            assertEquals(value, iterator.next());
+        }
+        assertNoSuchElementException(iterator);
+    }
+
+    private <T> void assertEmptyIterator(Iterator<T> iterator) {
+        assertFalse(iterator.hasNext());
+        assertNoSuchElementException(iterator);
+    }
+
+    private <T> void assertNoSuchElementException(Iterator<T> iterator) {
+        try {
+            iterator.next();
+            fail("expected " + NoSuchElementException.class.getSimpleName());
+        } catch (NoSuchElementException e) {
+
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java b/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java
index 255def7..9b327fb 100644
--- a/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java
+++ b/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java
@@ -60,22 +60,6 @@ public abstract class ModuleTestCase extends ESTestCase {
         fail("Did not find any binding to " + to.getName() + ". Found these bindings:\n" + s);
     }
 
-//    /** Configures the module and asserts "instance" is bound to "to". */
-//    public void assertInstanceBinding(Module module, Class to, Object instance) {
-//        List<Element> elements = Elements.getElements(module);
-//        for (Element element : elements) {
-//            if (element instanceof ProviderInstanceBinding) {
-//                assertEquals(instance, ((ProviderInstanceBinding) element).getProviderInstance().get());
-//                return;
-//            }
-//        }
-//        StringBuilder s = new StringBuilder();
-//        for (Element element : elements) {
-//            s.append(element + "\n");
-//        }
-//        fail("Did not find any binding to " + to.getName() + ". Found these bindings:\n" + s);
-//    }
-
     /**
      * Attempts to configure the module, and asserts an {@link IllegalArgumentException} is
      * caught, containing the given messages
diff --git a/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java b/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java
index afc17ce..2b37359 100644
--- a/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java
+++ b/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.common.io.stream;
 
 import org.apache.lucene.util.Constants;
+import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.test.ESTestCase;
@@ -478,4 +479,23 @@ public class BytesStreamsTests extends ESTestCase {
         getRandom().nextBytes(data);
         return data;
     }
+
+    public void testReadWriteGeoPoint() throws IOException {
+        {
+            BytesStreamOutput out = new BytesStreamOutput();
+            GeoPoint geoPoint = new GeoPoint(randomDouble(), randomDouble());
+            out.writeGenericValue(geoPoint);
+            StreamInput wrap = StreamInput.wrap(out.bytes());
+            GeoPoint point = (GeoPoint) wrap.readGenericValue();
+            assertEquals(point, geoPoint);
+        }
+        {
+            BytesStreamOutput out = new BytesStreamOutput();
+            GeoPoint geoPoint = new GeoPoint(randomDouble(), randomDouble());
+            out.writeGeoPoint(geoPoint);
+            StreamInput wrap = StreamInput.wrap(out.bytes());
+            GeoPoint point = wrap.readGeoPoint();
+            assertEquals(point, geoPoint);
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/common/lucene/LuceneTests.java b/core/src/test/java/org/elasticsearch/common/lucene/LuceneTests.java
index 2fb90c7..13ac6fd 100644
--- a/core/src/test/java/org/elasticsearch/common/lucene/LuceneTests.java
+++ b/core/src/test/java/org/elasticsearch/common/lucene/LuceneTests.java
@@ -20,10 +20,14 @@ package org.elasticsearch.common.lucene;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.*;
 import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.MockDirectoryWrapper;
 import org.apache.lucene.util.Version;
 import org.elasticsearch.test.ESTestCase;
@@ -322,4 +326,37 @@ public class LuceneTests extends ESTestCase {
         writer.close();
         dir.close();
     }
+
+    public void testCount() throws Exception {
+        Directory dir = newDirectory();
+        RandomIndexWriter w = new RandomIndexWriter(getRandom(), dir);
+
+        try (DirectoryReader reader = w.getReader()) {
+            // match_all does not match anything on an empty index
+            IndexSearcher searcher = newSearcher(reader);
+            assertFalse(Lucene.exists(searcher, new MatchAllDocsQuery()));
+        }
+
+        Document doc = new Document();
+        w.addDocument(doc);
+
+        doc.add(new StringField("foo", "bar", Store.NO));
+        w.addDocument(doc);
+
+        try (DirectoryReader reader = w.getReader()) {
+            IndexSearcher searcher = newSearcher(reader);
+            assertTrue(Lucene.exists(searcher, new MatchAllDocsQuery()));
+            assertFalse(Lucene.exists(searcher, new TermQuery(new Term("baz", "bar"))));
+            assertTrue(Lucene.exists(searcher, new TermQuery(new Term("foo", "bar"))));
+        }
+
+        w.deleteDocuments(new Term("foo", "bar"));
+        try (DirectoryReader reader = w.getReader()) {
+            IndexSearcher searcher = newSearcher(reader);
+            assertFalse(Lucene.exists(searcher, new TermQuery(new Term("foo", "bar"))));
+        }
+
+        w.close();
+        dir.close();
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQueryTests.java b/core/src/test/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQueryTests.java
index cdf7db9..43e151e 100644
--- a/core/src/test/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQueryTests.java
+++ b/core/src/test/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQueryTests.java
@@ -44,23 +44,23 @@ public class MultiPhrasePrefixQueryTests extends ESTestCase {
 
         MultiPhrasePrefixQuery query = new MultiPhrasePrefixQuery();
         query.add(new Term("field", "aa"));
-        assertThat(Lucene.count(searcher, query), equalTo(1l));
+        assertThat(searcher.count(query), equalTo(1));
 
         query = new MultiPhrasePrefixQuery();
         query.add(new Term("field", "aaa"));
         query.add(new Term("field", "bb"));
-        assertThat(Lucene.count(searcher, query), equalTo(1l));
+        assertThat(searcher.count(query), equalTo(1));
 
         query = new MultiPhrasePrefixQuery();
         query.setSlop(1);
         query.add(new Term("field", "aaa"));
         query.add(new Term("field", "cc"));
-        assertThat(Lucene.count(searcher, query), equalTo(1l));
+        assertThat(searcher.count(query), equalTo(1));
 
         query = new MultiPhrasePrefixQuery();
         query.setSlop(1);
         query.add(new Term("field", "xxx"));
-        assertThat(Lucene.count(searcher, query), equalTo(0l));
+        assertThat(searcher.count(query), equalTo(0));
     }
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/common/lucene/search/morelikethis/MoreLikeThisQueryTests.java b/core/src/test/java/org/elasticsearch/common/lucene/search/morelikethis/MoreLikeThisQueryTests.java
index 5db7e7e..119c595 100644
--- a/core/src/test/java/org/elasticsearch/common/lucene/search/morelikethis/MoreLikeThisQueryTests.java
+++ b/core/src/test/java/org/elasticsearch/common/lucene/search/morelikethis/MoreLikeThisQueryTests.java
@@ -65,7 +65,7 @@ public class MoreLikeThisQueryTests extends ESTestCase {
         mltQuery.setLikeText("lucene");
         mltQuery.setMinTermFrequency(1);
         mltQuery.setMinDocFreq(1);
-        long count = Lucene.count(searcher, mltQuery);
+        long count = searcher.count(mltQuery);
         assertThat(count, equalTo(2l));
 
         reader.close();
diff --git a/core/src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java b/core/src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java
index 98a4364..290af55 100644
--- a/core/src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java
+++ b/core/src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java
@@ -18,25 +18,16 @@
  */
 package org.elasticsearch.common.lucene.uid;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.core.KeywordAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
+import org.apache.lucene.document.*;
 import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexOptions;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.LeafReader;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SlowCompositeReaderWrapper;
-import org.apache.lucene.index.Term;
+import org.apache.lucene.index.*;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.Numbers;
@@ -50,17 +41,13 @@ import org.junit.Test;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.*;
 
 public class VersionsTests extends ESTestCase {
-
+    
     public static DirectoryReader reopen(DirectoryReader reader) throws IOException {
         return reopen(reader, true);
     }
@@ -112,7 +99,7 @@ public class VersionsTests extends ESTestCase {
         doc.add(uid);
         doc.add(version);
         writer.updateDocument(new Term(UidFieldMapper.NAME, "1"), doc);
-
+        
         directoryReader = reopen(directoryReader);
         assertThat(Versions.loadVersion(directoryReader, new Term(UidFieldMapper.NAME, "1")), equalTo(3l));
         assertThat(Versions.loadDocIdAndVersion(directoryReader, new Term(UidFieldMapper.NAME, "1")).version, equalTo(3l));
@@ -274,13 +261,8 @@ public class VersionsTests extends ESTestCase {
         iw.addDocument(document);
         iw.commit();
 
-        Map<String, Long> expectedVersions = new HashMap<>();
-        expectedVersions.put("1", 0L);
-        expectedVersions.put("2", 0L);
-        expectedVersions.put("3", 0L);
-        expectedVersions.put("4", 4L);
-        expectedVersions.put("5", 5L);
-        expectedVersions.put("6", 6L);
+        final Map<String, Long> expectedVersions = ImmutableMap.<String, Long>builder()
+                .put("1", 0L).put("2", 0L).put("3", 0L).put("4", 4L).put("5", 5L).put("6", 6L).build();
 
         // Force merge and check versions
         iw.forceMerge(1, true);
diff --git a/core/src/test/java/org/elasticsearch/common/network/InetAddressesTests.java b/core/src/test/java/org/elasticsearch/common/network/InetAddressesTests.java
new file mode 100644
index 0000000..2aa284d
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/network/InetAddressesTests.java
@@ -0,0 +1,217 @@
+/*
+ * Copyright (C) 2008 The Guava Authors
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.elasticsearch.common.network;
+
+import org.elasticsearch.test.ESTestCase;
+
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+
+public class InetAddressesTests extends ESTestCase {
+    public void testForStringBogusInput() {
+        String[] bogusInputs = {
+                "",
+                "016.016.016.016",
+                "016.016.016",
+                "016.016",
+                "016",
+                "000.000.000.000",
+                "000",
+                "0x0a.0x0a.0x0a.0x0a",
+                "0x0a.0x0a.0x0a",
+                "0x0a.0x0a",
+                "0x0a",
+                "42.42.42.42.42",
+                "42.42.42",
+                "42.42",
+                "42",
+                "42..42.42",
+                "42..42.42.42",
+                "42.42.42.42.",
+                "42.42.42.42...",
+                ".42.42.42.42",
+                "...42.42.42.42",
+                "42.42.42.-0",
+                "42.42.42.+0",
+                ".",
+                "...",
+                "bogus",
+                "bogus.com",
+                "192.168.0.1.com",
+                "12345.67899.-54321.-98765",
+                "257.0.0.0",
+                "42.42.42.-42",
+                "3ffe::1.net",
+                "3ffe::1::1",
+                "1::2::3::4:5",
+                "::7:6:5:4:3:2:",  // should end with ":0"
+                ":6:5:4:3:2:1::",  // should begin with "0:"
+                "2001::db:::1",
+                "FEDC:9878",
+                "+1.+2.+3.4",
+                "1.2.3.4e0",
+                "::7:6:5:4:3:2:1:0",  // too many parts
+                "7:6:5:4:3:2:1:0::",  // too many parts
+                "9:8:7:6:5:4:3::2:1",  // too many parts
+                "0:1:2:3::4:5:6:7",  // :: must remove at least one 0.
+                "3ffe:0:0:0:0:0:0:0:1",  // too many parts (9 instead of 8)
+                "3ffe::10000",  // hextet exceeds 16 bits
+                "3ffe::goog",
+                "3ffe::-0",
+                "3ffe::+0",
+                "3ffe::-1",
+                ":",
+                ":::",
+                "::1.2.3",
+                "::1.2.3.4.5",
+                "::1.2.3.4:",
+                "1.2.3.4::",
+                "2001:db8::1:",
+                ":2001:db8::1",
+                ":1:2:3:4:5:6:7",
+                "1:2:3:4:5:6:7:",
+                ":1:2:3:4:5:6:"
+        };
+
+        for (int i = 0; i < bogusInputs.length; i++) {
+            try {
+                InetAddresses.forString(bogusInputs[i]);
+                fail("IllegalArgumentException expected for '" + bogusInputs[i] + "'");
+            } catch (IllegalArgumentException expected) {
+                // expected behavior
+            }
+            assertFalse(InetAddresses.isInetAddress(bogusInputs[i]));
+        }
+    }
+
+    public void test3ff31() {
+        try {
+            InetAddresses.forString("3ffe:::1");
+            fail("IllegalArgumentException expected");
+        } catch (IllegalArgumentException expected) {
+            // expected behavior
+        }
+        assertFalse(InetAddresses.isInetAddress("016.016.016.016"));
+    }
+
+    public void testForStringIPv4Input() throws UnknownHostException {
+        String ipStr = "192.168.0.1";
+        InetAddress ipv4Addr = null;
+        // Shouldn't hit DNS, because it's an IP string literal.
+        ipv4Addr = InetAddress.getByName(ipStr);
+        assertEquals(ipv4Addr, InetAddresses.forString(ipStr));
+        assertTrue(InetAddresses.isInetAddress(ipStr));
+    }
+
+    public void testForStringIPv6Input() throws UnknownHostException {
+        String ipStr = "3ffe::1";
+        InetAddress ipv6Addr = null;
+        // Shouldn't hit DNS, because it's an IP string literal.
+        ipv6Addr = InetAddress.getByName(ipStr);
+        assertEquals(ipv6Addr, InetAddresses.forString(ipStr));
+        assertTrue(InetAddresses.isInetAddress(ipStr));
+    }
+
+    public void testForStringIPv6EightColons() throws UnknownHostException {
+        String[] eightColons = {
+                "::7:6:5:4:3:2:1",
+                "::7:6:5:4:3:2:0",
+                "7:6:5:4:3:2:1::",
+                "0:6:5:4:3:2:1::",
+        };
+
+        for (int i = 0; i < eightColons.length; i++) {
+            InetAddress ipv6Addr = null;
+            // Shouldn't hit DNS, because it's an IP string literal.
+            ipv6Addr = InetAddress.getByName(eightColons[i]);
+            assertEquals(ipv6Addr, InetAddresses.forString(eightColons[i]));
+            assertTrue(InetAddresses.isInetAddress(eightColons[i]));
+        }
+    }
+
+    public void testConvertDottedQuadToHex() throws UnknownHostException {
+        String[] ipStrings = {"7::0.128.0.127", "7::0.128.0.128",
+                "7::128.128.0.127", "7::0.128.128.127"};
+
+        for (String ipString : ipStrings) {
+            // Shouldn't hit DNS, because it's an IP string literal.
+            InetAddress ipv6Addr = InetAddress.getByName(ipString);
+            assertEquals(ipv6Addr, InetAddresses.forString(ipString));
+            assertTrue(InetAddresses.isInetAddress(ipString));
+        }
+    }
+
+    public void testToAddrStringIPv4() {
+        // Don't need to test IPv4 much; it just calls getHostAddress().
+        assertEquals("1.2.3.4",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("1.2.3.4")));
+    }
+
+    public void testToAddrStringIPv6() {
+        assertEquals("1:2:3:4:5:6:7:8",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("1:2:3:4:5:6:7:8")));
+        assertEquals("2001:0:0:4::8",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("2001:0:0:4:0:0:0:8")));
+        assertEquals("2001::4:5:6:7:8",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("2001:0:0:4:5:6:7:8")));
+        assertEquals("2001:0:3:4:5:6:7:8",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("2001:0:3:4:5:6:7:8")));
+        assertEquals("0:0:3::ffff",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("0:0:3:0:0:0:0:ffff")));
+        assertEquals("::4:0:0:0:ffff",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("0:0:0:4:0:0:0:ffff")));
+        assertEquals("::5:0:0:ffff",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("0:0:0:0:5:0:0:ffff")));
+        assertEquals("1::4:0:0:7:8",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("1:0:0:4:0:0:7:8")));
+        assertEquals("::",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("0:0:0:0:0:0:0:0")));
+        assertEquals("::1",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("0:0:0:0:0:0:0:1")));
+        assertEquals("2001:658:22a:cafe::",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("2001:0658:022a:cafe::")));
+        assertEquals("::102:304",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("::1.2.3.4")));
+    }
+
+    public void testToUriStringIPv4() {
+        String ipStr = "1.2.3.4";
+        InetAddress ip = InetAddresses.forString(ipStr);
+        assertEquals("1.2.3.4", InetAddresses.toUriString(ip));
+    }
+
+    public void testToUriStringIPv6() {
+        // Unfortunately the InetAddress.toString() method for IPv6 addresses
+        // does not collapse contiguous shorts of zeroes with the :: abbreviation.
+        String ipStr = "3ffe::1";
+        InetAddress ip = InetAddresses.forString(ipStr);
+        assertEquals("[3ffe::1]", InetAddresses.toUriString(ip));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/network/NetworkAddressTests.java b/core/src/test/java/org/elasticsearch/common/network/NetworkAddressTests.java
index 5847bb7..b53a56a 100644
--- a/core/src/test/java/org/elasticsearch/common/network/NetworkAddressTests.java
+++ b/core/src/test/java/org/elasticsearch/common/network/NetworkAddressTests.java
@@ -22,9 +22,11 @@ package org.elasticsearch.common.network;
 import org.elasticsearch.test.ESTestCase;
 
 import java.io.IOException;
+import java.net.Inet4Address;
 import java.net.Inet6Address;
 import java.net.InetAddress;
 import java.net.InetSocketAddress;
+import java.util.Random;
 
 /**
  * Tests for network address formatting. Please avoid using any methods that cause DNS lookups!
@@ -84,6 +86,32 @@ public class NetworkAddressTests extends ESTestCase {
         assertEquals("[::1]:1234", NetworkAddress.formatAddress(new InetSocketAddress(forgeScoped(null, "::1", 5), 1234)));
         assertEquals("[::1]:1234", NetworkAddress.formatAddress(new InetSocketAddress(forgeScoped("localhost", "::1", 5), 1234)));
     }
+
+    /** Test that ipv4 address formatting round trips */
+    public void testRoundTripV4() throws Exception {
+        byte bytes[] = new byte[4];
+        Random random = random();
+        for (int i = 0; i < 10000; i++) {
+            random.nextBytes(bytes);
+            InetAddress expected = Inet4Address.getByAddress(bytes);
+            String formatted = NetworkAddress.formatAddress(expected);
+            InetAddress actual = InetAddress.getByName(formatted);
+            assertEquals(expected, actual);
+        }
+    }
+
+    /** Test that ipv6 address formatting round trips */
+    public void testRoundTripV6() throws Exception {
+        byte bytes[] = new byte[16];
+        Random random = random();
+        for (int i = 0; i < 10000; i++) {
+            random.nextBytes(bytes);
+            InetAddress expected = Inet6Address.getByAddress(bytes);
+            String formatted = NetworkAddress.formatAddress(expected);
+            InetAddress actual = InetAddress.getByName(formatted);
+            assertEquals(expected, actual);
+        }
+    }
     
     /** creates address without any lookups. hostname can be null, for missing */
     private InetAddress forge(String hostname, String address) throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/common/path/PathTrieTests.java b/core/src/test/java/org/elasticsearch/common/path/PathTrieTests.java
index 1309b58..aec4fb2 100644
--- a/core/src/test/java/org/elasticsearch/common/path/PathTrieTests.java
+++ b/core/src/test/java/org/elasticsearch/common/path/PathTrieTests.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.common.path;
 
 import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
 
 import java.util.HashMap;
 import java.util.Map;
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java
index 23cf83b..4ce1a7c 100644
--- a/core/src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.common.xcontent.builder;
 
 import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.io.FastCharArrayWriter;
 import org.elasticsearch.common.io.PathUtils;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
@@ -351,4 +352,16 @@ public class XContentBuilderTests extends ESTestCase {
                 "  foobar: \"boom\"\n", string);
     }
 
+    public void testRenderGeoPoint() throws IOException {
+        XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON).prettyPrint();
+        builder.startObject().field("foo").value(new GeoPoint(1,2)).endObject();
+        String string = builder.string();
+        assertEquals("{\n" +
+                "  \"foo\" : {\n" +
+                "    \"lat\" : 1.0,\n" +
+                "    \"lon\" : 2.0\n" +
+                "  }\n" +
+                "}", string.trim());
+    }
+
 }
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/support/XContentMapValuesTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/support/XContentMapValuesTests.java
index abce42b..ba34812 100644
--- a/core/src/test/java/org/elasticsearch/common/xcontent/support/XContentMapValuesTests.java
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/support/XContentMapValuesTests.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.common.xcontent.support;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -37,12 +39,7 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.singletonMap;
-import static org.hamcrest.Matchers.hasEntry;
-import static org.hamcrest.Matchers.hasKey;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.*;
 import static org.hamcrest.core.IsEqual.equalTo;
 
 /**
@@ -564,7 +561,7 @@ public class XContentMapValuesTests extends ESTestCase {
                 assertEquals(XContentParser.Token.START_ARRAY, parser.nextToken());
             }
             assertEquals(
-                    Arrays.asList(singletonMap("foo", "bar"), emptyMap()),
+                    Arrays.asList(ImmutableMap.of("foo", "bar"), Collections.<String, Object>emptyMap()),
                     parser.list());
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
index 2ac6900..ca95e50 100644
--- a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
+++ b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
@@ -31,7 +31,7 @@ import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.DjbHashFunction;
+import org.elasticsearch.cluster.routing.Murmur3HashFunction;
 import org.elasticsearch.cluster.routing.allocation.command.MoveAllocationCommand;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Priority;
@@ -441,7 +441,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
                                 logger.info("[{}] Acquired semaphore and it has {} permits left", name, semaphore.availablePermits());
                                 try {
                                     id = Integer.toString(idGenerator.incrementAndGet());
-                                    int shard = ((InternalTestCluster) cluster()).getInstance(DjbHashFunction.class).hash(id) % numPrimaries;
+                                    int shard = Murmur3HashFunction.hash(id) % numPrimaries;
                                     logger.trace("[{}] indexing id [{}] through node [{}] targeting shard [{}]", name, id, node, shard);
                                     IndexResponse response = client.prepareIndex("test", "type", id).setSource("{}").setTimeout("1s").get();
                                     assertThat(response.getVersion(), equalTo(1l));
diff --git a/core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java b/core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java
index 9d1ce5c..a39b154 100644
--- a/core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java
+++ b/core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.discovery;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterState;
@@ -44,7 +45,6 @@ import org.junit.Test;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.TimeUnit;
 
-import static java.util.Collections.emptyMap;
 import static org.hamcrest.Matchers.equalTo;
 
 public class ZenFaultDetectionTests extends ESTestCase {
@@ -65,9 +65,9 @@ public class ZenFaultDetectionTests extends ESTestCase {
         super.setUp();
         threadPool = new ThreadPool(getClass().getName());
         serviceA = build(Settings.builder().put("name", "TS_A").build(), version0);
-        nodeA = new DiscoveryNode("TS_A", "TS_A", serviceA.boundAddress().publishAddress(), emptyMap(), version0);
+        nodeA = new DiscoveryNode("TS_A", "TS_A", serviceA.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), version0);
         serviceB = build(Settings.builder().put("name", "TS_B").build(), version1);
-        nodeB = new DiscoveryNode("TS_B", "TS_B", serviceB.boundAddress().publishAddress(), emptyMap(), version1);
+        nodeB = new DiscoveryNode("TS_B", "TS_B", serviceB.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), version1);
 
         // wait till all nodes are properly connected and the event has been sent, so tests in this class
         // will not get this callback called on the connections done in this setup
diff --git a/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTests.java b/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTests.java
index 98630ed..1d51c30 100644
--- a/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTests.java
+++ b/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTests.java
@@ -18,8 +18,6 @@
  */
 package org.elasticsearch.gateway;
 
-import com.google.common.collect.Iterators;
-
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.store.ChecksumIndexInput;
 import org.apache.lucene.store.Directory;
@@ -33,6 +31,7 @@ import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
+import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.common.io.FileSystemUtils;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.xcontent.ToXContent;
@@ -59,6 +58,7 @@ import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
+import java.util.stream.StreamSupport;
 
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.instanceOf;
@@ -535,7 +535,7 @@ public class MetaDataStateFormatTests extends ESTestCase {
 
     public Path[] content(String glob, Path dir) throws IOException {
         try (DirectoryStream<Path> stream = Files.newDirectoryStream(dir, glob)) {
-            return Iterators.toArray(stream.iterator(), Path.class);
+            return StreamSupport.stream(stream.spliterator(), false).toArray(length -> new Path[length]);
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java b/core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java
index e692b62..2b4ccf7 100644
--- a/core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java
+++ b/core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.gateway;
 
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterInfo;
 import org.elasticsearch.cluster.ClusterState;
@@ -28,15 +27,7 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.IndexRoutingTable;
-import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
-import org.elasticsearch.cluster.routing.RoutingNode;
-import org.elasticsearch.cluster.routing.RoutingNodes;
-import org.elasticsearch.cluster.routing.RoutingTable;
-import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.cluster.routing.ShardRoutingState;
-import org.elasticsearch.cluster.routing.TestShardRouting;
-import org.elasticsearch.cluster.routing.UnassignedInfo;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
 import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;
 import org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders;
@@ -58,7 +49,6 @@ import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicBoolean;
 
-import static java.util.Collections.unmodifiableMap;
 import static org.hamcrest.Matchers.equalTo;
 
 /**
@@ -368,8 +358,7 @@ public class ReplicaShardAllocatorTests extends ESAllocationTestCase {
             if (syncId != null) {
                 commitData.put(Engine.SYNC_COMMIT_ID, syncId);
             }
-            data.put(node, new TransportNodesListShardStoreMetaData.StoreFilesMetaData(allocated, shardId,
-                    new Store.MetadataSnapshot(unmodifiableMap(filesAsMap), unmodifiableMap(commitData), randomInt())));
+            data.put(node, new TransportNodesListShardStoreMetaData.StoreFilesMetaData(allocated, shardId, new Store.MetadataSnapshot(filesAsMap, commitData, randomInt())));
             return this;
         }
 
diff --git a/core/src/test/java/org/elasticsearch/get/GetActionIT.java b/core/src/test/java/org/elasticsearch/get/GetActionIT.java
index 55b104d..b26e3ec 100644
--- a/core/src/test/java/org/elasticsearch/get/GetActionIT.java
+++ b/core/src/test/java/org/elasticsearch/get/GetActionIT.java
@@ -25,11 +25,7 @@ import org.elasticsearch.action.ShardOperationFailedException;
 import org.elasticsearch.action.admin.indices.alias.Alias;
 import org.elasticsearch.action.admin.indices.flush.FlushResponse;
 import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.get.GetRequestBuilder;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.get.MultiGetRequest;
-import org.elasticsearch.action.get.MultiGetRequestBuilder;
-import org.elasticsearch.action.get.MultiGetResponse;
+import org.elasticsearch.action.get.*;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
@@ -53,14 +49,7 @@ import java.util.Set;
 import static java.util.Collections.singleton;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.hasKey;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
-import static org.hamcrest.Matchers.startsWith;
+import static org.hamcrest.Matchers.*;
 
 public class GetActionIT extends ESIntegTestCase {
 
@@ -600,7 +589,7 @@ public class GetActionIT extends ESIntegTestCase {
         assertThat(response.getResponses()[1].getResponse().getSourceAsMap().get("field").toString(), equalTo("value1"));
         assertThat(response.getResponses()[2].getFailure(), notNullValue());
         assertThat(response.getResponses()[2].getFailure().getId(), equalTo("1"));
-        assertThat(response.getResponses()[2].getFailure().getMessage(), startsWith("[type1][1]: version conflict, current [1], provided [2]"));
+        assertThat(response.getResponses()[2].getFailure().getMessage(), startsWith("[type1][1]: version conflict"));
         assertThat(response.getResponses()[2].getFailure().getFailure(), instanceOf(VersionConflictEngineException.class));
 
         //Version from Lucene index
@@ -623,7 +612,7 @@ public class GetActionIT extends ESIntegTestCase {
         assertThat(response.getResponses()[1].getResponse().getSourceAsMap().get("field").toString(), equalTo("value1"));
         assertThat(response.getResponses()[2].getFailure(), notNullValue());
         assertThat(response.getResponses()[2].getFailure().getId(), equalTo("1"));
-        assertThat(response.getResponses()[2].getFailure().getMessage(), startsWith("[type1][1]: version conflict, current [1], provided [2]"));
+        assertThat(response.getResponses()[2].getFailure().getMessage(), startsWith("[type1][1]: version conflict"));
         assertThat(response.getResponses()[2].getFailure().getFailure(), instanceOf(VersionConflictEngineException.class));
 
 
@@ -648,7 +637,7 @@ public class GetActionIT extends ESIntegTestCase {
         assertThat(response.getResponses()[1].getFailure(), notNullValue());
         assertThat(response.getResponses()[1].getFailure().getId(), equalTo("2"));
         assertThat(response.getResponses()[1].getIndex(), equalTo("test"));
-        assertThat(response.getResponses()[1].getFailure().getMessage(), startsWith("[type1][2]: version conflict, current [2], provided [1]"));
+        assertThat(response.getResponses()[1].getFailure().getMessage(), startsWith("[type1][2]: version conflict"));
         assertThat(response.getResponses()[2].getId(), equalTo("2"));
         assertThat(response.getResponses()[2].getIndex(), equalTo("test"));
         assertThat(response.getResponses()[2].getFailure(), nullValue());
@@ -674,7 +663,7 @@ public class GetActionIT extends ESIntegTestCase {
         assertThat(response.getResponses()[1].getFailure(), notNullValue());
         assertThat(response.getResponses()[1].getFailure().getId(), equalTo("2"));
         assertThat(response.getResponses()[1].getIndex(), equalTo("test"));
-        assertThat(response.getResponses()[1].getFailure().getMessage(), startsWith("[type1][2]: version conflict, current [2], provided [1]"));
+        assertThat(response.getResponses()[1].getFailure().getMessage(), startsWith("[type1][2]: version conflict"));
         assertThat(response.getResponses()[2].getId(), equalTo("2"));
         assertThat(response.getResponses()[2].getIndex(), equalTo("test"));
         assertThat(response.getResponses()[2].getFailure(), nullValue());
diff --git a/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java b/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
index a54be17..dd73e41 100644
--- a/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
+++ b/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
@@ -24,6 +24,8 @@ import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;
 import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;
 import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;
+import org.elasticsearch.action.admin.indices.stats.IndicesStatsResponse;
+import org.elasticsearch.action.admin.indices.stats.ShardStats;
 import org.elasticsearch.action.get.GetResponse;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.index.IndexResponse;
@@ -36,6 +38,7 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.discovery.Discovery;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.ShadowIndexShard;
+import org.elasticsearch.index.translog.TranslogStats;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.indices.recovery.RecoveryTarget;
 import org.elasticsearch.plugins.Plugin;
@@ -175,6 +178,7 @@ public class IndexWithShadowReplicasIT extends ESIntegTestCase {
         Settings idxSettings = Settings.builder()
                 .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
                 .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 2)
+                .put(IndexShard.INDEX_TRANSLOG_DISABLE_FLUSH, true)
                 .put(IndexMetaData.SETTING_DATA_PATH, dataPath.toAbsolutePath().toString())
                 .put(IndexMetaData.SETTING_SHADOW_REPLICAS, true)
                 .put(IndexMetaData.SETTING_SHARED_FILESYSTEM, true)
@@ -188,6 +192,21 @@ public class IndexWithShadowReplicasIT extends ESIntegTestCase {
         client().prepareIndex(IDX, "doc", "1").setSource("foo", "bar").get();
         client().prepareIndex(IDX, "doc", "2").setSource("foo", "bar").get();
 
+        IndicesStatsResponse indicesStatsResponse = client().admin().indices().prepareStats(IDX).clear().setTranslog(true).get();
+        assertEquals(2, indicesStatsResponse.getIndex(IDX).getPrimaries().getTranslog().estimatedNumberOfOperations());
+        assertEquals(2, indicesStatsResponse.getIndex(IDX).getTotal().getTranslog().estimatedNumberOfOperations());
+        for (IndicesService service : internalCluster().getInstances(IndicesService.class)) {
+            IndexService indexService = service.indexService(IDX);
+            if (indexService != null) {
+                IndexShard shard = indexService.getShard(0);
+                TranslogStats translogStats = shard.translogStats();
+                assertTrue(translogStats != null || shard instanceof ShadowIndexShard);
+                if (translogStats != null) {
+                    assertEquals(2, translogStats.estimatedNumberOfOperations());
+                }
+            }
+        }
+
         // Check that we can get doc 1 and 2, because we are doing realtime
         // gets and getting from the primary
         GetResponse gResp1 = client().prepareGet(IDX, "doc", "1").setRealtime(true).setFields("foo").get();
diff --git a/core/src/test/java/org/elasticsearch/index/VersionTypeTests.java b/core/src/test/java/org/elasticsearch/index/VersionTypeTests.java
index e4a97d2..3f7ea54 100644
--- a/core/src/test/java/org/elasticsearch/index/VersionTypeTests.java
+++ b/core/src/test/java/org/elasticsearch/index/VersionTypeTests.java
@@ -29,26 +29,31 @@ public class VersionTypeTests extends ESTestCase {
     @Test
     public void testInternalVersionConflict() throws Exception {
 
-        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(10, Versions.MATCH_ANY));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(10, Versions.MATCH_ANY, randomBoolean()));
         assertFalse(VersionType.INTERNAL.isVersionConflictForReads(10, Versions.MATCH_ANY));
         // if we don't have a version in the index we accept everything
-        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_SET, 10));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_SET, 10, randomBoolean()));
         assertFalse(VersionType.INTERNAL.isVersionConflictForReads(Versions.NOT_SET, 10));
-        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_SET, Versions.MATCH_ANY));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_SET, Versions.MATCH_ANY, randomBoolean()));
         assertFalse(VersionType.INTERNAL.isVersionConflictForReads(Versions.NOT_SET, Versions.MATCH_ANY));
 
         // if we didn't find a version (but the index does support it), we don't like it unless MATCH_ANY
-        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
+        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
         assertTrue(VersionType.INTERNAL.isVersionConflictForReads(Versions.NOT_FOUND, 10));
-        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.MATCH_ANY));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.MATCH_ANY, randomBoolean()));
         assertFalse(VersionType.INTERNAL.isVersionConflictForReads(Versions.NOT_FOUND, Versions.MATCH_ANY));
 
+        // deletes
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.MATCH_DELETED, true));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(10, Versions.MATCH_DELETED, true));
+
+
         // and the stupid usual case
-        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(10, 10));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(10, 10, randomBoolean()));
         assertFalse(VersionType.INTERNAL.isVersionConflictForReads(10, 10));
-        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(9, 10));
+        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(9, 10, randomBoolean()));
         assertTrue(VersionType.INTERNAL.isVersionConflictForReads(9, 10));
-        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(10, 9));
+        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(10, 9, randomBoolean()));
         assertTrue(VersionType.INTERNAL.isVersionConflictForReads(10, 9));
 
 // Old indexing code, dictating behavior
@@ -99,23 +104,23 @@ public class VersionTypeTests extends ESTestCase {
     @Test
     public void testExternalVersionConflict() throws Exception {
 
-        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
-        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_SET, 10));
+        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_SET, 10, randomBoolean()));
         // MATCH_ANY must throw an exception in the case of external version, as the version must be set! it used as the new value
-        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, Versions.MATCH_ANY));
+        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, Versions.MATCH_ANY, randomBoolean()));
 
         // if we didn't find a version (but the index does support it), we always accept
-        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND));
-        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
+        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
 
         assertTrue(VersionType.EXTERNAL.isVersionConflictForReads(Versions.NOT_FOUND, Versions.NOT_FOUND));
         assertTrue(VersionType.EXTERNAL.isVersionConflictForReads(Versions.NOT_FOUND, 10));
         assertFalse(VersionType.EXTERNAL.isVersionConflictForReads(Versions.NOT_FOUND, Versions.MATCH_ANY));
 
         // and the standard behavior
-        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, 10));
-        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(9, 10));
-        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, 9));
+        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, 10, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(9, 10, randomBoolean()));
+        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, 9, randomBoolean()));
 
         assertFalse(VersionType.EXTERNAL.isVersionConflictForReads(10, 10));
         assertTrue(VersionType.EXTERNAL.isVersionConflictForReads(9, 10));
@@ -137,14 +142,14 @@ public class VersionTypeTests extends ESTestCase {
     @Test
     public void testExternalGTEVersionConflict() throws Exception {
 
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_SET, 10));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_SET, 10, randomBoolean()));
         // MATCH_ANY must throw an exception in the case of external version, as the version must be set! it used as the new value
-        assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, Versions.MATCH_ANY));
+        assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, Versions.MATCH_ANY, randomBoolean()));
 
         // if we didn't find a version (but the index does support it), we always accept
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND));
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
 
         assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForReads(Versions.NOT_FOUND, Versions.NOT_FOUND));
         assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForReads(Versions.NOT_FOUND, 10));
@@ -152,9 +157,9 @@ public class VersionTypeTests extends ESTestCase {
 
 
         // and the standard behavior
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, 10));
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(9, 10));
-        assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, 9));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, 10, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(9, 10, randomBoolean()));
+        assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, 9, randomBoolean()));
 
         assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForReads(10, 10));
         assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForReads(9, 10));
@@ -166,14 +171,20 @@ public class VersionTypeTests extends ESTestCase {
     @Test
     public void testForceVersionConflict() throws Exception {
 
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_SET, 10));
-        // MATCH_ANY must throw an exception in the case of external version, as the version must be set! it used as the new value
-        assertTrue(VersionType.FORCE.isVersionConflictForWrites(10, Versions.MATCH_ANY));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_SET, 10, randomBoolean()));
+
+        // MATCH_ANY must throw an exception in the case of force version, as the version must be set! it used as the new value
+        try {
+            VersionType.FORCE.isVersionConflictForWrites(10, Versions.MATCH_ANY, randomBoolean());
+            fail();
+        } catch (IllegalStateException e) {
+            //yes!!
+        }
 
         // if we didn't find a version (but the index does support it), we always accept
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND));
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND, randomBoolean()));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
 
         assertFalse(VersionType.FORCE.isVersionConflictForReads(Versions.NOT_FOUND, Versions.NOT_FOUND));
         assertFalse(VersionType.FORCE.isVersionConflictForReads(Versions.NOT_FOUND, 10));
@@ -181,9 +192,9 @@ public class VersionTypeTests extends ESTestCase {
 
 
         // and the standard behavior
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(10, 10));
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(9, 10));
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(10, 9));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(10, 10, randomBoolean()));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(9, 10, randomBoolean()));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(10, 9, randomBoolean()));
         assertFalse(VersionType.FORCE.isVersionConflictForReads(10, 10));
         assertFalse(VersionType.FORCE.isVersionConflictForReads(9, 10));
         assertFalse(VersionType.FORCE.isVersionConflictForReads(10, 9));
diff --git a/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTests.java b/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTests.java
index 6a96086..c781a58 100644
--- a/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTests.java
+++ b/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTests.java
@@ -96,7 +96,7 @@ public class BitSetFilterCacheTests extends ESTestCase {
         // now cached
         assertThat(matchCount(filter, reader), equalTo(3));
         // There are 3 segments
-        assertThat(cache.getLoadedFilters().size(), equalTo(3l));
+        assertThat(cache.getLoadedFilters().weight(), equalTo(3L));
 
         writer.forceMerge(1);
         reader.close();
@@ -108,12 +108,12 @@ public class BitSetFilterCacheTests extends ESTestCase {
         // now cached
         assertThat(matchCount(filter, reader), equalTo(3));
         // Only one segment now, so the size must be 1
-        assertThat(cache.getLoadedFilters().size(), equalTo(1l));
+        assertThat(cache.getLoadedFilters().weight(), equalTo(1L));
 
         reader.close();
         writer.close();
         // There is no reference from readers and writer to any segment in the test index, so the size in the fbs cache must be 0
-        assertThat(cache.getLoadedFilters().size(), equalTo(0l));
+        assertThat(cache.getLoadedFilters().weight(), equalTo(0L));
     }
 
     public void testListener() throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/index/codec/postingformat/Elasticsearch090RWPostingsFormat.java b/core/src/test/java/org/elasticsearch/index/codec/postingformat/Elasticsearch090RWPostingsFormat.java
deleted file mode 100644
index a4285e6..0000000
--- a/core/src/test/java/org/elasticsearch/index/codec/postingformat/Elasticsearch090RWPostingsFormat.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.codec.postingformat;
-
-import com.google.common.collect.Iterators;
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FilterLeafReader;
-import org.apache.lucene.index.SegmentWriteState;
-import org.elasticsearch.common.util.BloomFilter;
-import org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat;
-import org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat.BloomFilteredFieldsConsumer;
-import org.elasticsearch.index.codec.postingsformat.Elasticsearch090PostingsFormat;
-import org.elasticsearch.index.mapper.internal.UidFieldMapper;
-
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.stream.StreamSupport;
-
-/** read-write version with blooms for testing */
-public class Elasticsearch090RWPostingsFormat extends Elasticsearch090PostingsFormat {
-    @Override
-    public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-        final PostingsFormat delegate = getDefaultWrapped();
-        final BloomFilteredFieldsConsumer fieldsConsumer = new BloomFilterPostingsFormat(delegate, BloomFilter.Factory.DEFAULT) {
-            @Override
-            public BloomFilteredFieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-                return new BloomFilteredFieldsConsumer(delegate.fieldsConsumer(state), state,delegate);
-            } 
-        }.fieldsConsumer(state);
-        return new FieldsConsumer() {
-
-            @Override
-            public void write(Fields fields) throws IOException {
-
-                Fields maskedFields = new FilterLeafReader.FilterFields(fields) {
-                    @Override
-                    public Iterator<String> iterator() {
-                        return StreamSupport.stream(this.in.spliterator(), false).filter(UID_FIELD_FILTER.negate()).iterator();
-                    }
-                };
-                fieldsConsumer.getDelegate().write(maskedFields);
-                maskedFields = new FilterLeafReader.FilterFields(fields) {
-                    @Override
-                    public Iterator<String> iterator() {
-                        return Iterators.singletonIterator(UidFieldMapper.NAME);
-                    }
-                };
-                // only go through bloom for the UID field
-                fieldsConsumer.write(maskedFields);
-            }
-
-            @Override
-            public void close() throws IOException {
-                fieldsConsumer.close();
-            }
-        };
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/codec/postingformat/PostingsFormatTests.java b/core/src/test/java/org/elasticsearch/index/codec/postingformat/PostingsFormatTests.java
deleted file mode 100644
index f988445..0000000
--- a/core/src/test/java/org/elasticsearch/index/codec/postingformat/PostingsFormatTests.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.codec.postingformat;
-
-import com.carrotsearch.randomizedtesting.annotations.Listeners;
-import com.carrotsearch.randomizedtesting.annotations.TimeoutSuite;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BasePostingsFormatTestCase;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-import org.apache.lucene.util.TimeUnits;
-import org.elasticsearch.test.junit.listeners.ReproduceInfoPrinter;
-
-/** Runs elasticsearch postings format against lucene's standard postings format tests */
-@Listeners({
-        ReproduceInfoPrinter.class
-})
-@TimeoutSuite(millis = TimeUnits.HOUR)
-@LuceneTestCase.SuppressSysoutChecks(bugUrl = "we log a lot on purpose")
-public class PostingsFormatTests extends BasePostingsFormatTestCase {
-
-    @Override
-    protected Codec getCodec() {
-        return TestUtil.alwaysPostingsFormat(new Elasticsearch090RWPostingsFormat());
-    }
-    
-}
diff --git a/core/src/test/java/org/elasticsearch/index/engine/EngineSearcherTotalHitsMatcher.java b/core/src/test/java/org/elasticsearch/index/engine/EngineSearcherTotalHitsMatcher.java
index 583b0f5..362ee9c 100644
--- a/core/src/test/java/org/elasticsearch/index/engine/EngineSearcherTotalHitsMatcher.java
+++ b/core/src/test/java/org/elasticsearch/index/engine/EngineSearcherTotalHitsMatcher.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.index.engine;
 
 import org.apache.lucene.search.Query;
-import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.hamcrest.Description;
 import org.hamcrest.Matcher;
@@ -46,7 +45,7 @@ public final class EngineSearcherTotalHitsMatcher extends TypeSafeMatcher<Engine
     @Override
     public boolean matchesSafely(Engine.Searcher searcher) {
         try {
-            this.count = (int) Lucene.count(searcher.searcher(), query);
+            this.count = (int) searcher.searcher().count(query);
             return count == totalHits;
         } catch (IOException e) {
             return false;
diff --git a/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java b/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
index e603254..d395f22 100644
--- a/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
+++ b/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.engine;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.log4j.AppenderSkeleton;
 import org.apache.log4j.Level;
 import org.apache.log4j.LogManager;
@@ -28,16 +29,7 @@ import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy;
-import org.apache.lucene.index.LiveIndexWriterConfig;
-import org.apache.lucene.index.LogByteSizeMergePolicy;
-import org.apache.lucene.index.MergePolicy;
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.SnapshotDeletionPolicy;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TieredMergePolicy;
+import org.apache.lucene.index.*;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.TermQuery;
@@ -69,22 +61,14 @@ import org.elasticsearch.index.analysis.AnalysisService;
 import org.elasticsearch.index.codec.CodecService;
 import org.elasticsearch.index.engine.Engine.Searcher;
 import org.elasticsearch.index.indexing.ShardIndexingService;
-import org.elasticsearch.index.mapper.ContentPath;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.DocumentMapperForType;
-import org.elasticsearch.index.mapper.DocumentMapperParser;
+import org.elasticsearch.index.mapper.*;
 import org.elasticsearch.index.mapper.Mapper.BuilderContext;
-import org.elasticsearch.index.mapper.MapperBuilders;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.mapper.Mapping;
-import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext.Document;
-import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.internal.SourceFieldMapper;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 import org.elasticsearch.index.mapper.object.RootObjectMapper;
 import org.elasticsearch.index.shard.*;
-import org.elasticsearch.index.similarity.SimilarityLookupService;
+import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.index.store.DirectoryService;
 import org.elasticsearch.index.store.DirectoryUtils;
 import org.elasticsearch.index.store.Store;
@@ -105,36 +89,21 @@ import java.nio.charset.Charset;
 import java.nio.file.DirectoryStream;
 import java.nio.file.Files;
 import java.nio.file.Path;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
+import java.util.*;
 import java.util.concurrent.BrokenBarrierException;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.CyclicBarrier;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReference;
-import java.util.regex.Pattern;
 
-import static java.util.Collections.emptyMap;
 import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
 import static org.elasticsearch.index.engine.Engine.Operation.Origin.PRIMARY;
 import static org.elasticsearch.index.engine.Engine.Operation.Origin.REPLICA;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.hasKey;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.*;
 
 public class InternalEngineTests extends ESTestCase {
 
-    private static final Pattern PARSE_LEGACY_ID_PATTERN = Pattern.compile("^" + Translog.TRANSLOG_FILE_PREFIX + "(\\d+)((\\.recovering))?$");
-
     protected final ShardId shardId = new ShardId(new Index("index"), 1);
 
     protected ThreadPool threadPool;
@@ -301,10 +270,10 @@ public class InternalEngineTests extends ESTestCase {
 
             // create a doc and refresh
             ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-            engine.create(new Engine.Create(newUid("1"), doc));
+            engine.index(new Engine.Index(newUid("1"), doc));
 
             ParsedDocument doc2 = testParsedDocument("2", "2", "test", null, -1, -1, testDocumentWithTextField(), B_2, null);
-            engine.create(new Engine.Create(newUid("2"), doc2));
+            engine.index(new Engine.Index(newUid("2"), doc2));
             engine.refresh("test");
 
             segments = engine.segments(false);
@@ -338,7 +307,7 @@ public class InternalEngineTests extends ESTestCase {
             engine.onSettingsChanged();
 
             ParsedDocument doc3 = testParsedDocument("3", "3", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-            engine.create(new Engine.Create(newUid("3"), doc3));
+            engine.index(new Engine.Index(newUid("3"), doc3));
             engine.refresh("test");
 
             segments = engine.segments(false);
@@ -386,7 +355,7 @@ public class InternalEngineTests extends ESTestCase {
             engine.config().setCompoundOnFlush(true);
             engine.onSettingsChanged();
             ParsedDocument doc4 = testParsedDocument("4", "4", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-            engine.create(new Engine.Create(newUid("4"), doc4));
+            engine.index(new Engine.Index(newUid("4"), doc4));
             engine.refresh("test");
 
             segments = engine.segments(false);
@@ -420,7 +389,7 @@ public class InternalEngineTests extends ESTestCase {
             assertThat(segments.isEmpty(), equalTo(true));
 
             ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-            engine.create(new Engine.Create(newUid("1"), doc));
+            engine.index(new Engine.Index(newUid("1"), doc));
             engine.refresh("test");
 
             segments = engine.segments(true);
@@ -428,10 +397,10 @@ public class InternalEngineTests extends ESTestCase {
             assertThat(segments.get(0).ramTree, notNullValue());
 
             ParsedDocument doc2 = testParsedDocument("2", "2", "test", null, -1, -1, testDocumentWithTextField(), B_2, null);
-            engine.create(new Engine.Create(newUid("2"), doc2));
+            engine.index(new Engine.Index(newUid("2"), doc2));
             engine.refresh("test");
             ParsedDocument doc3 = testParsedDocument("3", "3", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-            engine.create(new Engine.Create(newUid("3"), doc3));
+            engine.index(new Engine.Index(newUid("3"), doc3));
             engine.refresh("test");
 
             segments = engine.segments(true);
@@ -501,7 +470,7 @@ public class InternalEngineTests extends ESTestCase {
         Document document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
 
         CommitStats stats1 = engine.commitStats();
         assertThat(stats1.getGeneration(), greaterThan(0l));
@@ -552,7 +521,7 @@ public class InternalEngineTests extends ESTestCase {
     /* */
     public void testConcurrentGetAndFlush() throws Exception {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
 
         final AtomicReference<Engine.GetResult> latestGetResult = new AtomicReference<>();
         latestGetResult.set(engine.get(new Engine.Get(true, newUid("1"))));
@@ -597,7 +566,7 @@ public class InternalEngineTests extends ESTestCase {
         Document document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
 
         // its not there...
         searchResult = engine.acquireSearcher("test");
@@ -689,7 +658,7 @@ public class InternalEngineTests extends ESTestCase {
         document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED));
 
         // its not there...
         searchResult = engine.acquireSearcher("test");
@@ -750,7 +719,7 @@ public class InternalEngineTests extends ESTestCase {
 
         // create a document
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
 
         // its not there...
         searchResult = engine.acquireSearcher("test");
@@ -786,7 +755,7 @@ public class InternalEngineTests extends ESTestCase {
                      new LogByteSizeMergePolicy()), false)) {
             final String syncId = randomUnicodeOfCodepointLengthBetween(10, 20);
             ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-            engine.create(new Engine.Create(newUid("1"), doc));
+            engine.index(new Engine.Index(newUid("1"), doc));
             Engine.CommitId commitID = engine.flush();
             assertThat(commitID, equalTo(new Engine.CommitId(store.readLastCommittedSegmentsInfo().getId())));
             byte[] wrongBytes = Base64.decode(commitID.toString());
@@ -794,7 +763,7 @@ public class InternalEngineTests extends ESTestCase {
             Engine.CommitId wrongId = new Engine.CommitId(wrongBytes);
             assertEquals("should fail to sync flush with wrong id (but no docs)", engine.syncFlush(syncId + "1", wrongId),
                     Engine.SyncedFlushResult.COMMIT_MISMATCH);
-            engine.create(new Engine.Create(newUid("2"), doc));
+            engine.index(new Engine.Index(newUid("2"), doc));
             assertEquals("should fail to sync flush with right id but pending doc", engine.syncFlush(syncId + "2", commitID),
                     Engine.SyncedFlushResult.PENDING_OPERATIONS);
             commitID = engine.flush();
@@ -808,7 +777,7 @@ public class InternalEngineTests extends ESTestCase {
     public void testSycnedFlushSurvivesEngineRestart() throws IOException {
         final String syncId = randomUnicodeOfCodepointLengthBetween(10, 20);
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
         final Engine.CommitId commitID = engine.flush();
         assertEquals("should succeed to flush commit with right id and no pending doc", engine.syncFlush(syncId, commitID),
                 Engine.SyncedFlushResult.SUCCESS);
@@ -827,14 +796,14 @@ public class InternalEngineTests extends ESTestCase {
     public void testSycnedFlushVanishesOnReplay() throws IOException {
         final String syncId = randomUnicodeOfCodepointLengthBetween(10, 20);
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
         final Engine.CommitId commitID = engine.flush();
         assertEquals("should succeed to flush commit with right id and no pending doc", engine.syncFlush(syncId, commitID),
                 Engine.SyncedFlushResult.SUCCESS);
         assertEquals(store.readLastCommittedSegmentsInfo().getUserData().get(Engine.SYNC_COMMIT_ID), syncId);
         assertEquals(engine.getLastCommittedSegmentInfos().getUserData().get(Engine.SYNC_COMMIT_ID), syncId);
         doc = testParsedDocument("2", "2", "test", null, -1, -1, testDocumentWithTextField(), new BytesArray("{}"), null);
-        engine.create(new Engine.Create(newUid("2"), doc));
+        engine.index(new Engine.Index(newUid("2"), doc));
         EngineConfig config = engine.config();
         engine.close();
         final MockDirectoryWrapper directory = DirectoryUtils.getLeaf(store.directory(), MockDirectoryWrapper.class);
@@ -851,28 +820,16 @@ public class InternalEngineTests extends ESTestCase {
     @Test
     public void testVersioningNewCreate() {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, null);
-        Engine.Create create = new Engine.Create(newUid("1"), doc);
-        engine.create(create);
+        Engine.Index create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED);
+        engine.index(create);
         assertThat(create.version(), equalTo(1l));
 
-        create = new Engine.Create(newUid("1"), doc, create.version(), create.versionType().versionTypeForReplicationAndRecovery(), REPLICA, 0);
-        replicaEngine.create(create);
+        create = new Engine.Index(newUid("1"), doc, create.version(), create.versionType().versionTypeForReplicationAndRecovery(), REPLICA, 0);
+        replicaEngine.index(create);
         assertThat(create.version(), equalTo(1l));
     }
 
     @Test
-    public void testExternalVersioningNewCreate() {
-        ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, null);
-        Engine.Create create = new Engine.Create(newUid("1"), doc, 12, VersionType.EXTERNAL, Engine.Operation.Origin.PRIMARY, 0);
-        engine.create(create);
-        assertThat(create.version(), equalTo(12l));
-
-        create = new Engine.Create(newUid("1"), doc, create.version(), create.versionType().versionTypeForReplicationAndRecovery(), REPLICA, 0);
-        replicaEngine.create(create);
-        assertThat(create.version(), equalTo(12l));
-    }
-
-    @Test
     public void testVersioningNewIndex() {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, null);
         Engine.Index index = new Engine.Index(newUid("1"), doc);
@@ -1048,7 +1005,6 @@ public class InternalEngineTests extends ESTestCase {
                 final CountDownLatch indexed = new CountDownLatch(1);
 
                 Thread thread = new Thread() {
-                    @Override
                     public void run() {
                         try {
                             try {
@@ -1136,9 +1092,9 @@ public class InternalEngineTests extends ESTestCase {
         }
 
         // we shouldn't be able to create as well
-        Engine.Create create = new Engine.Create(newUid("1"), doc, 2l, VersionType.INTERNAL, PRIMARY, 0);
+        Engine.Index create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
         try {
-            engine.create(create);
+            engine.index(create);
         } catch (VersionConflictEngineException e) {
             // all is well
         }
@@ -1193,9 +1149,9 @@ public class InternalEngineTests extends ESTestCase {
         }
 
         // we shouldn't be able to create as well
-        Engine.Create create = new Engine.Create(newUid("1"), doc, 2l, VersionType.INTERNAL, PRIMARY, 0);
+        Engine.Index create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
         try {
-            engine.create(create);
+            engine.index(create);
         } catch (VersionConflictEngineException e) {
             // all is well
         }
@@ -1204,15 +1160,15 @@ public class InternalEngineTests extends ESTestCase {
     @Test
     public void testVersioningCreateExistsException() {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, null);
-        Engine.Create create = new Engine.Create(newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, 0);
-        engine.create(create);
+        Engine.Index create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
+        engine.index(create);
         assertThat(create.version(), equalTo(1l));
 
-        create = new Engine.Create(newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, 0);
+        create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
         try {
-            engine.create(create);
+            engine.index(create);
             fail();
-        } catch (DocumentAlreadyExistsException e) {
+        } catch (VersionConflictEngineException e) {
             // all is well
         }
     }
@@ -1220,17 +1176,17 @@ public class InternalEngineTests extends ESTestCase {
     @Test
     public void testVersioningCreateExistsExceptionWithFlush() {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, null);
-        Engine.Create create = new Engine.Create(newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, 0);
-        engine.create(create);
+        Engine.Index create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
+        engine.index(create);
         assertThat(create.version(), equalTo(1l));
 
         engine.flush();
 
-        create = new Engine.Create(newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, 0);
+        create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
         try {
-            engine.create(create);
+            engine.index(create);
             fail();
-        } catch (DocumentAlreadyExistsException e) {
+        } catch (VersionConflictEngineException e) {
             // all is well
         }
     }
@@ -1394,13 +1350,13 @@ public class InternalEngineTests extends ESTestCase {
         try {
             // First, with DEBUG, which should NOT log IndexWriter output:
             ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-            engine.create(new Engine.Create(newUid("1"), doc));
+            engine.index(new Engine.Index(newUid("1"), doc));
             engine.flush();
             assertFalse(mockAppender.sawIndexWriterMessage);
 
             // Again, with TRACE, which should log IndexWriter output:
             rootLogger.setLevel(Level.TRACE);
-            engine.create(new Engine.Create(newUid("2"), doc));
+            engine.index(new Engine.Index(newUid("2"), doc));
             engine.flush();
             assertTrue(mockAppender.sawIndexWriterMessage);
 
@@ -1429,14 +1385,14 @@ public class InternalEngineTests extends ESTestCase {
         try {
             // First, with DEBUG, which should NOT log IndexWriter output:
             ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-            engine.create(new Engine.Create(newUid("1"), doc));
+            engine.index(new Engine.Index(newUid("1"), doc));
             engine.flush();
             assertFalse(mockAppender.sawIndexWriterMessage);
             assertFalse(mockAppender.sawIndexWriterIFDMessage);
 
             // Again, with TRACE, which should only log IndexWriter IFD output:
             iwIFDLogger.setLevel(Level.TRACE);
-            engine.create(new Engine.Create(newUid("2"), doc));
+            engine.index(new Engine.Index(newUid("2"), doc));
             engine.flush();
             assertFalse(mockAppender.sawIndexWriterMessage);
             assertTrue(mockAppender.sawIndexWriterIFDMessage);
@@ -1636,8 +1592,8 @@ public class InternalEngineTests extends ESTestCase {
         final int numDocs = randomIntBetween(1, 10);
         for (int i = 0; i < numDocs; i++) {
             ParsedDocument doc = testParsedDocument(Integer.toString(i), Integer.toString(i), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-            Engine.Create firstIndexRequest = new Engine.Create(newUid(Integer.toString(i)), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime());
-            engine.create(firstIndexRequest);
+            Engine.Index firstIndexRequest = new Engine.Index(newUid(Integer.toString(i)), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, System.nanoTime());
+            engine.index(firstIndexRequest);
             assertThat(firstIndexRequest.version(), equalTo(1l));
         }
         engine.refresh("test");
@@ -1689,8 +1645,8 @@ public class InternalEngineTests extends ESTestCase {
         final int numDocs = randomIntBetween(1, 10);
         for (int i = 0; i < numDocs; i++) {
             ParsedDocument doc = testParsedDocument(Integer.toString(i), Integer.toString(i), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-            Engine.Create firstIndexRequest = new Engine.Create(newUid(Integer.toString(i)), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime());
-            engine.create(firstIndexRequest);
+            Engine.Index firstIndexRequest = new Engine.Index(newUid(Integer.toString(i)), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, System.nanoTime());
+            engine.index(firstIndexRequest);
             assertThat(firstIndexRequest.version(), equalTo(1l));
         }
         engine.refresh("test");
@@ -1717,7 +1673,7 @@ public class InternalEngineTests extends ESTestCase {
     private Mapping dynamicUpdate() {
         BuilderContext context = new BuilderContext(Settings.EMPTY, new ContentPath());
         final RootObjectMapper root = MapperBuilders.rootObject("some_type").build(context);
-        return new Mapping(Version.CURRENT, root, new MetadataFieldMapper[0], new Mapping.SourceTransform[0], emptyMap());
+        return new Mapping(Version.CURRENT, root, new MetadataFieldMapper[0], new Mapping.SourceTransform[0], ImmutableMap.<String, Object>of());
     }
 
     public void testUpgradeOldIndex() throws IOException {
@@ -1730,10 +1686,6 @@ public class InternalEngineTests extends ESTestCase {
         Collections.shuffle(indexes, random());
         for (Path indexFile : indexes.subList(0, scaledRandomIntBetween(1, indexes.size() / 2))) {
             final String indexName = indexFile.getFileName().toString().replace(".zip", "").toLowerCase(Locale.ROOT);
-            Version version = Version.fromString(indexName.replace("index-", ""));
-            if (version.onOrAfter(Version.V_2_0_0_beta1)) {
-                continue;
-            }
             Path unzipDir = createTempDir();
             Path unzipDataDir = unzipDir.resolve("data");
             // decompress the index
@@ -1753,11 +1705,9 @@ public class InternalEngineTests extends ESTestCase {
             assertTrue("[" + indexFile + "] missing index dir: " + src.toString(), Files.exists(src));
             assertTrue("[" + indexFile + "] missing translog dir: " + translog.toString(), Files.exists(translog));
             Path[] tlogFiles = filterExtraFSFiles(FileSystemUtils.files(translog));
-            assertEquals(Arrays.toString(tlogFiles), tlogFiles.length, 1);
+            assertEquals(Arrays.toString(tlogFiles), tlogFiles.length, 2); // ckp & tlog
+            Path tlogFile = tlogFiles[0].getFileName().toString().endsWith("tlog") ? tlogFiles[0] : tlogFiles[1];
             final long size = Files.size(tlogFiles[0]);
-
-            final long generation = TranslogTests.parseLegacyTranslogFile(tlogFiles[0]);
-            assertTrue(generation >= 1);
             logger.debug("upgrading index {} file: {} size: {}", indexName, tlogFiles[0].getFileName(), size);
             Directory directory = newFSDirectory(src.resolve("0").resolve("index"));
             Store store = createStore(directory);
@@ -1790,8 +1740,8 @@ public class InternalEngineTests extends ESTestCase {
                 final int numExtraDocs = randomIntBetween(1, 10);
                 for (int i = 0; i < numExtraDocs; i++) {
                     ParsedDocument doc = testParsedDocument("extra" + Integer.toString(i), "extra" + Integer.toString(i), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-                    Engine.Create firstIndexRequest = new Engine.Create(newUid(Integer.toString(i)), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime());
-                    engine.create(firstIndexRequest);
+                    Engine.Index firstIndexRequest = new Engine.Index(newUid(Integer.toString(i)), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, System.nanoTime());
+                    engine.index(firstIndexRequest);
                     assertThat(firstIndexRequest.version(), equalTo(1l));
                 }
                 engine.refresh("test");
@@ -1819,8 +1769,8 @@ public class InternalEngineTests extends ESTestCase {
         final int numDocs = randomIntBetween(1, 10);
         for (int i = 0; i < numDocs; i++) {
             ParsedDocument doc = testParsedDocument(Integer.toString(i), Integer.toString(i), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-            Engine.Create firstIndexRequest = new Engine.Create(newUid(Integer.toString(i)), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime());
-            engine.create(firstIndexRequest);
+            Engine.Index firstIndexRequest = new Engine.Index(newUid(Integer.toString(i)), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, System.nanoTime());
+            engine.index(firstIndexRequest);
             assertThat(firstIndexRequest.version(), equalTo(1l));
         }
         engine.refresh("test");
@@ -1868,8 +1818,8 @@ public class InternalEngineTests extends ESTestCase {
         int randomId = randomIntBetween(numDocs + 1, numDocs + 10);
         String uuidValue = "test#" + Integer.toString(randomId);
         ParsedDocument doc = testParsedDocument(uuidValue, Integer.toString(randomId), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-        Engine.Create firstIndexRequest = new Engine.Create(newUid(uuidValue), doc, 1, VersionType.EXTERNAL, PRIMARY, System.nanoTime());
-        engine.create(firstIndexRequest);
+        Engine.Index firstIndexRequest = new Engine.Index(newUid(uuidValue), doc, 1, VersionType.EXTERNAL, PRIMARY, System.nanoTime());
+        engine.index(firstIndexRequest);
         assertThat(firstIndexRequest.version(), equalTo(1l));
         if (flush) {
             engine.flush();
@@ -1914,15 +1864,15 @@ public class InternalEngineTests extends ESTestCase {
         public final AtomicInteger recoveredOps = new AtomicInteger(0);
 
         public TranslogHandler(String indexName, ESLogger logger) {
-            super(new ShardId("test", 0), null, null, null, null, logger);
+            super(new ShardId("test", 0), null, logger);
             Settings settings = Settings.settingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
             RootObjectMapper.Builder rootBuilder = new RootObjectMapper.Builder("test");
             Index index = new Index(indexName);
             AnalysisService analysisService = new AnalysisService(index, settings);
-            SimilarityLookupService similarityLookupService = new SimilarityLookupService(index, settings);
-            MapperService mapperService = new MapperService(index, settings, analysisService, similarityLookupService, null);
+            SimilarityService similarityService = new SimilarityService(index, settings);
+            MapperService mapperService = new MapperService(index, settings, analysisService, similarityService, null);
             DocumentMapper.Builder b = new DocumentMapper.Builder(settings, rootBuilder, mapperService);
-            DocumentMapperParser parser = new DocumentMapperParser(settings, mapperService, analysisService, similarityLookupService, null);
+            DocumentMapperParser parser = new DocumentMapperParser(settings, mapperService, analysisService, similarityService, null);
             this.docMapper = b.build(mapperService, parser);
 
         }
@@ -1936,21 +1886,14 @@ public class InternalEngineTests extends ESTestCase {
         protected void operationProcessed() {
             recoveredOps.incrementAndGet();
         }
-
-        @Override
-        public void performRecoveryOperation(Engine engine, Translog.Operation operation, boolean allowMappingUpdates) {
-            if (operation.opType() != Translog.Operation.Type.DELETE_BY_QUERY) { // we don't support del by query in this test
-                super.performRecoveryOperation(engine, operation, allowMappingUpdates);
-            }
-        }
     }
 
     public void testRecoverFromForeignTranslog() throws IOException {
         final int numDocs = randomIntBetween(1, 10);
         for (int i = 0; i < numDocs; i++) {
             ParsedDocument doc = testParsedDocument(Integer.toString(i), Integer.toString(i), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-            Engine.Create firstIndexRequest = new Engine.Create(newUid(Integer.toString(i)), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime());
-            engine.create(firstIndexRequest);
+            Engine.Index firstIndexRequest = new Engine.Index(newUid(Integer.toString(i)), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, System.nanoTime());
+            engine.index(firstIndexRequest);
             assertThat(firstIndexRequest.version(), equalTo(1l));
         }
         engine.refresh("test");
@@ -1968,7 +1911,7 @@ public class InternalEngineTests extends ESTestCase {
         engine.close();
 
         Translog translog = new Translog(new TranslogConfig(shardId, createTempDir(), Settings.EMPTY, Translog.Durabilty.REQUEST, BigArrays.NON_RECYCLING_INSTANCE, threadPool));
-        translog.add(new Translog.Create("test", "SomeBogusId", "{}".getBytes(Charset.forName("UTF-8"))));
+        translog.add(new Translog.Index("test", "SomeBogusId", "{}".getBytes(Charset.forName("UTF-8"))));
         assertEquals(generation.translogFileGeneration, translog.currentFileGeneration());
         translog.close();
 
diff --git a/core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java b/core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java
index b5987a9..2c6ee40 100644
--- a/core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java
+++ b/core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java
@@ -236,7 +236,7 @@ public class ShadowEngineTests extends ESTestCase {
     public void testCommitStats() {
         // create a doc and refresh
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
 
         CommitStats stats1 = replicaEngine.commitStats();
         assertThat(stats1.getGeneration(), greaterThan(0l));
@@ -271,10 +271,10 @@ public class ShadowEngineTests extends ESTestCase {
 
         // create a doc and refresh
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
 
         ParsedDocument doc2 = testParsedDocument("2", "2", "test", null, -1, -1, testDocumentWithTextField(), B_2, null);
-        primaryEngine.create(new Engine.Create(newUid("2"), doc2));
+        primaryEngine.index(new Engine.Index(newUid("2"), doc2));
         primaryEngine.refresh("test");
 
         segments = primaryEngine.segments(false);
@@ -334,7 +334,7 @@ public class ShadowEngineTests extends ESTestCase {
         primaryEngine.onSettingsChanged();
 
         ParsedDocument doc3 = testParsedDocument("3", "3", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-        primaryEngine.create(new Engine.Create(newUid("3"), doc3));
+        primaryEngine.index(new Engine.Index(newUid("3"), doc3));
         primaryEngine.refresh("test");
 
         segments = primaryEngine.segments(false);
@@ -407,7 +407,7 @@ public class ShadowEngineTests extends ESTestCase {
         primaryEngine.onSettingsChanged();
 
         ParsedDocument doc4 = testParsedDocument("4", "4", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-        primaryEngine.create(new Engine.Create(newUid("4"), doc4));
+        primaryEngine.index(new Engine.Index(newUid("4"), doc4));
         primaryEngine.refresh("test");
 
         segments = primaryEngine.segments(false);
@@ -441,7 +441,7 @@ public class ShadowEngineTests extends ESTestCase {
         assertThat(segments.isEmpty(), equalTo(true));
 
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
         primaryEngine.refresh("test");
 
         segments = primaryEngine.segments(true);
@@ -449,10 +449,10 @@ public class ShadowEngineTests extends ESTestCase {
         assertThat(segments.get(0).ramTree, notNullValue());
 
         ParsedDocument doc2 = testParsedDocument("2", "2", "test", null, -1, -1, testDocumentWithTextField(), B_2, null);
-        primaryEngine.create(new Engine.Create(newUid("2"), doc2));
+        primaryEngine.index(new Engine.Index(newUid("2"), doc2));
         primaryEngine.refresh("test");
         ParsedDocument doc3 = testParsedDocument("3", "3", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-        primaryEngine.create(new Engine.Create(newUid("3"), doc3));
+        primaryEngine.index(new Engine.Index(newUid("3"), doc3));
         primaryEngine.refresh("test");
 
         segments = primaryEngine.segments(true);
@@ -480,7 +480,7 @@ public class ShadowEngineTests extends ESTestCase {
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
         try {
-            replicaEngine.create(new Engine.Create(newUid("1"), doc));
+            replicaEngine.index(new Engine.Index(newUid("1"), doc));
             fail("should have thrown an exception");
         } catch (UnsupportedOperationException e) {}
         replicaEngine.refresh("test");
@@ -517,7 +517,7 @@ public class ShadowEngineTests extends ESTestCase {
         document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
         primaryEngine.flush();
         replicaEngine.refresh("test");
 
@@ -573,7 +573,7 @@ public class ShadowEngineTests extends ESTestCase {
         ParseContext.Document document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
 
         // its not there...
         searchResult = primaryEngine.acquireSearcher("test");
@@ -700,7 +700,7 @@ public class ShadowEngineTests extends ESTestCase {
         document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
 
         // its not there...
         searchResult = primaryEngine.acquireSearcher("test");
@@ -784,7 +784,7 @@ public class ShadowEngineTests extends ESTestCase {
 
         // create a document
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
 
         // its not there...
         searchResult = primaryEngine.acquireSearcher("test");
@@ -830,7 +830,7 @@ public class ShadowEngineTests extends ESTestCase {
     @Test
     public void testFailEngineOnCorruption() {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
         primaryEngine.flush();
         MockDirectoryWrapper leaf = DirectoryUtils.getLeaf(replicaEngine.config().getStore().directory(), MockDirectoryWrapper.class);
         leaf.setRandomIOExceptionRate(1.0);
@@ -869,7 +869,7 @@ public class ShadowEngineTests extends ESTestCase {
     public void testFailStart() throws IOException {
         // Need a commit point for this
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
         primaryEngine.flush();
 
         // this test fails if any reader, searcher or directory is not closed - MDW FTW
@@ -957,7 +957,7 @@ public class ShadowEngineTests extends ESTestCase {
         ParseContext.Document document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        pEngine.create(new Engine.Create(newUid("1"), doc));
+        pEngine.index(new Engine.Index(newUid("1"), doc));
         pEngine.flush(true, true);
 
         t.join();
@@ -965,4 +965,13 @@ public class ShadowEngineTests extends ESTestCase {
         // (shadow engine is already shut down in the try-with-resources)
         IOUtils.close(srStore, pEngine, pStore);
     }
+
+    public void testNoTranslog() {
+        try {
+            replicaEngine.getTranslog();
+            fail("shadow engine has no translog");
+        } catch (UnsupportedOperationException ex) {
+            // all good
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTestCase.java b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTestCase.java
index 6c3054f..117ef2f 100644
--- a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTestCase.java
@@ -601,10 +601,10 @@ public abstract class AbstractStringFieldDataTestCase extends AbstractFieldDataI
         assertThat(ifd.loadGlobal(topLevelReader), sameInstance(globalOrdinals));
         // 3 b/c 1 segment level caches and 1 top level cache
         // in case of doc values, we don't cache atomic FD, so only the top-level cache is there
-        assertThat(indicesFieldDataCache.getCache().size(), equalTo(hasDocValues() ? 1L : 4L));
+        assertThat(indicesFieldDataCache.getCache().weight(), equalTo(hasDocValues() ? 1L : 4L));
 
         IndexOrdinalsFieldData cachedInstance = null;
-        for (Accountable ramUsage : indicesFieldDataCache.getCache().asMap().values()) {
+        for (Accountable ramUsage : indicesFieldDataCache.getCache().values()) {
             if (ramUsage instanceof IndexOrdinalsFieldData) {
                 cachedInstance = (IndexOrdinalsFieldData) ramUsage;
                 break;
@@ -613,12 +613,12 @@ public abstract class AbstractStringFieldDataTestCase extends AbstractFieldDataI
         assertThat(cachedInstance, sameInstance(globalOrdinals));
         topLevelReader.close();
         // Now only 3 segment level entries, only the toplevel reader has been closed, but the segment readers are still used by IW
-        assertThat(indicesFieldDataCache.getCache().size(), equalTo(hasDocValues() ? 0L : 3L));
+        assertThat(indicesFieldDataCache.getCache().weight(), equalTo(hasDocValues() ? 0L : 3L));
 
         refreshReader();
         assertThat(ifd.loadGlobal(topLevelReader), not(sameInstance(globalOrdinals)));
 
         ifdService.clear();
-        assertThat(indicesFieldDataCache.getCache().size(), equalTo(0l));
+        assertThat(indicesFieldDataCache.getCache().weight(), equalTo(0l));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java
index 8326287..ce71635 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java
@@ -18,15 +18,12 @@
  */
 package org.elasticsearch.index.mapper;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.mapping.get.GetMappingsResponse;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.mapper.core.IntegerFieldMapper;
 import org.elasticsearch.index.mapper.core.StringFieldMapper;
@@ -34,7 +31,6 @@ import org.elasticsearch.test.ESSingleNodeTestCase;
 
 import java.io.IOException;
 
-import static java.util.Collections.emptyMap;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.nullValue;
@@ -190,7 +186,7 @@ public class DynamicMappingTests extends ESSingleNodeTestCase {
 
     private String serialize(ToXContent mapper) throws Exception {
         XContentBuilder builder = XContentFactory.jsonBuilder().startObject();
-        mapper.toXContent(builder, new ToXContent.MapParams(emptyMap()));
+        mapper.toXContent(builder, new ToXContent.MapParams(ImmutableMap.<String, String>of()));
         return builder.endObject().string();
     }
 
@@ -201,7 +197,7 @@ public class DynamicMappingTests extends ESSingleNodeTestCase {
         ctx.reset(XContentHelper.createParser(source.source()), new ParseContext.Document(), source);
         assertEquals(XContentParser.Token.START_OBJECT, ctx.parser().nextToken());
         ctx.parser().nextToken();
-        return DocumentParser.parseObject(ctx, mapper.root());
+        return DocumentParser.parseObject(ctx, mapper.root(), true);
     }
 
     public void testDynamicMappingsNotNeeded() throws Exception {
@@ -325,7 +321,7 @@ public class DynamicMappingTests extends ESSingleNodeTestCase {
         String mapping = XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties")
                 .startObject("foo").field("type", "object").endObject()
                 .endObject().endObject().endObject().string();
-
+        
         DocumentMapper mapper = parser.parse(mapping);
         assertEquals(mapping, serialize(mapper));
 
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/FieldTypeLookupTests.java b/core/src/test/java/org/elasticsearch/index/mapper/FieldTypeLookupTests.java
index 5e3b61a..6ab4ca3 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/FieldTypeLookupTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/FieldTypeLookupTests.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.mapper;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
@@ -30,7 +29,6 @@ import java.util.Arrays;
 import java.util.Collection;
 import java.util.Iterator;
 import java.util.List;
-import java.util.stream.StreamSupport;
 
 public class FieldTypeLookupTests extends ESTestCase {
 
@@ -61,7 +59,7 @@ public class FieldTypeLookupTests extends ESTestCase {
         assertNull(lookup.get("bar"));
         assertEquals(f.fieldType(), lookup2.getByIndexName("bar"));
         assertNull(lookup.getByIndexName("foo"));
-        assertEquals(1, Iterators.size(lookup2.iterator()));
+        assertEquals(1, size(lookup2.iterator()));
     }
 
     public void testAddExistingField() {
@@ -76,7 +74,7 @@ public class FieldTypeLookupTests extends ESTestCase {
         assertSame(f.fieldType(), f2.fieldType());
         assertSame(f.fieldType(), lookup2.get("foo"));
         assertSame(f.fieldType(), lookup2.getByIndexName("foo"));
-        assertEquals(1, Iterators.size(lookup2.iterator()));
+        assertEquals(1, size(lookup2.iterator()));
     }
 
     public void testAddExistingIndexName() {
@@ -92,7 +90,7 @@ public class FieldTypeLookupTests extends ESTestCase {
         assertSame(f.fieldType(), lookup2.get("foo"));
         assertSame(f.fieldType(), lookup2.get("bar"));
         assertSame(f.fieldType(), lookup2.getByIndexName("foo"));
-        assertEquals(2, Iterators.size(lookup2.iterator()));
+        assertEquals(2, size(lookup2.iterator()));
     }
 
     public void testAddExistingFullName() {
@@ -108,7 +106,7 @@ public class FieldTypeLookupTests extends ESTestCase {
         assertSame(f.fieldType(), lookup2.get("foo"));
         assertSame(f.fieldType(), lookup2.getByIndexName("foo"));
         assertSame(f.fieldType(), lookup2.getByIndexName("bar"));
-        assertEquals(1, Iterators.size(lookup2.iterator()));
+        assertEquals(1, size(lookup2.iterator()));
     }
 
     public void testAddExistingBridgeName() {
@@ -286,4 +284,16 @@ public class FieldTypeLookupTests extends ESTestCase {
         @Override
         protected void parseCreateField(ParseContext context, List list) throws IOException {}
     }
+
+    private int size(Iterator<MappedFieldType> iterator) {
+        if (iterator == null) {
+            throw new NullPointerException("iterator");
+        }
+        int count = 0;
+        while (iterator.hasNext()) {
+            count++;
+            iterator.next();
+        }
+        return count;
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java
index a7314c2..0e3a04a 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java
@@ -43,6 +43,7 @@ import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.MapperParsingException;
 import org.elasticsearch.index.mapper.ParseContext.Document;
 import org.elasticsearch.index.mapper.ParsedDocument;
+import org.elasticsearch.index.mapper.SourceToParse;
 import org.elasticsearch.index.mapper.internal.AllFieldMapper;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 import org.elasticsearch.index.mapper.internal.TimestampFieldMapper;
@@ -453,4 +454,17 @@ public class SimpleAllMapperTests extends ESSingleNodeTestCase {
         // the backcompat behavior is actually ignoring directly specifying _all
         assertFalse(field.getAllEntries().fields().iterator().hasNext());
     }
+
+    public void testIncludeInObjectNotAllowed() throws Exception {
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type").endObject().endObject().string();
+        DocumentMapper docMapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
+
+        try {
+            docMapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
+                .startObject().field("_all", "foo").endObject().bytes());
+            fail("Expected failure to parse metadata field");
+        } catch (MapperParsingException e) {
+            assertTrue(e.getMessage(), e.getMessage().contains("Field [_all] is a metadata field and cannot be added inside a document"));
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/binary/BinaryMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/binary/BinaryMappingTests.java
index b5a002d..0bc56b0 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/binary/BinaryMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/binary/BinaryMappingTests.java
@@ -93,35 +93,4 @@ public class BinaryMappingTests extends ESSingleNodeTestCase {
             assertEquals(new BytesArray(value), originalValue);
         }
     }
-
-    public void testCompressedBackCompat() throws Exception {
-        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
-                .startObject("properties")
-                .startObject("field")
-                .field("type", "binary")
-                .field("store", "yes")
-                .endObject()
-                .endObject()
-                .endObject().endObject().string();
-
-        Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_5_0).build();
-        DocumentMapper mapper = createIndex("test", settings).mapperService().documentMapperParser().parse(mapping);
-
-        final byte[] original = new byte[100];
-        original[56] = 1;
-        BytesStreamOutput out = new BytesStreamOutput();
-        try (StreamOutput compressed = CompressorFactory.defaultCompressor().streamOutput(out)) {
-            new BytesArray(original).writeTo(compressed);
-        }
-        final byte[] binaryValue = out.bytes().toBytes();
-        assertTrue(CompressorFactory.isCompressed(new BytesArray(binaryValue)));
-        
-        ParsedDocument doc = mapper.parse("test", "type", "id", XContentFactory.jsonBuilder().startObject().field("field", binaryValue).endObject().bytes());
-        BytesRef indexedValue = doc.rootDoc().getBinaryValue("field");
-        assertEquals(new BytesRef(binaryValue), indexedValue);
-        FieldMapper fieldMapper = mapper.mappers().smartNameFieldMapper("field");
-        Object originalValue = fieldMapper.fieldType().valueForSearch(indexedValue);
-        assertEquals(new BytesArray(original), originalValue);
-    }
-
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/core/BinaryFieldTypeTests.java b/core/src/test/java/org/elasticsearch/index/mapper/core/BinaryFieldTypeTests.java
index f241d55..7ab7886 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/core/BinaryFieldTypeTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/core/BinaryFieldTypeTests.java
@@ -28,15 +28,4 @@ public class BinaryFieldTypeTests extends FieldTypeTestCase {
     protected MappedFieldType createDefaultFieldType() {
         return new BinaryFieldMapper.BinaryFieldType();
     }
-
-    @Before
-    public void setupProperties() {
-        addModifier(new Modifier("try_uncompressing", false, true) {
-            @Override
-            public void modify(MappedFieldType ft) {
-                BinaryFieldMapper.BinaryFieldType bft = (BinaryFieldMapper.BinaryFieldType)ft;
-                bft.setTryUncompressing(!bft.tryUncompressing());
-            }
-        });
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java b/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java
index e0c7a30..3d2134f 100755
--- a/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java
@@ -19,10 +19,10 @@
 
 package org.elasticsearch.index.mapper.externalvalues;
 
-import com.google.common.collect.Iterators;
 import com.spatial4j.core.shape.Point;
 import org.apache.lucene.document.Field;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.builders.ShapeBuilder;
 import org.elasticsearch.common.settings.Settings;
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/id/IdMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/id/IdMappingTests.java
index 2688674..679b49e 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/id/IdMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/id/IdMappingTests.java
@@ -114,4 +114,17 @@ public class IdMappingTests extends ESSingleNodeTestCase {
         // _id is not indexed so we need to check _uid
         assertEquals(Uid.createUid("type", "1"), doc.rootDoc().get(UidFieldMapper.NAME));
     }
+
+    public void testIncludeInObjectNotAllowed() throws Exception {
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type").endObject().endObject().string();
+        DocumentMapper docMapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
+
+        try {
+            docMapper.parse(SourceToParse.source(XContentFactory.jsonBuilder()
+                .startObject().field("_id", "1").endObject().bytes()).type("type"));
+            fail("Expected failure to parse metadata field");
+        } catch (MapperParsingException e) {
+            assertTrue(e.getMessage(), e.getMessage().contains("Field [_id] is a metadata field and cannot be added inside a document"));
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java
index bdfb0e4..3719500 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java
@@ -23,6 +23,7 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.MapperParsingException;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.SourceToParse;
 import org.elasticsearch.index.mapper.Uid;
@@ -32,21 +33,18 @@ import static org.hamcrest.Matchers.nullValue;
 
 public class ParentMappingTests extends ESSingleNodeTestCase {
 
-    public void testParentNotSet() throws Exception {
+    public void testParentSetInDocNotAllowed() throws Exception {
         String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
                 .endObject().endObject().string();
         DocumentMapper docMapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
 
-        ParsedDocument doc = docMapper.parse(SourceToParse.source(XContentFactory.jsonBuilder()
-                .startObject()
-                .field("_parent", "1122")
-                .field("x_field", "x_value")
-                .endObject()
-                .bytes()).type("type").id("1"));
-
-        // no _parent mapping, dynamically used as a string field
-        assertNull(doc.parent());
-        assertNotNull(doc.rootDoc().get("_parent"));
+        try {
+            docMapper.parse(SourceToParse.source(XContentFactory.jsonBuilder()
+                .startObject().field("_parent", "1122").endObject().bytes()).type("type").id("1"));
+            fail("Expected failure to parse metadata field");
+        } catch (MapperParsingException e) {
+            assertTrue(e.getMessage(), e.getMessage().contains("Field [_parent] is a metadata field and cannot be added inside a document"));
+        }
     }
 
     public void testParentSetInDocBackcompat() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/routing/RoutingTypeMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/routing/RoutingTypeMapperTests.java
index 30fcb5f..7d0afdb 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/routing/RoutingTypeMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/routing/RoutingTypeMapperTests.java
@@ -32,6 +32,7 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
 import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.MapperParsingException;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.SourceToParse;
 import org.elasticsearch.test.ESSingleNodeTestCase;
@@ -113,7 +114,7 @@ public class RoutingTypeMapperTests extends ESSingleNodeTestCase {
         Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_4_2.id).build();
         DocumentMapper docMapper = createIndex("test", settings).mapperService().documentMapperParser().parse(mapping);
 
-        XContentBuilder doc = XContentFactory.jsonBuilder().startObject().field("_timestamp", 2000000).endObject();
+        XContentBuilder doc = XContentFactory.jsonBuilder().startObject().field("_routing", "foo").endObject();
         MappingMetaData mappingMetaData = new MappingMetaData(docMapper);
         IndexRequest request = new IndexRequest("test", "type", "1").source(doc);
         request.process(MetaData.builder().build(), mappingMetaData, true, "test");
@@ -122,4 +123,17 @@ public class RoutingTypeMapperTests extends ESSingleNodeTestCase {
         assertNull(request.routing());
         assertNull(docMapper.parse("test", "type", "1", doc.bytes()).rootDoc().get("_routing"));
     }
+
+    public void testIncludeInObjectNotAllowed() throws Exception {
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type").endObject().endObject().string();
+        DocumentMapper docMapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
+
+        try {
+            docMapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
+                .startObject().field("_routing", "foo").endObject().bytes());
+            fail("Expected failure to parse metadata field");
+        } catch (MapperParsingException e) {
+            assertTrue(e.getMessage(), e.getMessage().contains("Field [_routing] is a metadata field and cannot be added inside a document"));
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java
index a54b63d..e6b08fb 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.mapper.string;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.DocValuesType;
 import org.apache.lucene.index.IndexOptions;
 import org.apache.lucene.index.IndexableField;
@@ -43,7 +44,6 @@ import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.ParseContext.Document;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.core.StringFieldMapper;
-import org.elasticsearch.index.mapper.core.StringFieldMapper.Builder;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 import org.elasticsearch.test.VersionUtils;
 import org.junit.Before;
@@ -52,11 +52,8 @@ import org.junit.Test;
 import java.util.Arrays;
 import java.util.Map;
 
-import static java.util.Collections.emptyMap;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.elasticsearch.index.mapper.core.StringFieldMapper.Builder;
+import static org.hamcrest.Matchers.*;
 
 /**
  */
@@ -124,7 +121,7 @@ public class SimpleStringMappingTests extends ESSingleNodeTestCase {
     }
 
     private void assertParseIdemPotent(IndexableFieldType expected, DocumentMapper mapper) throws Exception {
-        String mapping = mapper.toXContent(XContentFactory.jsonBuilder().startObject(), new ToXContent.MapParams(emptyMap())).endObject().string();
+        String mapping = mapper.toXContent(XContentFactory.jsonBuilder().startObject(), new ToXContent.MapParams(ImmutableMap.<String, String>of())).endObject().string();
         mapper = parser.parse(mapping);
         ParsedDocument doc = mapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
                 .startObject()
@@ -217,7 +214,7 @@ public class SimpleStringMappingTests extends ESSingleNodeTestCase {
         assertThat(fieldType.omitNorms(), equalTo(false));
         assertParseIdemPotent(fieldType, defaultMapper);
     }
-
+    
     @Test
     public void testSearchQuoteAnalyzerSerialization() throws Exception {
         // Cases where search_quote_analyzer should not be added to the mapping.
@@ -253,7 +250,7 @@ public class SimpleStringMappingTests extends ESSingleNodeTestCase {
             Map<String, Object> serializedMap = getSerializedMap(fieldName, mapper);
             assertFalse(fieldName, serializedMap.containsKey("search_quote_analyzer"));
         }
-
+        
         // Cases where search_quote_analyzer should be present.
         mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
                 .startObject("properties")
@@ -271,20 +268,20 @@ public class SimpleStringMappingTests extends ESSingleNodeTestCase {
                 .endObject()
                 .endObject()
                 .endObject().endObject().string();
-
+        
         mapper = parser.parse(mapping);
         for (String fieldName : Arrays.asList("field1", "field2")) {
             Map<String, Object> serializedMap = getSerializedMap(fieldName, mapper);
             assertEquals(serializedMap.get("search_quote_analyzer"), "simple");
         }
     }
-
+    
     private Map<String, Object> getSerializedMap(String fieldName, DocumentMapper mapper) throws Exception {
         FieldMapper fieldMapper = mapper.mappers().smartNameFieldMapper(fieldName);
         XContentBuilder builder = JsonXContent.contentBuilder().startObject();
         fieldMapper.toXContent(builder, ToXContent.EMPTY_PARAMS).endObject();
         builder.close();
-
+        
         Map<String, Object> fieldMap;
         try (XContentParser parser = JsonXContent.jsonXContent.createParser(builder.bytes())) {
             fieldMap = parser.map();
@@ -467,7 +464,7 @@ public class SimpleStringMappingTests extends ESSingleNodeTestCase {
         assertEquals(DocValuesType.NONE, docValuesType(doc, "str3"));
         assertEquals(DocValuesType.NONE, docValuesType(doc, "str4"));
         assertEquals(DocValuesType.SORTED_SET, docValuesType(doc, "str5"));
-
+        
     }
 
     // TODO: this function shouldn't be necessary.  parsing should just add a single field that is indexed and dv
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java
index 057dc41..e51b6a6 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java
@@ -769,6 +769,21 @@ public class TimestampMappingTests extends ESSingleNodeTestCase {
         assertNull(docMapper.parse("test", "type", "1", doc.bytes()).rootDoc().get("_timestamp"));
     }
 
+    public void testIncludeInObjectNotAllowed() throws Exception {
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
+            .startObject("_timestamp").field("enabled", true).field("default", "1970").field("format", "YYYY").endObject()
+            .endObject().endObject().string();
+        DocumentMapper docMapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
+
+        try {
+            docMapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
+                .startObject().field("_timestamp", 2000000).endObject().bytes());
+            fail("Expected failure to parse metadata field");
+        } catch (MapperParsingException e) {
+            assertTrue(e.getMessage(), e.getMessage().contains("Field [_timestamp] is a metadata field and cannot be added inside a document"));
+        }
+    }
+
     public void testThatEpochCanBeIgnoredWithCustomFormat() throws Exception {
         String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
                 .startObject("_timestamp").field("enabled", true).field("format", "yyyyMMddHH").endObject()
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java
index c9b6131..b9f7a98 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java
@@ -310,6 +310,21 @@ public class TTLMappingTests extends ESSingleNodeTestCase {
         assertNull(docMapper.parse("test", "type", "1", doc.bytes()).rootDoc().get("_ttl"));
     }
 
+    public void testIncludeInObjectNotAllowed() throws Exception {
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
+            .startObject("_ttl").field("enabled", true).endObject()
+            .endObject().endObject().string();
+        DocumentMapper docMapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
+
+        try {
+            docMapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
+                .startObject().field("_ttl", "2d").endObject().bytes());
+            fail("Expected failure to parse metadata field");
+        } catch (MapperParsingException e) {
+            assertTrue(e.getMessage(), e.getMessage().contains("Field [_ttl] is a metadata field and cannot be added inside a document"));
+        }
+    }
+
     private org.elasticsearch.common.xcontent.XContentBuilder getMappingWithTtlEnabled() throws IOException {
         return getMappingWithTtlEnabled(null);
     }
diff --git a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
index 5837d5b..fce0a94 100644
--- a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
@@ -215,7 +215,7 @@ public abstract class AbstractQueryTestCase<QB extends AbstractQueryBuilder<QB>>
                 new IndexSettingsModule(index, indexSettings),
                 new IndexCacheModule(indexSettings),
                 new AnalysisModule(indexSettings, new IndicesAnalysisService(indexSettings)),
-                new SimilarityModule(indexSettings),
+                new SimilarityModule(index, indexSettings),
                 new IndexNameModule(index),
         new AbstractModule() {
                     @Override
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java b/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java
index 9dd1a55..19e48aa 100644
--- a/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java
@@ -22,6 +22,7 @@ package org.elasticsearch.index.query;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.geo.GeoDistance;
 import org.elasticsearch.common.geo.GeoPoint;
+import org.elasticsearch.common.geo.GeoUtils;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.index.search.geo.GeoDistanceRangeQuery;
 import org.junit.Test;
@@ -108,8 +109,12 @@ public class GeoDistanceRangeQueryTests extends AbstractQueryTestCase<GeoDistanc
         GeoDistanceRangeQuery geoQuery = (GeoDistanceRangeQuery) query;
         assertThat(geoQuery.fieldName(), equalTo(queryBuilder.fieldName()));
         if (queryBuilder.point() != null) {
-            assertThat(geoQuery.lat(), equalTo(queryBuilder.point().lat()));
-            assertThat(geoQuery.lon(), equalTo(queryBuilder.point().lon()));
+            GeoPoint expectedPoint = new GeoPoint(queryBuilder.point());
+            if (GeoValidationMethod.isCoerce(queryBuilder.getValidationMethod())) {
+                GeoUtils.normalizePoint(expectedPoint, true, true);
+            }
+            assertThat(geoQuery.lat(), equalTo(expectedPoint.lat()));
+            assertThat(geoQuery.lon(), equalTo(expectedPoint.lon()));
         }
         assertThat(geoQuery.geoDistance(), equalTo(queryBuilder.geoDistance()));
         if (queryBuilder.from() != null && queryBuilder.from() instanceof Number) {
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java
index 40a002d..985fbfd 100644
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java
@@ -96,7 +96,7 @@ public class TemplateQueryParserTests extends ESTestCase {
                 new IndexSettingsModule(index, settings),
                 new IndexCacheModule(settings),
                 new AnalysisModule(settings, new IndicesAnalysisService(settings)),
-                new SimilarityModule(settings),
+                new SimilarityModule(index, settings),
                 new IndexNameModule(index),
                 new AbstractModule() {
                     @Override
diff --git a/core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreTests.java b/core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreTests.java
new file mode 100644
index 0000000..bc2272e
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreTests.java
@@ -0,0 +1,534 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.query.functionscore;
+
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.*;
+import org.apache.lucene.search.*;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.BytesRef;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.lucene.search.function.*;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.index.fielddata.*;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.query.functionscore.exp.ExponentialDecayFunctionBuilder;
+import org.elasticsearch.index.query.functionscore.gauss.GaussDecayFunctionBuilder;
+import org.elasticsearch.index.query.functionscore.lin.LinearDecayFunctionBuilder;
+import org.elasticsearch.search.MultiValueMode;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.concurrent.ExecutionException;
+
+import static org.hamcrest.Matchers.closeTo;
+import static org.hamcrest.core.IsEqual.equalTo;
+
+public class FunctionScoreTests extends ESTestCase {
+
+    private static final String UNSUPPORTED = "Method not implemented. This is just a stub for testing.";
+
+
+    /**
+     * Stub for IndexFieldData. Needed by some score functions. Returns 1 as count always.
+     */
+    private static class IndexFieldDataStub implements IndexFieldData<AtomicFieldData> {
+        @Override
+        public MappedFieldType.Names getFieldNames() {
+            return new MappedFieldType.Names("test");
+        }
+
+        @Override
+        public FieldDataType getFieldDataType() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public AtomicFieldData load(LeafReaderContext context) {
+            return new AtomicFieldData() {
+
+                @Override
+                public ScriptDocValues getScriptValues() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public SortedBinaryDocValues getBytesValues() {
+                    return new SortedBinaryDocValues() {
+                        @Override
+                        public void setDocument(int docId) {
+                        }
+
+                        @Override
+                        public int count() {
+                            return 1;
+                        }
+
+                        @Override
+                        public BytesRef valueAt(int index) {
+                            return new BytesRef("0");
+                        }
+                    };
+                }
+
+                @Override
+                public long ramBytesUsed() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public Collection<Accountable> getChildResources() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public void close() {
+                }
+            };
+        }
+
+        @Override
+        public AtomicFieldData loadDirect(LeafReaderContext context) throws Exception {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public IndexFieldData.XFieldComparatorSource comparatorSource(@Nullable Object missingValue, MultiValueMode sortMode, IndexFieldData.XFieldComparatorSource.Nested nested) {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public void clear() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public void clear(IndexReader reader) {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public Index index() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+    }
+
+    /**
+     * Stub for IndexNumericFieldData needed by some score functions. Returns 1 as value always.
+     */
+    private static class IndexNumericFieldDataStub implements IndexNumericFieldData {
+
+        @Override
+        public NumericType getNumericType() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public MappedFieldType.Names getFieldNames() {
+            return new MappedFieldType.Names("test");
+        }
+
+        @Override
+        public FieldDataType getFieldDataType() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public AtomicNumericFieldData load(LeafReaderContext context) {
+            return new AtomicNumericFieldData() {
+                @Override
+                public SortedNumericDocValues getLongValues() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public SortedNumericDoubleValues getDoubleValues() {
+                    return new SortedNumericDoubleValues() {
+                        @Override
+                        public void setDocument(int doc) {
+                        }
+
+                        @Override
+                        public double valueAt(int index) {
+                            return 1;
+                        }
+
+                        @Override
+                        public int count() {
+                            return 1;
+                        }
+                    };
+                }
+
+                @Override
+                public ScriptDocValues getScriptValues() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public SortedBinaryDocValues getBytesValues() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public long ramBytesUsed() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public Collection<Accountable> getChildResources() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public void close() {
+                }
+            };
+        }
+
+        @Override
+        public AtomicNumericFieldData loadDirect(LeafReaderContext context) throws Exception {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public XFieldComparatorSource comparatorSource(@Nullable Object missingValue, MultiValueMode sortMode, XFieldComparatorSource.Nested nested) {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public void clear() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public void clear(IndexReader reader) {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public Index index() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+    }
+
+    private static final ScoreFunction RANDOM_SCORE_FUNCTION = new RandomScoreFunction(0, 0, new IndexFieldDataStub());
+    private static final ScoreFunction FIELD_VALUE_FACTOR_FUNCTION = new FieldValueFactorFunction("test", 1, FieldValueFactorFunction.Modifier.LN, new Double(1), null);
+    private static final ScoreFunction GAUSS_DECAY_FUNCTION = new DecayFunctionBuilder.NumericFieldDataScoreFunction(0, 1, 0.1, 0, GaussDecayFunctionBuilder.GAUSS_DECAY_FUNCTION, new IndexNumericFieldDataStub(), MultiValueMode.MAX);
+    private static final ScoreFunction EXP_DECAY_FUNCTION = new DecayFunctionBuilder.NumericFieldDataScoreFunction(0, 1, 0.1, 0, ExponentialDecayFunctionBuilder.EXP_DECAY_FUNCTION, new IndexNumericFieldDataStub(), MultiValueMode.MAX);
+    private static final ScoreFunction LIN_DECAY_FUNCTION = new DecayFunctionBuilder.NumericFieldDataScoreFunction(0, 1, 0.1, 0, LinearDecayFunctionBuilder.LINEAR_DECAY_FUNCTION, new IndexNumericFieldDataStub(), MultiValueMode.MAX);
+    private static final ScoreFunction WEIGHT_FACTOR_FUNCTION = new WeightFactorFunction(4);
+    private static final String TEXT = "The way out is through.";
+    private static final String FIELD = "test";
+    private static final Term TERM = new Term(FIELD, "through");
+    private Directory dir;
+    private IndexWriter w;
+    private DirectoryReader reader;
+    private IndexSearcher searcher;
+
+    @Before
+    public void initSearcher() throws IOException {
+        dir = newDirectory();
+        w = new IndexWriter(dir, newIndexWriterConfig(new StandardAnalyzer()));
+        Document d = new Document();
+        d.add(new TextField(FIELD, TEXT, Field.Store.YES));
+        d.add(new TextField("_uid", "1", Field.Store.YES));
+        w.addDocument(d);
+        w.commit();
+        reader = DirectoryReader.open(w, true);
+        searcher = newSearcher(reader);
+    }
+
+    @After
+    public void closeAllTheReaders() throws IOException {
+        reader.close();
+        w.close();
+        dir.close();
+    }
+
+    @Test
+    public void testExplainFunctionScoreQuery() throws IOException {
+
+        Explanation functionExplanation = getFunctionScoreExplanation(searcher, RANDOM_SCORE_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "random score function (seed: 0)");
+        assertThat(functionExplanation.getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFunctionScoreExplanation(searcher, FIELD_VALUE_FACTOR_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "field value function: ln(doc['test'].value?:1.0 * factor=1.0)");
+        assertThat(functionExplanation.getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFunctionScoreExplanation(searcher, GAUSS_DECAY_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "Function for field test:");
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].toString(), equalTo("0.1 = exp(-0.5*pow(MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)],2.0)/0.21714724095162594)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFunctionScoreExplanation(searcher, EXP_DECAY_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "Function for field test:");
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].toString(), equalTo("0.1 = exp(- MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)] * 2.3025850929940455)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFunctionScoreExplanation(searcher, LIN_DECAY_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "Function for field test:");
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].toString(), equalTo("0.1 = max(0.0, ((1.1111111111111112 - MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)])/1.1111111111111112)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFunctionScoreExplanation(searcher, WEIGHT_FACTOR_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "product of:");
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].toString(), equalTo("1.0 = constant score 1.0 - no function provided\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[1].toString(), equalTo("4.0 = weight\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails().length, equalTo(0));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[1].getDetails().length, equalTo(0));
+    }
+
+    public Explanation getFunctionScoreExplanation(IndexSearcher searcher, ScoreFunction scoreFunction) throws IOException {
+        FunctionScoreQuery functionScoreQuery = new FunctionScoreQuery(new TermQuery(TERM), scoreFunction, 0.0f, CombineFunction.AVG, 100);
+        Weight weight = searcher.createNormalizedWeight(functionScoreQuery, true);
+        Explanation explanation = weight.explain(searcher.getIndexReader().leaves().get(0), 0);
+        return explanation.getDetails()[1];
+    }
+
+    public void checkFunctionScoreExplanation(Explanation randomExplanation, String functionExpl) {
+        assertThat(randomExplanation.getDescription(), equalTo("min of:"));
+        assertThat(randomExplanation.getDetails()[0].getDescription(), equalTo(functionExpl));
+    }
+
+    @Test
+    public void testExplainFiltersFunctionScoreQuery() throws IOException {
+        Explanation functionExplanation = getFiltersFunctionScoreExplanation(searcher, RANDOM_SCORE_FUNCTION);
+        checkFiltersFunctionScoreExplanation(functionExplanation, "random score function (seed: 0)", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails().length, equalTo(0));
+
+        functionExplanation = getFiltersFunctionScoreExplanation(searcher, FIELD_VALUE_FACTOR_FUNCTION);
+        checkFiltersFunctionScoreExplanation(functionExplanation, "field value function: ln(doc['test'].value?:1.0 * factor=1.0)", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails().length, equalTo(0));
+
+        functionExplanation = getFiltersFunctionScoreExplanation(searcher, GAUSS_DECAY_FUNCTION);
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = exp(-0.5*pow(MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)],2.0)/0.21714724095162594)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFiltersFunctionScoreExplanation(searcher, EXP_DECAY_FUNCTION);
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = exp(- MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)] * 2.3025850929940455)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFiltersFunctionScoreExplanation(searcher, LIN_DECAY_FUNCTION);
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = max(0.0, ((1.1111111111111112 - MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)])/1.1111111111111112)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+
+        // now test all together
+        functionExplanation = getFiltersFunctionScoreExplanation(searcher
+                , RANDOM_SCORE_FUNCTION
+                , FIELD_VALUE_FACTOR_FUNCTION
+                , GAUSS_DECAY_FUNCTION
+                , EXP_DECAY_FUNCTION
+                , LIN_DECAY_FUNCTION
+        );
+
+        checkFiltersFunctionScoreExplanation(functionExplanation, "random score function (seed: 0)", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails().length, equalTo(0));
+
+        checkFiltersFunctionScoreExplanation(functionExplanation, "field value function: ln(doc['test'].value?:1.0 * factor=1.0)", 1);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[1].getDetails()[1].getDetails().length, equalTo(0));
+
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 2);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[2].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = exp(-0.5*pow(MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)],2.0)/0.21714724095162594)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[2].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 3);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[3].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = exp(- MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)] * 2.3025850929940455)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[3].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 4);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[4].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = max(0.0, ((1.1111111111111112 - MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)])/1.1111111111111112)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[4].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+    }
+
+    public Explanation getFiltersFunctionScoreExplanation(IndexSearcher searcher, ScoreFunction... scoreFunctions) throws IOException {
+        FiltersFunctionScoreQuery filtersFunctionScoreQuery = getFiltersFunctionScoreQuery(FiltersFunctionScoreQuery.ScoreMode.AVG, CombineFunction.AVG, scoreFunctions);
+        Weight weight = searcher.createNormalizedWeight(filtersFunctionScoreQuery, true);
+        Explanation explanation = weight.explain(searcher.getIndexReader().leaves().get(0), 0);
+        return explanation.getDetails()[1];
+    }
+
+    public FiltersFunctionScoreQuery getFiltersFunctionScoreQuery(FiltersFunctionScoreQuery.ScoreMode scoreMode, CombineFunction combineFunction, ScoreFunction... scoreFunctions) {
+        FiltersFunctionScoreQuery.FilterFunction[] filterFunctions = new FiltersFunctionScoreQuery.FilterFunction[scoreFunctions.length];
+        for (int i = 0; i < scoreFunctions.length; i++) {
+            filterFunctions[i] = new FiltersFunctionScoreQuery.FilterFunction(
+                    new TermQuery(TERM), scoreFunctions[i]);
+        }
+        return new FiltersFunctionScoreQuery(new TermQuery(TERM), scoreMode, filterFunctions, Float.MAX_VALUE, Float.MAX_VALUE * -1, combineFunction);
+    }
+
+    public void checkFiltersFunctionScoreExplanation(Explanation randomExplanation, String functionExpl, int whichFunction) {
+        assertThat(randomExplanation.getDescription(), equalTo("min of:"));
+        assertThat(randomExplanation.getDetails()[0].getDescription(), equalTo("function score, score mode [avg]"));
+        assertThat(randomExplanation.getDetails()[0].getDetails()[whichFunction].getDescription(), equalTo("function score, product of:"));
+        assertThat(randomExplanation.getDetails()[0].getDetails()[whichFunction].getDetails()[0].getDescription(), equalTo("match filter: " + FIELD + ":" + TERM.text()));
+        assertThat(randomExplanation.getDetails()[0].getDetails()[whichFunction].getDetails()[1].getDescription(), equalTo(functionExpl));
+    }
+
+    private static float[] randomFloats(int size) {
+        float[] weights = new float[size];
+        for (int i = 0; i < weights.length; i++) {
+            weights[i] = randomFloat() * (randomBoolean() ? 1.0f : -1.0f) * randomInt(100) + 1.e-5f;
+        }
+        return weights;
+    }
+
+    private static class ScoreFunctionStub extends ScoreFunction {
+        private float score;
+
+        ScoreFunctionStub(float score) {
+            super(CombineFunction.REPLACE);
+            this.score = score;
+        }
+
+        @Override
+        public LeafScoreFunction getLeafScoreFunction(LeafReaderContext ctx) throws IOException {
+            return new LeafScoreFunction() {
+                @Override
+                public double score(int docId, float subQueryScore) {
+                    return score;
+                }
+
+                @Override
+                public Explanation explainScore(int docId, Explanation subQueryScore) throws IOException {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+            };
+        }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
+
+        @Override
+        protected boolean doEquals(ScoreFunction other) {
+            return false;
+        }
+    }
+
+    @Test
+    public void simpleWeightedFunctionsTest() throws IOException, ExecutionException, InterruptedException {
+        int numFunctions = randomIntBetween(1, 3);
+        float[] weights = randomFloats(numFunctions);
+        float[] scores = randomFloats(numFunctions);
+        ScoreFunctionStub[] scoreFunctionStubs = new ScoreFunctionStub[numFunctions];
+        for (int i = 0; i < numFunctions; i++) {
+            scoreFunctionStubs[i] = new ScoreFunctionStub(scores[i]);
+        }
+        WeightFactorFunction[] weightFunctionStubs = new WeightFactorFunction[numFunctions];
+        for (int i = 0; i < numFunctions; i++) {
+            weightFunctionStubs[i] = new WeightFactorFunction(weights[i], scoreFunctionStubs[i]);
+        }
+
+        FiltersFunctionScoreQuery filtersFunctionScoreQueryWithWeights = getFiltersFunctionScoreQuery(
+                FiltersFunctionScoreQuery.ScoreMode.MULTIPLY
+                , CombineFunction.REPLACE
+                , weightFunctionStubs
+        );
+
+        TopDocs topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        float scoreWithWeight = topDocsWithWeights.scoreDocs[0].score;
+        double score = 1;
+        for (int i = 0; i < weights.length; i++) {
+            score *= weights[i] * scores[i];
+        }
+        assertThat(scoreWithWeight / score, closeTo(1, 1.e-5d));
+
+        filtersFunctionScoreQueryWithWeights = getFiltersFunctionScoreQuery(
+                FiltersFunctionScoreQuery.ScoreMode.SUM
+                , CombineFunction.REPLACE
+                , weightFunctionStubs
+        );
+
+        topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        scoreWithWeight = topDocsWithWeights.scoreDocs[0].score;
+        double sum = 0;
+        for (int i = 0; i < weights.length; i++) {
+            sum += weights[i] * scores[i];
+        }
+        assertThat(scoreWithWeight / sum, closeTo(1, 1.e-5d));
+
+        filtersFunctionScoreQueryWithWeights = getFiltersFunctionScoreQuery(
+                FiltersFunctionScoreQuery.ScoreMode.AVG
+                , CombineFunction.REPLACE
+                , weightFunctionStubs
+        );
+
+        topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        scoreWithWeight = topDocsWithWeights.scoreDocs[0].score;
+        double norm = 0;
+        sum = 0;
+        for (int i = 0; i < weights.length; i++) {
+            norm += weights[i];
+            sum += weights[i] * scores[i];
+        }
+        assertThat(scoreWithWeight * norm / sum, closeTo(1, 1.e-5d));
+
+        filtersFunctionScoreQueryWithWeights = getFiltersFunctionScoreQuery(
+                FiltersFunctionScoreQuery.ScoreMode.MIN
+                , CombineFunction.REPLACE
+                , weightFunctionStubs
+        );
+
+        topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        scoreWithWeight = topDocsWithWeights.scoreDocs[0].score;
+        double min = Double.POSITIVE_INFINITY;
+        for (int i = 0; i < weights.length; i++) {
+            min = Math.min(min, weights[i] * scores[i]);
+        }
+        assertThat(scoreWithWeight / min, closeTo(1, 1.e-5d));
+
+        filtersFunctionScoreQueryWithWeights = getFiltersFunctionScoreQuery(
+                FiltersFunctionScoreQuery.ScoreMode.MAX
+                , CombineFunction.REPLACE
+                , weightFunctionStubs
+        );
+
+        topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        scoreWithWeight = topDocsWithWeights.scoreDocs[0].score;
+        double max = Double.NEGATIVE_INFINITY;
+        for (int i = 0; i < weights.length; i++) {
+            max = Math.max(max, weights[i] * scores[i]);
+        }
+        assertThat(scoreWithWeight / max, closeTo(1, 1.e-5d));
+    }
+
+    @Test
+    public void checkWeightOnlyCreatesBoostFunction() throws IOException {
+        FunctionScoreQuery filtersFunctionScoreQueryWithWeights = new FunctionScoreQuery(new MatchAllDocsQuery(), new WeightFactorFunction(2), 0.0f, CombineFunction.MULTIPLY, 100);
+        TopDocs topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        float score = topDocsWithWeights.scoreDocs[0].score;
+        assertThat(score, equalTo(2.0f));
+    }
+}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
index c1bdd9d..7d4b10a 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
@@ -41,6 +41,7 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.*;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
@@ -69,7 +70,6 @@ import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.Uid;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.index.settings.IndexSettingsService;
 import org.elasticsearch.index.snapshots.IndexShardRepository;
 import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;
@@ -95,10 +95,7 @@ import java.util.concurrent.CyclicBarrier;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.atomic.AtomicBoolean;
 
-import static org.elasticsearch.cluster.metadata.IndexMetaData.EMPTY_PARAMS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_VERSION_CREATED;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.*;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
@@ -342,7 +339,8 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         client().prepareIndex("test", "test").setSource("{}").get();
         ensureGreen("test");
         IndicesService indicesService = getInstanceFromNode(IndicesService.class);
-        indicesService.indexService("test").getShardOrNull(0).markAsInactive();
+        Boolean result = indicesService.indexService("test").getShardOrNull(0).checkIdle(0);
+        assertEquals(Boolean.TRUE, result);
         assertBusy(new Runnable() { // should be very very quick
             @Override
             public void run() {
@@ -401,35 +399,6 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         assertEquals(durabilty, shard.getTranslogDurability());
     }
 
-    public void testDeleteByQueryBWC() {
-        Version version = VersionUtils.randomVersion(random());
-        assertAcked(client().admin().indices().prepareCreate("test")
-                .setSettings(SETTING_NUMBER_OF_SHARDS, 1, SETTING_NUMBER_OF_REPLICAS, 0, IndexMetaData.SETTING_VERSION_CREATED, version.id));
-        ensureGreen("test");
-        client().prepareIndex("test", "person").setSource("{ \"user\" : \"kimchy\" }").get();
-
-        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
-        IndexService test = indicesService.indexService("test");
-        IndexShard shard = test.getShardOrNull(0);
-        int numDocs = 1;
-        shard.state = IndexShardState.RECOVERING;
-        try {
-            shard.recoveryState().getTranslog().totalOperations(1);
-            shard.getEngine().config().getTranslogRecoveryPerformer().performRecoveryOperation(shard.getEngine(), new Translog.DeleteByQuery(new Engine.DeleteByQuery(null, new BytesArray("{\"term\" : { \"user\" : \"kimchy\" }}"), null, null, null, Engine.Operation.Origin.RECOVERY, 0, "person")), false);
-            assertTrue(version.onOrBefore(Version.V_1_0_0_Beta2));
-            numDocs = 0;
-        } catch (ParsingException ex) {
-            assertTrue(version.after(Version.V_1_0_0_Beta2));
-        } finally {
-            shard.state = IndexShardState.STARTED;
-        }
-        shard.getEngine().refresh("foo");
-
-        try (Engine.Searcher searcher = shard.getEngine().acquireSearcher("foo")) {
-            assertEquals(numDocs, searcher.reader().numDocs());
-        }
-    }
-
     public void testMinimumCompatVersion() {
         Version versionCreated = VersionUtils.randomVersion(random());
         assertAcked(client().admin().indices().prepareCreate("test")
@@ -628,9 +597,9 @@ public class IndexShardTests extends ESSingleNodeTestCase {
 
         shardIndexingService.addListener(new IndexingOperationListener() {
             @Override
-            public Engine.Index preIndex(Engine.Index index) {
+            public Engine.Index preIndex(Engine.Index operation) {
                 preIndexCalled.set(true);
-                return super.preIndex(index);
+                return super.preIndex(operation);
             }
         });
 
@@ -957,7 +926,7 @@ public class IndexShardTests extends ESSingleNodeTestCase {
             }
         };
 
-        IndexServicesProvider newProvider = new IndexServicesProvider(indexServices.getIndicesLifecycle(), indexServices.getThreadPool(), indexServices.getMapperService(), indexServices.getQueryParserService(), indexServices.getIndexCache(), indexServices.getIndexAliasesService(), indexServices.getIndicesQueryCache(), indexServices.getCodecService(), indexServices.getTermVectorsService(), indexServices.getIndexFieldDataService(), indexServices.getWarmer(), indexServices.getSimilarityService(), indexServices.getFactory(), indexServices.getBigArrays(), wrapper);
+        IndexServicesProvider newProvider = new IndexServicesProvider(indexServices.getIndicesLifecycle(), indexServices.getThreadPool(), indexServices.getMapperService(), indexServices.getQueryParserService(), indexServices.getIndexCache(), indexServices.getIndexAliasesService(), indexServices.getIndicesQueryCache(), indexServices.getCodecService(), indexServices.getTermVectorsService(), indexServices.getIndexFieldDataService(), indexServices.getWarmer(), indexServices.getSimilarityService(), indexServices.getFactory(), indexServices.getBigArrays(), wrapper, indexServices.getIndexingMemoryController());
         IndexShard newShard = new IndexShard(shard.shardId(), shard.indexSettings, shard.shardPath(), shard.store(), newProvider);
 
         ShardRoutingHelper.reinit(routing);
diff --git a/core/src/test/java/org/elasticsearch/index/similarity/SimilarityModuleTests.java b/core/src/test/java/org/elasticsearch/index/similarity/SimilarityModuleTests.java
new file mode 100644
index 0000000..a73d2a5
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/similarity/SimilarityModuleTests.java
@@ -0,0 +1,117 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.similarity;
+
+import org.apache.lucene.index.FieldInvertState;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.search.CollectionStatistics;
+import org.apache.lucene.search.TermStatistics;
+import org.apache.lucene.search.similarities.BM25Similarity;
+import org.apache.lucene.search.similarities.Similarity;
+import org.elasticsearch.common.inject.ModuleTestCase;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.Index;
+
+import java.io.IOException;
+
+public class SimilarityModuleTests extends ModuleTestCase {
+
+    public void testAddSimilarity() {
+        Settings indexSettings = Settings.settingsBuilder()
+                .put("index.similarity.my_similarity.type", "test_similarity")
+                .put("index.similarity.my_similarity.key", "there is a key")
+                .build();
+        SimilarityModule module = new SimilarityModule(new Index("foo"), indexSettings);
+        module.addSimilarity("test_similarity", (string, settings) -> new SimilarityProvider() {
+            @Override
+            public String name() {
+                return string;
+            }
+
+            @Override
+            public Similarity get() {
+                return new TestSimilarity(settings.get("key"));
+            }
+        });
+        assertInstanceBinding(module, SimilarityService.class, (inst) -> {
+            if (inst instanceof SimilarityService) {
+                assertNotNull(inst.getSimilarity("my_similarity"));
+                assertTrue(inst.getSimilarity("my_similarity").get() instanceof TestSimilarity);
+                assertEquals("my_similarity", inst.getSimilarity("my_similarity").name());
+                assertEquals("there is a key" , ((TestSimilarity)inst.getSimilarity("my_similarity").get()).key);
+                return true;
+            }
+            return false;
+        });
+    }
+
+    public void testSetupUnknownSimilarity() {
+        Settings indexSettings = Settings.settingsBuilder()
+                .put("index.similarity.my_similarity.type", "test_similarity")
+                .build();
+        SimilarityModule module = new SimilarityModule(new Index("foo"), indexSettings);
+        try {
+            assertInstanceBinding(module, SimilarityService.class, (inst) -> inst instanceof SimilarityService);
+        } catch (IllegalArgumentException ex) {
+            assertEquals("Unknown Similarity type [test_similarity] for [my_similarity]", ex.getMessage());
+        }
+    }
+
+
+    public void testSetupWithoutType() {
+        Settings indexSettings = Settings.settingsBuilder()
+                .put("index.similarity.my_similarity.foo", "bar")
+                .build();
+        SimilarityModule module = new SimilarityModule(new Index("foo"), indexSettings);
+        try {
+            assertInstanceBinding(module, SimilarityService.class, (inst) -> inst instanceof SimilarityService);
+        } catch (IllegalArgumentException ex) {
+            assertEquals("Similarity [my_similarity] must have an associated type", ex.getMessage());
+        }
+    }
+
+
+    private static class TestSimilarity extends Similarity {
+        private final Similarity delegate = new BM25Similarity();
+        private final String key;
+
+
+        public TestSimilarity(String key) {
+            if (key == null) {
+                throw new AssertionError("key is null");
+            }
+            this.key = key;
+        }
+
+        @Override
+        public long computeNorm(FieldInvertState state) {
+            return delegate.computeNorm(state);
+        }
+
+        @Override
+        public SimWeight computeWeight(CollectionStatistics collectionStats, TermStatistics... termStats) {
+            return delegate.computeWeight(collectionStats, termStats);
+        }
+
+        @Override
+        public SimScorer simScorer(SimWeight weight, LeafReaderContext context) throws IOException {
+            return delegate.simScorer(weight, context);
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/similarity/SimilarityTests.java b/core/src/test/java/org/elasticsearch/index/similarity/SimilarityTests.java
index 6a42ba7..28f5e5c 100644
--- a/core/src/test/java/org/elasticsearch/index/similarity/SimilarityTests.java
+++ b/core/src/test/java/org/elasticsearch/index/similarity/SimilarityTests.java
@@ -22,6 +22,7 @@ package org.elasticsearch.index.similarity;
 import org.apache.lucene.search.similarities.*;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 import org.junit.Test;
@@ -35,11 +36,9 @@ public class SimilarityTests extends ESSingleNodeTestCase {
 
     @Test
     public void testResolveDefaultSimilarities() {
-        SimilarityLookupService similarityLookupService = createIndex("foo").similarityService().similarityLookupService();
-        assertThat(similarityLookupService.similarity("default"), instanceOf(PreBuiltSimilarityProvider.class));
-        assertThat(similarityLookupService.similarity("default").get(), instanceOf(DefaultSimilarity.class));
-        assertThat(similarityLookupService.similarity("BM25"), instanceOf(PreBuiltSimilarityProvider.class));
-        assertThat(similarityLookupService.similarity("BM25").get(), instanceOf(BM25Similarity.class));
+        SimilarityService similarityService = createIndex("foo").similarityService();
+        assertThat(similarityService.getSimilarity("default").get(), instanceOf(DefaultSimilarity.class));
+        assertThat(similarityService.getSimilarity("BM25").get(), instanceOf(BM25Similarity.class));
     }
 
     @Test
@@ -54,8 +53,8 @@ public class SimilarityTests extends ESSingleNodeTestCase {
                 .put("index.similarity.my_similarity.type", "default")
                 .put("index.similarity.my_similarity.discount_overlaps", false)
                 .build();
-        SimilarityService similarityService = createIndex("foo", indexSettings).similarityService();
-        DocumentMapper documentMapper = similarityService.mapperService().documentMapperParser().parse(mapping);
+        IndexService indexService = createIndex("foo", indexSettings);
+        DocumentMapper documentMapper = indexService.mapperService().documentMapperParser().parse(mapping);
         assertThat(documentMapper.mappers().getMapper("field1").fieldType().similarity(), instanceOf(DefaultSimilarityProvider.class));
 
         DefaultSimilarity similarity = (DefaultSimilarity) documentMapper.mappers().getMapper("field1").fieldType().similarity().get();
@@ -76,8 +75,8 @@ public class SimilarityTests extends ESSingleNodeTestCase {
                 .put("index.similarity.my_similarity.b", 1.5f)
                 .put("index.similarity.my_similarity.discount_overlaps", false)
                 .build();
-        SimilarityService similarityService = createIndex("foo", indexSettings).similarityService();
-        DocumentMapper documentMapper = similarityService.mapperService().documentMapperParser().parse(mapping);
+        IndexService indexService = createIndex("foo", indexSettings);
+        DocumentMapper documentMapper = indexService.mapperService().documentMapperParser().parse(mapping);
         assertThat(documentMapper.mappers().getMapper("field1").fieldType().similarity(), instanceOf(BM25SimilarityProvider.class));
 
         BM25Similarity similarity = (BM25Similarity) documentMapper.mappers().getMapper("field1").fieldType().similarity().get();
@@ -101,8 +100,8 @@ public class SimilarityTests extends ESSingleNodeTestCase {
                 .put("index.similarity.my_similarity.normalization", "h2")
                 .put("index.similarity.my_similarity.normalization.h2.c", 3f)
                 .build();
-        SimilarityService similarityService = createIndex("foo", indexSettings).similarityService();
-        DocumentMapper documentMapper = similarityService.mapperService().documentMapperParser().parse(mapping);
+        IndexService indexService = createIndex("foo", indexSettings);
+        DocumentMapper documentMapper = indexService.mapperService().documentMapperParser().parse(mapping);
         assertThat(documentMapper.mappers().getMapper("field1").fieldType().similarity(), instanceOf(DFRSimilarityProvider.class));
 
         DFRSimilarity similarity = (DFRSimilarity) documentMapper.mappers().getMapper("field1").fieldType().similarity().get();
@@ -127,8 +126,8 @@ public class SimilarityTests extends ESSingleNodeTestCase {
                 .put("index.similarity.my_similarity.normalization", "h2")
                 .put("index.similarity.my_similarity.normalization.h2.c", 3f)
                 .build();
-        SimilarityService similarityService = createIndex("foo", indexSettings).similarityService();
-        DocumentMapper documentMapper = similarityService.mapperService().documentMapperParser().parse(mapping);
+        IndexService indexService = createIndex("foo", indexSettings);
+        DocumentMapper documentMapper = indexService.mapperService().documentMapperParser().parse(mapping);
         assertThat(documentMapper.mappers().getMapper("field1").fieldType().similarity(), instanceOf(IBSimilarityProvider.class));
 
         IBSimilarity similarity = (IBSimilarity) documentMapper.mappers().getMapper("field1").fieldType().similarity().get();
@@ -150,8 +149,8 @@ public class SimilarityTests extends ESSingleNodeTestCase {
                 .put("index.similarity.my_similarity.type", "LMDirichlet")
                 .put("index.similarity.my_similarity.mu", 3000f)
                 .build();
-        SimilarityService similarityService = createIndex("foo", indexSettings).similarityService();
-        DocumentMapper documentMapper = similarityService.mapperService().documentMapperParser().parse(mapping);
+        IndexService indexService = createIndex("foo", indexSettings);
+        DocumentMapper documentMapper = indexService.mapperService().documentMapperParser().parse(mapping);
         assertThat(documentMapper.mappers().getMapper("field1").fieldType().similarity(), instanceOf(LMDirichletSimilarityProvider.class));
 
         LMDirichletSimilarity similarity = (LMDirichletSimilarity) documentMapper.mappers().getMapper("field1").fieldType().similarity().get();
@@ -170,8 +169,8 @@ public class SimilarityTests extends ESSingleNodeTestCase {
                 .put("index.similarity.my_similarity.type", "LMJelinekMercer")
                 .put("index.similarity.my_similarity.lambda", 0.7f)
                 .build();
-        SimilarityService similarityService = createIndex("foo", indexSettings).similarityService();
-        DocumentMapper documentMapper = similarityService.mapperService().documentMapperParser().parse(mapping);
+        IndexService indexService = createIndex("foo", indexSettings);
+        DocumentMapper documentMapper = indexService.mapperService().documentMapperParser().parse(mapping);
         assertThat(documentMapper.mappers().getMapper("field1").fieldType().similarity(), instanceOf(LMJelinekMercerSimilarityProvider.class));
 
         LMJelinekMercerSimilarity similarity = (LMJelinekMercerSimilarity) documentMapper.mappers().getMapper("field1").fieldType().similarity().get();
diff --git a/core/src/test/java/org/elasticsearch/index/store/IndexStoreBWCTests.java b/core/src/test/java/org/elasticsearch/index/store/IndexStoreBWCTests.java
deleted file mode 100644
index e53358c..0000000
--- a/core/src/test/java/org/elasticsearch/index/store/IndexStoreBWCTests.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.store;
-
-import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-import org.apache.lucene.store.*;
-import org.apache.lucene.util.Constants;
-import org.elasticsearch.Version;
-import org.elasticsearch.action.admin.indices.settings.get.GetSettingsResponse;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.shard.ShardPath;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-import java.nio.file.Path;
-import java.util.Arrays;
-import java.util.Locale;
-
-/**
- */
-public class IndexStoreBWCTests extends ESSingleNodeTestCase {
-
-
-    public void testOldCoreTypesFail() {
-        try {
-            createIndex("test", Settings.builder().put(IndexStoreModule.STORE_TYPE, "nio_fs").build());
-            fail();
-        } catch (Exception ex) {
-        }
-        try {
-            createIndex("test", Settings.builder().put(IndexStoreModule.STORE_TYPE, "mmap_fs").build());
-            fail();
-        } catch (Exception ex) {
-        }
-        try {
-            createIndex("test", Settings.builder().put(IndexStoreModule.STORE_TYPE, "simple_fs").build());
-            fail();
-        } catch (Exception ex) {
-        }
-    }
-
-    public void testUpgradeCoreTypes() throws IOException {
-        String type = RandomPicks.randomFrom(random(), Arrays.asList("nio", "mmap", "simple"));
-        createIndex("test", Settings.builder()
-                .put(IndexStoreModule.STORE_TYPE, type+"fs")
-                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_7_0)
-                .build());
-
-        client().admin().indices().prepareClose("test").get();
-        client().admin().indices().prepareUpdateSettings("test").setSettings(Settings.builder()
-                .put(IndexStoreModule.STORE_TYPE, type + "_fs").build()).get();
-        GetSettingsResponse getSettingsResponse = client().admin().indices().prepareGetSettings("test").get();
-        String actualType = getSettingsResponse.getSetting("test", IndexStoreModule.STORE_TYPE);
-        assertEquals(type + "_fs", actualType);
-
-        // now reopen and upgrade
-        client().admin().indices().prepareOpen("test").get();
-
-        getSettingsResponse = client().admin().indices().prepareGetSettings("test").get();
-        actualType = getSettingsResponse.getSetting("test", IndexStoreModule.STORE_TYPE);
-        assertEquals(type+"fs", actualType);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/store/StoreTests.java b/core/src/test/java/org/elasticsearch/index/store/StoreTests.java
index 123e4e0..11d01c9 100644
--- a/core/src/test/java/org/elasticsearch/index/store/StoreTests.java
+++ b/core/src/test/java/org/elasticsearch/index/store/StoreTests.java
@@ -24,35 +24,9 @@ import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.lucene50.Lucene50SegmentInfoFormat;
 import org.apache.lucene.codecs.lucene53.Lucene53Codec;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.SortedDocValuesField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexFormatTooNewException;
-import org.apache.lucene.index.IndexFormatTooOldException;
-import org.apache.lucene.index.IndexNotFoundException;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy;
-import org.apache.lucene.index.NoDeletionPolicy;
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.index.SnapshotDeletionPolicy;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.BaseDirectoryWrapper;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.document.*;
+import org.apache.lucene.index.*;
+import org.apache.lucene.store.*;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.TestUtil;
@@ -81,30 +55,14 @@ import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.nio.file.NoSuchFileException;
 import java.nio.file.Path;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Date;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.Set;
+import java.util.*;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.zip.Adler32;
 
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.unmodifiableMap;
+import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
 import static org.elasticsearch.test.VersionUtils.randomVersion;
-import static org.hamcrest.Matchers.empty;
-import static org.hamcrest.Matchers.endsWith;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.*;
 
 public class StoreTests extends ESTestCase {
 
@@ -848,9 +806,9 @@ public class StoreTests extends ESTestCase {
         Map<String, StoreFileMetaData> metaDataMap = new HashMap<>();
         metaDataMap.put("segments_1", new StoreFileMetaData("segments_1", 50, null, null, new BytesRef(new byte[]{1})));
         metaDataMap.put("_0_1.del", new StoreFileMetaData("_0_1.del", 42, "foobarbaz", null, new BytesRef()));
-        Store.MetadataSnapshot first = new Store.MetadataSnapshot(unmodifiableMap(new HashMap<>(metaDataMap)), emptyMap(), 0);
+        Store.MetadataSnapshot first = new Store.MetadataSnapshot(metaDataMap, Collections.EMPTY_MAP, 0);
 
-        Store.MetadataSnapshot second = new Store.MetadataSnapshot(unmodifiableMap(new HashMap<>(metaDataMap)), emptyMap(), 0);
+        Store.MetadataSnapshot second = new Store.MetadataSnapshot(metaDataMap, Collections.EMPTY_MAP, 0);
         Store.RecoveryDiff recoveryDiff = first.recoveryDiff(second);
         assertEquals(recoveryDiff.toString(), recoveryDiff.different.size(), 2);
     }
@@ -1116,7 +1074,7 @@ public class StoreTests extends ESTestCase {
         Map<String, StoreFileMetaData> metaDataMap = new HashMap<>();
         metaDataMap.put("segments_1", new StoreFileMetaData("segments_1", 50, null, null, new BytesRef(new byte[]{1})));
         metaDataMap.put("_0_1.del", new StoreFileMetaData("_0_1.del", 42, "foobarbaz", null, new BytesRef()));
-        Store.MetadataSnapshot snapshot = new Store.MetadataSnapshot(unmodifiableMap(metaDataMap), emptyMap(), 0);
+        Store.MetadataSnapshot snapshot = new Store.MetadataSnapshot(metaDataMap, Collections.EMPTY_MAP, 0);
 
         final ShardId shardId = new ShardId(new Index("index"), 1);
         DirectoryService directoryService = new LuceneManagedDirectoryService(random());
@@ -1250,7 +1208,7 @@ public class StoreTests extends ESTestCase {
         Map<String, String> commitUserData = new HashMap<>();
         commitUserData.put("userdata_1", "test");
         commitUserData.put("userdata_2", "test");
-        return new Store.MetadataSnapshot(unmodifiableMap(storeFileMetaDataMap), unmodifiableMap(commitUserData), 0);
+        return new Store.MetadataSnapshot(storeFileMetaDataMap, commitUserData, 0);
     }
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java b/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
index 8764d1a..0b3e12d 100644
--- a/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
+++ b/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
@@ -34,6 +34,7 @@ import org.elasticsearch.common.io.FileSystemUtils;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.concurrent.AbstractRunnable;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
@@ -183,14 +184,14 @@ public class TranslogTests extends ESTestCase {
 
     @Test
     public void testRead() throws IOException {
-        Translog.Location loc1 = translog.add(new Translog.Create("test", "1", new byte[]{1}));
-        Translog.Location loc2 = translog.add(new Translog.Create("test", "2", new byte[]{2}));
+        Translog.Location loc1 = translog.add(new Translog.Index("test", "1", new byte[]{1}));
+        Translog.Location loc2 = translog.add(new Translog.Index("test", "2", new byte[]{2}));
         assertThat(translog.read(loc1).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{1})));
         assertThat(translog.read(loc2).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{2})));
         translog.sync();
         assertThat(translog.read(loc1).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{1})));
         assertThat(translog.read(loc2).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{2})));
-        Translog.Location loc3 = translog.add(new Translog.Create("test", "2", new byte[]{3}));
+        Translog.Location loc3 = translog.add(new Translog.Index("test", "2", new byte[]{3}));
         assertThat(translog.read(loc3).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{3})));
         translog.sync();
         assertThat(translog.read(loc3).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{3})));
@@ -215,19 +216,13 @@ public class TranslogTests extends ESTestCase {
         assertThat(snapshot, SnapshotMatchers.size(0));
         snapshot.close();
 
-        addToTranslogAndList(translog, ops, new Translog.Create("test", "1", new byte[]{1}));
-        snapshot = translog.newSnapshot();
-        assertThat(snapshot, SnapshotMatchers.equalsTo(ops));
-        assertThat(snapshot.estimatedTotalOperations(), equalTo(1));
-        snapshot.close();
-
-        addToTranslogAndList(translog, ops, new Translog.Index("test", "2", new byte[]{2}));
+        addToTranslogAndList(translog, ops, new Translog.Index("test", "1", new byte[]{1}));
         snapshot = translog.newSnapshot();
         assertThat(snapshot, SnapshotMatchers.equalsTo(ops));
         assertThat(snapshot.estimatedTotalOperations(), equalTo(ops.size()));
         snapshot.close();
 
-        addToTranslogAndList(translog, ops, new Translog.Delete(newUid("3")));
+        addToTranslogAndList(translog, ops, new Translog.Delete(newUid("2")));
         snapshot = translog.newSnapshot();
         assertThat(snapshot, SnapshotMatchers.equalsTo(ops));
         assertThat(snapshot.estimatedTotalOperations(), equalTo(ops.size()));
@@ -235,17 +230,13 @@ public class TranslogTests extends ESTestCase {
 
         snapshot = translog.newSnapshot();
 
-        Translog.Create create = (Translog.Create) snapshot.next();
-        assertThat(create != null, equalTo(true));
-        assertThat(create.source().toBytes(), equalTo(new byte[]{1}));
-
         Translog.Index index = (Translog.Index) snapshot.next();
         assertThat(index != null, equalTo(true));
-        assertThat(index.source().toBytes(), equalTo(new byte[]{2}));
+        assertThat(index.source().toBytes(), equalTo(new byte[]{1}));
 
         Translog.Delete delete = (Translog.Delete) snapshot.next();
         assertThat(delete != null, equalTo(true));
-        assertThat(delete.uid(), equalTo(newUid("3")));
+        assertThat(delete.uid(), equalTo(newUid("2")));
 
         assertThat(snapshot.next(), equalTo(null));
 
@@ -286,38 +277,63 @@ public class TranslogTests extends ESTestCase {
         final long firstOperationPosition = translog.getFirstOperationPosition();
         TranslogStats stats = stats();
         assertThat(stats.estimatedNumberOfOperations(), equalTo(0l));
-        long lastSize = stats.translogSizeInBytes().bytes();
+        long lastSize = stats.getTranslogSizeInBytes();
         assertThat((int) firstOperationPosition, greaterThan(CodecUtil.headerLength(TranslogWriter.TRANSLOG_CODEC)));
         assertThat(lastSize, equalTo(firstOperationPosition));
-
-        translog.add(new Translog.Create("test", "1", new byte[]{1}));
+        TranslogStats total = new TranslogStats();
+        translog.add(new Translog.Index("test", "1", new byte[]{1}));
         stats = stats();
+        total.add(stats);
         assertThat(stats.estimatedNumberOfOperations(), equalTo(1l));
-        assertThat(stats.translogSizeInBytes().bytes(), greaterThan(lastSize));
-        lastSize = stats.translogSizeInBytes().bytes();
+        assertThat(stats.getTranslogSizeInBytes(), greaterThan(lastSize));
+        lastSize = stats.getTranslogSizeInBytes();
 
-        translog.add(new Translog.Index("test", "2", new byte[]{2}));
+        translog.add(new Translog.Delete(newUid("2")));
         stats = stats();
+        total.add(stats);
         assertThat(stats.estimatedNumberOfOperations(), equalTo(2l));
-        assertThat(stats.translogSizeInBytes().bytes(), greaterThan(lastSize));
-        lastSize = stats.translogSizeInBytes().bytes();
+        assertThat(stats.getTranslogSizeInBytes(), greaterThan(lastSize));
+        lastSize = stats.getTranslogSizeInBytes();
 
         translog.add(new Translog.Delete(newUid("3")));
-        stats = stats();
-        assertThat(stats.estimatedNumberOfOperations(), equalTo(3l));
-        assertThat(stats.translogSizeInBytes().bytes(), greaterThan(lastSize));
-        lastSize = stats.translogSizeInBytes().bytes();
-
-        translog.add(new Translog.Delete(newUid("4")));
         translog.prepareCommit();
         stats = stats();
-        assertThat(stats.estimatedNumberOfOperations(), equalTo(4l));
-        assertThat(stats.translogSizeInBytes().bytes(), greaterThan(lastSize));
+        total.add(stats);
+        assertThat(stats.estimatedNumberOfOperations(), equalTo(3l));
+        assertThat(stats.getTranslogSizeInBytes(), greaterThan(lastSize));
 
         translog.commit();
         stats = stats();
+        total.add(stats);
         assertThat(stats.estimatedNumberOfOperations(), equalTo(0l));
-        assertThat(stats.translogSizeInBytes().bytes(), equalTo(firstOperationPosition));
+        assertThat(stats.getTranslogSizeInBytes(), equalTo(firstOperationPosition));
+        assertEquals(6, total.estimatedNumberOfOperations());
+        assertEquals(431, total.getTranslogSizeInBytes());
+
+        BytesStreamOutput out = new BytesStreamOutput();
+        total.writeTo(out);
+        TranslogStats copy = new TranslogStats();
+        copy.readFrom(StreamInput.wrap(out.bytes()));
+
+        assertEquals(6, copy.estimatedNumberOfOperations());
+        assertEquals(431, copy.getTranslogSizeInBytes());
+        assertEquals("\"translog\"{\n" +
+                "  \"operations\" : 6,\n" +
+                "  \"size_in_bytes\" : 431\n" +
+                "}", copy.toString().trim());
+
+        try {
+            new TranslogStats(1, -1);
+            fail("must be positive");
+        } catch (IllegalArgumentException ex) {
+            //all well
+        }
+        try {
+            new TranslogStats(-1, 1);
+            fail("must be positive");
+        } catch (IllegalArgumentException ex) {
+            //all well
+        }
     }
 
     @Test
@@ -327,7 +343,7 @@ public class TranslogTests extends ESTestCase {
         assertThat(snapshot, SnapshotMatchers.size(0));
         snapshot.close();
 
-        addToTranslogAndList(translog, ops, new Translog.Create("test", "1", new byte[]{1}));
+        addToTranslogAndList(translog, ops, new Translog.Index("test", "1", new byte[]{1}));
 
         snapshot = translog.newSnapshot();
         assertThat(snapshot, SnapshotMatchers.equalsTo(ops));
@@ -354,7 +370,7 @@ public class TranslogTests extends ESTestCase {
         assertThat(snapshot, SnapshotMatchers.size(0));
         snapshot.close();
 
-        addToTranslogAndList(translog, ops, new Translog.Create("test", "1", new byte[]{1}));
+        addToTranslogAndList(translog, ops, new Translog.Index("test", "1", new byte[]{1}));
         Translog.Snapshot snapshot1 = translog.newSnapshot();
 
         addToTranslogAndList(translog, ops, new Translog.Index("test", "2", new byte[]{2}));
@@ -375,7 +391,7 @@ public class TranslogTests extends ESTestCase {
 
     public void testSnapshotOnClosedTranslog() throws IOException {
         assertTrue(Files.exists(translogDir.resolve(Translog.getFilename(1))));
-        translog.add(new Translog.Create("test", "1", new byte[]{1}));
+        translog.add(new Translog.Index("test", "1", new byte[]{1}));
         translog.close();
         try {
             Translog.Snapshot snapshot = translog.newSnapshot();
@@ -388,7 +404,7 @@ public class TranslogTests extends ESTestCase {
     @Test
     public void deleteOnSnapshotRelease() throws Exception {
         ArrayList<Translog.Operation> firstOps = new ArrayList<>();
-        addToTranslogAndList(translog, firstOps, new Translog.Create("test", "1", new byte[]{1}));
+        addToTranslogAndList(translog, firstOps, new Translog.Index("test", "1", new byte[]{1}));
 
         Translog.Snapshot firstSnapshot = translog.newSnapshot();
         assertThat(firstSnapshot.estimatedTotalOperations(), equalTo(1));
@@ -463,10 +479,7 @@ public class TranslogTests extends ESTestCase {
                             Translog.Operation op;
                             switch (randomFrom(Translog.Operation.Type.values())) {
                                 case CREATE:
-                                    op = new Translog.Create("test", threadId + "_" + opCount,
-                                            randomUnicodeOfLengthBetween(1, 20 * 1024).getBytes("UTF-8"));
-                                    break;
-                                case SAVE:
+                                case INDEX:
                                     op = new Translog.Index("test", threadId + "_" + opCount,
                                             randomUnicodeOfLengthBetween(1, 20 * 1024).getBytes("UTF-8"));
                                     break;
@@ -475,9 +488,6 @@ public class TranslogTests extends ESTestCase {
                                             1 + randomInt(100000),
                                             randomFrom(VersionType.values()));
                                     break;
-                                case DELETE_BY_QUERY:
-                                    // deprecated
-                                    continue;
                                 default:
                                     throw new ElasticsearchException("not supported op type");
                             }
@@ -508,7 +518,7 @@ public class TranslogTests extends ESTestCase {
             Translog.Operation expectedOp = locationOperation.operation;
             assertEquals(expectedOp.opType(), op.opType());
             switch (op.opType()) {
-                case SAVE:
+                case INDEX:
                     Translog.Index indexOp = (Translog.Index) op;
                     Translog.Index expIndexOp = (Translog.Index) expectedOp;
                     assertEquals(expIndexOp.id(), indexOp.id());
@@ -518,16 +528,6 @@ public class TranslogTests extends ESTestCase {
                     assertEquals(expIndexOp.version(), indexOp.version());
                     assertEquals(expIndexOp.versionType(), indexOp.versionType());
                     break;
-                case CREATE:
-                    Translog.Create createOp = (Translog.Create) op;
-                    Translog.Create expCreateOp = (Translog.Create) expectedOp;
-                    assertEquals(expCreateOp.id(), createOp.id());
-                    assertEquals(expCreateOp.routing(), createOp.routing());
-                    assertEquals(expCreateOp.type(), createOp.type());
-                    assertEquals(expCreateOp.source(), createOp.source());
-                    assertEquals(expCreateOp.version(), createOp.version());
-                    assertEquals(expCreateOp.versionType(), createOp.versionType());
-                    break;
                 case DELETE:
                     Translog.Delete delOp = (Translog.Delete) op;
                     Translog.Delete expDelOp = (Translog.Delete) expectedOp;
@@ -550,7 +550,7 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         for (int op = 0; op < translogOperations; op++) {
             String ascii = randomAsciiOfLengthBetween(1, 50);
-            locations.add(translog.add(new Translog.Create("test", "" + op, ascii.getBytes("UTF-8"))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, ascii.getBytes("UTF-8"))));
         }
         translog.sync();
 
@@ -574,7 +574,7 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         for (int op = 0; op < translogOperations; op++) {
             String ascii = randomAsciiOfLengthBetween(1, 50);
-            locations.add(translog.add(new Translog.Create("test", "" + op, ascii.getBytes("UTF-8"))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, ascii.getBytes("UTF-8"))));
         }
         translog.sync();
 
@@ -638,7 +638,7 @@ public class TranslogTests extends ESTestCase {
     @Test
     public void testVerifyTranslogIsNotDeleted() throws IOException {
         assertFileIsPresent(translog, 1);
-        translog.add(new Translog.Create("test", "1", new byte[]{1}));
+        translog.add(new Translog.Index("test", "1", new byte[]{1}));
         Translog.Snapshot snapshot = translog.newSnapshot();
         assertThat(snapshot, SnapshotMatchers.size(1));
         assertFileIsPresent(translog, 1);
@@ -686,17 +686,12 @@ public class TranslogTests extends ESTestCase {
                         final Translog.Operation op;
                         switch (Translog.Operation.Type.values()[((int) (id % Translog.Operation.Type.values().length))]) {
                             case CREATE:
-                                op = new Translog.Create("type", "" + id, new byte[]{(byte) id});
-                                break;
-                            case SAVE:
+                            case INDEX:
                                 op = new Translog.Index("type", "" + id, new byte[]{(byte) id});
                                 break;
                             case DELETE:
                                 op = new Translog.Delete(newUid("" + id));
                                 break;
-                            case DELETE_BY_QUERY:
-                                // deprecated
-                                continue;
                             default:
                                 throw new ElasticsearchException("unknown type");
                         }
@@ -830,12 +825,12 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         int count = 0;
         for (int op = 0; op < translogOperations; op++) {
-            final Translog.Location location = translog.add(new Translog.Create("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8"))));
+            final Translog.Location location = translog.add(new Translog.Index("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8"))));
             if (randomBoolean()) {
                 assertTrue("at least one operation pending", translog.syncNeeded());
                 assertTrue("this operation has not been synced", translog.ensureSynced(location));
                 assertFalse("the last call to ensureSycned synced all previous ops", translog.syncNeeded()); // we are the last location so everything should be synced
-                translog.add(new Translog.Create("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8"))));
+                translog.add(new Translog.Index("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8"))));
                 assertTrue("one pending operation", translog.syncNeeded());
                 assertFalse("this op has been synced before", translog.ensureSynced(location)); // not syncing now
                 assertTrue("we only synced a previous operation yet", translog.syncNeeded());
@@ -858,7 +853,7 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         int count = 0;
         for (int op = 0; op < translogOperations; op++) {
-            locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8")))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8")))));
             if (rarely() && translogOperations > op+1) {
                 translog.commit();
             }
@@ -887,14 +882,14 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         int lastSynced = -1;
         for (int op = 0; op < translogOperations; op++) {
-            locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
             if (frequently()) {
                 translog.sync();
                 lastSynced = op;
             }
         }
         assertEquals(translogOperations, translog.totalOperations());
-        final Translog.Location lastLocation = translog.add(new Translog.Create("test", "" + translogOperations, Integer.toString(translogOperations).getBytes(Charset.forName("UTF-8"))));
+        final Translog.Location lastLocation = translog.add(new Translog.Index("test", "" + translogOperations, Integer.toString(translogOperations).getBytes(Charset.forName("UTF-8"))));
 
         final Checkpoint checkpoint = Checkpoint.read(translog.location().resolve(Translog.CHECKPOINT_FILE_NAME));
         try (final ImmutableTranslogReader reader = translog.openReader(translog.location().resolve(Translog.getFilename(translog.currentFileGeneration())), checkpoint)) {
@@ -975,7 +970,7 @@ public class TranslogTests extends ESTestCase {
         int minUncommittedOp = -1;
         final boolean commitOften = randomBoolean();
         for (int op = 0; op < translogOperations; op++) {
-            locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
             final boolean commit = commitOften ? frequently() : rarely();
             if (commit && op < translogOperations-1) {
                 translog.commit();
@@ -1017,7 +1012,7 @@ public class TranslogTests extends ESTestCase {
         Translog.TranslogGeneration translogGeneration = null;
         final boolean sync = randomBoolean();
         for (int op = 0; op < translogOperations; op++) {
-            locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
             if (op == prepareOp) {
                 translogGeneration = translog.getGeneration();
                 translog.prepareCommit();
@@ -1068,7 +1063,7 @@ public class TranslogTests extends ESTestCase {
         List<Translog.Operation> ops = new ArrayList<>();
         int translogOperations = randomIntBetween(10, 100);
         for (int op = 0; op < translogOperations; op++) {
-            Translog.Create test = new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")));
+            Translog.Index test = new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")));
             ops.add(test);
         }
         Translog.writeOperations(out, ops);
@@ -1083,8 +1078,8 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         try(Translog translog2 = create(createTempDir())) {
             for (int op = 0; op < translogOperations; op++) {
-                locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
-                locations2.add(translog2.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+                locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+                locations2.add(translog2.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
             }
             int iters = randomIntBetween(10, 100);
             for (int i = 0; i < iters; i++) {
@@ -1110,7 +1105,7 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(1, 10);
         int firstUncommitted = 0;
         for (int op = 0; op < translogOperations; op++) {
-            locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
             if (randomBoolean()) {
                 translog.commit();
                 firstUncommitted = op + 1;
@@ -1138,147 +1133,4 @@ public class TranslogTests extends ESTestCase {
             assertNull(snapshot.next());
         }
     }
-
-    public void testUpgradeOldTranslogFiles() throws IOException {
-        List<Path> indexes = new ArrayList<>();
-        try (DirectoryStream<Path> stream = Files.newDirectoryStream(getBwcIndicesPath(), "index-*.zip")) {
-            for (Path path : stream) {
-                indexes.add(path);
-            }
-        }
-        TranslogConfig config = this.translog.getConfig();
-        Translog.TranslogGeneration gen = translog.getGeneration();
-        this.translog.close();
-        try {
-            Translog.upgradeLegacyTranslog(logger, translog.getConfig());
-            fail("no generation set");
-        } catch (IllegalArgumentException ex) {
-
-        }
-        translog.getConfig().setTranslogGeneration(gen);
-        try {
-            Translog.upgradeLegacyTranslog(logger, translog.getConfig());
-            fail("already upgraded generation set");
-        } catch (IllegalArgumentException ex) {
-
-        }
-
-        for (Path indexFile : indexes) {
-            final String indexName = indexFile.getFileName().toString().replace(".zip", "").toLowerCase(Locale.ROOT);
-            Version version = Version.fromString(indexName.replace("index-", ""));
-            if (version.onOrAfter(Version.V_2_0_0_beta1)) {
-                continue;
-            }
-            Path unzipDir = createTempDir();
-            Path unzipDataDir = unzipDir.resolve("data");
-            // decompress the index
-            try (InputStream stream = Files.newInputStream(indexFile)) {
-                TestUtil.unzip(stream, unzipDir);
-            }
-            // check it is unique
-            assertTrue(Files.exists(unzipDataDir));
-            Path[] list = FileSystemUtils.files(unzipDataDir);
-            if (list.length != 1) {
-                throw new IllegalStateException("Backwards index must contain exactly one cluster but was " + list.length);
-            }
-            // the bwc scripts packs the indices under this path
-            Path src = list[0].resolve("nodes/0/indices/" + indexName);
-            Path translog = list[0].resolve("nodes/0/indices/" + indexName).resolve("0").resolve("translog");
-
-            assertTrue("[" + indexFile + "] missing index dir: " + src.toString(), Files.exists(src));
-            assertTrue("[" + indexFile + "] missing translog dir: " + translog.toString(), Files.exists(translog));
-            Path[] tlogFiles =  FileSystemUtils.files(translog);
-            assertEquals(tlogFiles.length, 1);
-            final long size = Files.size(tlogFiles[0]);
-
-            final long generation = parseLegacyTranslogFile(tlogFiles[0]);
-            assertTrue(generation >= 1);
-            logger.info("upgrading index {} file: {} size: {}", indexName, tlogFiles[0].getFileName(), size);
-            TranslogConfig upgradeConfig = new TranslogConfig(config.getShardId(), translog, config.getIndexSettings(), config.getDurabilty(), config.getBigArrays(), config.getThreadPool());
-            upgradeConfig.setTranslogGeneration(new Translog.TranslogGeneration(null, generation));
-            Translog.upgradeLegacyTranslog(logger, upgradeConfig);
-            try (Translog upgraded = new Translog(upgradeConfig)) {
-                assertEquals(generation + 1, upgraded.getGeneration().translogFileGeneration);
-                assertEquals(upgraded.getRecoveredReaders().size(), 1);
-                final long headerSize;
-                if (version.before(Version.V_1_4_0_Beta1)) {
-                    assertTrue(upgraded.getRecoveredReaders().get(0).getClass().toString(), upgraded.getRecoveredReaders().get(0).getClass() == LegacyTranslogReader.class);
-                   headerSize = 0;
-                } else {
-                    assertTrue(upgraded.getRecoveredReaders().get(0).getClass().toString(), upgraded.getRecoveredReaders().get(0).getClass() == LegacyTranslogReaderBase.class);
-                    headerSize = CodecUtil.headerLength(TranslogWriter.TRANSLOG_CODEC);
-                }
-                List<Translog.Operation> operations = new ArrayList<>();
-                try (Translog.Snapshot snapshot = upgraded.newSnapshot()) {
-                    Translog.Operation op = null;
-                    while ((op = snapshot.next()) != null) {
-                        operations.add(op);
-                    }
-                }
-                if (size > headerSize) {
-                    assertFalse(operations.toString(), operations.isEmpty());
-                } else {
-                    assertTrue(operations.toString(), operations.isEmpty());
-                }
-            }
-        }
-    }
-
-    /**
-     * this tests a set of files that has some of the operations flushed with a buffered translog such that tlogs are truncated.
-     * 3 of the 6 files are created with ES 1.3 and the rest is created wiht ES 1.4 such that both the checksummed as well as the
-     * super old version of the translog without a header is tested.
-     */
-    public void testOpenAndReadTruncatedLegacyTranslogs() throws IOException {
-        Path zip = getDataPath("/org/elasticsearch/index/translog/legacy_translogs.zip");
-        Path unzipDir = createTempDir();
-        try (InputStream stream = Files.newInputStream(zip)) {
-            TestUtil.unzip(stream, unzipDir);
-        }
-        TranslogConfig config = this.translog.getConfig();
-        int count = 0;
-        try (DirectoryStream<Path> stream = Files.newDirectoryStream(unzipDir)) {
-
-            for (Path legacyTranslog : stream) {
-                logger.debug("upgrading {} ", legacyTranslog.getFileName());
-                Path directory = legacyTranslog.resolveSibling("translog_" + count++);
-                Files.createDirectories(directory);
-                Files.copy(legacyTranslog, directory.resolve(legacyTranslog.getFileName()));
-                TranslogConfig upgradeConfig = new TranslogConfig(config.getShardId(), directory, config.getIndexSettings(), config.getDurabilty(), config.getBigArrays(), config.getThreadPool());
-                try {
-                    Translog.upgradeLegacyTranslog(logger, upgradeConfig);
-                    fail("no generation set");
-                } catch (IllegalArgumentException ex) {
-                    // expected
-                }
-                long generation = parseLegacyTranslogFile(legacyTranslog);
-                upgradeConfig.setTranslogGeneration(new Translog.TranslogGeneration(null, generation));
-                Translog.upgradeLegacyTranslog(logger, upgradeConfig);
-                try (Translog tlog = new Translog(upgradeConfig)) {
-                    List<Translog.Operation> operations = new ArrayList<>();
-                    try (Translog.Snapshot snapshot = tlog.newSnapshot()) {
-                        Translog.Operation op = null;
-                        while ((op = snapshot.next()) != null) {
-                            operations.add(op);
-                        }
-                    }
-                    logger.debug("num ops recovered: {} for file {} ", operations.size(), legacyTranslog.getFileName());
-                    assertFalse(operations.isEmpty());
-                }
-            }
-        }
-    }
-
-    public static long parseLegacyTranslogFile(Path translogFile) {
-        final String fileName = translogFile.getFileName().toString();
-        final Matcher matcher = PARSE_LEGACY_ID_PATTERN.matcher(fileName);
-        if (matcher.matches()) {
-            try {
-                return Long.parseLong(matcher.group(1));
-            } catch (NumberFormatException e) {
-                throw new IllegalStateException("number formatting issue in a file that passed PARSE_STRICT_ID_PATTERN: " + fileName + "]", e);
-            }
-        }
-        throw new IllegalArgumentException("can't parse id from file: " + fileName);
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java b/core/src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java
index 451fdf3..283124d 100644
--- a/core/src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java
+++ b/core/src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java
@@ -45,7 +45,7 @@ public class TranslogVersionTests extends ESTestCase {
             assertThat("a version0 stream is returned", reader instanceof LegacyTranslogReader, equalTo(true));
             try (final Translog.Snapshot snapshot = reader.newSnapshot()) {
                 final Translog.Operation operation = snapshot.next();
-                assertThat("operation is the correct type correctly", operation.opType() == Translog.Operation.Type.SAVE, equalTo(true));
+                assertThat("operation is the correct type correctly", operation.opType() == Translog.Operation.Type.INDEX, equalTo(true));
                 Translog.Index op = (Translog.Index) operation;
                 assertThat(op.id(), equalTo("1"));
                 assertThat(op.type(), equalTo("doc"));
@@ -73,8 +73,8 @@ public class TranslogVersionTests extends ESTestCase {
 
                 Translog.Operation operation = snapshot.next();
 
-                assertThat("operation is the correct type correctly", operation.opType() == Translog.Operation.Type.CREATE, equalTo(true));
-                Translog.Create op = (Translog.Create) operation;
+                assertThat("operation is the correct type correctly", operation.opType() == Translog.Operation.Type.INDEX, equalTo(true));
+                Translog.Index op = (Translog.Index) operation;
                 assertThat(op.id(), equalTo("Bwiq98KFSb6YjJQGeSpeiw"));
                 assertThat(op.type(), equalTo("doc"));
                 assertThat(op.source().toUtf8(), equalTo("{\"body\": \"foo\"}"));
diff --git a/core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerIT.java b/core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerIT.java
index 1526df3..3e95685 100644
--- a/core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerIT.java
@@ -35,8 +35,6 @@ import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.IndexShardState;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.test.ESIntegTestCase;
-import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
-import org.elasticsearch.test.ESIntegTestCase.Scope;
 import org.hamcrest.Matchers;
 import org.junit.Test;
 
@@ -57,6 +55,8 @@ import static org.elasticsearch.index.shard.IndexShardState.CREATED;
 import static org.elasticsearch.index.shard.IndexShardState.POST_RECOVERY;
 import static org.elasticsearch.index.shard.IndexShardState.RECOVERING;
 import static org.elasticsearch.index.shard.IndexShardState.STARTED;
+import static org.elasticsearch.test.ESIntegTestCase.ClusterScope;
+import static org.elasticsearch.test.ESIntegTestCase.Scope;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.Matchers.greaterThanOrEqualTo;
@@ -105,7 +105,7 @@ public class IndicesLifecycleListenerIT extends ESIntegTestCase {
         } catch (Exception e) {
             assertTrue(e.getMessage().contains("failing on purpose"));
             ClusterStateResponse resp = client().admin().cluster().prepareState().get();
-            assertFalse(resp.getState().routingTable().indicesRouting().keys().contains("failed"));
+            assertFalse(resp.getState().routingTable().indicesRouting().keySet().contains("failed"));
         }
     }
 
@@ -149,7 +149,7 @@ public class IndicesLifecycleListenerIT extends ESIntegTestCase {
         } catch (ElasticsearchException e) {
             assertTrue(e.getMessage().contains("failing on purpose"));
             ClusterStateResponse resp = client().admin().cluster().prepareState().get();
-            assertFalse(resp.getState().routingTable().indicesRouting().keys().contains("failed"));
+            assertFalse(resp.getState().routingTable().indicesRouting().keySet().contains("failed"));
         }
 
 
diff --git a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java b/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java
index aaca771..e14cc22 100644
--- a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java
@@ -82,7 +82,7 @@ public class IndexingMemoryControllerIT extends ESIntegTestCase {
         index("test1", "type", "1", "f", 1);
 
         // make shard the shard buffer was set to inactive size
-        final ByteSizeValue inactiveBuffer = EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER;
+        final ByteSizeValue inactiveBuffer = IndexingMemoryController.INACTIVE_SHARD_INDEXING_BUFFER;
         if (awaitBusy(() -> getIWBufferSize("test1") == inactiveBuffer.bytes()) == false) {
             fail("failed to update shard indexing buffer size for test1 index to [" + inactiveBuffer + "]; got: " + getIWBufferSize("test1"));
         }
diff --git a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java b/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java
index f6e21db..d3d9e96 100644
--- a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java
+++ b/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java
@@ -22,13 +22,17 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.index.engine.EngineConfig;
 import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.index.translog.TranslogConfig;
 import org.elasticsearch.test.ESTestCase;
 
 import java.util.ArrayList;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.not;
@@ -39,44 +43,28 @@ public class IndexingMemoryControllerTests extends ESTestCase {
 
         final static ByteSizeValue INACTIVE = new ByteSizeValue(-1);
 
-        final Map<ShardId, Long> translogIds = new HashMap<>();
-        final Map<ShardId, Long> translogOps = new HashMap<>();
-
         final Map<ShardId, ByteSizeValue> indexingBuffers = new HashMap<>();
         final Map<ShardId, ByteSizeValue> translogBuffers = new HashMap<>();
 
+        final Map<ShardId, Long> lastIndexTimeNanos = new HashMap<>();
+        final Set<ShardId> activeShards = new HashSet<>();
+
         long currentTimeSec = TimeValue.timeValueNanos(System.nanoTime()).seconds();
 
         public MockController(Settings settings) {
             super(Settings.builder()
                             .put(SHARD_INACTIVE_INTERVAL_TIME_SETTING, "200h") // disable it
-                            .put(SHARD_INACTIVE_TIME_SETTING, "0s") // immediate
+                            .put(SHARD_INACTIVE_TIME_SETTING, "1ms") // nearly immediate
                             .put(settings)
                             .build(),
                     null, null, 100 * 1024 * 1024); // fix jvm mem size to 100mb
         }
 
-        public void incTranslog(ShardId shard1, int id, int ops) {
-            setTranslog(shard1, translogIds.get(shard1) + id, translogOps.get(shard1) + ops);
-        }
-
-        public void setTranslog(ShardId id, long translogId, long ops) {
-            translogIds.put(id, translogId);
-            translogOps.put(id, ops);
-        }
-
         public void deleteShard(ShardId id) {
-            translogIds.remove(id);
-            translogOps.remove(id);
             indexingBuffers.remove(id);
             translogBuffers.remove(id);
         }
 
-        public void assertActive(ShardId id) {
-            assertThat(indexingBuffers.get(id), not(equalTo(INACTIVE)));
-            assertThat(translogBuffers.get(id), not(equalTo(INACTIVE)));
-        }
-
         public void assertBuffers(ShardId id, ByteSizeValue indexing, ByteSizeValue translog) {
             assertThat(indexingBuffers.get(id), equalTo(indexing));
             assertThat(translogBuffers.get(id), equalTo(translog));
@@ -94,29 +82,17 @@ public class IndexingMemoryControllerTests extends ESTestCase {
 
         @Override
         protected List<ShardId> availableShards() {
-            return new ArrayList<>(translogIds.keySet());
+            return new ArrayList<>(indexingBuffers.keySet());
         }
 
         @Override
         protected boolean shardAvailable(ShardId shardId) {
-            return translogIds.containsKey(shardId);
+            return indexingBuffers.containsKey(shardId);
         }
 
         @Override
-        protected void markShardAsInactive(ShardId shardId) {
-            indexingBuffers.put(shardId, INACTIVE);
-            translogBuffers.put(shardId, INACTIVE);
-        }
-
-        @Override
-        protected ShardIndexingStatus getTranslogStatus(ShardId shardId) {
-            if (!shardAvailable(shardId)) {
-                return null;
-            }
-            ShardIndexingStatus status = new ShardIndexingStatus();
-            status.translogId = translogIds.get(shardId);
-            status.translogNumberOfOperations = translogOps.get(shardId);
-            return status;
+        protected Boolean getShardActive(ShardId shardId) {
+            return activeShards.contains(shardId);
         }
 
         @Override
@@ -125,12 +101,34 @@ public class IndexingMemoryControllerTests extends ESTestCase {
             translogBuffers.put(shardId, shardTranslogBufferSize);
         }
 
+        @Override
+        protected Boolean checkIdle(ShardId shardId, long inactiveTimeNS) {
+            Long ns = lastIndexTimeNanos.get(shardId);
+            if (ns == null) {
+                return null;
+            } else if (currentTimeInNanos() - ns >= inactiveTimeNS) {
+                indexingBuffers.put(shardId, INACTIVE);
+                translogBuffers.put(shardId, INACTIVE);
+                activeShards.remove(shardId);
+                return true;
+            } else {
+                return false;
+            }
+        }
+
         public void incrementTimeSec(int sec) {
             currentTimeSec += sec;
         }
 
-        public void simulateFlush(ShardId shard) {
-            setTranslog(shard, translogIds.get(shard) + 1, 0);
+        public void simulateIndexing(ShardId shardId) {
+            lastIndexTimeNanos.put(shardId, currentTimeInNanos());
+            if (indexingBuffers.containsKey(shardId) == false) {
+                // First time we are seeing this shard; start it off with inactive buffers as IndexShard does:
+                indexingBuffers.put(shardId, IndexingMemoryController.INACTIVE_SHARD_INDEXING_BUFFER);
+                translogBuffers.put(shardId, IndexingMemoryController.INACTIVE_SHARD_TRANSLOG_BUFFER);
+            }
+            activeShards.add(shardId);
+            forceCheck();
         }
     }
 
@@ -139,14 +137,12 @@ public class IndexingMemoryControllerTests extends ESTestCase {
                 .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "10mb")
                 .put(IndexingMemoryController.TRANSLOG_BUFFER_SIZE_SETTING, "100kb").build());
         final ShardId shard1 = new ShardId("test", 1);
-        controller.setTranslog(shard1, randomInt(10), randomInt(10));
-        controller.forceCheck();
+        controller.simulateIndexing(shard1);
         controller.assertBuffers(shard1, new ByteSizeValue(10, ByteSizeUnit.MB), new ByteSizeValue(64, ByteSizeUnit.KB)); // translog is maxed at 64K
 
         // add another shard
         final ShardId shard2 = new ShardId("test", 2);
-        controller.setTranslog(shard2, randomInt(10), randomInt(10));
-        controller.forceCheck();
+        controller.simulateIndexing(shard2);
         controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
         controller.assertBuffers(shard2, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
 
@@ -161,8 +157,7 @@ public class IndexingMemoryControllerTests extends ESTestCase {
 
         // add a new one
         final ShardId shard3 = new ShardId("test", 3);
-        controller.setTranslog(shard3, randomInt(10), randomInt(10));
-        controller.forceCheck();
+        controller.simulateIndexing(shard3);
         controller.assertBuffers(shard3, new ByteSizeValue(10, ByteSizeUnit.MB), new ByteSizeValue(64, ByteSizeUnit.KB)); // translog is maxed at 64K
     }
 
@@ -174,48 +169,42 @@ public class IndexingMemoryControllerTests extends ESTestCase {
                 .build());
 
         final ShardId shard1 = new ShardId("test", 1);
-        controller.setTranslog(shard1, 0, 0);
+        controller.simulateIndexing(shard1);
         final ShardId shard2 = new ShardId("test", 2);
-        controller.setTranslog(shard2, 0, 0);
-        controller.forceCheck();
+        controller.simulateIndexing(shard2);
         controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
         controller.assertBuffers(shard2, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
 
         // index into both shards, move the clock and see that they are still active
-        controller.setTranslog(shard1, randomInt(2), randomInt(2) + 1);
-        controller.setTranslog(shard2, randomInt(2) + 1, randomInt(2));
-        // the controller doesn't know when the ops happened, so even if this is more
-        // than the inactive time the shard is still marked as active
+        controller.simulateIndexing(shard1);
+        controller.simulateIndexing(shard2);
+
         controller.incrementTimeSec(10);
         controller.forceCheck();
-        controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
-        controller.assertBuffers(shard2, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
 
-        // index into one shard only, see other shard is made inactive correctly
-        controller.incTranslog(shard1, randomInt(2), randomInt(2) + 1);
-        controller.forceCheck(); // register what happened with the controller (shard is still active)
-        controller.incrementTimeSec(3); // increment but not enough
-        controller.forceCheck();
-        controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
-        controller.assertBuffers(shard2, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
+        // both shards now inactive
+        controller.assertInActive(shard1);
+        controller.assertInActive(shard2);
 
-        controller.incrementTimeSec(3); // increment some more
+        // index into one shard only, see it becomes active
+        controller.simulateIndexing(shard1);
+        controller.assertBuffers(shard1, new ByteSizeValue(10, ByteSizeUnit.MB), new ByteSizeValue(64, ByteSizeUnit.KB));
+        controller.assertInActive(shard2);
+
+        controller.incrementTimeSec(3); // increment but not enough to become inactive
         controller.forceCheck();
         controller.assertBuffers(shard1, new ByteSizeValue(10, ByteSizeUnit.MB), new ByteSizeValue(64, ByteSizeUnit.KB));
         controller.assertInActive(shard2);
 
-        if (randomBoolean()) {
-            // once a shard gets inactive it will be synced flushed and a new translog generation will be made
-            controller.simulateFlush(shard2);
-            controller.forceCheck();
-            controller.assertInActive(shard2);
-        }
+        controller.incrementTimeSec(3); // increment some more
+        controller.forceCheck();
+        controller.assertInActive(shard1);
+        controller.assertInActive(shard2);
 
         // index some and shard becomes immediately active
-        controller.incTranslog(shard2, randomInt(2), 1 + randomInt(2)); // we must make sure translog ops is never 0
-        controller.forceCheck();
-        controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
-        controller.assertBuffers(shard2, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
+        controller.simulateIndexing(shard2);
+        controller.assertInActive(shard1);
+        controller.assertBuffers(shard2, new ByteSizeValue(10, ByteSizeUnit.MB), new ByteSizeValue(64, ByteSizeUnit.KB));
     }
 
     public void testMinShardBufferSizes() {
@@ -273,10 +262,9 @@ public class IndexingMemoryControllerTests extends ESTestCase {
 
     protected void assertTwoActiveShards(MockController controller, ByteSizeValue indexBufferSize, ByteSizeValue translogBufferSize) {
         final ShardId shard1 = new ShardId("test", 1);
-        controller.setTranslog(shard1, 0, 0);
+        controller.simulateIndexing(shard1);
         final ShardId shard2 = new ShardId("test", 2);
-        controller.setTranslog(shard2, 0, 0);
-        controller.forceCheck();
+        controller.simulateIndexing(shard2);
         controller.assertBuffers(shard1, indexBufferSize, translogBufferSize);
         controller.assertBuffers(shard2, indexBufferSize, translogBufferSize);
 
diff --git a/core/src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerIT.java b/core/src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerIT.java
index 234f3ea..4f0bd60 100644
--- a/core/src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerIT.java
@@ -188,7 +188,7 @@ public class RandomExceptionCircuitBreakerIT extends ESIntegTestCase {
                 for (String node : internalCluster().getNodeNames()) {
                     final IndicesFieldDataCache fdCache = internalCluster().getInstance(IndicesFieldDataCache.class, node);
                     // Clean up the cache, ensuring that entries' listeners have been called
-                    fdCache.getCache().cleanUp();
+                    fdCache.getCache().refresh();
                 }
                 NodesStatsResponse nodeStats = client().admin().cluster().prepareNodesStats()
                         .clear().setBreaker(true).execute().actionGet();
diff --git a/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java b/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
index dbdfc2b..a1b2508 100644
--- a/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
@@ -23,10 +23,7 @@ import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.admin.indices.mapping.put.PutMappingResponse;
 import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.cluster.ClusterInfo;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ClusterStateUpdateTask;
+import org.elasticsearch.cluster.*;
 import org.elasticsearch.cluster.block.ClusterBlocks;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MappingMetaData;
@@ -57,19 +54,12 @@ import org.elasticsearch.test.junit.annotations.TestLogging;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.Arrays;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
+import java.util.*;
 import java.util.concurrent.atomic.AtomicReference;
 
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.hasItem;
-import static org.hamcrest.Matchers.hasSize;
-import static org.hamcrest.Matchers.instanceOf;
+import static org.hamcrest.Matchers.*;
 
 /**
  */
@@ -98,7 +88,7 @@ public class RareClusterStateIT extends ESIntegTestCase {
         AllocationDeciders allocationDeciders = new AllocationDeciders(Settings.EMPTY, new AllocationDecider[0]);
         RoutingNodes routingNodes = new RoutingNodes(
                 ClusterState.builder(current)
-                        .routingTable(RoutingTable.builder(current.routingTable()).remove("a").addAsRecovery(current.metaData().index("a")).build())
+                        .routingTable(RoutingTable.builder(current.routingTable()).remove("a").addAsRecovery(current.metaData().index("a")))
                         .nodes(DiscoveryNodes.EMPTY_NODES)
                         .build(), false
         );
@@ -137,7 +127,7 @@ public class RareClusterStateIT extends ESIntegTestCase {
 
                 RoutingTable.Builder routingTable = RoutingTable.builder(updatedState.routingTable());
                 routingTable.addAsRecovery(updatedState.metaData().index(index));
-                updatedState = ClusterState.builder(updatedState).routingTable(routingTable.build()).build();
+                updatedState = ClusterState.builder(updatedState).routingTable(routingTable).build();
 
                 RoutingAllocation.Result result = allocationService.reroute(updatedState);
                 return ClusterState.builder(updatedState).routingResult(result).build();
diff --git a/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java b/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
index 1ffce8d..e98794c 100644
--- a/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
+++ b/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
@@ -587,7 +587,6 @@ public class PluginManagerIT extends ESIntegTestCase {
         PluginManager.checkForOfficialPlugins("analysis-phonetic");
         PluginManager.checkForOfficialPlugins("analysis-smartcn");
         PluginManager.checkForOfficialPlugins("analysis-stempel");
-        PluginManager.checkForOfficialPlugins("cloud-gce");
         PluginManager.checkForOfficialPlugins("delete-by-query");
         PluginManager.checkForOfficialPlugins("lang-expression");
         PluginManager.checkForOfficialPlugins("lang-groovy");
@@ -598,6 +597,7 @@ public class PluginManagerIT extends ESIntegTestCase {
         PluginManager.checkForOfficialPlugins("discovery-multicast");
         PluginManager.checkForOfficialPlugins("discovery-azure");
         PluginManager.checkForOfficialPlugins("discovery-ec2");
+        PluginManager.checkForOfficialPlugins("discovery-gce");
         PluginManager.checkForOfficialPlugins("repository-azure");
         PluginManager.checkForOfficialPlugins("repository-s3");
         PluginManager.checkForOfficialPlugins("store-smb");
diff --git a/core/src/test/java/org/elasticsearch/plugins/PluginManagerPermissionTests.java b/core/src/test/java/org/elasticsearch/plugins/PluginManagerPermissionTests.java
new file mode 100644
index 0000000..374c776
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/plugins/PluginManagerPermissionTests.java
@@ -0,0 +1,300 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plugins;
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.elasticsearch.Version;
+import org.elasticsearch.common.cli.CliToolTestCase.CaptureOutputTerminal;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.io.IOException;
+import java.net.URL;
+import java.nio.charset.Charset;
+import java.nio.file.*;
+import java.nio.file.attribute.BasicFileAttributes;
+import java.nio.file.attribute.PosixFilePermissions;
+import java.util.zip.ZipEntry;
+import java.util.zip.ZipOutputStream;
+
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.elasticsearch.plugins.PluginInfoTests.writeProperties;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
+
+// there are some lucene file systems that seem to cause problems (deleted files, dirs instead of files)
+@LuceneTestCase.SuppressFileSystems("*")
+public class PluginManagerPermissionTests extends ESTestCase {
+
+    private String pluginName = "my-plugin";
+    private CaptureOutputTerminal terminal = new CaptureOutputTerminal();
+    private Environment environment;
+    private boolean supportsPermissions;
+
+    @Before
+    public void setup() {
+        Path tempDir = createTempDir();
+        Settings.Builder settingsBuilder = settingsBuilder().put("path.home", tempDir);
+        if (randomBoolean()) {
+            settingsBuilder.put("path.plugins", createTempDir());
+        }
+
+        if (randomBoolean()) {
+            settingsBuilder.put("path.conf", createTempDir());
+        }
+
+        environment = new Environment(settingsBuilder.build());
+
+        supportsPermissions = tempDir.getFileSystem().supportedFileAttributeViews().contains("posix");
+    }
+
+    public void testThatUnaccessibleBinDirectoryAbortsPluginInstallation() throws Exception {
+        assumeTrue("File system does not support permissions, skipping", supportsPermissions);
+
+        URL pluginUrl = createPlugin(true, randomBoolean());
+
+        Path binPath = environment.binFile().resolve(pluginName);
+        Files.createDirectories(binPath);
+        try {
+            Files.setPosixFilePermissions(binPath, PosixFilePermissions.fromString("---------"));
+
+            PluginManager pluginManager = new PluginManager(environment, pluginUrl, PluginManager.OutputMode.VERBOSE, TimeValue.timeValueSeconds(10));
+            pluginManager.downloadAndExtract(pluginName, terminal);
+
+            fail("Expected IOException but did not happen");
+        } catch (IOException e) {
+            assertFileNotExists(environment.pluginsFile().resolve(pluginName));
+            assertFileNotExists(environment.configFile().resolve(pluginName));
+            // exists, because of our weird permissions above
+            assertDirectoryExists(environment.binFile().resolve(pluginName));
+
+            assertThat(terminal.getTerminalOutput(), hasItem(containsString("Error copying bin directory ")));
+        } finally {
+            Files.setPosixFilePermissions(binPath, PosixFilePermissions.fromString("rwxrwxrwx"));
+        }
+    }
+
+    public void testThatUnaccessiblePluginConfigDirectoryAbortsPluginInstallation() throws Exception {
+        assumeTrue("File system does not support permissions, skipping", supportsPermissions);
+
+        URL pluginUrl = createPlugin(randomBoolean(), true);
+
+        Path path = environment.configFile().resolve(pluginName);
+        Files.createDirectories(path);
+        Files.createFile(path.resolve("my-custom-config.yaml"));
+        Path binPath = environment.binFile().resolve(pluginName);
+        Files.createDirectories(binPath);
+
+        try {
+            Files.setPosixFilePermissions(path.resolve("my-custom-config.yaml"), PosixFilePermissions.fromString("---------"));
+            Files.setPosixFilePermissions(path, PosixFilePermissions.fromString("---------"));
+
+            PluginManager pluginManager = new PluginManager(environment, pluginUrl, PluginManager.OutputMode.VERBOSE, TimeValue.timeValueSeconds(10));
+            pluginManager.downloadAndExtract(pluginName, terminal);
+
+            fail("Expected IOException but did not happen, terminal output was " + terminal.getTerminalOutput());
+        } catch (IOException e) {
+            assertFileNotExists(environment.pluginsFile().resolve(pluginName));
+            assertFileNotExists(environment.binFile().resolve(pluginName));
+            // exists, because of our weird permissions above
+            assertDirectoryExists(environment.configFile().resolve(pluginName));
+
+            assertThat(terminal.getTerminalOutput(), hasItem(containsString("Error copying config directory ")));
+        } finally {
+            Files.setPosixFilePermissions(path, PosixFilePermissions.fromString("rwxrwxrwx"));
+            Files.setPosixFilePermissions(path.resolve("my-custom-config.yaml"), PosixFilePermissions.fromString("rwxrwxrwx"));
+        }
+    }
+
+    // config/bin are not writable, but the plugin does not need to put anything into it
+    public void testThatPluginWithoutBinAndConfigWorksEvenIfPermissionsAreWrong() throws Exception {
+        assumeTrue("File system does not support permissions, skipping", supportsPermissions);
+
+        URL pluginUrl = createPlugin(false, false);
+        Path path = environment.configFile().resolve(pluginName);
+        Files.createDirectories(path);
+        Files.createFile(path.resolve("my-custom-config.yaml"));
+        Path binPath = environment.binFile().resolve(pluginName);
+        Files.createDirectories(binPath);
+
+        try {
+            Files.setPosixFilePermissions(path.resolve("my-custom-config.yaml"), PosixFilePermissions.fromString("---------"));
+            Files.setPosixFilePermissions(path, PosixFilePermissions.fromString("---------"));
+            Files.setPosixFilePermissions(binPath, PosixFilePermissions.fromString("---------"));
+
+            PluginManager pluginManager = new PluginManager(environment, pluginUrl, PluginManager.OutputMode.VERBOSE, TimeValue.timeValueSeconds(10));
+            pluginManager.downloadAndExtract(pluginName, terminal);
+        } finally {
+            Files.setPosixFilePermissions(binPath, PosixFilePermissions.fromString("rwxrwxrwx"));
+            Files.setPosixFilePermissions(path, PosixFilePermissions.fromString("rwxrwxrwx"));
+            Files.setPosixFilePermissions(path.resolve("my-custom-config.yaml"), PosixFilePermissions.fromString("rwxrwxrwx"));
+        }
+
+    }
+
+    // plugins directory no accessible, should leave no other left over directories
+    public void testThatNonWritablePluginsDirectoryLeavesNoLeftOver() throws Exception {
+        assumeTrue("File system does not support permissions, skipping", supportsPermissions);
+
+        URL pluginUrl = createPlugin(true, true);
+        Files.createDirectories(environment.pluginsFile());
+
+        try {
+            Files.setPosixFilePermissions(environment.pluginsFile(), PosixFilePermissions.fromString("---------"));
+            PluginManager pluginManager = new PluginManager(environment, pluginUrl, PluginManager.OutputMode.VERBOSE, TimeValue.timeValueSeconds(10));
+            try {
+                pluginManager.downloadAndExtract(pluginName, terminal);
+                fail("Expected IOException due to read-only plugins/ directory");
+            } catch (IOException e) {
+                assertFileNotExists(environment.binFile().resolve(pluginName));
+                assertFileNotExists(environment.configFile().resolve(pluginName));
+
+                Files.setPosixFilePermissions(environment.pluginsFile(), PosixFilePermissions.fromString("rwxrwxrwx"));
+                assertDirectoryExists(environment.pluginsFile());
+                assertFileNotExists(environment.pluginsFile().resolve(pluginName));
+            }
+        } finally {
+            Files.setPosixFilePermissions(environment.pluginsFile(), PosixFilePermissions.fromString("rwxrwxrwx"));
+        }
+    }
+
+    public void testThatUnwriteableBackupFilesInConfigurationDirectoryAreReplaced() throws Exception {
+        assumeTrue("File system does not support permissions, skipping", supportsPermissions);
+
+        boolean pluginContainsExecutables = randomBoolean();
+        URL pluginUrl = createPlugin(pluginContainsExecutables, true);
+        Files.createDirectories(environment.configFile().resolve(pluginName));
+
+        Path configFile = environment.configFile().resolve(pluginName).resolve("my-custom-config.yaml");
+        Files.createFile(configFile);
+        Path backupConfigFile = environment.configFile().resolve(pluginName).resolve("my-custom-config.yaml.new");
+        Files.createFile(backupConfigFile);
+        Files.write(backupConfigFile, "foo".getBytes(Charset.forName("UTF-8")));
+
+        PluginManager pluginManager = new PluginManager(environment, pluginUrl, PluginManager.OutputMode.VERBOSE, TimeValue.timeValueSeconds(10));
+        try {
+            Files.setPosixFilePermissions(backupConfigFile, PosixFilePermissions.fromString("---------"));
+
+            pluginManager.downloadAndExtract(pluginName, terminal);
+
+            if (pluginContainsExecutables) {
+                assertDirectoryExists(environment.binFile().resolve(pluginName));
+            }
+            assertDirectoryExists(environment.pluginsFile().resolve(pluginName));
+            assertDirectoryExists(environment.configFile().resolve(pluginName));
+
+            assertFileExists(backupConfigFile);
+            Files.setPosixFilePermissions(backupConfigFile, PosixFilePermissions.fromString("rw-rw-rw-"));
+            String content = new String(Files.readAllBytes(backupConfigFile), Charset.forName("UTF-8"));
+            assertThat(content, is(not("foo")));
+        } finally {
+            Files.setPosixFilePermissions(backupConfigFile, PosixFilePermissions.fromString("rw-rw-rw-"));
+        }
+    }
+
+    public void testThatConfigDirectoryBeingAFileAbortsInstallationAndDoesNotAccidentallyDeleteThisFile() throws Exception {
+        assumeTrue("File system does not support permissions, skipping", supportsPermissions);
+
+        Files.createDirectories(environment.configFile());
+        Files.createFile(environment.configFile().resolve(pluginName));
+        URL pluginUrl = createPlugin(randomBoolean(), true);
+
+        PluginManager pluginManager = new PluginManager(environment, pluginUrl, PluginManager.OutputMode.VERBOSE, TimeValue.timeValueSeconds(10));
+
+        try {
+            pluginManager.downloadAndExtract(pluginName, terminal);
+            fail("Expected plugin installation to fail, but didnt");
+        } catch (IOException e) {
+            assertFileExists(environment.configFile().resolve(pluginName));
+            assertFileNotExists(environment.binFile().resolve(pluginName));
+            assertFileNotExists(environment.pluginsFile().resolve(pluginName));
+        }
+    }
+
+    public void testThatBinDirectoryBeingAFileAbortsInstallationAndDoesNotAccidentallyDeleteThisFile() throws Exception {
+        assumeTrue("File system does not support permissions, skipping", supportsPermissions);
+
+        Files.createDirectories(environment.binFile());
+        Files.createFile(environment.binFile().resolve(pluginName));
+        URL pluginUrl = createPlugin(true, randomBoolean());
+
+        PluginManager pluginManager = new PluginManager(environment, pluginUrl, PluginManager.OutputMode.VERBOSE, TimeValue.timeValueSeconds(10));
+
+        try {
+            pluginManager.downloadAndExtract(pluginName, terminal);
+            fail("Expected plugin installation to fail, but didnt");
+        } catch (IOException e) {
+            assertFileExists(environment.binFile().resolve(pluginName));
+            assertFileNotExists(environment.configFile().resolve(pluginName));
+            assertFileNotExists(environment.pluginsFile().resolve(pluginName));
+        }
+    }
+
+
+    private URL createPlugin(boolean withBinDir, boolean withConfigDir) throws IOException {
+        final Path structure = createTempDir().resolve("fake-plugin");
+        writeProperties(structure, "description", "fake desc",
+                "version", "1.0",
+                "elasticsearch.version", Version.CURRENT.toString(),
+                "jvm", "true",
+                "java.version", "1.7",
+                "name", pluginName,
+                "classname", pluginName);
+        if (withBinDir) {
+            // create bin dir
+            Path binDir = structure.resolve("bin");
+            Files.createDirectory(binDir);
+            Files.setPosixFilePermissions(binDir, PosixFilePermissions.fromString("rwxr-xr-x"));
+
+            // create executable
+            Path executable = binDir.resolve("my-binary");
+            Files.createFile(executable);
+            Files.setPosixFilePermissions(executable, PosixFilePermissions.fromString("rwxr-xr-x"));
+        }
+        if (withConfigDir) {
+            // create bin dir
+            Path configDir = structure.resolve("config");
+            Files.createDirectory(configDir);
+            Files.setPosixFilePermissions(configDir, PosixFilePermissions.fromString("rwxr-xr-x"));
+
+            // create config file
+            Path configFile = configDir.resolve("my-custom-config.yaml");
+            Files.createFile(configFile);
+            Files.write(configFile, "my custom config content".getBytes(Charset.forName("UTF-8")));
+            Files.setPosixFilePermissions(configFile, PosixFilePermissions.fromString("rw-r--r--"));
+        }
+
+        Path zip = createTempDir().resolve(structure.getFileName() + ".zip");
+        try (ZipOutputStream stream = new ZipOutputStream(Files.newOutputStream(zip))) {
+            Files.walkFileTree(structure, new SimpleFileVisitor<Path>() {
+                @Override
+                public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {
+                    stream.putNextEntry(new ZipEntry(structure.relativize(file).toString()));
+                    Files.copy(file, stream);
+                    return FileVisitResult.CONTINUE;
+                }
+            });
+        }
+        return zip.toUri().toURL();
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java b/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
index e38c993..efda8d2 100644
--- a/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
+++ b/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.script;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.script.ScriptService.ScriptType;
@@ -36,7 +38,6 @@ import java.util.Map;
 import java.util.Set;
 
 import static java.util.Collections.singleton;
-import static java.util.Collections.unmodifiableMap;
 import static java.util.Collections.unmodifiableSet;
 import static org.elasticsearch.common.util.set.Sets.newHashSet;
 import static org.hamcrest.CoreMatchers.equalTo;
@@ -243,14 +244,14 @@ public class ScriptModesTests extends ESTestCase {
         return ScriptModes.ENGINE_SETTINGS_PREFIX + "." + lang + "." + scriptType + "." + scriptContext.getKey();
     }
 
-    static Map<String, ScriptEngineService> buildScriptEnginesByLangMap(Set<ScriptEngineService> scriptEngines) {
-        Map<String, ScriptEngineService> builder = new HashMap<>();
+    static ImmutableMap<String, ScriptEngineService> buildScriptEnginesByLangMap(Set<ScriptEngineService> scriptEngines) {
+        ImmutableMap.Builder<String, ScriptEngineService> builder = ImmutableMap.builder();
         for (ScriptEngineService scriptEngine : scriptEngines) {
             for (String type : scriptEngine.types()) {
                 builder.put(type, scriptEngine);
             }
         }
-        return unmodifiableMap(builder);
+        return builder.build();
     }
 
     private static class CustomScriptEngineService implements ScriptEngineService {
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardSizeTermsIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardSizeTermsIT.java
index e76f48a..d045705 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardSizeTermsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardSizeTermsIT.java
@@ -18,13 +18,14 @@
  */
 package org.elasticsearch.search.aggregations.bucket;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.junit.Test;
 
 import java.util.Collection;
-import java.util.HashMap;
 import java.util.Map;
 
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
@@ -48,15 +49,16 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms  terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<String, Long> expected = new HashMap<>();
-        expected.put("1", 8l);
-        expected.put("3", 8l);
-        expected.put("2", 5l);
+        Map<String, Long> expected = ImmutableMap.<String, Long>builder()
+                .put("1", 8l)
+                .put("3", 8l)
+                .put("2", 5l)
+                .build();
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsString())));
         }
     }
-
+    
     @Test
     public void shardSizeEqualsSize_string() throws Exception {
         createIdx("type=string,index=not_analyzed");
@@ -72,10 +74,11 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms  terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<String, Long> expected = new HashMap<>();
-        expected.put("1", 8l);
-        expected.put("3", 8l);
-        expected.put("2", 4l);
+        Map<String, Long> expected = ImmutableMap.<String, Long>builder()
+                .put("1", 8l)
+                .put("3", 8l)
+                .put("2", 4l)
+                .build();
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsString())));
         }
@@ -97,10 +100,11 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3)); // we still only return 3 entries (based on the 'size' param)
-        Map<String, Long> expected = new HashMap<>();
-        expected.put("1", 8l);
-        expected.put("3", 8l);
-        expected.put("2", 5l); // <-- count is now fixed
+        Map<String, Long> expected = ImmutableMap.<String, Long>builder()
+                .put("1", 8l)
+                .put("3", 8l)
+                .put("2", 5l) // <-- count is now fixed
+                .build();
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsString())));
         }
@@ -122,15 +126,16 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3)); // we still only return 3 entries (based on the 'size' param)
-        Map<String, Long> expected = new HashMap<>();
-        expected.put("1", 5l);
-        expected.put("2", 4l);
-        expected.put("3", 3l); // <-- count is now fixed
+        Map<String, Long> expected = ImmutableMap.<String, Long>builder()
+                .put("1", 5l)
+                .put("2", 4l)
+                .put("3", 3l) // <-- count is now fixed
+                .build();
         for (Terms.Bucket bucket: buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKey())));
         }
     }
-
+    
     @Test
     public void noShardSizeTermOrder_string() throws Exception {
         createIdx("type=string,index=not_analyzed");
@@ -146,10 +151,11 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms  terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<String, Long> expected = new HashMap<>();
-        expected.put("1", 8l);
-        expected.put("2", 5l);
-        expected.put("3", 8l);
+        Map<String, Long> expected = ImmutableMap.<String, Long>builder()
+                .put("1", 8l)
+                .put("2", 5l)
+                .put("3", 8l)
+                .build();
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsString())));
         }
@@ -171,10 +177,11 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<Integer, Long> expected = new HashMap<>();
-        expected.put(1, 8l);
-        expected.put(3, 8l);
-        expected.put(2, 5l);
+        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
+                .put(1, 8l)
+                .put(3, 8l)
+                .put(2, 5l)
+                .build();
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -196,10 +203,11 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<Integer, Long> expected = new HashMap<>();
-        expected.put(1, 8l);
-        expected.put(3, 8l);
-        expected.put(2, 4l);
+        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
+                .put(1, 8l)
+                .put(3, 8l)
+                .put(2, 4l)
+                .build();
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -221,10 +229,11 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3)); // we still only return 3 entries (based on the 'size' param)
-        Map<Integer, Long> expected = new HashMap<>();
-        expected.put(1, 8l);
-        expected.put(3, 8l);
-        expected.put(2, 5l); // <-- count is now fixed
+        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
+                .put(1, 8l)
+                .put(3, 8l)
+                .put(2, 5l) // <-- count is now fixed
+                .build();
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -246,10 +255,11 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3)); // we still only return 3 entries (based on the 'size' param)
-        Map<Integer, Long> expected = new HashMap<>();
-        expected.put(1, 5l);
-        expected.put(2, 4l);
-        expected.put(3, 3l);
+        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
+                .put(1, 5l)
+                .put(2, 4l)
+                .put(3, 3l)
+                .build();
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -271,10 +281,11 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<Integer, Long> expected = new HashMap<>();
-        expected.put(1, 8l);
-        expected.put(2, 5l);
-        expected.put(3, 8l);
+        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
+                .put(1, 8l)
+                .put(2, 5l)
+                .put(3, 8l)
+                .build();
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -296,10 +307,11 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<Integer, Long> expected = new HashMap<>();
-        expected.put(1, 8l);
-        expected.put(3, 8l);
-        expected.put(2, 5l);
+        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
+                .put(1, 8l)
+                .put(3, 8l)
+                .put(2, 5l)
+                .build();
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -321,10 +333,11 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<Integer, Long> expected = new HashMap<>();
-        expected.put(1, 8l);
-        expected.put(3, 8l);
-        expected.put(2, 4l);
+        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
+                .put(1, 8l)
+                .put(3, 8l)
+                .put(2, 4l)
+                .build();
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -346,10 +359,11 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<Integer, Long> expected = new HashMap<>();
-        expected.put(1, 8l);
-        expected.put(3, 8l);
-        expected.put(2, 5l); // <-- count is now fixed
+        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
+                .put(1, 8l)
+                .put(3, 8l)
+                .put(2, 5l) // <-- count is now fixed
+                .build();
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -371,10 +385,11 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<Integer, Long> expected = new HashMap<>();
-        expected.put(1, 5l);
-        expected.put(2, 4l);
-        expected.put(3, 3l);
+        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
+                .put(1, 5l)
+                .put(2, 4l)
+                .put(3, 3l)
+                .build();
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -396,10 +411,11 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<Integer, Long> expected = new HashMap<>();
-        expected.put(1, 8l);
-        expected.put(2, 5l);
-        expected.put(3, 8l);
+        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
+                .put(1, 8l)
+                .put(2, 5l)
+                .put(3, 8l)
+                .build();
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
index fe942dc..ac4fcf8 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
@@ -19,12 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.moving.avg;
 
-
-import com.google.common.collect.EvictingQueue;
-
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.Bucket;
@@ -42,11 +40,7 @@ import org.junit.Test;
 import java.util.*;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.avg;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.histogram;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.max;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.min;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.range;
+import static org.elasticsearch.search.aggregations.AggregationBuilders.*;
 import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders.derivative;
 import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders.movingAvg;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
@@ -170,7 +164,7 @@ public class MovAvgIT extends ESIntegTestCase {
      */
     private void setupExpected(MovAvgType type, MetricTarget target, int windowSize) {
         ArrayList<Double> values = new ArrayList<>(numBuckets);
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
 
         for (PipelineAggregationHelperTests.MockBucket mockBucket : mockHisto) {
             double metricValue;
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgUnitTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgUnitTests.java
index 65e44b9..11c5e40 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgUnitTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgUnitTests.java
@@ -19,8 +19,8 @@
 
 package org.elasticsearch.search.aggregations.pipeline.moving.avg;
 
-import com.google.common.collect.EvictingQueue;
 import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.*;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
@@ -39,7 +39,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int numValues = randomIntBetween(1, 100);
         int windowSize = randomIntBetween(1, 50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < numValues; i++) {
 
             double randValue = randomDouble();
@@ -68,7 +68,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int windowSize = randomIntBetween(1, 50);
         int numPredictions = randomIntBetween(1, 50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < windowSize; i++) {
             window.offer(randomDouble());
         }
@@ -94,7 +94,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int numValues = randomIntBetween(1, 100);
         int windowSize = randomIntBetween(1, 50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < numValues; i++) {
             double randValue = randomDouble();
 
@@ -126,7 +126,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int windowSize = randomIntBetween(1, 50);
         int numPredictions = randomIntBetween(1,50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < windowSize; i++) {
             window.offer(randomDouble());
         }
@@ -158,7 +158,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int numValues = randomIntBetween(1, 100);
         int windowSize = randomIntBetween(1, 50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < numValues; i++) {
             double randValue = randomDouble();
 
@@ -193,7 +193,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int windowSize = randomIntBetween(1, 50);
         int numPredictions = randomIntBetween(1,50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < windowSize; i++) {
             window.offer(randomDouble());
         }
@@ -227,7 +227,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int numValues = randomIntBetween(1, 100);
         int windowSize = randomIntBetween(1, 50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < numValues; i++) {
             double randValue = randomDouble();
 
@@ -276,7 +276,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int windowSize = randomIntBetween(1, 50);
         int numPredictions = randomIntBetween(1, 50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < windowSize; i++) {
             window.offer(randomDouble());
         }
@@ -323,7 +323,7 @@ public class MovAvgUnitTests extends ESTestCase {
 
         int windowSize = randomIntBetween(period * 2, 50); // HW requires at least two periods of data
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < windowSize; i++) {
             window.offer(randomDouble());
         }
@@ -392,7 +392,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int windowSize = randomIntBetween(period * 2, 50); // HW requires at least two periods of data
         int numPredictions = randomIntBetween(1, 50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < windowSize; i++) {
             window.offer(randomDouble());
         }
@@ -465,7 +465,7 @@ public class MovAvgUnitTests extends ESTestCase {
 
         int windowSize = randomIntBetween(period * 2, 50); // HW requires at least two periods of data
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < windowSize; i++) {
             window.offer(randomDouble());
         }
@@ -533,7 +533,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int windowSize = randomIntBetween(period * 2, 50); // HW requires at least two periods of data
         int numPredictions = randomIntBetween(1, 50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < windowSize; i++) {
             window.offer(randomDouble());
         }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
index af68667..ccd4dcb 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
@@ -19,10 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.serialdiff;
 
-import com.google.common.collect.EvictingQueue;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
@@ -160,7 +160,7 @@ public class SerialDiffIT extends ESIntegTestCase {
      */
     private void setupExpected(MetricTarget target) {
         ArrayList<Double> values = new ArrayList<>(numBuckets);
-        EvictingQueue<Double> lagWindow = EvictingQueue.create(lag);
+        EvictingQueue<Double> lagWindow = new EvictingQueue<>(lag);
 
         int counter = 0;
         for (PipelineAggregationHelperTests.MockBucket mockBucket : mockHisto) {
diff --git a/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java b/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java
index 4804842..f3aff00 100644
--- a/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java
+++ b/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.search.fetch;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.PostingsEnum;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.util.BytesRef;
@@ -27,7 +28,10 @@ import org.elasticsearch.action.termvectors.TermVectorsRequest;
 import org.elasticsearch.action.termvectors.TermVectorsResponse;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.termvectors.TermVectorsService;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.search.SearchHitField;
 import org.elasticsearch.search.SearchModule;
@@ -46,8 +50,8 @@ import java.util.Collection;
 import java.util.HashMap;
 import java.util.Map;
 
-import static java.util.Collections.singletonMap;
 import static org.elasticsearch.client.Requests.indexRequest;
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
 import static org.hamcrest.Matchers.equalTo;
@@ -135,7 +139,7 @@ public class FetchSubPhasePluginIT extends ESIntegTestCase {
 
         @Override
         public Map<String, ? extends SearchParseElement> parseElements() {
-            return singletonMap("term_vectors_fetch", new TermVectorsFetchParseElement());
+            return ImmutableMap.of("term_vectors_fetch", new TermVectorsFetchParseElement());
         }
 
         @Override
diff --git a/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java b/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
index c2c2782..983fb52 100644
--- a/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
+++ b/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
@@ -823,33 +823,4 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
 
         }
     }
-
-    @Test
-    public void testExplainString() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate("test").addMapping(
-                "type1",
-                jsonBuilder().startObject().startObject("type1").startObject("properties").startObject("test").field("type", "string")
-                        .endObject().startObject("num").field("type", "double").endObject().endObject().endObject().endObject()));
-        ensureYellow();
-
-
-        client().prepareIndex().setType("type1").setId("1").setIndex("test")
-                .setSource(jsonBuilder().startObject().field("test", "value").array("num", 0.5, 0.7).endObject()).get();
-
-        refresh();
-
-        SearchResponse response = client().search(
-                searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                        searchSource().explain(true)
-                                .query(functionScoreQuery(termQuery("test", "value"), new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(gaussDecayFunction("num", 1.0, 5.0, 1.0)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(linearDecayFunction("num", 1.0, 5.0, 1.0)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(exponentialDecayFunction("num", 1.0, 5.0, 1.0))
-                                }).boostMode(CombineFunction.REPLACE)))).get();
-        String explanation = response.getHits().getAt(0).getExplanation().toString();
-        assertThat(explanation, containsString(" 1.0 = exp(-0.5*pow(MIN[Math.max(Math.abs(0.5(=doc value) - 1.0(=origin))) - 1.0(=offset), 0), Math.max(Math.abs(0.7(=doc value) - 1.0(=origin))) - 1.0(=offset), 0)],2.0)/18.033688011112044)"));
-        assertThat(explanation, containsString("1.0 = max(0.0, ((10.0 - MIN[Math.max(Math.abs(0.5(=doc value) - 1.0(=origin))) - 1.0(=offset), 0), Math.max(Math.abs(0.7(=doc value) - 1.0(=origin))) - 1.0(=offset), 0)])/10.0)"));
-        assertThat(explanation, containsString("1.0 = exp(- MIN[Math.max(Math.abs(0.5(=doc value) - 1.0(=origin))) - 1.0(=offset), 0), Math.max(Math.abs(0.7(=doc value) - 1.0(=origin))) - 1.0(=offset), 0)] * 0.13862943611198905)"));
-
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/search/query/ExistsMissingIT.java b/core/src/test/java/org/elasticsearch/search/query/ExistsMissingIT.java
index 349197d..4d85f8b 100644
--- a/core/src/test/java/org/elasticsearch/search/query/ExistsMissingIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/ExistsMissingIT.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.search.query;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.elasticsearch.action.explain.ExplainResponse;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
@@ -30,14 +32,11 @@ import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.test.ESIntegTestCase;
 
 import java.util.ArrayList;
-import java.util.HashMap;
 import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Locale;
 import java.util.Map;
 
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.singletonMap;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
@@ -91,17 +90,14 @@ public class ExistsMissingIT extends ESIntegTestCase {
 
         assertAcked(client().admin().indices().prepareCreate("idx").addMapping("type", mapping));
         @SuppressWarnings("unchecked")
-        Map<String, Object> barObject = new HashMap<>();
-        barObject.put("foo", "bar");
-        barObject.put("bar", singletonMap("bar", "foo"));
         final Map<String, Object>[] sources = new Map[] {
                 // simple property
-                singletonMap("foo", "bar"),
+                ImmutableMap.of("foo", "bar"),
                 // object fields
-                singletonMap("bar", barObject),
-                singletonMap("bar", singletonMap("baz", 42)),
+                ImmutableMap.of("bar", ImmutableMap.of("foo", "bar", "bar", ImmutableMap.of("bar", "foo"))),
+                ImmutableMap.of("bar", ImmutableMap.of("baz", 42)),
                 // empty doc
-                emptyMap()
+                ImmutableMap.of()
         };
         List<IndexRequestBuilder> reqs = new ArrayList<IndexRequestBuilder>();
         for (Map<String, Object> source : sources) {
diff --git a/core/src/test/java/org/elasticsearch/search/scroll/SearchScrollIT.java b/core/src/test/java/org/elasticsearch/search/scroll/SearchScrollIT.java
index e4e7a69..4aeb416 100644
--- a/core/src/test/java/org/elasticsearch/search/scroll/SearchScrollIT.java
+++ b/core/src/test/java/org/elasticsearch/search/scroll/SearchScrollIT.java
@@ -27,7 +27,10 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.UncategorizedExecutionException;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.rest.action.search.RestClearScrollAction;
@@ -294,6 +297,7 @@ public class SearchScrollIT extends ESIntegTestCase {
         assertThat(clearResponse.isSucceeded(), is(true));
         assertThat(clearResponse.getNumFreed(), greaterThan(0));
         assertThat(clearResponse.status(), equalTo(RestStatus.OK));
+        assertToXContentResponse(clearResponse, true, clearResponse.getNumFreed());
 
         assertThrows(client().prepareSearchScroll(searchResponse1.getScrollId()).setScroll(TimeValue.timeValueMinutes(2)), RestStatus.NOT_FOUND);
         assertThrows(client().prepareSearchScroll(searchResponse2.getScrollId()).setScroll(TimeValue.timeValueMinutes(2)), RestStatus.NOT_FOUND);
@@ -310,6 +314,7 @@ public class SearchScrollIT extends ESIntegTestCase {
         assertThat(response.isSucceeded(), is(true));
         assertThat(response.getNumFreed(), equalTo(0));
         assertThat(response.status(), equalTo(RestStatus.NOT_FOUND));
+        assertToXContentResponse(response, true, response.getNumFreed());
     }
 
     @Test
@@ -404,6 +409,7 @@ public class SearchScrollIT extends ESIntegTestCase {
         assertThat(clearResponse.isSucceeded(), is(true));
         assertThat(clearResponse.getNumFreed(), greaterThan(0));
         assertThat(clearResponse.status(), equalTo(RestStatus.OK));
+        assertToXContentResponse(clearResponse, true, clearResponse.getNumFreed());
 
         assertThrows(internalCluster().transportClient().prepareSearchScroll(searchResponse1.getScrollId()).setScroll(TimeValue.timeValueMinutes(2)), RestStatus.NOT_FOUND);
         assertThrows(internalCluster().transportClient().prepareSearchScroll(searchResponse2.getScrollId()).setScroll(TimeValue.timeValueMinutes(2)), RestStatus.NOT_FOUND);
@@ -593,4 +599,19 @@ public class SearchScrollIT extends ESIntegTestCase {
         }
     }
 
+    private void assertToXContentResponse(ClearScrollResponse response, boolean succeed, int numFreed) throws IOException {
+        XContentBuilder builder = XContentFactory.jsonBuilder();
+        builder.startObject();
+        response.toXContent(builder, ToXContent.EMPTY_PARAMS);
+        builder.endObject();
+
+        BytesReference bytesReference = builder.bytes();
+        Map<String, Object> map;
+        try (XContentParser parser = XContentFactory.xContent(bytesReference).createParser(bytesReference)) {
+            map = parser.map();
+        }
+
+        assertThat(map.get("succeeded"), is(succeed));
+        assertThat(map.get("num_freed"), equalTo(numFreed));
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/similarity/SimilarityIT.java b/core/src/test/java/org/elasticsearch/similarity/SimilarityIT.java
index 229f2a8..d486cdb 100644
--- a/core/src/test/java/org/elasticsearch/similarity/SimilarityIT.java
+++ b/core/src/test/java/org/elasticsearch/similarity/SimilarityIT.java
@@ -30,7 +30,6 @@ import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.not;
 
 public class SimilarityIT  extends ESIntegTestCase {
-    
 
     @Test
     public void testCustomBM25Similarity() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
index 7f38715..206dfee 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.snapshots;
 
+import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.Version;
@@ -28,11 +29,7 @@ import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotRes
 import org.elasticsearch.action.admin.cluster.snapshots.delete.DeleteSnapshotResponse;
 import org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsResponse;
 import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;
-import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotIndexShardStage;
-import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotIndexShardStatus;
-import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotIndexStatus;
-import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotStatus;
-import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotsStatusResponse;
+import org.elasticsearch.action.admin.cluster.snapshots.status.*;
 import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;
 import org.elasticsearch.action.admin.indices.flush.FlushResponse;
 import org.elasticsearch.action.admin.indices.settings.get.GetSettingsResponse;
@@ -82,24 +79,8 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
 import static org.elasticsearch.index.shard.IndexShard.INDEX_REFRESH_INTERVAL;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAliasesExist;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAliasesMissing;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAllSuccessful;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertBlocked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertIndexTemplateExists;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertIndexTemplateMissing;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
-import static org.hamcrest.Matchers.allOf;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.lessThan;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
-import static org.hamcrest.Matchers.startsWith;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 public class SharedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTestCase {
 
@@ -1843,7 +1824,7 @@ public class SharedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTestCas
             @Override
             public ClusterState execute(ClusterState currentState) {
                 // Simulate orphan snapshot
-                ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableOpenMap.builder();
+                ImmutableMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableMap.builder();
                 shards.put(new ShardId("test-idx", 0), new ShardSnapshotStatus("unknown-node", State.ABORTED));
                 shards.put(new ShardId("test-idx", 1), new ShardSnapshotStatus("unknown-node", State.ABORTED));
                 shards.put(new ShardId("test-idx", 2), new ShardSnapshotStatus("unknown-node", State.ABORTED));
diff --git a/core/src/test/java/org/elasticsearch/test/CompositeTestCluster.java b/core/src/test/java/org/elasticsearch/test/CompositeTestCluster.java
index 843fc2e..8b14ef0 100644
--- a/core/src/test/java/org/elasticsearch/test/CompositeTestCluster.java
+++ b/core/src/test/java/org/elasticsearch/test/CompositeTestCluster.java
@@ -19,7 +19,6 @@
 package org.elasticsearch.test;
 
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-import com.google.common.collect.Iterators;
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.action.admin.cluster.node.stats.NodeStats;
 import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
@@ -31,11 +30,7 @@ import org.elasticsearch.common.transport.TransportAddress;
 
 import java.io.IOException;
 import java.net.InetSocketAddress;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.Random;
+import java.util.*;
 import java.util.stream.Collectors;
 
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoTimeout;
@@ -247,7 +242,7 @@ public class CompositeTestCluster extends TestCluster {
 
     @Override
     public synchronized Iterator<Client> iterator() {
-        return Iterators.singletonIterator(client());
+        return Collections.singleton(client()).iterator();
     }
 
     /**
diff --git a/core/src/test/java/org/elasticsearch/test/ESTestCase.java b/core/src/test/java/org/elasticsearch/test/ESTestCase.java
index 8bbd978..78d004f 100644
--- a/core/src/test/java/org/elasticsearch/test/ESTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESTestCase.java
@@ -41,7 +41,6 @@ import org.elasticsearch.bootstrap.BootstrapForTesting;
 import org.elasticsearch.cache.recycler.MockPageCacheRecycler;
 import org.elasticsearch.client.Requests;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.routing.DjbHashFunction;
 import org.elasticsearch.common.io.PathUtils;
 import org.elasticsearch.common.io.PathUtilsForTesting;
 import org.elasticsearch.common.logging.ESLogger;
@@ -549,9 +548,6 @@ public abstract class ESTestCase extends LuceneTestCase {
     /** Return consistent index settings for the provided index version. */
     public static Settings.Builder settings(Version version) {
         Settings.Builder builder = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, version);
-        if (version.before(Version.V_2_0_0_beta1)) {
-            builder.put(IndexMetaData.SETTING_LEGACY_ROUTING_HASH_FUNCTION, DjbHashFunction.class);
-        }
         return builder;
     }
 
diff --git a/core/src/test/java/org/elasticsearch/test/InternalTestCluster.java b/core/src/test/java/org/elasticsearch/test/InternalTestCluster.java
index f7c44a5..22973d4 100644
--- a/core/src/test/java/org/elasticsearch/test/InternalTestCluster.java
+++ b/core/src/test/java/org/elasticsearch/test/InternalTestCluster.java
@@ -1852,7 +1852,7 @@ public final class InternalTestCluster extends TestCluster {
             for (NodeAndClient nodeAndClient : nodes.values()) {
                 final IndicesFieldDataCache fdCache = getInstanceFromNode(IndicesFieldDataCache.class, nodeAndClient.node);
                 // Clean up the cache, ensuring that entries' listeners have been called
-                fdCache.getCache().cleanUp();
+                fdCache.getCache().refresh();
 
                 final String name = nodeAndClient.name;
                 final CircuitBreakerService breakerService = getInstanceFromNode(CircuitBreakerService.class, nodeAndClient.node);
diff --git a/core/src/test/java/org/elasticsearch/test/rest/section/ApiCallSection.java b/core/src/test/java/org/elasticsearch/test/rest/section/ApiCallSection.java
index da6c0b3..852d71c 100644
--- a/core/src/test/java/org/elasticsearch/test/rest/section/ApiCallSection.java
+++ b/core/src/test/java/org/elasticsearch/test/rest/section/ApiCallSection.java
@@ -18,13 +18,9 @@
  */
 package org.elasticsearch.test.rest.section;
 
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
+import com.google.common.collect.ImmutableMap;
 
-import static java.util.Collections.unmodifiableMap;
+import java.util.*;
 
 /**
  * Represents a test fragment that contains the information needed to call an api
@@ -45,7 +41,7 @@ public class ApiCallSection {
 
     public Map<String, String> getParams() {
         //make sure we never modify the parameters once returned
-        return unmodifiableMap(params);
+        return ImmutableMap.copyOf(params);
     }
 
     public void addParam(String key, String value) {
diff --git a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
index c423fb7..50cb00a 100644
--- a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
+++ b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.transport;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
@@ -41,12 +42,8 @@ import java.util.concurrent.Semaphore;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicReference;
 
-import static java.util.Collections.emptyMap;
 import static org.elasticsearch.transport.TransportRequestOptions.options;
-import static org.hamcrest.Matchers.endsWith;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.*;
 
 /**
  *
@@ -74,12 +71,12 @@ public abstract class AbstractSimpleTransportTestCase extends ESTestCase {
                 Settings.builder().put("name", "TS_A", TransportService.SETTING_TRACE_LOG_INCLUDE, "", TransportService.SETTING_TRACE_LOG_EXCLUDE, "NOTHING").build(),
                 version0, new NamedWriteableRegistry()
         );
-        nodeA = new DiscoveryNode("TS_A", "TS_A", serviceA.boundAddress().publishAddress(), emptyMap(), version0);
+        nodeA = new DiscoveryNode("TS_A", "TS_A", serviceA.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), version0);
         serviceB = build(
                 Settings.builder().put("name", "TS_B", TransportService.SETTING_TRACE_LOG_INCLUDE, "", TransportService.SETTING_TRACE_LOG_EXCLUDE, "NOTHING").build(),
                 version1, new NamedWriteableRegistry()
         );
-        nodeB = new DiscoveryNode("TS_B", "TS_B", serviceB.boundAddress().publishAddress(), emptyMap(), version1);
+        nodeB = new DiscoveryNode("TS_B", "TS_B", serviceB.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), version1);
 
         // wait till all nodes are properly connected and the event has been sent, so tests in this class
         // will not get this callback called on the connections done in this setup
diff --git a/core/src/test/java/org/elasticsearch/transport/netty/NettyScheduledPingTests.java b/core/src/test/java/org/elasticsearch/transport/netty/NettyScheduledPingTests.java
index 4e03b8e..d268907 100644
--- a/core/src/test/java/org/elasticsearch/transport/netty/NettyScheduledPingTests.java
+++ b/core/src/test/java/org/elasticsearch/transport/netty/NettyScheduledPingTests.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.transport.netty;
 
+import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
@@ -28,19 +29,11 @@ import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.transport.MockTransportService;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.BaseTransportResponseHandler;
-import org.elasticsearch.transport.TransportChannel;
-import org.elasticsearch.transport.TransportException;
-import org.elasticsearch.transport.TransportRequest;
-import org.elasticsearch.transport.TransportRequestHandler;
-import org.elasticsearch.transport.TransportRequestOptions;
-import org.elasticsearch.transport.TransportResponse;
-import org.elasticsearch.transport.TransportResponseOptions;
+import org.elasticsearch.transport.*;
 import org.junit.Test;
 
 import java.io.IOException;
 
-import static java.util.Collections.emptyMap;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.greaterThan;
 
@@ -64,8 +57,8 @@ public class NettyScheduledPingTests extends ESTestCase {
         MockTransportService serviceB = new MockTransportService(settings, nettyB, threadPool);
         serviceB.start();
 
-        DiscoveryNode nodeA = new DiscoveryNode("TS_A", "TS_A", serviceA.boundAddress().publishAddress(), emptyMap(), Version.CURRENT);
-        DiscoveryNode nodeB = new DiscoveryNode("TS_B", "TS_B", serviceB.boundAddress().publishAddress(), emptyMap(), Version.CURRENT);
+        DiscoveryNode nodeA = new DiscoveryNode("TS_A", "TS_A", serviceA.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), Version.CURRENT);
+        DiscoveryNode nodeB = new DiscoveryNode("TS_B", "TS_B", serviceB.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), Version.CURRENT);
 
         serviceA.connectToNode(nodeB);
         serviceB.connectToNode(nodeA);
diff --git a/core/src/test/java/org/elasticsearch/update/UpdateIT.java b/core/src/test/java/org/elasticsearch/update/UpdateIT.java
new file mode 100644
index 0000000..8c62d97
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/update/UpdateIT.java
@@ -0,0 +1,1210 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.update;
+
+import org.elasticsearch.ElasticsearchTimeoutException;
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.ActionRequestValidationException;
+import org.elasticsearch.action.admin.indices.alias.Alias;
+import org.elasticsearch.action.delete.DeleteRequest;
+import org.elasticsearch.action.delete.DeleteResponse;
+import org.elasticsearch.action.get.GetResponse;
+import org.elasticsearch.action.update.UpdateRequest;
+import org.elasticsearch.action.update.UpdateRequestBuilder;
+import org.elasticsearch.action.update.UpdateResponse;
+import org.elasticsearch.client.transport.NoNodeAvailableException;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.index.VersionType;
+import org.elasticsearch.index.engine.DocumentMissingException;
+import org.elasticsearch.index.engine.VersionConflictEngineException;
+import org.elasticsearch.index.shard.MergePolicyConfig;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.ScriptEngineService;
+import org.elasticsearch.script.ScriptModule;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.script.SearchScript;
+import org.elasticsearch.search.lookup.SearchLookup;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.*;
+import java.util.concurrent.CopyOnWriteArrayList;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.TimeUnit;
+
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
+import static org.hamcrest.Matchers.*;
+
+public class UpdateIT extends ESIntegTestCase {
+
+    public static class PutFieldValuesScriptPlugin extends Plugin {
+
+        public PutFieldValuesScriptPlugin() {
+        }
+
+        @Override
+        public String name() {
+            return PutFieldValuesScriptEngine.NAME;
+        }
+
+        @Override
+        public String description() {
+            return "Mock script engine for " + UpdateIT.class;
+        }
+
+        public void onModule(ScriptModule module) {
+            module.addScriptEngine(PutFieldValuesScriptEngine.class);
+        }
+
+    }
+
+    public static class PutFieldValuesScriptEngine implements ScriptEngineService {
+
+        public static final String NAME = "put_values";
+
+        @Override
+        public void close() throws IOException {
+        }
+
+        @Override
+        public String[] types() {
+            return new String[] { NAME };
+        }
+
+        @Override
+        public String[] extensions() {
+            return types();
+        }
+
+        @Override
+        public boolean sandboxed() {
+            return true;
+        }
+
+        @Override
+        public Object compile(String script) {
+            return new Object(); // unused
+        }
+
+        @Override
+        public ExecutableScript executable(CompiledScript compiledScript, Map<String, Object> originalParams) {
+            return new ExecutableScript() {
+
+                Map<String, Object> vars = new HashMap<>();
+
+                @Override
+                public void setNextVar(String name, Object value) {
+                    vars.put(name, value);
+                }
+
+                @Override
+                public Object run() {
+                    Map<String, Object> ctx = (Map<String, Object>) vars.get("ctx");
+                    assertNotNull(ctx);
+
+                    Map<String, Object> params = new HashMap<>(originalParams);
+
+                    Map<String, Object> newCtx = (Map<String, Object>) params.remove("_ctx");
+                    if (newCtx != null) {
+                        assertFalse(newCtx.containsKey("_source"));
+                        ctx.putAll(newCtx);
+                    }
+
+                    Map<String, Object> source = (Map<String, Object>) ctx.get("_source");
+                    source.putAll(params);
+
+                    return ctx;
+                }
+
+                @Override
+                public Object unwrap(Object value) {
+                    return value;
+                }
+
+            };
+        }
+
+        @Override
+        public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, Map<String, Object> vars) {
+            throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public void scriptRemoved(CompiledScript script) {
+        }
+
+    }
+
+    public static class FieldIncrementScriptPlugin extends Plugin {
+
+        public FieldIncrementScriptPlugin() {
+        }
+
+        @Override
+        public String name() {
+            return FieldIncrementScriptEngine.NAME;
+        }
+
+        @Override
+        public String description() {
+            return "Mock script engine for " + UpdateIT.class;
+        }
+
+        public void onModule(ScriptModule module) {
+            module.addScriptEngine(FieldIncrementScriptEngine.class);
+        }
+
+    }
+
+    public static class FieldIncrementScriptEngine implements ScriptEngineService {
+
+        public static final String NAME = "field_inc";
+
+        @Override
+        public void close() throws IOException {
+        }
+
+        @Override
+        public String[] types() {
+            return new String[] { NAME };
+        }
+
+        @Override
+        public String[] extensions() {
+            return types();
+        }
+
+        @Override
+        public boolean sandboxed() {
+            return true;
+        }
+
+        @Override
+        public Object compile(String script) {
+            return script;
+        }
+
+        @Override
+        public ExecutableScript executable(CompiledScript compiledScript, Map<String, Object> params) {
+            final String field = (String) compiledScript.compiled();
+            return new ExecutableScript() {
+
+                Map<String, Object> vars = new HashMap<>();
+
+                @Override
+                public void setNextVar(String name, Object value) {
+                    vars.put(name, value);
+                }
+
+                @Override
+                public Object run() {
+                    Map<String, Object> ctx = (Map<String, Object>) vars.get("ctx");
+                    assertNotNull(ctx);
+                    Map<String, Object> source = (Map<String, Object>) ctx.get("_source");
+                    Number currentValue = (Number) source.get(field);
+                    Number inc = params == null ? 1L : (Number) params.getOrDefault("inc", 1);
+                    source.put(field, currentValue.longValue() + inc.longValue());
+                    return ctx;
+                }
+
+                @Override
+                public Object unwrap(Object value) {
+                    return value;
+                }
+
+            };
+        }
+
+        @Override
+        public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, Map<String, Object> vars) {
+            throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public void scriptRemoved(CompiledScript script) {
+        }
+
+    }
+
+    public static class ScriptedUpsertScriptPlugin extends Plugin {
+
+        public ScriptedUpsertScriptPlugin() {
+        }
+
+        @Override
+        public String name() {
+            return ScriptedUpsertScriptEngine.NAME;
+        }
+
+        @Override
+        public String description() {
+            return "Mock script engine for " + UpdateIT.class + ".testScriptedUpsert";
+        }
+
+        public void onModule(ScriptModule module) {
+            module.addScriptEngine(ScriptedUpsertScriptEngine.class);
+        }
+
+    }
+
+    public static class ScriptedUpsertScriptEngine implements ScriptEngineService {
+
+        public static final String NAME = "scripted_upsert";
+
+        @Override
+        public void close() throws IOException {
+        }
+
+        @Override
+        public String[] types() {
+            return new String[] { NAME };
+        }
+
+        @Override
+        public String[] extensions() {
+            return types();
+        }
+
+        @Override
+        public boolean sandboxed() {
+            return true;
+        }
+
+        @Override
+        public Object compile(String script) {
+            return new Object(); // unused
+        }
+
+        @Override
+        public ExecutableScript executable(CompiledScript compiledScript, Map<String, Object> params) {
+            return new ExecutableScript() {
+
+                Map<String, Object> vars = new HashMap<>();
+
+                @Override
+                public void setNextVar(String name, Object value) {
+                    vars.put(name, value);
+                }
+
+                @Override
+                public Object run() {
+                    Map<String, Object> ctx = (Map<String, Object>) vars.get("ctx");
+                    assertNotNull(ctx);
+                    Map<String, Object> source = (Map<String, Object>) ctx.get("_source");
+                    Number payment = (Number) params.get("payment");
+                    Number oldBalance = (Number) source.get("balance");
+                    int deduction = "create".equals(ctx.get("op")) ? payment.intValue() / 2 : payment.intValue();
+                    source.put("balance", oldBalance.intValue() - deduction);
+                    return ctx;
+                }
+
+                @Override
+                public Object unwrap(Object value) {
+                    return value;
+                }
+
+            };
+        }
+
+        @Override
+        public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, Map<String, Object> vars) {
+            throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public void scriptRemoved(CompiledScript script) {
+        }
+
+    }
+
+    public static class ExtractContextInSourceScriptPlugin extends Plugin {
+
+        public ExtractContextInSourceScriptPlugin() {
+        }
+
+        @Override
+        public String name() {
+            return ExtractContextInSourceScriptEngine.NAME;
+        }
+
+        @Override
+        public String description() {
+            return "Mock script engine for " + UpdateIT.class;
+        }
+
+        public void onModule(ScriptModule module) {
+            module.addScriptEngine(ExtractContextInSourceScriptEngine.class);
+        }
+
+    }
+
+    public static class ExtractContextInSourceScriptEngine implements ScriptEngineService {
+
+        public static final String NAME = "extract_ctx";
+
+        @Override
+        public void close() throws IOException {
+        }
+
+        @Override
+        public String[] types() {
+            return new String[] { NAME };
+        }
+
+        @Override
+        public String[] extensions() {
+            return types();
+        }
+
+        @Override
+        public boolean sandboxed() {
+            return true;
+        }
+
+        @Override
+        public Object compile(String script) {
+            return new Object(); // unused
+        }
+
+        @Override
+        public ExecutableScript executable(CompiledScript compiledScript, Map<String, Object> params) {
+            return new ExecutableScript() {
+
+                Map<String, Object> vars = new HashMap<>();
+
+                @Override
+                public void setNextVar(String name, Object value) {
+                    vars.put(name, value);
+                }
+
+                @Override
+                public Object run() {
+                    Map<String, Object> ctx = (Map<String, Object>) vars.get("ctx");
+                    assertNotNull(ctx);
+
+                    Map<String, Object> source = (Map<String, Object>) ctx.get("_source");
+                    Map<String, Object> ctxWithoutSource = new HashMap<>(ctx);
+                    ctxWithoutSource.remove("_source");
+                    source.put("update_context", ctxWithoutSource);
+
+                    return ctx;
+                }
+
+                @Override
+                public Object unwrap(Object value) {
+                    return value;
+                }
+
+            };
+        }
+
+        @Override
+        public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, Map<String, Object> vars) {
+            throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public void scriptRemoved(CompiledScript script) {
+        }
+
+    }
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return Arrays.asList(
+                PutFieldValuesScriptPlugin.class,
+                FieldIncrementScriptPlugin.class,
+                ScriptedUpsertScriptPlugin.class,
+                ExtractContextInSourceScriptPlugin.class);
+    }
+
+    private void createTestIndex() throws Exception {
+        logger.info("--> creating index test");
+
+        assertAcked(prepareCreate("test").addAlias(new Alias("alias"))
+                .addMapping("type1", XContentFactory.jsonBuilder()
+                        .startObject()
+                        .startObject("type1")
+                        .startObject("_timestamp").field("enabled", true).endObject()
+                        .startObject("_ttl").field("enabled", true).endObject()
+                        .endObject()
+                        .endObject()));
+    }
+
+    @Test
+    public void testUpsert() throws Exception {
+        createTestIndex();
+        ensureGreen();
+
+        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setUpsert(XContentFactory.jsonBuilder().startObject().field("field", 1).endObject())
+                .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null))
+                .execute().actionGet();
+        assertTrue(updateResponse.isCreated());
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+
+        for (int i = 0; i < 5; i++) {
+            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("1"));
+        }
+
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setUpsert(XContentFactory.jsonBuilder().startObject().field("field", 1).endObject())
+                .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null))
+                .execute().actionGet();
+        assertFalse(updateResponse.isCreated());
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+
+        for (int i = 0; i < 5; i++) {
+            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("2"));
+        }
+    }
+
+    @Test
+    public void testScriptedUpsert() throws Exception {
+        createTestIndex();
+        ensureGreen();
+
+        // Script logic is
+        // 1) New accounts take balance from "balance" in upsert doc and first payment is charged at 50%
+        // 2) Existing accounts subtract full payment from balance stored in elasticsearch
+
+        int openingBalance=10;
+
+        Map<String, Object> params = new HashMap<>();
+        params.put("payment", 2);
+
+        // Pay money from what will be a new account and opening balance comes from upsert doc
+        // provided by client
+        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setUpsert(XContentFactory.jsonBuilder().startObject().field("balance", openingBalance).endObject())
+                .setScriptedUpsert(true)
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "scripted_upsert", params))
+                .execute().actionGet();
+        assertTrue(updateResponse.isCreated());
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+
+        for (int i = 0; i < 5; i++) {
+            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("balance").toString(), equalTo("9"));
+        }
+
+        // Now pay money for an existing account where balance is stored in es
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setUpsert(XContentFactory.jsonBuilder().startObject().field("balance", openingBalance).endObject())
+                .setScriptedUpsert(true)
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "scripted_upsert", params))
+                .execute().actionGet();
+        assertFalse(updateResponse.isCreated());
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+
+        for (int i = 0; i < 5; i++) {
+            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("balance").toString(), equalTo("7"));
+        }
+    }
+
+    @Test
+    public void testUpsertDoc() throws Exception {
+        createTestIndex();
+        ensureGreen();
+
+        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setDoc(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
+                .setDocAsUpsert(true)
+                .setFields("_source")
+                .execute().actionGet();
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult(), notNullValue());
+        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz"));
+    }
+
+    @Test
+    // See: https://github.com/elasticsearch/elasticsearch/issues/3265
+    public void testNotUpsertDoc() throws Exception {
+        createTestIndex();
+        ensureGreen();
+
+        assertThrows(client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setDoc(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
+                .setDocAsUpsert(false)
+                .setFields("_source")
+                .execute(), DocumentMissingException.class);
+    }
+
+    @Test
+    public void testUpsertFields() throws Exception {
+        createTestIndex();
+        ensureGreen();
+
+        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setUpsert(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("extra", "foo")))
+                .setFields("_source")
+                .execute().actionGet();
+
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult(), notNullValue());
+        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz"));
+        assertThat(updateResponse.getGetResult().sourceAsMap().get("extra"), nullValue());
+
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setUpsert(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("extra", "foo")))
+                .setFields("_source")
+                .execute().actionGet();
+
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult(), notNullValue());
+        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz"));
+        assertThat(updateResponse.getGetResult().sourceAsMap().get("extra").toString(), equalTo("foo"));
+    }
+
+    @Test
+    public void testVersionedUpdate() throws Exception {
+        assertAcked(prepareCreate("test").addAlias(new Alias("alias")));
+        ensureGreen();
+
+        index("test", "type", "1", "text", "value"); // version is now 1
+
+        assertThrows(client().prepareUpdate(indexOrAlias(), "type", "1")
+                        .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("text", "v2"))).setVersion(2)
+                        .execute(),
+                VersionConflictEngineException.class);
+
+        client().prepareUpdate(indexOrAlias(), "type", "1")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("text", "v2"))).setVersion(1).get();
+        assertThat(client().prepareGet("test", "type", "1").get().getVersion(), equalTo(2l));
+
+        // and again with a higher version..
+        client().prepareUpdate(indexOrAlias(), "type", "1")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("text", "v3"))).setVersion(2).get();
+
+        assertThat(client().prepareGet("test", "type", "1").get().getVersion(), equalTo(3l));
+
+        // after delete
+        client().prepareDelete("test", "type", "1").get();
+        assertThrows(client().prepareUpdate("test", "type", "1")
+                        .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("text", "v2"))).setVersion(3)
+                        .execute(),
+                DocumentMissingException.class);
+
+        // external versioning
+        client().prepareIndex("test", "type", "2").setSource("text", "value").setVersion(10).setVersionType(VersionType.EXTERNAL).get();
+
+        assertThrows(client().prepareUpdate(indexOrAlias(), "type", "2")
+                        .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("text", "v2"))).setVersion(2)
+                        .setVersionType(VersionType.EXTERNAL).execute(),
+                ActionRequestValidationException.class);
+
+
+        // With force version
+        client().prepareUpdate(indexOrAlias(), "type", "2")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE,  "put_values", Collections.singletonMap("text", "v10")))
+                .setVersion(10).setVersionType(VersionType.FORCE).get();
+
+        GetResponse get = get("test", "type", "2");
+        assertThat(get.getVersion(), equalTo(10l));
+        assertThat((String) get.getSource().get("text"), equalTo("v10"));
+
+        // upserts - the combination with versions is a bit weird. Test are here to ensure we do not change our behavior unintentionally
+
+        // With internal versions, tt means "if object is there with version X, update it or explode. If it is not there, index.
+        client().prepareUpdate(indexOrAlias(), "type", "3")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("text", "v2")))
+                .setVersion(10).setUpsert("{ \"text\": \"v0\" }").get();
+        get = get("test", "type", "3");
+        assertThat(get.getVersion(), equalTo(1l));
+        assertThat((String) get.getSource().get("text"), equalTo("v0"));
+
+        // retry on conflict is rejected:
+        assertThrows(client().prepareUpdate(indexOrAlias(), "type", "1").setVersion(10).setRetryOnConflict(5), ActionRequestValidationException.class);
+    }
+
+    @Test
+    public void testIndexAutoCreation() throws Exception {
+        UpdateResponse updateResponse = client().prepareUpdate("test", "type1", "1")
+                .setUpsert(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("extra", "foo")))
+                .setFields("_source")
+                .execute().actionGet();
+
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult(), notNullValue());
+        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz"));
+        assertThat(updateResponse.getGetResult().sourceAsMap().get("extra"), nullValue());
+    }
+
+    @Test
+    public void testUpdate() throws Exception {
+        createTestIndex();
+        ensureGreen();
+
+        try {
+            client().prepareUpdate(indexOrAlias(), "type1", "1")
+                    .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null)).execute().actionGet();
+            fail();
+        } catch (DocumentMissingException e) {
+            // all is well
+        }
+
+        client().prepareIndex("test", "type1", "1").setSource("field", 1).execute().actionGet();
+
+        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null)).execute().actionGet();
+        assertThat(updateResponse.getVersion(), equalTo(2L));
+        assertFalse(updateResponse.isCreated());
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+
+        for (int i = 0; i < 5; i++) {
+            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("2"));
+        }
+
+        Map<String, Object> params = new HashMap<>();
+        params.put("inc", 3);
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", params)).execute().actionGet();
+        assertThat(updateResponse.getVersion(), equalTo(3L));
+        assertFalse(updateResponse.isCreated());
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+
+        for (int i = 0; i < 5; i++) {
+            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("5"));
+        }
+
+        // check noop
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("_ctx", Collections.singletonMap("op", "none")))).execute().actionGet();
+        assertThat(updateResponse.getVersion(), equalTo(3L));
+        assertFalse(updateResponse.isCreated());
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+
+        for (int i = 0; i < 5; i++) {
+            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("5"));
+        }
+
+        // check delete
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("_ctx", Collections.singletonMap("op", "delete")))).execute().actionGet();
+        assertThat(updateResponse.getVersion(), equalTo(4L));
+        assertFalse(updateResponse.isCreated());
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+
+        for (int i = 0; i < 5; i++) {
+            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.isExists(), equalTo(false));
+        }
+
+        // check TTL is kept after an update without TTL
+        client().prepareIndex("test", "type1", "2").setSource("field", 1).setTTL(86400000L).setRefresh(true).execute().actionGet();
+        GetResponse getResponse = client().prepareGet("test", "type1", "2").setFields("_ttl").execute().actionGet();
+        long ttl = ((Number) getResponse.getField("_ttl").getValue()).longValue();
+        assertThat(ttl, greaterThan(0L));
+        client().prepareUpdate(indexOrAlias(), "type1", "2")
+                .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null)).execute().actionGet();
+        getResponse = client().prepareGet("test", "type1", "2").setFields("_ttl").execute().actionGet();
+        ttl = ((Number) getResponse.getField("_ttl").getValue()).longValue();
+        assertThat(ttl, greaterThan(0L));
+
+        // check TTL update
+        client().prepareUpdate(indexOrAlias(), "type1", "2")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("_ctx", Collections.singletonMap("_ttl", 3600000)))).execute().actionGet();
+        getResponse = client().prepareGet("test", "type1", "2").setFields("_ttl").execute().actionGet();
+        ttl = ((Number) getResponse.getField("_ttl").getValue()).longValue();
+        assertThat(ttl, greaterThan(0L));
+        assertThat(ttl, lessThanOrEqualTo(3600000L));
+
+        // check timestamp update
+        client().prepareIndex("test", "type1", "3").setSource("field", 1).setRefresh(true).execute().actionGet();
+        client().prepareUpdate(indexOrAlias(), "type1", "3")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("_ctx", Collections.singletonMap("_timestamp", "2009-11-15T14:12:12")))).execute()
+                .actionGet();
+        getResponse = client().prepareGet("test", "type1", "3").setFields("_timestamp").execute().actionGet();
+        long timestamp = ((Number) getResponse.getField("_timestamp").getValue()).longValue();
+        assertThat(timestamp, equalTo(1258294332000L));
+
+        // check fields parameter
+        client().prepareIndex("test", "type1", "1").setSource("field", 1).execute().actionGet();
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null)).setFields("_source", "field")
+                .execute().actionGet();
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult(), notNullValue());
+        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult().sourceRef(), notNullValue());
+        assertThat(updateResponse.getGetResult().field("field").getValue(), notNullValue());
+
+        // check updates without script
+        // add new field
+        client().prepareIndex("test", "type1", "1").setSource("field", 1).execute().actionGet();
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1").setDoc(XContentFactory.jsonBuilder().startObject().field("field2", 2).endObject()).execute().actionGet();
+        for (int i = 0; i < 5; i++) {
+            getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("1"));
+            assertThat(getResponse.getSourceAsMap().get("field2").toString(), equalTo("2"));
+        }
+
+        // change existing field
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1").setDoc(XContentFactory.jsonBuilder().startObject().field("field", 3).endObject()).execute().actionGet();
+        for (int i = 0; i < 5; i++) {
+            getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("3"));
+            assertThat(getResponse.getSourceAsMap().get("field2").toString(), equalTo("2"));
+        }
+
+        // recursive map
+        Map<String, Object> testMap = new HashMap<>();
+        Map<String, Object> testMap2 = new HashMap<>();
+        Map<String, Object> testMap3 = new HashMap<>();
+        testMap3.put("commonkey", testMap);
+        testMap3.put("map3", 5);
+        testMap2.put("map2", 6);
+        testMap.put("commonkey", testMap2);
+        testMap.put("map1", 8);
+
+        client().prepareIndex("test", "type1", "1").setSource("map", testMap).execute().actionGet();
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1").setDoc(XContentFactory.jsonBuilder().startObject().field("map", testMap3).endObject()).execute().actionGet();
+        for (int i = 0; i < 5; i++) {
+            getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            Map map1 = (Map) getResponse.getSourceAsMap().get("map");
+            assertThat(map1.size(), equalTo(3));
+            assertThat(map1.containsKey("map1"), equalTo(true));
+            assertThat(map1.containsKey("map3"), equalTo(true));
+            assertThat(map1.containsKey("commonkey"), equalTo(true));
+            Map map2 = (Map) map1.get("commonkey");
+            assertThat(map2.size(), equalTo(3));
+            assertThat(map2.containsKey("map1"), equalTo(true));
+            assertThat(map2.containsKey("map2"), equalTo(true));
+            assertThat(map2.containsKey("commonkey"), equalTo(true));
+        }
+    }
+
+    @Test
+    public void testUpdateRequestWithBothScriptAndDoc() throws Exception {
+        createTestIndex();
+        ensureGreen();
+
+        try {
+            client().prepareUpdate(indexOrAlias(), "type1", "1")
+                    .setDoc(XContentFactory.jsonBuilder().startObject().field("field", 1).endObject())
+                    .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null))
+                    .execute().actionGet();
+            fail("Should have thrown ActionRequestValidationException");
+        } catch (ActionRequestValidationException e) {
+            assertThat(e.validationErrors().size(), equalTo(1));
+            assertThat(e.validationErrors().get(0), containsString("can't provide both script and doc"));
+            assertThat(e.getMessage(), containsString("can't provide both script and doc"));
+        }
+    }
+
+    @Test
+    public void testUpdateRequestWithScriptAndShouldUpsertDoc() throws Exception {
+        createTestIndex();
+        ensureGreen();
+        try {
+            client().prepareUpdate(indexOrAlias(), "type1", "1")
+                    .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null))
+                    .setDocAsUpsert(true)
+                    .execute().actionGet();
+            fail("Should have thrown ActionRequestValidationException");
+        } catch (ActionRequestValidationException e) {
+            assertThat(e.validationErrors().size(), equalTo(1));
+            assertThat(e.validationErrors().get(0), containsString("doc must be specified if doc_as_upsert is enabled"));
+            assertThat(e.getMessage(), containsString("doc must be specified if doc_as_upsert is enabled"));
+        }
+    }
+
+    @Test
+    public void testContextVariables() throws Exception {
+        assertAcked(prepareCreate("test").addAlias(new Alias("alias"))
+                        .addMapping("type1", XContentFactory.jsonBuilder()
+                                .startObject()
+                                .startObject("type1")
+                                .startObject("_timestamp").field("enabled", true).endObject()
+                                .startObject("_ttl").field("enabled", true).endObject()
+                                .endObject()
+                                .endObject())
+                        .addMapping("subtype1", XContentFactory.jsonBuilder()
+                                .startObject()
+                                .startObject("subtype1")
+                                .startObject("_parent").field("type", "type1").endObject()
+                                .startObject("_timestamp").field("enabled", true).endObject()
+                                .startObject("_ttl").field("enabled", true).endObject()
+                                .endObject()
+                                .endObject())
+        );
+        ensureGreen();
+
+        // Index some documents
+        long timestamp = System.currentTimeMillis();
+        client().prepareIndex()
+                .setIndex("test")
+                .setType("type1")
+                .setId("parentId1")
+                .setTimestamp(String.valueOf(timestamp-1))
+                .setSource("field1", 0, "content", "bar")
+                .execute().actionGet();
+
+        long ttl = 10000;
+        client().prepareIndex()
+                .setIndex("test")
+                .setType("subtype1")
+                .setId("id1")
+                .setParent("parentId1")
+                .setRouting("routing1")
+                .setTimestamp(String.valueOf(timestamp))
+                .setTTL(ttl)
+                .setSource("field1", 1, "content", "foo")
+                .execute().actionGet();
+
+        // Update the first object and note context variables values
+        UpdateResponse updateResponse = client().prepareUpdate("test", "subtype1", "id1")
+                .setRouting("routing1")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "extract_ctx", null))
+                .execute().actionGet();
+
+        assertEquals(2, updateResponse.getVersion());
+
+        GetResponse getResponse = client().prepareGet("test", "subtype1", "id1").setRouting("routing1").execute().actionGet();
+        Map<String, Object> updateContext = (Map<String, Object>) getResponse.getSourceAsMap().get("update_context");
+        assertEquals("test", updateContext.get("_index"));
+        assertEquals("subtype1", updateContext.get("_type"));
+        assertEquals("id1", updateContext.get("_id"));
+        assertEquals(1, updateContext.get("_version"));
+        assertEquals("parentId1", updateContext.get("_parent"));
+        assertEquals("routing1", updateContext.get("_routing"));
+        assertThat(((Integer) updateContext.get("_ttl")).longValue(), allOf(greaterThanOrEqualTo(ttl-3000), lessThanOrEqualTo(ttl)));
+
+        // Idem with the second object
+        updateResponse = client().prepareUpdate("test", "type1", "parentId1")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "extract_ctx", null))
+                .execute().actionGet();
+
+        assertEquals(2, updateResponse.getVersion());
+
+        getResponse = client().prepareGet("test", "type1", "parentId1").execute().actionGet();
+        updateContext = (Map<String, Object>) getResponse.getSourceAsMap().get("update_context");
+        assertEquals("test", updateContext.get("_index"));
+        assertEquals("type1", updateContext.get("_type"));
+        assertEquals("parentId1", updateContext.get("_id"));
+        assertEquals(1, updateContext.get("_version"));
+        assertNull(updateContext.get("_parent"));
+        assertNull(updateContext.get("_routing"));
+        assertNull(updateContext.get("_ttl"));
+    }
+
+    @Test
+    public void testConcurrentUpdateWithRetryOnConflict() throws Exception {
+        final boolean useBulkApi = randomBoolean();
+        createTestIndex();
+        ensureGreen();
+
+        int numberOfThreads = scaledRandomIntBetween(2,5);
+        final CountDownLatch latch = new CountDownLatch(numberOfThreads);
+        final CountDownLatch startLatch = new CountDownLatch(1);
+        final int numberOfUpdatesPerThread = scaledRandomIntBetween(100, 10000);
+        final List<Throwable> failures = new CopyOnWriteArrayList<>();
+        for (int i = 0; i < numberOfThreads; i++) {
+            Runnable r = new Runnable() {
+
+                @Override
+                public void run() {
+                    try {
+                        startLatch.await();
+                        for (int i = 0; i < numberOfUpdatesPerThread; i++) {
+                            if (useBulkApi) {
+                                UpdateRequestBuilder updateRequestBuilder = client().prepareUpdate(indexOrAlias(), "type1", Integer.toString(i))
+                                        .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null))
+                                        .setRetryOnConflict(Integer.MAX_VALUE)
+                                        .setUpsert(jsonBuilder().startObject().field("field", 1).endObject());
+                                client().prepareBulk().add(updateRequestBuilder).execute().actionGet();
+                            } else {
+                                client().prepareUpdate(indexOrAlias(), "type1", Integer.toString(i))
+                                        .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null))
+                                        .setRetryOnConflict(Integer.MAX_VALUE)
+                                        .setUpsert(jsonBuilder().startObject().field("field", 1).endObject())
+                                        .execute().actionGet();
+                            }
+                        }
+                    } catch (Throwable e) {
+                        failures.add(e);
+                    } finally {
+                        latch.countDown();
+                    }
+                }
+
+            };
+            new Thread(r).start();
+        }
+        startLatch.countDown();
+        latch.await();
+        for (Throwable throwable : failures) {
+            logger.info("Captured failure on concurrent update:", throwable);
+        }
+        assertThat(failures.size(), equalTo(0));
+        for (int i = 0; i < numberOfUpdatesPerThread; i++) {
+            GetResponse response = client().prepareGet("test", "type1", Integer.toString(i)).execute().actionGet();
+            assertThat(response.getId(), equalTo(Integer.toString(i)));
+            assertThat(response.isExists(), equalTo(true));
+            assertThat(response.getVersion(), equalTo((long) numberOfThreads));
+            assertThat((Integer) response.getSource().get("field"), equalTo(numberOfThreads));
+        }
+    }
+
+    @Test
+    public void stressUpdateDeleteConcurrency() throws Exception {
+        //We create an index with merging disabled so that deletes don't get merged away
+        assertAcked(prepareCreate("test")
+                .addMapping("type1", XContentFactory.jsonBuilder()
+                        .startObject()
+                        .startObject("type1")
+                        .startObject("_timestamp").field("enabled", true).endObject()
+                        .startObject("_ttl").field("enabled", true).endObject()
+                        .endObject()
+                        .endObject())
+                .setSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_ENABLED, false)));
+        ensureGreen();
+
+        final int numberOfThreads = scaledRandomIntBetween(3,5);
+        final int numberOfIdsPerThread = scaledRandomIntBetween(3,10);
+        final int numberOfUpdatesPerId = scaledRandomIntBetween(10,100);
+        final int retryOnConflict = randomIntBetween(0,1);
+        final CountDownLatch latch = new CountDownLatch(numberOfThreads);
+        final CountDownLatch startLatch = new CountDownLatch(1);
+        final List<Throwable> failures = new CopyOnWriteArrayList<>();
+
+        final class UpdateThread extends Thread {
+            final Map<Integer,Integer> failedMap = new HashMap<>();
+            final int numberOfIds;
+            final int updatesPerId;
+            final int maxUpdateRequests = numberOfIdsPerThread*numberOfUpdatesPerId;
+            final int maxDeleteRequests = numberOfIdsPerThread*numberOfUpdatesPerId;
+            private final Semaphore updateRequestsOutstanding = new Semaphore(maxUpdateRequests);
+            private final Semaphore deleteRequestsOutstanding = new Semaphore(maxDeleteRequests);
+
+            public UpdateThread(int numberOfIds, int updatesPerId) {
+                this.numberOfIds = numberOfIds;
+                this.updatesPerId = updatesPerId;
+            }
+
+            final class UpdateListener implements ActionListener<UpdateResponse> {
+                int id;
+
+                public UpdateListener(int id) {
+                    this.id = id;
+                }
+
+                @Override
+                public void onResponse(UpdateResponse updateResponse) {
+                    updateRequestsOutstanding.release(1);
+                }
+
+                @Override
+                public void onFailure(Throwable e) {
+                    synchronized (failedMap) {
+                        incrementMapValue(id, failedMap);
+                    }
+                    updateRequestsOutstanding.release(1);
+                }
+
+            }
+
+            final class DeleteListener implements ActionListener<DeleteResponse> {
+                int id;
+
+                public DeleteListener(int id) {
+                    this.id = id;
+                }
+
+                @Override
+                public void onResponse(DeleteResponse deleteResponse) {
+                    deleteRequestsOutstanding.release(1);
+                }
+
+                @Override
+                public void onFailure(Throwable e) {
+                    synchronized (failedMap) {
+                        incrementMapValue(id, failedMap);
+                    }
+                    deleteRequestsOutstanding.release(1);
+                }
+            }
+
+            @Override
+            public void run(){
+                try {
+                    startLatch.await();
+                    boolean hasWaitedForNoNode = false;
+                    for (int j = 0; j < numberOfIds; j++) {
+                        for (int k = 0; k < numberOfUpdatesPerId; ++k) {
+                            updateRequestsOutstanding.acquire();
+                            try {
+                                UpdateRequest ur = client().prepareUpdate("test", "type1", Integer.toString(j))
+                                        .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null))
+                                        .setRetryOnConflict(retryOnConflict)
+                                        .setUpsert(jsonBuilder().startObject().field("field", 1).endObject())
+                                        .request();
+                                client().update(ur, new UpdateListener(j));
+                            } catch (NoNodeAvailableException nne) {
+                                updateRequestsOutstanding.release();
+                                synchronized (failedMap) {
+                                    incrementMapValue(j, failedMap);
+                                }
+                                if (hasWaitedForNoNode) {
+                                    throw nne;
+                                }
+                                logger.warn("Got NoNodeException waiting for 1 second for things to recover.");
+                                hasWaitedForNoNode = true;
+                                Thread.sleep(1000);
+                            }
+
+                            try {
+                                deleteRequestsOutstanding.acquire();
+                                DeleteRequest dr = client().prepareDelete("test", "type1", Integer.toString(j)).request();
+                                client().delete(dr, new DeleteListener(j));
+                            } catch (NoNodeAvailableException nne) {
+                                deleteRequestsOutstanding.release();
+                                synchronized (failedMap) {
+                                    incrementMapValue(j, failedMap);
+                                }
+                                if (hasWaitedForNoNode) {
+                                    throw nne;
+                                }
+                                logger.warn("Got NoNodeException waiting for 1 second for things to recover.");
+                                hasWaitedForNoNode = true;
+                                Thread.sleep(1000); //Wait for no-node to clear
+                            }
+                        }
+                    }
+                } catch (Throwable e) {
+                    logger.error("Something went wrong", e);
+                    failures.add(e);
+                } finally {
+                    try {
+                        waitForOutstandingRequests(TimeValue.timeValueSeconds(60), updateRequestsOutstanding, maxUpdateRequests, "Update");
+                        waitForOutstandingRequests(TimeValue.timeValueSeconds(60), deleteRequestsOutstanding, maxDeleteRequests, "Delete");
+                    } catch (ElasticsearchTimeoutException ete) {
+                        failures.add(ete);
+                    }
+                    latch.countDown();
+                }
+            }
+
+            private void incrementMapValue(int j, Map<Integer,Integer> map) {
+                if (!map.containsKey(j)) {
+                    map.put(j, 0);
+                }
+                map.put(j, map.get(j) + 1);
+            }
+
+            private void waitForOutstandingRequests(TimeValue timeOut, Semaphore requestsOutstanding, int maxRequests, String name) {
+                long start = System.currentTimeMillis();
+                do {
+                    long msRemaining = timeOut.getMillis() - (System.currentTimeMillis() - start);
+                    logger.info("[{}] going to try and acquire [{}] in [{}]ms [{}] available to acquire right now",name, maxRequests,msRemaining, requestsOutstanding.availablePermits());
+                    try {
+                        requestsOutstanding.tryAcquire(maxRequests, msRemaining, TimeUnit.MILLISECONDS );
+                        return;
+                    } catch (InterruptedException ie) {
+                        //Just keep swimming
+                    }
+                } while ((System.currentTimeMillis() - start) < timeOut.getMillis());
+                throw new ElasticsearchTimeoutException("Requests were still outstanding after the timeout [" + timeOut + "] for type [" + name + "]" );
+            }
+        }
+        final List<UpdateThread> threads = new ArrayList<>();
+
+        for (int i = 0; i < numberOfThreads; i++) {
+            UpdateThread ut = new UpdateThread(numberOfIdsPerThread, numberOfUpdatesPerId);
+            ut.start();
+            threads.add(ut);
+        }
+
+        startLatch.countDown();
+        latch.await();
+
+        for (UpdateThread ut : threads){
+            ut.join(); //Threads should have finished because of the latch.await
+        }
+
+        //If are no errors every request received a response otherwise the test would have timedout
+        //aquiring the request outstanding semaphores.
+        for (Throwable throwable : failures) {
+            logger.info("Captured failure on concurrent update:", throwable);
+        }
+
+        assertThat(failures.size(), equalTo(0));
+
+        //Upsert all the ids one last time to make sure they are available at get time
+        //This means that we add 1 to the expected versions and attempts
+        //All the previous operations should be complete or failed at this point
+        for (int i = 0; i < numberOfIdsPerThread; ++i) {
+            UpdateResponse ur = client().prepareUpdate("test", "type1", Integer.toString(i))
+                    .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null))
+                .setRetryOnConflict(Integer.MAX_VALUE)
+                .setUpsert(jsonBuilder().startObject().field("field", 1).endObject())
+                .execute().actionGet();
+        }
+
+        refresh();
+
+        for (int i = 0; i < numberOfIdsPerThread; ++i) {
+            int totalFailures = 0;
+            GetResponse response = client().prepareGet("test", "type1", Integer.toString(i)).execute().actionGet();
+            if (response.isExists()) {
+                assertThat(response.getId(), equalTo(Integer.toString(i)));
+                int expectedVersion = (numberOfThreads * numberOfUpdatesPerId * 2) + 1;
+                for (UpdateThread ut : threads) {
+                    if (ut.failedMap.containsKey(i)) {
+                        totalFailures += ut.failedMap.get(i);
+                    }
+                }
+                expectedVersion -= totalFailures;
+                logger.error("Actual version [{}] Expected version [{}] Total failures [{}]", response.getVersion(), expectedVersion, totalFailures);
+                assertThat(response.getVersion(), equalTo((long) expectedVersion));
+                assertThat(response.getVersion() + totalFailures,
+                        equalTo(
+                                (long)((numberOfUpdatesPerId * numberOfThreads * 2) + 1)
+                ));
+            }
+        }
+    }
+
+    private static String indexOrAlias() {
+        return randomBoolean() ? "test" : "alias";
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/validate/RenderSearchTemplateIT.java b/core/src/test/java/org/elasticsearch/validate/RenderSearchTemplateIT.java
index b22dff7..10812c1 100644
--- a/core/src/test/java/org/elasticsearch/validate/RenderSearchTemplateIT.java
+++ b/core/src/test/java/org/elasticsearch/validate/RenderSearchTemplateIT.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.validate;
 
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateResponse;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateResponse;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.settings.Settings;
@@ -61,7 +61,7 @@ public class RenderSearchTemplateIT extends ESIntegTestCase {
         params.put("value", "bar");
         params.put("size", 20);
         Template template = new Template(TEMPLATE_CONTENTS, ScriptType.INLINE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        RenderSearchTemplateResponse response = client().admin().indices().prepareRenderSearchTemplate().template(template).get();
+        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
         assertThat(response, notNullValue());
         BytesReference source = response.source();
         assertThat(source, notNullValue());
@@ -75,7 +75,7 @@ public class RenderSearchTemplateIT extends ESIntegTestCase {
         params.put("value", "baz");
         params.put("size", 100);
         template = new Template(TEMPLATE_CONTENTS, ScriptType.INLINE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        response = client().admin().indices().prepareRenderSearchTemplate().template(template).get();
+        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
         assertThat(response, notNullValue());
         source = response.source();
         assertThat(source, notNullValue());
@@ -91,7 +91,7 @@ public class RenderSearchTemplateIT extends ESIntegTestCase {
         params.put("value", "bar");
         params.put("size", 20);
         Template template = new Template("index_template_1", ScriptType.INDEXED, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        RenderSearchTemplateResponse response = client().admin().indices().prepareRenderSearchTemplate().template(template).get();
+        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
         assertThat(response, notNullValue());
         BytesReference source = response.source();
         assertThat(source, notNullValue());
@@ -105,7 +105,7 @@ public class RenderSearchTemplateIT extends ESIntegTestCase {
         params.put("value", "baz");
         params.put("size", 100);
         template = new Template("index_template_1", ScriptType.INDEXED, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        response = client().admin().indices().prepareRenderSearchTemplate().template(template).get();
+        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
         assertThat(response, notNullValue());
         source = response.source();
         assertThat(source, notNullValue());
@@ -121,7 +121,7 @@ public class RenderSearchTemplateIT extends ESIntegTestCase {
         params.put("value", "bar");
         params.put("size", 20);
         Template template = new Template("file_template_1", ScriptType.FILE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        RenderSearchTemplateResponse response = client().admin().indices().prepareRenderSearchTemplate().template(template).get();
+        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
         assertThat(response, notNullValue());
         BytesReference source = response.source();
         assertThat(source, notNullValue());
@@ -135,7 +135,7 @@ public class RenderSearchTemplateIT extends ESIntegTestCase {
         params.put("value", "baz");
         params.put("size", 100);
         template = new Template("file_template_1", ScriptType.FILE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        response = client().admin().indices().prepareRenderSearchTemplate().template(template).get();
+        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
         assertThat(response, notNullValue());
         source = response.source();
         assertThat(source, notNullValue());
diff --git a/core/src/test/java/org/elasticsearch/versioning/SimpleVersioningIT.java b/core/src/test/java/org/elasticsearch/versioning/SimpleVersioningIT.java
index 5296e76..93c29e0 100644
--- a/core/src/test/java/org/elasticsearch/versioning/SimpleVersioningIT.java
+++ b/core/src/test/java/org/elasticsearch/versioning/SimpleVersioningIT.java
@@ -18,15 +18,6 @@
  */
 package org.elasticsearch.versioning;
 
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Random;
-import java.util.Set;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicInteger;
-
 import org.apache.lucene.util.TestUtil;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.action.bulk.BulkResponse;
@@ -37,12 +28,15 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.lucene.uid.Versions;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.VersionType;
-import org.elasticsearch.index.engine.DocumentAlreadyExistsException;
 import org.elasticsearch.index.engine.FlushNotAllowedEngineException;
 import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
+import java.util.*;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicInteger;
+
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
@@ -100,7 +94,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         }
 
         // deleting with a lower version works.
-        long v= randomIntBetween(12,14);
+        long v = randomIntBetween(12, 14);
         DeleteResponse deleteResponse = client().prepareDelete("test", "type", "1").setVersion(v).setVersionType(VersionType.FORCE).get();
         assertThat(deleteResponse.isFound(), equalTo(true));
         assertThat(deleteResponse.getVersion(), equalTo(v));
@@ -136,7 +130,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
                 VersionConflictEngineException.class);
 
         // Delete with a higher or equal version deletes all versions up to the given one.
-        long v= randomIntBetween(14,17);
+        long v = randomIntBetween(14, 17);
         DeleteResponse deleteResponse = client().prepareDelete("test", "type", "1").setVersion(v).setVersionType(VersionType.EXTERNAL_GTE).execute().actionGet();
         assertThat(deleteResponse.isFound(), equalTo(true));
         assertThat(deleteResponse.getVersion(), equalTo(v));
@@ -165,7 +159,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         assertThat(indexResponse.getVersion(), equalTo(14l));
 
         assertThrows(client().prepareIndex("test", "type", "1").setSource("field1", "value1_1").setVersion(13).setVersionType(VersionType.EXTERNAL).execute(),
-                     VersionConflictEngineException.class);
+                VersionConflictEngineException.class);
 
         if (randomBoolean()) {
             refresh();
@@ -176,8 +170,8 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // deleting with a lower version fails.
         assertThrows(
-            client().prepareDelete("test", "type", "1").setVersion(2).setVersionType(VersionType.EXTERNAL).execute(),
-            VersionConflictEngineException.class);
+                client().prepareDelete("test", "type", "1").setVersion(2).setVersionType(VersionType.EXTERNAL).execute(),
+                VersionConflictEngineException.class);
 
         // Delete with a higher version deletes all versions up to the given one.
         DeleteResponse deleteResponse = client().prepareDelete("test", "type", "1").setVersion(17).setVersionType(VersionType.EXTERNAL).execute().actionGet();
@@ -186,8 +180,8 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // Deleting with a lower version keeps on failing after a delete.
         assertThrows(
-            client().prepareDelete("test", "type", "1").setVersion(2).setVersionType(VersionType.EXTERNAL).execute(),
-            VersionConflictEngineException.class);
+                client().prepareDelete("test", "type", "1").setVersion(2).setVersionType(VersionType.EXTERNAL).execute(),
+                VersionConflictEngineException.class);
 
 
         // But delete with a higher version is OK.
@@ -206,8 +200,8 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         assertThat(deleteResponse.getVersion(), equalTo(20l));
 
         // Make sure that the next delete will be GC. Note we do it on the index settings so it will be cleaned up
-        HashMap<String,Object> newSettings = new HashMap<>();
-        newSettings.put("index.gc_deletes",-1);
+        HashMap<String, Object> newSettings = new HashMap<>();
+        newSettings.put("index.gc_deletes", -1);
         client().admin().indices().prepareUpdateSettings("test").setSettings(newSettings).execute().actionGet();
 
         Thread.sleep(300); // gc works based on estimated sampled time. Give it a chance...
@@ -221,7 +215,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
     public void testRequireUnitsOnUpdateSettings() throws Exception {
         createIndex("test");
         ensureGreen();
-        HashMap<String,Object> newSettings = new HashMap<>();
+        HashMap<String, Object> newSettings = new HashMap<>();
         newSettings.put("index.gc_deletes", "42");
         try {
             client().admin().indices().prepareUpdateSettings("test").setSettings(newSettings).execute().actionGet();
@@ -262,22 +256,12 @@ public class SimpleVersioningIT extends ESIntegTestCase {
                 VersionConflictEngineException.class);
 
         assertThrows(
-            client().prepareIndex("test", "type", "1").setSource("field1", "value1_1").setVersion(1).execute(),
-            VersionConflictEngineException.class);
-
-        assertThrows(
-            client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(1).execute(),
-                VersionConflictEngineException.class);
-        assertThrows(
-            client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(1).execute(),
+                client().prepareIndex("test", "type", "1").setSource("field1", "value1_1").setVersion(1).execute(),
                 VersionConflictEngineException.class);
 
         assertThrows(
-                client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(2).execute(),
-                DocumentAlreadyExistsException.class);
-        assertThrows(
-                client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(2).execute(),
-                DocumentAlreadyExistsException.class);
+                client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").execute(),
+                VersionConflictEngineException.class);
 
 
         assertThrows(client().prepareDelete("test", "type", "1").setVersion(1).execute(), VersionConflictEngineException.class);
@@ -334,10 +318,8 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         assertThrows(client().prepareIndex("test", "type", "1").setSource("field1", "value1_1").setVersion(1).execute(),
                 VersionConflictEngineException.class);
-        assertThrows(client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(1).execute(),
-                VersionConflictEngineException.class);
 
-        assertThrows(client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(1).execute(),
+        assertThrows(client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").execute(),
                 VersionConflictEngineException.class);
 
         assertThrows(client().prepareDelete("test", "type", "1").setVersion(1).execute(), VersionConflictEngineException.class);
@@ -377,90 +359,94 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         IDSource ids;
         final Random random = getRandom();
         switch (random.nextInt(6)) {
-        case 0:
-            // random simple
-            if (VERBOSE) {
-                System.out.println("TEST: use random simple ids");
-            }
-            ids = new IDSource() {
+            case 0:
+                // random simple
+                if (VERBOSE) {
+                    System.out.println("TEST: use random simple ids");
+                }
+                ids = new IDSource() {
                     @Override
                     public String next() {
                         return TestUtil.randomSimpleString(random);
                     }
                 };
-            break;
-        case 1:
-            // random realistic unicode
-            if (VERBOSE) {
-                System.out.println("TEST: use random realistic unicode ids");
-            }
-            ids = new IDSource() {
+                break;
+            case 1:
+                // random realistic unicode
+                if (VERBOSE) {
+                    System.out.println("TEST: use random realistic unicode ids");
+                }
+                ids = new IDSource() {
                     @Override
                     public String next() {
                         return TestUtil.randomRealisticUnicodeString(random);
                     }
                 };
-            break;
-        case 2:
-            // sequential
-            if (VERBOSE) {
-                System.out.println("TEST: use seuquential ids");
-            }
-            ids = new IDSource() {
+                break;
+            case 2:
+                // sequential
+                if (VERBOSE) {
+                    System.out.println("TEST: use seuquential ids");
+                }
+                ids = new IDSource() {
                     int upto;
+
                     @Override
                     public String next() {
                         return Integer.toString(upto++);
                     }
                 };
-            break;
-        case 3:
-            // zero-pad sequential
-            if (VERBOSE) {
-                System.out.println("TEST: use zero-pad seuquential ids");
-            }
-            ids = new IDSource() {
+                break;
+            case 3:
+                // zero-pad sequential
+                if (VERBOSE) {
+                    System.out.println("TEST: use zero-pad seuquential ids");
+                }
+                ids = new IDSource() {
                     final int radix = TestUtil.nextInt(random, Character.MIN_RADIX, Character.MAX_RADIX);
                     final String zeroPad = String.format(Locale.ROOT, "%0" + TestUtil.nextInt(random, 4, 20) + "d", 0);
                     int upto;
+
                     @Override
                     public String next() {
                         String s = Integer.toString(upto++);
                         return zeroPad.substring(zeroPad.length() - s.length()) + s;
                     }
                 };
-            break;
-        case 4:
-            // random long
-            if (VERBOSE) {
-                System.out.println("TEST: use random long ids");
-            }
-            ids = new IDSource() {
+                break;
+            case 4:
+                // random long
+                if (VERBOSE) {
+                    System.out.println("TEST: use random long ids");
+                }
+                ids = new IDSource() {
                     final int radix = TestUtil.nextInt(random, Character.MIN_RADIX, Character.MAX_RADIX);
                     int upto;
+
                     @Override
                     public String next() {
                         return Long.toString(random.nextLong() & 0x3ffffffffffffffL, radix);
                     }
                 };
-            break;
-        case 5:
-            // zero-pad random long
-            if (VERBOSE) {
-                System.out.println("TEST: use zero-pad random long ids");
-            }
-            ids = new IDSource() {
+                break;
+            case 5:
+                // zero-pad random long
+                if (VERBOSE) {
+                    System.out.println("TEST: use zero-pad random long ids");
+                }
+                ids = new IDSource() {
                     final int radix = TestUtil.nextInt(random, Character.MIN_RADIX, Character.MAX_RADIX);
                     final String zeroPad = String.format(Locale.ROOT, "%015d", 0);
                     int upto;
+
                     @Override
                     public String next() {
                         return Long.toString(random.nextLong() & 0x3ffffffffffffffL, radix);
                     }
                 };
-            break;
-        default:
-            throw new AssertionError();
+                break;
+            default:
+                throw new AssertionError();
         }
 
         return ids;
@@ -530,7 +516,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
             } else {
                 sb.append("  response: null");
             }
-            
+
             return sb.toString();
         }
     }
@@ -547,7 +533,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         // TODO: not great we don't test deletes GC here:
 
         // We test deletes, but can't rely on wall-clock delete GC:
-        HashMap<String,Object> newSettings = new HashMap<>();
+        HashMap<String, Object> newSettings = new HashMap<>();
         newSettings.put("index.gc_deletes", "1000000h");
         assertAcked(client().admin().indices().prepareUpdateSettings("test").setSettings(newSettings).execute().actionGet());
 
@@ -584,14 +570,14 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // Attach random versions to them:
         long version = 0;
-        final IDAndVersion[] idVersions = new IDAndVersion[TestUtil.nextInt(random, numIDs/2, numIDs*(TEST_NIGHTLY ? 8 : 2))];
-        final Map<String,IDAndVersion> truth = new HashMap<>();
+        final IDAndVersion[] idVersions = new IDAndVersion[TestUtil.nextInt(random, numIDs / 2, numIDs * (TEST_NIGHTLY ? 8 : 2))];
+        final Map<String, IDAndVersion> truth = new HashMap<>();
 
         if (VERBOSE) {
             System.out.println("TEST: use " + numIDs + " ids; " + idVersions.length + " operations");
         }
 
-        for(int i=0;i<idVersions.length;i++) {
+        for (int i = 0; i < idVersions.length; i++) {
 
             if (useMonotonicVersion) {
                 version += TestUtil.nextInt(random, 1, 10);
@@ -612,7 +598,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         }
 
         // Shuffle
-        for(int i = idVersions.length - 1; i > 0; i--) {
+        for (int i = idVersions.length - 1; i > 0; i--) {
             int index = random.nextInt(i + 1);
             IDAndVersion x = idVersions[index];
             idVersions[index] = idVersions[i];
@@ -620,7 +606,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         }
 
         if (VERBOSE) {
-            for(IDAndVersion idVersion : idVersions) {
+            for (IDAndVersion idVersion : idVersions) {
                 System.out.println("id=" + idVersion.id + " version=" + idVersion.version + " delete?=" + idVersion.delete + " truth?=" + (truth.get(idVersion.id) == idVersion));
             }
         }
@@ -629,109 +615,87 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         final CountDownLatch startingGun = new CountDownLatch(1);
         Thread[] threads = new Thread[TestUtil.nextInt(random, 1, TEST_NIGHTLY ? 20 : 5)];
         final long startTime = System.nanoTime();
-        for(int i=0;i<threads.length;i++) {
+        for (int i = 0; i < threads.length; i++) {
             final int threadID = i;
             threads[i] = new Thread() {
-                    @Override
-                    public void run() {
-                        try {
-                            //final Random threadRandom = RandomizedContext.current().getRandom();
-                            final Random threadRandom = getRandom();
-                            startingGun.await();
-                            while (true) {
-
-                                // TODO: sometimes use bulk:
-
-                                int index = upto.getAndIncrement();
-                                if (index >= idVersions.length) {
-                                    break;
-                                }
-                                if (VERBOSE && index % 100 == 0) {
-                                    System.out.println(Thread.currentThread().getName() + ": index=" + index);
-                                }
-                                IDAndVersion idVersion = idVersions[index];
-
-                                String id = idVersion.id;
-                                idVersion.threadID = threadID;
-                                idVersion.indexStartTime = System.nanoTime()-startTime;
-                                long version = idVersion.version;
-                                if (idVersion.delete) {
-                                    try {
-                                        idVersion.response = client().prepareDelete("test", "type", id)
+                @Override
+                public void run() {
+                    try {
+                        //final Random threadRandom = RandomizedContext.current().getRandom();
+                        final Random threadRandom = getRandom();
+                        startingGun.await();
+                        while (true) {
+
+                            // TODO: sometimes use bulk:
+
+                            int index = upto.getAndIncrement();
+                            if (index >= idVersions.length) {
+                                break;
+                            }
+                            if (VERBOSE && index % 100 == 0) {
+                                System.out.println(Thread.currentThread().getName() + ": index=" + index);
+                            }
+                            IDAndVersion idVersion = idVersions[index];
+
+                            String id = idVersion.id;
+                            idVersion.threadID = threadID;
+                            idVersion.indexStartTime = System.nanoTime() - startTime;
+                            long version = idVersion.version;
+                            if (idVersion.delete) {
+                                try {
+                                    idVersion.response = client().prepareDelete("test", "type", id)
                                             .setVersion(version)
                                             .setVersionType(VersionType.EXTERNAL).execute().actionGet();
-                                    } catch (VersionConflictEngineException vcee) {
-                                        // OK: our version is too old
-                                        assertThat(version, lessThanOrEqualTo(truth.get(id).version));
-                                        idVersion.versionConflict = true;
-                                    }
-                                } else {
-                                    for (int x=0;x<2;x++) {
-                                        // Try create first:
-                                    
-                                        IndexRequest.OpType op;
-                                        if (x == 0) {
-                                            op = IndexRequest.OpType.CREATE;
-                                        } else {
-                                            op = IndexRequest.OpType.INDEX;
-                                        }
-                                    
-                                        // index document
-                                        try {
-                                            idVersion.response = client().prepareIndex("test", "type", id)
-                                                .setSource("foo", "bar")
-                                                .setOpType(op)
-                                                .setVersion(version)
-                                                .setVersionType(VersionType.EXTERNAL).execute().actionGet();
-                                            break;
-                                        } catch (DocumentAlreadyExistsException daee) {
-                                            if (x == 0) {
-                                                // OK: id was already indexed by another thread, now use index:
-                                                idVersion.alreadyExists = true;
-                                            } else {
-                                                // Should not happen with op=INDEX:
-                                                throw daee;
-                                            }
-                                        } catch (VersionConflictEngineException vcee) {
-                                            // OK: our version is too old
-                                            assertThat(version, lessThanOrEqualTo(truth.get(id).version));
-                                            idVersion.versionConflict = true;
-                                        }
-                                    }
+                                } catch (VersionConflictEngineException vcee) {
+                                    // OK: our version is too old
+                                    assertThat(version, lessThanOrEqualTo(truth.get(id).version));
+                                    idVersion.versionConflict = true;
                                 }
-                                idVersion.indexFinishTime = System.nanoTime()-startTime;
-
-                                if (threadRandom.nextInt(100) == 7) {
-                                    System.out.println(threadID + ": TEST: now refresh at " + (System.nanoTime()-startTime));
-                                    refresh();
-                                    System.out.println(threadID + ": TEST: refresh done at " + (System.nanoTime()-startTime));
+                            } else {
+                                try {
+                                    idVersion.response = client().prepareIndex("test", "type", id)
+                                            .setSource("foo", "bar")
+                                            .setVersion(version).setVersionType(VersionType.EXTERNAL).get();
+
+                                } catch (VersionConflictEngineException vcee) {
+                                    // OK: our version is too old
+                                    assertThat(version, lessThanOrEqualTo(truth.get(id).version));
+                                    idVersion.versionConflict = true;
                                 }
-                                if (threadRandom.nextInt(100) == 7) {
-                                    System.out.println(threadID + ": TEST: now flush at " + (System.nanoTime()-startTime));
-                                    try {
-                                        flush();
-                                    } catch (FlushNotAllowedEngineException fnaee) {
-                                        // OK
-                                    }
-                                    System.out.println(threadID + ": TEST: flush done at " + (System.nanoTime()-startTime));
+                            }
+                            idVersion.indexFinishTime = System.nanoTime() - startTime;
+
+                            if (threadRandom.nextInt(100) == 7) {
+                                System.out.println(threadID + ": TEST: now refresh at " + (System.nanoTime() - startTime));
+                                refresh();
+                                System.out.println(threadID + ": TEST: refresh done at " + (System.nanoTime() - startTime));
+                            }
+                            if (threadRandom.nextInt(100) == 7) {
+                                System.out.println(threadID + ": TEST: now flush at " + (System.nanoTime() - startTime));
+                                try {
+                                    flush();
+                                } catch (FlushNotAllowedEngineException fnaee) {
+                                    // OK
                                 }
+                                System.out.println(threadID + ": TEST: flush done at " + (System.nanoTime() - startTime));
                             }
-                        } catch (Exception e) {
-                            throw new RuntimeException(e);
                         }
+                    } catch (Exception e) {
+                        throw new RuntimeException(e);
                     }
-                };
+                }
+            };
             threads[i].start();
         }
 
         startingGun.countDown();
-        for(Thread thread : threads) {
+        for (Thread thread : threads) {
             thread.join();
         }
 
         // Verify against truth:
         boolean failed = false;
-        for(String id : ids) {
+        for (String id : ids) {
             long expected;
             IDAndVersion idVersion = truth.get(id);
             if (idVersion != null && idVersion.delete == false) {
@@ -748,7 +712,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         if (failed) {
             System.out.println("All versions:");
-            for(int i=0;i<idVersions.length;i++) {
+            for (int i = 0; i < idVersions.length; i++) {
                 System.out.println("i=" + i + " " + idVersions[i]);
             }
             fail("wrong versions for some IDs");
@@ -760,36 +724,36 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // We require only one shard for this test, so that the 2nd delete provokes pruning the deletes map:
         client()
-            .admin()
-            .indices()
-            .prepareCreate("test")
-            .setSettings(Settings.settingsBuilder()
-                         .put("index.number_of_shards", 1))
-            .execute().
-            actionGet();
+                .admin()
+                .indices()
+                .prepareCreate("test")
+                .setSettings(Settings.settingsBuilder()
+                        .put("index.number_of_shards", 1))
+                .execute().
+                actionGet();
 
         ensureGreen();
 
-        HashMap<String,Object> newSettings = new HashMap<>();
+        HashMap<String, Object> newSettings = new HashMap<>();
         newSettings.put("index.gc_deletes", "10ms");
         newSettings.put("index.refresh_interval", "-1");
         client()
-            .admin()
-            .indices()
-            .prepareUpdateSettings("test")
-            .setSettings(newSettings)
-            .execute()
-            .actionGet();
+                .admin()
+                .indices()
+                .prepareUpdateSettings("test")
+                .setSettings(newSettings)
+                .execute()
+                .actionGet();
 
         // Index a doc:
         client()
-            .prepareIndex("test", "type", "id")
-            .setSource("foo", "bar")
-            .setOpType(IndexRequest.OpType.INDEX)
-            .setVersion(10)
-            .setVersionType(VersionType.EXTERNAL)
-            .execute()
-            .actionGet();
+                .prepareIndex("test", "type", "id")
+                .setSource("foo", "bar")
+                .setOpType(IndexRequest.OpType.INDEX)
+                .setVersion(10)
+                .setVersionType(VersionType.EXTERNAL)
+                .execute()
+                .actionGet();
 
         if (randomBoolean()) {
             // Force refresh so the add is sometimes visible in the searcher:
@@ -798,20 +762,20 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // Delete it
         client()
-            .prepareDelete("test", "type", "id")
-            .setVersion(11)
-            .setVersionType(VersionType.EXTERNAL)
-            .execute()
-            .actionGet();
+                .prepareDelete("test", "type", "id")
+                .setVersion(11)
+                .setVersionType(VersionType.EXTERNAL)
+                .execute()
+                .actionGet();
 
         // Real-time get should reflect delete:
         assertThat("doc should have been deleted",
-                   client()
-                   .prepareGet("test", "type", "id")
-                   .execute()
-                   .actionGet()
-                   .getVersion(),
-                   equalTo(-1L));
+                client()
+                        .prepareGet("test", "type", "id")
+                        .execute()
+                        .actionGet()
+                        .getVersion(),
+                equalTo(-1L));
 
         // ThreadPool.estimatedTimeInMillis has default granularity of 200 msec, so we must sleep at least that long; sleep much longer in
         // case system is busy:
@@ -819,20 +783,20 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // Delete an unrelated doc (provokes pruning deletes from versionMap)
         client()
-            .prepareDelete("test", "type", "id2")
-            .setVersion(11)
-            .setVersionType(VersionType.EXTERNAL)
-            .execute()
-            .actionGet();
+                .prepareDelete("test", "type", "id2")
+                .setVersion(11)
+                .setVersionType(VersionType.EXTERNAL)
+                .execute()
+                .actionGet();
 
         // Real-time get should still reflect delete:
         assertThat("doc should have been deleted",
-                   client()
-                   .prepareGet("test", "type", "id")
-                   .execute()
-                   .actionGet()
-                   .getVersion(),
-                   equalTo(-1L));
+                client()
+                        .prepareGet("test", "type", "id")
+                        .execute()
+                        .actionGet()
+                        .getVersion(),
+                equalTo(-1L));
     }
 
     @Test
@@ -842,25 +806,25 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         ensureGreen();
 
         // We test deletes, but can't rely on wall-clock delete GC:
-        HashMap<String,Object> newSettings = new HashMap<>();
+        HashMap<String, Object> newSettings = new HashMap<>();
         newSettings.put("index.gc_deletes", "0ms");
         client()
-            .admin()
-            .indices()
-            .prepareUpdateSettings("test")
-            .setSettings(newSettings)
-            .execute()
-            .actionGet();
+                .admin()
+                .indices()
+                .prepareUpdateSettings("test")
+                .setSettings(newSettings)
+                .execute()
+                .actionGet();
 
         // Index a doc:
         client()
-            .prepareIndex("test", "type", "id")
-            .setSource("foo", "bar")
-            .setOpType(IndexRequest.OpType.INDEX)
-            .setVersion(10)
-            .setVersionType(VersionType.EXTERNAL)
-            .execute()
-            .actionGet();
+                .prepareIndex("test", "type", "id")
+                .setSource("foo", "bar")
+                .setOpType(IndexRequest.OpType.INDEX)
+                .setVersion(10)
+                .setVersionType(VersionType.EXTERNAL)
+                .execute()
+                .actionGet();
 
         if (randomBoolean()) {
             // Force refresh so the add is sometimes visible in the searcher:
@@ -869,19 +833,19 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // Delete it
         client()
-            .prepareDelete("test", "type", "id")
-            .setVersion(11)
-            .setVersionType(VersionType.EXTERNAL)
-            .execute()
-            .actionGet();
+                .prepareDelete("test", "type", "id")
+                .setVersion(11)
+                .setVersionType(VersionType.EXTERNAL)
+                .execute()
+                .actionGet();
 
         // Real-time get should reflect delete even though index.gc_deletes is 0:
         assertThat("doc should have been deleted",
-                   client()
-                   .prepareGet("test", "type", "id")
-                   .execute()
-                   .actionGet()
-                   .getVersion(),
-                   equalTo(-1L));
+                client()
+                        .prepareGet("test", "type", "id")
+                        .execute()
+                        .actionGet()
+                        .getVersion(),
+                equalTo(-1L));
     }
 }
diff --git a/core/src/test/resources/indices/bwc/index-0.90.0.Beta1.zip b/core/src/test/resources/indices/bwc/index-0.90.0.Beta1.zip
deleted file mode 100644
index 5bbdea4..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.0.Beta1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.0.RC1.zip b/core/src/test/resources/indices/bwc/index-0.90.0.RC1.zip
deleted file mode 100644
index d9072ce..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.0.RC1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.0.RC2.zip b/core/src/test/resources/indices/bwc/index-0.90.0.RC2.zip
deleted file mode 100644
index dce299b..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.0.RC2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.0.zip b/core/src/test/resources/indices/bwc/index-0.90.0.zip
deleted file mode 100644
index 3ec908d..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.1.zip b/core/src/test/resources/indices/bwc/index-0.90.1.zip
deleted file mode 100644
index 67db98f..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.10.zip b/core/src/test/resources/indices/bwc/index-0.90.10.zip
deleted file mode 100644
index 6bdb9f2..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.10.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.11.zip b/core/src/test/resources/indices/bwc/index-0.90.11.zip
deleted file mode 100644
index b5253f9..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.11.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.12.zip b/core/src/test/resources/indices/bwc/index-0.90.12.zip
deleted file mode 100644
index 0392049..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.12.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.13.zip b/core/src/test/resources/indices/bwc/index-0.90.13.zip
deleted file mode 100644
index 025b4c3..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.13.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.2.zip b/core/src/test/resources/indices/bwc/index-0.90.2.zip
deleted file mode 100644
index 413e08e..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.3.zip b/core/src/test/resources/indices/bwc/index-0.90.3.zip
deleted file mode 100644
index c31d4de..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.4.zip b/core/src/test/resources/indices/bwc/index-0.90.4.zip
deleted file mode 100644
index 8b07a92..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.4.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.5.zip b/core/src/test/resources/indices/bwc/index-0.90.5.zip
deleted file mode 100644
index dfd0fd0..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.5.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.6.zip b/core/src/test/resources/indices/bwc/index-0.90.6.zip
deleted file mode 100644
index 1f3cff2..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.6.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.7.zip b/core/src/test/resources/indices/bwc/index-0.90.7.zip
deleted file mode 100644
index 6d0e65c..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.7.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.8.zip b/core/src/test/resources/indices/bwc/index-0.90.8.zip
deleted file mode 100644
index 8ff8ac3..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.8.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.9.zip b/core/src/test/resources/indices/bwc/index-0.90.9.zip
deleted file mode 100644
index 4445b39..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.9.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.0.0.Beta1.zip b/core/src/test/resources/indices/bwc/index-1.0.0.Beta1.zip
deleted file mode 100644
index 167dde8..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.0.0.Beta1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.0.0.Beta2.zip b/core/src/test/resources/indices/bwc/index-1.0.0.Beta2.zip
deleted file mode 100644
index 95fbfef..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.0.0.Beta2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.0.0.RC1.zip b/core/src/test/resources/indices/bwc/index-1.0.0.RC1.zip
deleted file mode 100644
index 3ced97a..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.0.0.RC1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.0.0.RC2.zip b/core/src/test/resources/indices/bwc/index-1.0.0.RC2.zip
deleted file mode 100644
index 1298cfb..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.0.0.RC2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.0.0.zip b/core/src/test/resources/indices/bwc/index-1.0.0.zip
deleted file mode 100644
index 2cb9abc..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.0.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.0.1.zip b/core/src/test/resources/indices/bwc/index-1.0.1.zip
deleted file mode 100644
index 844271b..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.0.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.0.2.zip b/core/src/test/resources/indices/bwc/index-1.0.2.zip
deleted file mode 100644
index dd8e393..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.0.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.0.3.zip b/core/src/test/resources/indices/bwc/index-1.0.3.zip
deleted file mode 100644
index e4437ef..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.0.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.1.0.zip b/core/src/test/resources/indices/bwc/index-1.1.0.zip
deleted file mode 100644
index 4f05370..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.1.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.1.1.zip b/core/src/test/resources/indices/bwc/index-1.1.1.zip
deleted file mode 100644
index effeb94..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.1.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.1.2.zip b/core/src/test/resources/indices/bwc/index-1.1.2.zip
deleted file mode 100644
index bedffa4..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.1.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.2.0.zip b/core/src/test/resources/indices/bwc/index-1.2.0.zip
deleted file mode 100644
index 4644a38..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.2.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.2.1.zip b/core/src/test/resources/indices/bwc/index-1.2.1.zip
deleted file mode 100644
index 553b46b..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.2.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.2.2.zip b/core/src/test/resources/indices/bwc/index-1.2.2.zip
deleted file mode 100644
index 3f51a47..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.2.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.2.3.zip b/core/src/test/resources/indices/bwc/index-1.2.3.zip
deleted file mode 100644
index 8c8bfbd..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.2.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.2.4.zip b/core/src/test/resources/indices/bwc/index-1.2.4.zip
deleted file mode 100644
index e3a1519..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.2.4.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.0.zip b/core/src/test/resources/indices/bwc/index-1.3.0.zip
deleted file mode 100644
index d98958d..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.1.zip b/core/src/test/resources/indices/bwc/index-1.3.1.zip
deleted file mode 100644
index 167d0f4..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.2.zip b/core/src/test/resources/indices/bwc/index-1.3.2.zip
deleted file mode 100644
index 756eaf6..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.3.zip b/core/src/test/resources/indices/bwc/index-1.3.3.zip
deleted file mode 100644
index 8470dee..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.4.zip b/core/src/test/resources/indices/bwc/index-1.3.4.zip
deleted file mode 100644
index 2175012..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.4.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.5.zip b/core/src/test/resources/indices/bwc/index-1.3.5.zip
deleted file mode 100644
index 19d1e56..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.5.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.6.zip b/core/src/test/resources/indices/bwc/index-1.3.6.zip
deleted file mode 100644
index ad8e8bd..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.6.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.7.zip b/core/src/test/resources/indices/bwc/index-1.3.7.zip
deleted file mode 100644
index 3a645a9..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.7.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.8.zip b/core/src/test/resources/indices/bwc/index-1.3.8.zip
deleted file mode 100644
index f8ab0a2..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.8.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.9.zip b/core/src/test/resources/indices/bwc/index-1.3.9.zip
deleted file mode 100644
index 5ef35b2..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.9.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.4.0.Beta1.zip b/core/src/test/resources/indices/bwc/index-1.4.0.Beta1.zip
deleted file mode 100644
index 4546f5d..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.4.0.Beta1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.4.0.zip b/core/src/test/resources/indices/bwc/index-1.4.0.zip
deleted file mode 100644
index 467d19a..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.4.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.4.1.zip b/core/src/test/resources/indices/bwc/index-1.4.1.zip
deleted file mode 100644
index 2adbb28..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.4.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.4.2.zip b/core/src/test/resources/indices/bwc/index-1.4.2.zip
deleted file mode 100644
index 4fac208..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.4.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.4.3.zip b/core/src/test/resources/indices/bwc/index-1.4.3.zip
deleted file mode 100644
index 1a0d667..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.4.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.4.4.zip b/core/src/test/resources/indices/bwc/index-1.4.4.zip
deleted file mode 100644
index 0328a9e..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.4.4.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.4.5.zip b/core/src/test/resources/indices/bwc/index-1.4.5.zip
deleted file mode 100644
index eeb25ab..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.4.5.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.5.0.zip b/core/src/test/resources/indices/bwc/index-1.5.0.zip
deleted file mode 100644
index f1dab08..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.5.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.5.1.zip b/core/src/test/resources/indices/bwc/index-1.5.1.zip
deleted file mode 100644
index 342e311..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.5.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.5.2.zip b/core/src/test/resources/indices/bwc/index-1.5.2.zip
deleted file mode 100644
index fb36b19..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.5.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.6.0.zip b/core/src/test/resources/indices/bwc/index-1.6.0.zip
deleted file mode 100644
index 02a5806..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.6.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.6.1.zip b/core/src/test/resources/indices/bwc/index-1.6.1.zip
deleted file mode 100644
index 04820f9..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.6.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.6.2.zip b/core/src/test/resources/indices/bwc/index-1.6.2.zip
deleted file mode 100644
index af6ce56..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.6.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.7.0.zip b/core/src/test/resources/indices/bwc/index-1.7.0.zip
deleted file mode 100644
index 941be64..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.7.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.7.1.zip b/core/src/test/resources/indices/bwc/index-1.7.1.zip
deleted file mode 100644
index debd797..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.7.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.7.2.zip b/core/src/test/resources/indices/bwc/index-1.7.2.zip
deleted file mode 100644
index 18bb6c7..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.7.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-2.0.0-rc1.zip b/core/src/test/resources/indices/bwc/index-2.0.0-rc1.zip
new file mode 100644
index 0000000..44e78b4
Binary files /dev/null and b/core/src/test/resources/indices/bwc/index-2.0.0-rc1.zip differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.0.0.Beta2.zip b/core/src/test/resources/indices/bwc/repo-1.0.0.Beta2.zip
deleted file mode 100644
index 020f6f4..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.0.0.Beta2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.0.0.RC1.zip b/core/src/test/resources/indices/bwc/repo-1.0.0.RC1.zip
deleted file mode 100644
index a84c507..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.0.0.RC1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.0.0.RC2.zip b/core/src/test/resources/indices/bwc/repo-1.0.0.RC2.zip
deleted file mode 100644
index e5d65eb..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.0.0.RC2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.0.0.zip b/core/src/test/resources/indices/bwc/repo-1.0.0.zip
deleted file mode 100644
index 13f778d..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.0.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.0.1.zip b/core/src/test/resources/indices/bwc/repo-1.0.1.zip
deleted file mode 100644
index 76ed278..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.0.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.0.2.zip b/core/src/test/resources/indices/bwc/repo-1.0.2.zip
deleted file mode 100644
index 762eabf..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.0.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.0.3.zip b/core/src/test/resources/indices/bwc/repo-1.0.3.zip
deleted file mode 100644
index ba79dea..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.0.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.1.0.zip b/core/src/test/resources/indices/bwc/repo-1.1.0.zip
deleted file mode 100644
index cbf84c7..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.1.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.1.1.zip b/core/src/test/resources/indices/bwc/repo-1.1.1.zip
deleted file mode 100644
index 00ea044..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.1.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.1.2.zip b/core/src/test/resources/indices/bwc/repo-1.1.2.zip
deleted file mode 100644
index 18abd99..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.1.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.2.0.zip b/core/src/test/resources/indices/bwc/repo-1.2.0.zip
deleted file mode 100644
index f5e62a1..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.2.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.2.1.zip b/core/src/test/resources/indices/bwc/repo-1.2.1.zip
deleted file mode 100644
index 935e71b..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.2.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.2.2.zip b/core/src/test/resources/indices/bwc/repo-1.2.2.zip
deleted file mode 100644
index d69b22a..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.2.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.2.3.zip b/core/src/test/resources/indices/bwc/repo-1.2.3.zip
deleted file mode 100644
index 295f9f7..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.2.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.2.4.zip b/core/src/test/resources/indices/bwc/repo-1.2.4.zip
deleted file mode 100644
index e9efc00..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.2.4.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.0.zip b/core/src/test/resources/indices/bwc/repo-1.3.0.zip
deleted file mode 100644
index 5a59e21..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.1.zip b/core/src/test/resources/indices/bwc/repo-1.3.1.zip
deleted file mode 100644
index 2ae1d7c..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.2.zip b/core/src/test/resources/indices/bwc/repo-1.3.2.zip
deleted file mode 100644
index c67b997..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.3.zip b/core/src/test/resources/indices/bwc/repo-1.3.3.zip
deleted file mode 100644
index 64e3235..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.4.zip b/core/src/test/resources/indices/bwc/repo-1.3.4.zip
deleted file mode 100644
index 55e6744..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.4.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.5.zip b/core/src/test/resources/indices/bwc/repo-1.3.5.zip
deleted file mode 100644
index 35a5fd4..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.5.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.6.zip b/core/src/test/resources/indices/bwc/repo-1.3.6.zip
deleted file mode 100644
index f1eb21c..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.6.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.7.zip b/core/src/test/resources/indices/bwc/repo-1.3.7.zip
deleted file mode 100644
index 543c13c..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.7.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.8.zip b/core/src/test/resources/indices/bwc/repo-1.3.8.zip
deleted file mode 100644
index 93abac3..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.8.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.9.zip b/core/src/test/resources/indices/bwc/repo-1.3.9.zip
deleted file mode 100644
index 7dc194d..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.9.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.4.0.Beta1.zip b/core/src/test/resources/indices/bwc/repo-1.4.0.Beta1.zip
deleted file mode 100644
index 5adf788..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.4.0.Beta1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.4.0.zip b/core/src/test/resources/indices/bwc/repo-1.4.0.zip
deleted file mode 100644
index 21f867c..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.4.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.4.1.zip b/core/src/test/resources/indices/bwc/repo-1.4.1.zip
deleted file mode 100644
index 18166ea..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.4.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.4.2.zip b/core/src/test/resources/indices/bwc/repo-1.4.2.zip
deleted file mode 100644
index f03625a..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.4.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.4.3.zip b/core/src/test/resources/indices/bwc/repo-1.4.3.zip
deleted file mode 100644
index d78fbb1..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.4.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.4.4.zip b/core/src/test/resources/indices/bwc/repo-1.4.4.zip
deleted file mode 100644
index 1e89446..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.4.4.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.4.5.zip b/core/src/test/resources/indices/bwc/repo-1.4.5.zip
deleted file mode 100644
index fcae439..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.4.5.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.5.0.zip b/core/src/test/resources/indices/bwc/repo-1.5.0.zip
deleted file mode 100644
index a55e6c4..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.5.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.5.1.zip b/core/src/test/resources/indices/bwc/repo-1.5.1.zip
deleted file mode 100644
index 7cccbbe..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.5.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.5.2.zip b/core/src/test/resources/indices/bwc/repo-1.5.2.zip
deleted file mode 100644
index cee5783..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.5.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.6.0.zip b/core/src/test/resources/indices/bwc/repo-1.6.0.zip
deleted file mode 100644
index 1c31a02..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.6.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.6.1.zip b/core/src/test/resources/indices/bwc/repo-1.6.1.zip
deleted file mode 100644
index 746f3ce..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.6.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.6.2.zip b/core/src/test/resources/indices/bwc/repo-1.6.2.zip
deleted file mode 100644
index de4c5be..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.6.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.7.0.zip b/core/src/test/resources/indices/bwc/repo-1.7.0.zip
deleted file mode 100644
index 893689b..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.7.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.7.1.zip b/core/src/test/resources/indices/bwc/repo-1.7.1.zip
deleted file mode 100644
index cd5c6c0..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.7.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.7.2.zip b/core/src/test/resources/indices/bwc/repo-1.7.2.zip
deleted file mode 100644
index a0daaac..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.7.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-2.0.0-rc1.zip b/core/src/test/resources/indices/bwc/repo-2.0.0-rc1.zip
new file mode 100644
index 0000000..b15cedf
Binary files /dev/null and b/core/src/test/resources/indices/bwc/repo-2.0.0-rc1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.0.Beta1.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.0.Beta1.zip
new file mode 100644
index 0000000..5bbdea4
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.0.Beta1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.0.RC1.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.0.RC1.zip
new file mode 100644
index 0000000..d9072ce
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.0.RC1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.0.RC2.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.0.RC2.zip
new file mode 100644
index 0000000..dce299b
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.0.RC2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.0.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.0.zip
new file mode 100644
index 0000000..3ec908d
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.1.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.1.zip
new file mode 100644
index 0000000..67db98f
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.10.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.10.zip
new file mode 100644
index 0000000..6bdb9f2
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.10.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.11.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.11.zip
new file mode 100644
index 0000000..b5253f9
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.11.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.12.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.12.zip
new file mode 100644
index 0000000..0392049
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.12.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.13.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.13.zip
new file mode 100644
index 0000000..025b4c3
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.13.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.2.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.2.zip
new file mode 100644
index 0000000..413e08e
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.3.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.3.zip
new file mode 100644
index 0000000..c31d4de
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.4.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.4.zip
new file mode 100644
index 0000000..8b07a92
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.4.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.5.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.5.zip
new file mode 100644
index 0000000..dfd0fd0
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.5.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.6.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.6.zip
new file mode 100644
index 0000000..1f3cff2
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.6.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.7.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.7.zip
new file mode 100644
index 0000000..6d0e65c
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.7.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.8.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.8.zip
new file mode 100644
index 0000000..8ff8ac3
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.8.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.9.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.9.zip
new file mode 100644
index 0000000..4445b39
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.9.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.0.0.Beta1.zip b/core/src/test/resources/indices/bwc/unsupported-1.0.0.Beta1.zip
new file mode 100644
index 0000000..167dde8
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.0.0.Beta1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.0.0.Beta2.zip b/core/src/test/resources/indices/bwc/unsupported-1.0.0.Beta2.zip
new file mode 100644
index 0000000..95fbfef
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.0.0.Beta2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.0.0.RC1.zip b/core/src/test/resources/indices/bwc/unsupported-1.0.0.RC1.zip
new file mode 100644
index 0000000..3ced97a
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.0.0.RC1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.0.0.RC2.zip b/core/src/test/resources/indices/bwc/unsupported-1.0.0.RC2.zip
new file mode 100644
index 0000000..1298cfb
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.0.0.RC2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.0.0.zip b/core/src/test/resources/indices/bwc/unsupported-1.0.0.zip
new file mode 100644
index 0000000..2cb9abc
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.0.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.0.1.zip b/core/src/test/resources/indices/bwc/unsupported-1.0.1.zip
new file mode 100644
index 0000000..844271b
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.0.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.0.2.zip b/core/src/test/resources/indices/bwc/unsupported-1.0.2.zip
new file mode 100644
index 0000000..dd8e393
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.0.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.0.3.zip b/core/src/test/resources/indices/bwc/unsupported-1.0.3.zip
new file mode 100644
index 0000000..e4437ef
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.0.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.1.0.zip b/core/src/test/resources/indices/bwc/unsupported-1.1.0.zip
new file mode 100644
index 0000000..4f05370
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.1.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.1.1.zip b/core/src/test/resources/indices/bwc/unsupported-1.1.1.zip
new file mode 100644
index 0000000..effeb94
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.1.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.1.2.zip b/core/src/test/resources/indices/bwc/unsupported-1.1.2.zip
new file mode 100644
index 0000000..bedffa4
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.1.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.2.0.zip b/core/src/test/resources/indices/bwc/unsupported-1.2.0.zip
new file mode 100644
index 0000000..4644a38
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.2.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.2.1.zip b/core/src/test/resources/indices/bwc/unsupported-1.2.1.zip
new file mode 100644
index 0000000..553b46b
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.2.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.2.2.zip b/core/src/test/resources/indices/bwc/unsupported-1.2.2.zip
new file mode 100644
index 0000000..3f51a47
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.2.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.2.3.zip b/core/src/test/resources/indices/bwc/unsupported-1.2.3.zip
new file mode 100644
index 0000000..8c8bfbd
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.2.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.2.4.zip b/core/src/test/resources/indices/bwc/unsupported-1.2.4.zip
new file mode 100644
index 0000000..e3a1519
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.2.4.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.0.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.0.zip
new file mode 100644
index 0000000..d98958d
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.1.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.1.zip
new file mode 100644
index 0000000..167d0f4
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.2.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.2.zip
new file mode 100644
index 0000000..756eaf6
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.3.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.3.zip
new file mode 100644
index 0000000..8470dee
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.4.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.4.zip
new file mode 100644
index 0000000..2175012
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.4.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.5.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.5.zip
new file mode 100644
index 0000000..19d1e56
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.5.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.6.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.6.zip
new file mode 100644
index 0000000..ad8e8bd
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.6.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.7.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.7.zip
new file mode 100644
index 0000000..3a645a9
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.7.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.8.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.8.zip
new file mode 100644
index 0000000..f8ab0a2
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.8.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.9.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.9.zip
new file mode 100644
index 0000000..5ef35b2
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.9.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.4.0.Beta1.zip b/core/src/test/resources/indices/bwc/unsupported-1.4.0.Beta1.zip
new file mode 100644
index 0000000..4546f5d
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.4.0.Beta1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.4.0.zip b/core/src/test/resources/indices/bwc/unsupported-1.4.0.zip
new file mode 100644
index 0000000..467d19a
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.4.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.4.1.zip b/core/src/test/resources/indices/bwc/unsupported-1.4.1.zip
new file mode 100644
index 0000000..2adbb28
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.4.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.4.2.zip b/core/src/test/resources/indices/bwc/unsupported-1.4.2.zip
new file mode 100644
index 0000000..4fac208
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.4.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.4.3.zip b/core/src/test/resources/indices/bwc/unsupported-1.4.3.zip
new file mode 100644
index 0000000..1a0d667
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.4.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.4.4.zip b/core/src/test/resources/indices/bwc/unsupported-1.4.4.zip
new file mode 100644
index 0000000..0328a9e
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.4.4.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.4.5.zip b/core/src/test/resources/indices/bwc/unsupported-1.4.5.zip
new file mode 100644
index 0000000..eeb25ab
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.4.5.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.5.0.zip b/core/src/test/resources/indices/bwc/unsupported-1.5.0.zip
new file mode 100644
index 0000000..f1dab08
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.5.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.5.1.zip b/core/src/test/resources/indices/bwc/unsupported-1.5.1.zip
new file mode 100644
index 0000000..342e311
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.5.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.5.2.zip b/core/src/test/resources/indices/bwc/unsupported-1.5.2.zip
new file mode 100644
index 0000000..fb36b19
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.5.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.6.0.zip b/core/src/test/resources/indices/bwc/unsupported-1.6.0.zip
new file mode 100644
index 0000000..02a5806
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.6.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.6.1.zip b/core/src/test/resources/indices/bwc/unsupported-1.6.1.zip
new file mode 100644
index 0000000..04820f9
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.6.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.6.2.zip b/core/src/test/resources/indices/bwc/unsupported-1.6.2.zip
new file mode 100644
index 0000000..af6ce56
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.6.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.7.0.zip b/core/src/test/resources/indices/bwc/unsupported-1.7.0.zip
new file mode 100644
index 0000000..941be64
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.7.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.7.1.zip b/core/src/test/resources/indices/bwc/unsupported-1.7.1.zip
new file mode 100644
index 0000000..debd797
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.7.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.7.2.zip b/core/src/test/resources/indices/bwc/unsupported-1.7.2.zip
new file mode 100644
index 0000000..18bb6c7
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.7.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.Beta2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.Beta2.zip
new file mode 100644
index 0000000..020f6f4
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.Beta2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.RC1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.RC1.zip
new file mode 100644
index 0000000..a84c507
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.RC1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.RC2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.RC2.zip
new file mode 100644
index 0000000..e5d65eb
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.RC2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.zip
new file mode 100644
index 0000000..13f778d
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.1.zip
new file mode 100644
index 0000000..76ed278
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.2.zip
new file mode 100644
index 0000000..762eabf
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.3.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.3.zip
new file mode 100644
index 0000000..ba79dea
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.0.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.0.zip
new file mode 100644
index 0000000..cbf84c7
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.1.zip
new file mode 100644
index 0000000..00ea044
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.2.zip
new file mode 100644
index 0000000..18abd99
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.0.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.0.zip
new file mode 100644
index 0000000..f5e62a1
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.1.zip
new file mode 100644
index 0000000..935e71b
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.2.zip
new file mode 100644
index 0000000..d69b22a
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.3.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.3.zip
new file mode 100644
index 0000000..295f9f7
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.4.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.4.zip
new file mode 100644
index 0000000..e9efc00
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.4.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.0.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.0.zip
new file mode 100644
index 0000000..5a59e21
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.1.zip
new file mode 100644
index 0000000..2ae1d7c
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.2.zip
new file mode 100644
index 0000000..c67b997
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.3.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.3.zip
new file mode 100644
index 0000000..64e3235
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.4.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.4.zip
new file mode 100644
index 0000000..55e6744
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.4.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.5.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.5.zip
new file mode 100644
index 0000000..35a5fd4
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.5.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.6.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.6.zip
new file mode 100644
index 0000000..f1eb21c
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.6.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.7.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.7.zip
new file mode 100644
index 0000000..543c13c
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.7.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.8.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.8.zip
new file mode 100644
index 0000000..93abac3
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.8.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.9.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.9.zip
new file mode 100644
index 0000000..7dc194d
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.9.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.0.Beta1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.0.Beta1.zip
new file mode 100644
index 0000000..5adf788
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.0.Beta1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.0.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.0.zip
new file mode 100644
index 0000000..21f867c
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.1.zip
new file mode 100644
index 0000000..18166ea
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.2.zip
new file mode 100644
index 0000000..f03625a
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.3.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.3.zip
new file mode 100644
index 0000000..d78fbb1
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.4.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.4.zip
new file mode 100644
index 0000000..1e89446
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.4.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.5.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.5.zip
new file mode 100644
index 0000000..fcae439
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.5.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.0.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.0.zip
new file mode 100644
index 0000000..a55e6c4
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.1.zip
new file mode 100644
index 0000000..7cccbbe
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.2.zip
new file mode 100644
index 0000000..cee5783
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.0.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.0.zip
new file mode 100644
index 0000000..1c31a02
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.1.zip
new file mode 100644
index 0000000..746f3ce
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.2.zip
new file mode 100644
index 0000000..de4c5be
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.0.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.0.zip
new file mode 100644
index 0000000..893689b
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.1.zip
new file mode 100644
index 0000000..cd5c6c0
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.2.zip
new file mode 100644
index 0000000..a0daaac
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.2.zip differ
diff --git a/core/src/test/resources/org/elasticsearch/cluster/routing/custom_routing_1_x.zip b/core/src/test/resources/org/elasticsearch/cluster/routing/custom_routing_1_x.zip
deleted file mode 100644
index 5772361..0000000
Binary files a/core/src/test/resources/org/elasticsearch/cluster/routing/custom_routing_1_x.zip and /dev/null differ
diff --git a/core/src/test/resources/org/elasticsearch/cluster/routing/default_routing_1_x.zip b/core/src/test/resources/org/elasticsearch/cluster/routing/default_routing_1_x.zip
deleted file mode 100644
index 2fffc0b..0000000
Binary files a/core/src/test/resources/org/elasticsearch/cluster/routing/default_routing_1_x.zip and /dev/null differ
diff --git a/dev-tools/prepare_release_candidate.py b/dev-tools/prepare_release_candidate.py
index 5f83893..24450a6 100644
--- a/dev-tools/prepare_release_candidate.py
+++ b/dev-tools/prepare_release_candidate.py
@@ -63,7 +63,7 @@ To install the deb from an APT repo:
 
 APT line sources.list line:
 
-deb http://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(major_minor_version)s/debian/ stable main
+deb http://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(package_repo_version)s/debian/ stable main
 
 To install the RPM, create a YUM file like:
 
@@ -73,7 +73,7 @@ containing:
 
 [elasticsearch-2.0]
 name=Elasticsearch repository for packages
-baseurl=http://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(major_minor_version)s/centos
+baseurl=http://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(package_repo_version)s/centos
 gpgcheck=1
 gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
 enabled=1
@@ -300,7 +300,7 @@ if __name__ == "__main__":
   ensure_checkout_is_clean()
   if not re.match('(\d+\.\d+)\.*',release_version):
     raise RuntimeError('illegal release version format: %s' % (release_version))
-  major_minor_version = re.match('(\d+\.\d+)\.*',release_version).group(1)
+  package_repo_version = '%s.x' % re.match('(\d+)\.*', release_version).group(1)
 
   print('*** Preparing release version: [%s]' % release_version)
 
@@ -348,13 +348,13 @@ if __name__ == "__main__":
   # repository push commands
   s3cmd_sync_to_staging_bucket_cmd = 's3cmd sync -P %s s3://%s/elasticsearch/staging/%s-%s/org/' % (localRepoElasticsearch, bucket, release_version, shortHash)
   s3_bucket_sync_to = '%s/elasticsearch/staging/%s-%s/repos/' % (bucket, release_version, shortHash)
-  s3cmd_sync_official_repo_cmd = 's3cmd sync s3://packages.elasticsearch.org/elasticsearch/%s s3://%s' % (major_minor_version, s3_bucket_sync_to)
+  s3cmd_sync_official_repo_cmd = 's3cmd sync s3://packages.elasticsearch.org/elasticsearch/%s s3://%s' % (package_repo_version, s3_bucket_sync_to)
 
-  debs3_prefix = 'elasticsearch/staging/%s-%s/repos/%s/debian' % (release_version, shortHash, major_minor_version)
+  debs3_prefix = 'elasticsearch/staging/%s-%s/repos/%s/debian' % (release_version, shortHash, package_repo_version)
   debs3_upload_cmd = 'deb-s3 upload --preserve-versions %s/distribution/deb/elasticsearch/%s/elasticsearch-%s.deb -b %s --prefix %s --sign %s --arch amd64' % (localRepoElasticsearch, release_version, release_version, bucket, debs3_prefix, gpg_key)
   debs3_list_cmd = 'deb-s3 list -b %s --prefix %s' % (bucket, debs3_prefix)
   debs3_verify_cmd = 'deb-s3 verify -b %s --prefix %s' % (bucket, debs3_prefix)
-  rpms3_prefix = 'elasticsearch/staging/%s-%s/repos/%s/centos' % (release_version, shortHash, major_minor_version)
+  rpms3_prefix = 'elasticsearch/staging/%s-%s/repos/%s/centos' % (release_version, shortHash, package_repo_version)
   rpms3_upload_cmd = 'rpm-s3 -v -b %s -p %s --sign --visibility public-read -k 0 %s' % (bucket, rpms3_prefix, rpm)
 
   if deploy_s3:
@@ -397,7 +397,7 @@ if __name__ == "__main__":
     print('NOTE: Running s3cmd might require you to create a config file with your credentials, if the s3cmd does not support suppliying them via the command line!')
 
   print('*** Once the release is deployed and published send out the following mail to dev@elastic.co:')
-  string_format_dict = {'version' : release_version, 'hash': shortHash, 'major_minor_version' : major_minor_version, 'bucket': bucket}
+  string_format_dict = {'version' : release_version, 'hash': shortHash, 'package_repo_version' : package_repo_version, 'bucket': bucket}
   print(MAIL_TEMPLATE % string_format_dict)
 
   print('')
@@ -406,7 +406,7 @@ if __name__ == "__main__":
 
   print('')
   print('To publish the release and the repo on S3 execute the following commands:')
-  print('   s3cmd cp --recursive s3://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(major_minor_version)s/ s3://packages.elasticsearch.org/elasticsearch/%(major_minor_version)s'  % string_format_dict)
+  print('   s3cmd cp --recursive s3://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(package_repo_version)s/ s3://packages.elasticsearch.org/elasticsearch/%(package_repo_version)s'  % string_format_dict)
   print('   s3cmd cp --recursive s3://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/org/ s3://%(bucket)s/elasticsearch/release/org'  % string_format_dict)
   print('Now go ahead and tag the release:')
   print('   git tag -a v%(version)s %(hash)s'  % string_format_dict)
diff --git a/dev-tools/smoke_test_rc.py b/dev-tools/smoke_test_rc.py
index 3bb11ca..b7bc00d 100644
--- a/dev-tools/smoke_test_rc.py
+++ b/dev-tools/smoke_test_rc.py
@@ -62,10 +62,10 @@ DEFAULT_PLUGINS = ["analysis-icu",
                    "analysis-phonetic",
                    "analysis-smartcn",
                    "analysis-stempel",
-                   "cloud-gce",
                    "delete-by-query",
                    "discovery-azure",
                    "discovery-ec2",
+                   "discovery-gce",
                    "discovery-multicast",
                    "lang-expression",
                    "lang-groovy",
diff --git a/dev-tools/src/main/resources/forbidden/all-signatures.txt b/dev-tools/src/main/resources/forbidden/all-signatures.txt
index 836c324..ee534b4 100644
--- a/dev-tools/src/main/resources/forbidden/all-signatures.txt
+++ b/dev-tools/src/main/resources/forbidden/all-signatures.txt
@@ -133,8 +133,8 @@ com.google.common.io.Resources
 com.google.common.hash.HashCode
 com.google.common.hash.HashFunction
 com.google.common.hash.Hashing
-com.google.common.collect.ImmutableMap
-com.google.common.collect.ImmutableMap$Builder
+com.google.common.collect.Iterators
+com.google.common.net.InetAddresses
 
 @defaultMessage Do not violate java's access system
 java.lang.reflect.AccessibleObject#setAccessible(boolean)
diff --git a/dev-tools/src/main/resources/forbidden/cluster-signatures.txt b/dev-tools/src/main/resources/forbidden/cluster-signatures.txt
deleted file mode 100644
index e3da53f..0000000
--- a/dev-tools/src/main/resources/forbidden/cluster-signatures.txt
+++ /dev/null
@@ -1,2 +0,0 @@
-@defaultMessage Prefer ImmutableOpenMap for cluster state
-java.util.Collections#unmodifiableMap(java.util.Map)
diff --git a/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties b/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties
index 67d139e..1588e11 100644
--- a/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties
+++ b/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties
@@ -24,7 +24,7 @@
 # jvm=true
 # classname=foo.bar.BazPlugin
 # description=My cool plugin
-# version=2.0
+# version=2.0.0-rc1
 # elasticsearch.version=2.0
 # java.version=1.7
 #
@@ -64,6 +64,10 @@ classname=${elasticsearch.plugin.classname}
 java.version=${maven.compiler.target}
 #
 # 'elasticsearch.version' version of elasticsearch compiled against
+# You will have to release a new version of the plugin for each new
+# elasticsearch release. This version is checked when the plugin
+# is loaded so Elasticsearch will refuse to start in the presence of
+# plugins with the incorrect elasticsearch.version.
 elasticsearch.version=${elasticsearch.version}
 #
 ### deprecated elements for jvm plugins :
diff --git a/distribution/deb/pom.xml b/distribution/deb/pom.xml
index 7237674..c43e32b 100644
--- a/distribution/deb/pom.xml
+++ b/distribution/deb/pom.xml
@@ -76,6 +76,7 @@
                                         <include>bin/elasticsearch</include>
                                         <include>bin/elasticsearch.in.sh</include>
                                         <include>bin/plugin</include>
+                                        <include>bin/elasticsearch-systemd-pre-exec</include>
                                     </includes>
                                 </resource>
                             </resources>
@@ -110,7 +111,7 @@
                                 <data>
                                     <src>${project.build.directory}/generated-packaging/deb/bin</src>
                                     <type>directory</type>
-                                    <includes>elasticsearch,elasticsearch.in.sh,plugin</includes>
+                                    <includes>elasticsearch,elasticsearch.in.sh,plugin,elasticsearch-systemd-pre-exec</includes>
                                     <mapper>
                                         <type>perm</type>
                                         <prefix>${packaging.elasticsearch.bin.dir}</prefix>
@@ -119,6 +120,19 @@
                                         <group>root</group>
                                     </mapper>
                                 </data>
+                                <!-- create the conf dir manually so it gets proper permissions -->
+                                <data>
+                                    <type>template</type>
+                                    <paths>
+                                        <path>${packaging.elasticsearch.conf.dir}</path>
+                                    </paths>
+                                    <mapper>
+                                        <type>perm</type>
+                                        <filemode>750</filemode>
+                                        <user>root</user>
+                                        <group>elasticsearch</group>
+                                    </mapper>
+                                </data>
                                 <!-- Add configuration files -->
                                 <data>
                                     <src>${project.basedir}/../src/main/resources/config</src>
@@ -127,8 +141,9 @@
                                     <mapper>
                                         <type>perm</type>
                                         <prefix>${packaging.elasticsearch.conf.dir}</prefix>
+                                        <filemode>750</filemode>
                                         <user>root</user>
-                                        <group>root</group>
+                                        <group>elasticsearch</group>
                                     </mapper>
                                 </data>
                                 <data>
@@ -136,6 +151,12 @@
                                     <paths>
                                         <path>${packaging.elasticsearch.conf.dir}/scripts</path>
                                     </paths>
+                                    <mapper>
+                                        <type>perm</type>
+                                        <filemode>750</filemode>
+                                        <user>root</user>
+                                        <group>elasticsearch</group>
+                                    </mapper>
                                 </data>
                                 <!-- Add environment vars file -->
                                 <data>
diff --git a/distribution/deb/src/main/packaging/init.d/elasticsearch b/distribution/deb/src/main/packaging/init.d/elasticsearch
index 9ea2beb..3a82bbe 100755
--- a/distribution/deb/src/main/packaging/init.d/elasticsearch
+++ b/distribution/deb/src/main/packaging/init.d/elasticsearch
@@ -74,9 +74,6 @@ DATA_DIR=/var/lib/$NAME
 # Elasticsearch configuration directory
 CONF_DIR=/etc/$NAME
 
-# Elasticsearch configuration file (elasticsearch.yml)
-CONF_FILE=$CONF_DIR/elasticsearch.yml
-
 # Maximum number of VMA (Virtual Memory Areas) a process can own
 MAX_MAP_COUNT=262144
 
@@ -93,10 +90,16 @@ if [ -f "$DEFAULT" ]; then
 	. "$DEFAULT"
 fi
 
+# CONF_FILE setting was removed
+if [ ! -z "$CONF_FILE" ]; then
+    echo "CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed."
+    exit 1
+fi
+
 # Define other required variables
 PID_FILE="$PID_DIR/$NAME.pid"
 DAEMON=$ES_HOME/bin/elasticsearch
-DAEMON_OPTS="-d -p $PID_FILE --default.config=$CONF_FILE --default.path.home=$ES_HOME --default.path.logs=$LOG_DIR --default.path.data=$DATA_DIR --default.path.conf=$CONF_DIR"
+DAEMON_OPTS="-d -p $PID_FILE --default.path.home=$ES_HOME --default.path.logs=$LOG_DIR --default.path.data=$DATA_DIR --default.path.conf=$CONF_DIR"
 
 export ES_HEAP_SIZE
 export ES_HEAP_NEWSIZE
diff --git a/distribution/deb/src/main/packaging/packaging.properties b/distribution/deb/src/main/packaging/packaging.properties
index f268cde..3635928 100644
--- a/distribution/deb/src/main/packaging/packaging.properties
+++ b/distribution/deb/src/main/packaging/packaging.properties
@@ -6,7 +6,6 @@ packaging.env.file=/etc/default/elasticsearch
 
 # Default configuration directory and file to use in bin/plugin script
 packaging.plugin.default.config.dir=${packaging.elasticsearch.conf.dir}
-packaging.plugin.default.config.file=${packaging.elasticsearch.conf.dir}/elasticsearch.yml
 
 # Simple marker to check that properties are correctly overridden
 packaging.type=deb
diff --git a/distribution/rpm/pom.xml b/distribution/rpm/pom.xml
index 37f7203..218e19e 100644
--- a/distribution/rpm/pom.xml
+++ b/distribution/rpm/pom.xml
@@ -79,6 +79,7 @@
                                         <include>bin/elasticsearch</include>
                                         <include>bin/elasticsearch.in.sh</include>
                                         <include>bin/plugin</include>
+                                        <include>bin/elasticsearch-systemd-pre-exec</include>
                                     </includes>
                                 </resource>
                             </resources>
@@ -127,6 +128,7 @@
                                         <include>elasticsearch</include>
                                         <include>elasticsearch.in.sh</include>
                                         <include>plugin</include>
+                                        <include>elasticsearch-systemd-pre-exec</include>
                                     </includes>
                                 </source>
                             </sources>
@@ -140,10 +142,14 @@
                                  that creates the conf.dir.-->
                             <directory>${packaging.elasticsearch.conf.dir}</directory>
                             <configuration>noreplace</configuration>
+                            <groupname>elasticsearch</groupname>
+                            <filemode>750</filemode>
                         </mapping>
                         <mapping>
                             <directory>${packaging.elasticsearch.conf.dir}/</directory>
                             <configuration>noreplace</configuration>
+                            <groupname>elasticsearch</groupname>
+                            <filemode>750</filemode>
                             <sources>
                                 <source>
                                     <location>${project.basedir}/../src/main/resources/config/</location>
@@ -156,6 +162,8 @@
                         <mapping>
                             <directory>${packaging.elasticsearch.conf.dir}/scripts</directory>
                             <configuration>noreplace</configuration>
+                            <groupname>elasticsearch</groupname>
+                            <filemode>750</filemode>
                         </mapping>
                         <!-- Add environment vars file -->
                         <mapping>
diff --git a/distribution/rpm/src/main/packaging/init.d/elasticsearch b/distribution/rpm/src/main/packaging/init.d/elasticsearch
index 9626dfc..924c678 100644
--- a/distribution/rpm/src/main/packaging/init.d/elasticsearch
+++ b/distribution/rpm/src/main/packaging/init.d/elasticsearch
@@ -40,7 +40,7 @@ MAX_MAP_COUNT=${packaging.os.max.map.count}
 LOG_DIR="${packaging.elasticsearch.log.dir}"
 DATA_DIR="${packaging.elasticsearch.data.dir}"
 CONF_DIR="${packaging.elasticsearch.conf.dir}"
-CONF_FILE="${packaging.elasticsearch.conf.dir}/elasticsearch.yml"
+
 PID_DIR="${packaging.elasticsearch.pid.dir}"
 
 # Source the default env file
@@ -49,6 +49,12 @@ if [ -f "$ES_ENV_FILE" ]; then
     . "$ES_ENV_FILE"
 fi
 
+# CONF_FILE setting was removed
+if [ ! -z "$CONF_FILE" ]; then
+    echo "CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed."
+    exit 1
+fi
+
 exec="$ES_HOME/bin/elasticsearch"
 prog="elasticsearch"
 pidfile="$PID_DIR/${prog}.pid"
@@ -83,7 +89,6 @@ checkJava() {
 start() {
     checkJava
     [ -x $exec ] || exit 5
-    [ -f $CONF_FILE ] || exit 6
     if [ -n "$MAX_LOCKED_MEMORY" -a -z "$ES_HEAP_SIZE" ]; then
         echo "MAX_LOCKED_MEMORY is set - ES_HEAP_SIZE must also be set"
         return 7
diff --git a/distribution/rpm/src/main/packaging/packaging.properties b/distribution/rpm/src/main/packaging/packaging.properties
index b5bf28a..bc4af5f 100644
--- a/distribution/rpm/src/main/packaging/packaging.properties
+++ b/distribution/rpm/src/main/packaging/packaging.properties
@@ -6,7 +6,6 @@ packaging.env.file=/etc/sysconfig/elasticsearch
 
 # Default configuration directory and file to use in bin/plugin script
 packaging.plugin.default.config.dir=${packaging.elasticsearch.conf.dir}
-packaging.plugin.default.config.file=${packaging.elasticsearch.conf.dir}/elasticsearch.yml
 
 # Simple marker to check that properties are correctly overridden
 packaging.type=rpm
diff --git a/distribution/src/main/packaging/env/elasticsearch b/distribution/src/main/packaging/env/elasticsearch
index cdf05bb..0c01d4f 100644
--- a/distribution/src/main/packaging/env/elasticsearch
+++ b/distribution/src/main/packaging/env/elasticsearch
@@ -8,9 +8,6 @@
 # Elasticsearch configuration directory
 #CONF_DIR=${packaging.elasticsearch.conf.dir}
 
-# Elasticsearch configuration file
-#CONF_FILE=$CONF_DIR/elasticsearch.yml
-
 # Elasticsearch data directory
 #DATA_DIR=${packaging.elasticsearch.data.dir}
 
diff --git a/distribution/src/main/packaging/packaging.properties b/distribution/src/main/packaging/packaging.properties
index ff95c9d..be5b604 100644
--- a/distribution/src/main/packaging/packaging.properties
+++ b/distribution/src/main/packaging/packaging.properties
@@ -8,7 +8,6 @@ packaging.env.file=
 
 # Default configuration directory and file to use in bin/plugin script
 packaging.plugin.default.config.dir=$ES_HOME/config
-packaging.plugin.default.config.file=$ES_HOME/config/elasticsearch.yml
 
 # Default values for min/max heap memory allocated to elasticsearch java process
 packaging.elasticsearch.heap.min=256m
diff --git a/distribution/src/main/packaging/systemd/elasticsearch.service b/distribution/src/main/packaging/systemd/elasticsearch.service
index cdcad9d..d8f56f7 100644
--- a/distribution/src/main/packaging/systemd/elasticsearch.service
+++ b/distribution/src/main/packaging/systemd/elasticsearch.service
@@ -7,7 +7,6 @@ After=network-online.target
 [Service]
 Environment=ES_HOME=${packaging.elasticsearch.home.dir}
 Environment=CONF_DIR=${packaging.elasticsearch.conf.dir}
-Environment=CONF_FILE=${packaging.elasticsearch.conf.dir}/elasticsearch.yml
 Environment=DATA_DIR=${packaging.elasticsearch.data.dir}
 Environment=LOG_DIR=${packaging.elasticsearch.log.dir}
 Environment=PID_DIR=${packaging.elasticsearch.pid.dir}
@@ -18,12 +17,13 @@ WorkingDirectory=${packaging.elasticsearch.home.dir}
 User=${packaging.elasticsearch.user}
 Group=${packaging.elasticsearch.group}
 
+ExecStartPre=${packaging.elasticsearch.bin.dir}/elasticsearch-systemd-pre-exec
+
 ExecStart=${packaging.elasticsearch.bin.dir}/elasticsearch \
                                                 -Des.pidfile=${PID_DIR}/elasticsearch.pid \
                                                 -Des.default.path.home=${ES_HOME} \
                                                 -Des.default.path.logs=${LOG_DIR} \
                                                 -Des.default.path.data=${DATA_DIR} \
-                                                -Des.default.config=${CONF_FILE} \
                                                 -Des.default.path.conf=${CONF_DIR}
 
 # Connects standard output to /dev/null
diff --git a/distribution/src/main/resources/bin/elasticsearch b/distribution/src/main/resources/bin/elasticsearch
index 878fcff..66f4657 100755
--- a/distribution/src/main/resources/bin/elasticsearch
+++ b/distribution/src/main/resources/bin/elasticsearch
@@ -42,10 +42,10 @@
 # Be aware that you will be entirely responsible for populating the needed
 # environment variables.
 
-
 # Maven will replace the project.name with elasticsearch below. If that
 # hasn't been done, we assume that this is not a packaged version and the
 # user has forgotten to run Maven to create a package.
+
 IS_PACKAGED_VERSION='${project.parent.artifactId}'
 if [ "$IS_PACKAGED_VERSION" != "distributions" ]; then
     cat >&2 << EOF
diff --git a/distribution/src/main/resources/bin/elasticsearch-systemd-pre-exec b/distribution/src/main/resources/bin/elasticsearch-systemd-pre-exec
new file mode 100755
index 0000000..a51d639
--- /dev/null
+++ b/distribution/src/main/resources/bin/elasticsearch-systemd-pre-exec
@@ -0,0 +1,7 @@
+#!/bin/sh
+
+# CONF_FILE setting was removed
+if [ ! -z "$CONF_FILE" ]; then
+    echo "CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed."
+    exit 1
+fi
diff --git a/distribution/src/main/resources/bin/plugin b/distribution/src/main/resources/bin/plugin
index c466d48..35dbe3a 100755
--- a/distribution/src/main/resources/bin/plugin
+++ b/distribution/src/main/resources/bin/plugin
@@ -1,5 +1,6 @@
 #!/bin/sh
 
+
 CDPATH=""
 SCRIPT="$0"
 
@@ -21,17 +22,10 @@ ES_HOME=`dirname "$SCRIPT"`/..
 # make ELASTICSEARCH_HOME absolute
 ES_HOME=`cd "$ES_HOME"; pwd`
 
+
 # Sets the default values for elasticsearch variables used in this script
 if [ -z "$CONF_DIR" ]; then
   CONF_DIR="${packaging.plugin.default.config.dir}"
-
-  if [ -z "$CONF_FILE" ]; then
-    CONF_FILE="$CONF_DIR/elasticsearch.yml"
-  fi
-fi
-
-if [ -z "$CONF_FILE" ]; then
-  CONF_FILE="${packaging.plugin.default.config.file}"
 fi
 
 # The default env file is defined at building/packaging time.
@@ -66,6 +60,12 @@ if [ "x$JAVA_TOOL_OPTIONS" != "x" ]; then
     unset JAVA_TOOL_OPTIONS
 fi
 
+# CONF_FILE setting was removed
+if [ ! -z "$CONF_FILE" ]; then
+    echo "CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed."
+    exit 1
+fi
+
 if [ -x "$JAVA_HOME/bin/java" ]; then
     JAVA=$JAVA_HOME/bin/java
 else
@@ -105,16 +105,6 @@ if [ -e "$CONF_DIR" ]; then
   esac
 fi
 
-if [ -e "$CONF_FILE" ]; then
-  case "$properties" in
-    *-Des.default.config=*|*-Des.config=*)
-    ;;
-    *)
-      properties="$properties -Des.default.config=\"$CONF_FILE\""
-    ;;
-  esac
-fi
-
 # full hostname passed through cut for portability on systems that do not support hostname -s
 # export on separate line for shells that do not support combining definition and export
 HOSTNAME=`hostname | cut -d. -f1`
diff --git a/distribution/src/main/resources/bin/service.bat b/distribution/src/main/resources/bin/service.bat
index 06c9c64..9822e6b 100644
--- a/distribution/src/main/resources/bin/service.bat
+++ b/distribution/src/main/resources/bin/service.bat
@@ -5,6 +5,8 @@ TITLE Elasticsearch Service ${project.version}
 
 if NOT DEFINED JAVA_HOME goto err
 
+if not "%CONF_FILE%" == "" goto conffileset
+
 set SCRIPT_DIR=%~dp0
 for %%I in ("%SCRIPT_DIR%..") do set ES_HOME=%%~dpfI
 
@@ -147,9 +149,7 @@ if "%DATA_DIR%" == "" set DATA_DIR=%ES_HOME%\data
 
 if "%CONF_DIR%" == "" set CONF_DIR=%ES_HOME%\config
 
-if "%CONF_FILE%" == "" set CONF_FILE=%ES_HOME%\config\elasticsearch.yml
-
-set ES_PARAMS=-Delasticsearch;-Des.path.home="%ES_HOME%";-Des.default.config="%CONF_FILE%";-Des.default.path.home="%ES_HOME%";-Des.default.path.logs="%LOG_DIR%";-Des.default.path.data="%DATA_DIR%";-Des.default.path.conf="%CONF_DIR%"
+set ES_PARAMS=-Delasticsearch;-Des.path.home="%ES_HOME%";-Des.default.path.home="%ES_HOME%";-Des.default.path.logs="%LOG_DIR%";-Des.default.path.data="%DATA_DIR%";-Des.default.path.conf="%CONF_DIR%"
 
 set JVM_OPTS=%JAVA_OPTS: =;%
 
@@ -207,4 +207,8 @@ set /a conv=%conv% * 1024
 set "%~2=%conv%"
 goto:eof
 
+:conffileset
+echo CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.
+goto:eof
+
 ENDLOCAL
diff --git a/docs/plugins/authors.asciidoc b/docs/plugins/authors.asciidoc
index 2ce05fd..e0db081 100644
--- a/docs/plugins/authors.asciidoc
+++ b/docs/plugins/authors.asciidoc
@@ -43,6 +43,72 @@ instance, see
 https://github.com/elastic/elasticsearch/blob/master/plugins/site-example/pom.xml[`plugins/site-example/pom.xml`].
 
 [float]
+==== Mandatory elements for all plugins
+
+
+[cols="<,<,<",options="header",]
+|=======================================================================
+|Element                    | Type   | Description
+
+|`description`              |String  | simple summary of the plugin
+
+|`version`                  |String  | plugin's version
+
+|`name`                     |String  | the plugin name
+
+|=======================================================================
+
+
+
+[float]
+==== Mandatory elements for Java plugins
+
+
+[cols="<,<,<",options="header",]
+|=======================================================================
+|Element                    | Type   | Description
+
+|`jvm`                      |Boolean | true if the `classname` class should be loaded
+from jar files in the root directory of the plugin.
+Note that only jar files in the root directory are added to the classpath for the plugin!
+If you need other resources, package them into a resources jar.
+
+|`classname`                |String  | the name of the class to load, fully-qualified.
+
+|`java.version`             |String  | version of java the code is built against.
+Use the system property `java.specification.version`. Version string must be a sequence
+of nonnegative decimal integers separated by "."'s and may have leading zeros.
+
+|`elasticsearch.version`    |String  | version of elasticsearch compiled against.
+
+|=======================================================================
+
+[IMPORTANT]
+.Plugin release lifecycle
+==============================================
+
+You will have to release a new version of the plugin for each new elasticsearch release.
+This version is checked when the plugin is loaded so Elasticsearch will refuse to start
+in the presence of plugins with the incorrect `elasticsearch.version`.
+
+==============================================
+
+
+[float]
+==== Mandatory elements for Site plugins
+
+
+[cols="<,<,<",options="header",]
+|=======================================================================
+|Element                    | Type   | Description
+
+|`site`                     |Boolean | true to indicate contents of the `_site/`
+directory in the root of the plugin should be served.
+
+|=======================================================================
+
+
+[float]
 === Testing your plugin
 
 When testing a Java plugin, it will only be auto-loaded if it is in the
diff --git a/docs/plugins/cloud-gce.asciidoc b/docs/plugins/cloud-gce.asciidoc
deleted file mode 100644
index 6d712ad..0000000
--- a/docs/plugins/cloud-gce.asciidoc
+++ /dev/null
@@ -1,454 +0,0 @@
-[[cloud-gce]]
-=== GCE Cloud Plugin
-
-The Google Compute Engine Cloud plugin uses the GCE API for unicast discovery.
-
-[[cloud-gce-install]]
-[float]
-==== Installation
-
-This plugin can be installed using the plugin manager:
-
-[source,sh]
-----------------------------------------------------------------
-sudo bin/plugin install cloud-gce
-----------------------------------------------------------------
-
-The plugin must be installed on every node in the cluster, and each node must
-be restarted after installation.
-
-[[cloud-gce-remove]]
-[float]
-==== Removal
-
-The plugin can be removed with the following command:
-
-[source,sh]
-----------------------------------------------------------------
-sudo bin/plugin remove cloud-gce
-----------------------------------------------------------------
-
-The node must be stopped before removing the plugin.
-
-[[cloud-gce-usage-discovery]]
-==== GCE Virtual Machine Discovery
-
-Google Compute Engine VM discovery allows to use the google APIs to perform automatic discovery (similar to multicast
-in non hostile multicast environments). Here is a simple sample configuration:
-
-[source,yaml]
---------------------------------------------------
-cloud:
-  gce:
-      project_id: <your-google-project-id>
-      zone: <your-zone>
-discovery:
-      type: gce
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-short]]
-===== How to start (short story)
-
-* Create Google Compute Engine instance (with compute rw permissions)
-* Install Elasticsearch
-* Install Google Compute Engine Cloud plugin
-* Modify `elasticsearch.yml` file
-* Start Elasticsearch
-
-[[cloud-gce-usage-discovery-long]]
-==== Setting up GCE Discovery
-
-
-[[cloud-gce-usage-discovery-long-prerequisites]]
-===== Prerequisites
-
-Before starting, you need:
-
-* Your project ID, e.g. `es-cloud`. Get it from https://code.google.com/apis/console/[Google API Console].
-* To install https://developers.google.com/cloud/sdk/[Google Cloud SDK]
-
-If you did not set it yet, you can define your default project you will work on:
-
-[source,sh]
---------------------------------------------------
-gcloud config set project es-cloud
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-long-login]]
-
-If you haven't already, login to Google Cloud
-
-[source,sh]
---------------------------------------------------
-gcloud auth login
---------------------------------------------------
-
-This will open your browser. You will be asked to sign-in to a Google account and
-authorize access to the Google Cloud SDK.
-
-[[cloud-gce-usage-discovery-long-first-instance]]
-===== Creating your first instance
-
-
-[source,sh]
---------------------------------------------------
-gcloud compute instances create myesnode1 \
-       --zone <your-zone> \
-       --scopes compute-rw
---------------------------------------------------
-
-When done, a report like this one should appears:
-
-[source,text]
---------------------------------------------------
-Created [https://www.googleapis.com/compute/v1/projects/es-cloud-1070/zones/us-central1-f/instances/myesnode1].
-NAME      ZONE          MACHINE_TYPE  PREEMPTIBLE INTERNAL_IP   EXTERNAL_IP   STATUS
-myesnode1 us-central1-f n1-standard-1             10.240.133.54 104.197.94.25 RUNNING
---------------------------------------------------
-
-You can now connect to your instance:
-
-[source,sh]
---------------------------------------------------
-# Connect using google cloud SDK
-gcloud compute ssh myesnode1 --zone europe-west1-a
-
-# Or using SSH with external IP address
-ssh -i ~/.ssh/google_compute_engine 192.158.29.199
---------------------------------------------------
-
-[IMPORTANT]
-.Service Account Permissions
-==============================================
-
-It's important when creating an instance that the correct permissions are set. At a minimum, you must ensure you have:
-
-[source,text]
---------------------------------------------------
-scopes=compute-rw
---------------------------------------------------
-
-Failing to set this will result in unauthorized messages when starting Elasticsearch.
-See [Machine Permissions](#machine-permissions).
-==============================================
-
-
-Once connected, install Elasticsearch:
-
-[source,sh]
---------------------------------------------------
-sudo apt-get update
-
-# Download Elasticsearch
-wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-2.0.0.deb
-
-# Prepare Java installation
-sudo apt-get install java8-runtime-headless
-
-# Prepare Elasticsearch installation
-sudo dpkg -i elasticsearch-2.0.0.deb
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-long-install-plugin]]
-===== Install elasticsearch cloud gce plugin
-
-Install the plugin:
-
-[source,sh]
---------------------------------------------------
-# Use Plugin Manager to install it
-sudo bin/plugin install cloud-gce
---------------------------------------------------
-
-Open the `elasticsearch.yml` file:
-
-[source,sh]
---------------------------------------------------
-sudo vi /etc/elasticsearch/elasticsearch.yml
---------------------------------------------------
-
-And add the following lines:
-
-[source,yaml]
---------------------------------------------------
-cloud:
-  gce:
-      project_id: es-cloud
-      zone: europe-west1-a
-discovery:
-      type: gce
---------------------------------------------------
-
-
-Start elasticsearch:
-
-[source,sh]
---------------------------------------------------
-sudo /etc/init.d/elasticsearch start
---------------------------------------------------
-
-If anything goes wrong, you should check logs:
-
-[source,sh]
---------------------------------------------------
-tail -f /var/log/elasticsearch/elasticsearch.log
---------------------------------------------------
-
-If needed, you can change log level to `TRACE` by opening `logging.yml`:
-
-[source,sh]
---------------------------------------------------
-sudo vi /etc/elasticsearch/logging.yml
---------------------------------------------------
-
-and adding the following line:
-
-[source,yaml]
---------------------------------------------------
-# discovery
-discovery.gce: TRACE
---------------------------------------------------
-
-
-
-[[cloud-gce-usage-discovery-cloning]]
-==== Cloning your existing machine
-
-In order to build a cluster on many nodes, you can clone your configured instance to new nodes.
-You won't have to reinstall everything!
-
-First create an image of your running instance and upload it to Google Cloud Storage:
-
-[source,sh]
---------------------------------------------------
-# Create an image of yur current instance
-sudo /usr/bin/gcimagebundle -d /dev/sda -o /tmp/
-
-# An image has been created in `/tmp` directory:
-ls /tmp
-e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz
-
-# Upload your image to Google Cloud Storage:
-# Create a bucket to hold your image, let's say `esimage`:
-gsutil mb gs://esimage
-
-# Copy your image to this bucket:
-gsutil cp /tmp/e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz gs://esimage
-
-# Then add your image to images collection:
-gcloud compute images create elasticsearch-2-0-0 --source-uri gs://esimage/e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz
-
-# If the previous command did not work for you, logout from your instance
-# and launch the same command from your local machine.
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-start-new-instances]]
-===== Start new instances
-
-As you have now an image, you can create as many instances as you need:
-
-[source,sh]
---------------------------------------------------
-# Just change node name (here myesnode2)
-gcloud compute instances create myesnode2 --image elasticsearch-2-0-0 --zone europe-west1-a
-
-# If you want to provide all details directly, you can use:
-gcloud compute instances create myesnode2 --image=elasticsearch-2-0-0 \
-       --zone europe-west1-a --machine-type f1-micro --scopes=compute-rw
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-remove-instance]]
-===== Remove an instance (aka shut it down)
-
-You can use https://cloud.google.com/console[Google Cloud Console] or CLI to manage your instances:
-
-[source,sh]
---------------------------------------------------
-# Stopping and removing instances
-gcloud compute instances delete myesnode1 myesnode2 \
-       --zone=europe-west1-a
-
-# Consider removing disk as well if you don't need them anymore
-gcloud compute disks deleted boot-myesnode1 boot-myesnode2  \
-       --zone=europe-west1-a
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-zones]]
-==== Using GCE zones
-
-`cloud.gce.zone` helps to retrieve instances running in a given zone. It should be one of the
-https://developers.google.com/compute/docs/zones#available[GCE supported zones].
-
-The GCE discovery can support multi zones although you need to be aware of network latency between zones.
-To enable discovery across more than one zone, just enter add your zone list to `cloud.gce.zone` setting:
-
-[source,yaml]
---------------------------------------------------
-cloud:
-  gce:
-      project_id: <your-google-project-id>
-      zone: ["<your-zone1>", "<your-zone2>"]
-discovery:
-      type: gce
---------------------------------------------------
-
-
-
-[[cloud-gce-usage-discovery-tags]]
-==== Filtering by tags
-
-The GCE discovery can also filter machines to include in the cluster based on tags using `discovery.gce.tags` settings.
-For example, setting `discovery.gce.tags` to `dev` will only filter instances having a tag set to `dev`. Several tags
-set will require all of those tags to be set for the instance to be included.
-
-One practical use for tag filtering is when an GCE cluster contains many nodes that are not running
-elasticsearch. In this case (particularly with high ping_timeout values) there is a risk that a new node's discovery
-phase will end before it has found the cluster (which will result in it declaring itself master of a new cluster
-with the same name - highly undesirable). Adding tag on elasticsearch GCE nodes and then filtering by that
-tag will resolve this issue.
-
-Add your tag when building the new instance:
-
-[source,sh]
---------------------------------------------------
-gcloud compute instances create myesnode1 --project=es-cloud \
-       --scopes=compute-rw \
-       --tags=elasticsearch,dev
---------------------------------------------------
-
-Then, define it in `elasticsearch.yml`:
-
-[source,yaml]
---------------------------------------------------
-cloud:
-  gce:
-      project_id: es-cloud
-      zone: europe-west1-a
-discovery:
-      type: gce
-      gce:
-            tags: elasticsearch, dev
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-port]]
-==== Changing default transport port
-
-By default, elasticsearch GCE plugin assumes that you run elasticsearch on 9300 default port.
-But you can specify the port value elasticsearch is meant to use using google compute engine metadata `es_port`:
-
-[[cloud-gce-usage-discovery-port-create]]
-===== When creating instance
-
-Add `--metadata es_port=9301` option:
-
-[source,sh]
---------------------------------------------------
-# when creating first instance
-gcloud compute instances create myesnode1 \
-       --scopes=compute-rw,storage-full \
-       --metadata es_port=9301
-
-# when creating an instance from an image
-gcloud compute instances create myesnode2 --image=elasticsearch-1-0-0-RC1 \
-       --zone europe-west1-a --machine-type f1-micro --scopes=compute-rw \
-       --metadata es_port=9301
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-port-run]]
-===== On a running instance
-
-[source,sh]
---------------------------------------------------
-gcloud compute instances add-metadata myesnode1 \
-       --zone europe-west1-a \
-       --metadata es_port=9301
---------------------------------------------------
-
-
-[[cloud-gce-usage-discovery-tips]]
-==== GCE Tips
-
-[[cloud-gce-usage-discovery-tips-projectid]]
-===== Store project id locally
-
-If you don't want to repeat the project id each time, you can save it in the local gcloud config
-
-[source,sh]
---------------------------------------------------
-gcloud config set project es-cloud
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-tips-permissions]]
-===== Machine Permissions
-
-If you have created a machine without the correct permissions, you will see `403 unauthorized` error messages. The only
-way to alter these permissions is to delete the instance (NOT THE DISK). Then create another with the correct permissions.
-
-Creating machines with gcloud::
-+
---
-Ensure the following flags are set:
-
-[source,text]
---------------------------------------------------
---scopes=compute-rw
---------------------------------------------------
---
-
-Creating with console (web)::
-+
---
-When creating an instance using the web portal, click _Show advanced options_.
-
-At the bottom of the page, under `PROJECT ACCESS`, choose `>> Compute >> Read Write`.
---
-
-Creating with knife google::
-+
---
-Set the service account scopes when creating the machine:
-
-[source,sh]
---------------------------------------------------
-knife google server create www1 \
-    -m n1-standard-1 \
-    -I debian-8 \
-    -Z us-central1-a \
-    -i ~/.ssh/id_rsa \
-    -x jdoe \
-    --gce-service-account-scopes https://www.googleapis.com/auth/compute.full_control
---------------------------------------------------
-
-Or, you may use the alias:
-
-[source,sh]
---------------------------------------------------
-    --gce-service-account-scopes compute-rw
---------------------------------------------------
---
-
-[[cloud-gce-usage-discovery-testing]]
-==== Testing GCE
-
-Integrations tests in this plugin require working GCE configuration and
-therefore disabled by default. To enable tests prepare a config file
-elasticsearch.yml with the following content:
-
-[source,yaml]
---------------------------------------------------
-cloud:
-  gce:
-      project_id: es-cloud
-      zone: europe-west1-a
-discovery:
-      type: gce
---------------------------------------------------
-
-Replaces `project_id` and `zone` with your settings.
-
-To run test:
-
-[source,sh]
---------------------------------------------------
-mvn -Dtests.gce=true -Dtests.config=/path/to/config/file/elasticsearch.yml clean test
---------------------------------------------------
diff --git a/docs/plugins/discovery-gce.asciidoc b/docs/plugins/discovery-gce.asciidoc
new file mode 100644
index 0000000..fef8646
--- /dev/null
+++ b/docs/plugins/discovery-gce.asciidoc
@@ -0,0 +1,500 @@
+[[discovery-gce]]
+=== GCE Discovery Plugin
+
+The Google Compute Engine Discovery plugin uses the GCE API for unicast discovery.
+
+[[discovery-gce-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install discovery-gce
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[discovery-gce-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove discovery-gce
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[discovery-gce-usage]]
+==== GCE Virtual Machine Discovery
+
+Google Compute Engine VM discovery allows to use the google APIs to perform automatic discovery (similar to multicast
+in non hostile multicast environments). Here is a simple sample configuration:
+
+[source,yaml]
+--------------------------------------------------
+cloud:
+  gce:
+      project_id: <your-google-project-id>
+      zone: <your-zone>
+discovery:
+      type: gce
+--------------------------------------------------
+
+
+[IMPORTANT]
+.Binding the network host
+==============================================
+
+It's important to define `network.host` as by default it's bound to `localhost`.
+
+You can use {ref}/modules-network.html[core network host settings] or
+<<discovery-gce-network-host,gce specific host settings>>:
+
+==============================================
+
+[[discovery-gce-network-host]]
+==== GCE Network Host
+
+When the `discovery-gce` plugin is installed, the following are also allowed
+as valid network host settings:
+
+[cols="<,<",options="header",]
+|==================================================================
+|GCE Host Value |Description
+|`_gce:privateIp:X_` |The private IP address of the machine for a given network interface.
+|`_gce:hostname_` |The hostname of the machine.
+|`_gce_` |Same as `_gce:privateIp:0_` (recommended).
+|==================================================================
+
+Examples:
+
+[source,yaml]
+--------------------------------------------------
+# get the IP address from network interface 1
+network.host: _gce:privateIp:1_
+# shortcut for _gce:privateIp:0_
+network.host: _gce_
+# Using GCE internal hostname (recommended)
+network.host: _gce:hostname_
+--------------------------------------------------
+
+[[discovery-gce-usage-short]]
+===== How to start (short story)
+
+* Create Google Compute Engine instance (with compute rw permissions)
+* Install Elasticsearch
+* Install Google Compute Engine Cloud plugin
+* Modify `elasticsearch.yml` file
+* Start Elasticsearch
+
+[[discovery-gce-usage-long]]
+==== Setting up GCE Discovery
+
+
+[[discovery-gce-usage-long-prerequisites]]
+===== Prerequisites
+
+Before starting, you need:
+
+* Your project ID, e.g. `es-cloud`. Get it from https://code.google.com/apis/console/[Google API Console].
+* To install https://developers.google.com/cloud/sdk/[Google Cloud SDK]
+
+If you did not set it yet, you can define your default project you will work on:
+
+[source,sh]
+--------------------------------------------------
+gcloud config set project es-cloud
+--------------------------------------------------
+
+[[discovery-gce-usage-long-login]]
+===== Login to Google Cloud
+
+If you haven't already, login to Google Cloud
+
+[source,sh]
+--------------------------------------------------
+gcloud auth login
+--------------------------------------------------
+
+This will open your browser. You will be asked to sign-in to a Google account and
+authorize access to the Google Cloud SDK.
+
+[[discovery-gce-usage-long-first-instance]]
+===== Creating your first instance
+
+
+[source,sh]
+--------------------------------------------------
+gcloud compute instances create myesnode1 \
+       --zone <your-zone> \
+       --scopes compute-rw
+--------------------------------------------------
+
+When done, a report like this one should appears:
+
+[source,text]
+--------------------------------------------------
+Created [https://www.googleapis.com/compute/v1/projects/es-cloud-1070/zones/us-central1-f/instances/myesnode1].
+NAME      ZONE          MACHINE_TYPE  PREEMPTIBLE INTERNAL_IP   EXTERNAL_IP   STATUS
+myesnode1 us-central1-f n1-standard-1             10.240.133.54 104.197.94.25 RUNNING
+--------------------------------------------------
+
+You can now connect to your instance:
+
+[source,sh]
+--------------------------------------------------
+# Connect using google cloud SDK
+gcloud compute ssh myesnode1 --zone europe-west1-a
+
+# Or using SSH with external IP address
+ssh -i ~/.ssh/google_compute_engine 192.158.29.199
+--------------------------------------------------
+
+[IMPORTANT]
+.Service Account Permissions
+==============================================
+
+It's important when creating an instance that the correct permissions are set. At a minimum, you must ensure you have:
+
+[source,text]
+--------------------------------------------------
+scopes=compute-rw
+--------------------------------------------------
+
+Failing to set this will result in unauthorized messages when starting Elasticsearch.
+See <<discovery-gce-usage-tips-permissions>>.
+==============================================
+
+
+Once connected, install Elasticsearch:
+
+[source,sh]
+--------------------------------------------------
+sudo apt-get update
+
+# Download Elasticsearch
+wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-2.0.0.deb
+
+# Prepare Java installation (Oracle)
+sudo echo "deb http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main" | sudo tee /etc/apt/sources.list.d/webupd8team-java.list
+sudo echo "deb-src http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main" | sudo tee -a /etc/apt/sources.list.d/webupd8team-java.list
+sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys EEA14886
+sudo apt-get update
+sudo apt-get install oracle-java8-installer
+
+# Prepare Java installation (or OpenJDK)
+# sudo apt-get install java8-runtime-headless
+
+# Prepare Elasticsearch installation
+sudo dpkg -i elasticsearch-2.0.0.deb
+--------------------------------------------------
+
+[[discovery-gce-usage-long-install-plugin]]
+===== Install elasticsearch discovery gce plugin
+
+Install the plugin:
+
+[source,sh]
+--------------------------------------------------
+# Use Plugin Manager to install it
+sudo bin/plugin install discovery-gce
+--------------------------------------------------
+
+Open the `elasticsearch.yml` file:
+
+[source,sh]
+--------------------------------------------------
+sudo vi /etc/elasticsearch/elasticsearch.yml
+--------------------------------------------------
+
+And add the following lines:
+
+[source,yaml]
+--------------------------------------------------
+cloud:
+  gce:
+      project_id: es-cloud
+      zone: europe-west1-a
+discovery:
+      type: gce
+--------------------------------------------------
+
+
+Start elasticsearch:
+
+[source,sh]
+--------------------------------------------------
+sudo /etc/init.d/elasticsearch start
+--------------------------------------------------
+
+If anything goes wrong, you should check logs:
+
+[source,sh]
+--------------------------------------------------
+tail -f /var/log/elasticsearch/elasticsearch.log
+--------------------------------------------------
+
+If needed, you can change log level to `TRACE` by opening `logging.yml`:
+
+[source,sh]
+--------------------------------------------------
+sudo vi /etc/elasticsearch/logging.yml
+--------------------------------------------------
+
+and adding the following line:
+
+[source,yaml]
+--------------------------------------------------
+# discovery
+discovery.gce: TRACE
+--------------------------------------------------
+
+
+
+[[discovery-gce-usage-cloning]]
+==== Cloning your existing machine
+
+In order to build a cluster on many nodes, you can clone your configured instance to new nodes.
+You won't have to reinstall everything!
+
+First create an image of your running instance and upload it to Google Cloud Storage:
+
+[source,sh]
+--------------------------------------------------
+# Create an image of your current instance
+sudo /usr/bin/gcimagebundle -d /dev/sda -o /tmp/
+
+# An image has been created in `/tmp` directory:
+ls /tmp
+e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz
+
+# Upload your image to Google Cloud Storage:
+# Create a bucket to hold your image, let's say `esimage`:
+gsutil mb gs://esimage
+
+# Copy your image to this bucket:
+gsutil cp /tmp/e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz gs://esimage
+
+# Then add your image to images collection:
+gcloud compute images create elasticsearch-2-0-0 --source-uri gs://esimage/e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz
+
+# If the previous command did not work for you, logout from your instance
+# and launch the same command from your local machine.
+--------------------------------------------------
+
+[[discovery-gce-usage-start-new-instances]]
+===== Start new instances
+
+As you have now an image, you can create as many instances as you need:
+
+[source,sh]
+--------------------------------------------------
+# Just change node name (here myesnode2)
+gcloud compute instances create myesnode2 --image elasticsearch-2-0-0 --zone europe-west1-a
+
+# If you want to provide all details directly, you can use:
+gcloud compute instances create myesnode2 --image=elasticsearch-2-0-0 \
+       --zone europe-west1-a --machine-type f1-micro --scopes=compute-rw
+--------------------------------------------------
+
+[[discovery-gce-usage-remove-instance]]
+===== Remove an instance (aka shut it down)
+
+You can use https://cloud.google.com/console[Google Cloud Console] or CLI to manage your instances:
+
+[source,sh]
+--------------------------------------------------
+# Stopping and removing instances
+gcloud compute instances delete myesnode1 myesnode2 \
+       --zone=europe-west1-a
+
+# Consider removing disk as well if you don't need them anymore
+gcloud compute disks deleted boot-myesnode1 boot-myesnode2  \
+       --zone=europe-west1-a
+--------------------------------------------------
+
+[[discovery-gce-usage-zones]]
+==== Using GCE zones
+
+`cloud.gce.zone` helps to retrieve instances running in a given zone. It should be one of the
+https://developers.google.com/compute/docs/zones#available[GCE supported zones].
+
+The GCE discovery can support multi zones although you need to be aware of network latency between zones.
+To enable discovery across more than one zone, just enter add your zone list to `cloud.gce.zone` setting:
+
+[source,yaml]
+--------------------------------------------------
+cloud:
+  gce:
+      project_id: <your-google-project-id>
+      zone: ["<your-zone1>", "<your-zone2>"]
+discovery:
+      type: gce
+--------------------------------------------------
+
+
+
+[[discovery-gce-usage-tags]]
+==== Filtering by tags
+
+The GCE discovery can also filter machines to include in the cluster based on tags using `discovery.gce.tags` settings.
+For example, setting `discovery.gce.tags` to `dev` will only filter instances having a tag set to `dev`. Several tags
+set will require all of those tags to be set for the instance to be included.
+
+One practical use for tag filtering is when an GCE cluster contains many nodes that are not running
+elasticsearch. In this case (particularly with high ping_timeout values) there is a risk that a new node's discovery
+phase will end before it has found the cluster (which will result in it declaring itself master of a new cluster
+with the same name - highly undesirable). Adding tag on elasticsearch GCE nodes and then filtering by that
+tag will resolve this issue.
+
+Add your tag when building the new instance:
+
+[source,sh]
+--------------------------------------------------
+gcloud compute instances create myesnode1 --project=es-cloud \
+       --scopes=compute-rw \
+       --tags=elasticsearch,dev
+--------------------------------------------------
+
+Then, define it in `elasticsearch.yml`:
+
+[source,yaml]
+--------------------------------------------------
+cloud:
+  gce:
+      project_id: es-cloud
+      zone: europe-west1-a
+discovery:
+      type: gce
+      gce:
+            tags: elasticsearch, dev
+--------------------------------------------------
+
+[[discovery-gce-usage-port]]
+==== Changing default transport port
+
+By default, elasticsearch GCE plugin assumes that you run elasticsearch on 9300 default port.
+But you can specify the port value elasticsearch is meant to use using google compute engine metadata `es_port`:
+
+[[discovery-gce-usage-port-create]]
+===== When creating instance
+
+Add `--metadata es_port=9301` option:
+
+[source,sh]
+--------------------------------------------------
+# when creating first instance
+gcloud compute instances create myesnode1 \
+       --scopes=compute-rw,storage-full \
+       --metadata es_port=9301
+
+# when creating an instance from an image
+gcloud compute instances create myesnode2 --image=elasticsearch-1-0-0-RC1 \
+       --zone europe-west1-a --machine-type f1-micro --scopes=compute-rw \
+       --metadata es_port=9301
+--------------------------------------------------
+
+[[discovery-gce-usage-port-run]]
+===== On a running instance
+
+[source,sh]
+--------------------------------------------------
+gcloud compute instances add-metadata myesnode1 \
+       --zone europe-west1-a \
+       --metadata es_port=9301
+--------------------------------------------------
+
+
+[[discovery-gce-usage-tips]]
+==== GCE Tips
+
+[[discovery-gce-usage-tips-projectid]]
+===== Store project id locally
+
+If you don't want to repeat the project id each time, you can save it in the local gcloud config
+
+[source,sh]
+--------------------------------------------------
+gcloud config set project es-cloud
+--------------------------------------------------
+
+[[discovery-gce-usage-tips-permissions]]
+===== Machine Permissions
+
+If you have created a machine without the correct permissions, you will see `403 unauthorized` error messages. The only
+way to alter these permissions is to delete the instance (NOT THE DISK). Then create another with the correct permissions.
+
+Creating machines with gcloud::
++
+--
+Ensure the following flags are set:
+
+[source,text]
+--------------------------------------------------
+--scopes=compute-rw
+--------------------------------------------------
+--
+
+Creating with console (web)::
++
+--
+When creating an instance using the web portal, click _Show advanced options_.
+
+At the bottom of the page, under `PROJECT ACCESS`, choose `>> Compute >> Read Write`.
+--
+
+Creating with knife google::
++
+--
+Set the service account scopes when creating the machine:
+
+[source,sh]
+--------------------------------------------------
+knife google server create www1 \
+    -m n1-standard-1 \
+    -I debian-8 \
+    -Z us-central1-a \
+    -i ~/.ssh/id_rsa \
+    -x jdoe \
+    --gce-service-account-scopes https://www.googleapis.com/auth/compute.full_control
+--------------------------------------------------
+
+Or, you may use the alias:
+
+[source,sh]
+--------------------------------------------------
+    --gce-service-account-scopes compute-rw
+--------------------------------------------------
+--
+
+[[discovery-gce-usage-testing]]
+==== Testing GCE
+
+Integrations tests in this plugin require working GCE configuration and
+therefore disabled by default. To enable tests prepare a config file
+elasticsearch.yml with the following content:
+
+[source,yaml]
+--------------------------------------------------
+cloud:
+  gce:
+      project_id: es-cloud
+      zone: europe-west1-a
+discovery:
+      type: gce
+--------------------------------------------------
+
+Replaces `project_id` and `zone` with your settings.
+
+To run test:
+
+[source,sh]
+--------------------------------------------------
+mvn -Dtests.gce=true -Dtests.config=/path/to/config/file/elasticsearch.yml clean test
+--------------------------------------------------
diff --git a/docs/plugins/discovery.asciidoc b/docs/plugins/discovery.asciidoc
index 289a020..1fab942 100644
--- a/docs/plugins/discovery.asciidoc
+++ b/docs/plugins/discovery.asciidoc
@@ -17,9 +17,9 @@ The EC2 discovery plugin uses the https://github.com/aws/aws-sdk-java[AWS API] f
 
 The Azure discovery plugin uses the Azure API for unicast discovery.
 
-<<cloud-gce,GCE Cloud>>::
+<<discovery-gce,GCE discovery>>::
 
-The Google Compute Engine Cloud plugin uses the GCE API for unicast discovery.
+The Google Compute Engine discovery plugin uses the GCE API for unicast discovery.
 
 <<discovery-multicast,Multicast>>::
 
@@ -38,7 +38,7 @@ include::discovery-ec2.asciidoc[]
 
 include::discovery-azure.asciidoc[]
 
-include::cloud-gce.asciidoc[]
+include::discovery-gce.asciidoc[]
 
 include::discovery-multicast.asciidoc[]
 
diff --git a/docs/plugins/plugin-script.asciidoc b/docs/plugins/plugin-script.asciidoc
index bae6613..52ff574 100644
--- a/docs/plugins/plugin-script.asciidoc
+++ b/docs/plugins/plugin-script.asciidoc
@@ -131,6 +131,8 @@ Plugins can be removed manually, by deleting the appropriate directory under
 sudo bin/plugin remove [pluginname]
 -----------------------------------
 
+After a Java plugin has been removed, you will need to restart the node to complete the removal process.
+
 === Other command line parameters
 
 The `plugin` scripts supports a number of other command line parameters:
diff --git a/docs/reference/aggregations/bucket/datehistogram-aggregation.asciidoc b/docs/reference/aggregations/bucket/datehistogram-aggregation.asciidoc
index cdd7601..5afff0c 100644
--- a/docs/reference/aggregations/bucket/datehistogram-aggregation.asciidoc
+++ b/docs/reference/aggregations/bucket/datehistogram-aggregation.asciidoc
@@ -281,7 +281,7 @@ had a value.
 {
     "aggs" : {
         "publish_date" : {
-             "datehistogram" : {
+             "date_histogram" : {
                  "field" : "publish_date",
                  "interval": "year",
                  "missing": "2000-01-01" <1>
diff --git a/docs/reference/aggregations/pipeline.asciidoc b/docs/reference/aggregations/pipeline.asciidoc
index 4410db3..e4cdae5 100644
--- a/docs/reference/aggregations/pipeline.asciidoc
+++ b/docs/reference/aggregations/pipeline.asciidoc
@@ -2,8 +2,6 @@
 
 == Pipeline Aggregations
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 Pipeline aggregations work on the outputs produced from other aggregations rather than from document sets, adding
diff --git a/docs/reference/aggregations/pipeline/avg-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/avg-bucket-aggregation.asciidoc
index b2b9d93f..541ffec 100644
--- a/docs/reference/aggregations/pipeline/avg-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/avg-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-avg-bucket-aggregation]]
 === Avg Bucket Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A sibling pipeline aggregation which calculates the (mean) average value of a specified metric in a sibling aggregation. 
diff --git a/docs/reference/aggregations/pipeline/bucket-script-aggregation.asciidoc b/docs/reference/aggregations/pipeline/bucket-script-aggregation.asciidoc
index 81372c1..b1bbfcd 100644
--- a/docs/reference/aggregations/pipeline/bucket-script-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/bucket-script-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-bucket-script-aggregation]]
 === Bucket Script Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A parent pipeline aggregation which executes a script which can perform per bucket computations on specified metrics 
diff --git a/docs/reference/aggregations/pipeline/bucket-selector-aggregation.asciidoc b/docs/reference/aggregations/pipeline/bucket-selector-aggregation.asciidoc
index cef1e67..2d80abc 100644
--- a/docs/reference/aggregations/pipeline/bucket-selector-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/bucket-selector-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-bucket-selector-aggregation]]
 === Bucket Selector Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A parent pipeline aggregation which executes a script which determines whether the current bucket will be retained 
diff --git a/docs/reference/aggregations/pipeline/cumulative-sum-aggregation.asciidoc b/docs/reference/aggregations/pipeline/cumulative-sum-aggregation.asciidoc
index 823c5c8..e29dbbe 100644
--- a/docs/reference/aggregations/pipeline/cumulative-sum-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/cumulative-sum-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-cumulative-sum-aggregation]]
 === Cumulative Sum Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A parent pipeline aggregation which calculates the cumulative sum of a specified metric in a parent histogram (or date_histogram) 
diff --git a/docs/reference/aggregations/pipeline/derivative-aggregation.asciidoc b/docs/reference/aggregations/pipeline/derivative-aggregation.asciidoc
index 48296ca..f68a811 100644
--- a/docs/reference/aggregations/pipeline/derivative-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/derivative-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-derivative-aggregation]]
 === Derivative Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A parent pipeline aggregation which calculates the derivative of a specified metric in a parent histogram (or date_histogram) 
diff --git a/docs/reference/aggregations/pipeline/extended-stats-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/extended-stats-bucket-aggregation.asciidoc
index bbf610a..0a44685 100644
--- a/docs/reference/aggregations/pipeline/extended-stats-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/extended-stats-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-extended-stats-bucket-aggregation]]
 === Extended Stats Bucket Aggregation
 
-coming[2.1.0]
-
 experimental[]
 
 A sibling pipeline aggregation which calculates a variety of stats across all bucket of a specified metric in a sibling aggregation.
diff --git a/docs/reference/aggregations/pipeline/max-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/max-bucket-aggregation.asciidoc
index 310a643..96094d0 100644
--- a/docs/reference/aggregations/pipeline/max-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/max-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-max-bucket-aggregation]]
 === Max Bucket Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A sibling pipeline aggregation which identifies the bucket(s) with the maximum value of a specified metric in a sibling aggregation
diff --git a/docs/reference/aggregations/pipeline/min-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/min-bucket-aggregation.asciidoc
index 11d3d55..c970384 100644
--- a/docs/reference/aggregations/pipeline/min-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/min-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-min-bucket-aggregation]]
 === Min Bucket Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A sibling pipeline aggregation which identifies the bucket(s) with the minimum value of a specified metric in a sibling aggregation 
diff --git a/docs/reference/aggregations/pipeline/movavg-aggregation.asciidoc b/docs/reference/aggregations/pipeline/movavg-aggregation.asciidoc
index 6fe91cb..968c596 100644
--- a/docs/reference/aggregations/pipeline/movavg-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/movavg-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-movavg-aggregation]]
 === Moving Average Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 Given an ordered series of data, the Moving Average aggregation will slide a window across the data and emit the average
diff --git a/docs/reference/aggregations/pipeline/percentiles-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/percentiles-bucket-aggregation.asciidoc
index 2476969..4e6423f 100644
--- a/docs/reference/aggregations/pipeline/percentiles-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/percentiles-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-percentiles-bucket-aggregation]]
 === Percentiles Bucket Aggregation
 
-coming[2.1.0]
-
 experimental[]
 
 A sibling pipeline aggregation which calculates percentiles across all bucket of a specified metric in a sibling aggregation.
diff --git a/docs/reference/aggregations/pipeline/serial-diff-aggregation.asciidoc b/docs/reference/aggregations/pipeline/serial-diff-aggregation.asciidoc
index 7193510..17cfea9 100644
--- a/docs/reference/aggregations/pipeline/serial-diff-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/serial-diff-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-serialdiff-aggregation]]
 === Serial Differencing Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 Serial differencing is a technique where values in a time series are subtracted from itself at
diff --git a/docs/reference/aggregations/pipeline/stats-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/stats-bucket-aggregation.asciidoc
index 7d6d24d..f524032 100644
--- a/docs/reference/aggregations/pipeline/stats-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/stats-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-stats-bucket-aggregation]]
 === Stats Bucket Aggregation
 
-coming[2.1.0]
-
 experimental[]
 
 A sibling pipeline aggregation which calculates a variety of stats across all bucket of a specified metric in a sibling aggregation.
diff --git a/docs/reference/aggregations/pipeline/sum-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/sum-bucket-aggregation.asciidoc
index 56d786f..52022b3 100644
--- a/docs/reference/aggregations/pipeline/sum-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/sum-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-sum-bucket-aggregation]]
 === Sum Bucket Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A sibling pipeline aggregation which calculates the sum across all bucket of a specified metric in a sibling aggregation. 
diff --git a/docs/reference/docs/bulk.asciidoc b/docs/reference/docs/bulk.asciidoc
index b9b7d47..ef066eb 100644
--- a/docs/reference/docs/bulk.asciidoc
+++ b/docs/reference/docs/bulk.asciidoc
@@ -131,8 +131,6 @@ operation based on the `_parent` / `_routing` mapping.
 [[bulk-timestamp]]
 === Timestamp
 
-deprecated[2.0.0,The `_timestamp` field is deprecated.  Instead, use a normal <<date,`date`>> field and set its value explicitly]
-
 Each bulk item can include the timestamp value using the
 `_timestamp`/`timestamp` field. It automatically follows the behavior of
 the index operation based on the `_timestamp` mapping.
@@ -141,8 +139,6 @@ the index operation based on the `_timestamp` mapping.
 [[bulk-ttl]]
 === TTL
 
-deprecated[2.0.0,The current `_ttl` implementation is deprecated and will be replaced with a different implementation in a future version]
-
 Each bulk item can include the ttl value using the `_ttl`/`ttl` field.
 It automatically follows the behavior of the index operation based on
 the `_ttl` mapping.
diff --git a/docs/reference/docs/index_.asciidoc b/docs/reference/docs/index_.asciidoc
index ab542c5..089af3e 100644
--- a/docs/reference/docs/index_.asciidoc
+++ b/docs/reference/docs/index_.asciidoc
@@ -257,8 +257,6 @@ specified using the `routing` parameter.
 [[index-timestamp]]
 === Timestamp
 
-deprecated[2.0.0,The `_timestamp` field is deprecated.  Instead, use a normal <<date,`date`>> field and set its value explicitly]
-
 A document can be indexed with a `timestamp` associated with it. The
 `timestamp` value of a document can be set using the `timestamp`
 parameter. For example:
@@ -281,8 +279,6 @@ page>>.
 [[index-ttl]]
 === TTL
 
-deprecated[2.0.0,The current `_ttl` implementation is deprecated and will be replaced with a different implementation in a future version]
-
 
 A document can be indexed with a `ttl` (time to live) associated with
 it. Expired documents will be expunged automatically. The expiration
diff --git a/docs/reference/docs/termvectors.asciidoc b/docs/reference/docs/termvectors.asciidoc
index 7530ff7..0e10843 100644
--- a/docs/reference/docs/termvectors.asciidoc
+++ b/docs/reference/docs/termvectors.asciidoc
@@ -81,8 +81,6 @@ omit :
 [float]
 ==== Distributed frequencies
 
-coming[2.0.0-beta1]
-
 Setting `dfs` to `true` (default is `false`) will return the term statistics
 or the field statistics of the entire index, and not just at the shard. Use it
 with caution as distributed frequencies can have a serious performance impact.
@@ -90,8 +88,6 @@ with caution as distributed frequencies can have a serious performance impact.
 [float]
 ==== Terms Filtering
 
-coming[2.0.0-beta1]
-
 With the parameter `filter`, the terms returned could also be filtered based
 on their tf-idf scores. This could be useful in order find out a good
 characteristic vector of a document. This feature works in a similar manner to
diff --git a/docs/reference/getting-started.asciidoc b/docs/reference/getting-started.asciidoc
index 2731fca..a30046e 100755
--- a/docs/reference/getting-started.asciidoc
+++ b/docs/reference/getting-started.asciidoc
@@ -862,7 +862,7 @@ In the previous section, we skipped over a little detail called the document sco
 
 But queries do not always need to produce scores, in particular when they are only used for "filtering" the document set. Elasticsearch detects these situations and automatically optimizes query execution in order not to compute useless scores.
 
-The <<query-dsl-range-query,`range` query>> that we introduced in the previous section also supports `filter` clauses which allow to use a query to restrict the documents that will be matched by other clauses, without changing how scores are computed. As an example, let's introduce the <<query-dsl-range-query,`range` query>>, which allows us to filter documents by a range of values. This is generally used for numeric or date filtering.
+The <<query-dsl-bool-query,`bool` query>> that we introduced in the previous section also supports `filter` clauses which allow to use a query to restrict the documents that will be matched by other clauses, without changing how scores are computed. As an example, let's introduce the <<query-dsl-range-query,`range` query>>, which allows us to filter documents by a range of values. This is generally used for numeric or date filtering.
 
 This example uses a bool query to return all accounts with balances between 20000 and 30000, inclusive. In other words, we want to find accounts with a balance that is greater than or equal to 20000 and less than or equal to 30000.
 
diff --git a/docs/reference/index.asciidoc b/docs/reference/index.asciidoc
index e36a66a..8e34747 100644
--- a/docs/reference/index.asciidoc
+++ b/docs/reference/index.asciidoc
@@ -1,11 +1,12 @@
 [[elasticsearch-reference]]
 = Elasticsearch Reference
 
-:version:   2.0.0-beta1
-:branch:    2.0
-:jdk:       1.8.0_25
-:defguide:  https://www.elastic.co/guide/en/elasticsearch/guide/current
-:plugins:   https://www.elastic.co/guide/en/elasticsearch/plugins/master
+:version:       3.0.0-beta1
+:major-version: 3.x
+:branch:        3.0
+:jdk:           1.8.0_25
+:defguide:      https://www.elastic.co/guide/en/elasticsearch/guide/current
+:plugins:       https://www.elastic.co/guide/en/elasticsearch/plugins/master
 
 include::getting-started.asciidoc[]
 
diff --git a/docs/reference/indices/analyze.asciidoc b/docs/reference/indices/analyze.asciidoc
index d17f409..1a256a6 100644
--- a/docs/reference/indices/analyze.asciidoc
+++ b/docs/reference/indices/analyze.asciidoc
@@ -16,8 +16,6 @@ curl -XGET 'localhost:9200/_analyze' -d '
 }'
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 If text parameter is provided as array of strings, it is analyzed as a multi-valued field.
 
 [source,js]
@@ -29,8 +27,6 @@ curl -XGET 'localhost:9200/_analyze' -d '
 }'
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 Or by building a custom transient analyzer out of tokenizers,
 token filters and char filters. Token filters can use the shorter 'filters'
 parameter name:
@@ -53,8 +49,6 @@ curl -XGET 'localhost:9200/_analyze' -d '
 }'
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 It can also run against a specific index:
 
 [source,js]
@@ -78,8 +72,6 @@ curl -XGET 'localhost:9200/test/_analyze' -d '
 }'
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 Also, the analyzer can be derived based on a field mapping, for example:
 
 [source,js]
@@ -91,8 +83,6 @@ curl -XGET 'localhost:9200/test/_analyze' -d '
 }'
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 Will cause the analysis to happen based on the analyzer configured in the
 mapping for `obj1.field1` (and if not, the default index analyzer).
 
diff --git a/docs/reference/mapping/fields/parent-field.asciidoc b/docs/reference/mapping/fields/parent-field.asciidoc
index 22e46c4..64f4a99 100644
--- a/docs/reference/mapping/fields/parent-field.asciidoc
+++ b/docs/reference/mapping/fields/parent-field.asciidoc
@@ -1,8 +1,6 @@
 [[mapping-parent-field]]
 === `_parent` field
 
-added[2.0.0-beta1,The parent-child implementation has been completely rewritten. It is advisable to reindex any 1.x indices which use parent-child to take advantage of the new optimizations]
-
 A parent-child relationship can be established between documents in the same
 index by making one mapping type the parent of another:
 
diff --git a/docs/reference/mapping/fields/timestamp-field.asciidoc b/docs/reference/mapping/fields/timestamp-field.asciidoc
index 3f4bf8a..5971a02 100644
--- a/docs/reference/mapping/fields/timestamp-field.asciidoc
+++ b/docs/reference/mapping/fields/timestamp-field.asciidoc
@@ -1,8 +1,6 @@
 [[mapping-timestamp-field]]
 === `_timestamp` field
 
-deprecated[2.0.0,The `_timestamp` field is deprecated.  Instead, use a normal <<date,`date`>> field and set its value explicitly]
-
 The `_timestamp` field, when enabled, allows a timestamp to be indexed and
 stored with a document. The timestamp may be specified manually, generated
 automatically, or set to a default value:
diff --git a/docs/reference/mapping/fields/ttl-field.asciidoc b/docs/reference/mapping/fields/ttl-field.asciidoc
index 5394d28..07ce8a8 100644
--- a/docs/reference/mapping/fields/ttl-field.asciidoc
+++ b/docs/reference/mapping/fields/ttl-field.asciidoc
@@ -1,8 +1,6 @@
 [[mapping-ttl-field]]
 === `_ttl` field
 
-deprecated[2.0.0,The current `_ttl` implementation is deprecated and will be replaced with a different implementation in a future version]
-
 Some types of documents, such as session data or special offers, come with an
 expiration date. The `_ttl` field allows you to specify the minimum time a
 document should live, after which time the document is deleted automatically.
diff --git a/docs/reference/migration/migrate_2_0/crud.asciidoc b/docs/reference/migration/migrate_2_0/crud.asciidoc
index 060cfc7..a7c947e 100644
--- a/docs/reference/migration/migrate_2_0/crud.asciidoc
+++ b/docs/reference/migration/migrate_2_0/crud.asciidoc
@@ -76,7 +76,7 @@ might return:
   "_index":     "my_index",
   "_type":      "my_type",
   "_id":        "1",
-  "_timestamp": 10000000, <1>,
+  "_timestamp": 10000000, <1>
   "_source": {
     "foo" : [ "bar" ]
   }
diff --git a/docs/reference/migration/migrate_2_0/java.asciidoc b/docs/reference/migration/migrate_2_0/java.asciidoc
index 9871df4..ef9c7ef 100644
--- a/docs/reference/migration/migrate_2_0/java.asciidoc
+++ b/docs/reference/migration/migrate_2_0/java.asciidoc
@@ -22,8 +22,8 @@ Client client = TransportClient.builder().settings(settings).build();
 --------------------------------------------------
 
 The transport client also no longer supports loading settings from config files.
-If you have have a config file, you can load into settings yourself before
-consturcting the transport client:
+If you have a config file, you can load it into settings yourself before
+constructing the transport client:
 
 [source,java]
 --------------------------------------------------
@@ -32,6 +32,13 @@ Settings settings = Settings.settingsBuilder()
 Client client = TransportClient.builder().settings(settings).build();
 --------------------------------------------------
 
+==== Exception are only thrown on total failure
+
+Previously, many APIs would throw an exception if any shard failed to execute
+the request. Now the exception is only thrown if all shards fail the request.
+The responses for these APIs will always have a `getShardFailures` method that
+you can and should check for failures.
+
 ==== Automatically thread client listeners
 
 Previously, the user had to set request listener threads to `true` when on the
@@ -109,7 +116,7 @@ new InetSocketTransportAddress(new InetSocketAddress("127.0.0.1", 0));
 Elasticsearch used to shade its dependencies and to relocate packages. We no longer use shading or relocation.
 You might need to change your imports to the original package names:
 
-* `com.google.common` was `org.elasticsearch.common` 
+* `com.google.common` was `org.elasticsearch.common`
 * `com.carrotsearch.hppc` was `org.elasticsearch.common.hppc`
 * `jsr166e` was `org.elasticsearch.common.util.concurrent.jsr166e`
 * `com.fasterxml.jackson` was `org.elasticsearch.common.jackson`
@@ -121,4 +128,3 @@ You might need to change your imports to the original package names:
 * `com.tdunning.math.stats` was `org.elasticsearch.common.stats`
 * `org.apache.commons.lang` was `org.elasticsearch.common.lang`
 * `org.apache.commons.cli` was `org.elasticsearch.common.cli.commons`
- 
diff --git a/docs/reference/migration/migrate_2_0/mapping.asciidoc b/docs/reference/migration/migrate_2_0/mapping.asciidoc
index c028a2e..86d81b2 100644
--- a/docs/reference/migration/migrate_2_0/mapping.asciidoc
+++ b/docs/reference/migration/migrate_2_0/mapping.asciidoc
@@ -160,7 +160,7 @@ You can no longer create fields with dots in the name.
 
 In 1.x, Elasticsearch would issue a warning if a type name included a dot,
 e.g. `my.type`.  Now that type names are no longer used to distinguish between
-fields in differnt types, this warning has been relaxed: type names may now
+fields in different types, this warning has been relaxed: type names may now
 contain dots, but they may not *begin* with a dot.  The only exception to this
 is the special `.percolator` type.
 
diff --git a/docs/reference/migration/migrate_2_0/settings.asciidoc b/docs/reference/migration/migrate_2_0/settings.asciidoc
index 17d36c5..923d506 100644
--- a/docs/reference/migration/migrate_2_0/settings.asciidoc
+++ b/docs/reference/migration/migrate_2_0/settings.asciidoc
@@ -3,7 +3,7 @@
 ==== Command line flags
 
 Command line flags using single dash notation must be now specified as the first arguments.
-For example if previously using: 
+For example if previously using:
 
 [source,sh]
 ---------------
@@ -14,7 +14,7 @@ This will now need to be changed to:
 
 [source,sh]
 ---------------
-./elasticsearch -Des.path.conf=/opt/elasticsearch/conf/test_node --node.name=test_node 
+./elasticsearch -Des.path.conf=/opt/elasticsearch/conf/test_node --node.name=test_node
 ---------------
 
 for the flag to take effect.
@@ -164,3 +164,19 @@ the `logging.yml` configuration file with the `file.layout.conversionPattern`
 setting.
 
 Remove mapping.date.round_ceil setting for date math parsing #8889 (issues: #8556, #8598)
+
+==== Custom config file
+
+It is no longer possible to specify a custom config file with the `CONF_FILE`
+environment variable, or the `-Des.config`, `-Des.default.config`, or
+`-Delasticsearch.config` parameters.
+
+Instead, the config file must be named `elasticsearch.yml` and must be located
+in the default `config/` directory, or in the directory specified in the
+`CONF_DIR` environment variable.
+
+==== `ES_CLASSPATH removed`
+
+The `ES_CLASSPATH` environment variable is no longer used to set the class
+path. External libraries should preferably be loaded using the plugin
+mechanism or, if you really must, be copied to the `lib/` directory.
diff --git a/docs/reference/migration/migrate_3_0.asciidoc b/docs/reference/migration/migrate_3_0.asciidoc
index 897098f..c1ba60c 100644
--- a/docs/reference/migration/migrate_3_0.asciidoc
+++ b/docs/reference/migration/migrate_3_0.asciidoc
@@ -121,6 +121,25 @@ function that it supports and it's able to parse. The function object can then t
 function through the new `toFunction(QueryShardContext)` method, which returns a lucene function to be executed
 on the data node.
 
+==== Cloud AWS plugin
+
+Cloud AWS plugin has been split in two plugins:
+
+* {plugins}/discovery-ec2.html[Discovery EC2 plugin]
+* {plugins}/repository-s3.html[Repository S3 plugin]
+
+==== Cloud Azure plugin
+
+Cloud Azure plugin has been split in three plugins:
+
+* {plugins}/discovery-azure.html[Discovery Azure plugin]
+* {plugins}/repository-azure.html[Repository Azure plugin]
+* {plugins}/store-smb.html[Store SMB plugin]
+
+==== Cloud GCE plugin
+
+Cloud GCE plugin has been renamed to {plugins}/discovery-gce.html[Discovery GCE plugin].
+
 === Java-API
 
 ==== BoostingQueryBuilder
@@ -266,3 +285,8 @@ of string values: see `FilterFunctionScoreQuery.ScoreMode` and `CombineFunction`
 
 For simplicity, only one way of adding the ids to the existing list (empty by default)  is left: `addIds(String...)`
 
+==== DocumentAlreadyExistsException removed
+
+`DocumentAlreadyExistsException` is removed and a `VersionConflictException` is thrown instead (with a better
+error description). This will influence code that use the `IndexRequest.opType()` or `IndexRequest.create()`
+to index a document only if it doesn't already exist.
diff --git a/docs/reference/modules/discovery/gce.asciidoc b/docs/reference/modules/discovery/gce.asciidoc
index bb9c89f..ea367d5 100644
--- a/docs/reference/modules/discovery/gce.asciidoc
+++ b/docs/reference/modules/discovery/gce.asciidoc
@@ -2,5 +2,5 @@
 === Google Compute Engine Discovery
 
 Google Compute Engine (GCE) discovery allows to use the GCE APIs to perform automatic discovery (similar to multicast).
-Please check the https://github.com/elasticsearch/elasticsearch-cloud-gce[plugin website]
-to find the full documentation.
+It is available as a plugin. See {plugins}/discovery-gce.html[discovery-gce] for more information.
+
diff --git a/docs/reference/modules/network.asciidoc b/docs/reference/modules/network.asciidoc
index 70b4d8c..7105d2d 100644
--- a/docs/reference/modules/network.asciidoc
+++ b/docs/reference/modules/network.asciidoc
@@ -54,6 +54,10 @@ provided network interface. For example `_en0:ipv6_`.
 When the `discovery-ec2` plugin is installed, you can use
 {plugins}/discovery-ec2-discovery.html#discovery-ec2-network-host[ec2 specific host settings].
 
+When the `discovery-gce` plugin is installed, you can use
+{plugins}/discovery-gce-network-host.html[gce specific host settings].
+
+
 [float]
 [[tcp-settings]]
 === TCP Settings
diff --git a/docs/reference/modules/scripting.asciidoc b/docs/reference/modules/scripting.asciidoc
index 7729ce2..50be5fd 100644
--- a/docs/reference/modules/scripting.asciidoc
+++ b/docs/reference/modules/scripting.asciidoc
@@ -351,28 +351,86 @@ to `false`.
 [float]
 === Native (Java) Scripts
 
-Even though `groovy` is pretty fast, this allows to register native Java based
-scripts for faster execution.
+Sometimes `groovy` and `expressions` aren't enough. For those times you can
+implement a native script.
 
-In order to allow for scripts, the `NativeScriptFactory` needs to be
-implemented that constructs the script that will be executed. There are
-two main types, one that extends `AbstractExecutableScript` and one that
-extends `AbstractSearchScript` (probably the one most users will extend,
-with additional helper classes in `AbstractLongSearchScript`,
-`AbstractDoubleSearchScript`, and `AbstractFloatSearchScript`).
+The best way to implement a native script is to write a plugin and install it.
+The plugin {plugins}/plugin-authors.html[documentation] has more information on
+how to write a plugin so that Elasticsearch will properly load it.
 
-Registering them can either be done by settings, for example:
-`script.native.my.type` set to `sample.MyNativeScriptFactory` will
-register a script named `my`. Another option is in a plugin, access
-`ScriptModule` and call `registerScript` on it.
+To register the actual script you'll need to implement `NativeScriptFactory`
+to construct the script. The actual script will extend either
+`AbstractExecutableScript` or `AbstractSearchScript`. The second one is likely
+the most useful and has several helpful subclasses you can extend like
+`AbstractLongSearchScript`, `AbstractDoubleSearchScript`, and
+`AbstractFloatSearchScript`. Finally, your plugin should register the native
+script by declaring the `onModule(ScriptModule)` method.
 
-Executing the script is done by specifying the `lang` as `native`, and
-the name of the script as the `script`.
+If you squashed the whole thing into one class it'd look like:
+
+[source,java]
+--------------------------------------------------
+public class MyNativeScriptPlugin extends Plugin {
+    @Override
+    public String name() {
+        return "my-native-script";
+    }
+    @Override
+    public String description() {
+        return "my native script that does something great";
+    }
+    public void onModule(ScriptModule scriptModule) {
+        scriptModule.registerScript("my_script", MyNativeScriptFactory.class);
+    }
+
+    public static class MyNativeScriptFactory implements NativeScriptFactory {
+        @Override
+        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
+            return new MyNativeScript();
+        }
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
+    }
+
+    public static class MyNativeScript extends AbstractFloatSearchScript {
+        @Override
+        public float runAsFloat() {
+            float a = (float) source().get("a");
+            float b = (float) source().get("b");
+            return a * b;
+        }
+    }
+}
+--------------------------------------------------
+
+You can execute the script by specifying its `lang` as `native`, and the name
+of the script as the `id`:
+
+[source,js]
+--------------------------------------------------
+curl -XPOST localhost:9200/_search -d '{
+  "query": {
+    "function_score": {
+      "query": {
+        "match": {
+          "body": "foo"
+        }
+      },
+      "functions": [
+        {
+          "script_score": {
+            "id": "my_script",
+            "lang" : "native"
+          }
+        }
+      ]
+    }
+  }
+}'
+--------------------------------------------------
 
-Note, the scripts need to be in the classpath of elasticsearch. One
-simple way to do it is to create a directory under plugins (choose a
-descriptive name), and place the jar / classes files there. They will be
-automatically loaded.
 
 [float]
 === Lucene Expressions Scripts
@@ -624,16 +682,3 @@ power of the second argument.
 |`hypot(x, y)` |Returns sqrt(_x2_ + _y2_) without intermediate overflow
 or underflow.
 |=======================================================================
-
-[float]
-=== Arithmetic precision in MVEL
-
-When dividing two numbers using MVEL based scripts, the engine tries to
-be smart and adheres to the default behaviour of java. This means if you
-divide two integers (you might have configured the fields as integer in
-the mapping), the result will also be an integer. This means, if a
-calculation like `1/num` is happening in your scripts and `num` is an
-integer with the value of `8`, the result is `0` even though you were
-expecting it to be `0.125`. You may need to enforce precision by
-explicitly using a double like `1.0/num` in order to get the expected
-result.
diff --git a/docs/reference/modules/snapshots.asciidoc b/docs/reference/modules/snapshots.asciidoc
index 4830e43..50ee4df 100644
--- a/docs/reference/modules/snapshots.asciidoc
+++ b/docs/reference/modules/snapshots.asciidoc
@@ -121,7 +121,7 @@ The following settings are supported:
  using size value notation, i.e. 1g, 10m, 5k. Defaults to `null` (unlimited chunk size).
 `max_restore_bytes_per_sec`:: Throttles per node restore rate. Defaults to `40mb` per second.
 `max_snapshot_bytes_per_sec`:: Throttles per node snapshot rate. Defaults to `40mb` per second.
-`readonly`:: Makes repository read-only. coming[2.1.0]  Defaults to `false`.
+`readonly`:: Makes repository read-only.  Defaults to `false`.
 
 [float]
 ===== Read-only URL Repository
@@ -259,7 +259,7 @@ GET /_snapshot/my_backup/_all
 -----------------------------------
 // AUTOSENSE
 
-coming[2.0.0-beta1] A currently running snapshot can be retrieved using the following command:
+A currently running snapshot can be retrieved using the following command:
 
 [source,sh]
 -----------------------------------
diff --git a/docs/reference/query-dsl/mlt-query.asciidoc b/docs/reference/query-dsl/mlt-query.asciidoc
index 9c42881..ee4b695 100644
--- a/docs/reference/query-dsl/mlt-query.asciidoc
+++ b/docs/reference/query-dsl/mlt-query.asciidoc
@@ -149,7 +149,7 @@ input, the other one for term selection and for query formation.
 ==== Document Input Parameters
 
 [horizontal]
-`like`:: coming[2.0.0-beta1]
+`like`::
 The only *required* parameter of the MLT query is `like` and follows a
 versatile syntax, in which the user can specify free form text and/or a single
 or multiple documents (see examples above). The syntax to specify documents is
@@ -162,7 +162,7 @@ follows a similar syntax to the `per_field_analyzer` parameter of the
 Additionally, to provide documents not necessarily present in the index,
 <<docs-termvectors-artificial-doc,artificial documents>> are also supported.
 
-`unlike`:: coming[2.0.0-beta1] 
+`unlike`:: 
 The `unlike` parameter is used in conjunction with `like` in order not to
 select terms found in a chosen set of documents. In other words, we could ask
 for documents `like: "Apple"`, but `unlike: "cake crumble tree"`. The syntax
@@ -172,10 +172,10 @@ is the same as `like`.
 A list of fields to fetch and analyze the text from. Defaults to the `_all`
 field for free text and to all possible fields for document inputs.
 
-`like_text`:: deprecated[2.0.0-beta1,Replaced by `like`]
+`like_text`::
 The text to find documents like it.
 
-`ids` or `docs`:: deprecated[2.0.0-beta1,Replaced by `like`]
+`ids` or `docs`::
 A list of documents following the same syntax as the <<docs-multi-get,Multi GET API>>.
 
 [float]
diff --git a/docs/reference/search/request/scroll.asciidoc b/docs/reference/search/request/scroll.asciidoc
index 2921441..825564d 100644
--- a/docs/reference/search/request/scroll.asciidoc
+++ b/docs/reference/search/request/scroll.asciidoc
@@ -63,8 +63,6 @@ curl -XGET <1> 'localhost:9200/_search/scroll' <2> -d'
 '
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 <1> `GET` or `POST` can be used.
 <2> The URL should not include the `index` or `type` name -- these
     are specified in the original `search` request instead.
@@ -151,8 +149,6 @@ curl -XDELETE localhost:9200/_search/scroll -d '
 }'
 ---------------------------------------
 
-coming[2.0.0-beta1, Body based parameters were added in 2.0.0]
-
 Multiple scroll IDs can be passed as array:
 
 [source,js]
@@ -163,8 +159,6 @@ curl -XDELETE localhost:9200/_search/scroll -d '
 }'
 ---------------------------------------
 
-coming[2.0.0-beta1, Body based parameters were added in 2.0.0]
-
 All search contexts can be cleared with the `_all` parameter:
 
 [source,js]
diff --git a/docs/reference/setup.asciidoc b/docs/reference/setup.asciidoc
index 9d14e62..15f23e6 100644
--- a/docs/reference/setup.asciidoc
+++ b/docs/reference/setup.asciidoc
@@ -10,6 +10,14 @@ then check the <<setup-installation,installation>> docs.
 NOTE: Elasticsearch can also be installed from our repositories using `apt` or `yum`.
 See <<setup-repositories>>.
 
+[[supported-platforms]]
+[float]
+== Supported platforms
+
+The matrix of officially supported operating systems and JVMs is available here:
+link:/support/matrix[Support Matrix].  Elasticsearch is tested on the listed
+platforms, but it is possible that it will work on other platforms too.
+
 [[setup-installation]]
 [float]
 == Installation
diff --git a/docs/reference/setup/as-a-service.asciidoc b/docs/reference/setup/as-a-service.asciidoc
index 01bbd2d..1bd6d9b 100644
--- a/docs/reference/setup/as-a-service.asciidoc
+++ b/docs/reference/setup/as-a-service.asciidoc
@@ -22,7 +22,6 @@ Each package features a configuration file, which allows you to set the followin
 `LOG_DIR`::               Log directory, defaults to `/var/log/elasticsearch`
 `DATA_DIR`::              Data directory, defaults to `/var/lib/elasticsearch`
 `CONF_DIR`::              Configuration file directory (which needs to include `elasticsearch.yml` and `logging.yml` files), defaults to `/etc/elasticsearch`
-`CONF_FILE`::             Path to configuration file, defaults to `/etc/elasticsearch/elasticsearch.yml`
 `ES_JAVA_OPTS`::          Any additional java options you may want to apply. This may be useful, if you need to set the `node.name` property, but do not want to change the `elasticsearch.yml` configuration file, because it is distributed via a provisioning system like puppet or chef. Example: `ES_JAVA_OPTS="-Des.node.name=search-01"`
 `RESTART_ON_UPGRADE`::    Configure restart on package upgrade, defaults to `false`. This means you will have to restart your elasticsearch instance after installing a package manually. The reason for this is to ensure, that upgrades in a cluster do not result in a continuous shard reallocation resulting in high network traffic and reducing the response times of your cluster.
 `ES_GC_LOG_FILE` ::       The absolute log file path for creating a garbage collection logfile, which is done by the JVM. Note that this logfile can grow pretty quick and thus is disabled by default.
@@ -40,6 +39,8 @@ sudo update-rc.d elasticsearch defaults 95 10
 sudo /etc/init.d/elasticsearch start
 --------------------------------------------------
 
+Users running Debian 8 or Ubuntu 14 or later may require configuration of systemd instead of `update-rc.d`. In those cases, please refer to the <<using-systemd>> section.
+
 [float]
 ===== Installing the oracle JDK
 
@@ -70,11 +71,11 @@ sudo /sbin/chkconfig --add elasticsearch
 sudo service elasticsearch start
 --------------------------------------------------
 
-
+[[using-systemd]]
 [float]
-===== Using systemd
+==== Using systemd
 
-Distributions like SUSE do not use the `chkconfig` tool to register services, but rather `systemd` and its command `/bin/systemctl` to start and stop services (at least in newer versions, otherwise use the `chkconfig` commands above). The configuration file is also placed at `/etc/sysconfig/elasticsearch`. After installing the RPM, you have to change the systemd configuration and then start up elasticsearch
+Distributions like Debian Jessie, Ubuntu 14, and many of the SUSE derivitives do not use the `chkconfig` tool to register services, but rather `systemd` and its command `/bin/systemctl` to start and stop services (at least in newer versions, otherwise use the `chkconfig` commands above). The configuration file is also placed at `/etc/sysconfig/elasticsearch` if the system is rpm based and `/etc/default/elasticsearch` if it is deb. After installing the RPM, you have to change the systemd configuration and then start up elasticsearch
 
 [source,sh]
 --------------------------------------------------
diff --git a/docs/reference/setup/configuration.asciidoc b/docs/reference/setup/configuration.asciidoc
index 45c384b..0768891 100644
--- a/docs/reference/setup/configuration.asciidoc
+++ b/docs/reference/setup/configuration.asciidoc
@@ -298,14 +298,6 @@ Enter value for [node.name]:
 NOTE: Elasticsearch will not start if `${prompt.text}` or `${prompt.secret}`
 is used in the settings and the process is run as a service or in the background.
 
-The location of the configuration file can be set externally using a
-system property:
-
-[source,sh]
---------------------------------------------------
-$ elasticsearch -Des.config=/path/to/config/file
---------------------------------------------------
-
 [float]
 [[configuration-index-settings]]
 === Index Settings
diff --git a/docs/reference/setup/repositories.asciidoc b/docs/reference/setup/repositories.asciidoc
index 79b8959..70b000e 100644
--- a/docs/reference/setup/repositories.asciidoc
+++ b/docs/reference/setup/repositories.asciidoc
@@ -6,7 +6,7 @@ binary packages, but no source packages, as the packages are created as part of
 build.
 
 We have split the major versions in separate urls to avoid accidental upgrades across major version.
-For all 0.90.x releases use 0.90 as version number, for 1.0.x use 1.0, for 1.1.x use 1.1 etc.
+For all 2.x releases use 2.x as version number, for 3.x.y use 3.x etc...
 
 We use the PGP key https://pgp.mit.edu/pks/lookup?op=vindex&search=0xD27D666CD88E42B4[D88E42B4],
 Elasticsearch Signing Key, with fingerprint
@@ -25,11 +25,11 @@ Download and install the Public Signing Key:
 wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
 --------------------------------------------------
 
-Save the repository definition to  `/etc/apt/sources.list.d/elasticsearch-{branch}.list`:
+Save the repository definition to  +/etc/apt/sources.list.d/elasticsearch-{major-version}.list+:
 
 ["source","sh",subs="attributes,callouts"]
 --------------------------------------------------
-echo "deb http://packages.elastic.co/elasticsearch/{branch}/debian stable main" | sudo tee -a /etc/apt/sources.list.d/elasticsearch-{branch}.list
+echo "deb http://packages.elastic.co/elasticsearch/{major-version}/debian stable main" | sudo tee -a /etc/apt/sources.list.d/elasticsearch-{major-version}.list
 --------------------------------------------------
 
 [WARNING]
@@ -57,9 +57,9 @@ If two entries exist for the same Elasticsearch repository, you will see an erro
 
 ["literal",subs="attributes,callouts"]
 
-Duplicate sources.list entry http://packages.elastic.co/elasticsearch/{branch}/debian/ ...`
+Duplicate sources.list entry http://packages.elastic.co/elasticsearch/{major-version}/debian/ ...`
 
-Examine +/etc/apt/sources.list.d/elasticsearch-{branch}.list+ for the duplicate entry or locate the duplicate entry amongst the files in `/etc/apt/sources.list.d/` and the `/etc/apt/sources.list` file.
+Examine +/etc/apt/sources.list.d/elasticsearch-{major-version}.list+ for the duplicate entry or locate the duplicate entry amongst the files in `/etc/apt/sources.list.d/` and the `/etc/apt/sources.list` file.
 ==================================================
 
 Configure Elasticsearch to automatically start during bootup. If your
@@ -93,9 +93,9 @@ in a file with a `.repo` suffix, for example `elasticsearch.repo`
 
 ["source","sh",subs="attributes,callouts"]
 --------------------------------------------------
-[elasticsearch-{branch}]
-name=Elasticsearch repository for {branch}.x packages
-baseurl=http://packages.elastic.co/elasticsearch/{branch}/centos
+[elasticsearch-{major-version}]
+name=Elasticsearch repository for {major-version} packages
+baseurl=http://packages.elastic.co/elasticsearch/{major-version}/centos
 gpgcheck=1
 gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
 enabled=1
diff --git a/plugins/analysis-kuromoji/src/main/java/org/elasticsearch/index/analysis/JapaneseStopTokenFilterFactory.java b/plugins/analysis-kuromoji/src/main/java/org/elasticsearch/index/analysis/JapaneseStopTokenFilterFactory.java
index 9515971..433d03d 100644
--- a/plugins/analysis-kuromoji/src/main/java/org/elasticsearch/index/analysis/JapaneseStopTokenFilterFactory.java
+++ b/plugins/analysis-kuromoji/src/main/java/org/elasticsearch/index/analysis/JapaneseStopTokenFilterFactory.java
@@ -25,6 +25,7 @@ import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.ja.JapaneseAnalyzer;
 import org.apache.lucene.analysis.util.CharArraySet;
 import org.apache.lucene.search.suggest.analyzing.SuggestStopFilter;
+import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.assistedinject.Assisted;
 import org.elasticsearch.common.settings.Settings;
@@ -35,10 +36,8 @@ import org.elasticsearch.index.settings.IndexSettings;
 import java.util.Map;
 import java.util.Set;
 
-import static java.util.Collections.singletonMap;
-
 public class JapaneseStopTokenFilterFactory extends AbstractTokenFilterFactory{
-    private static final Map<String, Set<?>> NAMED_STOP_WORDS = singletonMap("_japanese_", JapaneseAnalyzer.getDefaultStopSet());
+
 
     private final CharArraySet stopWords;
 
@@ -51,7 +50,10 @@ public class JapaneseStopTokenFilterFactory extends AbstractTokenFilterFactory{
         super(index, indexSettings, name, settings);
         this.ignoreCase = settings.getAsBoolean("ignore_case", false);
         this.removeTrailing = settings.getAsBoolean("remove_trailing", true);
-        this.stopWords = Analysis.parseWords(env, settings, "stopwords", JapaneseAnalyzer.getDefaultStopSet(), NAMED_STOP_WORDS, ignoreCase);
+        Map<String, Set<?>> namedStopWords = MapBuilder.<String, Set<?>>newMapBuilder()
+            .put("_japanese_", JapaneseAnalyzer.getDefaultStopSet())
+            .immutableMap();
+        this.stopWords = Analysis.parseWords(env, settings, "stopwords", JapaneseAnalyzer.getDefaultStopSet(), namedStopWords, ignoreCase);
     }
 
     @Override
diff --git a/plugins/cloud-gce/.local-3.0.0-SNAPSHOT-integTest-execution-times.log b/plugins/cloud-gce/.local-3.0.0-SNAPSHOT-integTest-execution-times.log
deleted file mode 100644
index 7636f46..0000000
--- a/plugins/cloud-gce/.local-3.0.0-SNAPSHOT-integTest-execution-times.log
+++ /dev/null
@@ -1 +0,0 @@
-org.elasticsearch.cloud.gce.CloudGCERestIT=1367
diff --git a/plugins/cloud-gce/.local-3.0.0-SNAPSHOT-test-execution-times.log b/plugins/cloud-gce/.local-3.0.0-SNAPSHOT-test-execution-times.log
deleted file mode 100644
index 40b2939..0000000
--- a/plugins/cloud-gce/.local-3.0.0-SNAPSHOT-test-execution-times.log
+++ /dev/null
@@ -1,2 +0,0 @@
-org.elasticsearch.discovery.gce.GceDiscoverySettingsTests=700
-org.elasticsearch.discovery.gce.GceDiscoveryTests=929
diff --git a/plugins/cloud-gce/licenses/commons-codec-1.6.jar.sha1 b/plugins/cloud-gce/licenses/commons-codec-1.6.jar.sha1
deleted file mode 100644
index bf78aff..0000000
--- a/plugins/cloud-gce/licenses/commons-codec-1.6.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-b7f0fc8f61ecadeb3695f0b9464755eee44374d4
diff --git a/plugins/cloud-gce/licenses/commons-codec-LICENSE.txt b/plugins/cloud-gce/licenses/commons-codec-LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/cloud-gce/licenses/commons-codec-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/cloud-gce/licenses/commons-codec-NOTICE.txt b/plugins/cloud-gce/licenses/commons-codec-NOTICE.txt
deleted file mode 100644
index 5691644..0000000
--- a/plugins/cloud-gce/licenses/commons-codec-NOTICE.txt
+++ /dev/null
@@ -1,17 +0,0 @@
-Apache Commons Codec
-Copyright 2002-2015 The Apache Software Foundation
-
-This product includes software developed at
-The Apache Software Foundation (http://www.apache.org/).
-
-src/test/org/apache/commons/codec/language/DoubleMetaphoneTest.java
-contains test data from http://aspell.net/test/orig/batch0.tab.
-Copyright (C) 2002 Kevin Atkinson (kevina@gnu.org)
-
-===============================================================================
-
-The content of package org.apache.commons.codec.language.bm has been translated
-from the original php source code available at http://stevemorse.org/phoneticinfo.htm
-with permission from the original authors.
-Original source copyright:
-Copyright (c) 2008 Alexander Beider & Stephen P. Morse.
diff --git a/plugins/cloud-gce/licenses/commons-logging-1.1.3.jar.sha1 b/plugins/cloud-gce/licenses/commons-logging-1.1.3.jar.sha1
deleted file mode 100644
index c8756c4..0000000
--- a/plugins/cloud-gce/licenses/commons-logging-1.1.3.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-f6f66e966c70a83ffbdb6f17a0919eaf7c8aca7f
diff --git a/plugins/cloud-gce/licenses/commons-logging-LICENSE.txt b/plugins/cloud-gce/licenses/commons-logging-LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/cloud-gce/licenses/commons-logging-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/cloud-gce/licenses/commons-logging-NOTICE.txt b/plugins/cloud-gce/licenses/commons-logging-NOTICE.txt
deleted file mode 100644
index d3d6e14..0000000
--- a/plugins/cloud-gce/licenses/commons-logging-NOTICE.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-Apache Commons Logging
-Copyright 2003-2014 The Apache Software Foundation
-
-This product includes software developed at
-The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/cloud-gce/licenses/google-LICENSE.txt b/plugins/cloud-gce/licenses/google-LICENSE.txt
deleted file mode 100644
index 980a15a..0000000
--- a/plugins/cloud-gce/licenses/google-LICENSE.txt
+++ /dev/null
@@ -1,201 +0,0 @@
-                                Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "{}"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright {yyyy} {name of copyright owner}
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/cloud-gce/licenses/google-NOTICE.txt b/plugins/cloud-gce/licenses/google-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/plugins/cloud-gce/licenses/google-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/plugins/cloud-gce/licenses/google-api-client-1.20.0.jar.sha1 b/plugins/cloud-gce/licenses/google-api-client-1.20.0.jar.sha1
deleted file mode 100644
index 08c24d1..0000000
--- a/plugins/cloud-gce/licenses/google-api-client-1.20.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-d3e66209ae9e749b2d6833761e7885f60f285564
diff --git a/plugins/cloud-gce/licenses/google-api-services-compute-v1-rev71-1.20.0.jar.sha1 b/plugins/cloud-gce/licenses/google-api-services-compute-v1-rev71-1.20.0.jar.sha1
deleted file mode 100644
index c6e6948..0000000
--- a/plugins/cloud-gce/licenses/google-api-services-compute-v1-rev71-1.20.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-2fa36fff3b5bf59a63c4f2bbfac1f88251cd7986
diff --git a/plugins/cloud-gce/licenses/google-http-client-1.20.0.jar.sha1 b/plugins/cloud-gce/licenses/google-http-client-1.20.0.jar.sha1
deleted file mode 100644
index 66a2247..0000000
--- a/plugins/cloud-gce/licenses/google-http-client-1.20.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-93d82db2bca534960253f43424b2ba9d7638b4d2
diff --git a/plugins/cloud-gce/licenses/google-http-client-jackson2-1.20.0.jar.sha1 b/plugins/cloud-gce/licenses/google-http-client-jackson2-1.20.0.jar.sha1
deleted file mode 100644
index 6d861e6..0000000
--- a/plugins/cloud-gce/licenses/google-http-client-jackson2-1.20.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-2408070b2abec043624d35b35e30450f1b663858
diff --git a/plugins/cloud-gce/licenses/google-oauth-client-1.20.0.jar.sha1 b/plugins/cloud-gce/licenses/google-oauth-client-1.20.0.jar.sha1
deleted file mode 100644
index c35c4bc..0000000
--- a/plugins/cloud-gce/licenses/google-oauth-client-1.20.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-1d086ac5756475ddf451af2e2df6e288d18608ca
diff --git a/plugins/cloud-gce/licenses/httpclient-4.3.6.jar.sha1 b/plugins/cloud-gce/licenses/httpclient-4.3.6.jar.sha1
deleted file mode 100644
index 3d35ee9..0000000
--- a/plugins/cloud-gce/licenses/httpclient-4.3.6.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-4c47155e3e6c9a41a28db36680b828ced53b8af4
diff --git a/plugins/cloud-gce/licenses/httpclient-LICENSE.txt b/plugins/cloud-gce/licenses/httpclient-LICENSE.txt
deleted file mode 100644
index 32f01ed..0000000
--- a/plugins/cloud-gce/licenses/httpclient-LICENSE.txt
+++ /dev/null
@@ -1,558 +0,0 @@
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-=========================================================================
-
-This project includes Public Suffix List copied from
-<https://publicsuffix.org/list/effective_tld_names.dat>
-licensed under the terms of the Mozilla Public License, v. 2.0
-
-Full license text: <http://mozilla.org/MPL/2.0/>
-
-Mozilla Public License Version 2.0
-==================================
-
-1. Definitions
---------------
-
-1.1. "Contributor"
-    means each individual or legal entity that creates, contributes to
-    the creation of, or owns Covered Software.
-
-1.2. "Contributor Version"
-    means the combination of the Contributions of others (if any) used
-    by a Contributor and that particular Contributor's Contribution.
-
-1.3. "Contribution"
-    means Covered Software of a particular Contributor.
-
-1.4. "Covered Software"
-    means Source Code Form to which the initial Contributor has attached
-    the notice in Exhibit A, the Executable Form of such Source Code
-    Form, and Modifications of such Source Code Form, in each case
-    including portions thereof.
-
-1.5. "Incompatible With Secondary Licenses"
-    means
-
-    (a) that the initial Contributor has attached the notice described
-        in Exhibit B to the Covered Software; or
-
-    (b) that the Covered Software was made available under the terms of
-        version 1.1 or earlier of the License, but not also under the
-        terms of a Secondary License.
-
-1.6. "Executable Form"
-    means any form of the work other than Source Code Form.
-
-1.7. "Larger Work"
-    means a work that combines Covered Software with other material, in
-    a separate file or files, that is not Covered Software.
-
-1.8. "License"
-    means this document.
-
-1.9. "Licensable"
-    means having the right to grant, to the maximum extent possible,
-    whether at the time of the initial grant or subsequently, any and
-    all of the rights conveyed by this License.
-
-1.10. "Modifications"
-    means any of the following:
-
-    (a) any file in Source Code Form that results from an addition to,
-        deletion from, or modification of the contents of Covered
-        Software; or
-
-    (b) any new file in Source Code Form that contains any Covered
-        Software.
-
-1.11. "Patent Claims" of a Contributor
-    means any patent claim(s), including without limitation, method,
-    process, and apparatus claims, in any patent Licensable by such
-    Contributor that would be infringed, but for the grant of the
-    License, by the making, using, selling, offering for sale, having
-    made, import, or transfer of either its Contributions or its
-    Contributor Version.
-
-1.12. "Secondary License"
-    means either the GNU General Public License, Version 2.0, the GNU
-    Lesser General Public License, Version 2.1, the GNU Affero General
-    Public License, Version 3.0, or any later versions of those
-    licenses.
-
-1.13. "Source Code Form"
-    means the form of the work preferred for making modifications.
-
-1.14. "You" (or "Your")
-    means an individual or a legal entity exercising rights under this
-    License. For legal entities, "You" includes any entity that
-    controls, is controlled by, or is under common control with You. For
-    purposes of this definition, "control" means (a) the power, direct
-    or indirect, to cause the direction or management of such entity,
-    whether by contract or otherwise, or (b) ownership of more than
-    fifty percent (50%) of the outstanding shares or beneficial
-    ownership of such entity.
-
-2. License Grants and Conditions
---------------------------------
-
-2.1. Grants
-
-Each Contributor hereby grants You a world-wide, royalty-free,
-non-exclusive license:
-
-(a) under intellectual property rights (other than patent or trademark)
-    Licensable by such Contributor to use, reproduce, make available,
-    modify, display, perform, distribute, and otherwise exploit its
-    Contributions, either on an unmodified basis, with Modifications, or
-    as part of a Larger Work; and
-
-(b) under Patent Claims of such Contributor to make, use, sell, offer
-    for sale, have made, import, and otherwise transfer either its
-    Contributions or its Contributor Version.
-
-2.2. Effective Date
-
-The licenses granted in Section 2.1 with respect to any Contribution
-become effective for each Contribution on the date the Contributor first
-distributes such Contribution.
-
-2.3. Limitations on Grant Scope
-
-The licenses granted in this Section 2 are the only rights granted under
-this License. No additional rights or licenses will be implied from the
-distribution or licensing of Covered Software under this License.
-Notwithstanding Section 2.1(b) above, no patent license is granted by a
-Contributor:
-
-(a) for any code that a Contributor has removed from Covered Software;
-    or
-
-(b) for infringements caused by: (i) Your and any other third party's
-    modifications of Covered Software, or (ii) the combination of its
-    Contributions with other software (except as part of its Contributor
-    Version); or
-
-(c) under Patent Claims infringed by Covered Software in the absence of
-    its Contributions.
-
-This License does not grant any rights in the trademarks, service marks,
-or logos of any Contributor (except as may be necessary to comply with
-the notice requirements in Section 3.4).
-
-2.4. Subsequent Licenses
-
-No Contributor makes additional grants as a result of Your choice to
-distribute the Covered Software under a subsequent version of this
-License (see Section 10.2) or under the terms of a Secondary License (if
-permitted under the terms of Section 3.3).
-
-2.5. Representation
-
-Each Contributor represents that the Contributor believes its
-Contributions are its original creation(s) or it has sufficient rights
-to grant the rights to its Contributions conveyed by this License.
-
-2.6. Fair Use
-
-This License is not intended to limit any rights You have under
-applicable copyright doctrines of fair use, fair dealing, or other
-equivalents.
-
-2.7. Conditions
-
-Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted
-in Section 2.1.
-
-3. Responsibilities
--------------------
-
-3.1. Distribution of Source Form
-
-All distribution of Covered Software in Source Code Form, including any
-Modifications that You create or to which You contribute, must be under
-the terms of this License. You must inform recipients that the Source
-Code Form of the Covered Software is governed by the terms of this
-License, and how they can obtain a copy of this License. You may not
-attempt to alter or restrict the recipients' rights in the Source Code
-Form.
-
-3.2. Distribution of Executable Form
-
-If You distribute Covered Software in Executable Form then:
-
-(a) such Covered Software must also be made available in Source Code
-    Form, as described in Section 3.1, and You must inform recipients of
-    the Executable Form how they can obtain a copy of such Source Code
-    Form by reasonable means in a timely manner, at a charge no more
-    than the cost of distribution to the recipient; and
-
-(b) You may distribute such Executable Form under the terms of this
-    License, or sublicense it under different terms, provided that the
-    license for the Executable Form does not attempt to limit or alter
-    the recipients' rights in the Source Code Form under this License.
-
-3.3. Distribution of a Larger Work
-
-You may create and distribute a Larger Work under terms of Your choice,
-provided that You also comply with the requirements of this License for
-the Covered Software. If the Larger Work is a combination of Covered
-Software with a work governed by one or more Secondary Licenses, and the
-Covered Software is not Incompatible With Secondary Licenses, this
-License permits You to additionally distribute such Covered Software
-under the terms of such Secondary License(s), so that the recipient of
-the Larger Work may, at their option, further distribute the Covered
-Software under the terms of either this License or such Secondary
-License(s).
-
-3.4. Notices
-
-You may not remove or alter the substance of any license notices
-(including copyright notices, patent notices, disclaimers of warranty,
-or limitations of liability) contained within the Source Code Form of
-the Covered Software, except that You may alter any license notices to
-the extent required to remedy known factual inaccuracies.
-
-3.5. Application of Additional Terms
-
-You may choose to offer, and to charge a fee for, warranty, support,
-indemnity or liability obligations to one or more recipients of Covered
-Software. However, You may do so only on Your own behalf, and not on
-behalf of any Contributor. You must make it absolutely clear that any
-such warranty, support, indemnity, or liability obligation is offered by
-You alone, and You hereby agree to indemnify every Contributor for any
-liability incurred by such Contributor as a result of warranty, support,
-indemnity or liability terms You offer. You may include additional
-disclaimers of warranty and limitations of liability specific to any
-jurisdiction.
-
-4. Inability to Comply Due to Statute or Regulation
----------------------------------------------------
-
-If it is impossible for You to comply with any of the terms of this
-License with respect to some or all of the Covered Software due to
-statute, judicial order, or regulation then You must: (a) comply with
-the terms of this License to the maximum extent possible; and (b)
-describe the limitations and the code they affect. Such description must
-be placed in a text file included with all distributions of the Covered
-Software under this License. Except to the extent prohibited by statute
-or regulation, such description must be sufficiently detailed for a
-recipient of ordinary skill to be able to understand it.
-
-5. Termination
---------------
-
-5.1. The rights granted under this License will terminate automatically
-if You fail to comply with any of its terms. However, if You become
-compliant, then the rights granted under this License from a particular
-Contributor are reinstated (a) provisionally, unless and until such
-Contributor explicitly and finally terminates Your grants, and (b) on an
-ongoing basis, if such Contributor fails to notify You of the
-non-compliance by some reasonable means prior to 60 days after You have
-come back into compliance. Moreover, Your grants from a particular
-Contributor are reinstated on an ongoing basis if such Contributor
-notifies You of the non-compliance by some reasonable means, this is the
-first time You have received notice of non-compliance with this License
-from such Contributor, and You become compliant prior to 30 days after
-Your receipt of the notice.
-
-5.2. If You initiate litigation against any entity by asserting a patent
-infringement claim (excluding declaratory judgment actions,
-counter-claims, and cross-claims) alleging that a Contributor Version
-directly or indirectly infringes any patent, then the rights granted to
-You by any and all Contributors for the Covered Software under Section
-2.1 of this License shall terminate.
-
-5.3. In the event of termination under Sections 5.1 or 5.2 above, all
-end user license agreements (excluding distributors and resellers) which
-have been validly granted by You or Your distributors under this License
-prior to termination shall survive termination.
-
-************************************************************************
-*                                                                      *
-*  6. Disclaimer of Warranty                                           *
-*  -------------------------                                           *
-*                                                                      *
-*  Covered Software is provided under this License on an "as is"       *
-*  basis, without warranty of any kind, either expressed, implied, or  *
-*  statutory, including, without limitation, warranties that the       *
-*  Covered Software is free of defects, merchantable, fit for a        *
-*  particular purpose or non-infringing. The entire risk as to the     *
-*  quality and performance of the Covered Software is with You.        *
-*  Should any Covered Software prove defective in any respect, You     *
-*  (not any Contributor) assume the cost of any necessary servicing,   *
-*  repair, or correction. This disclaimer of warranty constitutes an   *
-*  essential part of this License. No use of any Covered Software is   *
-*  authorized under this License except under this disclaimer.         *
-*                                                                      *
-************************************************************************
-
-************************************************************************
-*                                                                      *
-*  7. Limitation of Liability                                          *
-*  --------------------------                                          *
-*                                                                      *
-*  Under no circumstances and under no legal theory, whether tort      *
-*  (including negligence), contract, or otherwise, shall any           *
-*  Contributor, or anyone who distributes Covered Software as          *
-*  permitted above, be liable to You for any direct, indirect,         *
-*  special, incidental, or consequential damages of any character      *
-*  including, without limitation, damages for lost profits, loss of    *
-*  goodwill, work stoppage, computer failure or malfunction, or any    *
-*  and all other commercial damages or losses, even if such party      *
-*  shall have been informed of the possibility of such damages. This   *
-*  limitation of liability shall not apply to liability for death or   *
-*  personal injury resulting from such party's negligence to the       *
-*  extent applicable law prohibits such limitation. Some               *
-*  jurisdictions do not allow the exclusion or limitation of           *
-*  incidental or consequential damages, so this exclusion and          *
-*  limitation may not apply to You.                                    *
-*                                                                      *
-************************************************************************
-
-8. Litigation
--------------
-
-Any litigation relating to this License may be brought only in the
-courts of a jurisdiction where the defendant maintains its principal
-place of business and such litigation shall be governed by laws of that
-jurisdiction, without reference to its conflict-of-law provisions.
-Nothing in this Section shall prevent a party's ability to bring
-cross-claims or counter-claims.
-
-9. Miscellaneous
-----------------
-
-This License represents the complete agreement concerning the subject
-matter hereof. If any provision of this License is held to be
-unenforceable, such provision shall be reformed only to the extent
-necessary to make it enforceable. Any law or regulation which provides
-that the language of a contract shall be construed against the drafter
-shall not be used to construe this License against a Contributor.
-
-10. Versions of the License
----------------------------
-
-10.1. New Versions
-
-Mozilla Foundation is the license steward. Except as provided in Section
-10.3, no one other than the license steward has the right to modify or
-publish new versions of this License. Each version will be given a
-distinguishing version number.
-
-10.2. Effect of New Versions
-
-You may distribute the Covered Software under the terms of the version
-of the License under which You originally received the Covered Software,
-or under the terms of any subsequent version published by the license
-steward.
-
-10.3. Modified Versions
-
-If you create software not governed by this License, and you want to
-create a new license for such software, you may create and use a
-modified version of this License if you rename the license and remove
-any references to the name of the license steward (except to note that
-such modified license differs from this License).
-
-10.4. Distributing Source Code Form that is Incompatible With Secondary
-Licenses
-
-If You choose to distribute Source Code Form that is Incompatible With
-Secondary Licenses under the terms of this version of the License, the
-notice described in Exhibit B of this License must be attached.
-
-Exhibit A - Source Code Form License Notice
--------------------------------------------
-
-  This Source Code Form is subject to the terms of the Mozilla Public
-  License, v. 2.0. If a copy of the MPL was not distributed with this
-  file, You can obtain one at http://mozilla.org/MPL/2.0/.
-
-If it is not possible or desirable to put the notice in a particular
-file, then You may include the notice in a location (such as a LICENSE
-file in a relevant directory) where a recipient would be likely to look
-for such a notice.
-
-You may add additional accurate notices of copyright ownership.
-
-Exhibit B - "Incompatible With Secondary Licenses" Notice
----------------------------------------------------------
-
-  This Source Code Form is "Incompatible With Secondary Licenses", as
-  defined by the Mozilla Public License, v. 2.0.
diff --git a/plugins/cloud-gce/licenses/httpclient-NOTICE.txt b/plugins/cloud-gce/licenses/httpclient-NOTICE.txt
deleted file mode 100644
index 4f60581..0000000
--- a/plugins/cloud-gce/licenses/httpclient-NOTICE.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-Apache HttpComponents Client
-Copyright 1999-2015 The Apache Software Foundation
-
-This product includes software developed at
-The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/cloud-gce/licenses/httpcore-4.3.3.jar.sha1 b/plugins/cloud-gce/licenses/httpcore-4.3.3.jar.sha1
deleted file mode 100644
index 5d9c0e2..0000000
--- a/plugins/cloud-gce/licenses/httpcore-4.3.3.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-f91b7a4aadc5cf486df6e4634748d7dd7a73f06d
diff --git a/plugins/cloud-gce/licenses/httpcore-LICENSE.txt b/plugins/cloud-gce/licenses/httpcore-LICENSE.txt
deleted file mode 100644
index 72819a9..0000000
--- a/plugins/cloud-gce/licenses/httpcore-LICENSE.txt
+++ /dev/null
@@ -1,241 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-=========================================================================
-
-This project contains annotations in the package org.apache.http.annotation
-which are derived from JCIP-ANNOTATIONS
-Copyright (c) 2005 Brian Goetz and Tim Peierls.
-See http://www.jcip.net and the Creative Commons Attribution License
-(http://creativecommons.org/licenses/by/2.5)
-Full text: http://creativecommons.org/licenses/by/2.5/legalcode
-
-License
-
-THE WORK (AS DEFINED BELOW) IS PROVIDED UNDER THE TERMS OF THIS CREATIVE COMMONS PUBLIC LICENSE ("CCPL" OR "LICENSE"). THE WORK IS PROTECTED BY COPYRIGHT AND/OR OTHER APPLICABLE LAW. ANY USE OF THE WORK OTHER THAN AS AUTHORIZED UNDER THIS LICENSE OR COPYRIGHT LAW IS PROHIBITED.
-
-BY EXERCISING ANY RIGHTS TO THE WORK PROVIDED HERE, YOU ACCEPT AND AGREE TO BE BOUND BY THE TERMS OF THIS LICENSE. THE LICENSOR GRANTS YOU THE RIGHTS CONTAINED HERE IN CONSIDERATION OF YOUR ACCEPTANCE OF SUCH TERMS AND CONDITIONS.
-
-1. Definitions
-
-    "Collective Work" means a work, such as a periodical issue, anthology or encyclopedia, in which the Work in its entirety in unmodified form, along with a number of other contributions, constituting separate and independent works in themselves, are assembled into a collective whole. A work that constitutes a Collective Work will not be considered a Derivative Work (as defined below) for the purposes of this License.
-    "Derivative Work" means a work based upon the Work or upon the Work and other pre-existing works, such as a translation, musical arrangement, dramatization, fictionalization, motion picture version, sound recording, art reproduction, abridgment, condensation, or any other form in which the Work may be recast, transformed, or adapted, except that a work that constitutes a Collective Work will not be considered a Derivative Work for the purpose of this License. For the avoidance of doubt, where the Work is a musical composition or sound recording, the synchronization of the Work in timed-relation with a moving image ("synching") will be considered a Derivative Work for the purpose of this License.
-    "Licensor" means the individual or entity that offers the Work under the terms of this License.
-    "Original Author" means the individual or entity who created the Work.
-    "Work" means the copyrightable work of authorship offered under the terms of this License.
-    "You" means an individual or entity exercising rights under this License who has not previously violated the terms of this License with respect to the Work, or who has received express permission from the Licensor to exercise rights under this License despite a previous violation.
-
-2. Fair Use Rights. Nothing in this license is intended to reduce, limit, or restrict any rights arising from fair use, first sale or other limitations on the exclusive rights of the copyright owner under copyright law or other applicable laws.
-
-3. License Grant. Subject to the terms and conditions of this License, Licensor hereby grants You a worldwide, royalty-free, non-exclusive, perpetual (for the duration of the applicable copyright) license to exercise the rights in the Work as stated below:
-
-    to reproduce the Work, to incorporate the Work into one or more Collective Works, and to reproduce the Work as incorporated in the Collective Works;
-    to create and reproduce Derivative Works;
-    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission the Work including as incorporated in Collective Works;
-    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission Derivative Works.
-
-    For the avoidance of doubt, where the work is a musical composition:
-        Performance Royalties Under Blanket Licenses. Licensor waives the exclusive right to collect, whether individually or via a performance rights society (e.g. ASCAP, BMI, SESAC), royalties for the public performance or public digital performance (e.g. webcast) of the Work.
-        Mechanical Rights and Statutory Royalties. Licensor waives the exclusive right to collect, whether individually or via a music rights agency or designated agent (e.g. Harry Fox Agency), royalties for any phonorecord You create from the Work ("cover version") and distribute, subject to the compulsory license created by 17 USC Section 115 of the US Copyright Act (or the equivalent in other jurisdictions).
-    Webcasting Rights and Statutory Royalties. For the avoidance of doubt, where the Work is a sound recording, Licensor waives the exclusive right to collect, whether individually or via a performance-rights society (e.g. SoundExchange), royalties for the public digital performance (e.g. webcast) of the Work, subject to the compulsory license created by 17 USC Section 114 of the US Copyright Act (or the equivalent in other jurisdictions).
-
-The above rights may be exercised in all media and formats whether now known or hereafter devised. The above rights include the right to make such modifications as are technically necessary to exercise the rights in other media and formats. All rights not expressly granted by Licensor are hereby reserved.
-
-4. Restrictions.The license granted in Section 3 above is expressly made subject to and limited by the following restrictions:
-
-    You may distribute, publicly display, publicly perform, or publicly digitally perform the Work only under the terms of this License, and You must include a copy of, or the Uniform Resource Identifier for, this License with every copy or phonorecord of the Work You distribute, publicly display, publicly perform, or publicly digitally perform. You may not offer or impose any terms on the Work that alter or restrict the terms of this License or the recipients' exercise of the rights granted hereunder. You may not sublicense the Work. You must keep intact all notices that refer to this License and to the disclaimer of warranties. You may not distribute, publicly display, publicly perform, or publicly digitally perform the Work with any technological measures that control access or use of the Work in a manner inconsistent with the terms of this License Agreement. The above applies to the Work as incorporated in a Collective Work, but this does not require the Collective Work apart from the Work itself to be made subject to the terms of this License. If You create a Collective Work, upon notice from any Licensor You must, to the extent practicable, remove from the Collective Work any credit as required by clause 4(b), as requested. If You create a Derivative Work, upon notice from any Licensor You must, to the extent practicable, remove from the Derivative Work any credit as required by clause 4(b), as requested.
-    If you distribute, publicly display, publicly perform, or publicly digitally perform the Work or any Derivative Works or Collective Works, You must keep intact all copyright notices for the Work and provide, reasonable to the medium or means You are utilizing: (i) the name of the Original Author (or pseudonym, if applicable) if supplied, and/or (ii) if the Original Author and/or Licensor designate another party or parties (e.g. a sponsor institute, publishing entity, journal) for attribution in Licensor's copyright notice, terms of service or by other reasonable means, the name of such party or parties; the title of the Work if supplied; to the extent reasonably practicable, the Uniform Resource Identifier, if any, that Licensor specifies to be associated with the Work, unless such URI does not refer to the copyright notice or licensing information for the Work; and in the case of a Derivative Work, a credit identifying the use of the Work in the Derivative Work (e.g., "French translation of the Work by Original Author," or "Screenplay based on original Work by Original Author"). Such credit may be implemented in any reasonable manner; provided, however, that in the case of a Derivative Work or Collective Work, at a minimum such credit will appear where any other comparable authorship credit appears and in a manner at least as prominent as such other comparable authorship credit.
-
-5. Representations, Warranties and Disclaimer
-
-UNLESS OTHERWISE MUTUALLY AGREED TO BY THE PARTIES IN WRITING, LICENSOR OFFERS THE WORK AS-IS AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE WORK, EXPRESS, IMPLIED, STATUTORY OR OTHERWISE, INCLUDING, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTIBILITY, FITNESS FOR A PARTICULAR PURPOSE, NONINFRINGEMENT, OR THE ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OF ABSENCE OF ERRORS, WHETHER OR NOT DISCOVERABLE. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES, SO SUCH EXCLUSION MAY NOT APPLY TO YOU.
-
-6. Limitation on Liability. EXCEPT TO THE EXTENT REQUIRED BY APPLICABLE LAW, IN NO EVENT WILL LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY FOR ANY SPECIAL, INCIDENTAL, CONSEQUENTIAL, PUNITIVE OR EXEMPLARY DAMAGES ARISING OUT OF THIS LICENSE OR THE USE OF THE WORK, EVEN IF LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
-
-7. Termination
-
-    This License and the rights granted hereunder will terminate automatically upon any breach by You of the terms of this License. Individuals or entities who have received Derivative Works or Collective Works from You under this License, however, will not have their licenses terminated provided such individuals or entities remain in full compliance with those licenses. Sections 1, 2, 5, 6, 7, and 8 will survive any termination of this License.
-    Subject to the above terms and conditions, the license granted here is perpetual (for the duration of the applicable copyright in the Work). Notwithstanding the above, Licensor reserves the right to release the Work under different license terms or to stop distributing the Work at any time; provided, however that any such election will not serve to withdraw this License (or any other license that has been, or is required to be, granted under the terms of this License), and this License will continue in full force and effect unless terminated as stated above.
-
-8. Miscellaneous
-
-    Each time You distribute or publicly digitally perform the Work or a Collective Work, the Licensor offers to the recipient a license to the Work on the same terms and conditions as the license granted to You under this License.
-    Each time You distribute or publicly digitally perform a Derivative Work, Licensor offers to the recipient a license to the original Work on the same terms and conditions as the license granted to You under this License.
-    If any provision of this License is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this License, and without further action by the parties to this agreement, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable.
-    No term or provision of this License shall be deemed waived and no breach consented to unless such waiver or consent shall be in writing and signed by the party to be charged with such waiver or consent.
-    This License constitutes the entire agreement between the parties with respect to the Work licensed here. There are no understandings, agreements or representations with respect to the Work not specified here. Licensor shall not be bound by any additional provisions that may appear in any communication from You. This License may not be modified without the mutual written agreement of the Licensor and You.
diff --git a/plugins/cloud-gce/licenses/httpcore-NOTICE.txt b/plugins/cloud-gce/licenses/httpcore-NOTICE.txt
deleted file mode 100644
index c0be50a..0000000
--- a/plugins/cloud-gce/licenses/httpcore-NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Apache HttpComponents Core
-Copyright 2005-2014 The Apache Software Foundation
-
-This product includes software developed at
-The Apache Software Foundation (http://www.apache.org/).
-
-This project contains annotations derived from JCIP-ANNOTATIONS
-Copyright (c) 2005 Brian Goetz and Tim Peierls. See http://www.jcip.net
diff --git a/plugins/cloud-gce/licenses/jsr305-1.3.9.jar.sha1 b/plugins/cloud-gce/licenses/jsr305-1.3.9.jar.sha1
deleted file mode 100644
index c04a429..0000000
--- a/plugins/cloud-gce/licenses/jsr305-1.3.9.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-40719ea6961c0cb6afaeb6a921eaa1f6afd4cfdf
diff --git a/plugins/cloud-gce/licenses/jsr305-LICENSE.txt b/plugins/cloud-gce/licenses/jsr305-LICENSE.txt
deleted file mode 100644
index 0cb8710..0000000
--- a/plugins/cloud-gce/licenses/jsr305-LICENSE.txt
+++ /dev/null
@@ -1,29 +0,0 @@
-Copyright (c) 2007-2009, JSR305 expert group
-All rights reserved.
-
-http://www.opensource.org/licenses/bsd-license.php
-
-Redistribution and use in source and binary forms, with or without 
-modification, are permitted provided that the following conditions are met:
-
-    * Redistributions of source code must retain the above copyright notice, 
-      this list of conditions and the following disclaimer.
-    * Redistributions in binary form must reproduce the above copyright notice, 
-      this list of conditions and the following disclaimer in the documentation 
-      and/or other materials provided with the distribution.
-    * Neither the name of the JSR305 expert group nor the names of its 
-      contributors may be used to endorse or promote products derived from 
-      this software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" 
-AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, 
-THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE 
-ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE 
-LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR 
-CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF 
-SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS 
-INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN 
-CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) 
-ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE 
-POSSIBILITY OF SUCH DAMAGE.
-
diff --git a/plugins/cloud-gce/licenses/jsr305-NOTICE.txt b/plugins/cloud-gce/licenses/jsr305-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/plugins/cloud-gce/licenses/jsr305-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/plugins/cloud-gce/pom.xml b/plugins/cloud-gce/pom.xml
deleted file mode 100644
index da987f8..0000000
--- a/plugins/cloud-gce/pom.xml
+++ /dev/null
@@ -1,68 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<!-- Licensed to Elasticsearch under one or more contributor
-license agreements. See the NOTICE file distributed with this work for additional
-information regarding copyright ownership. Elasticsearch licenses this file to you
-under the Apache License, Version 2.0 (the "License"); you may not use this
-file except in compliance with the License. You may obtain a copy of the
-License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by
-applicable law or agreed to in writing, software distributed under the License
-is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-KIND, either express or implied. See the License for the specific language
-governing permissions and limitations under the License. -->
-
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
-    <modelVersion>4.0.0</modelVersion>
-
-    <parent>
-        <groupId>org.elasticsearch.plugin</groupId>
-        <artifactId>plugins</artifactId>
-        <version>3.0.0-SNAPSHOT</version>
-    </parent>
-
-    <artifactId>cloud-gce</artifactId>
-    <name>Plugin: Cloud: Google Compute Engine</name>
-    <description>The Google Compute Engine (GCE) Cloud plugin allows to use GCE API for the unicast discovery mechanism.</description>
-
-    <properties>
-        <elasticsearch.plugin.classname>org.elasticsearch.plugin.cloud.gce.CloudGcePlugin</elasticsearch.plugin.classname>
-        <google.gce.version>v1-rev71-1.20.0</google.gce.version>
-        <!-- currently has no unit tests -->
-        <tests.rest.suite>cloud_gce</tests.rest.suite>
-        <tests.rest.load_packaged>false</tests.rest.load_packaged>
-        <xlint.options>-Xlint:-rawtypes,-unchecked</xlint.options>
-    </properties>
-
-    <dependencies>
-        <!-- Google APIs -->
-        <dependency>
-          <groupId>com.google.apis</groupId>
-          <artifactId>google-api-services-compute</artifactId>
-          <version>${google.gce.version}</version>
-          <exclusions>
-              <exclusion>
-                 <groupId>com.google.guava</groupId>
-                 <artifactId>guava-jdk5</artifactId>
-              </exclusion>
-          </exclusions>
-        </dependency>
-        <!-- We need to force here the compile scope as it was defined as test scope in plugins/pom.xml -->
-        <!-- TODO: remove this dependency when we will have a REST Test module -->
-        <dependency>
-            <groupId>org.apache.httpcomponents</groupId>
-            <artifactId>httpclient</artifactId>
-            <scope>compile</scope>
-        </dependency>
-    </dependencies>
-
-    <build>
-        <plugins>
-            <plugin>
-                <groupId>org.apache.maven.plugins</groupId>
-                <artifactId>maven-assembly-plugin</artifactId>
-            </plugin>
-        </plugins>
-    </build>
-
-</project>
diff --git a/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeService.java b/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeService.java
deleted file mode 100644
index 6ba857d..0000000
--- a/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeService.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.gce;
-
-import com.google.api.services.compute.model.Instance;
-import org.elasticsearch.common.component.LifecycleComponent;
-
-import java.util.Collection;
-
-/**
- *
- */
-public interface GceComputeService extends LifecycleComponent<GceComputeService> {
-    static final public class Fields {
-        public static final String PROJECT = "cloud.gce.project_id";
-        public static final String ZONE = "cloud.gce.zone";
-        public static final String REFRESH = "cloud.gce.refresh_interval";
-        public static final String TAGS = "discovery.gce.tags";
-        public static final String VERSION = "Elasticsearch/GceCloud/1.0";
-    }
-
-    public Collection<Instance> instances();
-}
diff --git a/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java b/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java
deleted file mode 100644
index 2a9bf6d..0000000
--- a/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java
+++ /dev/null
@@ -1,192 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.gce;
-
-import com.google.api.client.googleapis.compute.ComputeCredential;
-import com.google.api.client.googleapis.javanet.GoogleNetHttpTransport;
-import com.google.api.client.http.HttpTransport;
-import com.google.api.client.json.JsonFactory;
-import com.google.api.client.json.jackson2.JacksonFactory;
-import com.google.api.services.compute.Compute;
-import com.google.api.services.compute.model.Instance;
-import com.google.api.services.compute.model.InstanceList;
-
-import org.elasticsearch.SpecialPermission;
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.component.AbstractLifecycleComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-
-import java.io.IOException;
-import java.security.AccessController;
-import java.security.GeneralSecurityException;
-import java.security.PrivilegedActionException;
-import java.security.PrivilegedExceptionAction;
-import java.util.*;
-
-/**
- *
- */
-public class GceComputeServiceImpl extends AbstractLifecycleComponent<GceComputeService>
-    implements GceComputeService {
-
-    private final String project;
-    private final List<String> zones;
-
-    // Forcing Google Token API URL as set in GCE SDK to
-    //      http://metadata/computeMetadata/v1/instance/service-accounts/default/token
-    // See https://developers.google.com/compute/docs/metadata#metadataserver
-    public static final String TOKEN_SERVER_ENCODED_URL = "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token";
-
-    @Override
-    public Collection<Instance> instances() {
-        logger.debug("get instances for project [{}], zones [{}]", project, zones);
-        final List<Instance> instances = zones.stream().map((zoneId) -> {
-            try {
-                // hack around code messiness in GCE code
-                // TODO: get this fixed
-                SecurityManager sm = System.getSecurityManager();
-                if (sm != null) {
-                    sm.checkPermission(new SpecialPermission());
-                }
-                InstanceList instanceList = AccessController.doPrivileged(new PrivilegedExceptionAction<InstanceList>() {
-                    @Override
-                    public InstanceList run() throws Exception {
-                        Compute.Instances.List list = client().instances().list(project, zoneId);
-                        return list.execute();
-                    }
-                });
-                if (instanceList.isEmpty()) {
-                    return Collections.EMPTY_LIST;
-                }
-                return instanceList.getItems();
-            } catch (PrivilegedActionException e) {
-                logger.warn("Problem fetching instance list for zone {}", zoneId);
-                logger.debug("Full exception:", e);
-                return Collections.EMPTY_LIST;
-            }
-        }).reduce(new ArrayList<>(), (a, b) -> {
-            a.addAll(b);
-            return a;
-        });
-
-        if (instances.isEmpty()) {
-            logger.warn("disabling GCE discovery. Can not get list of nodes");
-        }
-
-        return instances;
-    }
-
-    private Compute client;
-    private TimeValue refreshInterval = null;
-    private long lastRefresh;
-
-    /** Global instance of the HTTP transport. */
-    private HttpTransport gceHttpTransport;
-
-    /** Global instance of the JSON factory. */
-    private JsonFactory gceJsonFactory;
-
-    @Inject
-    public GceComputeServiceImpl(Settings settings) {
-        super(settings);
-        this.project = settings.get(Fields.PROJECT);
-        String[] zoneList = settings.getAsArray(Fields.ZONE);
-        this.zones = Arrays.asList(zoneList);
-    }
-
-    protected synchronized HttpTransport getGceHttpTransport() throws GeneralSecurityException, IOException {
-        if (gceHttpTransport == null) {
-            gceHttpTransport = GoogleNetHttpTransport.newTrustedTransport();
-        }
-        return gceHttpTransport;
-    }
-
-    public synchronized Compute client() {
-        if (refreshInterval != null && refreshInterval.millis() != 0) {
-            if (client != null &&
-                    (refreshInterval.millis() < 0 || (System.currentTimeMillis() - lastRefresh) < refreshInterval.millis())) {
-                if (logger.isTraceEnabled()) logger.trace("using cache to retrieve client");
-                return client;
-            }
-            lastRefresh = System.currentTimeMillis();
-        }
-
-        try {
-            gceJsonFactory = new JacksonFactory();
-
-            logger.info("starting GCE discovery service");
-            ComputeCredential credential = new ComputeCredential.Builder(getGceHttpTransport(), gceJsonFactory)
-                        .setTokenServerEncodedUrl(TOKEN_SERVER_ENCODED_URL)
-                    .build();
-
-            // hack around code messiness in GCE code
-            // TODO: get this fixed
-            SecurityManager sm = System.getSecurityManager();
-            if (sm != null) {
-                sm.checkPermission(new SpecialPermission());
-            }
-            AccessController.doPrivileged(new PrivilegedExceptionAction<Void>() {
-                @Override
-                public Void run() throws IOException {
-                    credential.refreshToken();
-                    return null;
-                }
-            });
-
-            logger.debug("token [{}] will expire in [{}] s", credential.getAccessToken(), credential.getExpiresInSeconds());
-            if (credential.getExpiresInSeconds() != null) {
-                refreshInterval = TimeValue.timeValueSeconds(credential.getExpiresInSeconds()-1);
-            }
-
-            // Once done, let's use this token
-            this.client = new Compute.Builder(getGceHttpTransport(), gceJsonFactory, null)
-                    .setApplicationName(Fields.VERSION)
-                    .setHttpRequestInitializer(credential)
-                    .build();
-        } catch (Exception e) {
-            logger.warn("unable to start GCE discovery service", e);
-            throw new IllegalArgumentException("unable to start GCE discovery service", e);
-        }
-
-        return this.client;
-    }
-
-    @Override
-    protected void doStart() throws ElasticsearchException {
-    }
-
-    @Override
-    protected void doStop() throws ElasticsearchException {
-        if (gceHttpTransport != null) {
-            try {
-                gceHttpTransport.shutdown();
-            } catch (IOException e) {
-                logger.warn("unable to shutdown GCE Http Transport", e);
-            }
-            gceHttpTransport = null;
-        }
-    }
-
-    @Override
-    protected void doClose() throws ElasticsearchException {
-    }
-}
diff --git a/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceModule.java b/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceModule.java
deleted file mode 100644
index 8db0dec..0000000
--- a/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceModule.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.gce;
-
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-
-public class GceModule extends AbstractModule {
-    // pkg private so tests can override with mock
-    static Class<? extends GceComputeService> computeServiceImpl = GceComputeServiceImpl.class;
-
-    public static Class<? extends GceComputeService> getComputeServiceImpl() {
-        return computeServiceImpl;
-    }
-
-    @Override
-    protected void configure() {
-        bind(GceComputeService.class).to(computeServiceImpl).asEagerSingleton();
-    }
-}
diff --git a/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java b/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
deleted file mode 100755
index f20d1c7..0000000
--- a/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.gce;
-
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.discovery.DiscoverySettings;
-import org.elasticsearch.discovery.zen.ZenDiscovery;
-import org.elasticsearch.discovery.zen.elect.ElectMasterService;
-import org.elasticsearch.discovery.zen.ping.ZenPingService;
-import org.elasticsearch.node.settings.NodeSettingsService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-/**
- *
- */
-public class GceDiscovery extends ZenDiscovery {
-
-    public static final String GCE = "gce";
-
-    @Inject
-    public GceDiscovery(Settings settings, ClusterName clusterName, ThreadPool threadPool, TransportService transportService,
-                        ClusterService clusterService, NodeSettingsService nodeSettingsService, ZenPingService pingService,
-                        DiscoverySettings discoverySettings,
-                        ElectMasterService electMasterService) {
-        super(settings, clusterName, threadPool, transportService, clusterService, nodeSettingsService,
-                pingService, electMasterService, discoverySettings);
-    }
-}
diff --git a/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java b/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java
deleted file mode 100644
index 8feb9b8..0000000
--- a/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java
+++ /dev/null
@@ -1,249 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.gce;
-
-import com.google.api.services.compute.model.AccessConfig;
-import com.google.api.services.compute.model.Instance;
-import com.google.api.services.compute.model.NetworkInterface;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cloud.gce.GceComputeService;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.network.NetworkAddress;
-import org.elasticsearch.common.network.NetworkService;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.transport.TransportAddress;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.discovery.zen.ping.unicast.UnicastHostsProvider;
-import org.elasticsearch.transport.TransportService;
-
-import java.io.IOException;
-import java.net.InetAddress;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.List;
-
-import static org.elasticsearch.cloud.gce.GceComputeService.Fields;
-
-/**
- *
- */
-public class GceUnicastHostsProvider extends AbstractComponent implements UnicastHostsProvider {
-
-    static final class Status {
-        private static final String TERMINATED = "TERMINATED";
-    }
-
-    private final GceComputeService gceComputeService;
-    private TransportService transportService;
-    private NetworkService networkService;
-
-    private final Version version;
-    private final String project;
-    private final String[] zones;
-    private final String[] tags;
-
-    private final TimeValue refreshInterval;
-    private long lastRefresh;
-    private List<DiscoveryNode> cachedDiscoNodes;
-
-    @Inject
-    public GceUnicastHostsProvider(Settings settings, GceComputeService gceComputeService,
-            TransportService transportService,
-            NetworkService networkService,
-            Version version) {
-        super(settings);
-        this.gceComputeService = gceComputeService;
-        this.transportService = transportService;
-        this.networkService = networkService;
-        this.version = version;
-
-        this.refreshInterval = settings.getAsTime(Fields.REFRESH, TimeValue.timeValueSeconds(0));
-        this.project = settings.get(Fields.PROJECT);
-        this.zones = settings.getAsArray(Fields.ZONE);
-
-        this.tags = settings.getAsArray(Fields.TAGS);
-        if (logger.isDebugEnabled()) {
-            logger.debug("using tags {}", Arrays.asList(this.tags));
-        }
-    }
-
-    /**
-     * We build the list of Nodes from GCE Management API
-     * Information can be cached using `plugins.refresh_interval` property if needed.
-     * Setting `plugins.refresh_interval` to `-1` will cause infinite caching.
-     * Setting `plugins.refresh_interval` to `0` will disable caching (default).
-     */
-    @Override
-    public List<DiscoveryNode> buildDynamicNodes() {
-        if (refreshInterval.millis() != 0) {
-            if (cachedDiscoNodes != null &&
-                    (refreshInterval.millis() < 0 || (System.currentTimeMillis() - lastRefresh) < refreshInterval.millis())) {
-                if (logger.isTraceEnabled()) logger.trace("using cache to retrieve node list");
-                return cachedDiscoNodes;
-            }
-            lastRefresh = System.currentTimeMillis();
-        }
-        logger.debug("start building nodes list using GCE API");
-
-        cachedDiscoNodes = new ArrayList<>();
-        String ipAddress = null;
-        try {
-            InetAddress inetAddress = networkService.resolvePublishHostAddress(null);
-            if (inetAddress != null) {
-                ipAddress = NetworkAddress.formatAddress(inetAddress);
-            }
-        } catch (IOException e) {
-            // We can't find the publish host address... Hmmm. Too bad :-(
-            // We won't simply filter it
-        }
-
-        try {
-            Collection<Instance> instances = gceComputeService.instances();
-
-            if (instances == null) {
-                logger.trace("no instance found for project [{}], zones [{}].", this.project, this.zones);
-                return cachedDiscoNodes;
-            }
-
-            for (Instance instance : instances) {
-                String name = instance.getName();
-                String type = instance.getMachineType();
-
-                String status = instance.getStatus();
-                logger.trace("gce instance {} with status {} found.", name, status);
-
-                // We don't want to connect to TERMINATED status instances
-                // See https://github.com/elasticsearch/elasticsearch-cloud-gce/issues/3
-                if (Status.TERMINATED.equals(status)) {
-                    logger.debug("node {} is TERMINATED. Ignoring", name);
-                    continue;
-                }
-
-                // see if we need to filter by tag
-                boolean filterByTag = false;
-                if (tags.length > 0) {
-                    logger.trace("start filtering instance {} with tags {}.", name, tags);
-                    if (instance.getTags() == null || instance.getTags().isEmpty()
-                            || instance.getTags().getItems() == null || instance.getTags().getItems().isEmpty()) {
-                        // If this instance have no tag, we filter it
-                        logger.trace("no tags for this instance but we asked for tags. {} won't be part of the cluster.", name);
-                        filterByTag = true;
-                    } else {
-                        // check that all tags listed are there on the instance
-                        logger.trace("comparing instance tags {} with tags filter {}.", instance.getTags().getItems(), tags);
-                        for (String tag : tags) {
-                            boolean found = false;
-                            for (String instancetag : instance.getTags().getItems()) {
-                                if (instancetag.equals(tag)) {
-                                    found = true;
-                                    break;
-                                }
-                            }
-                            if (!found) {
-                                filterByTag = true;
-                                break;
-                            }
-                        }
-                    }
-                }
-                if (filterByTag) {
-                    logger.trace("filtering out instance {} based tags {}, not part of {}", name, tags,
-                            instance.getTags() == null || instance.getTags().getItems() == null ? "" : instance.getTags());
-                    continue;
-                } else {
-                    logger.trace("instance {} with tags {} is added to discovery", name, tags);
-                }
-
-                String ip_public = null;
-                String ip_private = null;
-
-                List<NetworkInterface> interfaces = instance.getNetworkInterfaces();
-
-                for (NetworkInterface networkInterface : interfaces) {
-                    if (ip_public == null) {
-                        // Trying to get Public IP Address (For future use)
-                        if (networkInterface.getAccessConfigs() != null) {
-                            for (AccessConfig accessConfig : networkInterface.getAccessConfigs()) {
-                                if (Strings.hasText(accessConfig.getNatIP())) {
-                                    ip_public = accessConfig.getNatIP();
-                                    break;
-                                }
-                            }
-                        }
-                    }
-
-                    if (ip_private == null) {
-                        ip_private = networkInterface.getNetworkIP();
-                    }
-
-                    // If we have both public and private, we can stop here
-                    if (ip_private != null && ip_public != null) break;
-                }
-
-                try {
-                    if (ip_private.equals(ipAddress)) {
-                        // We found the current node.
-                        // We can ignore it in the list of DiscoveryNode
-                        logger.trace("current node found. Ignoring {} - {}", name, ip_private);
-                    } else {
-                        String address = ip_private;
-                        // Test if we have es_port metadata defined here
-                        if (instance.getMetadata() != null && instance.getMetadata().containsKey("es_port")) {
-                            Object es_port = instance.getMetadata().get("es_port");
-                            logger.trace("es_port is defined with {}", es_port);
-                            if (es_port instanceof String) {
-                                address = address.concat(":").concat((String) es_port);
-                            } else {
-                                // Ignoring other values
-                                logger.trace("es_port is instance of {}. Ignoring...", es_port.getClass().getName());
-                            }
-                        }
-
-                        // ip_private is a single IP Address. We need to build a TransportAddress from it
-                        // If user has set `es_port` metadata, we don't need to ping all ports
-                        // we only limit to 1 addresses, makes no sense to ping 100 ports
-                        TransportAddress[] addresses = transportService.addressesFromString(address, 1);
-
-                        for (TransportAddress transportAddress : addresses) {
-                            logger.trace("adding {}, type {}, address {}, transport_address {}, status {}", name, type,
-                                    ip_private, transportAddress, status);
-                            cachedDiscoNodes.add(new DiscoveryNode("#cloud-" + name + "-" + 0, transportAddress, version.minimumCompatibilityVersion()));
-                        }
-                    }
-                } catch (Exception e) {
-                    logger.warn("failed to add {}, address {}", e, name, ip_private);
-                }
-
-            }
-        } catch (Throwable e) {
-            logger.warn("Exception caught during discovery: {}", e, e.getMessage());
-        }
-
-        logger.debug("{} node(s) added", cachedDiscoNodes.size());
-        logger.debug("using dynamic discovery nodes {}", cachedDiscoNodes);
-
-        return cachedDiscoNodes;
-    }
-}
diff --git a/plugins/cloud-gce/src/main/java/org/elasticsearch/plugin/cloud/gce/CloudGcePlugin.java b/plugins/cloud-gce/src/main/java/org/elasticsearch/plugin/cloud/gce/CloudGcePlugin.java
deleted file mode 100644
index 0ff8d4e..0000000
--- a/plugins/cloud-gce/src/main/java/org/elasticsearch/plugin/cloud/gce/CloudGcePlugin.java
+++ /dev/null
@@ -1,128 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.cloud.gce;
-
-import org.elasticsearch.cloud.gce.GceComputeService;
-import org.elasticsearch.cloud.gce.GceModule;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.component.LifecycleComponent;
-import org.elasticsearch.common.inject.Module;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.discovery.DiscoveryModule;
-import org.elasticsearch.discovery.gce.GceDiscovery;
-import org.elasticsearch.discovery.gce.GceUnicastHostsProvider;
-import org.elasticsearch.plugins.Plugin;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-
-/**
- *
- */
-public class CloudGcePlugin extends Plugin {
-
-    private final Settings settings;
-    protected final ESLogger logger = Loggers.getLogger(CloudGcePlugin.class);
-
-    public CloudGcePlugin(Settings settings) {
-        this.settings = settings;
-    }
-
-    @Override
-    public String name() {
-        return "cloud-gce";
-    }
-
-    @Override
-    public String description() {
-        return "Cloud Google Compute Engine Plugin";
-    }
-
-    @Override
-    public Collection<Module> nodeModules() {
-        List<Module> modules = new ArrayList<>();
-        if (isDiscoveryAlive(settings, logger)) {
-            modules.add(new GceModule());
-        }
-        return modules;
-    }
-
-    @Override
-    public Collection<Class<? extends LifecycleComponent>> nodeServices() {
-        Collection<Class<? extends LifecycleComponent>> services = new ArrayList<>();
-        if (isDiscoveryAlive(settings, logger)) {
-            services.add(GceModule.getComputeServiceImpl());
-        }
-        return services;
-    }
-
-    public void onModule(DiscoveryModule discoveryModule) {
-        if (isDiscoveryAlive(settings, logger)) {
-            discoveryModule.addDiscoveryType("gce", GceDiscovery.class);
-            discoveryModule.addUnicastHostProvider(GceUnicastHostsProvider.class);
-        }
-    }
-
-    /**
-     * Check if discovery is meant to start
-     *
-     * @return true if we can start gce discovery features
-     */
-    public static boolean isDiscoveryAlive(Settings settings, ESLogger logger) {
-        // User set discovery.type: gce
-        if (GceDiscovery.GCE.equalsIgnoreCase(settings.get("discovery.type")) == false) {
-            logger.debug("discovery.type not set to {}", GceDiscovery.GCE);
-            return false;
-        }
-
-        if (checkProperty(GceComputeService.Fields.PROJECT, settings.get(GceComputeService.Fields.PROJECT), logger) == false ||
-                checkProperty(GceComputeService.Fields.ZONE, settings.getAsArray(GceComputeService.Fields.ZONE), logger) == false) {
-            logger.debug("one or more gce discovery settings are missing. " +
-                            "Check elasticsearch.yml file. Should have [{}] and [{}].",
-                    GceComputeService.Fields.PROJECT,
-                    GceComputeService.Fields.ZONE);
-            return false;
-        }
-
-        logger.trace("all required properties for gce discovery are set!");
-
-        return true;
-    }
-
-    private static boolean checkProperty(String name, String value, ESLogger logger) {
-        if (!Strings.hasText(value)) {
-            logger.warn("{} is not set.", name);
-            return false;
-        }
-        return true;
-    }
-
-    private static boolean checkProperty(String name, String[] values, ESLogger logger) {
-        if (values == null || values.length == 0) {
-            logger.warn("{} is not set.", name);
-            return false;
-        }
-        return true;
-    }
-
-}
diff --git a/plugins/cloud-gce/src/test/java/org/elasticsearch/cloud/gce/CloudGCERestIT.java b/plugins/cloud-gce/src/test/java/org/elasticsearch/cloud/gce/CloudGCERestIT.java
deleted file mode 100644
index e209655..0000000
--- a/plugins/cloud-gce/src/test/java/org/elasticsearch/cloud/gce/CloudGCERestIT.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.gce;
-
-import com.carrotsearch.randomizedtesting.annotations.Name;
-import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
-import org.elasticsearch.plugin.cloud.gce.CloudGcePlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.rest.ESRestTestCase;
-import org.elasticsearch.test.rest.RestTestCandidate;
-import org.elasticsearch.test.rest.parser.RestTestParseException;
-
-import java.io.IOException;
-import java.util.Collection;
-
-public class CloudGCERestIT extends ESRestTestCase {
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(CloudGcePlugin.class);
-    }
-
-    public CloudGCERestIT(@Name("yaml") RestTestCandidate testCandidate) {
-        super(testCandidate);
-    }
-
-    @ParametersFactory
-    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
-        return ESRestTestCase.createParameters(0, 1);
-    }
-}
-
diff --git a/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceComputeServiceMock.java b/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceComputeServiceMock.java
deleted file mode 100644
index 1892297..0000000
--- a/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceComputeServiceMock.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.gce;
-
-import com.google.api.client.http.HttpTransport;
-import com.google.api.client.http.LowLevelHttpRequest;
-import com.google.api.client.http.LowLevelHttpResponse;
-import com.google.api.client.json.Json;
-import com.google.api.client.testing.http.MockHttpTransport;
-import com.google.api.client.testing.http.MockLowLevelHttpRequest;
-import com.google.api.client.testing.http.MockLowLevelHttpResponse;
-import org.elasticsearch.cloud.gce.GceComputeServiceImpl;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.Streams;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.util.Callback;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.net.URL;
-import java.security.GeneralSecurityException;
-
-/**
- *
- */
-public class GceComputeServiceMock extends GceComputeServiceImpl {
-
-    protected HttpTransport mockHttpTransport;
-
-    public GceComputeServiceMock(Settings settings) {
-        super(settings);
-        this.mockHttpTransport = configureMock();
-    }
-
-    @Override
-    protected HttpTransport getGceHttpTransport() throws GeneralSecurityException, IOException {
-        return this.mockHttpTransport;
-    }
-
-    protected HttpTransport configureMock() {
-        HttpTransport transport = new MockHttpTransport() {
-            @Override
-            public LowLevelHttpRequest buildRequest(String method, final String url) throws IOException {
-                return new MockLowLevelHttpRequest() {
-                    @Override
-                    public LowLevelHttpResponse execute() throws IOException {
-                        MockLowLevelHttpResponse response = new MockLowLevelHttpResponse();
-                        response.setStatusCode(200);
-                        response.setContentType(Json.MEDIA_TYPE);
-                        if (url.equals(TOKEN_SERVER_ENCODED_URL)) {
-                            logger.info("--> Simulate GCE Auth response for [{}]", url);
-                            response.setContent(readGoogleInternalJsonResponse(url));
-                        } else {
-                            logger.info("--> Simulate GCE API response for [{}]", url);
-                            response.setContent(readGoogleApiJsonResponse(url));
-                        }
-
-                        return response;
-                    }
-                };
-            }
-        };
-
-        return transport;
-    }
-
-    private String readGoogleInternalJsonResponse(String url) throws IOException {
-        return readJsonResponse(url, "http://metadata.google.internal/");
-    }
-
-    private String readGoogleApiJsonResponse(String url) throws IOException {
-        return readJsonResponse(url, "https://www.googleapis.com/");
-    }
-
-    private String readJsonResponse(String url, String urlRoot) throws IOException {
-        // We extract from the url the mock file path we want to use
-        String mockFileName = Strings.replace(url, urlRoot, "") + ".json";
-
-        logger.debug("--> read mock file from [{}]", mockFileName);
-        URL resource = GceComputeServiceMock.class.getResource(mockFileName);
-        try (InputStream is = resource.openStream()) {
-            final StringBuilder sb = new StringBuilder();
-            Streams.readAllLines(is, new Callback<String>() {
-                @Override
-                public void handle(String s) {
-                    sb.append(s).append("\n");
-                }
-            });
-            String response = sb.toString();
-            logger.trace("{}", response);
-            return response;
-        } catch (IOException e) {
-            throw e;
-        }
-    }
-}
diff --git a/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoverySettingsTests.java b/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoverySettingsTests.java
deleted file mode 100644
index 90b331d..0000000
--- a/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoverySettingsTests.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.gce;
-
-import org.elasticsearch.cloud.gce.GceModule;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.cloud.gce.CloudGcePlugin;
-import org.elasticsearch.test.ESTestCase;
-
-import static org.hamcrest.Matchers.is;
-
-public class GceDiscoverySettingsTests extends ESTestCase {
-    public void testDiscoveryReady() {
-        Settings settings = Settings.builder()
-                .put("discovery.type", "gce")
-                .put("cloud.gce.project_id", "gce_id")
-                .putArray("cloud.gce.zone", "gce_zones_1", "gce_zones_2")
-                .build();
-
-        boolean discoveryReady = CloudGcePlugin.isDiscoveryAlive(settings, logger);
-        assertThat(discoveryReady, is(true));
-    }
-
-    public void testDiscoveryNotReady() {
-        Settings settings = Settings.EMPTY;
-        boolean discoveryReady = CloudGcePlugin.isDiscoveryAlive(settings, logger);
-        assertThat(discoveryReady, is(false));
-
-        settings = Settings.builder()
-                .put("discovery.type", "gce")
-                .build();
-
-        discoveryReady = CloudGcePlugin.isDiscoveryAlive(settings, logger);
-        assertThat(discoveryReady, is(false));
-
-        settings = Settings.builder()
-                .put("discovery.type", "gce")
-                .put("cloud.gce.project_id", "gce_id")
-                .build();
-
-        discoveryReady = CloudGcePlugin.isDiscoveryAlive(settings, logger);
-        assertThat(discoveryReady, is(false));
-
-
-        settings = Settings.builder()
-                .put("discovery.type", "gce")
-                .putArray("cloud.gce.zone", "gce_zones_1", "gce_zones_2")
-                .build();
-
-        discoveryReady = CloudGcePlugin.isDiscoveryAlive(settings, logger);
-        assertThat(discoveryReady, is(false));
-
-        settings = Settings.builder()
-                .put("cloud.gce.project_id", "gce_id")
-                .putArray("cloud.gce.zone", "gce_zones_1", "gce_zones_2")
-                .build();
-
-        discoveryReady = CloudGcePlugin.isDiscoveryAlive(settings, logger);
-        assertThat(discoveryReady, is(false));
-    }
-}
diff --git a/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java b/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java
deleted file mode 100644
index b18cca1..0000000
--- a/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java
+++ /dev/null
@@ -1,217 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.gce;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cloud.gce.GceComputeService;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.network.NetworkService;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.transport.MockTransportService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.local.LocalTransport;
-import org.junit.*;
-
-import java.util.List;
-import java.util.Locale;
-
-import static org.hamcrest.Matchers.hasSize;
-import static org.hamcrest.Matchers.is;
-
-/**
- * This test class uses a GCE HTTP Mock system which allows to simulate JSON Responses.
- *
- * To implement a new test you'll need to create an `instances.json` file which contains expected response
- * for a given project-id and zone under the src/test/resources/org/elasticsearch/discovery/gce with dir name:
- *
- * compute/v1/projects/[project-id]/zones/[zone]
- *
- * By default, project-id is the test method name, lowercase.
- *
- * For example, if you create a test `myNewAwesomeTest` with following settings:
- *
- * Settings nodeSettings = Settings.builder()
- *  .put(GceComputeService.Fields.PROJECT, projectName)
- *  .put(GceComputeService.Fields.ZONE, "europe-west1-b")
- *  .build();
- *
- *  You need to create a file under `src/test/resources/org/elasticsearch/discovery/gce/` named:
- *
- *  compute/v1/projects/mynewawesometest/zones/europe-west1-b/instances.json
- *
- */
-public class GceDiscoveryTests extends ESTestCase {
-
-    protected static ThreadPool threadPool;
-    protected MockTransportService transportService;
-    protected GceComputeService mock;
-    protected String projectName;
-
-    @BeforeClass
-    public static void createThreadPool() {
-        threadPool = new ThreadPool(GceDiscoveryTests.class.getName());
-    }
-
-    @AfterClass
-    public static void stopThreadPool() {
-        if (threadPool !=null) {
-            threadPool.shutdownNow();
-            threadPool = null;
-        }
-    }
-
-    @Before
-    public void setProjectName() {
-        projectName = getTestName().toLowerCase(Locale.ROOT);
-    }
-
-    @Before
-    public void createTransportService() {
-        transportService = new MockTransportService(
-                Settings.EMPTY,
-                new LocalTransport(Settings.EMPTY, threadPool, Version.CURRENT, new NamedWriteableRegistry()), threadPool);
-    }
-
-    @After
-    public void stopGceComputeService() {
-        if (mock != null) {
-            mock.stop();
-        }
-    }
-
-    protected List<DiscoveryNode> buildDynamicNodes(GceComputeService gceComputeService, Settings nodeSettings) {
-        GceUnicastHostsProvider provider = new GceUnicastHostsProvider(nodeSettings, gceComputeService,
-                transportService, new NetworkService(Settings.EMPTY), Version.CURRENT);
-
-        List<DiscoveryNode> discoveryNodes = provider.buildDynamicNodes();
-        logger.info("--> nodes found: {}", discoveryNodes);
-        return discoveryNodes;
-    }
-
-    @Test
-    public void nodesWithDifferentTagsAndNoTagSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    @Test
-    public void nodesWithDifferentTagsAndOneTagSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .putArray(GceComputeService.Fields.TAGS, "elasticsearch")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(1));
-        assertThat(discoveryNodes.get(0).getId(), is("#cloud-test2-0"));
-    }
-
-    @Test
-    public void nodesWithDifferentTagsAndTwoTagSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .putArray(GceComputeService.Fields.TAGS, "elasticsearch", "dev")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(1));
-        assertThat(discoveryNodes.get(0).getId(), is("#cloud-test2-0"));
-    }
-
-    @Test
-    public void nodesWithSameTagsAndNoTagSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    @Test
-    public void nodesWithSameTagsAndOneTagSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .putArray(GceComputeService.Fields.TAGS, "elasticsearch")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    @Test
-    public void nodesWithSameTagsAndTwoTagsSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .putArray(GceComputeService.Fields.TAGS, "elasticsearch", "dev")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    @Test
-    public void multipleZonesAndTwoNodesInSameZone() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "europe-west1-b")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    @Test
-    public void multipleZonesAndTwoNodesInDifferentZones() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "europe-west1-b")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    /**
-     * For issue https://github.com/elastic/elasticsearch-cloud-gce/issues/43
-     */
-    @Test
-    public void zeroNode43() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "us-central1-b")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(0));
-    }
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/europe-west1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/europe-west1-b/instances.json
deleted file mode 100644
index 049e0e1..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/europe-west1-b/instances.json
+++ /dev/null
@@ -1,36 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 1",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test1",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/us-central1-a/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/us-central1-a/instances.json
deleted file mode 100644
index 7e1e5d5..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/us-central1-a/instances.json
+++ /dev/null
@@ -1,36 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 2",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test2",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "us-central1-a"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/europe-west1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/europe-west1-b/instances.json
deleted file mode 100644
index 78de693..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/europe-west1-b/instances.json
+++ /dev/null
@@ -1,67 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 1",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test1",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    },
-    {
-      "description": "ES Node 2",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test2",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/us-central1-a/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/us-central1-a/instances.json
deleted file mode 100644
index 54c3836..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/us-central1-a/instances.json
+++ /dev/null
@@ -1,5 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandnotagset/zones/europe-west1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandnotagset/zones/europe-west1-b/instances.json
deleted file mode 100644
index 1ca810c..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandnotagset/zones/europe-west1-b/instances.json
+++ /dev/null
@@ -1,66 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 1",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test1",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    },
-    {
-      "description": "ES Node 2",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test2",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandonetagset/zones/europe-west1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandonetagset/zones/europe-west1-b/instances.json
deleted file mode 100644
index 1ca810c..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandonetagset/zones/europe-west1-b/instances.json
+++ /dev/null
@@ -1,66 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 1",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test1",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    },
-    {
-      "description": "ES Node 2",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test2",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandtwotagset/zones/europe-west1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandtwotagset/zones/europe-west1-b/instances.json
deleted file mode 100644
index 1ca810c..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandtwotagset/zones/europe-west1-b/instances.json
+++ /dev/null
@@ -1,66 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 1",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test1",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    },
-    {
-      "description": "ES Node 2",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test2",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandnotagset/zones/europe-west1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandnotagset/zones/europe-west1-b/instances.json
deleted file mode 100644
index 78de693..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandnotagset/zones/europe-west1-b/instances.json
+++ /dev/null
@@ -1,67 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 1",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test1",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    },
-    {
-      "description": "ES Node 2",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test2",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandonetagset/zones/europe-west1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandonetagset/zones/europe-west1-b/instances.json
deleted file mode 100644
index 78de693..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandonetagset/zones/europe-west1-b/instances.json
+++ /dev/null
@@ -1,67 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 1",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test1",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    },
-    {
-      "description": "ES Node 2",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test2",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandtwotagsset/zones/europe-west1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandtwotagsset/zones/europe-west1-b/instances.json
deleted file mode 100644
index 78de693..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandtwotagsset/zones/europe-west1-b/instances.json
+++ /dev/null
@@ -1,67 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 1",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test1",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    },
-    {
-      "description": "ES Node 2",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test2",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-a/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-a/instances.json
deleted file mode 100644
index 54c3836..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-a/instances.json
+++ /dev/null
@@ -1,5 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-b/instances.json
deleted file mode 100644
index 54c3836..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-b/instances.json
+++ /dev/null
@@ -1,5 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/service-accounts/default/token.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/service-accounts/default/token.json
deleted file mode 100644
index b338f61..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/service-accounts/default/token.json
+++ /dev/null
@@ -1,4 +0,0 @@
-{
-  "access_token":"1/fFAGRNJru1FTz70BzhT3Zg",
-  "token_type":"Bearer"
-}
diff --git a/plugins/cloud-gce/src/test/resources/rest-api-spec/test/cloud_gce/10_basic.yaml b/plugins/cloud-gce/src/test/resources/rest-api-spec/test/cloud_gce/10_basic.yaml
deleted file mode 100644
index 9cff52e..0000000
--- a/plugins/cloud-gce/src/test/resources/rest-api-spec/test/cloud_gce/10_basic.yaml
+++ /dev/null
@@ -1,14 +0,0 @@
-# Integration tests for Cloud GCE components
-#
-"Cloud GCE loaded":
-    - do:
-        cluster.state: {}
-
-    # Get master node id
-    - set: { master_node: master }
-
-    - do:
-        nodes.info: {}
-
-    - match:  { nodes.$master.plugins.0.name: cloud-gce  }
-    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java
index f04d3ec..3788f82 100755
--- a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java
@@ -91,31 +91,25 @@ public class Ec2NameResolver extends AbstractComponent implements CustomNameReso
      * @return the appropriate host resolved from ec2 meta-data, or null if it cannot be obtained.
      * @see CustomNameResolver#resolveIfPossible(String)
      */
-    public InetAddress[] resolve(Ec2HostnameType type, boolean warnOnFailure) {
-        URLConnection urlConnection = null;
+    public InetAddress[] resolve(Ec2HostnameType type) throws IOException {
         InputStream in = null;
+        String metadataUrl = AwsEc2ServiceImpl.EC2_METADATA_URL + type.ec2Name;
         try {
-            URL url = new URL(AwsEc2ServiceImpl.EC2_METADATA_URL + type.ec2Name);
+            URL url = new URL(metadataUrl);
             logger.debug("obtaining ec2 hostname from ec2 meta-data url {}", url);
-            urlConnection = url.openConnection();
+            URLConnection urlConnection = url.openConnection();
             urlConnection.setConnectTimeout(2000);
             in = urlConnection.getInputStream();
             BufferedReader urlReader = new BufferedReader(new InputStreamReader(in, StandardCharsets.UTF_8));
 
             String metadataResult = urlReader.readLine();
             if (metadataResult == null || metadataResult.length() == 0) {
-                logger.error("no ec2 metadata returned from {}", url);
-                return null;
+                throw new IOException("no gce metadata returned from [" + url + "] for [" + type.configName + "]");
             }
             // only one address: because we explicitly ask for only one via the Ec2HostnameType
             return new InetAddress[] { InetAddress.getByName(metadataResult) };
         } catch (IOException e) {
-            if (warnOnFailure) {
-                logger.warn("failed to get metadata for [" + type.configName + "]", e);
-            } else {
-                logger.debug("failed to get metadata for [" + type.configName + "]", e);
-            }
-            return null;
+            throw new IOException("IOException caught when fetching InetAddress from [" + metadataUrl + "]", e);
         } finally {
             IOUtils.closeWhileHandlingException(in);
         }
@@ -128,10 +122,10 @@ public class Ec2NameResolver extends AbstractComponent implements CustomNameReso
     }
 
     @Override
-    public InetAddress[] resolveIfPossible(String value) {
+    public InetAddress[] resolveIfPossible(String value) throws IOException {
         for (Ec2HostnameType type : Ec2HostnameType.values()) {
             if (type.configName.equals(value)) {
-                return resolve(type, true);
+                return resolve(type);
             }
         }
         return null;
diff --git a/plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/Ec2NetworkTests.java b/plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/Ec2NetworkTests.java
new file mode 100644
index 0000000..8aa9ca5
--- /dev/null
+++ b/plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/Ec2NetworkTests.java
@@ -0,0 +1,187 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.ec2;
+
+import org.elasticsearch.cloud.aws.network.Ec2NameResolver;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.net.InetAddress;
+
+import static org.hamcrest.Matchers.arrayContaining;
+import static org.hamcrest.Matchers.containsString;
+
+/**
+ * Test for EC2 network.host settings.
+ */
+public class Ec2NetworkTests extends ESTestCase {
+
+    /**
+     * Test for network.host: _ec2_
+     */
+    @Test
+    public void networkHostEc2() throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", "_ec2_")
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        networkService.addCustomNameResolver(new Ec2NameResolver(nodeSettings));
+        // TODO we need to replace that with a mock. For now we check the URL we are supposed to reach.
+        try {
+            networkService.resolveBindHostAddress(null);
+        } catch (IOException e) {
+            assertThat(e.getMessage(), containsString("local-ipv4"));
+        }
+    }
+
+    /**
+     * Test for network.host: _ec2:publicIp_
+     */
+    @Test
+    public void networkHostEc2PublicIp() throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", "_ec2:publicIp_")
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        networkService.addCustomNameResolver(new Ec2NameResolver(nodeSettings));
+        // TODO we need to replace that with a mock. For now we check the URL we are supposed to reach.
+        try {
+            networkService.resolveBindHostAddress(null);
+        } catch (IOException e) {
+            assertThat(e.getMessage(), containsString("public-ipv4"));
+        }
+    }
+
+    /**
+     * Test for network.host: _ec2:privateIp_
+     */
+    @Test
+    public void networkHostEc2PrivateIp() throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", "_ec2:privateIp_")
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        networkService.addCustomNameResolver(new Ec2NameResolver(nodeSettings));
+        // TODO we need to replace that with a mock. For now we check the URL we are supposed to reach.
+        try {
+            networkService.resolveBindHostAddress(null);
+        } catch (IOException e) {
+            assertThat(e.getMessage(), containsString("local-ipv4"));
+        }
+    }
+
+    /**
+     * Test for network.host: _ec2:privateIpv4_
+     */
+    @Test
+    public void networkHostEc2PrivateIpv4() throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", "_ec2:privateIpv4_")
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        networkService.addCustomNameResolver(new Ec2NameResolver(nodeSettings));
+        // TODO we need to replace that with a mock. For now we check the URL we are supposed to reach.
+        try {
+            networkService.resolveBindHostAddress(null);
+        } catch (IOException e) {
+            assertThat(e.getMessage(), containsString("local-ipv4"));
+        }
+    }
+
+    /**
+     * Test for network.host: _ec2:privateDns_
+     */
+    @Test
+    public void networkHostEc2PrivateDns() throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", "_ec2:privateDns_")
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        networkService.addCustomNameResolver(new Ec2NameResolver(nodeSettings));
+        // TODO we need to replace that with a mock. For now we check the URL we are supposed to reach.
+        try {
+            networkService.resolveBindHostAddress(null);
+        } catch (IOException e) {
+            assertThat(e.getMessage(), containsString("local-hostname"));
+        }
+    }
+
+    /**
+     * Test for network.host: _ec2:publicIpv4_
+     */
+    @Test
+    public void networkHostEc2PublicIpv4() throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", "_ec2:publicIpv4_")
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        networkService.addCustomNameResolver(new Ec2NameResolver(nodeSettings));
+        // TODO we need to replace that with a mock. For now we check the URL we are supposed to reach.
+        try {
+            networkService.resolveBindHostAddress(null);
+        } catch (IOException e) {
+            assertThat(e.getMessage(), containsString("public-ipv4"));
+        }
+    }
+
+    /**
+     * Test for network.host: _ec2:publicDns_
+     */
+    @Test
+    public void networkHostEc2PublicDns() throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", "_ec2:publicDns_")
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        networkService.addCustomNameResolver(new Ec2NameResolver(nodeSettings));
+        // TODO we need to replace that with a mock. For now we check the URL we are supposed to reach.
+        try {
+            networkService.resolveBindHostAddress(null);
+        } catch (IOException e) {
+            assertThat(e.getMessage(), containsString("public-hostname"));
+        }
+    }
+
+    /**
+     * Test that we don't have any regression with network host core settings such as
+     * network.host: _local_
+     */
+    @Test
+    public void networkHostCoreLocal() throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", "_local_")
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        networkService.addCustomNameResolver(new Ec2NameResolver(nodeSettings));
+        InetAddress[] addresses = networkService.resolveBindHostAddress(null);
+        assertThat(addresses, arrayContaining(networkService.resolveBindHostAddress("_local_")));
+    }
+}
diff --git a/plugins/discovery-gce/licenses/commons-codec-1.6.jar.sha1 b/plugins/discovery-gce/licenses/commons-codec-1.6.jar.sha1
new file mode 100644
index 0000000..bf78aff
--- /dev/null
+++ b/plugins/discovery-gce/licenses/commons-codec-1.6.jar.sha1
@@ -0,0 +1 @@
+b7f0fc8f61ecadeb3695f0b9464755eee44374d4
diff --git a/plugins/discovery-gce/licenses/commons-codec-LICENSE.txt b/plugins/discovery-gce/licenses/commons-codec-LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/discovery-gce/licenses/commons-codec-LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/discovery-gce/licenses/commons-codec-NOTICE.txt b/plugins/discovery-gce/licenses/commons-codec-NOTICE.txt
new file mode 100644
index 0000000..5691644
--- /dev/null
+++ b/plugins/discovery-gce/licenses/commons-codec-NOTICE.txt
@@ -0,0 +1,17 @@
+Apache Commons Codec
+Copyright 2002-2015 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
+
+src/test/org/apache/commons/codec/language/DoubleMetaphoneTest.java
+contains test data from http://aspell.net/test/orig/batch0.tab.
+Copyright (C) 2002 Kevin Atkinson (kevina@gnu.org)
+
+===============================================================================
+
+The content of package org.apache.commons.codec.language.bm has been translated
+from the original php source code available at http://stevemorse.org/phoneticinfo.htm
+with permission from the original authors.
+Original source copyright:
+Copyright (c) 2008 Alexander Beider & Stephen P. Morse.
diff --git a/plugins/discovery-gce/licenses/commons-logging-1.1.3.jar.sha1 b/plugins/discovery-gce/licenses/commons-logging-1.1.3.jar.sha1
new file mode 100644
index 0000000..c8756c4
--- /dev/null
+++ b/plugins/discovery-gce/licenses/commons-logging-1.1.3.jar.sha1
@@ -0,0 +1 @@
+f6f66e966c70a83ffbdb6f17a0919eaf7c8aca7f
diff --git a/plugins/discovery-gce/licenses/commons-logging-LICENSE.txt b/plugins/discovery-gce/licenses/commons-logging-LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/discovery-gce/licenses/commons-logging-LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/discovery-gce/licenses/commons-logging-NOTICE.txt b/plugins/discovery-gce/licenses/commons-logging-NOTICE.txt
new file mode 100644
index 0000000..d3d6e14
--- /dev/null
+++ b/plugins/discovery-gce/licenses/commons-logging-NOTICE.txt
@@ -0,0 +1,5 @@
+Apache Commons Logging
+Copyright 2003-2014 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/discovery-gce/licenses/google-LICENSE.txt b/plugins/discovery-gce/licenses/google-LICENSE.txt
new file mode 100644
index 0000000..980a15a
--- /dev/null
+++ b/plugins/discovery-gce/licenses/google-LICENSE.txt
@@ -0,0 +1,201 @@
+                                Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "{}"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright {yyyy} {name of copyright owner}
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/discovery-gce/licenses/google-NOTICE.txt b/plugins/discovery-gce/licenses/google-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/discovery-gce/licenses/google-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/discovery-gce/licenses/google-api-client-1.20.0.jar.sha1 b/plugins/discovery-gce/licenses/google-api-client-1.20.0.jar.sha1
new file mode 100644
index 0000000..08c24d1
--- /dev/null
+++ b/plugins/discovery-gce/licenses/google-api-client-1.20.0.jar.sha1
@@ -0,0 +1 @@
+d3e66209ae9e749b2d6833761e7885f60f285564
diff --git a/plugins/discovery-gce/licenses/google-api-services-compute-v1-rev71-1.20.0.jar.sha1 b/plugins/discovery-gce/licenses/google-api-services-compute-v1-rev71-1.20.0.jar.sha1
new file mode 100644
index 0000000..c6e6948
--- /dev/null
+++ b/plugins/discovery-gce/licenses/google-api-services-compute-v1-rev71-1.20.0.jar.sha1
@@ -0,0 +1 @@
+2fa36fff3b5bf59a63c4f2bbfac1f88251cd7986
diff --git a/plugins/discovery-gce/licenses/google-http-client-1.20.0.jar.sha1 b/plugins/discovery-gce/licenses/google-http-client-1.20.0.jar.sha1
new file mode 100644
index 0000000..66a2247
--- /dev/null
+++ b/plugins/discovery-gce/licenses/google-http-client-1.20.0.jar.sha1
@@ -0,0 +1 @@
+93d82db2bca534960253f43424b2ba9d7638b4d2
diff --git a/plugins/discovery-gce/licenses/google-http-client-jackson2-1.20.0.jar.sha1 b/plugins/discovery-gce/licenses/google-http-client-jackson2-1.20.0.jar.sha1
new file mode 100644
index 0000000..6d861e6
--- /dev/null
+++ b/plugins/discovery-gce/licenses/google-http-client-jackson2-1.20.0.jar.sha1
@@ -0,0 +1 @@
+2408070b2abec043624d35b35e30450f1b663858
diff --git a/plugins/discovery-gce/licenses/google-oauth-client-1.20.0.jar.sha1 b/plugins/discovery-gce/licenses/google-oauth-client-1.20.0.jar.sha1
new file mode 100644
index 0000000..c35c4bc
--- /dev/null
+++ b/plugins/discovery-gce/licenses/google-oauth-client-1.20.0.jar.sha1
@@ -0,0 +1 @@
+1d086ac5756475ddf451af2e2df6e288d18608ca
diff --git a/plugins/discovery-gce/licenses/httpclient-4.3.6.jar.sha1 b/plugins/discovery-gce/licenses/httpclient-4.3.6.jar.sha1
new file mode 100644
index 0000000..3d35ee9
--- /dev/null
+++ b/plugins/discovery-gce/licenses/httpclient-4.3.6.jar.sha1
@@ -0,0 +1 @@
+4c47155e3e6c9a41a28db36680b828ced53b8af4
diff --git a/plugins/discovery-gce/licenses/httpclient-LICENSE.txt b/plugins/discovery-gce/licenses/httpclient-LICENSE.txt
new file mode 100644
index 0000000..32f01ed
--- /dev/null
+++ b/plugins/discovery-gce/licenses/httpclient-LICENSE.txt
@@ -0,0 +1,558 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+=========================================================================
+
+This project includes Public Suffix List copied from
+<https://publicsuffix.org/list/effective_tld_names.dat>
+licensed under the terms of the Mozilla Public License, v. 2.0
+
+Full license text: <http://mozilla.org/MPL/2.0/>
+
+Mozilla Public License Version 2.0
+==================================
+
+1. Definitions
+--------------
+
+1.1. "Contributor"
+    means each individual or legal entity that creates, contributes to
+    the creation of, or owns Covered Software.
+
+1.2. "Contributor Version"
+    means the combination of the Contributions of others (if any) used
+    by a Contributor and that particular Contributor's Contribution.
+
+1.3. "Contribution"
+    means Covered Software of a particular Contributor.
+
+1.4. "Covered Software"
+    means Source Code Form to which the initial Contributor has attached
+    the notice in Exhibit A, the Executable Form of such Source Code
+    Form, and Modifications of such Source Code Form, in each case
+    including portions thereof.
+
+1.5. "Incompatible With Secondary Licenses"
+    means
+
+    (a) that the initial Contributor has attached the notice described
+        in Exhibit B to the Covered Software; or
+
+    (b) that the Covered Software was made available under the terms of
+        version 1.1 or earlier of the License, but not also under the
+        terms of a Secondary License.
+
+1.6. "Executable Form"
+    means any form of the work other than Source Code Form.
+
+1.7. "Larger Work"
+    means a work that combines Covered Software with other material, in
+    a separate file or files, that is not Covered Software.
+
+1.8. "License"
+    means this document.
+
+1.9. "Licensable"
+    means having the right to grant, to the maximum extent possible,
+    whether at the time of the initial grant or subsequently, any and
+    all of the rights conveyed by this License.
+
+1.10. "Modifications"
+    means any of the following:
+
+    (a) any file in Source Code Form that results from an addition to,
+        deletion from, or modification of the contents of Covered
+        Software; or
+
+    (b) any new file in Source Code Form that contains any Covered
+        Software.
+
+1.11. "Patent Claims" of a Contributor
+    means any patent claim(s), including without limitation, method,
+    process, and apparatus claims, in any patent Licensable by such
+    Contributor that would be infringed, but for the grant of the
+    License, by the making, using, selling, offering for sale, having
+    made, import, or transfer of either its Contributions or its
+    Contributor Version.
+
+1.12. "Secondary License"
+    means either the GNU General Public License, Version 2.0, the GNU
+    Lesser General Public License, Version 2.1, the GNU Affero General
+    Public License, Version 3.0, or any later versions of those
+    licenses.
+
+1.13. "Source Code Form"
+    means the form of the work preferred for making modifications.
+
+1.14. "You" (or "Your")
+    means an individual or a legal entity exercising rights under this
+    License. For legal entities, "You" includes any entity that
+    controls, is controlled by, or is under common control with You. For
+    purposes of this definition, "control" means (a) the power, direct
+    or indirect, to cause the direction or management of such entity,
+    whether by contract or otherwise, or (b) ownership of more than
+    fifty percent (50%) of the outstanding shares or beneficial
+    ownership of such entity.
+
+2. License Grants and Conditions
+--------------------------------
+
+2.1. Grants
+
+Each Contributor hereby grants You a world-wide, royalty-free,
+non-exclusive license:
+
+(a) under intellectual property rights (other than patent or trademark)
+    Licensable by such Contributor to use, reproduce, make available,
+    modify, display, perform, distribute, and otherwise exploit its
+    Contributions, either on an unmodified basis, with Modifications, or
+    as part of a Larger Work; and
+
+(b) under Patent Claims of such Contributor to make, use, sell, offer
+    for sale, have made, import, and otherwise transfer either its
+    Contributions or its Contributor Version.
+
+2.2. Effective Date
+
+The licenses granted in Section 2.1 with respect to any Contribution
+become effective for each Contribution on the date the Contributor first
+distributes such Contribution.
+
+2.3. Limitations on Grant Scope
+
+The licenses granted in this Section 2 are the only rights granted under
+this License. No additional rights or licenses will be implied from the
+distribution or licensing of Covered Software under this License.
+Notwithstanding Section 2.1(b) above, no patent license is granted by a
+Contributor:
+
+(a) for any code that a Contributor has removed from Covered Software;
+    or
+
+(b) for infringements caused by: (i) Your and any other third party's
+    modifications of Covered Software, or (ii) the combination of its
+    Contributions with other software (except as part of its Contributor
+    Version); or
+
+(c) under Patent Claims infringed by Covered Software in the absence of
+    its Contributions.
+
+This License does not grant any rights in the trademarks, service marks,
+or logos of any Contributor (except as may be necessary to comply with
+the notice requirements in Section 3.4).
+
+2.4. Subsequent Licenses
+
+No Contributor makes additional grants as a result of Your choice to
+distribute the Covered Software under a subsequent version of this
+License (see Section 10.2) or under the terms of a Secondary License (if
+permitted under the terms of Section 3.3).
+
+2.5. Representation
+
+Each Contributor represents that the Contributor believes its
+Contributions are its original creation(s) or it has sufficient rights
+to grant the rights to its Contributions conveyed by this License.
+
+2.6. Fair Use
+
+This License is not intended to limit any rights You have under
+applicable copyright doctrines of fair use, fair dealing, or other
+equivalents.
+
+2.7. Conditions
+
+Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted
+in Section 2.1.
+
+3. Responsibilities
+-------------------
+
+3.1. Distribution of Source Form
+
+All distribution of Covered Software in Source Code Form, including any
+Modifications that You create or to which You contribute, must be under
+the terms of this License. You must inform recipients that the Source
+Code Form of the Covered Software is governed by the terms of this
+License, and how they can obtain a copy of this License. You may not
+attempt to alter or restrict the recipients' rights in the Source Code
+Form.
+
+3.2. Distribution of Executable Form
+
+If You distribute Covered Software in Executable Form then:
+
+(a) such Covered Software must also be made available in Source Code
+    Form, as described in Section 3.1, and You must inform recipients of
+    the Executable Form how they can obtain a copy of such Source Code
+    Form by reasonable means in a timely manner, at a charge no more
+    than the cost of distribution to the recipient; and
+
+(b) You may distribute such Executable Form under the terms of this
+    License, or sublicense it under different terms, provided that the
+    license for the Executable Form does not attempt to limit or alter
+    the recipients' rights in the Source Code Form under this License.
+
+3.3. Distribution of a Larger Work
+
+You may create and distribute a Larger Work under terms of Your choice,
+provided that You also comply with the requirements of this License for
+the Covered Software. If the Larger Work is a combination of Covered
+Software with a work governed by one or more Secondary Licenses, and the
+Covered Software is not Incompatible With Secondary Licenses, this
+License permits You to additionally distribute such Covered Software
+under the terms of such Secondary License(s), so that the recipient of
+the Larger Work may, at their option, further distribute the Covered
+Software under the terms of either this License or such Secondary
+License(s).
+
+3.4. Notices
+
+You may not remove or alter the substance of any license notices
+(including copyright notices, patent notices, disclaimers of warranty,
+or limitations of liability) contained within the Source Code Form of
+the Covered Software, except that You may alter any license notices to
+the extent required to remedy known factual inaccuracies.
+
+3.5. Application of Additional Terms
+
+You may choose to offer, and to charge a fee for, warranty, support,
+indemnity or liability obligations to one or more recipients of Covered
+Software. However, You may do so only on Your own behalf, and not on
+behalf of any Contributor. You must make it absolutely clear that any
+such warranty, support, indemnity, or liability obligation is offered by
+You alone, and You hereby agree to indemnify every Contributor for any
+liability incurred by such Contributor as a result of warranty, support,
+indemnity or liability terms You offer. You may include additional
+disclaimers of warranty and limitations of liability specific to any
+jurisdiction.
+
+4. Inability to Comply Due to Statute or Regulation
+---------------------------------------------------
+
+If it is impossible for You to comply with any of the terms of this
+License with respect to some or all of the Covered Software due to
+statute, judicial order, or regulation then You must: (a) comply with
+the terms of this License to the maximum extent possible; and (b)
+describe the limitations and the code they affect. Such description must
+be placed in a text file included with all distributions of the Covered
+Software under this License. Except to the extent prohibited by statute
+or regulation, such description must be sufficiently detailed for a
+recipient of ordinary skill to be able to understand it.
+
+5. Termination
+--------------
+
+5.1. The rights granted under this License will terminate automatically
+if You fail to comply with any of its terms. However, if You become
+compliant, then the rights granted under this License from a particular
+Contributor are reinstated (a) provisionally, unless and until such
+Contributor explicitly and finally terminates Your grants, and (b) on an
+ongoing basis, if such Contributor fails to notify You of the
+non-compliance by some reasonable means prior to 60 days after You have
+come back into compliance. Moreover, Your grants from a particular
+Contributor are reinstated on an ongoing basis if such Contributor
+notifies You of the non-compliance by some reasonable means, this is the
+first time You have received notice of non-compliance with this License
+from such Contributor, and You become compliant prior to 30 days after
+Your receipt of the notice.
+
+5.2. If You initiate litigation against any entity by asserting a patent
+infringement claim (excluding declaratory judgment actions,
+counter-claims, and cross-claims) alleging that a Contributor Version
+directly or indirectly infringes any patent, then the rights granted to
+You by any and all Contributors for the Covered Software under Section
+2.1 of this License shall terminate.
+
+5.3. In the event of termination under Sections 5.1 or 5.2 above, all
+end user license agreements (excluding distributors and resellers) which
+have been validly granted by You or Your distributors under this License
+prior to termination shall survive termination.
+
+************************************************************************
+*                                                                      *
+*  6. Disclaimer of Warranty                                           *
+*  -------------------------                                           *
+*                                                                      *
+*  Covered Software is provided under this License on an "as is"       *
+*  basis, without warranty of any kind, either expressed, implied, or  *
+*  statutory, including, without limitation, warranties that the       *
+*  Covered Software is free of defects, merchantable, fit for a        *
+*  particular purpose or non-infringing. The entire risk as to the     *
+*  quality and performance of the Covered Software is with You.        *
+*  Should any Covered Software prove defective in any respect, You     *
+*  (not any Contributor) assume the cost of any necessary servicing,   *
+*  repair, or correction. This disclaimer of warranty constitutes an   *
+*  essential part of this License. No use of any Covered Software is   *
+*  authorized under this License except under this disclaimer.         *
+*                                                                      *
+************************************************************************
+
+************************************************************************
+*                                                                      *
+*  7. Limitation of Liability                                          *
+*  --------------------------                                          *
+*                                                                      *
+*  Under no circumstances and under no legal theory, whether tort      *
+*  (including negligence), contract, or otherwise, shall any           *
+*  Contributor, or anyone who distributes Covered Software as          *
+*  permitted above, be liable to You for any direct, indirect,         *
+*  special, incidental, or consequential damages of any character      *
+*  including, without limitation, damages for lost profits, loss of    *
+*  goodwill, work stoppage, computer failure or malfunction, or any    *
+*  and all other commercial damages or losses, even if such party      *
+*  shall have been informed of the possibility of such damages. This   *
+*  limitation of liability shall not apply to liability for death or   *
+*  personal injury resulting from such party's negligence to the       *
+*  extent applicable law prohibits such limitation. Some               *
+*  jurisdictions do not allow the exclusion or limitation of           *
+*  incidental or consequential damages, so this exclusion and          *
+*  limitation may not apply to You.                                    *
+*                                                                      *
+************************************************************************
+
+8. Litigation
+-------------
+
+Any litigation relating to this License may be brought only in the
+courts of a jurisdiction where the defendant maintains its principal
+place of business and such litigation shall be governed by laws of that
+jurisdiction, without reference to its conflict-of-law provisions.
+Nothing in this Section shall prevent a party's ability to bring
+cross-claims or counter-claims.
+
+9. Miscellaneous
+----------------
+
+This License represents the complete agreement concerning the subject
+matter hereof. If any provision of this License is held to be
+unenforceable, such provision shall be reformed only to the extent
+necessary to make it enforceable. Any law or regulation which provides
+that the language of a contract shall be construed against the drafter
+shall not be used to construe this License against a Contributor.
+
+10. Versions of the License
+---------------------------
+
+10.1. New Versions
+
+Mozilla Foundation is the license steward. Except as provided in Section
+10.3, no one other than the license steward has the right to modify or
+publish new versions of this License. Each version will be given a
+distinguishing version number.
+
+10.2. Effect of New Versions
+
+You may distribute the Covered Software under the terms of the version
+of the License under which You originally received the Covered Software,
+or under the terms of any subsequent version published by the license
+steward.
+
+10.3. Modified Versions
+
+If you create software not governed by this License, and you want to
+create a new license for such software, you may create and use a
+modified version of this License if you rename the license and remove
+any references to the name of the license steward (except to note that
+such modified license differs from this License).
+
+10.4. Distributing Source Code Form that is Incompatible With Secondary
+Licenses
+
+If You choose to distribute Source Code Form that is Incompatible With
+Secondary Licenses under the terms of this version of the License, the
+notice described in Exhibit B of this License must be attached.
+
+Exhibit A - Source Code Form License Notice
+-------------------------------------------
+
+  This Source Code Form is subject to the terms of the Mozilla Public
+  License, v. 2.0. If a copy of the MPL was not distributed with this
+  file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+If it is not possible or desirable to put the notice in a particular
+file, then You may include the notice in a location (such as a LICENSE
+file in a relevant directory) where a recipient would be likely to look
+for such a notice.
+
+You may add additional accurate notices of copyright ownership.
+
+Exhibit B - "Incompatible With Secondary Licenses" Notice
+---------------------------------------------------------
+
+  This Source Code Form is "Incompatible With Secondary Licenses", as
+  defined by the Mozilla Public License, v. 2.0.
diff --git a/plugins/discovery-gce/licenses/httpclient-NOTICE.txt b/plugins/discovery-gce/licenses/httpclient-NOTICE.txt
new file mode 100644
index 0000000..4f60581
--- /dev/null
+++ b/plugins/discovery-gce/licenses/httpclient-NOTICE.txt
@@ -0,0 +1,5 @@
+Apache HttpComponents Client
+Copyright 1999-2015 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/discovery-gce/licenses/httpcore-4.3.3.jar.sha1 b/plugins/discovery-gce/licenses/httpcore-4.3.3.jar.sha1
new file mode 100644
index 0000000..5d9c0e2
--- /dev/null
+++ b/plugins/discovery-gce/licenses/httpcore-4.3.3.jar.sha1
@@ -0,0 +1 @@
+f91b7a4aadc5cf486df6e4634748d7dd7a73f06d
diff --git a/plugins/discovery-gce/licenses/httpcore-LICENSE.txt b/plugins/discovery-gce/licenses/httpcore-LICENSE.txt
new file mode 100644
index 0000000..72819a9
--- /dev/null
+++ b/plugins/discovery-gce/licenses/httpcore-LICENSE.txt
@@ -0,0 +1,241 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+=========================================================================
+
+This project contains annotations in the package org.apache.http.annotation
+which are derived from JCIP-ANNOTATIONS
+Copyright (c) 2005 Brian Goetz and Tim Peierls.
+See http://www.jcip.net and the Creative Commons Attribution License
+(http://creativecommons.org/licenses/by/2.5)
+Full text: http://creativecommons.org/licenses/by/2.5/legalcode
+
+License
+
+THE WORK (AS DEFINED BELOW) IS PROVIDED UNDER THE TERMS OF THIS CREATIVE COMMONS PUBLIC LICENSE ("CCPL" OR "LICENSE"). THE WORK IS PROTECTED BY COPYRIGHT AND/OR OTHER APPLICABLE LAW. ANY USE OF THE WORK OTHER THAN AS AUTHORIZED UNDER THIS LICENSE OR COPYRIGHT LAW IS PROHIBITED.
+
+BY EXERCISING ANY RIGHTS TO THE WORK PROVIDED HERE, YOU ACCEPT AND AGREE TO BE BOUND BY THE TERMS OF THIS LICENSE. THE LICENSOR GRANTS YOU THE RIGHTS CONTAINED HERE IN CONSIDERATION OF YOUR ACCEPTANCE OF SUCH TERMS AND CONDITIONS.
+
+1. Definitions
+
+    "Collective Work" means a work, such as a periodical issue, anthology or encyclopedia, in which the Work in its entirety in unmodified form, along with a number of other contributions, constituting separate and independent works in themselves, are assembled into a collective whole. A work that constitutes a Collective Work will not be considered a Derivative Work (as defined below) for the purposes of this License.
+    "Derivative Work" means a work based upon the Work or upon the Work and other pre-existing works, such as a translation, musical arrangement, dramatization, fictionalization, motion picture version, sound recording, art reproduction, abridgment, condensation, or any other form in which the Work may be recast, transformed, or adapted, except that a work that constitutes a Collective Work will not be considered a Derivative Work for the purpose of this License. For the avoidance of doubt, where the Work is a musical composition or sound recording, the synchronization of the Work in timed-relation with a moving image ("synching") will be considered a Derivative Work for the purpose of this License.
+    "Licensor" means the individual or entity that offers the Work under the terms of this License.
+    "Original Author" means the individual or entity who created the Work.
+    "Work" means the copyrightable work of authorship offered under the terms of this License.
+    "You" means an individual or entity exercising rights under this License who has not previously violated the terms of this License with respect to the Work, or who has received express permission from the Licensor to exercise rights under this License despite a previous violation.
+
+2. Fair Use Rights. Nothing in this license is intended to reduce, limit, or restrict any rights arising from fair use, first sale or other limitations on the exclusive rights of the copyright owner under copyright law or other applicable laws.
+
+3. License Grant. Subject to the terms and conditions of this License, Licensor hereby grants You a worldwide, royalty-free, non-exclusive, perpetual (for the duration of the applicable copyright) license to exercise the rights in the Work as stated below:
+
+    to reproduce the Work, to incorporate the Work into one or more Collective Works, and to reproduce the Work as incorporated in the Collective Works;
+    to create and reproduce Derivative Works;
+    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission the Work including as incorporated in Collective Works;
+    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission Derivative Works.
+
+    For the avoidance of doubt, where the work is a musical composition:
+        Performance Royalties Under Blanket Licenses. Licensor waives the exclusive right to collect, whether individually or via a performance rights society (e.g. ASCAP, BMI, SESAC), royalties for the public performance or public digital performance (e.g. webcast) of the Work.
+        Mechanical Rights and Statutory Royalties. Licensor waives the exclusive right to collect, whether individually or via a music rights agency or designated agent (e.g. Harry Fox Agency), royalties for any phonorecord You create from the Work ("cover version") and distribute, subject to the compulsory license created by 17 USC Section 115 of the US Copyright Act (or the equivalent in other jurisdictions).
+    Webcasting Rights and Statutory Royalties. For the avoidance of doubt, where the Work is a sound recording, Licensor waives the exclusive right to collect, whether individually or via a performance-rights society (e.g. SoundExchange), royalties for the public digital performance (e.g. webcast) of the Work, subject to the compulsory license created by 17 USC Section 114 of the US Copyright Act (or the equivalent in other jurisdictions).
+
+The above rights may be exercised in all media and formats whether now known or hereafter devised. The above rights include the right to make such modifications as are technically necessary to exercise the rights in other media and formats. All rights not expressly granted by Licensor are hereby reserved.
+
+4. Restrictions.The license granted in Section 3 above is expressly made subject to and limited by the following restrictions:
+
+    You may distribute, publicly display, publicly perform, or publicly digitally perform the Work only under the terms of this License, and You must include a copy of, or the Uniform Resource Identifier for, this License with every copy or phonorecord of the Work You distribute, publicly display, publicly perform, or publicly digitally perform. You may not offer or impose any terms on the Work that alter or restrict the terms of this License or the recipients' exercise of the rights granted hereunder. You may not sublicense the Work. You must keep intact all notices that refer to this License and to the disclaimer of warranties. You may not distribute, publicly display, publicly perform, or publicly digitally perform the Work with any technological measures that control access or use of the Work in a manner inconsistent with the terms of this License Agreement. The above applies to the Work as incorporated in a Collective Work, but this does not require the Collective Work apart from the Work itself to be made subject to the terms of this License. If You create a Collective Work, upon notice from any Licensor You must, to the extent practicable, remove from the Collective Work any credit as required by clause 4(b), as requested. If You create a Derivative Work, upon notice from any Licensor You must, to the extent practicable, remove from the Derivative Work any credit as required by clause 4(b), as requested.
+    If you distribute, publicly display, publicly perform, or publicly digitally perform the Work or any Derivative Works or Collective Works, You must keep intact all copyright notices for the Work and provide, reasonable to the medium or means You are utilizing: (i) the name of the Original Author (or pseudonym, if applicable) if supplied, and/or (ii) if the Original Author and/or Licensor designate another party or parties (e.g. a sponsor institute, publishing entity, journal) for attribution in Licensor's copyright notice, terms of service or by other reasonable means, the name of such party or parties; the title of the Work if supplied; to the extent reasonably practicable, the Uniform Resource Identifier, if any, that Licensor specifies to be associated with the Work, unless such URI does not refer to the copyright notice or licensing information for the Work; and in the case of a Derivative Work, a credit identifying the use of the Work in the Derivative Work (e.g., "French translation of the Work by Original Author," or "Screenplay based on original Work by Original Author"). Such credit may be implemented in any reasonable manner; provided, however, that in the case of a Derivative Work or Collective Work, at a minimum such credit will appear where any other comparable authorship credit appears and in a manner at least as prominent as such other comparable authorship credit.
+
+5. Representations, Warranties and Disclaimer
+
+UNLESS OTHERWISE MUTUALLY AGREED TO BY THE PARTIES IN WRITING, LICENSOR OFFERS THE WORK AS-IS AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE WORK, EXPRESS, IMPLIED, STATUTORY OR OTHERWISE, INCLUDING, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTIBILITY, FITNESS FOR A PARTICULAR PURPOSE, NONINFRINGEMENT, OR THE ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OF ABSENCE OF ERRORS, WHETHER OR NOT DISCOVERABLE. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES, SO SUCH EXCLUSION MAY NOT APPLY TO YOU.
+
+6. Limitation on Liability. EXCEPT TO THE EXTENT REQUIRED BY APPLICABLE LAW, IN NO EVENT WILL LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY FOR ANY SPECIAL, INCIDENTAL, CONSEQUENTIAL, PUNITIVE OR EXEMPLARY DAMAGES ARISING OUT OF THIS LICENSE OR THE USE OF THE WORK, EVEN IF LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+
+7. Termination
+
+    This License and the rights granted hereunder will terminate automatically upon any breach by You of the terms of this License. Individuals or entities who have received Derivative Works or Collective Works from You under this License, however, will not have their licenses terminated provided such individuals or entities remain in full compliance with those licenses. Sections 1, 2, 5, 6, 7, and 8 will survive any termination of this License.
+    Subject to the above terms and conditions, the license granted here is perpetual (for the duration of the applicable copyright in the Work). Notwithstanding the above, Licensor reserves the right to release the Work under different license terms or to stop distributing the Work at any time; provided, however that any such election will not serve to withdraw this License (or any other license that has been, or is required to be, granted under the terms of this License), and this License will continue in full force and effect unless terminated as stated above.
+
+8. Miscellaneous
+
+    Each time You distribute or publicly digitally perform the Work or a Collective Work, the Licensor offers to the recipient a license to the Work on the same terms and conditions as the license granted to You under this License.
+    Each time You distribute or publicly digitally perform a Derivative Work, Licensor offers to the recipient a license to the original Work on the same terms and conditions as the license granted to You under this License.
+    If any provision of this License is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this License, and without further action by the parties to this agreement, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable.
+    No term or provision of this License shall be deemed waived and no breach consented to unless such waiver or consent shall be in writing and signed by the party to be charged with such waiver or consent.
+    This License constitutes the entire agreement between the parties with respect to the Work licensed here. There are no understandings, agreements or representations with respect to the Work not specified here. Licensor shall not be bound by any additional provisions that may appear in any communication from You. This License may not be modified without the mutual written agreement of the Licensor and You.
diff --git a/plugins/discovery-gce/licenses/httpcore-NOTICE.txt b/plugins/discovery-gce/licenses/httpcore-NOTICE.txt
new file mode 100644
index 0000000..c0be50a
--- /dev/null
+++ b/plugins/discovery-gce/licenses/httpcore-NOTICE.txt
@@ -0,0 +1,8 @@
+Apache HttpComponents Core
+Copyright 2005-2014 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
+
+This project contains annotations derived from JCIP-ANNOTATIONS
+Copyright (c) 2005 Brian Goetz and Tim Peierls. See http://www.jcip.net
diff --git a/plugins/discovery-gce/licenses/jsr305-1.3.9.jar.sha1 b/plugins/discovery-gce/licenses/jsr305-1.3.9.jar.sha1
new file mode 100644
index 0000000..c04a429
--- /dev/null
+++ b/plugins/discovery-gce/licenses/jsr305-1.3.9.jar.sha1
@@ -0,0 +1 @@
+40719ea6961c0cb6afaeb6a921eaa1f6afd4cfdf
diff --git a/plugins/discovery-gce/licenses/jsr305-LICENSE.txt b/plugins/discovery-gce/licenses/jsr305-LICENSE.txt
new file mode 100644
index 0000000..0cb8710
--- /dev/null
+++ b/plugins/discovery-gce/licenses/jsr305-LICENSE.txt
@@ -0,0 +1,29 @@
+Copyright (c) 2007-2009, JSR305 expert group
+All rights reserved.
+
+http://www.opensource.org/licenses/bsd-license.php
+
+Redistribution and use in source and binary forms, with or without 
+modification, are permitted provided that the following conditions are met:
+
+    * Redistributions of source code must retain the above copyright notice, 
+      this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright notice, 
+      this list of conditions and the following disclaimer in the documentation 
+      and/or other materials provided with the distribution.
+    * Neither the name of the JSR305 expert group nor the names of its 
+      contributors may be used to endorse or promote products derived from 
+      this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" 
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, 
+THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE 
+ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE 
+LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR 
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF 
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS 
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN 
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) 
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE 
+POSSIBILITY OF SUCH DAMAGE.
+
diff --git a/plugins/discovery-gce/licenses/jsr305-NOTICE.txt b/plugins/discovery-gce/licenses/jsr305-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/discovery-gce/licenses/jsr305-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/discovery-gce/pom.xml b/plugins/discovery-gce/pom.xml
new file mode 100644
index 0000000..b7c1c0a
--- /dev/null
+++ b/plugins/discovery-gce/pom.xml
@@ -0,0 +1,68 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!-- Licensed to Elasticsearch under one or more contributor
+license agreements. See the NOTICE file distributed with this work for additional
+information regarding copyright ownership. Elasticsearch licenses this file to you
+under the Apache License, Version 2.0 (the "License"); you may not use this
+file except in compliance with the License. You may obtain a copy of the
+License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by
+applicable law or agreed to in writing, software distributed under the License
+is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied. See the License for the specific language
+governing permissions and limitations under the License. -->
+
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.elasticsearch.plugin</groupId>
+        <artifactId>plugins</artifactId>
+        <version>3.0.0-SNAPSHOT</version>
+    </parent>
+
+    <artifactId>discovery-gce</artifactId>
+    <name>Plugin: Discovery: Google Compute Engine</name>
+    <description>The Google Compute Engine (GCE) Discovery plugin allows to use GCE API for the unicast discovery mechanism.</description>
+
+    <properties>
+        <elasticsearch.plugin.classname>org.elasticsearch.plugin.discovery.gce.GceDiscoveryPlugin</elasticsearch.plugin.classname>
+        <google.gce.version>v1-rev71-1.20.0</google.gce.version>
+        <!-- currently has no unit tests -->
+        <tests.rest.suite>discovery_gce</tests.rest.suite>
+        <tests.rest.load_packaged>false</tests.rest.load_packaged>
+        <xlint.options>-Xlint:-rawtypes,-unchecked</xlint.options>
+    </properties>
+
+    <dependencies>
+        <!-- Google APIs -->
+        <dependency>
+          <groupId>com.google.apis</groupId>
+          <artifactId>google-api-services-compute</artifactId>
+          <version>${google.gce.version}</version>
+          <exclusions>
+              <exclusion>
+                 <groupId>com.google.guava</groupId>
+                 <artifactId>guava-jdk5</artifactId>
+              </exclusion>
+          </exclusions>
+        </dependency>
+        <!-- We need to force here the compile scope as it was defined as test scope in plugins/pom.xml -->
+        <!-- TODO: remove this dependency when we will have a REST Test module -->
+        <dependency>
+            <groupId>org.apache.httpcomponents</groupId>
+            <artifactId>httpclient</artifactId>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeService.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeService.java
new file mode 100644
index 0000000..c7f4598
--- /dev/null
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeService.java
@@ -0,0 +1,57 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.gce;
+
+import com.google.api.services.compute.model.Instance;
+import org.elasticsearch.common.component.LifecycleComponent;
+
+import java.io.IOException;
+import java.util.Collection;
+
+public interface GceComputeService extends LifecycleComponent<GceComputeService> {
+    final class Fields {
+        public static final String PROJECT = "cloud.gce.project_id";
+        public static final String ZONE = "cloud.gce.zone";
+        public static final String REFRESH = "cloud.gce.refresh_interval";
+        public static final String TAGS = "discovery.gce.tags";
+        public static final String VERSION = "Elasticsearch/GceCloud/1.0";
+    }
+
+    /**
+     * Return a collection of running instances within the same GCE project
+     * @return a collection of running instances within the same GCE project
+     */
+    Collection<Instance> instances();
+
+    /**
+     * <p>Gets metadata on the current running machine (call to
+     * http://metadata.google.internal/computeMetadata/v1/instance/xxx).</p>
+     * <p>For example, you can retrieve network information by replacing xxx with:</p>
+     * <ul>
+     *     <li>`hostname` when we need to resolve the host name</li>
+     *     <li>`network-interfaces/0/ip` when we need to resolve private IP</li>
+     * </ul>
+     * @see org.elasticsearch.cloud.gce.network.GceNameResolver for bindings
+     * @param metadataPath path to metadata information
+     * @return extracted information (for example a hostname or an IP address)
+     * @throws IOException in case metadata URL is not accessible
+     */
+    String metadata(String metadataPath) throws IOException;
+}
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java
new file mode 100644
index 0000000..a29c21e
--- /dev/null
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java
@@ -0,0 +1,228 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.gce;
+
+import com.google.api.client.googleapis.compute.ComputeCredential;
+import com.google.api.client.googleapis.javanet.GoogleNetHttpTransport;
+import com.google.api.client.http.GenericUrl;
+import com.google.api.client.http.HttpHeaders;
+import com.google.api.client.http.HttpResponse;
+import com.google.api.client.http.HttpTransport;
+import com.google.api.client.json.JsonFactory;
+import com.google.api.client.json.jackson2.JacksonFactory;
+import com.google.api.services.compute.Compute;
+import com.google.api.services.compute.model.Instance;
+import com.google.api.services.compute.model.InstanceList;
+
+import org.elasticsearch.SpecialPermission;
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.cloud.gce.network.GceNameResolver;
+import org.elasticsearch.common.component.AbstractLifecycleComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.TimeValue;
+
+import java.io.IOException;
+import java.net.URL;
+import java.security.AccessController;
+import java.security.GeneralSecurityException;
+import java.security.PrivilegedActionException;
+import java.security.PrivilegedExceptionAction;
+import java.util.*;
+
+public class GceComputeServiceImpl extends AbstractLifecycleComponent<GceComputeService>
+    implements GceComputeService {
+
+    private final String project;
+    private final List<String> zones;
+
+    // Forcing Google Token API URL as set in GCE SDK to
+    //      http://metadata/computeMetadata/v1/instance/service-accounts/default/token
+    // See https://developers.google.com/compute/docs/metadata#metadataserver
+    public static final String GCE_METADATA_URL = "http://metadata.google.internal/computeMetadata/v1/instance";
+    public static final String TOKEN_SERVER_ENCODED_URL = GCE_METADATA_URL + "/service-accounts/default/token";
+
+    @Override
+    public Collection<Instance> instances() {
+        logger.debug("get instances for project [{}], zones [{}]", project, zones);
+        final List<Instance> instances = zones.stream().map((zoneId) -> {
+            try {
+                // hack around code messiness in GCE code
+                // TODO: get this fixed
+                SecurityManager sm = System.getSecurityManager();
+                if (sm != null) {
+                    sm.checkPermission(new SpecialPermission());
+                }
+                InstanceList instanceList = AccessController.doPrivileged(new PrivilegedExceptionAction<InstanceList>() {
+                    @Override
+                    public InstanceList run() throws Exception {
+                        Compute.Instances.List list = client().instances().list(project, zoneId);
+                        return list.execute();
+                    }
+                });
+                if (instanceList.isEmpty()) {
+                    return Collections.EMPTY_LIST;
+                }
+                return instanceList.getItems();
+            } catch (PrivilegedActionException e) {
+                logger.warn("Problem fetching instance list for zone {}", zoneId);
+                logger.debug("Full exception:", e);
+                return Collections.EMPTY_LIST;
+            }
+        }).reduce(new ArrayList<>(), (a, b) -> {
+            a.addAll(b);
+            return a;
+        });
+
+        if (instances.isEmpty()) {
+            logger.warn("disabling GCE discovery. Can not get list of nodes");
+        }
+
+        return instances;
+    }
+
+    @Override
+    public String metadata(String metadataPath) throws IOException {
+        String urlMetadataNetwork = GCE_METADATA_URL + "/" + metadataPath;
+        logger.debug("get metadata from [{}]", urlMetadataNetwork);
+        URL url = new URL(urlMetadataNetwork);
+        HttpHeaders headers;
+        try {
+            // hack around code messiness in GCE code
+            // TODO: get this fixed
+            headers = AccessController.doPrivileged(new PrivilegedExceptionAction<HttpHeaders>() {
+                @Override
+                public HttpHeaders run() throws IOException {
+                    return new HttpHeaders();
+                }
+            });
+
+            // This is needed to query meta data: https://cloud.google.com/compute/docs/metadata
+            headers.put("Metadata-Flavor", "Google");
+            HttpResponse response;
+            response = getGceHttpTransport().createRequestFactory()
+                    .buildGetRequest(new GenericUrl(url))
+                    .setHeaders(headers)
+                    .execute();
+            String metadata = response.parseAsString();
+            logger.debug("metadata found [{}]", metadata);
+            return metadata;
+        } catch (Exception e) {
+            throw new IOException("failed to fetch metadata from [" + urlMetadataNetwork + "]", e);
+        }
+    }
+
+    private Compute client;
+    private TimeValue refreshInterval = null;
+    private long lastRefresh;
+
+    /** Global instance of the HTTP transport. */
+    private HttpTransport gceHttpTransport;
+
+    /** Global instance of the JSON factory. */
+    private JsonFactory gceJsonFactory;
+
+    @Inject
+    public GceComputeServiceImpl(Settings settings, NetworkService networkService) {
+        super(settings);
+        this.project = settings.get(Fields.PROJECT);
+        String[] zoneList = settings.getAsArray(Fields.ZONE);
+        this.zones = Arrays.asList(zoneList);
+        networkService.addCustomNameResolver(new GceNameResolver(settings, this));
+    }
+
+    protected synchronized HttpTransport getGceHttpTransport() throws GeneralSecurityException, IOException {
+        if (gceHttpTransport == null) {
+            gceHttpTransport = GoogleNetHttpTransport.newTrustedTransport();
+        }
+        return gceHttpTransport;
+    }
+
+    public synchronized Compute client() {
+        if (refreshInterval != null && refreshInterval.millis() != 0) {
+            if (client != null &&
+                    (refreshInterval.millis() < 0 || (System.currentTimeMillis() - lastRefresh) < refreshInterval.millis())) {
+                if (logger.isTraceEnabled()) logger.trace("using cache to retrieve client");
+                return client;
+            }
+            lastRefresh = System.currentTimeMillis();
+        }
+
+        try {
+            gceJsonFactory = new JacksonFactory();
+
+            logger.info("starting GCE discovery service");
+            ComputeCredential credential = new ComputeCredential.Builder(getGceHttpTransport(), gceJsonFactory)
+                        .setTokenServerEncodedUrl(TOKEN_SERVER_ENCODED_URL)
+                    .build();
+
+            // hack around code messiness in GCE code
+            // TODO: get this fixed
+            SecurityManager sm = System.getSecurityManager();
+            if (sm != null) {
+                sm.checkPermission(new SpecialPermission());
+            }
+            AccessController.doPrivileged(new PrivilegedExceptionAction<Void>() {
+                @Override
+                public Void run() throws IOException {
+                    credential.refreshToken();
+                    return null;
+                }
+            });
+
+            logger.debug("token [{}] will expire in [{}] s", credential.getAccessToken(), credential.getExpiresInSeconds());
+            if (credential.getExpiresInSeconds() != null) {
+                refreshInterval = TimeValue.timeValueSeconds(credential.getExpiresInSeconds()-1);
+            }
+
+            // Once done, let's use this token
+            this.client = new Compute.Builder(getGceHttpTransport(), gceJsonFactory, null)
+                    .setApplicationName(Fields.VERSION)
+                    .setHttpRequestInitializer(credential)
+                    .build();
+        } catch (Exception e) {
+            logger.warn("unable to start GCE discovery service", e);
+            throw new IllegalArgumentException("unable to start GCE discovery service", e);
+        }
+
+        return this.client;
+    }
+
+    @Override
+    protected void doStart() throws ElasticsearchException {
+    }
+
+    @Override
+    protected void doStop() throws ElasticsearchException {
+        if (gceHttpTransport != null) {
+            try {
+                gceHttpTransport.shutdown();
+            } catch (IOException e) {
+                logger.warn("unable to shutdown GCE Http Transport", e);
+            }
+            gceHttpTransport = null;
+        }
+    }
+
+    @Override
+    protected void doClose() throws ElasticsearchException {
+    }
+}
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceModule.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceModule.java
new file mode 100644
index 0000000..e1b8d6c
--- /dev/null
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceModule.java
@@ -0,0 +1,36 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.gce;
+
+import org.elasticsearch.common.inject.AbstractModule;
+
+public class GceModule extends AbstractModule {
+    // pkg private so tests can override with mock
+    static Class<? extends GceComputeService> computeServiceImpl = GceComputeServiceImpl.class;
+
+    public static Class<? extends GceComputeService> getComputeServiceImpl() {
+        return computeServiceImpl;
+    }
+
+    @Override
+    protected void configure() {
+        bind(GceComputeService.class).to(computeServiceImpl).asEagerSingleton();
+    }
+}
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/network/GceNameResolver.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/network/GceNameResolver.java
new file mode 100644
index 0000000..22d79fb
--- /dev/null
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/network/GceNameResolver.java
@@ -0,0 +1,132 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.gce.network;
+
+import org.elasticsearch.cloud.gce.GceComputeService;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.network.NetworkService.CustomNameResolver;
+import org.elasticsearch.common.settings.Settings;
+
+import java.io.IOException;
+import java.net.InetAddress;
+
+/**
+ * <p>Resolves certain GCE related 'meta' hostnames into an actual hostname
+ * obtained from gce meta-data.</p>
+ * Valid config values for {@link GceAddressResolverType}s are -
+ * <ul>
+ * <li>_gce_ - maps to privateIp</li>
+ * <li>_gce:privateIp_</li>
+ * <li>_gce:hostname_</li>
+ * </ul>
+ */
+public class GceNameResolver extends AbstractComponent implements CustomNameResolver {
+
+    private final GceComputeService gceComputeService;
+
+    /**
+     * enum that can be added to over time with more meta-data types
+     */
+    private enum GceAddressResolverType {
+
+        /**
+         * Using the hostname
+         */
+        PRIVATE_DNS("gce:hostname", "hostname"),
+        /**
+         * Can be gce:privateIp, gce:privateIp:X where X is the network interface
+         */
+        PRIVATE_IP("gce:privateIp", "network-interfaces/{{network}}/ip"),
+        /**
+         * same as "gce:privateIp" or "gce:privateIp:0"
+         */
+        GCE("gce", PRIVATE_IP.gceName);
+
+        final String configName;
+        final String gceName;
+
+        GceAddressResolverType(String configName, String gceName) {
+            this.configName = configName;
+            this.gceName = gceName;
+        }
+    }
+
+    /**
+     * Construct a {@link CustomNameResolver}.
+     */
+    public GceNameResolver(Settings settings, GceComputeService gceComputeService) {
+        super(settings);
+        this.gceComputeService = gceComputeService;
+    }
+
+    /**
+     * @param value the gce hostname type to discover.
+     * @return the appropriate host resolved from gce meta-data.
+     * @see CustomNameResolver#resolveIfPossible(String)
+     */
+    private InetAddress[] resolve(String value) throws IOException {
+        String gceMetadataPath;
+        if (value.equals(GceAddressResolverType.GCE.configName)) {
+            // We replace network placeholder with default network interface value: 0
+            gceMetadataPath = Strings.replace(GceAddressResolverType.GCE.gceName, "{{network}}", "0");
+        } else if (value.equals(GceAddressResolverType.PRIVATE_DNS.configName)) {
+            gceMetadataPath = GceAddressResolverType.PRIVATE_DNS.gceName;
+        } else if (value.startsWith(GceAddressResolverType.PRIVATE_IP.configName)) {
+            // We extract the network interface from gce:privateIp:XX
+            String network = "0";
+            String[] privateIpConfig = Strings.splitStringToArray(value, ':');
+            if (privateIpConfig != null && privateIpConfig.length == 3) {
+                network = privateIpConfig[2];
+            }
+
+            // We replace network placeholder with network interface value
+            gceMetadataPath = Strings.replace(GceAddressResolverType.PRIVATE_IP.gceName, "{{network}}", network);
+        } else {
+            throw new IllegalArgumentException("[" + value + "] is not one of the supported GCE network.host setting. " +
+                    "Expecting _gce_, _gce:privateIp:X_, _gce:hostname_");
+        }
+
+        try {
+            String metadataResult = gceComputeService.metadata(gceMetadataPath);
+            if (metadataResult == null || metadataResult.length() == 0) {
+                throw new IOException("no gce metadata returned from [" + gceMetadataPath + "] for [" + value + "]");
+            }
+            // only one address: because we explicitly ask for only one via the GceHostnameType
+            return new InetAddress[] { InetAddress.getByName(metadataResult) };
+        } catch (IOException e) {
+            throw new IOException("IOException caught when fetching InetAddress from [" + gceMetadataPath + "]", e);
+        }
+    }
+
+    @Override
+    public InetAddress[] resolveDefault() {
+        return null; // using this, one has to explicitly specify _gce_ in network setting
+    }
+
+    @Override
+    public InetAddress[] resolveIfPossible(String value) throws IOException {
+        // We only try to resolve network.host setting when it starts with _gce
+        if (value.startsWith("gce")) {
+            return resolve(value);
+        }
+        return null;
+    }
+}
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
new file mode 100755
index 0000000..f20d1c7
--- /dev/null
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.gce;
+
+import org.elasticsearch.cluster.ClusterName;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.DiscoverySettings;
+import org.elasticsearch.discovery.zen.ZenDiscovery;
+import org.elasticsearch.discovery.zen.elect.ElectMasterService;
+import org.elasticsearch.discovery.zen.ping.ZenPingService;
+import org.elasticsearch.node.settings.NodeSettingsService;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.TransportService;
+
+/**
+ *
+ */
+public class GceDiscovery extends ZenDiscovery {
+
+    public static final String GCE = "gce";
+
+    @Inject
+    public GceDiscovery(Settings settings, ClusterName clusterName, ThreadPool threadPool, TransportService transportService,
+                        ClusterService clusterService, NodeSettingsService nodeSettingsService, ZenPingService pingService,
+                        DiscoverySettings discoverySettings,
+                        ElectMasterService electMasterService) {
+        super(settings, clusterName, threadPool, transportService, clusterService, nodeSettingsService,
+                pingService, electMasterService, discoverySettings);
+    }
+}
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java
new file mode 100644
index 0000000..8feb9b8
--- /dev/null
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java
@@ -0,0 +1,249 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.gce;
+
+import com.google.api.services.compute.model.AccessConfig;
+import com.google.api.services.compute.model.Instance;
+import com.google.api.services.compute.model.NetworkInterface;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.cloud.gce.GceComputeService;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.network.NetworkAddress;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.transport.TransportAddress;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.discovery.zen.ping.unicast.UnicastHostsProvider;
+import org.elasticsearch.transport.TransportService;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
+
+import static org.elasticsearch.cloud.gce.GceComputeService.Fields;
+
+/**
+ *
+ */
+public class GceUnicastHostsProvider extends AbstractComponent implements UnicastHostsProvider {
+
+    static final class Status {
+        private static final String TERMINATED = "TERMINATED";
+    }
+
+    private final GceComputeService gceComputeService;
+    private TransportService transportService;
+    private NetworkService networkService;
+
+    private final Version version;
+    private final String project;
+    private final String[] zones;
+    private final String[] tags;
+
+    private final TimeValue refreshInterval;
+    private long lastRefresh;
+    private List<DiscoveryNode> cachedDiscoNodes;
+
+    @Inject
+    public GceUnicastHostsProvider(Settings settings, GceComputeService gceComputeService,
+            TransportService transportService,
+            NetworkService networkService,
+            Version version) {
+        super(settings);
+        this.gceComputeService = gceComputeService;
+        this.transportService = transportService;
+        this.networkService = networkService;
+        this.version = version;
+
+        this.refreshInterval = settings.getAsTime(Fields.REFRESH, TimeValue.timeValueSeconds(0));
+        this.project = settings.get(Fields.PROJECT);
+        this.zones = settings.getAsArray(Fields.ZONE);
+
+        this.tags = settings.getAsArray(Fields.TAGS);
+        if (logger.isDebugEnabled()) {
+            logger.debug("using tags {}", Arrays.asList(this.tags));
+        }
+    }
+
+    /**
+     * We build the list of Nodes from GCE Management API
+     * Information can be cached using `plugins.refresh_interval` property if needed.
+     * Setting `plugins.refresh_interval` to `-1` will cause infinite caching.
+     * Setting `plugins.refresh_interval` to `0` will disable caching (default).
+     */
+    @Override
+    public List<DiscoveryNode> buildDynamicNodes() {
+        if (refreshInterval.millis() != 0) {
+            if (cachedDiscoNodes != null &&
+                    (refreshInterval.millis() < 0 || (System.currentTimeMillis() - lastRefresh) < refreshInterval.millis())) {
+                if (logger.isTraceEnabled()) logger.trace("using cache to retrieve node list");
+                return cachedDiscoNodes;
+            }
+            lastRefresh = System.currentTimeMillis();
+        }
+        logger.debug("start building nodes list using GCE API");
+
+        cachedDiscoNodes = new ArrayList<>();
+        String ipAddress = null;
+        try {
+            InetAddress inetAddress = networkService.resolvePublishHostAddress(null);
+            if (inetAddress != null) {
+                ipAddress = NetworkAddress.formatAddress(inetAddress);
+            }
+        } catch (IOException e) {
+            // We can't find the publish host address... Hmmm. Too bad :-(
+            // We won't simply filter it
+        }
+
+        try {
+            Collection<Instance> instances = gceComputeService.instances();
+
+            if (instances == null) {
+                logger.trace("no instance found for project [{}], zones [{}].", this.project, this.zones);
+                return cachedDiscoNodes;
+            }
+
+            for (Instance instance : instances) {
+                String name = instance.getName();
+                String type = instance.getMachineType();
+
+                String status = instance.getStatus();
+                logger.trace("gce instance {} with status {} found.", name, status);
+
+                // We don't want to connect to TERMINATED status instances
+                // See https://github.com/elasticsearch/elasticsearch-cloud-gce/issues/3
+                if (Status.TERMINATED.equals(status)) {
+                    logger.debug("node {} is TERMINATED. Ignoring", name);
+                    continue;
+                }
+
+                // see if we need to filter by tag
+                boolean filterByTag = false;
+                if (tags.length > 0) {
+                    logger.trace("start filtering instance {} with tags {}.", name, tags);
+                    if (instance.getTags() == null || instance.getTags().isEmpty()
+                            || instance.getTags().getItems() == null || instance.getTags().getItems().isEmpty()) {
+                        // If this instance have no tag, we filter it
+                        logger.trace("no tags for this instance but we asked for tags. {} won't be part of the cluster.", name);
+                        filterByTag = true;
+                    } else {
+                        // check that all tags listed are there on the instance
+                        logger.trace("comparing instance tags {} with tags filter {}.", instance.getTags().getItems(), tags);
+                        for (String tag : tags) {
+                            boolean found = false;
+                            for (String instancetag : instance.getTags().getItems()) {
+                                if (instancetag.equals(tag)) {
+                                    found = true;
+                                    break;
+                                }
+                            }
+                            if (!found) {
+                                filterByTag = true;
+                                break;
+                            }
+                        }
+                    }
+                }
+                if (filterByTag) {
+                    logger.trace("filtering out instance {} based tags {}, not part of {}", name, tags,
+                            instance.getTags() == null || instance.getTags().getItems() == null ? "" : instance.getTags());
+                    continue;
+                } else {
+                    logger.trace("instance {} with tags {} is added to discovery", name, tags);
+                }
+
+                String ip_public = null;
+                String ip_private = null;
+
+                List<NetworkInterface> interfaces = instance.getNetworkInterfaces();
+
+                for (NetworkInterface networkInterface : interfaces) {
+                    if (ip_public == null) {
+                        // Trying to get Public IP Address (For future use)
+                        if (networkInterface.getAccessConfigs() != null) {
+                            for (AccessConfig accessConfig : networkInterface.getAccessConfigs()) {
+                                if (Strings.hasText(accessConfig.getNatIP())) {
+                                    ip_public = accessConfig.getNatIP();
+                                    break;
+                                }
+                            }
+                        }
+                    }
+
+                    if (ip_private == null) {
+                        ip_private = networkInterface.getNetworkIP();
+                    }
+
+                    // If we have both public and private, we can stop here
+                    if (ip_private != null && ip_public != null) break;
+                }
+
+                try {
+                    if (ip_private.equals(ipAddress)) {
+                        // We found the current node.
+                        // We can ignore it in the list of DiscoveryNode
+                        logger.trace("current node found. Ignoring {} - {}", name, ip_private);
+                    } else {
+                        String address = ip_private;
+                        // Test if we have es_port metadata defined here
+                        if (instance.getMetadata() != null && instance.getMetadata().containsKey("es_port")) {
+                            Object es_port = instance.getMetadata().get("es_port");
+                            logger.trace("es_port is defined with {}", es_port);
+                            if (es_port instanceof String) {
+                                address = address.concat(":").concat((String) es_port);
+                            } else {
+                                // Ignoring other values
+                                logger.trace("es_port is instance of {}. Ignoring...", es_port.getClass().getName());
+                            }
+                        }
+
+                        // ip_private is a single IP Address. We need to build a TransportAddress from it
+                        // If user has set `es_port` metadata, we don't need to ping all ports
+                        // we only limit to 1 addresses, makes no sense to ping 100 ports
+                        TransportAddress[] addresses = transportService.addressesFromString(address, 1);
+
+                        for (TransportAddress transportAddress : addresses) {
+                            logger.trace("adding {}, type {}, address {}, transport_address {}, status {}", name, type,
+                                    ip_private, transportAddress, status);
+                            cachedDiscoNodes.add(new DiscoveryNode("#cloud-" + name + "-" + 0, transportAddress, version.minimumCompatibilityVersion()));
+                        }
+                    }
+                } catch (Exception e) {
+                    logger.warn("failed to add {}, address {}", e, name, ip_private);
+                }
+
+            }
+        } catch (Throwable e) {
+            logger.warn("Exception caught during discovery: {}", e, e.getMessage());
+        }
+
+        logger.debug("{} node(s) added", cachedDiscoNodes.size());
+        logger.debug("using dynamic discovery nodes {}", cachedDiscoNodes);
+
+        return cachedDiscoNodes;
+    }
+}
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/plugin/discovery/gce/GceDiscoveryPlugin.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/plugin/discovery/gce/GceDiscoveryPlugin.java
new file mode 100644
index 0000000..a17c396
--- /dev/null
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/plugin/discovery/gce/GceDiscoveryPlugin.java
@@ -0,0 +1,125 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plugin.discovery.gce;
+
+import org.elasticsearch.cloud.gce.GceComputeService;
+import org.elasticsearch.cloud.gce.GceModule;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.component.LifecycleComponent;
+import org.elasticsearch.common.inject.Module;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.DiscoveryModule;
+import org.elasticsearch.discovery.gce.GceDiscovery;
+import org.elasticsearch.discovery.gce.GceUnicastHostsProvider;
+import org.elasticsearch.plugins.Plugin;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+
+public class GceDiscoveryPlugin extends Plugin {
+
+    private final Settings settings;
+    protected final ESLogger logger = Loggers.getLogger(GceDiscoveryPlugin.class);
+
+    public GceDiscoveryPlugin(Settings settings) {
+        this.settings = settings;
+    }
+
+    @Override
+    public String name() {
+        return "discovery-gce";
+    }
+
+    @Override
+    public String description() {
+        return "Cloud Google Compute Engine Discovery Plugin";
+    }
+
+    @Override
+    public Collection<Module> nodeModules() {
+        List<Module> modules = new ArrayList<>();
+        if (isDiscoveryAlive(settings, logger)) {
+            modules.add(new GceModule());
+        }
+        return modules;
+    }
+
+    @Override
+    public Collection<Class<? extends LifecycleComponent>> nodeServices() {
+        Collection<Class<? extends LifecycleComponent>> services = new ArrayList<>();
+        if (isDiscoveryAlive(settings, logger)) {
+            services.add(GceModule.getComputeServiceImpl());
+        }
+        return services;
+    }
+
+    public void onModule(DiscoveryModule discoveryModule) {
+        if (isDiscoveryAlive(settings, logger)) {
+            discoveryModule.addDiscoveryType("gce", GceDiscovery.class);
+            discoveryModule.addUnicastHostProvider(GceUnicastHostsProvider.class);
+        }
+    }
+
+    /**
+     * Check if discovery is meant to start
+     *
+     * @return true if we can start gce discovery features
+     */
+    public static boolean isDiscoveryAlive(Settings settings, ESLogger logger) {
+        // User set discovery.type: gce
+        if (GceDiscovery.GCE.equalsIgnoreCase(settings.get("discovery.type")) == false) {
+            logger.debug("discovery.type not set to {}", GceDiscovery.GCE);
+            return false;
+        }
+
+        if (checkProperty(GceComputeService.Fields.PROJECT, settings.get(GceComputeService.Fields.PROJECT), logger) == false ||
+                checkProperty(GceComputeService.Fields.ZONE, settings.getAsArray(GceComputeService.Fields.ZONE), logger) == false) {
+            logger.debug("one or more gce discovery settings are missing. " +
+                            "Check elasticsearch.yml file. Should have [{}] and [{}].",
+                    GceComputeService.Fields.PROJECT,
+                    GceComputeService.Fields.ZONE);
+            return false;
+        }
+
+        logger.trace("all required properties for gce discovery are set!");
+
+        return true;
+    }
+
+    private static boolean checkProperty(String name, String value, ESLogger logger) {
+        if (!Strings.hasText(value)) {
+            logger.warn("{} is not set.", name);
+            return false;
+        }
+        return true;
+    }
+
+    private static boolean checkProperty(String name, String[] values, ESLogger logger) {
+        if (values == null || values.length == 0) {
+            logger.warn("{} is not set.", name);
+            return false;
+        }
+        return true;
+    }
+
+}
diff --git a/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/DiscoveryGCERestIT.java b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/DiscoveryGCERestIT.java
new file mode 100644
index 0000000..1a21839
--- /dev/null
+++ b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/DiscoveryGCERestIT.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.gce;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.plugin.discovery.gce.GceDiscoveryPlugin;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+import java.util.Collection;
+
+public class DiscoveryGCERestIT extends ESRestTestCase {
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(GceDiscoveryPlugin.class);
+    }
+
+    public DiscoveryGCERestIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+}
+
diff --git a/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceComputeServiceMock.java b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceComputeServiceMock.java
new file mode 100644
index 0000000..dcbd53f
--- /dev/null
+++ b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceComputeServiceMock.java
@@ -0,0 +1,113 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.gce;
+
+import com.google.api.client.http.HttpTransport;
+import com.google.api.client.http.LowLevelHttpRequest;
+import com.google.api.client.http.LowLevelHttpResponse;
+import com.google.api.client.json.Json;
+import com.google.api.client.testing.http.MockHttpTransport;
+import com.google.api.client.testing.http.MockLowLevelHttpRequest;
+import com.google.api.client.testing.http.MockLowLevelHttpResponse;
+import org.elasticsearch.cloud.gce.GceComputeServiceImpl;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.io.Streams;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.util.Callback;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.net.URL;
+import java.security.GeneralSecurityException;
+
+/**
+ *
+ */
+public class GceComputeServiceMock extends GceComputeServiceImpl {
+
+    protected HttpTransport mockHttpTransport;
+
+    public GceComputeServiceMock(Settings settings, NetworkService networkService) {
+        super(settings, networkService);
+        this.mockHttpTransport = configureMock();
+    }
+
+    @Override
+    protected HttpTransport getGceHttpTransport() throws GeneralSecurityException, IOException {
+        return this.mockHttpTransport;
+    }
+
+    protected HttpTransport configureMock() {
+        return new MockHttpTransport() {
+            @Override
+            public LowLevelHttpRequest buildRequest(String method, final String url) throws IOException {
+                return new MockLowLevelHttpRequest() {
+                    @Override
+                    public LowLevelHttpResponse execute() throws IOException {
+                        MockLowLevelHttpResponse response = new MockLowLevelHttpResponse();
+                        response.setStatusCode(200);
+                        response.setContentType(Json.MEDIA_TYPE);
+                        if (url.startsWith(GCE_METADATA_URL)) {
+                            logger.info("--> Simulate GCE Auth/Metadata response for [{}]", url);
+                            response.setContent(readGoogleInternalJsonResponse(url));
+                        } else {
+                            logger.info("--> Simulate GCE API response for [{}]", url);
+                            response.setContent(readGoogleApiJsonResponse(url));
+                        }
+
+                        return response;
+                    }
+                };
+            }
+        };
+    }
+
+    private String readGoogleInternalJsonResponse(String url) throws IOException {
+        return readJsonResponse(url, "http://metadata.google.internal/");
+    }
+
+    private String readGoogleApiJsonResponse(String url) throws IOException {
+        return readJsonResponse(url, "https://www.googleapis.com/");
+    }
+
+    private String readJsonResponse(String url, String urlRoot) throws IOException {
+        // We extract from the url the mock file path we want to use
+        String mockFileName = Strings.replace(url, urlRoot, "");
+
+        logger.debug("--> read mock file from [{}]", mockFileName);
+        URL resource = GceComputeServiceMock.class.getResource(mockFileName);
+        if (resource == null) {
+            throw new IOException("can't read [" + url + "] in src/test/resources/org/elasticsearch/discovery/gce");
+        }
+        try (InputStream is = resource.openStream()) {
+            final StringBuilder sb = new StringBuilder();
+            Streams.readAllLines(is, new Callback<String>() {
+                @Override
+                public void handle(String s) {
+                    sb.append(s);
+                }
+            });
+            String response = sb.toString();
+            logger.trace("{}", response);
+            return response;
+        }
+    }
+}
diff --git a/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoverySettingsTests.java b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoverySettingsTests.java
new file mode 100644
index 0000000..334c685
--- /dev/null
+++ b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoverySettingsTests.java
@@ -0,0 +1,77 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.gce;
+
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.plugin.discovery.gce.GceDiscoveryPlugin;
+import org.elasticsearch.test.ESTestCase;
+
+import static org.hamcrest.Matchers.is;
+
+public class GceDiscoverySettingsTests extends ESTestCase {
+    public void testDiscoveryReady() {
+        Settings settings = Settings.builder()
+                .put("discovery.type", "gce")
+                .put("cloud.gce.project_id", "gce_id")
+                .putArray("cloud.gce.zone", "gce_zones_1", "gce_zones_2")
+                .build();
+
+        boolean discoveryReady = GceDiscoveryPlugin.isDiscoveryAlive(settings, logger);
+        assertThat(discoveryReady, is(true));
+    }
+
+    public void testDiscoveryNotReady() {
+        Settings settings = Settings.EMPTY;
+        boolean discoveryReady = GceDiscoveryPlugin.isDiscoveryAlive(settings, logger);
+        assertThat(discoveryReady, is(false));
+
+        settings = Settings.builder()
+                .put("discovery.type", "gce")
+                .build();
+
+        discoveryReady = GceDiscoveryPlugin.isDiscoveryAlive(settings, logger);
+        assertThat(discoveryReady, is(false));
+
+        settings = Settings.builder()
+                .put("discovery.type", "gce")
+                .put("cloud.gce.project_id", "gce_id")
+                .build();
+
+        discoveryReady = GceDiscoveryPlugin.isDiscoveryAlive(settings, logger);
+        assertThat(discoveryReady, is(false));
+
+
+        settings = Settings.builder()
+                .put("discovery.type", "gce")
+                .putArray("cloud.gce.zone", "gce_zones_1", "gce_zones_2")
+                .build();
+
+        discoveryReady = GceDiscoveryPlugin.isDiscoveryAlive(settings, logger);
+        assertThat(discoveryReady, is(false));
+
+        settings = Settings.builder()
+                .put("cloud.gce.project_id", "gce_id")
+                .putArray("cloud.gce.zone", "gce_zones_1", "gce_zones_2")
+                .build();
+
+        discoveryReady = GceDiscoveryPlugin.isDiscoveryAlive(settings, logger);
+        assertThat(discoveryReady, is(false));
+    }
+}
diff --git a/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java
new file mode 100644
index 0000000..450ff72
--- /dev/null
+++ b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java
@@ -0,0 +1,223 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.gce;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.cloud.gce.GceComputeService;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.transport.MockTransportService;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.local.LocalTransport;
+import org.junit.*;
+
+import java.util.List;
+import java.util.Locale;
+
+import static org.hamcrest.Matchers.hasSize;
+import static org.hamcrest.Matchers.is;
+
+/**
+ * This test class uses a GCE HTTP Mock system which allows to simulate JSON Responses.
+ *
+ * To implement a new test you'll need to create an `instances.json` file which contains expected response
+ * for a given project-id and zone under the src/test/resources/org/elasticsearch/discovery/gce with dir name:
+ *
+ * compute/v1/projects/[project-id]/zones/[zone]
+ *
+ * By default, project-id is the test method name, lowercase.
+ *
+ * For example, if you create a test `myNewAwesomeTest` with following settings:
+ *
+ * Settings nodeSettings = Settings.builder()
+ *  .put(GceComputeService.Fields.PROJECT, projectName)
+ *  .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+ *  .build();
+ *
+ *  You need to create a file under `src/test/resources/org/elasticsearch/discovery/gce/` named:
+ *
+ *  compute/v1/projects/mynewawesometest/zones/europe-west1-b/instances.json
+ *
+ */
+public class GceDiscoveryTests extends ESTestCase {
+
+    protected static ThreadPool threadPool;
+    protected MockTransportService transportService;
+    protected NetworkService networkService;
+    protected GceComputeService mock;
+    protected String projectName;
+
+    @BeforeClass
+    public static void createThreadPool() {
+        threadPool = new ThreadPool(GceDiscoveryTests.class.getName());
+    }
+
+    @AfterClass
+    public static void stopThreadPool() {
+        if (threadPool !=null) {
+            threadPool.shutdownNow();
+            threadPool = null;
+        }
+    }
+
+    @Before
+    public void setProjectName() {
+        projectName = getTestName().toLowerCase(Locale.ROOT);
+    }
+
+    @Before
+    public void createTransportService() {
+        transportService = new MockTransportService(
+                Settings.EMPTY,
+                new LocalTransport(Settings.EMPTY, threadPool, Version.CURRENT, new NamedWriteableRegistry()), threadPool);
+    }
+
+    @Before
+    public void createNetworkService() {
+        networkService = new NetworkService(Settings.EMPTY);
+    }
+
+    @After
+    public void stopGceComputeService() {
+        if (mock != null) {
+            mock.stop();
+        }
+    }
+
+    protected List<DiscoveryNode> buildDynamicNodes(GceComputeService gceComputeService, Settings nodeSettings) {
+        GceUnicastHostsProvider provider = new GceUnicastHostsProvider(nodeSettings, gceComputeService,
+                transportService, new NetworkService(Settings.EMPTY), Version.CURRENT);
+
+        List<DiscoveryNode> discoveryNodes = provider.buildDynamicNodes();
+        logger.info("--> nodes found: {}", discoveryNodes);
+        return discoveryNodes;
+    }
+
+    @Test
+    public void nodesWithDifferentTagsAndNoTagSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    @Test
+    public void nodesWithDifferentTagsAndOneTagSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .putArray(GceComputeService.Fields.TAGS, "elasticsearch")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(1));
+        assertThat(discoveryNodes.get(0).getId(), is("#cloud-test2-0"));
+    }
+
+    @Test
+    public void nodesWithDifferentTagsAndTwoTagSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .putArray(GceComputeService.Fields.TAGS, "elasticsearch", "dev")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(1));
+        assertThat(discoveryNodes.get(0).getId(), is("#cloud-test2-0"));
+    }
+
+    @Test
+    public void nodesWithSameTagsAndNoTagSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    @Test
+    public void nodesWithSameTagsAndOneTagSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .putArray(GceComputeService.Fields.TAGS, "elasticsearch")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    @Test
+    public void nodesWithSameTagsAndTwoTagsSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .putArray(GceComputeService.Fields.TAGS, "elasticsearch", "dev")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    @Test
+    public void multipleZonesAndTwoNodesInSameZone() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "europe-west1-b")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    @Test
+    public void multipleZonesAndTwoNodesInDifferentZones() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "europe-west1-b")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    /**
+     * For issue https://github.com/elastic/elasticsearch-cloud-gce/issues/43
+     */
+    @Test
+    public void zeroNode43() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "us-central1-b")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(0));
+    }
+}
diff --git a/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceNetworkTests.java b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceNetworkTests.java
new file mode 100644
index 0000000..7550cdc
--- /dev/null
+++ b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceNetworkTests.java
@@ -0,0 +1,132 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.gce;
+
+import org.elasticsearch.cloud.gce.network.GceNameResolver;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.net.InetAddress;
+
+import static org.hamcrest.Matchers.arrayContaining;
+import static org.hamcrest.Matchers.containsString;
+
+/**
+ * Test for GCE network.host settings.
+ * Related to https://github.com/elastic/elasticsearch/issues/13605
+ */
+public class GceNetworkTests extends ESTestCase {
+
+    /**
+     * Test for network.host: _gce_
+     */
+    @Test
+    public void networkHostGceDefault() throws IOException {
+        resolveGce("_gce_", InetAddress.getByName("10.240.0.2"));
+    }
+
+    /**
+     * Test for network.host: _gce:privateIp_
+     */
+    @Test
+    public void networkHostPrivateIp() throws IOException {
+        resolveGce("_gce:privateIp_", InetAddress.getByName("10.240.0.2"));
+    }
+
+    /**
+     * Test for network.host: _gce:hostname_
+     */
+    @Test
+    public void networkHostPrivateDns() throws IOException {
+        resolveGce("_gce:hostname_", InetAddress.getByName("localhost"));
+    }
+
+    /**
+     * Test for network.host: _gce:doesnotexist_
+     * This should raise an IllegalArgumentException as this setting does not exist
+     */
+    @Test
+    public void networkHostWrongSetting() throws IOException {
+        resolveGce("_gce:doesnotexist_", (InetAddress) null);
+    }
+
+    /**
+     * Test with multiple network interfaces:
+     * network.host: _gce:privateIp:0_
+     * network.host: _gce:privateIp:1_
+     */
+    @Test
+    public void networkHostPrivateIpInterface() throws IOException {
+        resolveGce("_gce:privateIp:0_", InetAddress.getByName("10.240.0.2"));
+        resolveGce("_gce:privateIp:1_", InetAddress.getByName("10.150.0.1"));
+    }
+
+    /**
+     * Test that we don't have any regression with network host core settings such as
+     * network.host: _local_
+     */
+    @Test
+    public void networkHostCoreLocal() throws IOException {
+        resolveGce("_local_", new NetworkService(Settings.EMPTY).resolveBindHostAddress(NetworkService.DEFAULT_NETWORK_HOST));
+    }
+
+    /**
+     * Utility test method to test different settings
+     * @param gceNetworkSetting tested network.host property
+     * @param expected expected InetAddress, null if we expect an exception
+     * @throws IOException Well... If something goes wrong :)
+     */
+    private void resolveGce(String gceNetworkSetting, InetAddress expected) throws IOException {
+        resolveGce(gceNetworkSetting, expected == null ? null : new InetAddress [] { expected });
+    }
+
+    /**
+     * Utility test method to test different settings
+     * @param gceNetworkSetting tested network.host property
+     * @param expected expected InetAddress, null if we expect an exception
+     * @throws IOException Well... If something goes wrong :)
+     */
+    private void resolveGce(String gceNetworkSetting, InetAddress[] expected) throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", gceNetworkSetting)
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        GceComputeServiceMock mock = new GceComputeServiceMock(nodeSettings, networkService);
+        networkService.addCustomNameResolver(new GceNameResolver(nodeSettings, mock));
+        try {
+            InetAddress[] addresses = networkService.resolveBindHostAddress(null);
+            if (expected == null) {
+                fail("We should get a IllegalArgumentException when setting network.host: _gce:doesnotexist_");
+            }
+            assertThat(addresses, arrayContaining(expected));
+        } catch (IllegalArgumentException e) {
+            if (expected != null) {
+                // We were expecting something and not an exception
+                throw e;
+            }
+            // We check that we get the expected exception
+            assertThat(e.getMessage(), containsString("is not one of the supported GCE network.host setting"));
+        }
+    }
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/europe-west1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/europe-west1-b/instances
new file mode 100644
index 0000000..049e0e1
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/europe-west1-b/instances
@@ -0,0 +1,36 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 1",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test1",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/us-central1-a/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/us-central1-a/instances
new file mode 100644
index 0000000..7e1e5d5
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/us-central1-a/instances
@@ -0,0 +1,36 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 2",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test2",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "us-central1-a"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/europe-west1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/europe-west1-b/instances
new file mode 100644
index 0000000..78de693
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/europe-west1-b/instances
@@ -0,0 +1,67 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 1",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test1",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    },
+    {
+      "description": "ES Node 2",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test2",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/us-central1-a/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/us-central1-a/instances
new file mode 100644
index 0000000..54c3836
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/us-central1-a/instances
@@ -0,0 +1,5 @@
+{
+  "id": "dummy",
+  "items":[
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandnotagset/zones/europe-west1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandnotagset/zones/europe-west1-b/instances
new file mode 100644
index 0000000..1ca810c
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandnotagset/zones/europe-west1-b/instances
@@ -0,0 +1,66 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 1",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test1",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    },
+    {
+      "description": "ES Node 2",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test2",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandonetagset/zones/europe-west1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandonetagset/zones/europe-west1-b/instances
new file mode 100644
index 0000000..1ca810c
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandonetagset/zones/europe-west1-b/instances
@@ -0,0 +1,66 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 1",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test1",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    },
+    {
+      "description": "ES Node 2",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test2",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandtwotagset/zones/europe-west1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandtwotagset/zones/europe-west1-b/instances
new file mode 100644
index 0000000..1ca810c
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandtwotagset/zones/europe-west1-b/instances
@@ -0,0 +1,66 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 1",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test1",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    },
+    {
+      "description": "ES Node 2",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test2",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandnotagset/zones/europe-west1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandnotagset/zones/europe-west1-b/instances
new file mode 100644
index 0000000..78de693
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandnotagset/zones/europe-west1-b/instances
@@ -0,0 +1,67 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 1",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test1",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    },
+    {
+      "description": "ES Node 2",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test2",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandonetagset/zones/europe-west1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandonetagset/zones/europe-west1-b/instances
new file mode 100644
index 0000000..78de693
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandonetagset/zones/europe-west1-b/instances
@@ -0,0 +1,67 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 1",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test1",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    },
+    {
+      "description": "ES Node 2",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test2",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandtwotagsset/zones/europe-west1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandtwotagsset/zones/europe-west1-b/instances
new file mode 100644
index 0000000..78de693
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandtwotagsset/zones/europe-west1-b/instances
@@ -0,0 +1,67 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 1",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test1",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    },
+    {
+      "description": "ES Node 2",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test2",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-a/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-a/instances
new file mode 100644
index 0000000..54c3836
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-a/instances
@@ -0,0 +1,5 @@
+{
+  "id": "dummy",
+  "items":[
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-b/instances
new file mode 100644
index 0000000..54c3836
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-b/instances
@@ -0,0 +1,5 @@
+{
+  "id": "dummy",
+  "items":[
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/hostname b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/hostname
new file mode 100644
index 0000000..2fbb50c
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/hostname
@@ -0,0 +1 @@
+localhost
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/network-interfaces/0/ip b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/network-interfaces/0/ip
new file mode 100644
index 0000000..1ac79d6
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/network-interfaces/0/ip
@@ -0,0 +1 @@
+10.240.0.2
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/network-interfaces/1/ip b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/network-interfaces/1/ip
new file mode 100644
index 0000000..e3bb0f8
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/network-interfaces/1/ip
@@ -0,0 +1 @@
+10.150.0.1
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/service-accounts/default/token b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/service-accounts/default/token
new file mode 100644
index 0000000..b338f61
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/service-accounts/default/token
@@ -0,0 +1,4 @@
+{
+  "access_token":"1/fFAGRNJru1FTz70BzhT3Zg",
+  "token_type":"Bearer"
+}
diff --git a/plugins/discovery-gce/src/test/resources/rest-api-spec/test/discovery_gce/10_basic.yaml b/plugins/discovery-gce/src/test/resources/rest-api-spec/test/discovery_gce/10_basic.yaml
new file mode 100644
index 0000000..8f5fbdc
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/rest-api-spec/test/discovery_gce/10_basic.yaml
@@ -0,0 +1,14 @@
+# Integration tests for Discovery GCE components
+#
+"Discovery GCE loaded":
+    - do:
+        cluster.state: {}
+
+    # Get master node id
+    - set: { master_node: master }
+
+    - do:
+        nodes.info: {}
+
+    - match:  { nodes.$master.plugins.0.name: discovery-gce  }
+    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java b/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java
index e05fc31..5b61787 100644
--- a/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java
+++ b/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.plugin.discovery.multicast;
 
-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
 import org.apache.lucene.util.Constants;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.Version;
@@ -46,13 +44,7 @@ import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.discovery.zen.ping.PingContextProvider;
 import org.elasticsearch.discovery.zen.ping.ZenPing;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.EmptyTransportResponseHandler;
-import org.elasticsearch.transport.TransportChannel;
-import org.elasticsearch.transport.TransportException;
-import org.elasticsearch.transport.TransportRequest;
-import org.elasticsearch.transport.TransportRequestHandler;
-import org.elasticsearch.transport.TransportResponse;
-import org.elasticsearch.transport.TransportService;
+import org.elasticsearch.transport.*;
 
 import java.io.IOException;
 import java.net.SocketAddress;
@@ -491,8 +483,8 @@ public class MulticastZenPing extends AbstractLifecycleComponent<ZenPing> implem
                 }
 
                 builder.startObject("attributes");
-                for (ObjectObjectCursor<String, String> attr : localNode.attributes()) {
-                    builder.field(attr.key, attr.value);
+                for (Map.Entry<String, String> attr : localNode.attributes().entrySet()) {
+                    builder.field(attr.getKey(), attr.getValue());
                 }
                 builder.endObject();
 
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BulkTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BulkTests.java
index 6cba9ae..fa6e91b 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BulkTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BulkTests.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.messy.tests;
 
-import java.nio.charset.StandardCharsets;
-
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.alias.Alias;
 import org.elasticsearch.action.bulk.BulkItemResponse;
@@ -49,21 +47,15 @@ import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
+import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
 import java.util.concurrent.CyclicBarrier;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertExists;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.nullValue;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 public class BulkTests extends ESIntegTestCase {
 
@@ -190,8 +182,8 @@ public class BulkTests extends ESIntegTestCase {
         assertThat(((UpdateResponse) bulkResponse.getItems()[2].getResponse()).getVersion(), equalTo(3l));
 
         bulkResponse = client().prepareBulk()
-                .add(client().prepareIndex("test", "type", "e1").setCreate(true).setSource("field", "1").setVersion(10).setVersionType(VersionType.EXTERNAL))
-                .add(client().prepareIndex("test", "type", "e2").setCreate(true).setSource("field", "1").setVersion(10).setVersionType(VersionType.EXTERNAL))
+                .add(client().prepareIndex("test", "type", "e1").setSource("field", "1").setVersion(10).setVersionType(VersionType.EXTERNAL))
+                .add(client().prepareIndex("test", "type", "e2").setSource("field", "1").setVersion(10).setVersionType(VersionType.EXTERNAL))
                 .add(client().prepareIndex("test", "type", "e1").setSource("field", "2").setVersion(12).setVersionType(VersionType.EXTERNAL)).get();
 
         assertTrue(((IndexResponse) bulkResponse.getItems()[0].getResponse()).isCreated());
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java
index 99b334e..51fc5a4 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java
@@ -19,20 +19,12 @@
 
 package org.elasticsearch.messy.tests;
 
-import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchType;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
-import org.elasticsearch.common.lucene.search.function.FieldValueFactorFunction;
 import org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.MatchAllQueryBuilder;
 import org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilder;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
-import org.elasticsearch.index.query.functionscore.weight.WeightBuilder;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.groovy.GroovyPlugin;
@@ -49,385 +41,26 @@ import java.util.concurrent.ExecutionException;
 
 import static org.elasticsearch.client.Requests.searchRequest;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
 import static org.elasticsearch.index.query.QueryBuilders.functionScoreQuery;
 import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.exponentialDecayFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.fieldValueFactorFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.gaussDecayFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.linearDecayFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.randomFunction;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.scriptFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.weightFactorFunction;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.hamcrest.Matchers.closeTo;
 import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
 import static org.hamcrest.Matchers.is;
 
 public class FunctionScoreTests extends ESIntegTestCase {
 
     static final String TYPE = "type";
     static final String INDEX = "index";
-    static final String TEXT_FIELD = "text_field";
-    static final String DOUBLE_FIELD = "double_field";
-    static final String GEO_POINT_FIELD = "geo_point_field";
-    static final XContentBuilder SIMPLE_DOC;
-    static final XContentBuilder MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD;
 
     @Override
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         return Collections.singleton(GroovyPlugin.class);
     }
-    
-    @Test
-    public void testExplainQueryOnlyOnce() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate("test").addMapping(
-                "type1",
-                jsonBuilder().startObject().startObject("type1").startObject("properties").startObject("test").field("type", "string")
-                        .endObject().startObject("num").field("type", "float").endObject().endObject().endObject().endObject()));
-        ensureYellow();
-
-        client().prepareIndex()
-                .setType("type1")
-                .setId("1")
-                .setIndex("test")
-                .setSource(
-                        jsonBuilder().startObject().field("test", "value").field("num", 10).endObject()).get();
-        refresh();
-
-        SearchResponse response = client().search(
-                searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                        searchSource().explain(true).query(
-                                functionScoreQuery(termQuery("test", "value"), new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(gaussDecayFunction("num", 5, 5)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(exponentialDecayFunction("num", 5, 5)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(linearDecayFunction("num", 5, 5))
-                                })))).get();
-        String explanation = response.getHits().getAt(0).explanation().toString();
-
-        checkQueryExplanationAppearsOnlyOnce(explanation);
-        response = client().search(
-                searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                        searchSource().explain(true).query(
-                                functionScoreQuery(termQuery("test", "value"), fieldValueFactorFunction("num"))))).get();
-        explanation = response.getHits().getAt(0).explanation().toString();
-        checkQueryExplanationAppearsOnlyOnce(explanation);
-
-        response = client().search(
-                searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                        searchSource().explain(true).query(
-                                functionScoreQuery(termQuery("test", "value"), randomFunction(10))))).get();
-        explanation = response.getHits().getAt(0).explanation().toString();
-
-        checkQueryExplanationAppearsOnlyOnce(explanation);
-    }
-
-    private void checkQueryExplanationAppearsOnlyOnce(String explanation) {
-        // use some substring of the query explanation and see if it appears twice
-        String queryExplanation = "idf(docFreq=1, maxDocs=1)";
-        int queryExplanationIndex = explanation.indexOf(queryExplanation, 0);
-        assertThat(queryExplanationIndex, greaterThan(-1));
-        queryExplanationIndex = explanation.indexOf(queryExplanation, queryExplanationIndex + 1);
-        assertThat(queryExplanationIndex, equalTo(-1));
-    }
-
-    static {
-        XContentBuilder simpleDoc;
-        XContentBuilder mappingWithDoubleAndGeoPointAndTestField;
-        try {
-            simpleDoc = jsonBuilder().startObject()
-                    .field(TEXT_FIELD, "value")
-                    .startObject(GEO_POINT_FIELD)
-                    .field("lat", 10)
-                    .field("lon", 20)
-                    .endObject()
-                    .field(DOUBLE_FIELD, Math.E)
-                    .endObject();
-        } catch (IOException e) {
-            throw new ElasticsearchException("Exception while initializing FunctionScoreIT", e);
-        }
-        SIMPLE_DOC = simpleDoc;
-        try {
-
-            mappingWithDoubleAndGeoPointAndTestField = jsonBuilder().startObject()
-                    .startObject(TYPE)
-                    .startObject("properties")
-                    .startObject(TEXT_FIELD)
-                    .field("type", "string")
-                    .endObject()
-                    .startObject(GEO_POINT_FIELD)
-                    .field("type", "geo_point")
-                    .endObject()
-                    .startObject(DOUBLE_FIELD)
-                    .field("type", "double")
-                    .endObject()
-                    .endObject()
-                    .endObject()
-                    .endObject();
-        } catch (IOException e) {
-            throw new ElasticsearchException("Exception while initializing FunctionScoreIT", e);
-        }
-        MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD = mappingWithDoubleAndGeoPointAndTestField;
-    }
-
-    @Test
-    public void testExplain() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate(INDEX).addMapping(
-                TYPE, MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD
-        ));
-        ensureYellow();
-
-        index(INDEX, TYPE, "1", SIMPLE_DOC);
-        refresh();
-
-        SearchResponse responseWithWeights = client().search(
-                searchRequest().source(
-                        searchSource().query(
-                                functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(gaussDecayFunction(GEO_POINT_FIELD, new GeoPoint(10, 20), "1000km")),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(fieldValueFactorFunction(DOUBLE_FIELD).modifier(FieldValueFactorFunction.Modifier.LN).setWeight(2)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(scriptFunction(new Script("_index['" + TEXT_FIELD + "']['value'].tf()")).setWeight(3))
-                                })).explain(true))).actionGet();
-
-        assertThat(
-                responseWithWeights.getHits().getAt(0).getExplanation().toString(),
-                equalTo("6.0 = function score, product of:\n  1.0 = ConstantScore(text_field:value), product of:\n    1.0 = boost\n    1.0 = queryNorm\n  6.0 = min of:\n    6.0 = function score, score mode [multiply]\n      1.0 = function score, product of:\n        1.0 = match filter: *:*\n        1.0 = Function for field geo_point_field:\n          1.0 = exp(-0.5*pow(MIN of: [Math.max(arcDistance([10.0, 20.0](=doc value),[10.0, 20.0](=origin)) - 0.0(=offset), 0)],2.0)/7.213475204444817E11)\n      2.0 = function score, product of:\n        1.0 = match filter: *:*\n        2.0 = product of:\n          1.0 = field value function: ln(doc['double_field'].value * factor=1.0)\n          2.0 = weight\n      3.0 = function score, product of:\n        1.0 = match filter: *:*\n        3.0 = product of:\n          1.0 = script score function, computed with script:\"[script: _index['text_field']['value'].tf(), type: inline, lang: null, params: null]\n            1.0 = _score: \n              1.0 = ConstantScore(text_field:value), product of:\n                1.0 = boost\n                1.0 = queryNorm\n          3.0 = weight\n    3.4028235E38 = maxBoost\n"));
-        responseWithWeights = client().search(
-                searchRequest().source(
-                        searchSource().query(
-                                functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), weightFactorFunction(4.0f)))
-                                .explain(true))).actionGet();
-        assertThat(
-                responseWithWeights.getHits().getAt(0).getExplanation().toString(),
-                equalTo("4.0 = function score, product of:\n  1.0 = ConstantScore(text_field:value), product of:\n    1.0 = boost\n    1.0 = queryNorm\n  4.0 = min of:\n    4.0 = product of:\n      1.0 = constant score 1.0 - no function provided\n      4.0 = weight\n    3.4028235E38 = maxBoost\n"));
-
-    }
-
-    @Test
-    public void simpleWeightedFunctionsTest() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate(INDEX).addMapping(
-                TYPE, MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD
-        ));
-        ensureYellow();
-
-        index(INDEX, TYPE, "1", SIMPLE_DOC);
-        refresh();
-        SearchResponse response = client().search(
-                searchRequest().source(
-                        searchSource().query(
-                                functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(gaussDecayFunction(GEO_POINT_FIELD, new GeoPoint(10, 20), "1000km")),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(fieldValueFactorFunction(DOUBLE_FIELD).modifier(FieldValueFactorFunction.Modifier.LN)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(scriptFunction(new Script("_index['" + TEXT_FIELD + "']['value'].tf()")))
-                                })))).actionGet();
-        SearchResponse responseWithWeights = client().search(
-                searchRequest().source(
-                        searchSource().query(
-                                functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(gaussDecayFunction(GEO_POINT_FIELD, new GeoPoint(10, 20), "1000km").setWeight(2)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(fieldValueFactorFunction(DOUBLE_FIELD).modifier(FieldValueFactorFunction.Modifier.LN).setWeight(2)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(scriptFunction(new Script("_index['" + TEXT_FIELD + "']['value'].tf()")).setWeight(2))
-                                })))).actionGet();
-
-        assertSearchResponse(response);
-        assertThat(response.getHits().getAt(0).getScore(), is(1.0f));
-        assertThat(responseWithWeights.getHits().getAt(0).getScore(), is(8.0f));
-    }
-
-    @Test
-    public void simpleWeightedFunctionsTestWithRandomWeightsAndRandomCombineMode() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate(INDEX).addMapping(
-                TYPE,
-                MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD));
-        ensureYellow();
-
-        XContentBuilder doc = SIMPLE_DOC;
-        index(INDEX, TYPE, "1", doc);
-        refresh();
-        ScoreFunctionBuilder[] scoreFunctionBuilders = getScoreFunctionBuilders();
-        float[] weights = createRandomWeights(scoreFunctionBuilders.length);
-        float[] scores = getScores(scoreFunctionBuilders);
-        int weightscounter = 0;
-        FunctionScoreQueryBuilder.FilterFunctionBuilder[] filterFunctionBuilders = new FunctionScoreQueryBuilder.FilterFunctionBuilder[scoreFunctionBuilders.length];
-        for (ScoreFunctionBuilder builder : scoreFunctionBuilders) {
-            filterFunctionBuilders[weightscounter] = new FunctionScoreQueryBuilder.FilterFunctionBuilder(builder.setWeight(weights[weightscounter]));
-            weightscounter++;
-        }
-        FiltersFunctionScoreQuery.ScoreMode scoreMode = randomFrom(FiltersFunctionScoreQuery.ScoreMode.AVG, FiltersFunctionScoreQuery.ScoreMode.SUM,
-                FiltersFunctionScoreQuery.ScoreMode.MIN, FiltersFunctionScoreQuery.ScoreMode.MAX, FiltersFunctionScoreQuery.ScoreMode.MULTIPLY);
-        FunctionScoreQueryBuilder withWeights = functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), filterFunctionBuilders).scoreMode(scoreMode);
-
-        SearchResponse responseWithWeights = client().search(
-                searchRequest().source(searchSource().query(withWeights))
-        ).actionGet();
-
-        double expectedScore = computeExpectedScore(weights, scores, scoreMode);
-        assertThat((float) expectedScore / responseWithWeights.getHits().getAt(0).getScore(), is(1.0f));
-    }
-
-    protected double computeExpectedScore(float[] weights, float[] scores, FiltersFunctionScoreQuery.ScoreMode scoreMode) {
-        double expectedScore;
-        switch(scoreMode) {
-            case MULTIPLY:
-                expectedScore = 1.0;
-                break;
-            case MAX:
-                expectedScore = Float.MAX_VALUE * -1.0;
-                break;
-            case MIN:
-                expectedScore = Float.MAX_VALUE;
-                break;
-            default:
-                expectedScore = 0.0;
-                break;
-        }
-
-        float weightSum = 0;
-        for (int i = 0; i < weights.length; i++) {
-            double functionScore = (double) weights[i] * scores[i];
-            weightSum += weights[i];
-            switch(scoreMode) {
-                case AVG:
-                    expectedScore += functionScore;
-                    break;
-                case MAX:
-                    expectedScore = Math.max(functionScore, expectedScore);
-                    break;
-                case MIN:
-                    expectedScore = Math.min(functionScore, expectedScore);
-                    break;
-                case SUM:
-                    expectedScore += functionScore;
-                    break;
-                case MULTIPLY:
-                    expectedScore *= functionScore;
-                    break;
-                default:
-                    throw new UnsupportedOperationException();
-            }
-        }
-        if (scoreMode == FiltersFunctionScoreQuery.ScoreMode.AVG) {
-            expectedScore /= weightSum;
-        }
-        return expectedScore;
-    }
-
-    @Test
-    public void simpleWeightedFunctionsTestSingleFunction() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate(INDEX).addMapping(
-                TYPE,
-                MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD));
-        ensureYellow();
-
-        XContentBuilder doc = jsonBuilder().startObject()
-                .field(TEXT_FIELD, "value")
-                .startObject(GEO_POINT_FIELD)
-                .field("lat", 12)
-                .field("lon", 21)
-                .endObject()
-                .field(DOUBLE_FIELD, 10)
-                .endObject();
-        index(INDEX, TYPE, "1", doc);
-        refresh();
-        ScoreFunctionBuilder[] scoreFunctionBuilders = getScoreFunctionBuilders();
-        ScoreFunctionBuilder scoreFunctionBuilder = scoreFunctionBuilders[randomInt(3)];
-        float[] weights = createRandomWeights(1);
-        float[] scores = getScores(scoreFunctionBuilder);
-        FunctionScoreQueryBuilder withWeights = functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), scoreFunctionBuilder.setWeight(weights[0]));
-
-        SearchResponse responseWithWeights = client().search(
-                searchRequest().source(searchSource().query(withWeights))
-        ).actionGet();
-
-        assertThat( (double) scores[0] * weights[0]/ responseWithWeights.getHits().getAt(0).getScore(), closeTo(1.0, 1.e-6));
-
-    }
-
-    private float[] getScores(ScoreFunctionBuilder... scoreFunctionBuilders) {
-        float[] scores = new float[scoreFunctionBuilders.length];
-        int scorecounter = 0;
-        for (ScoreFunctionBuilder builder : scoreFunctionBuilders) {
-            SearchResponse response = client().search(
-                    searchRequest().source(
-                            searchSource().query(
-                                    functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), builder)
-                            ))).actionGet();
-            scores[scorecounter] = response.getHits().getAt(0).getScore();
-            scorecounter++;
-        }
-        return scores;
-    }
-
-    private float[] createRandomWeights(int size) {
-        float[] weights = new float[size];
-        for (int i = 0; i < weights.length; i++) {
-            weights[i] = randomFloat() * (randomBoolean() ? 1.0f : -1.0f) * randomInt(100) + 1.e-6f;
-        }
-        return weights;
-    }
-
-    public ScoreFunctionBuilder[] getScoreFunctionBuilders() {
-        ScoreFunctionBuilder[] builders = new ScoreFunctionBuilder[4];
-        builders[0] = gaussDecayFunction(GEO_POINT_FIELD, new GeoPoint(10, 20), "1000km");
-        builders[1] = randomFunction(10);
-        builders[2] = fieldValueFactorFunction(DOUBLE_FIELD).modifier(FieldValueFactorFunction.Modifier.LN);
-        builders[3] = scriptFunction(new Script("_index['" + TEXT_FIELD + "']['value'].tf()"));
-        return builders;
-    }
 
-    @Test
-    public void checkWeightOnlyCreatesBoostFunction() throws IOException {
-        assertAcked(prepareCreate(INDEX).addMapping(
-                TYPE,
-                MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD));
-        ensureYellow();
-
-        index(INDEX, TYPE, "1", SIMPLE_DOC);
-        refresh();
-        String query =jsonBuilder().startObject()
-                .startObject("query")
-                .startObject("function_score")
-                .startArray("functions")
-                .startObject()
-                .field("weight",2)
-                .endObject()
-                .endArray()
-                .endObject()
-                .endObject()
-                .endObject().string();
-        SearchResponse response = client().search(
-                searchRequest().source(new BytesArray(query))
-                ).actionGet();
-        assertSearchResponse(response);
-        assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
-
-        query =jsonBuilder().startObject()
-                .startObject("query")
-                .startObject("function_score")
-                .field("weight",2)
-                .endObject()
-                .endObject()
-                .endObject().string();
-        response = client().search(
-                searchRequest().source(new BytesArray(query))
-        ).actionGet();
-        assertSearchResponse(response);
-        assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
-        response = client().search(
-                searchRequest().source(searchSource().query(functionScoreQuery(new WeightBuilder().setWeight(2.0f))))
-        ).actionGet();
-        assertSearchResponse(response);
-        assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
-        response = client().search(
-                searchRequest().source(searchSource().query(functionScoreQuery(weightFactorFunction(2.0f))))
-        ).actionGet();
-        assertSearchResponse(response);
-        assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
-    }
 
     @Test
     public void testScriptScoresNested() throws IOException {
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TransformOnIndexMapperTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TransformOnIndexMapperTests.java
index 69da9c7..ace6f68 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TransformOnIndexMapperTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TransformOnIndexMapperTests.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.messy.tests;
 
+import com.google.common.collect.ImmutableMap;
+
 import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.elasticsearch.action.get.GetResponse;
 import org.elasticsearch.action.search.SearchResponse;
@@ -40,7 +42,6 @@ import java.util.Collections;
 import java.util.Map;
 import java.util.concurrent.ExecutionException;
 
-import static java.util.Collections.singletonMap;
 import static org.elasticsearch.index.query.QueryBuilders.termQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertExists;
@@ -57,12 +58,12 @@ import static org.hamcrest.Matchers.not;
  */
 @SuppressCodecs("*") // requires custom completion format
 public class TransformOnIndexMapperTests extends ESIntegTestCase {
-
+    
     @Override
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         return Collections.singleton(GroovyPlugin.class);
     }
-
+    
     @Test
     public void searchOnTransformed() throws Exception {
         setup(true);
@@ -170,7 +171,7 @@ public class TransformOnIndexMapperTests extends ESIntegTestCase {
         if (getRandom().nextBoolean()) {
             script = script.replace("sourceField", "'content'");
         } else {
-            builder.field("params", singletonMap("sourceField", "content"));
+            builder.field("params", ImmutableMap.of("sourceField", "content"));
         }
         builder.field("script", script);
     }
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/UpdateTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/UpdateTests.java
deleted file mode 100644
index 3c2b230..0000000
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/UpdateTests.java
+++ /dev/null
@@ -1,859 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.messy.tests;
-
-import org.elasticsearch.ElasticsearchTimeoutException;
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionRequestValidationException;
-import org.elasticsearch.action.admin.indices.alias.Alias;
-import org.elasticsearch.action.delete.DeleteRequest;
-import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.update.UpdateRequest;
-import org.elasticsearch.action.update.UpdateRequestBuilder;
-import org.elasticsearch.action.update.UpdateResponse;
-import org.elasticsearch.client.transport.NoNodeAvailableException;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.VersionType;
-import org.elasticsearch.index.engine.DocumentMissingException;
-import org.elasticsearch.index.engine.VersionConflictEngineException;
-import org.elasticsearch.index.shard.MergePolicyConfig;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.groovy.GroovyPlugin;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Test;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.CopyOnWriteArrayList;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.Semaphore;
-import java.util.concurrent.TimeUnit;
-
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.lessThanOrEqualTo;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
-
-public class UpdateTests extends ESIntegTestCase {
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return Collections.singleton(GroovyPlugin.class);
-    }
-    
-    private void createTestIndex() throws Exception {
-        logger.info("--> creating index test");
-
-        assertAcked(prepareCreate("test").addAlias(new Alias("alias"))
-                .addMapping("type1", XContentFactory.jsonBuilder()
-                        .startObject()
-                        .startObject("type1")
-                        .startObject("_timestamp").field("enabled", true).endObject()
-                        .startObject("_ttl").field("enabled", true).endObject()
-                        .endObject()
-                        .endObject()));
-    }
-
-    @Test
-    public void testUpsert() throws Exception {
-        createTestIndex();
-        ensureGreen();
-
-        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setUpsert(XContentFactory.jsonBuilder().startObject().field("field", 1).endObject())
-                .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null))
-                .execute().actionGet();
-        assertTrue(updateResponse.isCreated());
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-
-        for (int i = 0; i < 5; i++) {
-            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("1"));
-        }
-
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setUpsert(XContentFactory.jsonBuilder().startObject().field("field", 1).endObject())
-                .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null))
-                .execute().actionGet();
-        assertFalse(updateResponse.isCreated());
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-
-        for (int i = 0; i < 5; i++) {
-            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("2"));
-        }
-    }
-
-    @Test
-    public void testScriptedUpsert() throws Exception {
-        createTestIndex();
-        ensureGreen();
-        
-        // Script logic is 
-        // 1) New accounts take balance from "balance" in upsert doc and first payment is charged at 50%
-        // 2) Existing accounts subtract full payment from balance stored in elasticsearch
-        
-        String script="int oldBalance=ctx._source.balance;"+
-                      "int deduction=ctx.op == \"create\" ? (payment/2) :  payment;"+
-                      "ctx._source.balance=oldBalance-deduction;";
-        int openingBalance=10;
-
-        Map<String, Object> params = new HashMap<>();
-        params.put("payment", 2);
-
-        // Pay money from what will be a new account and opening balance comes from upsert doc
-        // provided by client
-        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setUpsert(XContentFactory.jsonBuilder().startObject().field("balance", openingBalance).endObject())
-                .setScriptedUpsert(true)
-.setScript(new Script(script, ScriptService.ScriptType.INLINE, null, params))
-                .execute().actionGet();
-        assertTrue(updateResponse.isCreated());
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-
-        for (int i = 0; i < 5; i++) {
-            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("balance").toString(), equalTo("9"));
-        }
-
-        // Now pay money for an existing account where balance is stored in es 
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setUpsert(XContentFactory.jsonBuilder().startObject().field("balance", openingBalance).endObject())
-                .setScriptedUpsert(true)
-.setScript(new Script(script, ScriptService.ScriptType.INLINE, null, params))
-                .execute().actionGet();
-        assertFalse(updateResponse.isCreated());
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-
-        for (int i = 0; i < 5; i++) {
-            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("balance").toString(), equalTo("7"));
-        }
-    }
-
-    @Test
-    public void testUpsertDoc() throws Exception {
-        createTestIndex();
-        ensureGreen();
-
-        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setDoc(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
-                .setDocAsUpsert(true)
-                .setFields("_source")
-                .execute().actionGet();
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult(), notNullValue());
-        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz"));
-    }
-
-    @Test
-    // See: https://github.com/elasticsearch/elasticsearch/issues/3265
-    public void testNotUpsertDoc() throws Exception {
-        createTestIndex();
-        ensureGreen();
-
-        assertThrows(client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setDoc(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
-                .setDocAsUpsert(false)
-                .setFields("_source")
-                .execute(), DocumentMissingException.class);
-    }
-
-    @Test
-    public void testUpsertFields() throws Exception {
-        createTestIndex();
-        ensureGreen();
-
-        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setUpsert(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
-                .setScript(new Script("ctx._source.extra = \"foo\"", ScriptService.ScriptType.INLINE, null, null))
-                .setFields("_source")
-                .execute().actionGet();
-
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult(), notNullValue());
-        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz"));
-        assertThat(updateResponse.getGetResult().sourceAsMap().get("extra"), nullValue());
-
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setUpsert(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
-                .setScript(new Script("ctx._source.extra = \"foo\"", ScriptService.ScriptType.INLINE, null, null))
-                .setFields("_source")
-                .execute().actionGet();
-
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult(), notNullValue());
-        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz"));
-        assertThat(updateResponse.getGetResult().sourceAsMap().get("extra").toString(), equalTo("foo"));
-    }
-
-    @Test
-    public void testVersionedUpdate() throws Exception {
-        assertAcked(prepareCreate("test").addAlias(new Alias("alias")));
-        ensureGreen();
-
-        index("test", "type", "1", "text", "value"); // version is now 1
-
-        assertThrows(client().prepareUpdate(indexOrAlias(), "type", "1")
-                        .setScript(new Script("ctx._source.text = 'v2'", ScriptService.ScriptType.INLINE, null, null)).setVersion(2)
-                        .execute(),
-                VersionConflictEngineException.class);
-
-        client().prepareUpdate(indexOrAlias(), "type", "1")
-                .setScript(new Script("ctx._source.text = 'v2'", ScriptService.ScriptType.INLINE, null, null)).setVersion(1).get();
-        assertThat(client().prepareGet("test", "type", "1").get().getVersion(), equalTo(2l));
-
-        // and again with a higher version..
-        client().prepareUpdate(indexOrAlias(), "type", "1")
-                .setScript(new Script("ctx._source.text = 'v3'", ScriptService.ScriptType.INLINE, null, null)).setVersion(2).get();
-
-        assertThat(client().prepareGet("test", "type", "1").get().getVersion(), equalTo(3l));
-
-        // after delete
-        client().prepareDelete("test", "type", "1").get();
-        assertThrows(client().prepareUpdate("test", "type", "1")
-                        .setScript(new Script("ctx._source.text = 'v2'", ScriptService.ScriptType.INLINE, null, null)).setVersion(3)
-                        .execute(),
-                DocumentMissingException.class);
-
-        // external versioning
-        client().prepareIndex("test", "type", "2").setSource("text", "value").setVersion(10).setVersionType(VersionType.EXTERNAL).get();
-
-        assertThrows(client().prepareUpdate(indexOrAlias(), "type", "2")
-                        .setScript(new Script("ctx._source.text = 'v2'", ScriptService.ScriptType.INLINE, null, null)).setVersion(2)
-                        .setVersionType(VersionType.EXTERNAL).execute(),
-                ActionRequestValidationException.class);
-
-        // upserts - the combination with versions is a bit weird. Test are here to ensure we do not change our behavior unintentionally
-
-        // With internal versions, tt means "if object is there with version X, update it or explode. If it is not there, index.
-        client().prepareUpdate(indexOrAlias(), "type", "3")
-                .setScript(new Script("ctx._source.text = 'v2'", ScriptService.ScriptType.INLINE, null, null))
-                .setVersion(10).setUpsert("{ \"text\": \"v0\" }").get();
-        GetResponse get = get("test", "type", "3");
-        assertThat(get.getVersion(), equalTo(1l));
-        assertThat((String) get.getSource().get("text"), equalTo("v0"));
-
-        // With force version
-        client().prepareUpdate(indexOrAlias(), "type", "4")
-                .setScript(new Script("ctx._source.text = 'v2'", ScriptService.ScriptType.INLINE, null, null))
-                .setVersion(10).setVersionType(VersionType.FORCE).setUpsert("{ \"text\": \"v0\" }").get();
-
-        get = get("test", "type", "4");
-        assertThat(get.getVersion(), equalTo(10l));
-        assertThat((String) get.getSource().get("text"), equalTo("v0"));
-
-
-        // retry on conflict is rejected:
-        assertThrows(client().prepareUpdate(indexOrAlias(), "type", "1").setVersion(10).setRetryOnConflict(5), ActionRequestValidationException.class);
-    }
-
-    @Test
-    public void testIndexAutoCreation() throws Exception {
-        UpdateResponse updateResponse = client().prepareUpdate("test", "type1", "1")
-                .setUpsert(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
-                .setScript(new Script("ctx._source.extra = \"foo\"", ScriptService.ScriptType.INLINE, null, null))
-                .setFields("_source")
-                .execute().actionGet();
-
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult(), notNullValue());
-        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz"));
-        assertThat(updateResponse.getGetResult().sourceAsMap().get("extra"), nullValue());
-    }
-
-    @Test
-    public void testUpdate() throws Exception {
-        createTestIndex();
-        ensureGreen();
-
-        try {
-            client().prepareUpdate(indexOrAlias(), "type1", "1")
-                    .setScript(new Script("ctx._source.field++", ScriptService.ScriptType.INLINE, null, null)).execute().actionGet();
-            fail();
-        } catch (DocumentMissingException e) {
-            // all is well
-        }
-
-        client().prepareIndex("test", "type1", "1").setSource("field", 1).execute().actionGet();
-
-        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null)).execute().actionGet();
-        assertThat(updateResponse.getVersion(), equalTo(2L));
-        assertFalse(updateResponse.isCreated());
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-
-        for (int i = 0; i < 5; i++) {
-            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("2"));
-        }
-
-        Map<String, Object> params = new HashMap<>();
-        params.put("count", 3);
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setScript(new Script("ctx._source.field += count", ScriptService.ScriptType.INLINE, null, params)).execute().actionGet();
-        assertThat(updateResponse.getVersion(), equalTo(3L));
-        assertFalse(updateResponse.isCreated());
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-
-        for (int i = 0; i < 5; i++) {
-            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("5"));
-        }
-
-        // check noop
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setScript(new Script("ctx.op = 'none'", ScriptService.ScriptType.INLINE, null, null)).execute().actionGet();
-        assertThat(updateResponse.getVersion(), equalTo(3L));
-        assertFalse(updateResponse.isCreated());
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-
-        for (int i = 0; i < 5; i++) {
-            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("5"));
-        }
-
-        // check delete
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setScript(new Script("ctx.op = 'delete'", ScriptService.ScriptType.INLINE, null, null)).execute().actionGet();
-        assertThat(updateResponse.getVersion(), equalTo(4L));
-        assertFalse(updateResponse.isCreated());
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-
-        for (int i = 0; i < 5; i++) {
-            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.isExists(), equalTo(false));
-        }
-
-        // check TTL is kept after an update without TTL
-        client().prepareIndex("test", "type1", "2").setSource("field", 1).setTTL(86400000L).setRefresh(true).execute().actionGet();
-        GetResponse getResponse = client().prepareGet("test", "type1", "2").setFields("_ttl").execute().actionGet();
-        long ttl = ((Number) getResponse.getField("_ttl").getValue()).longValue();
-        assertThat(ttl, greaterThan(0L));
-        client().prepareUpdate(indexOrAlias(), "type1", "2")
-                .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null)).execute().actionGet();
-        getResponse = client().prepareGet("test", "type1", "2").setFields("_ttl").execute().actionGet();
-        ttl = ((Number) getResponse.getField("_ttl").getValue()).longValue();
-        assertThat(ttl, greaterThan(0L));
-
-        // check TTL update
-        client().prepareUpdate(indexOrAlias(), "type1", "2")
-                .setScript(new Script("ctx._ttl = 3600000", ScriptService.ScriptType.INLINE, null, null)).execute().actionGet();
-        getResponse = client().prepareGet("test", "type1", "2").setFields("_ttl").execute().actionGet();
-        ttl = ((Number) getResponse.getField("_ttl").getValue()).longValue();
-        assertThat(ttl, greaterThan(0L));
-        assertThat(ttl, lessThanOrEqualTo(3600000L));
-
-        // check timestamp update
-        client().prepareIndex("test", "type1", "3").setSource("field", 1).setRefresh(true).execute().actionGet();
-        client().prepareUpdate(indexOrAlias(), "type1", "3")
-                .setScript(new Script("ctx._timestamp = \"2009-11-15T14:12:12\"", ScriptService.ScriptType.INLINE, null, null)).execute()
-                .actionGet();
-        getResponse = client().prepareGet("test", "type1", "3").setFields("_timestamp").execute().actionGet();
-        long timestamp = ((Number) getResponse.getField("_timestamp").getValue()).longValue();
-        assertThat(timestamp, equalTo(1258294332000L));
-
-        // check fields parameter
-        client().prepareIndex("test", "type1", "1").setSource("field", 1).execute().actionGet();
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null)).setFields("_source", "field")
-                .execute().actionGet();
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult(), notNullValue());
-        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult().sourceRef(), notNullValue());
-        assertThat(updateResponse.getGetResult().field("field").getValue(), notNullValue());
-
-        // check updates without script
-        // add new field
-        client().prepareIndex("test", "type1", "1").setSource("field", 1).execute().actionGet();
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1").setDoc(XContentFactory.jsonBuilder().startObject().field("field2", 2).endObject()).execute().actionGet();
-        for (int i = 0; i < 5; i++) {
-            getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("1"));
-            assertThat(getResponse.getSourceAsMap().get("field2").toString(), equalTo("2"));
-        }
-
-        // change existing field
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1").setDoc(XContentFactory.jsonBuilder().startObject().field("field", 3).endObject()).execute().actionGet();
-        for (int i = 0; i < 5; i++) {
-            getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("3"));
-            assertThat(getResponse.getSourceAsMap().get("field2").toString(), equalTo("2"));
-        }
-
-        // recursive map
-        Map<String, Object> testMap = new HashMap<>();
-        Map<String, Object> testMap2 = new HashMap<>();
-        Map<String, Object> testMap3 = new HashMap<>();
-        testMap3.put("commonkey", testMap);
-        testMap3.put("map3", 5);
-        testMap2.put("map2", 6);
-        testMap.put("commonkey", testMap2);
-        testMap.put("map1", 8);
-
-        client().prepareIndex("test", "type1", "1").setSource("map", testMap).execute().actionGet();
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1").setDoc(XContentFactory.jsonBuilder().startObject().field("map", testMap3).endObject()).execute().actionGet();
-        for (int i = 0; i < 5; i++) {
-            getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            Map map1 = (Map) getResponse.getSourceAsMap().get("map");
-            assertThat(map1.size(), equalTo(3));
-            assertThat(map1.containsKey("map1"), equalTo(true));
-            assertThat(map1.containsKey("map3"), equalTo(true));
-            assertThat(map1.containsKey("commonkey"), equalTo(true));
-            Map map2 = (Map) map1.get("commonkey");
-            assertThat(map2.size(), equalTo(3));
-            assertThat(map2.containsKey("map1"), equalTo(true));
-            assertThat(map2.containsKey("map2"), equalTo(true));
-            assertThat(map2.containsKey("commonkey"), equalTo(true));
-        }
-    }
-
-    @Test
-    public void testUpdateRequestWithBothScriptAndDoc() throws Exception {
-        createTestIndex();
-        ensureGreen();
-
-        try {
-            client().prepareUpdate(indexOrAlias(), "type1", "1")
-                    .setDoc(XContentFactory.jsonBuilder().startObject().field("field", 1).endObject())
-                    .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null))
-                    .execute().actionGet();
-            fail("Should have thrown ActionRequestValidationException");
-        } catch (ActionRequestValidationException e) {
-            assertThat(e.validationErrors().size(), equalTo(1));
-            assertThat(e.validationErrors().get(0), containsString("can't provide both script and doc"));
-            assertThat(e.getMessage(), containsString("can't provide both script and doc"));
-        }
-    }
-
-    @Test
-    public void testUpdateRequestWithScriptAndShouldUpsertDoc() throws Exception {
-        createTestIndex();
-        ensureGreen();
-        try {
-            client().prepareUpdate(indexOrAlias(), "type1", "1")
-                    .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null))
-                    .setDocAsUpsert(true)
-                    .execute().actionGet();
-            fail("Should have thrown ActionRequestValidationException");
-        } catch (ActionRequestValidationException e) {
-            assertThat(e.validationErrors().size(), equalTo(1));
-            assertThat(e.validationErrors().get(0), containsString("doc must be specified if doc_as_upsert is enabled"));
-            assertThat(e.getMessage(), containsString("doc must be specified if doc_as_upsert is enabled"));
-        }
-    }
-
-    @Test
-    public void testContextVariables() throws Exception {
-        assertAcked(prepareCreate("test").addAlias(new Alias("alias"))
-                        .addMapping("type1", XContentFactory.jsonBuilder()
-                                .startObject()
-                                .startObject("type1")
-                                .startObject("_timestamp").field("enabled", true).endObject()
-                                .startObject("_ttl").field("enabled", true).endObject()
-                                .endObject()
-                                .endObject())
-                        .addMapping("subtype1", XContentFactory.jsonBuilder()
-                                .startObject()
-                                .startObject("subtype1")
-                                .startObject("_parent").field("type", "type1").endObject()
-                                .startObject("_timestamp").field("enabled", true).endObject()
-                                .startObject("_ttl").field("enabled", true).endObject()
-                                .endObject()
-                                .endObject())
-        );
-        ensureGreen();
-
-        // Index some documents
-        long timestamp = System.currentTimeMillis();
-        client().prepareIndex()
-                .setIndex("test")
-                .setType("type1")
-                .setId("parentId1")
-                .setTimestamp(String.valueOf(timestamp-1))
-                .setSource("field1", 0, "content", "bar")
-                .execute().actionGet();
-
-        long ttl = 10000;
-        client().prepareIndex()
-                .setIndex("test")
-                .setType("subtype1")
-                .setId("id1")
-                .setParent("parentId1")
-                .setRouting("routing1")
-                .setTimestamp(String.valueOf(timestamp))
-                .setTTL(ttl)
-                .setSource("field1", 1, "content", "foo")
-                .execute().actionGet();
-
-        // Update the first object and note context variables values
-        Map<String, Object> scriptParams = new HashMap<>();
-        scriptParams.put("delim", "_");
-        UpdateResponse updateResponse = client().prepareUpdate("test", "subtype1", "id1")
-                .setRouting("routing1")
-                .setScript(
-                        new Script("assert ctx._index == \"test\" : \"index should be \\\"test\\\"\"\n"
-                                +
-                                "assert ctx._type == \"subtype1\" : \"type should be \\\"subtype1\\\"\"\n" +
-                                "assert ctx._id == \"id1\" : \"id should be \\\"id1\\\"\"\n" +
-                                "assert ctx._version == 1 : \"version should be 1\"\n" +
-                                "assert ctx._parent == \"parentId1\" : \"parent should be \\\"parentId1\\\"\"\n" +
-                                "assert ctx._routing == \"routing1\" : \"routing should be \\\"routing1\\\"\"\n" +
-                                "assert ctx._timestamp == " + timestamp + " : \"timestamp should be " + timestamp + "\"\n" +
-                                // ttl has a 3-second leeway, because it's always counting down
-                                "assert ctx._ttl <= " + ttl + " : \"ttl should be <= " + ttl + " but was \" + ctx._ttl\n" +
-                                "assert ctx._ttl >= " + (ttl-3000) + " : \"ttl should be <= " + (ttl-3000) + " but was \" + ctx._ttl\n" +
-                                "ctx._source.content = ctx._source.content + delim + ctx._source.content;\n" +
-                                "ctx._source.field1 += 1;\n",
- ScriptService.ScriptType.INLINE, null, scriptParams))
-                .execute().actionGet();
-
-        assertEquals(2, updateResponse.getVersion());
-
-        GetResponse getResponse = client().prepareGet("test", "subtype1", "id1").setRouting("routing1").execute().actionGet();
-        assertEquals(2, getResponse.getSourceAsMap().get("field1"));
-        assertEquals("foo_foo", getResponse.getSourceAsMap().get("content"));
-
-        // Idem with the second object
-        scriptParams = new HashMap<>();
-        scriptParams.put("delim", "_");
-        updateResponse = client().prepareUpdate("test", "type1", "parentId1")
-                .setScript(
-                        new Script(
-                        "assert ctx._index == \"test\" : \"index should be \\\"test\\\"\"\n" +
-                        "assert ctx._type == \"type1\" : \"type should be \\\"type1\\\"\"\n" +
-                        "assert ctx._id == \"parentId1\" : \"id should be \\\"parentId1\\\"\"\n" +
-                        "assert ctx._version == 1 : \"version should be 1\"\n" +
-                        "assert ctx._parent == null : \"parent should be null\"\n" +
-                        "assert ctx._routing == null : \"routing should be null\"\n" +
-                        "assert ctx._timestamp == " + (timestamp - 1) + " : \"timestamp should be " + (timestamp - 1) + "\"\n" +
-                        "assert ctx._ttl == null : \"ttl should be null\"\n" +
-                        "ctx._source.content = ctx._source.content + delim + ctx._source.content;\n" +
-                        "ctx._source.field1 += 1;\n",
- ScriptService.ScriptType.INLINE, null, scriptParams))
-                .execute().actionGet();
-
-        assertEquals(2, updateResponse.getVersion());
-
-        getResponse = client().prepareGet("test", "type1", "parentId1").execute().actionGet();
-        assertEquals(1, getResponse.getSourceAsMap().get("field1"));
-        assertEquals("bar_bar", getResponse.getSourceAsMap().get("content"));
-    }
-
-    @Test
-    public void testConcurrentUpdateWithRetryOnConflict() throws Exception {
-        final boolean useBulkApi = randomBoolean();
-        createTestIndex();
-        ensureGreen();
-
-        int numberOfThreads = scaledRandomIntBetween(2,5);
-        final CountDownLatch latch = new CountDownLatch(numberOfThreads);
-        final CountDownLatch startLatch = new CountDownLatch(1);
-        final int numberOfUpdatesPerThread = scaledRandomIntBetween(100, 10000);
-        final List<Throwable> failures = new CopyOnWriteArrayList<>();
-        for (int i = 0; i < numberOfThreads; i++) {
-            Runnable r = new Runnable() {
-
-                @Override
-                public void run() {
-                    try {
-                        startLatch.await();
-                        for (int i = 0; i < numberOfUpdatesPerThread; i++) {
-                            if (useBulkApi) {
-                                UpdateRequestBuilder updateRequestBuilder = client().prepareUpdate(indexOrAlias(), "type1", Integer.toString(i))
-                                        .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null))
-                                        .setRetryOnConflict(Integer.MAX_VALUE)
-                                        .setUpsert(jsonBuilder().startObject().field("field", 1).endObject());
-                                client().prepareBulk().add(updateRequestBuilder).execute().actionGet();
-                            } else {
-                                client().prepareUpdate(indexOrAlias(), "type1", Integer.toString(i))
-                                        .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null))
-                                        .setRetryOnConflict(Integer.MAX_VALUE)
-                                        .setUpsert(jsonBuilder().startObject().field("field", 1).endObject())
-                                        .execute().actionGet();
-                            }
-                        }
-                    } catch (Throwable e) {
-                        failures.add(e);
-                    } finally {
-                        latch.countDown();
-                    }
-                }
-
-            };
-            new Thread(r).start();
-        }
-        startLatch.countDown();
-        latch.await();
-        for (Throwable throwable : failures) {
-            logger.info("Captured failure on concurrent update:", throwable);
-        }
-        assertThat(failures.size(), equalTo(0));
-        for (int i = 0; i < numberOfUpdatesPerThread; i++) {
-            GetResponse response = client().prepareGet("test", "type1", Integer.toString(i)).execute().actionGet();
-            assertThat(response.getId(), equalTo(Integer.toString(i)));
-            assertThat(response.isExists(), equalTo(true));
-            assertThat(response.getVersion(), equalTo((long) numberOfThreads));
-            assertThat((Integer) response.getSource().get("field"), equalTo(numberOfThreads));
-        }
-    }
-
-    @Test
-    public void stressUpdateDeleteConcurrency() throws Exception {
-        //We create an index with merging disabled so that deletes don't get merged away
-        assertAcked(prepareCreate("test")
-                .addMapping("type1", XContentFactory.jsonBuilder()
-                        .startObject()
-                        .startObject("type1")
-                        .startObject("_timestamp").field("enabled", true).endObject()
-                        .startObject("_ttl").field("enabled", true).endObject()
-                        .endObject()
-                        .endObject())
-                .setSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_ENABLED, false)));
-        ensureGreen();
-
-        final int numberOfThreads = scaledRandomIntBetween(3,5);
-        final int numberOfIdsPerThread = scaledRandomIntBetween(3,10);
-        final int numberOfUpdatesPerId = scaledRandomIntBetween(10,100);
-        final int retryOnConflict = randomIntBetween(0,1);
-        final CountDownLatch latch = new CountDownLatch(numberOfThreads);
-        final CountDownLatch startLatch = new CountDownLatch(1);
-        final List<Throwable> failures = new CopyOnWriteArrayList<>();
-
-        final class UpdateThread extends Thread {
-            final Map<Integer,Integer> failedMap = new HashMap<>();
-            final int numberOfIds;
-            final int updatesPerId;
-            final int maxUpdateRequests = numberOfIdsPerThread*numberOfUpdatesPerId;
-            final int maxDeleteRequests = numberOfIdsPerThread*numberOfUpdatesPerId;
-            private final Semaphore updateRequestsOutstanding = new Semaphore(maxUpdateRequests);
-            private final Semaphore deleteRequestsOutstanding = new Semaphore(maxDeleteRequests);
-
-            public UpdateThread(int numberOfIds, int updatesPerId) {
-                this.numberOfIds = numberOfIds;
-                this.updatesPerId = updatesPerId;
-            }
-
-            final class UpdateListener implements ActionListener<UpdateResponse> {
-                int id;
-
-                public UpdateListener(int id) {
-                    this.id = id;
-                }
-
-                @Override
-                public void onResponse(UpdateResponse updateResponse) {
-                    updateRequestsOutstanding.release(1);
-                }
-
-                @Override
-                public void onFailure(Throwable e) {
-                    synchronized (failedMap) {
-                        incrementMapValue(id, failedMap);
-                    }
-                    updateRequestsOutstanding.release(1);
-                }
-
-            }
-
-            final class DeleteListener implements ActionListener<DeleteResponse> {
-                int id;
-
-                public DeleteListener(int id) {
-                    this.id = id;
-                }
-
-                @Override
-                public void onResponse(DeleteResponse deleteResponse) {
-                    deleteRequestsOutstanding.release(1);
-                }
-
-                @Override
-                public void onFailure(Throwable e) {
-                    synchronized (failedMap) {
-                        incrementMapValue(id, failedMap);
-                    }
-                    deleteRequestsOutstanding.release(1);
-                }
-            }
-
-            @Override
-            public void run(){
-                try {
-                    startLatch.await();
-                    boolean hasWaitedForNoNode = false;
-                    for (int j = 0; j < numberOfIds; j++) {
-                        for (int k = 0; k < numberOfUpdatesPerId; ++k) {
-                            updateRequestsOutstanding.acquire();
-                            try {
-                                UpdateRequest ur = client().prepareUpdate("test", "type1", Integer.toString(j))
-                                        .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null))
-                                        .setRetryOnConflict(retryOnConflict)
-                                        .setUpsert(jsonBuilder().startObject().field("field", 1).endObject())
-                                        .request();
-                                client().update(ur, new UpdateListener(j));
-                            } catch (NoNodeAvailableException nne) {
-                                updateRequestsOutstanding.release();
-                                synchronized (failedMap) {
-                                    incrementMapValue(j, failedMap);
-                                }
-                                if (hasWaitedForNoNode) {
-                                    throw nne;
-                                }
-                                logger.warn("Got NoNodeException waiting for 1 second for things to recover.");
-                                hasWaitedForNoNode = true;
-                                Thread.sleep(1000);
-                            }
-
-                            try {
-                                deleteRequestsOutstanding.acquire();
-                                DeleteRequest dr = client().prepareDelete("test", "type1", Integer.toString(j)).request();
-                                client().delete(dr, new DeleteListener(j));
-                            } catch (NoNodeAvailableException nne) {
-                                deleteRequestsOutstanding.release();
-                                synchronized (failedMap) {
-                                    incrementMapValue(j, failedMap);
-                                }
-                                if (hasWaitedForNoNode) {
-                                    throw nne;
-                                }
-                                logger.warn("Got NoNodeException waiting for 1 second for things to recover.");
-                                hasWaitedForNoNode = true;
-                                Thread.sleep(1000); //Wait for no-node to clear
-                            }
-                        }
-                    }
-                } catch (Throwable e) {
-                    logger.error("Something went wrong", e);
-                    failures.add(e);
-                } finally {
-                    try {
-                        waitForOutstandingRequests(TimeValue.timeValueSeconds(60), updateRequestsOutstanding, maxUpdateRequests, "Update");
-                        waitForOutstandingRequests(TimeValue.timeValueSeconds(60), deleteRequestsOutstanding, maxDeleteRequests, "Delete");
-                    } catch (ElasticsearchTimeoutException ete) {
-                        failures.add(ete);
-                    }
-                    latch.countDown();
-                }
-            }
-
-            private void incrementMapValue(int j, Map<Integer,Integer> map) {
-                if (!map.containsKey(j)) {
-                    map.put(j, 0);
-                }
-                map.put(j, map.get(j) + 1);
-            }
-
-            private void waitForOutstandingRequests(TimeValue timeOut, Semaphore requestsOutstanding, int maxRequests, String name) {
-                long start = System.currentTimeMillis();
-                do {
-                    long msRemaining = timeOut.getMillis() - (System.currentTimeMillis() - start);
-                    logger.info("[{}] going to try and acquire [{}] in [{}]ms [{}] available to acquire right now",name, maxRequests,msRemaining, requestsOutstanding.availablePermits());
-                    try {
-                        requestsOutstanding.tryAcquire(maxRequests, msRemaining, TimeUnit.MILLISECONDS );
-                        return;
-                    } catch (InterruptedException ie) {
-                        //Just keep swimming
-                    }
-                } while ((System.currentTimeMillis() - start) < timeOut.getMillis());
-                throw new ElasticsearchTimeoutException("Requests were still outstanding after the timeout [" + timeOut + "] for type [" + name + "]" );
-            }
-        }
-        final List<UpdateThread> threads = new ArrayList<>();
-
-        for (int i = 0; i < numberOfThreads; i++) {
-            UpdateThread ut = new UpdateThread(numberOfIdsPerThread, numberOfUpdatesPerId);
-            ut.start();
-            threads.add(ut);
-        }
-
-        startLatch.countDown();
-        latch.await();
-
-        for (UpdateThread ut : threads){
-            ut.join(); //Threads should have finished because of the latch.await
-        }
-
-        //If are no errors every request received a response otherwise the test would have timedout
-        //aquiring the request outstanding semaphores.
-        for (Throwable throwable : failures) {
-            logger.info("Captured failure on concurrent update:", throwable);
-        }
-
-        assertThat(failures.size(), equalTo(0));
-
-        //Upsert all the ids one last time to make sure they are available at get time
-        //This means that we add 1 to the expected versions and attempts
-        //All the previous operations should be complete or failed at this point
-        for (int i = 0; i < numberOfIdsPerThread; ++i) {
-            UpdateResponse ur = client().prepareUpdate("test", "type1", Integer.toString(i))
-                    .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null))
-                .setRetryOnConflict(Integer.MAX_VALUE)
-                .setUpsert(jsonBuilder().startObject().field("field", 1).endObject())
-                .execute().actionGet();
-        }
-
-        refresh();
-
-        for (int i = 0; i < numberOfIdsPerThread; ++i) {
-            int totalFailures = 0;
-            GetResponse response = client().prepareGet("test", "type1", Integer.toString(i)).execute().actionGet();
-            if (response.isExists()) {
-                assertThat(response.getId(), equalTo(Integer.toString(i)));
-                int expectedVersion = (numberOfThreads * numberOfUpdatesPerId * 2) + 1;
-                for (UpdateThread ut : threads) {
-                    if (ut.failedMap.containsKey(i)) {
-                        totalFailures += ut.failedMap.get(i);
-                    }
-                }
-                expectedVersion -= totalFailures;
-                logger.error("Actual version [{}] Expected version [{}] Total failures [{}]", response.getVersion(), expectedVersion, totalFailures);
-                assertThat(response.getVersion(), equalTo((long) expectedVersion));
-                assertThat(response.getVersion() + totalFailures,
-                        equalTo(
-                                (long)((numberOfUpdatesPerId * numberOfThreads * 2) + 1)
-                ));
-            }
-        }
-    }
-
-    private static String indexOrAlias() {
-        return randomBoolean() ? "test" : "alias";
-    }
-}
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java
index c4d9366..a0d2c78 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java
@@ -80,7 +80,6 @@
   renamed:    core/src/test/java/org/elasticsearch/search/aggregations/metrics/TDigestPercentilesIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentilesTests.java
   renamed:    core/src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TopHitsTests.java
   renamed:    core/src/test/java/org/elasticsearch/index/mapper/TransformOnIndexMapperIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TransformOnIndexMapperTests.java
-  renamed:    core/src/test/java/org/elasticsearch/update/UpdateIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/UpdateTests.java
   renamed:    core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ValueCountTests.java
   renamed:    core/src/main/java/org/elasticsearch/script/groovy/GroovyScriptCompilationException.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyRestIT.java
   renamed:    core/src/test/java/org/elasticsearch/script/GroovyScriptIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyScriptTests.java
diff --git a/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonSecurityTests.java b/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonSecurityTests.java
index a753ffa..fd60607 100644
--- a/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonSecurityTests.java
+++ b/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonSecurityTests.java
@@ -68,7 +68,10 @@ public class PythonSecurityTests extends ESTestCase {
             fail("did not get expected exception");
         } catch (PyException expected) {
             Throwable cause = expected.getCause();
-            assertNotNull("null cause for exception: " + expected, cause);
+            // TODO: fix jython localization bugs: https://github.com/elastic/elasticsearch/issues/13967
+            // this is the correct assert:
+            // assertNotNull("null cause for exception: " + expected, cause);
+            assertNotNull("null cause for exception", cause);
             assertTrue("unexpected exception: " + cause, cause instanceof SecurityException);
         }
     }
diff --git a/plugins/pom.xml b/plugins/pom.xml
index d248c7f..d080a55 100644
--- a/plugins/pom.xml
+++ b/plugins/pom.xml
@@ -388,10 +388,10 @@
         <module>analysis-phonetic</module>
         <module>analysis-smartcn</module>
         <module>analysis-stempel</module>
-        <module>cloud-gce</module>
         <module>delete-by-query</module>
         <module>discovery-azure</module>
         <module>discovery-ec2</module>
+        <module>discovery-gce</module>
         <module>discovery-multicast</module>
         <module>lang-expression</module>
         <module>lang-groovy</module>
diff --git a/pom.xml b/pom.xml
index f966923..7242c5c 100644
--- a/pom.xml
+++ b/pom.xml
@@ -986,8 +986,7 @@
                             <headerDefinition>${elasticsearch.license.headerDefinition}</headerDefinition>
                         </headerDefinitions>
                         <includes>
-                            <include>src/main/java/org/elasticsearch/**/*.java</include>
-                            <include>src/test/java/org/elasticsearch/**/*.java</include>
+                            <include>src/**/*.java</include>
                         </includes>
                     </configuration>
                     <executions>
diff --git a/qa/smoke-test-plugins/pom.xml b/qa/smoke-test-plugins/pom.xml
index 238ea9c..f9d4b79 100644
--- a/qa/smoke-test-plugins/pom.xml
+++ b/qa/smoke-test-plugins/pom.xml
@@ -271,7 +271,7 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>cloud-gce</artifactId>
+                   <artifactId>discovery-gce</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
diff --git a/qa/vagrant/pom.xml b/qa/vagrant/pom.xml
index c0f7535..e303da5 100644
--- a/qa/vagrant/pom.xml
+++ b/qa/vagrant/pom.xml
@@ -143,7 +143,7 @@
                                 </artifactItem>
                                 <artifactItem>
                                     <groupId>org.elasticsearch.plugin</groupId>
-                                    <artifactId>cloud-gce</artifactId>
+                                    <artifactId>discovery-gce</artifactId>
                                     <version>${elasticsearch.version}</version>
                                     <type>zip</type>
                                 </artifactItem>
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/os_package.bash b/qa/vagrant/src/test/resources/packaging/scripts/os_package.bash
index 4907cdf..f48532c 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/os_package.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/os_package.bash
@@ -72,38 +72,39 @@ verify_package_installation() {
 
     getent group elasticsearch
 
-    assert_file "$ESHOME" d root 755
-    assert_file "$ESHOME/bin" d root 755
-    assert_file "$ESHOME/lib" d root 755
-    assert_file "$ESCONFIG" d root 755
-    assert_file "$ESCONFIG/elasticsearch.yml" f root 644
-    assert_file "$ESCONFIG/logging.yml" f root 644
-    assert_file "$ESDATA" d elasticsearch 755
-    assert_file "$ESLOG" d elasticsearch 755
-    assert_file "$ESPLUGINS" d elasticsearch 755
-    assert_file "$ESPIDDIR" d elasticsearch 755
-    assert_file "$ESHOME/NOTICE.txt" f root 644
-    assert_file "$ESHOME/README.textile" f root 644
+    assert_file "$ESHOME" d root root 755
+    assert_file "$ESHOME/bin" d root root 755
+    assert_file "$ESHOME/lib" d root root 755
+    assert_file "$ESCONFIG" d root elasticsearch 750
+    assert_file "$ESCONFIG/elasticsearch.yml" f root elasticsearch 750
+    assert_file "$ESCONFIG/logging.yml" f root elasticsearch 750
+    assert_file "$ESSCRIPTS" d root elasticsearch 750
+    assert_file "$ESDATA" d elasticsearch elasticsearch 755
+    assert_file "$ESLOG" d elasticsearch elasticsearch 755
+    assert_file "$ESPLUGINS" d elasticsearch elasticsearch 755
+    assert_file "$ESPIDDIR" d elasticsearch elasticsearch 755
+    assert_file "$ESHOME/NOTICE.txt" f root root 644
+    assert_file "$ESHOME/README.textile" f root root 644
 
     if is_dpkg; then
         # Env file
-        assert_file "/etc/default/elasticsearch" f root 644
+        assert_file "/etc/default/elasticsearch" f root root 644
 
         # Doc files
-        assert_file "/usr/share/doc/elasticsearch" d root 755
-        assert_file "/usr/share/doc/elasticsearch/copyright" f root 644
+        assert_file "/usr/share/doc/elasticsearch" d root root 755
+        assert_file "/usr/share/doc/elasticsearch/copyright" f root root 644
     fi
 
     if is_rpm; then
         # Env file
-        assert_file "/etc/sysconfig/elasticsearch" f root 644
+        assert_file "/etc/sysconfig/elasticsearch" f root root 644
         # License file
-        assert_file "/usr/share/elasticsearch/LICENSE.txt" f root 644
+        assert_file "/usr/share/elasticsearch/LICENSE.txt" f root root 644
     fi
 
     if is_systemd; then
-        assert_file "/usr/lib/systemd/system/elasticsearch.service" f root 644
-        assert_file "/usr/lib/tmpfiles.d/elasticsearch.conf" f root 644
-        assert_file "/usr/lib/sysctl.d/elasticsearch.conf" f root 644
+        assert_file "/usr/lib/systemd/system/elasticsearch.service" f root root 644
+        assert_file "/usr/lib/tmpfiles.d/elasticsearch.conf" f root root 644
+        assert_file "/usr/lib/sysctl.d/elasticsearch.conf" f root root 644
     fi
 }
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash b/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
index 7f24b8a..599f6ba 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
@@ -150,7 +150,8 @@ assert_file() {
     local file="$1"
     local type=$2
     local user=$3
-    local privileges=$4
+    local group=$4
+    local privileges=$5
 
     assert_file_exist "$file"
 
@@ -167,6 +168,11 @@ assert_file() {
         [ "$realuser" = "$user" ]
     fi
 
+    if [ "x$group" != "x" ]; then
+        realgroup=$(find "$file" -maxdepth 0 -printf "%g")
+        [ "$realgroup" = "$group" ]
+    fi
+
     if [ "x$privileges" != "x" ]; then
         realprivileges=$(find "$file" -maxdepth 0 -printf "%m")
         [ "$realprivileges" = "$privileges" ]
@@ -239,36 +245,13 @@ clean_before_test() {
 start_elasticsearch_service() {
     local desiredStatus=${1:-green}
 
-    if [ -f "/tmp/elasticsearch/bin/elasticsearch" ]; then
-        # su and the Elasticsearch init script work together to break bats.
-        # sudo isolates bats enough from the init script so everything continues
-        # to tick along
-        sudo -u elasticsearch /tmp/elasticsearch/bin/elasticsearch -d \
-            -p /tmp/elasticsearch/elasticsearch.pid
-    elif is_systemd; then
-        run systemctl daemon-reload
-        [ "$status" -eq 0 ]
-
-        run systemctl enable elasticsearch.service
-        [ "$status" -eq 0 ]
-
-        run systemctl is-enabled elasticsearch.service
-        [ "$status" -eq 0 ]
-
-        run systemctl start elasticsearch.service
-        [ "$status" -eq 0 ]
-
-    elif is_sysvinit; then
-        run service elasticsearch start
-        [ "$status" -eq 0 ]
-    fi
+    run_elasticsearch_service 0
 
     wait_for_elasticsearch_status $desiredStatus
 
     if [ -r "/tmp/elasticsearch/elasticsearch.pid" ]; then
         pid=$(cat /tmp/elasticsearch/elasticsearch.pid)
         [ "x$pid" != "x" ] && [ "$pid" -gt 0 ]
-
         echo "Looking for elasticsearch pid...."
         ps $pid
     elif is_systemd; then
@@ -284,6 +267,64 @@ start_elasticsearch_service() {
     fi
 }
 
+# Start elasticsearch
+# $1 expected status code
+# $2 additional command line args
+run_elasticsearch_service() {
+    local expectedStatus=$1
+    local commandLineArgs=$2
+    # Set the CONF_DIR setting in case we start as a service
+    if [ ! -z "$CONF_DIR" ] ; then
+        if is_dpkg ; then
+            echo "CONF_DIR=$CONF_DIR" >> /etc/default/elasticsearch;
+        elif is_rpm; then
+            echo "CONF_DIR=$CONF_DIR" >> /etc/sysconfig/elasticsearch;
+        fi
+    fi
+
+    if [ -f "/tmp/elasticsearch/bin/elasticsearch" ]; then
+        if [ -z "$CONF_DIR" ]; then
+            local CONF_DIR=""
+        fi
+        # we must capture the exit code to compare so we don't want to start as background process in case we expect something other than 0
+        local background=""
+        local timeoutCommand=""
+        if [ "$expectedStatus" = 0 ]; then
+            background="-d"
+        else
+            timeoutCommand="timeout 60s "
+        fi
+        # su and the Elasticsearch init script work together to break bats.
+        # sudo isolates bats enough from the init script so everything continues
+        # to tick along
+        run sudo -u elasticsearch bash <<BASH
+# If jayatana is installed then we try to use it. Elasticsearch should ignore it even when we try.
+# If it doesn't ignore it then Elasticsearch will fail to start because of security errors.
+# This line is attempting to emulate the on login behavior of /usr/share/upstart/sessions/jayatana.conf
+[ -f /usr/share/java/jayatanaag.jar ] && export JAVA_TOOL_OPTIONS="-javaagent:/usr/share/java/jayatanaag.jar"
+# And now we can start Elasticsearch normally, in the background (-d) and with a pidfile (-p).
+$timeoutCommand/tmp/elasticsearch/bin/elasticsearch $background -p /tmp/elasticsearch/elasticsearch.pid -Des.path.conf=$CONF_DIR $commandLineArgs
+BASH
+        [ "$status" -eq "$expectedStatus" ]
+    elif is_systemd; then
+        run systemctl daemon-reload
+        [ "$status" -eq 0 ]
+
+        run systemctl enable elasticsearch.service
+        [ "$status" -eq 0 ]
+
+        run systemctl is-enabled elasticsearch.service
+        [ "$status" -eq 0 ]
+
+        run systemctl start elasticsearch.service
+        [ "$status" -eq "$expectedStatus" ]
+
+    elif is_sysvinit; then
+        run service elasticsearch start
+        [ "$status" -eq "$expectedStatus" ]
+    fi
+}
+
 stop_elasticsearch_service() {
     if [ -r "/tmp/elasticsearch/elasticsearch.pid" ]; then
         pid=$(cat /tmp/elasticsearch/elasticsearch.pid)
@@ -319,7 +360,7 @@ wait_for_elasticsearch_status() {
           if [ -e "$ESLOG/elasticsearch.log" ]; then
               cat "$ESLOG/elasticsearch.log"
           else
-              echo "The elasticsearch log doesn't exist. Maybe /vag/log/messages has something:"
+              echo "The elasticsearch log doesn't exist. Maybe /var/log/messages has something:"
               tail -n20 /var/log/messages
           fi
           false
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash b/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
index a199cb9..bd9e28e 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
@@ -83,6 +83,35 @@ else
     }
 fi
 
+@test "[$GROUP] install jvm-example plugin with a custom CONFIG_FILE and check failure" {
+    local relativePath=${1:-$(readlink -m jvm-example-*.zip)}
+    CONF_FILE="$ESCONFIG/elasticsearch.yml" run sudo -E -u $ESPLUGIN_COMMAND_USER "$ESHOME/bin/plugin" install "file://$relativePath"
+    # this should fail because CONF_FILE is no longer supported
+    [ $status = 1 ]
+    CONF_FILE="$ESCONFIG/elasticsearch.yml" run sudo -E -u $ESPLUGIN_COMMAND_USER "$ESHOME/bin/plugin" remove jvm-example
+    echo "status is $status"
+    [ $status = 1 ]
+}
+
+@test "[$GROUP] start elasticsearch with a custom CONFIG_FILE and check failure" {
+    local CONF_FILE="$ESCONFIG/elasticsearch.yml"
+
+    if is_dpkg; then
+        echo "CONF_FILE=$CONF_FILE" >> /etc/default/elasticsearch;
+    elif is_rpm; then
+        echo "CONF_FILE=$CONF_FILE" >> /etc/sysconfig/elasticsearch;
+    fi
+
+    run_elasticsearch_service 1 -Des.default.config="$CONF_FILE"
+
+    # remove settings again otherwise cleaning up before next testrun will fail
+    if is_dpkg ; then
+        sudo sed -i '/CONF_FILE/d' /etc/default/elasticsearch
+    elif is_rpm; then
+        sudo sed -i '/CONF_FILE/d' /etc/sysconfig/elasticsearch
+    fi
+}
+
 @test "[$GROUP] install jvm-example plugin with a custom path.plugins" {
     # Clean up after the last time this test was run
     rm -rf /tmp/plugins.*
@@ -111,6 +140,9 @@ fi
     move_config
 
     CONF_DIR="$ESCONFIG" install_jvm_example
+    CONF_DIR="$ESCONFIG" start_elasticsearch_service
+    diff  <(curl -s localhost:9200/_cat/configured_example | sed 's/ //g') <(echo "foo")
+    stop_elasticsearch_service
     CONF_DIR="$ESCONFIG" remove_jvm_example
 }
 
@@ -172,7 +204,7 @@ fi
 }
 
 @test "[$GROUP] install gce plugin" {
-    install_and_check_plugin cloud gce google-api-client-*.jar
+    install_and_check_plugin discovery gce google-api-client-*.jar
 }
 
 @test "[$GROUP] install delete by query plugin" {
@@ -276,7 +308,7 @@ fi
 }
 
 @test "[$GROUP] remove gce plugin" {
-    remove_plugin cloud-gce
+    remove_plugin discovery-gce
 }
 
 @test "[$GROUP] remove delete by query plugin" {
@@ -357,21 +389,38 @@ fi
     local relativePath=${1:-$(readlink -m jvm-example-*.zip)}
     sudo -E -u $ESPLUGIN_COMMAND_USER "$ESHOME/bin/plugin" install "file://$relativePath" > /tmp/plugin-cli-output
     local loglines=$(cat /tmp/plugin-cli-output | wc -l)
-    [ "$loglines" = "6" ] || {
-        echo "Expected 6 lines but the output was:"
-        cat /tmp/plugin-cli-output
-        false
-    }
+    if [ "$GROUP" == "TAR PLUGINS" ]; then
+    # tar extraction does not create the plugins directory so the plugin tool will print an additional line that the directory will be created
+        [ "$loglines" -eq "7" ] || {
+            echo "Expected 7 lines but the output was:"
+            cat /tmp/plugin-cli-output
+            false
+        }
+    else
+        [ "$loglines" -eq "6" ] || {
+            echo "Expected 6 lines but the output was:"
+            cat /tmp/plugin-cli-output
+            false
+        }
+    fi
     remove_jvm_example
 
     local relativePath=${1:-$(readlink -m jvm-example-*.zip)}
     sudo -E -u $ESPLUGIN_COMMAND_USER "$ESHOME/bin/plugin" install "file://$relativePath" -Des.logger.level=DEBUG > /tmp/plugin-cli-output
     local loglines=$(cat /tmp/plugin-cli-output | wc -l)
-    [ "$loglines" -gt "6" ] || {
-        echo "Expected more than 6 lines but the output was:"
-        cat /tmp/plugin-cli-output
-        false
-    }
+    if [ "$GROUP" == "TAR PLUGINS" ]; then
+        [ "$loglines" -gt "7" ] || {
+            echo "Expected more than 7 lines but the output was:"
+            cat /tmp/plugin-cli-output
+            false
+        }
+    else
+        [ "$loglines" -gt "6" ] || {
+            echo "Expected more than 6 lines but the output was:"
+            cat /tmp/plugin-cli-output
+            false
+        }
+    fi
     remove_jvm_example
 }
 
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/cluster.stats.json b/rest-api-spec/src/main/resources/rest-api-spec/api/cluster.stats.json
index 42c13dc..2bccb20 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/cluster.stats.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/cluster.stats.json
@@ -20,6 +20,10 @@
             "type": "boolean",
             "description": "Whether to return time and byte values in human-readable format.",
             "default": false
+        },
+        "timeout": {
+          "type" : "time",
+          "description" : "Explicit operation timeout"
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get.json b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get.json
index 14fae15..5c426f9 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get.json
@@ -13,7 +13,8 @@
         },
         "feature":{
           "type":"list",
-          "description":"A comma-separated list of features"
+          "description":"A comma-separated list of features",
+          "options": ["_settings", "_mappings", "_warmers", "_aliases"]
         }
       },
       "params":{
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.hot_threads.json b/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.hot_threads.json
index 5b49782..854cde1 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.hot_threads.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.hot_threads.json
@@ -24,7 +24,7 @@
           "type" : "number",
           "description" : "Specify the number of threads to provide information for (default: 3)"
         },
-	"ignore_idle_threads": {
+        "ignore_idle_threads": {
           "type" : "boolean",
           "description" : "Don't show threads that are in known-idle places, such as waiting on a socket select or pulling from an empty task queue (default: true)"
         },
@@ -32,6 +32,10 @@
           "type" : "enum",
           "options" : ["cpu", "wait", "block"],
           "description" : "The type to sample (default: cpu)"
+        },
+        "timeout": {
+          "type" : "time",
+          "description" : "Explicit operation timeout"
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.info.json b/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.info.json
index d8044c8..43be35a 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.info.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.info.json
@@ -25,6 +25,10 @@
             "type": "boolean",
             "description": "Whether to return time and byte values in human-readable format.",
             "default": false
+        },
+        "timeout": {
+          "type" : "time",
+          "description" : "Explicit operation timeout"
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.stats.json b/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.stats.json
index 5eef2c1..8742941 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.stats.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.stats.json
@@ -59,6 +59,10 @@
         "types" : {
           "type" : "list",
           "description" : "A comma-separated list of document types for the `indexing` index metric"
+        },
+        "timeout": {
+          "type" : "time",
+          "description" : "Explicit operation timeout"
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/update.json b/rest-api-spec/src/main/resources/rest-api-spec/api/update.json
index 20fc352..37a04cb 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/update.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/update.json
@@ -82,6 +82,10 @@
           "type": "enum",
           "options": ["internal", "force"],
           "description": "Specific version type"
+        },
+        "detect_noop": {
+          "type": "boolean",
+          "description": "Specifying as true will cause Elasticsearch to check if there are changes and, if there arent, turn the update request into a noop."
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/create/35_external_version.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/create/35_external_version.yaml
deleted file mode 100644
index 8ee11b0..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/create/35_external_version.yaml
+++ /dev/null
@@ -1,33 +0,0 @@
----
-"External version":
-
- - do:
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external
-          version:        5
-
- - match:   { _version: 5}
-
- - do:
-      catch:             conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external
-          version:        5
-
- - do:
-      catch:              conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external
-          version:        6
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/create/36_external_gte_version.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/create/36_external_gte_version.yaml
deleted file mode 100644
index febb7a5..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/create/36_external_gte_version.yaml
+++ /dev/null
@@ -1,33 +0,0 @@
----
-"External version":
-
- - do:
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external_gte
-          version:        5
-
- - match:   { _version: 5}
-
- - do:
-      catch:             conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external_gte
-          version:        5
-
- - do:
-      catch:              conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external_gte
-          version:        6
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/create/37_force_version.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/create/37_force_version.yaml
deleted file mode 100644
index 393d16f..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/create/37_force_version.yaml
+++ /dev/null
@@ -1,33 +0,0 @@
----
-"External version":
-
- - do:
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   force
-          version:        5
-
- - match:   { _version: 5}
-
- - do:
-      catch:             conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   force
-          version:        5
-
- - do:
-      catch:              conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   force
-          version:        6
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/update/30_internal_version.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/update/30_internal_version.yaml
index 3f13b09..17c4806 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/update/30_internal_version.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/update/30_internal_version.yaml
@@ -2,7 +2,7 @@
 "Internal version":
 
  - do:
-      catch:        conflict
+      catch:        missing
       update:
           index:    test_1
           type:     test
@@ -10,7 +10,14 @@
           version:  1
           body:
             doc:    { foo: baz }
-            upsert: { foo: bar }
+
+ - do:
+      index:
+          index:    test_1
+          type:     test
+          id:       1
+          body:
+            doc:    { foo: baz }
 
  - do:
       catch:        conflict
@@ -18,7 +25,6 @@
           index:    test_1
           type:     test
           id:       1
-          version:  1
+          version:  2
           body:
             doc:    { foo: baz }
-            upsert: { foo: bar }
