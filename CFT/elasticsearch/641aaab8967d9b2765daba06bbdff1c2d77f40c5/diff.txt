diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
index a93bdb4..6d6bbd6 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
@@ -49,7 +49,6 @@ import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchService;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.internal.DefaultSearchContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.internal.ShardSearchLocalRequest;
@@ -76,20 +75,17 @@ public class TransportValidateQueryAction extends TransportBroadcastAction<Valid
 
     private final BigArrays bigArrays;
 
-    private final FetchPhase fetchPhase;
-
     @Inject
     public TransportValidateQueryAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
-            TransportService transportService, IndicesService indicesService, ScriptService scriptService,
-            PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, ActionFilters actionFilters,
-            IndexNameExpressionResolver indexNameExpressionResolver, FetchPhase fetchPhase) {
+                                        TransportService transportService, IndicesService indicesService,
+                                        ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
+                                        BigArrays bigArrays, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {
         super(settings, ValidateQueryAction.NAME, threadPool, clusterService, transportService, actionFilters,
                 indexNameExpressionResolver, ValidateQueryRequest::new, ShardValidateQueryRequest::new, ThreadPool.Names.SEARCH);
         this.indicesService = indicesService;
         this.scriptService = scriptService;
         this.pageCacheRecycler = pageCacheRecycler;
         this.bigArrays = bigArrays;
-        this.fetchPhase = fetchPhase;
     }
 
     @Override
@@ -175,9 +171,11 @@ public class TransportValidateQueryAction extends TransportBroadcastAction<Valid
         Engine.Searcher searcher = indexShard.acquireSearcher("validate_query");
 
         DefaultSearchContext searchContext = new DefaultSearchContext(0,
-                new ShardSearchLocalRequest(request.types(), request.nowInMillis(), request.filteringAliases()), null, searcher,
-                indexService, indexShard, scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(),
-                parseFieldMatcher, SearchService.NO_TIMEOUT, fetchPhase);
+                new ShardSearchLocalRequest(request.types(), request.nowInMillis(), request.filteringAliases()),
+                null, searcher, indexService, indexShard,
+                scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher,
+                SearchService.NO_TIMEOUT
+        );
         SearchContext.setCurrent(searchContext);
         try {
             searchContext.parsedQuery(queryShardContext.toQuery(request.query()));
diff --git a/core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java b/core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java
index 1a91b89..7b6253c 100644
--- a/core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java
+++ b/core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java
@@ -44,7 +44,6 @@ import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchService;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.internal.DefaultSearchContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.internal.ShardSearchLocalRequest;
@@ -69,20 +68,17 @@ public class TransportExplainAction extends TransportSingleShardAction<ExplainRe
 
     private final BigArrays bigArrays;
 
-    private final FetchPhase fetchPhase;
-
     @Inject
     public TransportExplainAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
-            TransportService transportService, IndicesService indicesService, ScriptService scriptService,
-            PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, ActionFilters actionFilters,
-            IndexNameExpressionResolver indexNameExpressionResolver, FetchPhase fetchPhase) {
+                                  TransportService transportService, IndicesService indicesService,
+                                  ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
+                                  BigArrays bigArrays, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {
         super(settings, ExplainAction.NAME, threadPool, clusterService, transportService, actionFilters, indexNameExpressionResolver,
                 ExplainRequest::new, ThreadPool.Names.GET);
         this.indicesService = indicesService;
         this.scriptService = scriptService;
         this.pageCacheRecycler = pageCacheRecycler;
         this.bigArrays = bigArrays;
-        this.fetchPhase = fetchPhase;
     }
 
     @Override
@@ -115,10 +111,13 @@ public class TransportExplainAction extends TransportSingleShardAction<ExplainRe
             return new ExplainResponse(shardId.getIndex(), request.type(), request.id(), false);
         }
 
-        SearchContext context = new DefaultSearchContext(0,
-                new ShardSearchLocalRequest(new String[] { request.type() }, request.nowInMillis, request.filteringAlias()), null,
-                result.searcher(), indexService, indexShard, scriptService, pageCacheRecycler, bigArrays,
-                threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher, SearchService.NO_TIMEOUT, fetchPhase);
+        SearchContext context = new DefaultSearchContext(
+                0, new ShardSearchLocalRequest(new String[]{request.type()}, request.nowInMillis, request.filteringAlias()),
+                null, result.searcher(), indexService, indexShard,
+                scriptService, pageCacheRecycler,
+                bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher,
+                SearchService.NO_TIMEOUT
+        );
         SearchContext.setCurrent(context);
 
         try {
diff --git a/core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java
index 2a3b7ec..e4b3a04 100644
--- a/core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java
@@ -26,8 +26,7 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.sort.SortBuilder;
 
@@ -153,26 +152,16 @@ public class PercolateRequestBuilder extends BroadcastOperationRequestBuilder<Pe
     }
 
     /**
-     * Delegates to
-     * {@link PercolateSourceBuilder#addAggregation(AggregatorFactory)}
+     * Delegates to {@link PercolateSourceBuilder#addAggregation(AbstractAggregationBuilder)}
      */
-    public PercolateRequestBuilder addAggregation(AggregatorFactory<?> aggregationBuilder) {
+    public PercolateRequestBuilder addAggregation(AbstractAggregationBuilder aggregationBuilder) {
         sourceBuilder().addAggregation(aggregationBuilder);
         return this;
     }
 
     /**
-     * Delegates to
-     * {@link PercolateSourceBuilder#addAggregation(PipelineAggregatorFactory)}
-     */
-    public PercolateRequestBuilder addAggregation(PipelineAggregatorFactory aggregationBuilder) {
-        sourceBuilder().addAggregation(aggregationBuilder);
-        return this;
-    }
-
-    /**
-     * Sets the percolate request definition directly on the request. This will
-     * overwrite any definitions set by any of the delegate methods.
+     * Sets the percolate request definition directly on the request.
+     * This will overwrite any definitions set by any of the delegate methods.
      */
     public PercolateRequestBuilder setSource(PercolateSourceBuilder source) {
         sourceBuilder = source;
diff --git a/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java b/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java
index fe625e6..b080039 100644
--- a/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java
@@ -29,8 +29,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.sort.ScoreSortBuilder;
 import org.elasticsearch.search.sort.SortBuilder;
@@ -52,8 +51,7 @@ public class PercolateSourceBuilder extends ToXContentToBytes {
     private List<SortBuilder> sorts;
     private Boolean trackScores;
     private HighlightBuilder highlightBuilder;
-    private List<AggregatorFactory<?>> aggregationFactorys;
-    private List<PipelineAggregatorFactory> pipelineAggregationFactorys;
+    private List<AbstractAggregationBuilder> aggregations;
 
     /**
      * Sets the document to run the percolate queries against.
@@ -125,22 +123,11 @@ public class PercolateSourceBuilder extends ToXContentToBytes {
     /**
      * Add an aggregation definition.
      */
-    public PercolateSourceBuilder addAggregation(AggregatorFactory<?> aggregationBuilder) {
-        if (aggregationFactorys == null) {
-            aggregationFactorys = new ArrayList<>();
+    public PercolateSourceBuilder addAggregation(AbstractAggregationBuilder aggregationBuilder) {
+        if (aggregations == null) {
+            aggregations = new ArrayList<>();
         }
-        aggregationFactorys.add(aggregationBuilder);
-        return this;
-    }
-
-    /**
-     * Add an aggregation definition.
-     */
-    public PercolateSourceBuilder addAggregation(PipelineAggregatorFactory aggregationBuilder) {
-        if (pipelineAggregationFactorys == null) {
-            pipelineAggregationFactorys = new ArrayList<>();
-        }
-        pipelineAggregationFactorys.add(aggregationBuilder);
+        aggregations.add(aggregationBuilder);
         return this;
     }
 
@@ -172,18 +159,11 @@ public class PercolateSourceBuilder extends ToXContentToBytes {
         if (highlightBuilder != null) {
             highlightBuilder.toXContent(builder, params);
         }
-        if (aggregationFactorys != null || pipelineAggregationFactorys != null) {
+        if (aggregations != null) {
             builder.field("aggregations");
             builder.startObject();
-            if (aggregationFactorys != null) {
-                for (AggregatorFactory<?> aggregation : aggregationFactorys) {
-                    aggregation.toXContent(builder, params);
-                }
-            }
-            if (pipelineAggregationFactorys != null) {
-                for (PipelineAggregatorFactory aggregation : pipelineAggregationFactorys) {
-                    aggregation.toXContent(builder, params);
-                }
+            for (AbstractAggregationBuilder aggregation : aggregations) {
+                aggregation.toXContent(builder, params);
             }
             builder.endObject();
         }
diff --git a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
index 9e2c053..4c538e5 100644
--- a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
@@ -28,8 +28,7 @@ import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
 import org.elasticsearch.search.highlight.HighlightBuilder;
@@ -363,17 +362,9 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     }
 
     /**
-     * Adds an aggregation to the search operation.
+     * Adds an get to the search operation.
      */
-    public SearchRequestBuilder addAggregation(AggregatorFactory<?> aggregation) {
-        sourceBuilder().aggregation(aggregation);
-        return this;
-    }
-
-    /**
-     * Adds an aggregation to the search operation.
-     */
-    public SearchRequestBuilder addAggregation(PipelineAggregatorFactory aggregation) {
+    public SearchRequestBuilder addAggregation(AbstractAggregationBuilder aggregation) {
         sourceBuilder().aggregation(aggregation);
         return this;
     }
@@ -400,27 +391,27 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
 
     /**
      * Clears all rescorers on the builder and sets the first one.  To use multiple rescore windows use
-     * {@link #addRescorer(org.elasticsearch.search.rescore.RescoreBuilder.Rescorer, int)}.
+     * {@link #addRescorer(org.elasticsearch.search.rescore.RescoreBuilder, int)}.
      *
      * @param rescorer rescorer configuration
      * @return this for chaining
      */
-    public SearchRequestBuilder setRescorer(RescoreBuilder.Rescorer rescorer) {
+    public SearchRequestBuilder setRescorer(RescoreBuilder<?> rescorer) {
         sourceBuilder().clearRescorers();
         return addRescorer(rescorer);
     }
 
     /**
      * Clears all rescorers on the builder and sets the first one.  To use multiple rescore windows use
-     * {@link #addRescorer(org.elasticsearch.search.rescore.RescoreBuilder.Rescorer, int)}.
+     * {@link #addRescorer(org.elasticsearch.search.rescore.RescoreBuilder, int)}.
      *
      * @param rescorer rescorer configuration
      * @param window   rescore window
      * @return this for chaining
      */
-    public SearchRequestBuilder setRescorer(RescoreBuilder.Rescorer rescorer, int window) {
+    public SearchRequestBuilder setRescorer(RescoreBuilder rescorer, int window) {
         sourceBuilder().clearRescorers();
-        return addRescorer(rescorer, window);
+        return addRescorer(rescorer.windowSize(window));
     }
 
     /**
@@ -429,8 +420,8 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
      * @param rescorer rescorer configuration
      * @return this for chaining
      */
-    public SearchRequestBuilder addRescorer(RescoreBuilder.Rescorer rescorer) {
-        sourceBuilder().addRescorer(new RescoreBuilder(rescorer));
+    public SearchRequestBuilder addRescorer(RescoreBuilder<?> rescorer) {
+        sourceBuilder().addRescorer(rescorer);
         return this;
     }
 
@@ -441,8 +432,8 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
      * @param window   rescore window
      * @return this for chaining
      */
-    public SearchRequestBuilder addRescorer(RescoreBuilder.Rescorer rescorer, int window) {
-        sourceBuilder().addRescorer(new RescoreBuilder(rescorer).windowSize(window));
+    public SearchRequestBuilder addRescorer(RescoreBuilder<?> rescorer, int window) {
+        sourceBuilder().addRescorer(rescorer.windowSize(window));
         return this;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
index 263b8c4..02e937d 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
@@ -37,9 +37,7 @@ import org.elasticsearch.common.geo.builders.ShapeBuilder;
 import org.elasticsearch.common.text.Text;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
-import org.elasticsearch.search.rescore.RescoreBuilder.Rescorer;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.rescore.RescoreBuilder;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
 
@@ -63,7 +61,6 @@ import java.util.HashMap;
 import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.function.Function;
 import java.util.function.Supplier;
 
 import static org.elasticsearch.ElasticsearchException.readException;
@@ -666,20 +663,6 @@ public abstract class StreamInput extends InputStream {
     }
 
     /**
-     * Reads a {@link AggregatorFactory} from the current stream
-     */
-    public AggregatorFactory readAggregatorFactory() throws IOException {
-        return readNamedWriteable(AggregatorFactory.class);
-    }
-
-    /**
-     * Reads a {@link PipelineAggregatorFactory} from the current stream
-     */
-    public PipelineAggregatorFactory readPipelineAggregatorFactory() throws IOException {
-        return readNamedWriteable(PipelineAggregatorFactory.class);
-    }
-
-    /**
      * Reads a {@link QueryBuilder} from the current stream
      */
     public QueryBuilder readQuery() throws IOException {
@@ -694,10 +677,10 @@ public abstract class StreamInput extends InputStream {
     }
 
     /**
-     * Reads a {@link QueryBuilder} from the current stream
+     * Reads a {@link RescoreBuilder} from the current stream
      */
-    public Rescorer readRescorer() throws IOException {
-        return readNamedWriteable(Rescorer.class);
+    public RescoreBuilder<?> readRescorer() throws IOException {
+        return readNamedWriteable(RescoreBuilder.class);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
index 591081f..0863717 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
@@ -36,9 +36,7 @@ import org.elasticsearch.common.geo.builders.ShapeBuilder;
 import org.elasticsearch.common.text.Text;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
-import org.elasticsearch.search.rescore.RescoreBuilder.Rescorer;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.rescore.RescoreBuilder;
 import org.joda.time.ReadableInstant;
 
 import java.io.EOFException;
@@ -642,20 +640,6 @@ public abstract class StreamOutput extends OutputStream {
     }
 
     /**
-     * Writes a {@link AggregatorFactory} to the current stream
-     */
-    public void writeAggregatorFactory(AggregatorFactory factory) throws IOException {
-        writeNamedWriteable(factory);
-    }
-
-    /**
-     * Writes a {@link PipelineAggregatorFactory} to the current stream
-     */
-    public void writePipelineAggregatorFactory(PipelineAggregatorFactory factory) throws IOException {
-        writeNamedWriteable(factory);
-    }
-
-    /**
      * Writes a {@link QueryBuilder} to the current stream
      */
     public void writeQuery(QueryBuilder queryBuilder) throws IOException {
@@ -692,12 +676,12 @@ public abstract class StreamOutput extends OutputStream {
         for (T obj: list) {
             obj.writeTo(this);
         }
-    }
+     }
 
      /**
-     * Writes a {@link Rescorer} to the current stream
+     * Writes a {@link RescoreBuilder} to the current stream
      */
-    public void writeRescorer(Rescorer rescorer) throws IOException {
+    public void writeRescorer(RescoreBuilder<?> rescorer) throws IOException {
         writeNamedWriteable(rescorer);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/network/Cidrs.java b/core/src/main/java/org/elasticsearch/common/network/Cidrs.java
index f0bd4fb..d055724 100644
--- a/core/src/main/java/org/elasticsearch/common/network/Cidrs.java
+++ b/core/src/main/java/org/elasticsearch/common/network/Cidrs.java
@@ -113,8 +113,4 @@ public final class Cidrs {
         assert octets.length == 4;
         return octetsToString(octets) + "/" + networkMask;
     }
-
-    public static String createCIDR(long ipAddress, int networkMask) {
-        return octetsToCIDR(longToOctets(ipAddress), networkMask);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/rounding/Rounding.java b/core/src/main/java/org/elasticsearch/common/rounding/Rounding.java
index 7e94ebb..89a2679 100644
--- a/core/src/main/java/org/elasticsearch/common/rounding/Rounding.java
+++ b/core/src/main/java/org/elasticsearch/common/rounding/Rounding.java
@@ -19,13 +19,11 @@
 package org.elasticsearch.common.rounding;
 
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A strategy for rounding long values.
@@ -63,12 +61,6 @@ public abstract class Rounding implements Streamable {
      */
     public abstract long nextRoundingValue(long value);
 
-    @Override
-    public abstract boolean equals(Object obj);
-
-    @Override
-    public abstract int hashCode();
-
     /**
      * Rounding strategy which is based on an interval
      *
@@ -78,8 +70,6 @@ public abstract class Rounding implements Streamable {
 
         final static byte ID = 0;
 
-        public static final ParseField INTERVAL_FIELD = new ParseField("interval");
-
         private long interval;
 
         public Interval() { // for serialization
@@ -136,31 +126,12 @@ public abstract class Rounding implements Streamable {
         public void writeTo(StreamOutput out) throws IOException {
             out.writeVLong(interval);
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(interval);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            Interval other = (Interval) obj;
-            return Objects.equals(interval, other.interval);
-        }
     }
 
     public static class FactorRounding extends Rounding {
 
         final static byte ID = 7;
 
-        public static final ParseField FACTOR_FIELD = new ParseField("factor");
-
         private Rounding rounding;
 
         private float factor;
@@ -195,7 +166,7 @@ public abstract class Rounding implements Streamable {
 
         @Override
         public void readFrom(StreamInput in) throws IOException {
-            rounding = Rounding.Streams.read(in);
+            rounding = (TimeZoneRounding) Rounding.Streams.read(in);
             factor = in.readFloat();
         }
 
@@ -204,32 +175,12 @@ public abstract class Rounding implements Streamable {
             Rounding.Streams.write(rounding, out);
             out.writeFloat(factor);
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(rounding, factor);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            FactorRounding other = (FactorRounding) obj;
-            return Objects.equals(rounding, other.rounding)
-                    && Objects.equals(factor, other.factor);
-        }
     }
 
     public static class OffsetRounding extends Rounding {
 
         final static byte ID = 8;
 
-        public static final ParseField OFFSET_FIELD = new ParseField("offset");
-
         private Rounding rounding;
 
         private long offset;
@@ -273,24 +224,6 @@ public abstract class Rounding implements Streamable {
             Rounding.Streams.write(rounding, out);
             out.writeLong(offset);
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(rounding, offset);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            OffsetRounding other = (OffsetRounding) obj;
-            return Objects.equals(rounding, other.rounding)
-                    && Objects.equals(offset, other.offset);
-        }
     }
 
     public static class Streams {
diff --git a/core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java b/core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java
index 4189e41..1e6bbb6 100644
--- a/core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java
+++ b/core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.common.rounding;
 
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.TimeValue;
@@ -28,13 +27,10 @@ import org.joda.time.DateTimeZone;
 import org.joda.time.DurationField;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  */
 public abstract class TimeZoneRounding extends Rounding {
-    public static final ParseField INTERVAL_FIELD = new ParseField("interval");
-    public static final ParseField TIME_ZONE_FIELD = new ParseField("time_zone");
 
     public static Builder builder(DateTimeUnit unit) {
         return new Builder(unit);
@@ -161,24 +157,6 @@ public abstract class TimeZoneRounding extends Rounding {
             out.writeByte(unit.id());
             out.writeString(timeZone.getID());
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(unit, timeZone);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            TimeUnitRounding other = (TimeUnitRounding) obj;
-            return Objects.equals(unit, other.unit)
-                    && Objects.equals(timeZone, other.timeZone);
-        }
     }
 
     static class TimeIntervalRounding extends TimeZoneRounding {
@@ -236,23 +214,5 @@ public abstract class TimeZoneRounding extends Rounding {
             out.writeVLong(interval);
             out.writeString(timeZone.getID());
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(interval, timeZone);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            TimeIntervalRounding other = (TimeIntervalRounding) obj;
-            return Objects.equals(interval, other.interval)
-                    && Objects.equals(timeZone, other.timeZone);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/ingest/IngestMetadata.java b/core/src/main/java/org/elasticsearch/ingest/IngestMetadata.java
index 0e50751..32fade4 100644
--- a/core/src/main/java/org/elasticsearch/ingest/IngestMetadata.java
+++ b/core/src/main/java/org/elasticsearch/ingest/IngestMetadata.java
@@ -22,8 +22,6 @@ package org.elasticsearch.ingest;
 import org.elasticsearch.cluster.AbstractDiffable;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.collect.HppcMaps;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.ObjectParser;
diff --git a/core/src/main/java/org/elasticsearch/ingest/IngestService.java b/core/src/main/java/org/elasticsearch/ingest/IngestService.java
index bc7cd75..8af82b2 100644
--- a/core/src/main/java/org/elasticsearch/ingest/IngestService.java
+++ b/core/src/main/java/org/elasticsearch/ingest/IngestService.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.ingest;
 
-import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.threadpool.ThreadPool;
@@ -28,8 +27,7 @@ import java.io.Closeable;
 import java.io.IOException;
 
 /**
- * Instantiates and wires all the services that the ingest plugin will be needing.
- * Also the bootstrapper is in charge of starting and stopping the ingest plugin based on the cluster state.
+ * Holder class for several ingest related services.
  */
 public class IngestService implements Closeable {
 
diff --git a/core/src/main/java/org/elasticsearch/ingest/PipelineConfiguration.java b/core/src/main/java/org/elasticsearch/ingest/PipelineConfiguration.java
index 90ab2a7..3bd80ed 100644
--- a/core/src/main/java/org/elasticsearch/ingest/PipelineConfiguration.java
+++ b/core/src/main/java/org/elasticsearch/ingest/PipelineConfiguration.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.ingest;
 
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.Build;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -34,7 +32,6 @@ import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 import java.util.Map;
-import java.util.Objects;
 import java.util.function.BiFunction;
 
 /**
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/CompoundProcessor.java b/core/src/main/java/org/elasticsearch/ingest/core/CompoundProcessor.java
index bc5fd19..699720e 100644
--- a/core/src/main/java/org/elasticsearch/ingest/core/CompoundProcessor.java
+++ b/core/src/main/java/org/elasticsearch/ingest/core/CompoundProcessor.java
@@ -25,7 +25,6 @@ import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
-import java.util.stream.Collectors;
 
 /**
  * A Processor that executes a list of other "processors". It executes a separate list of
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/DeDotProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/DeDotProcessor.java
index 295a988..b8f8661 100644
--- a/core/src/main/java/org/elasticsearch/ingest/processor/DeDotProcessor.java
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/DeDotProcessor.java
@@ -23,7 +23,6 @@ import org.elasticsearch.ingest.core.AbstractProcessor;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.ConfigurationUtils;
 import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
 
 import java.util.HashMap;
 import java.util.Iterator;
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessor.java
index 65b4b60..76c7b3c 100644
--- a/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessor.java
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessor.java
@@ -23,7 +23,6 @@ import org.elasticsearch.ingest.core.AbstractProcessor;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.ConfigurationUtils;
 import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
 import org.elasticsearch.ingest.core.TemplateService;
 
 import java.util.Map;
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/GsubProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/GsubProcessor.java
index 3dc4b3f..0ec7fba 100644
--- a/core/src/main/java/org/elasticsearch/ingest/processor/GsubProcessor.java
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/GsubProcessor.java
@@ -23,7 +23,6 @@ import org.elasticsearch.ingest.core.AbstractProcessor;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.IngestDocument;
 import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
 
 import java.util.Map;
 import java.util.regex.Matcher;
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/JoinProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/JoinProcessor.java
index 3516929..dd729dd 100644
--- a/core/src/main/java/org/elasticsearch/ingest/processor/JoinProcessor.java
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/JoinProcessor.java
@@ -23,7 +23,6 @@ import org.elasticsearch.ingest.core.AbstractProcessor;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.IngestDocument;
 import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
 
 import java.util.List;
 import java.util.Map;
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/RemoveProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/RemoveProcessor.java
index e994954..a39ac8f 100644
--- a/core/src/main/java/org/elasticsearch/ingest/processor/RemoveProcessor.java
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/RemoveProcessor.java
@@ -24,7 +24,6 @@ import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.IngestDocument;
 import org.elasticsearch.ingest.core.TemplateService;
 import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
 
 import java.util.Map;
 
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/RenameProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/RenameProcessor.java
index 7726a72..6088315 100644
--- a/core/src/main/java/org/elasticsearch/ingest/processor/RenameProcessor.java
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/RenameProcessor.java
@@ -23,7 +23,6 @@ import org.elasticsearch.ingest.core.AbstractProcessor;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.IngestDocument;
 import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
 
 import java.util.Map;
 
diff --git a/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java b/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
index 69eaa5c..7d4e18c 100644
--- a/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
+++ b/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
@@ -19,7 +19,6 @@
 package org.elasticsearch.percolator;
 
 import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
-
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.LeafReaderContext;
@@ -27,6 +26,7 @@ import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Sort;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Counter;
 import org.elasticsearch.action.percolate.PercolateShardRequest;
 import org.elasticsearch.action.search.SearchType;
@@ -48,6 +48,7 @@ import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
+import org.elasticsearch.index.percolator.PercolatorQueriesRegistry;
 import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.similarity.SimilarityService;
@@ -56,7 +57,6 @@ import org.elasticsearch.search.SearchHitField;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -82,6 +82,7 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
+import java.util.concurrent.ConcurrentMap;
 
 /**
  */
@@ -120,15 +121,13 @@ public class PercolateContext extends SearchContext {
     private Sort sort;
     private final Map<String, FetchSubPhaseContext> subPhaseContexts = new HashMap<>();
     private final Map<Class<?>, Collector> queryCollectors = new HashMap<>();
-    private final FetchPhase fetchPhase;
 
     public PercolateContext(PercolateShardRequest request, SearchShardTarget searchShardTarget, IndexShard indexShard,
-            IndexService indexService, PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, ScriptService scriptService,
-            Query aliasFilter, ParseFieldMatcher parseFieldMatcher, FetchPhase fetchPhase) {
+                            IndexService indexService, PageCacheRecycler pageCacheRecycler,
+                            BigArrays bigArrays, ScriptService scriptService, Query aliasFilter, ParseFieldMatcher parseFieldMatcher) {
         super(parseFieldMatcher, request);
         this.indexShard = indexShard;
         this.indexService = indexService;
-        this.fetchPhase = fetchPhase;
         this.fieldDataService = indexService.fieldData();
         this.mapperService = indexService.mapperService();
         this.searchShardTarget = searchShardTarget;
@@ -160,7 +159,6 @@ public class PercolateContext extends SearchContext {
         this.startTime = 0;
         this.numberOfShards = 0;
         this.onlyCount = true;
-        this.fetchPhase = null;
     }
 
     public IndexSearcher docSearcher() {
@@ -657,11 +655,6 @@ public class PercolateContext extends SearchContext {
     }
 
     @Override
-    public FetchPhase fetchPhase() {
-        return fetchPhase;
-    }
-
-    @Override
     public MappedFieldType smartNameFieldType(String name) {
         return mapperService().fullName(name);
     }
diff --git a/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java b/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
index bc6dca1..6ac0ca6 100644
--- a/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
+++ b/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.percolator;
 
-
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.memory.ExtendedMemoryIndex;
@@ -71,7 +70,6 @@ import org.elasticsearch.search.aggregations.InternalAggregations;
 import org.elasticsearch.search.aggregations.bucket.global.GlobalAggregator;
 import org.elasticsearch.search.aggregations.pipeline.SiblingPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.highlight.HighlightField;
 import org.elasticsearch.search.highlight.HighlightPhase;
 import org.elasticsearch.search.internal.SearchContext;
@@ -100,25 +98,23 @@ public class PercolatorService extends AbstractComponent {
     private final HighlightPhase highlightPhase;
     private final AggregationPhase aggregationPhase;
     private final PageCacheRecycler pageCacheRecycler;
+    private final ParseFieldMatcher parseFieldMatcher;
     private final CloseableThreadLocal<MemoryIndex> cache;
     private final IndexNameExpressionResolver indexNameExpressionResolver;
     private final PercolateDocumentParser percolateDocumentParser;
 
     private final PercolatorIndex single;
     private final PercolatorIndex multi;
-    private final ParseFieldMatcher parseFieldMatcher;
-    private final FetchPhase fetchPhase;
 
     @Inject
     public PercolatorService(Settings settings, IndexNameExpressionResolver indexNameExpressionResolver, IndicesService indicesService,
                              PageCacheRecycler pageCacheRecycler, BigArrays bigArrays,
                              HighlightPhase highlightPhase, ClusterService clusterService,
                              AggregationPhase aggregationPhase, ScriptService scriptService,
-                             PercolateDocumentParser percolateDocumentParser, FetchPhase fetchPhase) {
+                             PercolateDocumentParser percolateDocumentParser) {
         super(settings);
         this.indexNameExpressionResolver = indexNameExpressionResolver;
         this.percolateDocumentParser = percolateDocumentParser;
-        this.fetchPhase = fetchPhase;
         this.parseFieldMatcher = new ParseFieldMatcher(settings);
         this.indicesService = indicesService;
         this.pageCacheRecycler = pageCacheRecycler;
@@ -145,7 +141,7 @@ public class PercolatorService extends AbstractComponent {
             long finalCount = 0;
             for (PercolateShardResponse shardResponse : shardResponses) {
                 finalCount += shardResponse.topDocs().totalHits;
-    }
+            }
 
             InternalAggregations reducedAggregations = reduceAggregations(shardResponses, headersContext);
             return new PercolatorService.ReduceResult(finalCount, reducedAggregations);
@@ -189,10 +185,10 @@ public class PercolatorService extends AbstractComponent {
         );
         Query aliasFilter = percolateIndexService.aliasFilter(indexShard.getQueryShardContext(), filteringAliases);
 
-        SearchShardTarget searchShardTarget = new SearchShardTarget(clusterService.localNode().id(), request.shardId().getIndex(),
-                request.shardId().id());
-        final PercolateContext context = new PercolateContext(request, searchShardTarget, indexShard, percolateIndexService,
-                pageCacheRecycler, bigArrays, scriptService, aliasFilter, parseFieldMatcher, fetchPhase);
+        SearchShardTarget searchShardTarget = new SearchShardTarget(clusterService.localNode().id(), request.shardId().getIndex(), request.shardId().id());
+        final PercolateContext context = new PercolateContext(
+                request, searchShardTarget, indexShard, percolateIndexService, pageCacheRecycler, bigArrays, scriptService, aliasFilter, parseFieldMatcher
+        );
         SearchContext.setCurrent(context);
         try {
             ParsedDocument parsedDocument = percolateDocumentParser.parse(request, context, percolateIndexService.mapperService(), percolateIndexService.getQueryShardContext());
@@ -220,15 +216,15 @@ public class PercolatorService extends AbstractComponent {
             if (context.aggregations() != null) {
                 AggregationContext aggregationContext = new AggregationContext(context);
                 context.aggregations().aggregationContext(aggregationContext);
-                context.aggregations().factories().init(aggregationContext);
-                Aggregator[] aggregators = context.aggregations().factories().createTopLevelAggregators();
+
+                Aggregator[] aggregators = context.aggregations().factories().createTopLevelAggregators(aggregationContext);
                 List<Aggregator> aggregatorCollectors = new ArrayList<>(aggregators.length);
                 for (int i = 0; i < aggregators.length; i++) {
                     if (!(aggregators[i] instanceof GlobalAggregator)) {
                         Aggregator aggregator = aggregators[i];
                         aggregatorCollectors.add(aggregator);
+                    }
                 }
-            }
                 context.aggregations().aggregators(aggregators);
                 aggregatorCollector = BucketCollector.wrap(aggregatorCollectors);
                 aggregatorCollector.preCollection();
@@ -250,14 +246,14 @@ public class PercolatorService extends AbstractComponent {
         }
         if (context.percolateQuery() != null || context.aliasFilter() != null) {
             BooleanQuery.Builder bq = new BooleanQuery.Builder();
-                        if (context.percolateQuery() != null) {
+            if (context.percolateQuery() != null) {
                 bq.add(context.percolateQuery(), MUST);
-                        }
+            }
             if (context.aliasFilter() != null) {
                 bq.add(context.aliasFilter(), FILTER);
-                        }
+            }
             builder.setPercolateQuery(bq.build());
-                    }
+        }
         PercolatorQuery percolatorQuery = builder.build();
 
         if (context.isOnlyCount() || context.size() == 0) {
@@ -266,20 +262,20 @@ public class PercolatorService extends AbstractComponent {
             if (aggregatorCollector != null) {
                 aggregatorCollector.postCollection();
                 aggregationPhase.execute(context);
-                        }
+            }
             return new PercolateShardResponse(new TopDocs(collector.getTotalHits(), Lucene.EMPTY_SCORE_DOCS, 0f), Collections.emptyMap(), Collections.emptyMap(), context);
         } else {
             int size = context.size();
             if (size > context.searcher().getIndexReader().maxDoc()) {
                 // prevent easy OOM if more than the total number of docs that exist is requested...
                 size = context.searcher().getIndexReader().maxDoc();
-        }
+            }
             TopScoreDocCollector collector = TopScoreDocCollector.create(size);
             context.searcher().search(percolatorQuery, MultiCollector.wrap(collector, aggregatorCollector));
             if (aggregatorCollector != null) {
                 aggregatorCollector.postCollection();
                 aggregationPhase.execute(context);
-    }
+            }
 
             TopDocs topDocs = collector.topDocs();
             Map<Integer, String> ids = new HashMap<>(topDocs.scoreDocs.length);
@@ -288,7 +284,7 @@ public class PercolatorService extends AbstractComponent {
                 if (context.trackScores() == false) {
                     // No sort or tracking scores was provided, so use special value to indicate to not show the scores:
                     scoreDoc.score = NO_SCORE;
-        }
+                }
 
                 int segmentIdx = ReaderUtil.subIndex(scoreDoc.doc, context.searcher().getIndexReader().leaves());
                 LeafReaderContext atomicReaderContext = context.searcher().getIndexReader().leaves().get(segmentIdx);
@@ -301,17 +297,17 @@ public class PercolatorService extends AbstractComponent {
                     Query query = queriesRegistry.getPercolateQueries().get(new BytesRef(id));
                     context.parsedQuery(new ParsedQuery(query));
                     context.hitContext().cache().clear();
-                                highlightPhase.hitExecute(context, context.hitContext());
+                    highlightPhase.hitExecute(context, context.hitContext());
                     hls.put(scoreDoc.doc, context.hitContext().hit().getHighlightFields());
-                            }
-                        }
-            return new PercolateShardResponse(topDocs, ids, hls, context);
                 }
             }
+            return new PercolateShardResponse(topDocs, ids, hls, context);
+        }
+    }
 
     public void close() {
         cache.close();
-        }
+    }
 
     private InternalAggregations reduceAggregations(List<PercolateShardResponse> shardResults, HasContextAndHeaders headersContext) {
         if (shardResults.get(0).aggregations() == null) {
@@ -332,12 +328,12 @@ public class PercolatorService extends AbstractComponent {
                 for (SiblingPipelineAggregator pipelineAggregator : pipelineAggregators) {
                     InternalAggregation newAgg = pipelineAggregator.doReduce(new InternalAggregations(newAggs), new InternalAggregation.ReduceContext(bigArrays, scriptService, headersContext));
                     newAggs.add(newAgg);
-            }
+                }
                 aggregations = new InternalAggregations(newAggs);
-        }
+            }
         }
         return aggregations;
-        }
+    }
 
     public final static class ReduceResult {
 
diff --git a/core/src/main/java/org/elasticsearch/script/Script.java b/core/src/main/java/org/elasticsearch/script/Script.java
index dc0e383..fb2226e 100644
--- a/core/src/main/java/org/elasticsearch/script/Script.java
+++ b/core/src/main/java/org/elasticsearch/script/Script.java
@@ -34,24 +34,12 @@ import org.elasticsearch.script.ScriptService.ScriptType;
 
 import java.io.IOException;
 import java.util.Map;
-import java.util.function.Supplier;
 
 /**
  * Script holds all the parameters necessary to compile or find in cache and then execute a script.
  */
 public class Script implements ToXContent, Streamable {
 
-    /**
-     * A {@link Supplier} implementation for use when reading a {@link Script}
-     * using {@link StreamInput#readOptionalStreamable(Supplier)}
-     */
-    public static final Supplier<Script> SUPPLIER = new Supplier<Script>() {
-
-        @Override
-        public Script get() {
-            return new Script();
-        }
-    };
     public static final ScriptType DEFAULT_TYPE = ScriptType.INLINE;
     private static final ScriptParser PARSER = new ScriptParser();
 
@@ -86,7 +74,7 @@ public class Script implements ToXContent, Streamable {
 
     /**
      * Constructor for Script.
-     *
+     * 
      * @param script
      *            The cache key of the script to be compiled/executed. For
      *            inline scripts this is the actual script source code. For
@@ -124,7 +112,7 @@ public class Script implements ToXContent, Streamable {
 
     /**
      * Method for getting the type.
-     *
+     * 
      * @return The type of script -- inline, indexed, or file.
      */
     public ScriptType getType() {
@@ -133,7 +121,7 @@ public class Script implements ToXContent, Streamable {
 
     /**
      * Method for getting language.
-     *
+     * 
      * @return The language of the script to be compiled/executed.
      */
     public String getLang() {
@@ -142,7 +130,7 @@ public class Script implements ToXContent, Streamable {
 
     /**
      * Method for getting the parameters.
-     *
+     * 
      * @return The map of parameters the script will be executed with.
      */
     public Map<String, Object> getParams() {
diff --git a/core/src/main/java/org/elasticsearch/search/SearchModule.java b/core/src/main/java/org/elasticsearch/search/SearchModule.java
index 5442fb3..739b970 100644
--- a/core/src/main/java/org/elasticsearch/search/SearchModule.java
+++ b/core/src/main/java/org/elasticsearch/search/SearchModule.java
@@ -19,14 +19,6 @@
 
 package org.elasticsearch.search;
 
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.function.Supplier;
-
 import org.apache.lucene.search.BooleanQuery;
 import org.elasticsearch.common.geo.ShapesAvailability;
 import org.elasticsearch.common.geo.builders.CircleBuilder;
@@ -104,7 +96,6 @@ import org.elasticsearch.index.query.functionscore.script.ScriptScoreFunctionPar
 import org.elasticsearch.index.query.functionscore.weight.WeightBuilder;
 import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.action.SearchServiceTransportAction;
-import org.elasticsearch.search.aggregations.AggregationBinaryParseElement;
 import org.elasticsearch.search.aggregations.AggregationParseElement;
 import org.elasticsearch.search.aggregations.AggregationPhase;
 import org.elasticsearch.search.aggregations.Aggregator;
@@ -136,7 +127,6 @@ import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanc
 import org.elasticsearch.search.aggregations.bucket.range.geodistance.InternalGeoDistance;
 import org.elasticsearch.search.aggregations.bucket.range.ipv4.InternalIPv4Range;
 import org.elasticsearch.search.aggregations.bucket.range.ipv4.IpRangeParser;
-import org.elasticsearch.search.aggregations.bucket.sampler.DiversifiedSamplerParser;
 import org.elasticsearch.search.aggregations.bucket.sampler.InternalSampler;
 import org.elasticsearch.search.aggregations.bucket.sampler.SamplerParser;
 import org.elasticsearch.search.aggregations.bucket.sampler.UnmappedSampler;
@@ -217,34 +207,38 @@ import org.elasticsearch.search.controller.SearchPhaseController;
 import org.elasticsearch.search.dfs.DfsPhase;
 import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSubPhase;
-import org.elasticsearch.search.fetch.FieldsParseElement;
 import org.elasticsearch.search.fetch.explain.ExplainFetchSubPhase;
 import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsFetchSubPhase;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsFetchSubPhase;
 import org.elasticsearch.search.fetch.matchedqueries.MatchedQueriesFetchSubPhase;
 import org.elasticsearch.search.fetch.parent.ParentFieldSubFetchPhase;
 import org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase;
-import org.elasticsearch.search.fetch.script.ScriptFieldsParseElement;
-import org.elasticsearch.search.fetch.source.FetchSourceParseElement;
 import org.elasticsearch.search.fetch.source.FetchSourceSubPhase;
 import org.elasticsearch.search.fetch.version.VersionFetchSubPhase;
 import org.elasticsearch.search.highlight.HighlightPhase;
 import org.elasticsearch.search.highlight.Highlighter;
-import org.elasticsearch.search.highlight.HighlighterParseElement;
 import org.elasticsearch.search.highlight.Highlighters;
 import org.elasticsearch.search.query.QueryPhase;
-import org.elasticsearch.search.sort.SortParseElement;
+import org.elasticsearch.search.rescore.QueryRescorerBuilder;
+import org.elasticsearch.search.rescore.RescoreBuilder;
 import org.elasticsearch.search.suggest.Suggester;
 import org.elasticsearch.search.suggest.Suggesters;
 
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.function.Supplier;
+
 /**
  *
  */
 public class SearchModule extends AbstractModule {
 
-    private final Set<Aggregator.Parser> aggParsers = new HashSet<>();
-    private final Set<PipelineAggregator.Parser> pipelineAggParsers = new HashSet<>();
+    private final Set<Class<? extends Aggregator.Parser>> aggParsers = new HashSet<>();
+    private final Set<Class<? extends PipelineAggregator.Parser>> pipelineAggParsers = new HashSet<>();
     private final Highlighters highlighters = new Highlighters();
     private final Suggesters suggesters = new Suggesters();
     /**
@@ -259,8 +253,8 @@ public class SearchModule extends AbstractModule {
      */
     private final List<Supplier<QueryParser<?>>> queryParsers = new ArrayList<>();
     private final Set<Class<? extends FetchSubPhase>> fetchSubPhases = new HashSet<>();
-    private final Set<SignificanceHeuristicParser> heuristicParsers = new HashSet<>();
-    private final Set<MovAvgModel.AbstractModelParser> modelParsers = new HashSet<>();
+    private final Set<Class<? extends SignificanceHeuristicParser>> heuristicParsers = new HashSet<>();
+    private final Set<Class<? extends MovAvgModel.AbstractModelParser>> modelParsers = new HashSet<>();
 
     private final Settings settings;
     private final NamedWriteableRegistry namedWriteableRegistry;
@@ -305,11 +299,11 @@ public class SearchModule extends AbstractModule {
         fetchSubPhases.add(subPhase);
     }
 
-    public void registerHeuristicParser(SignificanceHeuristicParser parser) {
+    public void registerHeuristicParser(Class<? extends SignificanceHeuristicParser> parser) {
         heuristicParsers.add(parser);
     }
 
-    public void registerModelParser(MovAvgModel.AbstractModelParser parser) {
+    public void registerModelParser(Class<? extends MovAvgModel.AbstractModelParser> parser) {
         modelParsers.add(parser);
     }
 
@@ -318,24 +312,24 @@ public class SearchModule extends AbstractModule {
      *
      * @param parser The parser for the custom aggregator.
      */
-    public void registerAggregatorParser(Aggregator.Parser parser) {
+    public void registerAggregatorParser(Class<? extends Aggregator.Parser> parser) {
         aggParsers.add(parser);
     }
 
-    public void registerPipelineParser(PipelineAggregator.Parser parser) {
+    public void registerPipelineParser(Class<? extends PipelineAggregator.Parser> parser) {
         pipelineAggParsers.add(parser);
     }
 
     @Override
     protected void configure() {
-        IndicesQueriesRegistry indicesQueriesRegistry = buildQueryParserRegistry();
-        bind(IndicesQueriesRegistry.class).toInstance(indicesQueriesRegistry);
         configureSearch();
-        configureAggs(indicesQueriesRegistry);
+        configureAggs();
         configureHighlighters();
         configureSuggesters();
+        bind(IndicesQueriesRegistry.class).toInstance(buildQueryParserRegistry());
         configureFetchSubPhase();
         configureShapes();
+        configureRescorers();
     }
 
     protected void configureFetchSubPhase() {
@@ -377,67 +371,75 @@ public class SearchModule extends AbstractModule {
        highlighters.bind(binder());
     }
 
-    protected void configureAggs(IndicesQueriesRegistry indicesQueriesRegistry) {
-
-        MovAvgModelParserMapper movAvgModelParserMapper = new MovAvgModelParserMapper(modelParsers);
-
-        SignificanceHeuristicParserMapper significanceHeuristicParserMapper = new SignificanceHeuristicParserMapper(heuristicParsers);
-
-        registerAggregatorParser(new AvgParser());
-        registerAggregatorParser(new SumParser());
-        registerAggregatorParser(new MinParser());
-        registerAggregatorParser(new MaxParser());
-        registerAggregatorParser(new StatsParser());
-        registerAggregatorParser(new ExtendedStatsParser());
-        registerAggregatorParser(new ValueCountParser());
-        registerAggregatorParser(new PercentilesParser());
-        registerAggregatorParser(new PercentileRanksParser());
-        registerAggregatorParser(new CardinalityParser());
-        registerAggregatorParser(new GlobalParser());
-        registerAggregatorParser(new MissingParser());
-        registerAggregatorParser(new FilterParser());
-        registerAggregatorParser(new FiltersParser(indicesQueriesRegistry));
-        registerAggregatorParser(new SamplerParser());
-        registerAggregatorParser(new DiversifiedSamplerParser());
-        registerAggregatorParser(new TermsParser());
-        registerAggregatorParser(new SignificantTermsParser(significanceHeuristicParserMapper, indicesQueriesRegistry));
-        registerAggregatorParser(new RangeParser());
-        registerAggregatorParser(new DateRangeParser());
-        registerAggregatorParser(new IpRangeParser());
-        registerAggregatorParser(new HistogramParser());
-        registerAggregatorParser(new DateHistogramParser());
-        registerAggregatorParser(new GeoDistanceParser());
-        registerAggregatorParser(new GeoHashGridParser());
-        registerAggregatorParser(new NestedParser());
-        registerAggregatorParser(new ReverseNestedParser());
-        registerAggregatorParser(new TopHitsParser(new SortParseElement(), new FetchSourceParseElement(), new HighlighterParseElement(),
-                new FieldDataFieldsParseElement(), new ScriptFieldsParseElement(), new FieldsParseElement()));
-        registerAggregatorParser(new GeoBoundsParser());
-        registerAggregatorParser(new GeoCentroidParser());
-        registerAggregatorParser(new ScriptedMetricParser());
-        registerAggregatorParser(new ChildrenParser());
-
-        registerPipelineParser(new DerivativeParser());
-        registerPipelineParser(new MaxBucketParser());
-        registerPipelineParser(new MinBucketParser());
-        registerPipelineParser(new AvgBucketParser());
-        registerPipelineParser(new SumBucketParser());
-        registerPipelineParser(new StatsBucketParser());
-        registerPipelineParser(new ExtendedStatsBucketParser());
-        registerPipelineParser(new PercentilesBucketParser());
-        registerPipelineParser(new MovAvgParser(movAvgModelParserMapper));
-        registerPipelineParser(new CumulativeSumParser());
-        registerPipelineParser(new BucketScriptParser());
-        registerPipelineParser(new BucketSelectorParser());
-        registerPipelineParser(new SerialDiffParser());
-
-        AggregatorParsers aggregatorParsers = new AggregatorParsers(aggParsers, pipelineAggParsers, namedWriteableRegistry);
-        AggregationParseElement aggParseElement = new AggregationParseElement(aggregatorParsers, indicesQueriesRegistry);
-        AggregationBinaryParseElement aggBinaryParseElement = new AggregationBinaryParseElement(aggregatorParsers, indicesQueriesRegistry);
-        AggregationPhase aggPhase = new AggregationPhase(aggParseElement, aggBinaryParseElement);
-        bind(AggregatorParsers.class).toInstance(aggregatorParsers);
-        bind(AggregationParseElement.class).toInstance(aggParseElement);
-        bind(AggregationPhase.class).toInstance(aggPhase);
+    protected void configureAggs() {
+        Multibinder<Aggregator.Parser> multibinderAggParser = Multibinder.newSetBinder(binder(), Aggregator.Parser.class);
+        multibinderAggParser.addBinding().to(AvgParser.class);
+        multibinderAggParser.addBinding().to(SumParser.class);
+        multibinderAggParser.addBinding().to(MinParser.class);
+        multibinderAggParser.addBinding().to(MaxParser.class);
+        multibinderAggParser.addBinding().to(StatsParser.class);
+        multibinderAggParser.addBinding().to(ExtendedStatsParser.class);
+        multibinderAggParser.addBinding().to(ValueCountParser.class);
+        multibinderAggParser.addBinding().to(PercentilesParser.class);
+        multibinderAggParser.addBinding().to(PercentileRanksParser.class);
+        multibinderAggParser.addBinding().to(CardinalityParser.class);
+        multibinderAggParser.addBinding().to(GlobalParser.class);
+        multibinderAggParser.addBinding().to(MissingParser.class);
+        multibinderAggParser.addBinding().to(FilterParser.class);
+        multibinderAggParser.addBinding().to(FiltersParser.class);
+        multibinderAggParser.addBinding().to(SamplerParser.class);
+        multibinderAggParser.addBinding().to(TermsParser.class);
+        multibinderAggParser.addBinding().to(SignificantTermsParser.class);
+        multibinderAggParser.addBinding().to(RangeParser.class);
+        multibinderAggParser.addBinding().to(DateRangeParser.class);
+        multibinderAggParser.addBinding().to(IpRangeParser.class);
+        multibinderAggParser.addBinding().to(HistogramParser.class);
+        multibinderAggParser.addBinding().to(DateHistogramParser.class);
+        multibinderAggParser.addBinding().to(GeoDistanceParser.class);
+        multibinderAggParser.addBinding().to(GeoHashGridParser.class);
+        multibinderAggParser.addBinding().to(NestedParser.class);
+        multibinderAggParser.addBinding().to(ReverseNestedParser.class);
+        multibinderAggParser.addBinding().to(TopHitsParser.class);
+        multibinderAggParser.addBinding().to(GeoBoundsParser.class);
+        multibinderAggParser.addBinding().to(GeoCentroidParser.class);
+        multibinderAggParser.addBinding().to(ScriptedMetricParser.class);
+        multibinderAggParser.addBinding().to(ChildrenParser.class);
+        for (Class<? extends Aggregator.Parser> parser : aggParsers) {
+            multibinderAggParser.addBinding().to(parser);
+        }
+
+        Multibinder<PipelineAggregator.Parser> multibinderPipelineAggParser = Multibinder.newSetBinder(binder(), PipelineAggregator.Parser.class);
+        multibinderPipelineAggParser.addBinding().to(DerivativeParser.class);
+        multibinderPipelineAggParser.addBinding().to(MaxBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(MinBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(AvgBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(SumBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(StatsBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(ExtendedStatsBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(PercentilesBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(MovAvgParser.class);
+        multibinderPipelineAggParser.addBinding().to(CumulativeSumParser.class);
+        multibinderPipelineAggParser.addBinding().to(BucketScriptParser.class);
+        multibinderPipelineAggParser.addBinding().to(BucketSelectorParser.class);
+        multibinderPipelineAggParser.addBinding().to(SerialDiffParser.class);
+        for (Class<? extends PipelineAggregator.Parser> parser : pipelineAggParsers) {
+            multibinderPipelineAggParser.addBinding().to(parser);
+        }
+        bind(AggregatorParsers.class).asEagerSingleton();
+        bind(AggregationParseElement.class).asEagerSingleton();
+        bind(AggregationPhase.class).asEagerSingleton();
+
+        Multibinder<SignificanceHeuristicParser> heuristicParserMultibinder = Multibinder.newSetBinder(binder(), SignificanceHeuristicParser.class);
+        for (Class<? extends SignificanceHeuristicParser> clazz : heuristicParsers) {
+            heuristicParserMultibinder.addBinding().to(clazz);
+        }
+        bind(SignificanceHeuristicParserMapper.class);
+
+        Multibinder<MovAvgModel.AbstractModelParser> modelParserMultibinder = Multibinder.newSetBinder(binder(), MovAvgModel.AbstractModelParser.class);
+        for (Class<? extends MovAvgModel.AbstractModelParser> clazz : modelParsers) {
+            modelParserMultibinder.addBinding().to(clazz);
+        }
+        bind(MovAvgModelParserMapper.class);
     }
 
     protected void configureSearch() {
@@ -468,6 +470,10 @@ public class SearchModule extends AbstractModule {
         }
     }
 
+    private void configureRescorers() {
+        namedWriteableRegistry.registerPrototype(RescoreBuilder.class, QueryRescorerBuilder.PROTOTYPE);
+    }
+
     private void registerBuiltinFunctionScoreParsers() {
         registerFunctionScoreParser(new ScriptScoreFunctionParser());
         registerFunctionScoreParser(new GaussDecayFunctionParser());
diff --git a/core/src/main/java/org/elasticsearch/search/SearchService.java b/core/src/main/java/org/elasticsearch/search/SearchService.java
index a91014d..8c9803c 100644
--- a/core/src/main/java/org/elasticsearch/search/SearchService.java
+++ b/core/src/main/java/org/elasticsearch/search/SearchService.java
@@ -61,7 +61,6 @@ import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MappedFieldType.Loading;
 import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.search.stats.ShardSearchStats;
@@ -101,6 +100,7 @@ import org.elasticsearch.search.query.QuerySearchRequest;
 import org.elasticsearch.search.query.QuerySearchResult;
 import org.elasticsearch.search.query.QuerySearchResultProvider;
 import org.elasticsearch.search.query.ScrollQuerySearchResult;
+import org.elasticsearch.search.rescore.RescoreBuilder;
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.io.IOException;
@@ -558,10 +558,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
 
         Engine.Searcher engineSearcher = searcher == null ? indexShard.acquireSearcher("search") : searcher;
 
-        DefaultSearchContext context = new DefaultSearchContext(idGenerator.incrementAndGet(), request, shardTarget, engineSearcher,
-                indexService,
-                indexShard, scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher,
-                defaultSearchTimeout, fetchPhase);
+        DefaultSearchContext context = new DefaultSearchContext(idGenerator.incrementAndGet(), request, shardTarget, engineSearcher, indexService, indexShard, scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher, defaultSearchTimeout);
         SearchContext.setCurrent(context);
 
         try {
@@ -755,7 +752,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
                     // ignore
                 }
                 XContentLocation location = completeAggregationsParser != null ? completeAggregationsParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse aggregation source [" + sSource + "]", location, e);
+                throw new SearchParseException(context, "failed to parse rescore source [" + sSource + "]", location, e);
             }
         }
         if (source.suggest() != null) {
@@ -776,33 +773,12 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
             }
         }
         if (source.rescores() != null) {
-            XContentParser completeRescoreParser = null;
             try {
-                XContentBuilder completeRescoreBuilder = XContentFactory.jsonBuilder();
-                completeRescoreBuilder.startObject();
-                completeRescoreBuilder.startArray("rescore");
-                for (BytesReference rescore : source.rescores()) {
-                    XContentParser parser = XContentFactory.xContent(rescore).createParser(rescore);
-                    parser.nextToken();
-                    completeRescoreBuilder.copyCurrentStructure(parser);
+                for (RescoreBuilder<?> rescore : source.rescores()) {
+                    context.addRescore(rescore.build(context.indexShard().getQueryShardContext()));
                 }
-                completeRescoreBuilder.endArray();
-                completeRescoreBuilder.endObject();
-                BytesReference completeRescoreBytes = completeRescoreBuilder.bytes();
-                completeRescoreParser = XContentFactory.xContent(completeRescoreBytes).createParser(completeRescoreBytes);
-                completeRescoreParser.nextToken();
-                completeRescoreParser.nextToken();
-                completeRescoreParser.nextToken();
-                this.elementParsers.get("rescore").parse(completeRescoreParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = completeRescoreParser != null ? completeRescoreParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse rescore source [" + sSource + "]", location, e);
+            } catch (IOException e) {
+                throw new SearchContextException(context, "failed to create RescoreSearchContext", e);
             }
         }
         if (source.fields() != null) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AbstractAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/AbstractAggregationBuilder.java
new file mode 100644
index 0000000..31ee3fd
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AbstractAggregationBuilder.java
@@ -0,0 +1,45 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations;
+
+import org.elasticsearch.common.xcontent.ToXContent;
+
+/**
+ * Base structure for aggregation builders.
+ */
+public abstract class AbstractAggregationBuilder implements ToXContent {
+
+    private final String name;
+    protected final String type;
+
+    /**
+     * Sole constructor, typically used by sub-classes.
+     */
+    protected AbstractAggregationBuilder(String name, String type) {
+        this.name = name;
+        this.type = type;
+    }
+
+    /**
+     * Return the name of the aggregation that is being built.
+     */
+    public String getName() {
+        return name;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBinaryParseElement.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBinaryParseElement.java
index d1fd5b1..f2a9f08 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBinaryParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBinaryParseElement.java
@@ -21,7 +21,6 @@ package org.elasticsearch.search.aggregations;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.internal.SearchContext;
 
 /**
@@ -30,8 +29,8 @@ import org.elasticsearch.search.internal.SearchContext;
 public class AggregationBinaryParseElement extends AggregationParseElement {
 
     @Inject
-    public AggregationBinaryParseElement(AggregatorParsers aggregatorParsers, IndicesQueriesRegistry queriesRegistry) {
-        super(aggregatorParsers, queriesRegistry);
+    public AggregationBinaryParseElement(AggregatorParsers aggregatorParsers) {
+        super(aggregatorParsers);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilder.java
new file mode 100644
index 0000000..ed1ba6c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilder.java
@@ -0,0 +1,147 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations;
+
+import org.elasticsearch.ElasticsearchGenerationException;
+import org.elasticsearch.client.Requests;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * A base class for all bucket aggregation builders.
+ */
+public abstract class AggregationBuilder<B extends AggregationBuilder<B>> extends AbstractAggregationBuilder {
+
+    private List<AbstractAggregationBuilder> aggregations;
+    private BytesReference aggregationsBinary;
+    private Map<String, Object> metaData;
+
+    /**
+     * Sole constructor, typically used by sub-classes.
+     */
+    protected AggregationBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    /**
+     * Add a sub get to this bucket get.
+     */
+    @SuppressWarnings("unchecked")
+    public B subAggregation(AbstractAggregationBuilder aggregation) {
+        if (aggregations == null) {
+            aggregations = new ArrayList<>();
+        }
+        aggregations.add(aggregation);
+        return (B) this;
+    }
+
+    /**
+     * Sets a raw (xcontent / json) sub addAggregation.
+     */
+    public B subAggregation(byte[] aggregationsBinary) {
+        return subAggregation(aggregationsBinary, 0, aggregationsBinary.length);
+    }
+
+    /**
+     * Sets a raw (xcontent / json) sub addAggregation.
+     */
+    public B subAggregation(byte[] aggregationsBinary, int aggregationsBinaryOffset, int aggregationsBinaryLength) {
+        return subAggregation(new BytesArray(aggregationsBinary, aggregationsBinaryOffset, aggregationsBinaryLength));
+    }
+
+    /**
+     * Sets a raw (xcontent / json) sub addAggregation.
+     */
+    @SuppressWarnings("unchecked")
+    public B subAggregation(BytesReference aggregationsBinary) {
+        this.aggregationsBinary = aggregationsBinary;
+        return (B) this;
+    }
+
+    /**
+     * Sets a raw (xcontent / json) sub addAggregation.
+     */
+    public B subAggregation(XContentBuilder aggs) {
+        return subAggregation(aggs.bytes());
+    }
+
+    /**
+     * Sets a raw (xcontent / json) sub addAggregation.
+     */
+    public B subAggregation(Map<String, Object> aggs) {
+        try {
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(aggs);
+            return subAggregation(builder);
+        } catch (IOException e) {
+            throw new ElasticsearchGenerationException("Failed to generate [" + aggs + "]", e);
+        }
+    }
+
+    /**
+     * Sets the meta data to be included in the aggregation response
+     */
+    public B setMetaData(Map<String, Object> metaData) {
+        this.metaData = metaData;
+        return (B)this;
+    }
+
+    @Override
+    public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(getName());
+
+        if (this.metaData != null) {
+            builder.field("meta", this.metaData);
+        }
+        builder.field(type);
+        internalXContent(builder, params);
+
+        if (aggregations != null || aggregationsBinary != null) {
+
+            if (aggregations != null) {
+                builder.startObject("aggregations");
+                for (AbstractAggregationBuilder subAgg : aggregations) {
+                    subAgg.toXContent(builder, params);
+                }
+                builder.endObject();
+            }
+
+            if (aggregationsBinary != null) {
+                if (XContentFactory.xContentType(aggregationsBinary) == builder.contentType()) {
+                    builder.rawField("aggregations", aggregationsBinary);
+                } else {
+                    builder.field("aggregations_binary", aggregationsBinary);
+                }
+            }
+
+        }
+
+        return builder.endObject();
+    }
+
+    protected abstract XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException;
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java
index 347c2f8..dd9c5a3 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java
@@ -19,67 +19,64 @@
 package org.elasticsearch.search.aggregations;
 
 import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.bucket.children.Children;
-import org.elasticsearch.search.aggregations.bucket.children.ParentToChildrenAggregator;
+import org.elasticsearch.search.aggregations.bucket.children.ChildrenBuilder;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
-import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator;
+import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.filters.Filters;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.KeyedFilter;
+import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGrid;
-import org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridParser;
+import org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridBuilder;
 import org.elasticsearch.search.aggregations.bucket.global.Global;
-import org.elasticsearch.search.aggregations.bucket.global.GlobalAggregator;
+import org.elasticsearch.search.aggregations.bucket.global.GlobalBuilder;
+import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramBuilder;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.DateHistogramFactory;
+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramBuilder;
 import org.elasticsearch.search.aggregations.bucket.missing.Missing;
-import org.elasticsearch.search.aggregations.bucket.missing.MissingAggregator;
+import org.elasticsearch.search.aggregations.bucket.missing.MissingBuilder;
 import org.elasticsearch.search.aggregations.bucket.nested.Nested;
-import org.elasticsearch.search.aggregations.bucket.nested.NestedAggregator;
+import org.elasticsearch.search.aggregations.bucket.nested.NestedBuilder;
 import org.elasticsearch.search.aggregations.bucket.nested.ReverseNested;
-import org.elasticsearch.search.aggregations.bucket.nested.ReverseNestedAggregator;
+import org.elasticsearch.search.aggregations.bucket.nested.ReverseNestedBuilder;
 import org.elasticsearch.search.aggregations.bucket.range.Range;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.date.DateRangeAggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanceParser.GeoDistanceFactory;
-import org.elasticsearch.search.aggregations.bucket.range.ipv4.IPv4RangeAggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.range.RangeBuilder;
+import org.elasticsearch.search.aggregations.bucket.range.date.DateRangeBuilder;
+import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanceBuilder;
+import org.elasticsearch.search.aggregations.bucket.range.ipv4.IPv4RangeBuilder;
 import org.elasticsearch.search.aggregations.bucket.sampler.Sampler;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator;
+import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms;
-import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsBuilder;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
-import org.elasticsearch.search.aggregations.metrics.avg.AvgAggregator;
+import org.elasticsearch.search.aggregations.metrics.avg.AvgBuilder;
 import org.elasticsearch.search.aggregations.metrics.cardinality.Cardinality;
-import org.elasticsearch.search.aggregations.metrics.cardinality.CardinalityAggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.cardinality.CardinalityBuilder;
 import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBounds;
-import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsAggregator;
+import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsBuilder;
 import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroid;
-import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator;
+import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidBuilder;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
-import org.elasticsearch.search.aggregations.metrics.max.MaxAggregator;
+import org.elasticsearch.search.aggregations.metrics.max.MaxBuilder;
 import org.elasticsearch.search.aggregations.metrics.min.Min;
-import org.elasticsearch.search.aggregations.metrics.min.MinAggregator;
+import org.elasticsearch.search.aggregations.metrics.min.MinBuilder;
 import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanks;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksAggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksBuilder;
 import org.elasticsearch.search.aggregations.metrics.percentiles.Percentiles;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesAggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesBuilder;
 import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetric;
-import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator;
+import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricBuilder;
 import org.elasticsearch.search.aggregations.metrics.stats.Stats;
-import org.elasticsearch.search.aggregations.metrics.stats.StatsAggregator;
+import org.elasticsearch.search.aggregations.metrics.stats.StatsBuilder;
 import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStats;
-import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStatsAggregator;
+import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStatsBuilder;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
-import org.elasticsearch.search.aggregations.metrics.sum.SumAggregator;
+import org.elasticsearch.search.aggregations.metrics.sum.SumBuilder;
 import org.elasticsearch.search.aggregations.metrics.tophits.TopHits;
-import org.elasticsearch.search.aggregations.metrics.tophits.TopHitsAggregator;
+import org.elasticsearch.search.aggregations.metrics.tophits.TopHitsBuilder;
 import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCount;
-import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCountAggregator;
+import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCountBuilder;
 
 /**
  * Utility class to create aggregations.
@@ -92,234 +89,217 @@ public class AggregationBuilders {
     /**
      * Create a new {@link ValueCount} aggregation with the given name.
      */
-    public static ValueCountAggregator.Factory count(String name) {
-        return new ValueCountAggregator.Factory(name, null);
+    public static ValueCountBuilder count(String name) {
+        return new ValueCountBuilder(name);
     }
 
     /**
      * Create a new {@link Avg} aggregation with the given name.
      */
-    public static AvgAggregator.Factory avg(String name) {
-        return new AvgAggregator.Factory(name);
+    public static AvgBuilder avg(String name) {
+        return new AvgBuilder(name);
     }
 
     /**
      * Create a new {@link Max} aggregation with the given name.
      */
-    public static MaxAggregator.Factory max(String name) {
-        return new MaxAggregator.Factory(name);
+    public static MaxBuilder max(String name) {
+        return new MaxBuilder(name);
     }
 
     /**
      * Create a new {@link Min} aggregation with the given name.
      */
-    public static MinAggregator.Factory min(String name) {
-        return new MinAggregator.Factory(name);
+    public static MinBuilder min(String name) {
+        return new MinBuilder(name);
     }
 
     /**
      * Create a new {@link Sum} aggregation with the given name.
      */
-    public static SumAggregator.Factory sum(String name) {
-        return new SumAggregator.Factory(name);
+    public static SumBuilder sum(String name) {
+        return new SumBuilder(name);
     }
 
     /**
      * Create a new {@link Stats} aggregation with the given name.
      */
-    public static StatsAggregator.Factory stats(String name) {
-        return new StatsAggregator.Factory(name);
+    public static StatsBuilder stats(String name) {
+        return new StatsBuilder(name);
     }
 
     /**
      * Create a new {@link ExtendedStats} aggregation with the given name.
      */
-    public static ExtendedStatsAggregator.Factory extendedStats(String name) {
-        return new ExtendedStatsAggregator.Factory(name);
+    public static ExtendedStatsBuilder extendedStats(String name) {
+        return new ExtendedStatsBuilder(name);
     }
 
     /**
      * Create a new {@link Filter} aggregation with the given name.
      */
-    public static FilterAggregator.Factory filter(String name, QueryBuilder<?> filter) {
-        return new FilterAggregator.Factory(name, filter);
+    public static FilterAggregationBuilder filter(String name) {
+        return new FilterAggregationBuilder(name);
     }
 
     /**
      * Create a new {@link Filters} aggregation with the given name.
      */
-    public static FiltersAggregator.Factory filters(String name, KeyedFilter... filters) {
-        return new FiltersAggregator.Factory(name, filters);
-    }
-
-    /**
-     * Create a new {@link Filters} aggregation with the given name.
-     */
-    public static FiltersAggregator.Factory filters(String name, QueryBuilder<?>... filters) {
-        return new FiltersAggregator.Factory(name, filters);
-    }
-
-    /**
-     * Create a new {@link Sampler} aggregation with the given name.
-     */
-    public static SamplerAggregator.Factory sampler(String name) {
-        return new SamplerAggregator.Factory(name);
+    public static FiltersAggregationBuilder filters(String name) {
+        return new FiltersAggregationBuilder(name);
     }
 
     /**
      * Create a new {@link Sampler} aggregation with the given name.
      */
-    public static SamplerAggregator.DiversifiedFactory diversifiedSampler(String name) {
-        return new SamplerAggregator.DiversifiedFactory(name);
+    public static SamplerAggregationBuilder sampler(String name) {
+        return new SamplerAggregationBuilder(name);
     }
 
     /**
      * Create a new {@link Global} aggregation with the given name.
      */
-    public static GlobalAggregator.Factory global(String name) {
-        return new GlobalAggregator.Factory(name);
+    public static GlobalBuilder global(String name) {
+        return new GlobalBuilder(name);
     }
 
     /**
      * Create a new {@link Missing} aggregation with the given name.
      */
-    public static MissingAggregator.Factory missing(String name) {
-        return new MissingAggregator.Factory(name, null);
+    public static MissingBuilder missing(String name) {
+        return new MissingBuilder(name);
     }
 
     /**
      * Create a new {@link Nested} aggregation with the given name.
      */
-    public static NestedAggregator.Factory nested(String name, String path) {
-        return new NestedAggregator.Factory(name, path);
+    public static NestedBuilder nested(String name) {
+        return new NestedBuilder(name);
     }
 
     /**
      * Create a new {@link ReverseNested} aggregation with the given name.
      */
-    public static ReverseNestedAggregator.Factory reverseNested(String name) {
-        return new ReverseNestedAggregator.Factory(name);
+    public static ReverseNestedBuilder reverseNested(String name) {
+        return new ReverseNestedBuilder(name);
     }
 
     /**
      * Create a new {@link Children} aggregation with the given name.
      */
-    public static ParentToChildrenAggregator.Factory children(String name, String childType) {
-        return new ParentToChildrenAggregator.Factory(name, childType);
+    public static ChildrenBuilder children(String name) {
+        return new ChildrenBuilder(name);
     }
 
     /**
      * Create a new {@link GeoDistance} aggregation with the given name.
      */
-    public static GeoDistanceFactory geoDistance(String name, GeoPoint origin) {
-        return new GeoDistanceFactory(name, origin);
+    public static GeoDistanceBuilder geoDistance(String name) {
+        return new GeoDistanceBuilder(name);
     }
 
     /**
      * Create a new {@link Histogram} aggregation with the given name.
      */
-    public static HistogramAggregator.Factory<?> histogram(String name) {
-        return new HistogramAggregator.Factory<>(name);
+    public static HistogramBuilder histogram(String name) {
+        return new HistogramBuilder(name);
     }
 
     /**
      * Create a new {@link GeoHashGrid} aggregation with the given name.
      */
-    public static GeoHashGridParser.GeoGridFactory geohashGrid(String name) {
-        return new GeoHashGridParser.GeoGridFactory(name);
+    public static GeoHashGridBuilder geohashGrid(String name) {
+        return new GeoHashGridBuilder(name);
     }
 
     /**
      * Create a new {@link SignificantTerms} aggregation with the given name.
      */
-    public static SignificantTermsAggregatorFactory significantTerms(String name) {
-        return new SignificantTermsAggregatorFactory(name, null);
+    public static SignificantTermsBuilder significantTerms(String name) {
+        return new SignificantTermsBuilder(name);
     }
 
     /**
-     * Create a new {@link DateHistogramFactory} aggregation with the given
-     * name.
+     * Create a new {@link DateHistogramBuilder} aggregation with the given name.
      */
-    public static DateHistogramFactory dateHistogram(String name) {
-        return new DateHistogramFactory(name);
+    public static DateHistogramBuilder dateHistogram(String name) {
+        return new DateHistogramBuilder(name);
     }
 
     /**
      * Create a new {@link Range} aggregation with the given name.
      */
-    public static RangeAggregator.Factory range(String name) {
-        return new RangeAggregator.Factory(name);
+    public static RangeBuilder range(String name) {
+        return new RangeBuilder(name);
     }
 
     /**
-     * Create a new {@link DateRangeAggregatorFactory} aggregation with the
-     * given name.
+     * Create a new {@link DateRangeBuilder} aggregation with the given name.
      */
-    public static DateRangeAggregatorFactory dateRange(String name) {
-        return new DateRangeAggregatorFactory(name);
+    public static DateRangeBuilder dateRange(String name) {
+        return new DateRangeBuilder(name);
     }
 
     /**
-     * Create a new {@link IPv4RangeAggregatorFactory} aggregation with the
-     * given name.
+     * Create a new {@link IPv4RangeBuilder} aggregation with the given name.
      */
-    public static IPv4RangeAggregatorFactory ipRange(String name) {
-        return new IPv4RangeAggregatorFactory(name);
+    public static IPv4RangeBuilder ipRange(String name) {
+        return new IPv4RangeBuilder(name);
     }
 
     /**
      * Create a new {@link Terms} aggregation with the given name.
      */
-    public static TermsAggregatorFactory terms(String name) {
-        return new TermsAggregatorFactory(name, null);
+    public static TermsBuilder terms(String name) {
+        return new TermsBuilder(name);
     }
 
     /**
      * Create a new {@link Percentiles} aggregation with the given name.
      */
-    public static PercentilesAggregatorFactory percentiles(String name) {
-        return new PercentilesAggregatorFactory(name);
+    public static PercentilesBuilder percentiles(String name) {
+        return new PercentilesBuilder(name);
     }
 
     /**
      * Create a new {@link PercentileRanks} aggregation with the given name.
      */
-    public static PercentileRanksAggregatorFactory percentileRanks(String name) {
-        return new PercentileRanksAggregatorFactory(name);
+    public static PercentileRanksBuilder percentileRanks(String name) {
+        return new PercentileRanksBuilder(name);
     }
 
     /**
      * Create a new {@link Cardinality} aggregation with the given name.
      */
-    public static CardinalityAggregatorFactory cardinality(String name) {
-        return new CardinalityAggregatorFactory(name, null);
+    public static CardinalityBuilder cardinality(String name) {
+        return new CardinalityBuilder(name);
     }
 
     /**
      * Create a new {@link TopHits} aggregation with the given name.
      */
-    public static TopHitsAggregator.Factory topHits(String name) {
-        return new TopHitsAggregator.Factory(name);
+    public static TopHitsBuilder topHits(String name) {
+        return new TopHitsBuilder(name);
     }
 
     /**
      * Create a new {@link GeoBounds} aggregation with the given name.
      */
-    public static GeoBoundsAggregator.Factory geoBounds(String name) {
-        return new GeoBoundsAggregator.Factory(name);
+    public static GeoBoundsBuilder geoBounds(String name) {
+        return new GeoBoundsBuilder(name);
     }
 
     /**
      * Create a new {@link GeoCentroid} aggregation with the given name.
      */
-    public static GeoCentroidAggregator.Factory geoCentroid(String name) {
-        return new GeoCentroidAggregator.Factory(name);
+    public static GeoCentroidBuilder geoCentroid(String name) {
+        return new GeoCentroidBuilder(name);
     }
 
     /**
      * Create a new {@link ScriptedMetric} aggregation with the given name.
      */
-    public static ScriptedMetricAggregator.Factory scriptedMetric(String name) {
-        return new ScriptedMetricAggregator.Factory(name);
+    public static ScriptedMetricBuilder scriptedMetric(String name) {
+        return new ScriptedMetricBuilder(name);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationParseElement.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationParseElement.java
index f4eae59..767cbfb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationParseElement.java
@@ -20,8 +20,6 @@ package org.elasticsearch.search.aggregations;
 
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.internal.SearchContext;
 
@@ -51,20 +49,15 @@ import org.elasticsearch.search.internal.SearchContext;
 public class AggregationParseElement implements SearchParseElement {
 
     private final AggregatorParsers aggregatorParsers;
-    private IndicesQueriesRegistry queriesRegistry;
 
     @Inject
-    public AggregationParseElement(AggregatorParsers aggregatorParsers, IndicesQueriesRegistry queriesRegistry) {
+    public AggregationParseElement(AggregatorParsers aggregatorParsers) {
         this.aggregatorParsers = aggregatorParsers;
-        this.queriesRegistry = queriesRegistry;
     }
 
     @Override
     public void parse(XContentParser parser, SearchContext context) throws Exception {
-        QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-        parseContext.reset(parser);
-        parseContext.parseFieldMatcher(context.parseFieldMatcher());
-        AggregatorFactories factories = aggregatorParsers.parseAggregators(parser, parseContext);
+        AggregatorFactories factories = aggregatorParsers.parseAggregators(parser, context);
         context.aggregations(new SearchContextAggregations(factories));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
index 50b0e06..0681996 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
@@ -72,13 +72,12 @@ public class AggregationPhase implements SearchPhase {
         if (context.aggregations() != null) {
             AggregationContext aggregationContext = new AggregationContext(context);
             context.aggregations().aggregationContext(aggregationContext);
-            context.aggregations().factories().init(aggregationContext);
 
             List<Aggregator> collectors = new ArrayList<>();
             Aggregator[] aggregators;
             try {
                 AggregatorFactories factories = context.aggregations().factories();
-                aggregators = factories.createTopLevelAggregators();
+                aggregators = factories.createTopLevelAggregators(aggregationContext);
                 for (int i = 0; i < aggregators.length; i++) {
                     if (aggregators[i] instanceof GlobalAggregator == false) {
                         collectors.add(aggregators[i]);
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java
index f104399..8ee4d1f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java
@@ -22,14 +22,11 @@ package org.elasticsearch.search.aggregations;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -62,13 +59,8 @@ public abstract class Aggregator extends BucketCollector implements Releasable {
          * @return                  The resolved aggregator factory or {@code null} in case the aggregation should be skipped
          * @throws java.io.IOException      When parsing fails
          */
-        AggregatorFactory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException;
+        AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException;
 
-        /**
-         * @return an empty {@link AggregatorFactory} instance for this parser
-         *         that can be used for deserialization
-         */
-        AggregatorFactory<?> getFactoryPrototypes();
     }
 
     /**
@@ -115,7 +107,7 @@ public abstract class Aggregator extends BucketCollector implements Releasable {
     public abstract InternalAggregation buildEmptyAggregation();
 
     /** Aggregation mode for sub aggregations. */
-    public enum SubAggCollectionMode implements Writeable<SubAggCollectionMode> {
+    public enum SubAggCollectionMode {
 
         /**
          * Creates buckets and delegates to child aggregators in a single pass over
@@ -151,19 +143,5 @@ public abstract class Aggregator extends BucketCollector implements Releasable {
             }
             throw new ElasticsearchParseException("no [{}] found for value [{}]", KEY.getPreferredName(), value);
         }
-
-        @Override
-        public SubAggCollectionMode readFrom(StreamInput in) throws IOException {
-            int ordinal = in.readVInt();
-            if (ordinal < 0 || ordinal >= values().length) {
-                throw new IOException("Unknown SubAggCollectionMode ordinal [" + ordinal + "]");
-            }
-            return values()[ordinal];
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVInt(ordinal());
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java
index c12494c..6a1cd27 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java
@@ -18,11 +18,6 @@
  */
 package org.elasticsearch.search.aggregations;
 
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
@@ -31,44 +26,33 @@ import org.elasticsearch.search.aggregations.support.AggregationPath.PathElement
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.Set;
 
 /**
  *
  */
-public class AggregatorFactories extends ToXContentToBytes implements Writeable<AggregatorFactories> {
+public class AggregatorFactories {
 
-    public static final AggregatorFactories EMPTY = new AggregatorFactories(new AggregatorFactory<?>[0],
-            new ArrayList<PipelineAggregatorFactory>());
+    public static final AggregatorFactories EMPTY = new Empty();
 
-    private AggregatorFactory<?> parent;
-    private AggregatorFactory<?>[] factories;
+    private AggregatorFactory parent;
+    private AggregatorFactory[] factories;
     private List<PipelineAggregatorFactory> pipelineAggregatorFactories;
 
     public static Builder builder() {
         return new Builder();
     }
 
-    private AggregatorFactories(AggregatorFactory<?>[] factories,
-            List<PipelineAggregatorFactory> pipelineAggregators) {
+    private AggregatorFactories(AggregatorFactory[] factories, List<PipelineAggregatorFactory> pipelineAggregators) {
         this.factories = factories;
         this.pipelineAggregatorFactories = pipelineAggregators;
     }
 
-    public void init(AggregationContext context) {
-        for (AggregatorFactory<?> factory : factories) {
-            factory.init(context);
-        }
-    }
-
     public List<PipelineAggregator> createPipelineAggregators() throws IOException {
         List<PipelineAggregator> pipelineAggregators = new ArrayList<>();
         for (PipelineAggregatorFactory factory : this.pipelineAggregatorFactories) {
@@ -82,53 +66,42 @@ public class AggregatorFactories extends ToXContentToBytes implements Writeable<
      * buckets.
      */
     public Aggregator[] createSubAggregators(Aggregator parent) throws IOException {
-        Aggregator[] aggregators = new Aggregator[countAggregators()];
+        Aggregator[] aggregators = new Aggregator[count()];
         for (int i = 0; i < factories.length; ++i) {
             // TODO: sometimes even sub aggregations always get called with bucket 0, eg. if
             // you have a terms agg under a top-level filter agg. We should have a way to
             // propagate the fact that only bucket 0 will be collected with single-bucket
             // aggs
             final boolean collectsFromSingleBucket = false;
-            aggregators[i] = factories[i].create(parent, collectsFromSingleBucket);
+            aggregators[i] = factories[i].create(parent.context(), parent, collectsFromSingleBucket);
         }
         return aggregators;
     }
 
-    public Aggregator[] createTopLevelAggregators() throws IOException {
+    public Aggregator[] createTopLevelAggregators(AggregationContext ctx) throws IOException {
         // These aggregators are going to be used with a single bucket ordinal, no need to wrap the PER_BUCKET ones
         Aggregator[] aggregators = new Aggregator[factories.length];
         for (int i = 0; i < factories.length; i++) {
             // top-level aggs only get called with bucket 0
             final boolean collectsFromSingleBucket = true;
-            aggregators[i] = factories[i].create(null, collectsFromSingleBucket);
+            aggregators[i] = factories[i].create(ctx, null, collectsFromSingleBucket);
         }
         return aggregators;
     }
 
-    /**
-     * @return the number of sub-aggregator factories not including pipeline
-     *         aggregator factories
-     */
-    public int countAggregators() {
+    public int count() {
         return factories.length;
     }
 
-    /**
-     * @return the number of pipeline aggregator factories
-     */
-    public int countPipelineAggregators() {
-        return pipelineAggregatorFactories.size();
-    }
-
-    void setParent(AggregatorFactory<?> parent) {
+    void setParent(AggregatorFactory parent) {
         this.parent = parent;
-        for (AggregatorFactory<?> factory : factories) {
+        for (AggregatorFactory factory : factories) {
             factory.parent = parent;
         }
     }
 
     public void validate() {
-        for (AggregatorFactory<?> factory : factories) {
+        for (AggregatorFactory factory : factories) {
             factory.validate();
         }
         for (PipelineAggregatorFactory factory : pipelineAggregatorFactories) {
@@ -136,24 +109,35 @@ public class AggregatorFactories extends ToXContentToBytes implements Writeable<
         }
     }
 
+    private final static class Empty extends AggregatorFactories {
+
+        private static final AggregatorFactory[] EMPTY_FACTORIES = new AggregatorFactory[0];
+        private static final Aggregator[] EMPTY_AGGREGATORS = new Aggregator[0];
+        private static final List<PipelineAggregatorFactory> EMPTY_PIPELINE_AGGREGATORS = new ArrayList<>();
+
+        private Empty() {
+            super(EMPTY_FACTORIES, EMPTY_PIPELINE_AGGREGATORS);
+        }
+
+        @Override
+        public Aggregator[] createSubAggregators(Aggregator parent) {
+            return EMPTY_AGGREGATORS;
+        }
+
+        @Override
+        public Aggregator[] createTopLevelAggregators(AggregationContext ctx) {
+            return EMPTY_AGGREGATORS;
+        }
+
+    }
+
     public static class Builder {
 
         private final Set<String> names = new HashSet<>();
-        private final List<AggregatorFactory<?>> factories = new ArrayList<>();
+        private final List<AggregatorFactory> factories = new ArrayList<>();
         private final List<PipelineAggregatorFactory> pipelineAggregatorFactories = new ArrayList<>();
-        private boolean skipResolveOrder;
-
-        public Builder addAggregators(AggregatorFactories factories) {
-            for (AggregatorFactory<?> factory : factories.factories) {
-                addAggregator(factory);
-            }
-            for (PipelineAggregatorFactory factory : factories.pipelineAggregatorFactories) {
-                addPipelineAggregator(factory);
-            }
-            return this;
-        }
 
-        public Builder addAggregator(AggregatorFactory<?> factory) {
+        public Builder addAggregator(AggregatorFactory factory) {
             if (!names.add(factory.name)) {
                 throw new IllegalArgumentException("Two sibling aggregations cannot have the same name: [" + factory.name + "]");
             }
@@ -166,35 +150,21 @@ public class AggregatorFactories extends ToXContentToBytes implements Writeable<
             return this;
         }
 
-        /**
-         * FOR TESTING ONLY
-         */
-        Builder skipResolveOrder() {
-            this.skipResolveOrder = true;
-            return this;
-        }
-
         public AggregatorFactories build() {
             if (factories.isEmpty() && pipelineAggregatorFactories.isEmpty()) {
                 return EMPTY;
             }
-            List<PipelineAggregatorFactory> orderedpipelineAggregators = null;
-            if (skipResolveOrder) {
-                orderedpipelineAggregators = new ArrayList<>(pipelineAggregatorFactories);
-            } else {
-                orderedpipelineAggregators = resolvePipelineAggregatorOrder(this.pipelineAggregatorFactories, this.factories);
-            }
-            return new AggregatorFactories(factories.toArray(new AggregatorFactory<?>[factories.size()]), orderedpipelineAggregators);
+            List<PipelineAggregatorFactory> orderedpipelineAggregators = resolvePipelineAggregatorOrder(this.pipelineAggregatorFactories, this.factories);
+            return new AggregatorFactories(factories.toArray(new AggregatorFactory[factories.size()]), orderedpipelineAggregators);
         }
 
-        private List<PipelineAggregatorFactory> resolvePipelineAggregatorOrder(List<PipelineAggregatorFactory> pipelineAggregatorFactories,
-                List<AggregatorFactory<?>> aggFactories) {
+        private List<PipelineAggregatorFactory> resolvePipelineAggregatorOrder(List<PipelineAggregatorFactory> pipelineAggregatorFactories, List<AggregatorFactory> aggFactories) {
             Map<String, PipelineAggregatorFactory> pipelineAggregatorFactoriesMap = new HashMap<>();
             for (PipelineAggregatorFactory factory : pipelineAggregatorFactories) {
                 pipelineAggregatorFactoriesMap.put(factory.getName(), factory);
             }
-            Map<String, AggregatorFactory<?>> aggFactoriesMap = new HashMap<>();
-            for (AggregatorFactory<?> aggFactory : aggFactories) {
+            Map<String, AggregatorFactory> aggFactoriesMap = new HashMap<>();
+            for (AggregatorFactory aggFactory : aggFactories) {
                 aggFactoriesMap.put(aggFactory.name, aggFactory);
             }
             List<PipelineAggregatorFactory> orderedPipelineAggregatorrs = new LinkedList<>();
@@ -208,7 +178,7 @@ public class AggregatorFactories extends ToXContentToBytes implements Writeable<
             return orderedPipelineAggregatorrs;
         }
 
-        private void resolvePipelineAggregatorOrder(Map<String, AggregatorFactory<?>> aggFactoriesMap,
+        private void resolvePipelineAggregatorOrder(Map<String, AggregatorFactory> aggFactoriesMap,
                 Map<String, PipelineAggregatorFactory> pipelineAggregatorFactoriesMap,
                 List<PipelineAggregatorFactory> orderedPipelineAggregators, List<PipelineAggregatorFactory> unmarkedFactories, Set<PipelineAggregatorFactory> temporarilyMarked,
                 PipelineAggregatorFactory factory) {
@@ -223,7 +193,7 @@ public class AggregatorFactories extends ToXContentToBytes implements Writeable<
                     if (bucketsPath.equals("_count") || bucketsPath.equals("_key")) {
                         continue;
                     } else if (aggFactoriesMap.containsKey(firstAggName)) {
-                        AggregatorFactory<?> aggFactory = aggFactoriesMap.get(firstAggName);
+                        AggregatorFactory aggFactory = aggFactoriesMap.get(firstAggName);
                         for (int i = 1; i < bucketsPathElements.size(); i++) {
                             PathElement pathElement = bucketsPathElements.get(i);
                             String aggName = pathElement.name;
@@ -232,9 +202,9 @@ public class AggregatorFactories extends ToXContentToBytes implements Writeable<
                             } else {
                                 // Check the non-pipeline sub-aggregator
                                 // factories
-                                AggregatorFactory<?>[] subFactories = aggFactory.factories.factories;
+                                AggregatorFactory[] subFactories = aggFactory.factories.factories;
                                 boolean foundSubFactory = false;
-                                for (AggregatorFactory<?> subFactory : subFactories) {
+                                for (AggregatorFactory subFactory : subFactories) {
                                     if (aggName.equals(subFactory.name)) {
                                         aggFactory = subFactory;
                                         foundSubFactory = true;
@@ -275,79 +245,12 @@ public class AggregatorFactories extends ToXContentToBytes implements Writeable<
             }
         }
 
-        AggregatorFactory<?>[] getAggregatorFactories() {
-            return this.factories.toArray(new AggregatorFactory<?>[this.factories.size()]);
+        AggregatorFactory[] getAggregatorFactories() {
+            return this.factories.toArray(new AggregatorFactory[this.factories.size()]);
         }
 
         List<PipelineAggregatorFactory> getPipelineAggregatorFactories() {
             return this.pipelineAggregatorFactories;
         }
     }
-
-    @Override
-    public AggregatorFactories readFrom(StreamInput in) throws IOException {
-        int factoriesSize = in.readVInt();
-        AggregatorFactory<?>[] factoriesList = new AggregatorFactory<?>[factoriesSize];
-        for (int i = 0; i < factoriesSize; i++) {
-            AggregatorFactory<?> factory = in.readAggregatorFactory();
-            factoriesList[i] = factory;
-        }
-        int pipelineFactoriesSize = in.readVInt();
-        List<PipelineAggregatorFactory> pipelineAggregatorFactoriesList = new ArrayList<PipelineAggregatorFactory>(pipelineFactoriesSize);
-        for (int i = 0; i < pipelineFactoriesSize; i++) {
-            PipelineAggregatorFactory factory = in.readPipelineAggregatorFactory();
-            pipelineAggregatorFactoriesList.add(factory);
-        }
-        AggregatorFactories aggregatorFactories = new AggregatorFactories(factoriesList,
-                Collections.unmodifiableList(pipelineAggregatorFactoriesList));
-        return aggregatorFactories;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(this.factories.length);
-        for (AggregatorFactory<?> factory : factories) {
-            out.writeAggregatorFactory(factory);
-        }
-        out.writeVInt(this.pipelineAggregatorFactories.size());
-        for (PipelineAggregatorFactory factory : pipelineAggregatorFactories) {
-            out.writePipelineAggregatorFactory(factory);
-        }
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        if (factories != null) {
-            for (AggregatorFactory<?> subAgg : factories) {
-                subAgg.toXContent(builder, params);
-            }
-        }
-        if (pipelineAggregatorFactories != null) {
-            for (PipelineAggregatorFactory subAgg : pipelineAggregatorFactories) {
-                subAgg.toXContent(builder, params);
-            }
-        }
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(Arrays.hashCode(factories), pipelineAggregatorFactories);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null)
-            return false;
-        if (getClass() != obj.getClass())
-            return false;
-        AggregatorFactories other = (AggregatorFactories) obj;
-        if (!Objects.deepEquals(factories, other.factories))
-            return false;
-        if (!Objects.equals(pipelineAggregatorFactories, other.pipelineAggregatorFactories))
-            return false;
-        return true;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java
index ef944c7..680e3ef 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java
@@ -18,40 +18,29 @@
  */
 package org.elasticsearch.search.aggregations;
 
-
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.Scorer;
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.NamedWriteable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.ObjectArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
 import org.elasticsearch.search.internal.SearchContext.Lifetime;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * A factory that knows how to create an {@link Aggregator} of a specific type.
  */
-public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extends ToXContentToBytes
-        implements NamedWriteable<AggregatorFactory<AF>> {
+public abstract class AggregatorFactory {
 
     protected String name;
-    protected Type type;
-    protected AggregatorFactory<?> parent;
+    protected String type;
+    protected AggregatorFactory parent;
     protected AggregatorFactories factories = AggregatorFactories.EMPTY;
     protected Map<String, Object> metaData;
-    private AggregationContext context;
 
     /**
      * Constructs a new aggregator factory.
@@ -59,66 +48,22 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extend
      * @param name  The aggregation name
      * @param type  The aggregation type
      */
-    public AggregatorFactory(String name, Type type) {
+    public AggregatorFactory(String name, String type) {
         this.name = name;
         this.type = type;
     }
 
     /**
-     * Initializes this factory with the given {@link AggregationContext} ready
-     * to create {@link Aggregator}s
-     */
-    public final void init(AggregationContext context) {
-        this.context = context;
-        doInit(context);
-        this.factories.init(context);
-    }
-
-    /**
-     * Allows the {@link AggregatorFactory} to initialize any state prior to
-     * using it to create {@link Aggregator}s.
-     *
-     * @param context
-     *            the {@link AggregationContext} to use during initialization.
-     */
-    protected void doInit(AggregationContext context) {
-    }
-
-    /**
      * Registers sub-factories with this factory. The sub-factory will be responsible for the creation of sub-aggregators under the
      * aggregator created by this factory.
      *
      * @param subFactories  The sub-factories
      * @return  this factory (fluent interface)
      */
-    public AF subFactories(AggregatorFactories subFactories) {
+    public AggregatorFactory subFactories(AggregatorFactories subFactories) {
         this.factories = subFactories;
         this.factories.setParent(this);
-        return (AF) this;
-    }
-
-    /**
-     * Add a sub aggregation to this aggregation.
-     */
-    @SuppressWarnings("unchecked")
-    public AF subAggregation(AggregatorFactory<?> aggregation) {
-        AggregatorFactories.Builder builder = AggregatorFactories.builder();
-        builder.addAggregators(factories);
-        builder.addAggregator(aggregation);
-        factories = builder.build();
-        return (AF) this;
-    }
-
-    /**
-     * Add a sub aggregation to this aggregation.
-     */
-    @SuppressWarnings("unchecked")
-    public AF subAggregation(PipelineAggregatorFactory aggregation) {
-        AggregatorFactories.Builder builder = AggregatorFactories.builder();
-        builder.addAggregators(factories);
-        builder.addPipelineAggregator(aggregation);
-        factories = builder.build();
-        return (AF) this;
+        return this;
     }
 
     public String name() {
@@ -136,7 +81,7 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extend
     /**
      * @return  The parent factory if one exists (will always return {@code null} for top level aggregator factories).
      */
-    public AggregatorFactory<?> parent() {
+    public AggregatorFactory parent() {
         return parent;
     }
 
@@ -146,83 +91,32 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extend
     /**
      * Creates the aggregator
      *
+     * @param context               The aggregation context
      * @param parent                The parent aggregator (if this is a top level factory, the parent will be {@code null})
      * @param collectsFromSingleBucket  If true then the created aggregator will only be collected with <tt>0</tt> as a bucket ordinal.
      *                              Some factories can take advantage of this in order to return more optimized implementations.
      *
      * @return                      The created aggregator
      */
-    public final Aggregator create(Aggregator parent, boolean collectsFromSingleBucket) throws IOException {
+    public final Aggregator create(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket) throws IOException {
         return createInternal(context, parent, collectsFromSingleBucket, this.factories.createPipelineAggregators(), this.metaData);
     }
 
     public void doValidate() {
     }
 
-    public AF setMetaData(Map<String, Object> metaData) {
+    public void setMetaData(Map<String, Object> metaData) {
         this.metaData = metaData;
-        return (AF) this;
-    }
-
-    @Override
-    public final AggregatorFactory<AF> readFrom(StreamInput in) throws IOException {
-        String name = in.readString();
-        AggregatorFactory<AF> factory = doReadFrom(name, in);
-        factory.factories = AggregatorFactories.EMPTY.readFrom(in);
-        factory.factories.setParent(this);
-        factory.metaData = in.readMap();
-        return factory;
     }
 
-    protected abstract AggregatorFactory<AF> doReadFrom(String name, StreamInput in) throws IOException;
-
-    @Override
-    public final void writeTo(StreamOutput out) throws IOException {
-        out.writeString(name);
-        doWriteTo(out);
-        factories.writeTo(out);
-        out.writeMap(metaData);
-    }
-
-    protected abstract void doWriteTo(StreamOutput out) throws IOException;
-
-    @Override
-    public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(name);
-
-        if (this.metaData != null) {
-            builder.field("meta", this.metaData);
-        }
-        builder.field(type.name());
-        internalXContent(builder, params);
-
-        if (factories != null && (factories.countAggregators() + factories.countPipelineAggregators()) > 0) {
-            builder.field("aggregations");
-            factories.toXContent(builder, params);
-
-        }
-
-        return builder.endObject();
-    }
 
-    protected abstract XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException;
-
-    @Override
-    public String getWriteableName() {
-        return type.stream().toUtf8();
-    }
-
-    public String getType() {
-        return type.name();
-    }
 
     /**
      * Utility method. Given an {@link AggregatorFactory} that creates {@link Aggregator}s that only know how
      * to collect bucket <tt>0</tt>, this returns an aggregator that can collect any bucket.
      */
-    protected static Aggregator asMultiBucketAggregator(final AggregatorFactory factory,
-            final AggregationContext context, final Aggregator parent) throws IOException {
-        final Aggregator first = factory.create(parent, true);
+    protected static Aggregator asMultiBucketAggregator(final AggregatorFactory factory, final AggregationContext context, final Aggregator parent) throws IOException {
+        final Aggregator first = factory.create(context, parent, true);
         final BigArrays bigArrays = context.bigArrays();
         return new Aggregator() {
 
@@ -303,7 +197,7 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extend
                         if (collector == null) {
                             Aggregator aggregator = aggregators.get(bucket);
                             if (aggregator == null) {
-                                aggregator = factory.create(parent, true);
+                                aggregator = factory.create(context, parent, true);
                                 aggregator.preCollection();
                                 aggregators.set(bucket, aggregator);
                             }
@@ -340,31 +234,4 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extend
         };
     }
 
-    @Override
-    public int hashCode() {
-        return Objects.hash(factories, metaData, name, type, doHashCode());
-    }
-
-    protected abstract int doHashCode();
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null)
-            return false;
-        if (getClass() != obj.getClass())
-            return false;
-        AggregatorFactory<AF> other = (AggregatorFactory<AF>) obj;
-        if (!Objects.equals(name, other.name))
-            return false;
-        if (!Objects.equals(type, other.type))
-            return false;
-        if (!Objects.equals(metaData, other.metaData))
-            return false;
-        if (!Objects.equals(factories, other.factories))
-            return false;
-        return doEquals(obj);
-    }
-
-    protected abstract boolean doEquals(Object obj);
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
index 357fedc..f38138f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
@@ -18,14 +18,13 @@
  */
 package org.elasticsearch.search.aggregations;
 
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.HashMap;
@@ -55,20 +54,15 @@ public class AggregatorParsers {
      *            ).
      */
     @Inject
-    public AggregatorParsers(Set<Aggregator.Parser> aggParsers, Set<PipelineAggregator.Parser> pipelineAggregatorParsers,
-            NamedWriteableRegistry namedWriteableRegistry) {
+    public AggregatorParsers(Set<Aggregator.Parser> aggParsers, Set<PipelineAggregator.Parser> pipelineAggregatorParsers) {
         Map<String, Aggregator.Parser> aggParsersBuilder = new HashMap<>(aggParsers.size());
         for (Aggregator.Parser parser : aggParsers) {
             aggParsersBuilder.put(parser.type(), parser);
-            AggregatorFactory<?> factoryPrototype = parser.getFactoryPrototypes();
-            namedWriteableRegistry.registerPrototype(AggregatorFactory.class, factoryPrototype);
         }
         this.aggParsers = unmodifiableMap(aggParsersBuilder);
         Map<String, PipelineAggregator.Parser> pipelineAggregatorParsersBuilder = new HashMap<>(pipelineAggregatorParsers.size());
         for (PipelineAggregator.Parser parser : pipelineAggregatorParsers) {
             pipelineAggregatorParsersBuilder.put(parser.type(), parser);
-            PipelineAggregatorFactory factoryPrototype = parser.getFactoryPrototype();
-            namedWriteableRegistry.registerPrototype(PipelineAggregatorFactory.class, factoryPrototype);
         }
         this.pipelineAggregatorParsers = unmodifiableMap(pipelineAggregatorParsersBuilder);
     }
@@ -99,37 +93,37 @@ public class AggregatorParsers {
      * Parses the aggregation request recursively generating aggregator factories in turn.
      *
      * @param parser    The input xcontent that will be parsed.
-     * @param parseContext   The parse context.
+     * @param context   The search context.
      *
      * @return          The parsed aggregator factories.
      *
      * @throws IOException When parsing fails for unknown reasons.
      */
-    public AggregatorFactories parseAggregators(XContentParser parser, QueryParseContext parseContext) throws IOException {
-        return parseAggregators(parser, parseContext, 0);
+    public AggregatorFactories parseAggregators(XContentParser parser, SearchContext context) throws IOException {
+        return parseAggregators(parser, context, 0);
     }
 
 
-    private AggregatorFactories parseAggregators(XContentParser parser, QueryParseContext parseContext, int level) throws IOException {
+    private AggregatorFactories parseAggregators(XContentParser parser, SearchContext context, int level) throws IOException {
         Matcher validAggMatcher = VALID_AGG_NAME.matcher("");
         AggregatorFactories.Builder factories = new AggregatorFactories.Builder();
 
         XContentParser.Token token = null;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token != XContentParser.Token.FIELD_NAME) {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " in [aggs]: aggregations definitions must start with the name of the aggregation.");
+                throw new SearchParseException(context, "Unexpected token " + token
+                        + " in [aggs]: aggregations definitions must start with the name of the aggregation.", parser.getTokenLocation());
             }
             final String aggregationName = parser.currentName();
             if (!validAggMatcher.reset(aggregationName).matches()) {
-                throw new ParsingException(parser.getTokenLocation(), "Invalid aggregation name [" + aggregationName
-                        + "]. Aggregation names must be alpha-numeric and can only contain '_' and '-'");
+                throw new SearchParseException(context, "Invalid aggregation name [" + aggregationName
+                        + "]. Aggregation names must be alpha-numeric and can only contain '_' and '-'", parser.getTokenLocation());
             }
 
             token = parser.nextToken();
             if (token != XContentParser.Token.START_OBJECT) {
-                throw new ParsingException(parser.getTokenLocation(), "Aggregation definition for [" + aggregationName + " starts with a ["
-                        + token + "], expected a [" + XContentParser.Token.START_OBJECT + "].");
+                throw new SearchParseException(context, "Aggregation definition for [" + aggregationName + " starts with a [" + token
+                        + "], expected a [" + XContentParser.Token.START_OBJECT + "].", parser.getTokenLocation());
             }
 
             AggregatorFactory aggFactory = null;
@@ -140,8 +134,7 @@ public class AggregatorParsers {
 
             while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                 if (token != XContentParser.Token.FIELD_NAME) {
-                    throw new ParsingException(
-                            parser.getTokenLocation(), "Expected [" + XContentParser.Token.FIELD_NAME + "] under a ["
+                    throw new SearchParseException(context, "Expected [" + XContentParser.Token.FIELD_NAME + "] under a ["
                             + XContentParser.Token.START_OBJECT + "], but got a [" + token + "] in [" + aggregationName + "]",
                             parser.getTokenLocation());
                 }
@@ -150,8 +143,7 @@ public class AggregatorParsers {
                 token = parser.nextToken();
                 if ("aggregations_binary".equals(fieldName)) {
                     if (subFactories != null) {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Found two sub aggregation definitions under [" + aggregationName + "]",
+                        throw new SearchParseException(context, "Found two sub aggregation definitions under [" + aggregationName + "]",
                                 parser.getTokenLocation());
                     }
                     XContentParser binaryParser = null;
@@ -159,17 +151,17 @@ public class AggregatorParsers {
                         byte[] source = parser.binaryValue();
                         binaryParser = XContentFactory.xContent(source).createParser(source);
                     } else {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Expected [" + XContentParser.Token.VALUE_STRING + " or " + XContentParser.Token.VALUE_EMBEDDED_OBJECT
-                                        + "] for [" + fieldName + "], but got a [" + token + "] in [" + aggregationName + "]");
+                        throw new SearchParseException(context, "Expected [" + XContentParser.Token.VALUE_STRING + " or "
+                                + XContentParser.Token.VALUE_EMBEDDED_OBJECT + "] for [" + fieldName + "], but got a [" + token + "] in ["
+                                + aggregationName + "]", parser.getTokenLocation());
                     }
                     XContentParser.Token binaryToken = binaryParser.nextToken();
                     if (binaryToken != XContentParser.Token.START_OBJECT) {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Expected [" + XContentParser.Token.START_OBJECT + "] as first token when parsing [" + fieldName
-                                        + "], but got a [" + binaryToken + "] in [" + aggregationName + "]");
+                        throw new SearchParseException(context, "Expected [" + XContentParser.Token.START_OBJECT
+                                + "] as first token when parsing [" + fieldName + "], but got a [" + binaryToken + "] in ["
+                                + aggregationName + "]", parser.getTokenLocation());
                     }
-                    subFactories = parseAggregators(binaryParser, parseContext, level + 1);
+                    subFactories = parseAggregators(binaryParser, context, level + 1);
                 } else if (token == XContentParser.Token.START_OBJECT) {
                     switch (fieldName) {
                     case "meta":
@@ -178,42 +170,42 @@ public class AggregatorParsers {
                     case "aggregations":
                     case "aggs":
                         if (subFactories != null) {
-                            throw new ParsingException(parser.getTokenLocation(),
-                                    "Found two sub aggregation definitions under [" + aggregationName + "]");
+                            throw new SearchParseException(context,
+                                    "Found two sub aggregation definitions under [" + aggregationName + "]", parser.getTokenLocation());
                         }
-                        subFactories = parseAggregators(parser, parseContext, level + 1);
+                        subFactories = parseAggregators(parser, context, level + 1);
                         break;
                     default:
                         if (aggFactory != null) {
-                            throw new ParsingException(parser.getTokenLocation(), "Found two aggregation type definitions in ["
-                                    + aggregationName + "]: [" + aggFactory.type + "] and [" + fieldName + "]");
+                            throw new SearchParseException(context, "Found two aggregation type definitions in [" + aggregationName
+                                    + "]: [" + aggFactory.type + "] and [" + fieldName + "]", parser.getTokenLocation());
                         }
                         if (pipelineAggregatorFactory != null) {
-                            throw new ParsingException(parser.getTokenLocation(), "Found two aggregation type definitions in ["
-                                    + aggregationName + "]: [" + pipelineAggregatorFactory + "] and [" + fieldName + "]");
+                            throw new SearchParseException(context, "Found two aggregation type definitions in [" + aggregationName
+                                    + "]: [" + pipelineAggregatorFactory + "] and [" + fieldName + "]", parser.getTokenLocation());
                         }
 
                         Aggregator.Parser aggregatorParser = parser(fieldName);
                         if (aggregatorParser == null) {
                             PipelineAggregator.Parser pipelineAggregatorParser = pipelineAggregator(fieldName);
                             if (pipelineAggregatorParser == null) {
-                                throw new ParsingException(parser.getTokenLocation(),
-                                        "Could not find aggregator type [" + fieldName + "] in [" + aggregationName + "]");
+                                throw new SearchParseException(context, "Could not find aggregator type [" + fieldName + "] in ["
+                                        + aggregationName + "]", parser.getTokenLocation());
                             } else {
-                                pipelineAggregatorFactory = pipelineAggregatorParser.parse(aggregationName, parser, parseContext);
+                                pipelineAggregatorFactory = pipelineAggregatorParser.parse(aggregationName, parser, context);
                             }
                         } else {
-                            aggFactory = aggregatorParser.parse(aggregationName, parser, parseContext);
+                            aggFactory = aggregatorParser.parse(aggregationName, parser, context);
                         }
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.START_OBJECT + "] under ["
-                            + fieldName + "], but got a [" + token + "] in [" + aggregationName + "]");
+                    throw new SearchParseException(context, "Expected [" + XContentParser.Token.START_OBJECT + "] under [" + fieldName
+                            + "], but got a [" + token + "] in [" + aggregationName + "]", parser.getTokenLocation());
                 }
             }
 
             if (aggFactory == null && pipelineAggregatorFactory == null) {
-                throw new ParsingException(parser.getTokenLocation(), "Missing definition for aggregation [" + aggregationName + "]",
+                throw new SearchParseException(context, "Missing definition for aggregation [" + aggregationName + "]",
                         parser.getTokenLocation());
             } else if (aggFactory != null) {
                 assert pipelineAggregatorFactory == null;
@@ -233,8 +225,7 @@ public class AggregatorParsers {
             } else {
                 assert pipelineAggregatorFactory != null;
                 if (subFactories != null) {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Aggregation [" + aggregationName + "] cannot define sub-aggregations",
+                    throw new SearchParseException(context, "Aggregation [" + aggregationName + "] cannot define sub-aggregations",
                             parser.getTokenLocation());
                 }
                 if (level == 0) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/ValuesSourceAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/ValuesSourceAggregationBuilder.java
new file mode 100644
index 0000000..2d3c0cf
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/ValuesSourceAggregationBuilder.java
@@ -0,0 +1,112 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.ScriptService.ScriptType;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * A base class for all bucket aggregation builders that are based on values (either script generated or field data values)
+ */
+public abstract class ValuesSourceAggregationBuilder<B extends ValuesSourceAggregationBuilder<B>> extends AggregationBuilder<B> {
+
+    private Script script;
+    private String field;
+    @Deprecated
+    private String scriptString;
+    @Deprecated
+    private String lang;
+    @Deprecated
+    private Map<String, Object> params;
+    private Object missing;
+
+    /**
+     * Constructs a new builder.
+     *
+     * @param name  The name of the aggregation.
+     * @param type  The type of the aggregation.
+     */
+    protected ValuesSourceAggregationBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    /**
+     * Sets the field from which the values will be extracted.
+     *
+     * @param field     The name of the field
+     * @return          This builder (fluent interface support)
+     */
+    @SuppressWarnings("unchecked")
+    public B field(String field) {
+        this.field = field;
+        return (B) this;
+    }
+
+    /**
+     * Sets the script which generates the values. If the script is configured along with the field (as in {@link #field(String)}), then
+     * this script will be treated as a {@code value script}. A <i>value script</i> will be applied on the values that are extracted from
+     * the field data (you can refer to that value in the script using the {@code _value} reserved variable). If only the script is configured
+     * (and the no field is configured next to it), then the script will be responsible to generate the values that will be aggregated.
+     *
+     * @param script    The configured script.
+     * @return          This builder (fluent interface support)
+     */
+    @SuppressWarnings("unchecked")
+    public B script(Script script) {
+        this.script = script;
+        return (B) this;
+    }
+
+    /**
+     * Configure the value to use when documents miss a value.
+     */
+    public B missing(Object missingValue) {
+        this.missing = missingValue;
+        return (B) this;
+    }
+
+    @Override
+    protected final XContentBuilder internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
+        builder.startObject();
+        if (field != null) {
+            builder.field("field", field);
+        }
+
+        if (script == null) {
+            if (scriptString != null) {
+                builder.field("script", new Script(scriptString, ScriptType.INLINE, lang, params));
+            }
+        } else {
+            builder.field("script", script);
+        }
+        if (missing != null) {
+            builder.field("missing", missing);
+        }
+
+        doInternalXContent(builder, builderParams);
+        return builder.endObject();
+    }
+
+    protected abstract XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException;
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenBuilder.java
new file mode 100644
index 0000000..dc2670c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenBuilder.java
@@ -0,0 +1,58 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.bucket.children;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Children} aggregation.
+ */
+public class ChildrenBuilder extends AggregationBuilder<ChildrenBuilder> {
+
+    private String childType;
+
+    /**
+     * Sole constructor.
+     */
+    public ChildrenBuilder(String name) {
+        super(name, InternalChildren.TYPE.name());
+    }
+
+    /**
+     * Set the type of children documents. This parameter is compulsory.
+     */
+    public ChildrenBuilder childType(String childType) {
+        this.childType = childType;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (childType == null) {
+            throw new SearchSourceBuilderException("child_type must be set on children aggregation [" + getName() + "]");
+        }
+        builder.field("type", childType);
+        return builder.endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java
index a5f7133..438e872 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java
@@ -18,11 +18,18 @@
  */
 package org.elasticsearch.search.aggregations.bucket.children;
 
-import org.elasticsearch.common.ParsingException;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.FieldContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -37,7 +44,7 @@ public class ChildrenParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorFactory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         String childType = null;
 
         XContentParser.Token token;
@@ -49,25 +56,45 @@ public class ChildrenParser implements Aggregator.Parser {
                 if ("type".equals(currentFieldName)) {
                     childType = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + aggregationName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (childType == null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Missing [child_type] field for children aggregation [" + aggregationName + "]");
+            throw new SearchParseException(context, "Missing [child_type] field for children aggregation [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
+        ValuesSourceConfig<ValuesSource.Bytes.WithOrdinals.ParentChild> config = new ValuesSourceConfig<>(ValuesSource.Bytes.WithOrdinals.ParentChild.class);
+        DocumentMapper childDocMapper = context.mapperService().documentMapper(childType);
 
-        return new ParentToChildrenAggregator.Factory(aggregationName, childType);
+        String parentType = null;
+        Query parentFilter = null;
+        Query childFilter = null;
+        if (childDocMapper != null) {
+            ParentFieldMapper parentFieldMapper = childDocMapper.parentFieldMapper();
+            if (!parentFieldMapper.active()) {
+                throw new SearchParseException(context, "[children] no [_parent] field not configured that points to a parent type", parser.getTokenLocation());
             }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new ParentToChildrenAggregator.Factory(null, null);
+            parentType = parentFieldMapper.type();
+            DocumentMapper parentDocMapper = context.mapperService().documentMapper(parentType);
+            if (parentDocMapper != null) {
+                // TODO: use the query API
+                parentFilter = parentDocMapper.typeFilter();
+                childFilter = childDocMapper.typeFilter();
+                ParentChildIndexFieldData parentChildIndexFieldData = context.fieldData().getForField(parentFieldMapper.fieldType());
+                config.fieldContext(new FieldContext(parentFieldMapper.fieldType().name(), parentChildIndexFieldData, parentFieldMapper.fieldType()));
+            } else {
+                config.unmapped(true);
+            }
+        } else {
+            config.unmapped(true);
+        }
+        return new ParentToChildrenAggregator.Factory(aggregationName, config, parentType, parentFilter, childFilter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java
index 32dce9a..63819b9 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java
@@ -27,18 +27,10 @@ import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.util.LongArray;
 import org.elasticsearch.common.util.LongObjectPagedHashMap;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
-import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -47,25 +39,19 @@ import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.FieldContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 // The RecordingPerReaderBucketCollector assumes per segment recording which isn't the case for this
 // aggregation, for this reason that collector can't be used
 public class ParentToChildrenAggregator extends SingleBucketAggregator {
 
-    static final ParseField TYPE_FIELD = new ParseField("type");
-
     private final String parentType;
     private final Weight childFilter;
     private final Weight parentFilter;
@@ -189,27 +175,17 @@ public class ParentToChildrenAggregator extends SingleBucketAggregator {
         Releasables.close(parentOrdToBuckets, parentOrdToOtherBuckets);
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.Bytes.WithOrdinals.ParentChild, Factory> {
-
-        private String parentType;
-        private final String childType;
-        private Query parentFilter;
-        private Query childFilter;
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.Bytes.WithOrdinals.ParentChild> {
 
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param childType
-         *            the type of children documents
-         */
-        public Factory(String name, String childType) {
-            super(name, InternalChildren.TYPE, ValuesSourceType.BYTES, ValueType.STRING);
-            this.childType = childType;
-        }
+        private final String parentType;
+        private final Query parentFilter;
+        private final Query childFilter;
 
-        @Override
-        public void doInit(AggregationContext context) {
-            resolveConfig(context);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Bytes.WithOrdinals.ParentChild> config, String parentType, Query parentFilter, Query childFilter) {
+            super(name, InternalChildren.TYPE.name(), config);
+            this.parentType = parentType;
+            this.parentFilter = parentFilter;
+            this.childFilter = childFilter;
         }
 
         @Override
@@ -234,62 +210,5 @@ public class ParentToChildrenAggregator extends SingleBucketAggregator {
                     valuesSource, maxOrd, pipelineAggregators, metaData);
         }
 
-        private void resolveConfig(AggregationContext aggregationContext) {
-            config = new ValuesSourceConfig<>(ValuesSourceType.BYTES);
-            DocumentMapper childDocMapper = aggregationContext.searchContext().mapperService().documentMapper(childType);
-
-            if (childDocMapper != null) {
-                ParentFieldMapper parentFieldMapper = childDocMapper.parentFieldMapper();
-                if (!parentFieldMapper.active()) {
-                    throw new SearchParseException(aggregationContext.searchContext(),
-                            "[children] no [_parent] field not configured that points to a parent type", null); // NOCOMMIT fix exception args
-                }
-                parentType = parentFieldMapper.type();
-                DocumentMapper parentDocMapper = aggregationContext.searchContext().mapperService().documentMapper(parentType);
-                if (parentDocMapper != null) {
-                    parentFilter = parentDocMapper.typeFilter();
-                    childFilter = childDocMapper.typeFilter();
-                    ParentChildIndexFieldData parentChildIndexFieldData = aggregationContext.searchContext().fieldData()
-                            .getForField(parentFieldMapper.fieldType());
-                    config.fieldContext(new FieldContext(parentFieldMapper.fieldType().name(), parentChildIndexFieldData,
-                            parentFieldMapper.fieldType()));
-                } else {
-                    config.unmapped(true);
-                }
-            } else {
-                config.unmapped(true);
-            }
-        }
-
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(TYPE_FIELD.getPreferredName(), childType);
-            return builder;
-        }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            String childType = in.readString();
-            Factory factory = new Factory(name, childType);
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeString(childType);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(childType);
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(childType, other.childType);
-        }
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregationBuilder.java
new file mode 100644
index 0000000..45d40bd
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregationBuilder.java
@@ -0,0 +1,60 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.filter;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Filter} aggregation.
+ */
+public class FilterAggregationBuilder extends AggregationBuilder<FilterAggregationBuilder> {
+
+    private QueryBuilder filter;
+
+    /**
+     * Sole constructor.
+     */
+    public FilterAggregationBuilder(String name) {
+        super(name, InternalFilter.TYPE.name());
+    }
+
+    /**
+     * Set the filter to use, only documents that match this filter will fall
+     * into the bucket defined by this {@link Filter} aggregation.
+     */
+    public FilterAggregationBuilder filter(QueryBuilder filter) {
+        this.filter = filter;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (filter == null) {
+            throw new SearchSourceBuilderException("filter must be set on filter aggregation [" + getName() + "]");
+        }
+        filter.toXContent(builder, params);
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java
index 7e70ffa..30c34c3 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java
@@ -23,11 +23,7 @@ import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
@@ -41,7 +37,6 @@ import org.elasticsearch.search.aggregations.support.AggregationContext;
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Aggregate all docs that match a filter.
@@ -86,20 +81,12 @@ public class FilterAggregator extends SingleBucketAggregator {
         return new InternalFilter(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends AggregatorFactory<Factory> {
+    public static class Factory extends AggregatorFactory {
 
-        private QueryBuilder<?> filter;
+        private final Query filter;
 
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param filter
-         *            Set the filter to use, only documents that match this
-         *            filter will fall into the bucket defined by this
-         *            {@link Filter} aggregation.
-         */
-        public Factory(String name, QueryBuilder<?> filter) {
-            super(name, InternalFilter.TYPE);
+        public Factory(String name, Query filter) {
+            super(name, InternalFilter.TYPE.name());
             this.filter = filter;
         }
 
@@ -116,42 +103,11 @@ public class FilterAggregator extends SingleBucketAggregator {
             IndexSearcher contextSearcher = context.searchContext().searcher();
             if (searcher != contextSearcher) {
                 searcher = contextSearcher;
-                Query filter = this.filter.toQuery(context.searchContext().indexShard().getQueryShardContext());
                 weight = contextSearcher.createNormalizedWeight(filter, false);
             }
             return new FilterAggregator(name, weight, factories, context, parent, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (filter != null) {
-                filter.toXContent(builder, params);
-            }
-            return builder;
-        }
-
-        @Override
-        protected Factory doReadFrom(String name, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, in.readQuery());
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeQuery(filter);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(filter);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(filter, other.filter);
-        }
-
     }
 }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java
index c79b447..48702da 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java
@@ -18,13 +18,12 @@
  */
 package org.elasticsearch.search.aggregations.bucket.filter;
 
-import org.elasticsearch.common.ParsingException;
+import org.apache.lucene.search.MatchAllDocsQuery;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.MatchAllQueryBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -39,21 +38,10 @@ public class FilterParser implements Aggregator.Parser {
     }
 
     @Override
-    public FilterAggregator.Factory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
-        QueryBuilder<?> filter = context.parseInnerQueryBuilder();
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        ParsedQuery filter = context.indexShard().getQueryShardContext().parseInnerFilter(parser);
 
-        if (filter == null) {
-            throw new ParsingException(null, "filter cannot be null in filter aggregation [{}]", aggregationName);
-        }
-
-        FilterAggregator.Factory factory = new FilterAggregator.Factory(aggregationName,
-                filter == null ? new MatchAllQueryBuilder() : filter);
-        return factory;
-    }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new FilterAggregator.Factory(null, null);
+        return new FilterAggregator.Factory(aggregationName, filter == null ? new MatchAllDocsQuery() : filter.query());
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregationBuilder.java
new file mode 100644
index 0000000..6f61a89
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregationBuilder.java
@@ -0,0 +1,126 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.filters;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Builder for the {@link Filters} aggregation.
+ */
+public class FiltersAggregationBuilder extends AggregationBuilder<FiltersAggregationBuilder> {
+
+    private Map<String, QueryBuilder> keyedFilters = null;
+    private List<QueryBuilder> nonKeyedFilters = null;
+    private Boolean otherBucket;
+    private String otherBucketKey;
+
+    /**
+     * Sole constructor.
+     */
+    public FiltersAggregationBuilder(String name) {
+        super(name, InternalFilters.TYPE.name());
+    }
+
+    /**
+     * Add a new filter with the given key.
+     * NOTE: if a filter was already defined for this key, then this filter will replace it.
+     * NOTE: the same {@link FiltersAggregationBuilder} cannot have both keyed and non-keyed filters
+     */
+    public FiltersAggregationBuilder filter(String key, QueryBuilder filter) {
+        if (keyedFilters == null) {
+            keyedFilters = new LinkedHashMap<>();
+        }
+        keyedFilters.put(key, filter);
+        return this;
+    }
+
+    /**
+     * Add a new filter with no key.
+     * NOTE: the same {@link FiltersAggregationBuilder} cannot have both keyed and non-keyed filters.
+     */
+    public FiltersAggregationBuilder filter(QueryBuilder filter) {
+        if (nonKeyedFilters == null) {
+            nonKeyedFilters = new ArrayList<>();
+        }
+        nonKeyedFilters.add(filter);
+        return this;
+    }
+
+    /**
+     * Include a bucket for documents not matching any filter
+     */
+    public FiltersAggregationBuilder otherBucket(boolean otherBucket) {
+        this.otherBucket = otherBucket;
+        return this;
+    }
+
+    /**
+     * The key to use for the bucket for documents not matching any filter. Will
+     * implicitly enable the other bucket if set.
+     */
+    public FiltersAggregationBuilder otherBucketKey(String otherBucketKey) {
+        this.otherBucketKey = otherBucketKey;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (keyedFilters == null && nonKeyedFilters == null) {
+            throw new SearchSourceBuilderException("At least one filter must be set on filter aggregation [" + getName() + "]");
+        }
+        if (keyedFilters != null && nonKeyedFilters != null) {
+            throw new SearchSourceBuilderException("Cannot add both keyed and non-keyed filters to filters aggregation");
+        }
+
+        if (keyedFilters != null) {
+            builder.startObject(FiltersParser.FILTERS_FIELD.getPreferredName());
+            for (Map.Entry<String, QueryBuilder> entry : keyedFilters.entrySet()) {
+                builder.field(entry.getKey());
+                entry.getValue().toXContent(builder, params);
+            }
+            builder.endObject();
+        }
+        if (nonKeyedFilters != null) {
+            builder.startArray(FiltersParser.FILTERS_FIELD.getPreferredName());
+            for (QueryBuilder filterBuilder : nonKeyedFilters) {
+                filterBuilder.toXContent(builder, params);
+            }
+            builder.endArray();
+
+        }
+        if (otherBucketKey != null) {
+            builder.field(FiltersParser.OTHER_BUCKET_KEY_FIELD.getPreferredName(), otherBucketKey);
+        }
+        if (otherBucket != null) {
+            builder.field(FiltersParser.OTHER_BUCKET_FIELD.getPreferredName(), otherBucket);
+        }
+        return builder.endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java
index e0866cf..c16089e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java
@@ -24,14 +24,7 @@ import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
@@ -45,75 +38,23 @@ import org.elasticsearch.search.aggregations.support.AggregationContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class FiltersAggregator extends BucketsAggregator {
 
-    public static final ParseField FILTERS_FIELD = new ParseField("filters");
-    public static final ParseField OTHER_BUCKET_FIELD = new ParseField("other_bucket");
-    public static final ParseField OTHER_BUCKET_KEY_FIELD = new ParseField("other_bucket_key");
+    static class KeyedFilter {
 
-    public static class KeyedFilter implements Writeable<KeyedFilter>, ToXContent {
+        final String key;
+        final Query filter;
 
-        static final KeyedFilter PROTOTYPE = new KeyedFilter(null, null);
-        private final String key;
-        private final QueryBuilder<?> filter;
-
-        public KeyedFilter(String key, QueryBuilder<?> filter) {
+        KeyedFilter(String key, Query filter) {
             this.key = key;
             this.filter = filter;
         }
-
-        public String key() {
-            return key;
-        }
-
-        public QueryBuilder<?> filter() {
-            return filter;
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(key, filter);
-            return builder;
-        }
-
-        @Override
-        public KeyedFilter readFrom(StreamInput in) throws IOException {
-            String key = in.readString();
-            QueryBuilder<?> filter = in.readQuery();
-            return new KeyedFilter(key, filter);
-    }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeString(key);
-            out.writeQuery(filter);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(key, filter);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            KeyedFilter other = (KeyedFilter) obj;
-            return Objects.equals(key, other.key)
-                    && Objects.equals(filter, other.filter);
-        }
     }
 
     private final String[] keys;
@@ -198,75 +139,23 @@ public class FiltersAggregator extends BucketsAggregator {
         return owningBucketOrdinal * totalNumKeys + filterOrd;
     }
 
-    public static class Factory extends AggregatorFactory<Factory> {
+    public static class Factory extends AggregatorFactory {
 
         private final List<KeyedFilter> filters;
-        private final boolean keyed;
-        private boolean otherBucket = false;
-        private String otherBucketKey = "_other_";
+        private final String[] keys;
+        private boolean keyed;
+        private String otherBucketKey;
 
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param filters
-         *            the KeyedFilters to use with this aggregation.
-         */
-        public Factory(String name, KeyedFilter... filters) {
-            this(name, Arrays.asList(filters));
-        }
-
-        private Factory(String name, List<KeyedFilter> filters) {
-            super(name, InternalFilters.TYPE);
+        public Factory(String name, List<KeyedFilter> filters, boolean keyed, String otherBucketKey) {
+            super(name, InternalFilters.TYPE.name());
             this.filters = filters;
-            this.keyed = true;
-        }
-
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param filters
-         *            the filters to use with this aggregation
-         */
-        public Factory(String name, QueryBuilder<?>... filters) {
-            super(name, InternalFilters.TYPE);
-            List<KeyedFilter> keyedFilters = new ArrayList<>(filters.length);
-            for (int i = 0; i < filters.length; i++) {
-                keyedFilters.add(new KeyedFilter(String.valueOf(i), filters[i]));
-            }
-            this.filters = keyedFilters;
-            this.keyed = false;
-        }
-
-        /**
-         * Set whether to include a bucket for documents not matching any filter
-         */
-        public Factory otherBucket(boolean otherBucket) {
-            this.otherBucket = otherBucket;
-            return this;
-        }
-
-        /**
-         * Get whether to include a bucket for documents not matching any filter
-         */
-        public boolean otherBucket() {
-            return otherBucket;
-        }
-
-        /**
-         * Set the key to use for the bucket for documents not matching any
-         * filter.
-         */
-        public Factory otherBucketKey(String otherBucketKey) {
+            this.keyed = keyed;
             this.otherBucketKey = otherBucketKey;
-            return this;
-        }
-
-        /**
-         * Get the key to use for the bucket for documents not matching any
-         * filter.
-         */
-        public String otherBucketKey() {
-            return otherBucketKey;
+            this.keys = new String[filters.size()];
+            for (int i = 0; i < filters.size(); ++i) {
+                KeyedFilter keyedFilter = filters.get(i);
+                this.keys[i] = keyedFilter.key;
+            }
         }
 
         // TODO: refactor in order to initialize the factory once with its parent,
@@ -274,7 +163,6 @@ public class FiltersAggregator extends BucketsAggregator {
         // (since create may be called thousands of times)
 
         private IndexSearcher searcher;
-        private String[] keys;
         private Weight[] weights;
 
         @Override
@@ -284,93 +172,12 @@ public class FiltersAggregator extends BucketsAggregator {
             if (searcher != contextSearcher) {
                 searcher = contextSearcher;
                 weights = new Weight[filters.size()];
-                keys = new String[filters.size()];
                 for (int i = 0; i < filters.size(); ++i) {
                     KeyedFilter keyedFilter = filters.get(i);
-                    this.keys[i] = keyedFilter.key;
-                    Query filter = keyedFilter.filter.toFilter(context.searchContext().indexShard().getQueryShardContext());
-                    this.weights[i] = contextSearcher.createNormalizedWeight(filter, false);
-                }
-            }
-            return new FiltersAggregator(name, factories, keys, weights, keyed, otherBucket ? otherBucketKey : null, context, parent,
-                    pipelineAggregators, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            if (keyed) {
-                builder.startObject(FILTERS_FIELD.getPreferredName());
-                for (KeyedFilter keyedFilter : filters) {
-                    builder.field(keyedFilter.key(), keyedFilter.filter());
-                }
-                builder.endObject();
-            } else {
-                builder.startArray(FILTERS_FIELD.getPreferredName());
-                for (KeyedFilter keyedFilter : filters) {
-                    builder.value(keyedFilter.filter());
-                }
-                builder.endArray();
-            }
-            builder.field(OTHER_BUCKET_FIELD.getPreferredName(), otherBucket);
-            builder.field(OTHER_BUCKET_KEY_FIELD.getPreferredName(), otherBucketKey);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected Factory doReadFrom(String name, StreamInput in) throws IOException {
-            Factory factory;
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<KeyedFilter> filters = new ArrayList<>(size);
-                for (int i = 0; i < size; i++) {
-                    filters.add(KeyedFilter.PROTOTYPE.readFrom(in));
+                    this.weights[i] = contextSearcher.createNormalizedWeight(keyedFilter.filter, false);
                 }
-                factory = new Factory(name, filters);
-            } else {
-                int size = in.readVInt();
-                QueryBuilder<?>[] filters = new QueryBuilder<?>[size];
-                for (int i = 0; i < size; i++) {
-                    filters[i] = in.readQuery();
-                }
-                factory = new Factory(name, filters);
             }
-            factory.otherBucket = in.readBoolean();
-            factory.otherBucketKey = in.readString();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeBoolean(keyed);
-            if (keyed) {
-                out.writeVInt(filters.size());
-                for (KeyedFilter keyedFilter : filters) {
-                    keyedFilter.writeTo(out);
-                }
-            } else {
-                out.writeVInt(filters.size());
-                for (KeyedFilter keyedFilter : filters) {
-                    out.writeQuery(keyedFilter.filter());
-                }
-            }
-            out.writeBoolean(otherBucket);
-            out.writeString(otherBucketKey);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(filters, keyed, otherBucket, otherBucketKey);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(filters, other.filters)
-                    && Objects.equals(keyed, other.keyed)
-                    && Objects.equals(otherBucket, other.otherBucket)
-                    && Objects.equals(otherBucketKey, other.otherBucketKey);
+            return new FiltersAggregator(name, factories, keys, weights, keyed, otherBucketKey, context, parent, pipelineAggregators, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java
index cd38f6d..8ed3707 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java
@@ -20,15 +20,13 @@
 package org.elasticsearch.search.aggregations.bucket.filters;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
+import org.elasticsearch.index.query.ParsedQuery;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -42,12 +40,6 @@ public class FiltersParser implements Aggregator.Parser {
     public static final ParseField FILTERS_FIELD = new ParseField("filters");
     public static final ParseField OTHER_BUCKET_FIELD = new ParseField("other_bucket");
     public static final ParseField OTHER_BUCKET_KEY_FIELD = new ParseField("other_bucket_key");
-    private final IndicesQueriesRegistry queriesRegistry;
-
-    @Inject
-    public FiltersParser(IndicesQueriesRegistry queriesRegistry) {
-        this.queriesRegistry = queriesRegistry;
-    }
 
     @Override
     public String type() {
@@ -55,15 +47,15 @@ public class FiltersParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorFactory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-        List<FiltersAggregator.KeyedFilter> keyedFilters = null;
-        List<QueryBuilder<?>> nonKeyedFilters = null;
+        List<FiltersAggregator.KeyedFilter> filters = new ArrayList<>();
 
         XContentParser.Token token = null;
         String currentFieldName = null;
+        Boolean keyed = null;
         String otherBucketKey = null;
-        Boolean otherBucket = false;
+        boolean otherBucket = false;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -71,53 +63,50 @@ public class FiltersParser implements Aggregator.Parser {
                 if (context.parseFieldMatcher().match(currentFieldName, OTHER_BUCKET_FIELD)) {
                     otherBucket = parser.booleanValue();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.VALUE_STRING) {
                 if (context.parseFieldMatcher().match(currentFieldName, OTHER_BUCKET_KEY_FIELD)) {
                     otherBucketKey = parser.text();
+                    otherBucket = true;
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (context.parseFieldMatcher().match(currentFieldName, FILTERS_FIELD)) {
-                    keyedFilters = new ArrayList<>();
+                    keyed = true;
                     String key = null;
                     while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                         if (token == XContentParser.Token.FIELD_NAME) {
                             key = parser.currentName();
                         } else {
-                            QueryParseContext queryParseContext = new QueryParseContext(queriesRegistry);
-                            queryParseContext.reset(parser);
-                            queryParseContext.parseFieldMatcher(context.parseFieldMatcher());
-                            QueryBuilder<?> filter = queryParseContext.parseInnerQueryBuilder();
-                            keyedFilters
-                                    .add(new FiltersAggregator.KeyedFilter(key, filter == null ? QueryBuilders.matchAllQuery() : filter));
+                            ParsedQuery filter = context.indexShard().getQueryShardContext().parseInnerFilter(parser);
+                            filters.add(new FiltersAggregator.KeyedFilter(key, filter == null ? Queries.newMatchAllQuery() : filter.query()));
                         }
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, FILTERS_FIELD)) {
-                    nonKeyedFilters = new ArrayList<>();
+                    keyed = false;
+                    int idx = 0;
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        QueryParseContext queryParseContext = new QueryParseContext(queriesRegistry);
-                        queryParseContext.reset(parser);
-                        queryParseContext.parseFieldMatcher(context.parseFieldMatcher());
-                        QueryBuilder<?> filter = queryParseContext.parseInnerQueryBuilder();
-                        nonKeyedFilters.add(filter == null ? QueryBuilders.matchAllQuery() : filter);
+                        ParsedQuery filter = context.indexShard().getQueryShardContext().parseInnerFilter(parser);
+                        filters.add(new FiltersAggregator.KeyedFilter(String.valueOf(idx), filter == null ? Queries.newMatchAllQuery()
+                                : filter.query()));
+                        idx++;
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
             }
         }
 
@@ -125,24 +114,7 @@ public class FiltersParser implements Aggregator.Parser {
             otherBucketKey = "_other_";
         }
 
-        FiltersAggregator.Factory factory;
-        if (keyedFilters != null) {
-            factory = new FiltersAggregator.Factory(aggregationName, keyedFilters.toArray(new FiltersAggregator.KeyedFilter[keyedFilters.size()]));
-        } else {
-            factory = new FiltersAggregator.Factory(aggregationName, nonKeyedFilters.toArray(new QueryBuilder<?>[nonKeyedFilters.size()]));
-        }
-        if (otherBucket != null) {
-            factory.otherBucket(otherBucket);
-        }
-        if (otherBucketKey != null) {
-            factory.otherBucketKey(otherBucketKey);
-        }
-        return factory;
-    }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new FiltersAggregator.Factory(null, new FiltersAggregator.KeyedFilter[0]);
+        return new FiltersAggregator.Factory(aggregationName, filters, keyed, otherBucketKey);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridBuilder.java
new file mode 100644
index 0000000..a1f12f4
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridBuilder.java
@@ -0,0 +1,97 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.bucket.geogrid;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Creates an aggregation based on bucketing points into GeoHashes
+ */
+public class GeoHashGridBuilder extends AggregationBuilder<GeoHashGridBuilder> {
+
+
+    private String field;
+    private int precision = GeoHashGridParams.DEFAULT_PRECISION;
+    private int requiredSize = GeoHashGridParams.DEFAULT_MAX_NUM_CELLS;
+    private int shardSize = 0;
+
+    /**
+     * Sole constructor.
+     */
+    public GeoHashGridBuilder(String name) {
+        super(name, InternalGeoHashGrid.TYPE.name());
+    }
+
+    /**
+     * Set the field to use to get geo points.
+     */
+    public GeoHashGridBuilder field(String field) {
+        this.field = field;
+        return this;
+    }
+
+    /**
+     * Set the geohash precision to use for this aggregation. The higher the
+     * precision, the more fine-grained this aggregation will be.
+     */
+    public GeoHashGridBuilder precision(int precision) {
+        this.precision = GeoHashGridParams.checkPrecision(precision);
+        return this;
+    }
+
+    /**
+     * Set the number of buckets to return.
+     */
+    public GeoHashGridBuilder size(int requiredSize) {
+        this.requiredSize = requiredSize;
+        return this;
+    }
+
+    /**
+     * Expert: Set the number of buckets to get on each shard to improve
+     * accuracy.
+     */
+    public GeoHashGridBuilder shardSize(int shardSize) {
+        this.shardSize = shardSize;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (field != null) {
+            builder.field("field", field);
+        }
+        if (precision != GeoHashGridParams.DEFAULT_PRECISION) {
+            builder.field(GeoHashGridParams.FIELD_PRECISION.getPreferredName(), precision);
+        }
+        if (requiredSize != GeoHashGridParams.DEFAULT_MAX_NUM_CELLS) {
+            builder.field(GeoHashGridParams.FIELD_SIZE.getPreferredName(), requiredSize);
+        }
+        if (shardSize != 0) {
+            builder.field(GeoHashGridParams.FIELD_SHARD_SIZE.getPreferredName(), shardSize);
+        }
+
+        return builder.endObject();
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
index 091daba..6473b5a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
@@ -21,134 +21,108 @@ package org.elasticsearch.search.aggregations.bucket.geogrid;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.SortedNumericDocValues;
 import org.apache.lucene.util.GeoHashUtils;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.index.fielddata.SortedBinaryDocValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortingNumericDocValues;
 import org.elasticsearch.index.query.GeoBoundingBoxQueryBuilder;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.BucketUtils;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.GeoPointValuesSourceParser;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Aggregates Geo information into cells determined by geohashes of a given precision.
  * WARNING - for high-precision geohashes it may prove necessary to use a {@link GeoBoundingBoxQueryBuilder}
  * aggregation to focus in on a smaller area to avoid generating too many buckets and using too much RAM
  */
-public class GeoHashGridParser extends GeoPointValuesSourceParser {
-
-    public static final int DEFAULT_PRECISION = 5;
-    public static final int DEFAULT_MAX_NUM_CELLS = 10000;
-
-    public GeoHashGridParser() {
-        super(false, false);
-    }
+public class GeoHashGridParser implements Aggregator.Parser {
 
     @Override
     public String type() {
         return InternalGeoHashGrid.TYPE.name();
     }
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new GeoGridFactory(null);
-    }
 
     @Override
-    protected GeoGridFactory createFactory(
-            String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        GeoGridFactory factory = new GeoGridFactory(aggregationName);
-        Integer precision = (Integer) otherOptions.get(GeoHashGridParams.FIELD_PRECISION);
-        if (precision != null) {
-            factory.precision(precision);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoHashGrid.TYPE, context).build();
+
+        int precision = GeoHashGridParams.DEFAULT_PRECISION;
+        int requiredSize = GeoHashGridParams.DEFAULT_MAX_NUM_CELLS;
+        int shardSize = -1;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_NUMBER ||
+                    token == XContentParser.Token.VALUE_STRING) { //Be lenient and also allow numbers enclosed in quotes
+                if (context.parseFieldMatcher().match(currentFieldName, GeoHashGridParams.FIELD_PRECISION)) {
+                    precision = GeoHashGridParams.checkPrecision(parser.intValue());
+                } else if (context.parseFieldMatcher().match(currentFieldName, GeoHashGridParams.FIELD_SIZE)) {
+                    requiredSize = parser.intValue();
+                } else if (context.parseFieldMatcher().match(currentFieldName, GeoHashGridParams.FIELD_SHARD_SIZE)) {
+                    shardSize = parser.intValue();
                 }
-        Integer size = (Integer) otherOptions.get(GeoHashGridParams.FIELD_SIZE);
-        if (size != null) {
-            factory.size(size);
-        }
-        Integer shardSize = (Integer) otherOptions.get(GeoHashGridParams.FIELD_SHARD_SIZE);
-        if (shardSize != null) {
-            factory.shardSize(shardSize);
+            } else if (token != XContentParser.Token.START_OBJECT) {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
         }
-        return factory;
-    }
 
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.VALUE_NUMBER || token == XContentParser.Token.VALUE_STRING) {
-            if (parseFieldMatcher.match(currentFieldName, GeoHashGridParams.FIELD_PRECISION)) {
-                otherOptions.put(GeoHashGridParams.FIELD_PRECISION, parser.intValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, GeoHashGridParams.FIELD_SIZE)) {
-                otherOptions.put(GeoHashGridParams.FIELD_SIZE, parser.intValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, GeoHashGridParams.FIELD_SHARD_SIZE)) {
-                otherOptions.put(GeoHashGridParams.FIELD_SHARD_SIZE, parser.intValue());
-                return true;
+        if (shardSize == 0) {
+            shardSize = Integer.MAX_VALUE;
         }
+
+        if (requiredSize == 0) {
+            requiredSize = Integer.MAX_VALUE;
         }
-        return false;
+
+        if (shardSize < 0) {
+            //Use default heuristic to avoid any wrong-ranking caused by distributed counting
+            shardSize = BucketUtils.suggestShardSideQueueSize(requiredSize, context.numberOfShards());
         }
 
-    public static class GeoGridFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, GeoGridFactory> {
+        if (shardSize < requiredSize) {
+            shardSize = requiredSize;
+        }
 
-        private int precision = DEFAULT_PRECISION;
-        private int requiredSize = DEFAULT_MAX_NUM_CELLS;
-        private int shardSize = -1;
+        return new GeoGridFactory(aggregationName, vsParser.config(), precision, requiredSize, shardSize);
 
-        public GeoGridFactory(String name) {
-            super(name, InternalGeoHashGrid.TYPE, ValuesSourceType.GEOPOINT, ValueType.GEOPOINT);
     }
 
-        public GeoGridFactory precision(int precision) {
-            this.precision = GeoHashGridParams.checkPrecision(precision);
-            return this;
-        }
-
-        public int precision() {
-            return precision;
-        }
 
-        public GeoGridFactory size(int size) {
-            this.requiredSize = size;
-            return this;
-        }
+    static class GeoGridFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint> {
 
-        public int size() {
-            return requiredSize;
-        }
+        private final int precision;
+        private final int requiredSize;
+        private final int shardSize;
 
-        public GeoGridFactory shardSize(int shardSize) {
+        public GeoGridFactory(String name, ValuesSourceConfig<ValuesSource.GeoPoint> config, int precision, int requiredSize, int shardSize) {
+            super(name, InternalGeoHashGrid.TYPE.name(), config);
+            this.precision = precision;
+            this.requiredSize = requiredSize;
             this.shardSize = shardSize;
-            return this;
-        }
-
-        public int shardSize() {
-            return shardSize;
         }
 
         @Override
@@ -157,7 +131,6 @@ public class GeoHashGridParser extends GeoPointValuesSourceParser {
             final InternalAggregation aggregation = new InternalGeoHashGrid(name, requiredSize,
                     Collections.<InternalGeoHashGrid.Bucket> emptyList(), pipelineAggregators, metaData);
             return new NonCollectingAggregator(name, aggregationContext, parent, pipelineAggregators, metaData) {
-                @Override
                 public InternalAggregation buildEmptyAggregation() {
                     return aggregation;
                 }
@@ -168,23 +141,6 @@ public class GeoHashGridParser extends GeoPointValuesSourceParser {
         protected Aggregator doCreateInternal(final ValuesSource.GeoPoint valuesSource, AggregationContext aggregationContext,
                 Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
                 throws IOException {
-            if (shardSize == 0) {
-                shardSize = Integer.MAX_VALUE;
-            }
-
-            if (requiredSize == 0) {
-                requiredSize = Integer.MAX_VALUE;
-            }
-
-            if (shardSize < 0) {
-                // Use default heuristic to avoid any wrong-ranking caused by
-                // distributed counting
-                shardSize = BucketUtils.suggestShardSideQueueSize(requiredSize, aggregationContext.searchContext().numberOfShards());
-            }
-
-            if (shardSize < requiredSize) {
-                shardSize = requiredSize;
-            }
             if (collectsFromSingleBucket == false) {
                 return asMultiBucketAggregator(this, aggregationContext, parent);
             }
@@ -194,52 +150,6 @@ public class GeoHashGridParser extends GeoPointValuesSourceParser {
 
         }
 
-        @Override
-        protected GeoGridFactory innerReadFrom(
-                String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            GeoGridFactory factory = new GeoGridFactory(name);
-            factory.precision = in.readVInt();
-            factory.requiredSize = in.readVInt();
-            factory.shardSize = in.readVInt();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(precision);
-            out.writeVInt(requiredSize);
-            out.writeVInt(shardSize);
-        }
-
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(GeoHashGridParams.FIELD_PRECISION.getPreferredName(), precision);
-            builder.field(GeoHashGridParams.FIELD_SIZE.getPreferredName(), requiredSize);
-            builder.field(GeoHashGridParams.FIELD_SHARD_SIZE.getPreferredName(), shardSize);
-            return builder;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            GeoGridFactory other = (GeoGridFactory) obj;
-            if (precision != other.precision) {
-                return false;
-            }
-            if (requiredSize != other.requiredSize) {
-                return false;
-            }
-            if (shardSize != other.shardSize) {
-                return false;
-            }
-            return true;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(precision, requiredSize, shardSize);
-        }
-
         private static class CellValues extends SortingNumericDocValues {
             private MultiGeoPointValues geoValues;
             private int precision;
@@ -297,4 +207,4 @@ public class GeoHashGridParser extends GeoPointValuesSourceParser {
 
         }
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java
index c0f4e5f..63f47e1 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java
@@ -19,9 +19,6 @@
 package org.elasticsearch.search.aggregations.bucket.global;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
@@ -71,10 +68,10 @@ public class GlobalAggregator extends SingleBucketAggregator {
         throw new UnsupportedOperationException("global aggregations cannot serve as sub-aggregations, hence should never be called on #buildEmptyAggregations");
     }
 
-    public static class Factory extends AggregatorFactory<Factory> {
+    public static class Factory extends AggregatorFactory {
 
         public Factory(String name) {
-            super(name, InternalGlobal.TYPE);
+            super(name, InternalGlobal.TYPE.name());
         }
 
         @Override
@@ -90,32 +87,5 @@ public class GlobalAggregator extends SingleBucketAggregator {
             return new GlobalAggregator(name, factories, context, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected Factory doReadFrom(String name, StreamInput in) throws IOException {
-            return new Factory(name);
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            // Nothing to write
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            return true;
-        }
-
-        @Override
-        protected int doHashCode() {
-            return 0;
-        }
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalBuilder.java
new file mode 100644
index 0000000..3e9f2ba
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalBuilder.java
@@ -0,0 +1,43 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.global;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Global} aggregation.
+ */
+public class GlobalBuilder extends AggregationBuilder<GlobalBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public GlobalBuilder(String name) {
+        super(name, InternalGlobal.TYPE.name());
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        return builder.startObject().endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalParser.java
index 97b6a33..c70cf0f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalParser.java
@@ -19,9 +19,9 @@
 package org.elasticsearch.search.aggregations.bucket.global;
 
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -36,14 +36,9 @@ public class GlobalParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorFactory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         parser.nextToken();
         return new GlobalAggregator.Factory(aggregationName);
     }
 
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new GlobalAggregator.Factory(null);
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java
new file mode 100644
index 0000000..67caf37
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java
@@ -0,0 +1,186 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.histogram;
+
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+import org.joda.time.DateTime;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@code DateHistogram} aggregation.
+ */
+public class DateHistogramBuilder extends ValuesSourceAggregationBuilder<DateHistogramBuilder> {
+
+    private Object interval;
+    private Histogram.Order order;
+    private Long minDocCount;
+    private Object extendedBoundsMin;
+    private Object extendedBoundsMax;
+    private String timeZone;
+    private String format;
+    private String offset;
+
+    /**
+     * Sole constructor.
+     */
+    public DateHistogramBuilder(String name) {
+        super(name, InternalDateHistogram.TYPE.name());
+    }
+
+    /**
+     * Set the interval in milliseconds.
+     */
+    public DateHistogramBuilder interval(long interval) {
+        this.interval = interval;
+        return this;
+    }
+
+    /**
+     * Set the interval.
+     */
+    public DateHistogramBuilder interval(DateHistogramInterval interval) {
+        this.interval = interval;
+        return this;
+    }
+
+    /**
+     * Set the order by which the buckets will be returned.
+     */
+    public DateHistogramBuilder order(Histogram.Order order) {
+        this.order = order;
+        return this;
+    }
+
+    /**
+     * Set the minimum document count per bucket. Buckets with less documents
+     * than this min value will not be returned.
+     */
+    public DateHistogramBuilder minDocCount(long minDocCount) {
+        this.minDocCount = minDocCount;
+        return this;
+    }
+
+    /**
+     * Set the timezone in which to translate dates before computing buckets.
+     */
+    public DateHistogramBuilder timeZone(String timeZone) {
+        this.timeZone = timeZone;
+        return this;
+    }
+
+    /**
+     * @param offset sets the offset of time intervals in this histogram
+     * @return the current builder
+     */
+    public DateHistogramBuilder offset(String offset) {
+       this.offset = offset;
+       return this;
+    }
+
+    /**
+     * Set the format to use for dates.
+     */
+    public DateHistogramBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    /**
+     * Set extended bounds for the histogram. In case the lower value in the
+     * histogram would be greater than <code>min</code> or the upper value would
+     * be less than <code>max</code>, empty buckets will be generated.
+     */
+    public DateHistogramBuilder extendedBounds(Long min, Long max) {
+        extendedBoundsMin = min;
+        extendedBoundsMax = max;
+        return this;
+    }
+
+    /**
+     * Set extended bounds for the histogram. In case the lower value in the
+     * histogram would be greater than <code>min</code> or the upper value would
+     * be less than <code>max</code>, empty buckets will be generated.
+     */
+    public DateHistogramBuilder extendedBounds(String min, String max) {
+        extendedBoundsMin = min;
+        extendedBoundsMax = max;
+        return this;
+    }
+
+    /**
+     * Set extended bounds for the histogram. In case the lower value in the
+     * histogram would be greater than <code>min</code> or the upper value would
+     * be less than <code>max</code>, empty buckets will be generated.
+     */
+    public DateHistogramBuilder extendedBounds(DateTime min, DateTime max) {
+        extendedBoundsMin = min;
+        extendedBoundsMax = max;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (interval == null) {
+            throw new SearchSourceBuilderException("[interval] must be defined for histogram aggregation [" + getName() + "]");
+        }
+        if (interval instanceof Number) {
+            interval = TimeValue.timeValueMillis(((Number) interval).longValue()).toString();
+        }
+        builder.field("interval", interval);
+
+        if (minDocCount != null) {
+            builder.field("min_doc_count", minDocCount);
+        }
+
+        if (order != null) {
+            builder.field("order");
+            order.toXContent(builder, params);
+        }
+
+        if (timeZone != null) {
+            builder.field("time_zone", timeZone);
+        }
+
+        if (offset != null) {
+            builder.field("offset", offset);
+        }
+
+        if (format != null) {
+            builder.field("format", format);
+        }
+
+        if (extendedBoundsMin != null || extendedBoundsMax != null) {
+            builder.startObject(DateHistogramParser.EXTENDED_BOUNDS.getPreferredName());
+            if (extendedBoundsMin != null) {
+                builder.field("min", extendedBoundsMin);
+            }
+            if (extendedBoundsMax != null) {
+                builder.field("max", extendedBoundsMax);
+            }
+            builder.endObject();
+        }
+
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java
index cd2c854..7e99b38 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java
@@ -19,17 +19,10 @@
 
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
-import java.util.Objects;
-
 /**
  * The interval the date histogram is based on.
  */
-public class DateHistogramInterval implements Writeable<DateHistogramInterval> {
+public class DateHistogramInterval {
 
     public static final DateHistogramInterval SECOND = new DateHistogramInterval("1s");
     public static final DateHistogramInterval MINUTE = new DateHistogramInterval("1m");
@@ -40,10 +33,6 @@ public class DateHistogramInterval implements Writeable<DateHistogramInterval> {
     public static final DateHistogramInterval QUARTER = new DateHistogramInterval("1q");
     public static final DateHistogramInterval YEAR = new DateHistogramInterval("1y");
 
-    public static final DateHistogramInterval readFromStream(StreamInput in) throws IOException {
-        return SECOND.readFrom(in);
-    }
-
     public static DateHistogramInterval seconds(int sec) {
         return new DateHistogramInterval(sec + "s");
     }
@@ -74,31 +63,4 @@ public class DateHistogramInterval implements Writeable<DateHistogramInterval> {
     public String toString() {
         return expression;
     }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(expression);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        DateHistogramInterval other = (DateHistogramInterval) obj;
-        return Objects.equals(expression, other.expression);
-    }
-
-    @Override
-    public DateHistogramInterval readFrom(StreamInput in) throws IOException {
-        return new DateHistogramInterval(in.readString());
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(expression);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
index 89b864c..694abf2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
@@ -19,23 +19,55 @@
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.rounding.DateTimeUnit;
 import org.elasticsearch.common.rounding.Rounding;
+import org.elasticsearch.common.rounding.TimeZoneRounding;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.DateHistogramFactory;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
+import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  *
  */
-public class DateHistogramParser extends HistogramParser {
+public class DateHistogramParser implements Aggregator.Parser {
+
+    static final ParseField EXTENDED_BOUNDS = new ParseField("extended_bounds");
+    static final ParseField OFFSET = new ParseField("offset");
+    static final ParseField INTERVAL = new ParseField("interval");
+
+    public static final Map<String, DateTimeUnit> DATE_FIELD_UNITS;
 
-    public DateHistogramParser() {
-        super(true);
+    static {
+        Map<String, DateTimeUnit> dateFieldUnits = new HashMap<>();
+        dateFieldUnits.put("year", DateTimeUnit.YEAR_OF_CENTURY);
+        dateFieldUnits.put("1y", DateTimeUnit.YEAR_OF_CENTURY);
+        dateFieldUnits.put("quarter", DateTimeUnit.QUARTER);
+        dateFieldUnits.put("1q", DateTimeUnit.QUARTER);
+        dateFieldUnits.put("month", DateTimeUnit.MONTH_OF_YEAR);
+        dateFieldUnits.put("1M", DateTimeUnit.MONTH_OF_YEAR);
+        dateFieldUnits.put("week", DateTimeUnit.WEEK_OF_WEEKYEAR);
+        dateFieldUnits.put("1w", DateTimeUnit.WEEK_OF_WEEKYEAR);
+        dateFieldUnits.put("day", DateTimeUnit.DAY_OF_MONTH);
+        dateFieldUnits.put("1d", DateTimeUnit.DAY_OF_MONTH);
+        dateFieldUnits.put("hour", DateTimeUnit.HOUR_OF_DAY);
+        dateFieldUnits.put("1h", DateTimeUnit.HOUR_OF_DAY);
+        dateFieldUnits.put("minute", DateTimeUnit.MINUTES_OF_HOUR);
+        dateFieldUnits.put("1m", DateTimeUnit.MINUTES_OF_HOUR);
+        dateFieldUnits.put("second", DateTimeUnit.SECOND_OF_MINUTE);
+        dateFieldUnits.put("1s", DateTimeUnit.SECOND_OF_MINUTE);
+        DATE_FIELD_UNITS = unmodifiableMap(dateFieldUnits);
     }
 
     @Override
@@ -44,47 +76,127 @@ public class DateHistogramParser extends HistogramParser {
     }
 
     @Override
-    protected Object parseStringInterval(String text) {
-        return new DateHistogramInterval(text);
-    }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-    @Override
-    protected DateHistogramFactory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        HistogramAggregator.DateHistogramFactory factory = new HistogramAggregator.DateHistogramFactory(aggregationName);
-        Object interval = otherOptions.get(Rounding.Interval.INTERVAL_FIELD);
-        if (interval == null) {
-            throw new ParsingException(null, "Missing required field [interval] for histogram aggregation [" + aggregationName + "]");
-        } else if (interval instanceof Long) {
-            factory.interval((Long) interval);
-        } else if (interval instanceof DateHistogramInterval) {
-            factory.dateHistogramInterval((DateHistogramInterval) interval);
-        }
-        Long offset = (Long) otherOptions.get(Rounding.OffsetRounding.OFFSET_FIELD);
-        if (offset != null) {
-            factory.offset(offset);
-        }
+        ValuesSourceParser vsParser = ValuesSourceParser.numeric(aggregationName, InternalDateHistogram.TYPE, context)
+                .targetValueType(ValueType.DATE)
+                .formattable(true)
+                .timezoneAware(true)
+                .build();
 
-        ExtendedBounds extendedBounds = (ExtendedBounds) otherOptions.get(ExtendedBounds.EXTENDED_BOUNDS_FIELD);
-        if (extendedBounds != null) {
-            factory.extendedBounds(extendedBounds);
-        }
-        Boolean keyed = (Boolean) otherOptions.get(HistogramAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
+        boolean keyed = false;
+        long minDocCount = 0;
+        ExtendedBounds extendedBounds = null;
+        InternalOrder order = (InternalOrder) Histogram.Order.KEY_ASC;
+        String interval = null;
+        long offset = 0;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_STRING) {
+                if (context.parseFieldMatcher().match(currentFieldName, OFFSET)) {
+                    offset = parseOffset(parser.text());
+                } else if (context.parseFieldMatcher().match(currentFieldName, INTERVAL)) {
+                    interval = parser.text();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                if ("min_doc_count".equals(currentFieldName) || "minDocCount".equals(currentFieldName)) {
+                    minDocCount = parser.longValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.START_OBJECT) {
+                if ("order".equals(currentFieldName)) {
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            currentFieldName = parser.currentName();
+                        } else if (token == XContentParser.Token.VALUE_STRING) {
+                            String dir = parser.text();
+                            boolean asc = "asc".equals(dir);
+                            order = resolveOrder(currentFieldName, asc);
+                            //TODO should we throw an error if the value is not "asc" or "desc"???
+                        }
+                    }
+                } else if (context.parseFieldMatcher().match(currentFieldName, EXTENDED_BOUNDS)) {
+                    extendedBounds = new ExtendedBounds();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            currentFieldName = parser.currentName();
+                        } else if (token == XContentParser.Token.VALUE_STRING) {
+                            if ("min".equals(currentFieldName)) {
+                                extendedBounds.minAsStr = parser.text();
+                            } else if ("max".equals(currentFieldName)) {
+                                extendedBounds.maxAsStr = parser.text();
+                            } else {
+                                throw new SearchParseException(context, "Unknown extended_bounds key for a " + token + " in aggregation ["
+                                        + aggregationName + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                            }
+                        } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                            if ("min".equals(currentFieldName)) {
+                                extendedBounds.min = parser.longValue();
+                            } else if ("max".equals(currentFieldName)) {
+                                extendedBounds.max = parser.longValue();
+                            } else {
+                                throw new SearchParseException(context, "Unknown extended_bounds key for a " + token + " in aggregation ["
+                                        + aggregationName + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                            }
+                        } else {
+                            throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                    + currentFieldName + "].", parser.getTokenLocation());
+                        }
+                    }
+
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
         }
-        Long minDocCount = (Long) otherOptions.get(HistogramAggregator.MIN_DOC_COUNT_FIELD);
-        if (minDocCount != null) {
-            factory.minDocCount(minDocCount);
+
+        if (interval == null) {
+            throw new SearchParseException(context,
+                    "Missing required field [interval] for histogram aggregation [" + aggregationName + "]", parser.getTokenLocation());
         }
-        InternalOrder order = (InternalOrder) otherOptions.get(HistogramAggregator.ORDER_FIELD);
-        if (order != null) {
-            factory.order(order);
+
+        TimeZoneRounding.Builder tzRoundingBuilder;
+        DateTimeUnit dateTimeUnit = DATE_FIELD_UNITS.get(interval);
+        if (dateTimeUnit != null) {
+            tzRoundingBuilder = TimeZoneRounding.builder(dateTimeUnit);
+        } else {
+            // the interval is a time value?
+            tzRoundingBuilder = TimeZoneRounding.builder(TimeValue.parseTimeValue(interval, null, getClass().getSimpleName() + ".interval"));
         }
-        return factory;
+
+        Rounding rounding = tzRoundingBuilder
+                .timeZone(vsParser.input().timezone())
+                .offset(offset).build();
+
+        ValuesSourceConfig config = vsParser.config();
+        return new HistogramAggregator.Factory(aggregationName, config, rounding, order, keyed, minDocCount, extendedBounds,
+                new InternalDateHistogram.Factory());
+
     }
 
-    static InternalOrder resolveOrder(String key, boolean asc) {
+    private static InternalOrder resolveOrder(String key, boolean asc) {
         if ("_key".equals(key) || "_time".equals(key)) {
             return (InternalOrder) (asc ? InternalOrder.KEY_ASC : InternalOrder.KEY_DESC);
         }
@@ -94,13 +206,11 @@ public class DateHistogramParser extends HistogramParser {
         return new InternalOrder.Aggregation(key, asc);
     }
 
-    @Override
-    protected long parseStringOffset(String offset) throws IOException {
-        return DateHistogramFactory.parseStringOffset(offset);
-    }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return HistogramAggregator.DateHistogramFactory.PROTOTYPE;
+    private long parseOffset(String offset) throws IOException {
+        if (offset.charAt(0) == '-') {
+            return -TimeValue.parseTimeValue(offset.substring(1), null, getClass().getSimpleName() + ".parseOffset").millis();
+        }
+        int beginIndex = offset.charAt(0) == '+' ? 1 : 0;
+        return TimeValue.parseTimeValue(offset.substring(beginIndex), null, getClass().getSimpleName() + ".parseOffset").millis();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java
index 91a3d23..c703058 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java
@@ -19,32 +19,19 @@
 
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.rounding.Rounding;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.support.format.ValueParser;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  *
  */
-public class ExtendedBounds implements ToXContent {
-
-    static final ParseField EXTENDED_BOUNDS_FIELD = new ParseField("extended_bounds");
-    static final ParseField MIN_FIELD = new ParseField("min");
-    static final ParseField MAX_FIELD = new ParseField("max");
-
-    private static final ExtendedBounds PROTOTYPE = new ExtendedBounds();
+public class ExtendedBounds {
 
     Long min;
     Long max;
@@ -54,16 +41,11 @@ public class ExtendedBounds implements ToXContent {
 
     ExtendedBounds() {} //for serialization
 
-    public ExtendedBounds(Long min, Long max) {
+    ExtendedBounds(Long min, Long max) {
         this.min = min;
         this.max = max;
     }
 
-    public ExtendedBounds(String minAsStr, String maxAsStr) {
-        this.minAsStr = minAsStr;
-        this.maxAsStr = maxAsStr;
-    }
-
     void processAndValidate(String aggName, SearchContext context, ValueParser parser) {
         assert parser != null;
         if (minAsStr != null) {
@@ -95,8 +77,6 @@ public class ExtendedBounds implements ToXContent {
         } else {
             out.writeBoolean(false);
         }
-        out.writeOptionalString(minAsStr);
-        out.writeOptionalString(maxAsStr);
     }
 
     static ExtendedBounds readFrom(StreamInput in) throws IOException {
@@ -107,79 +87,6 @@ public class ExtendedBounds implements ToXContent {
         if (in.readBoolean()) {
             bounds.max = in.readLong();
         }
-        bounds.minAsStr = in.readOptionalString();
-        bounds.maxAsStr = in.readOptionalString();
         return bounds;
     }
-
-    public ExtendedBounds fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher, String aggregationName)
-            throws IOException {
-        XContentParser.Token token = null;
-        String currentFieldName = null;
-        ExtendedBounds extendedBounds = new ExtendedBounds();
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token == XContentParser.Token.VALUE_STRING) {
-                if ("min".equals(currentFieldName)) {
-                    extendedBounds.minAsStr = parser.text();
-                } else if ("max".equals(currentFieldName)) {
-                    extendedBounds.maxAsStr = parser.text();
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown extended_bounds key for a " + token
-                            + " in aggregation [" + aggregationName + "]: [" + currentFieldName + "].");
-                }
-            } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                if (parseFieldMatcher.match(currentFieldName, MIN_FIELD)) {
-                    extendedBounds.min = parser.longValue(true);
-                } else if (parseFieldMatcher.match(currentFieldName, MAX_FIELD)) {
-                    extendedBounds.max = parser.longValue(true);
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown extended_bounds key for a " + token
-                            + " in aggregation [" + aggregationName + "]: [" + currentFieldName + "].");
-                }
-            }
-        }
-        return extendedBounds;
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(EXTENDED_BOUNDS_FIELD.getPreferredName());
-        if (min != null) {
-            builder.field(MIN_FIELD.getPreferredName(), min);
-        } else {
-            builder.field(MIN_FIELD.getPreferredName(), minAsStr);
-        }
-        if (max != null) {
-            builder.field(MAX_FIELD.getPreferredName(), max);
-        } else {
-            builder.field(MAX_FIELD.getPreferredName(), maxAsStr);
-        }
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(min, max);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        ExtendedBounds other = (ExtendedBounds) obj;
-        return Objects.equals(min, other.min)
-                && Objects.equals(min, other.min);
-    }
-
-    public static ExtendedBounds parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, String aggregationName)
-            throws IOException {
-        return PROTOTYPE.fromXContent(parser, parseFieldMatcher, aggregationName);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java
index d357562..d2ca0a9 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java
@@ -21,18 +21,10 @@ package org.elasticsearch.search.aggregations.bucket.histogram;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.SortedNumericDocValues;
 import org.apache.lucene.util.CollectionUtil;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.internal.Nullable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
-import org.elasticsearch.common.rounding.DateTimeUnit;
 import org.elasticsearch.common.rounding.Rounding;
-import org.elasticsearch.common.rounding.TimeZoneRounding;
-import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.LongHash;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -41,29 +33,19 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collections;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
-
-import static java.util.Collections.unmodifiableMap;
 
 public class HistogramAggregator extends BucketsAggregator {
 
-    public static final ParseField ORDER_FIELD = new ParseField("order");
-    public static final ParseField KEYED_FIELD = new ParseField("keyed");
-    public static final ParseField MIN_DOC_COUNT_FIELD = new ParseField("min_doc_count");
-
     private final ValuesSource.Numeric valuesSource;
     private final ValueFormatter formatter;
     private final Rounding rounding;
@@ -162,117 +144,48 @@ public class HistogramAggregator extends BucketsAggregator {
         Releasables.close(bucketOrds);
     }
 
-    public static class Factory<AF extends Factory<AF>> extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, AF> {
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric> {
 
-        public static final Factory PROTOTYPE = new Factory("");
-
-        private long interval;
-        private long offset = 0;
-        private InternalOrder order = (InternalOrder) Histogram.Order.KEY_ASC;
-        private boolean keyed = false;
-        private long minDocCount = 0;
-        private ExtendedBounds extendedBounds;
+        private final Rounding rounding;
+        private final InternalOrder order;
+        private final boolean keyed;
+        private final long minDocCount;
+        private final ExtendedBounds extendedBounds;
         private final InternalHistogram.Factory<?> histogramFactory;
 
-        public Factory(String name) {
-            this(name, InternalHistogram.HISTOGRAM_FACTORY);
-        }
-
-        private Factory(String name, InternalHistogram.Factory<?> histogramFactory) {
-            super(name, histogramFactory.type(), ValuesSourceType.NUMERIC, histogramFactory.valueType());
-            this.histogramFactory = histogramFactory;
-        }
-
-        public long interval() {
-            return interval;
-        }
-
-        public AF interval(long interval) {
-            this.interval = interval;
-            return (AF) this;
-        }
-
-        public long offset() {
-            return offset;
-        }
-
-        public AF offset(long offset) {
-            this.offset = offset;
-            return (AF) this;
-        }
-
-        public Histogram.Order order() {
-            return order;
-        }
-
-        public AF order(Histogram.Order order) {
-            this.order = (InternalOrder) order;
-            return (AF) this;
-        }
-
-        public boolean keyed() {
-            return keyed;
-        }
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> config,
+                       Rounding rounding, InternalOrder order, boolean keyed, long minDocCount,
+                       ExtendedBounds extendedBounds, InternalHistogram.Factory<?> histogramFactory) {
 
-        public AF keyed(boolean keyed) {
+            super(name, histogramFactory.type(), config);
+            this.rounding = rounding;
+            this.order = order;
             this.keyed = keyed;
-            return (AF) this;
-        }
-
-        public long minDocCount() {
-            return minDocCount;
-        }
-
-        public AF minDocCount(long minDocCount) {
             this.minDocCount = minDocCount;
-            return (AF) this;
-        }
-
-        public ExtendedBounds extendedBounds() {
-            return extendedBounds;
-        }
-
-        public AF extendedBounds(ExtendedBounds extendedBounds) {
             this.extendedBounds = extendedBounds;
-            return (AF) this;
+            this.histogramFactory = histogramFactory;
         }
 
-        public InternalHistogram.Factory<?> getHistogramFactory() {
-            return histogramFactory;
+        public long minDocCount() {
+            return minDocCount;
         }
 
         @Override
         protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
                 Map<String, Object> metaData) throws IOException {
-            Rounding rounding = createRounding();
             return new HistogramAggregator(name, factories, rounding, order, keyed, minDocCount, extendedBounds, null, config.formatter(),
                     histogramFactory, aggregationContext, parent, pipelineAggregators, metaData);
         }
 
-        protected Rounding createRounding() {
-            if (interval < 1) {
-                throw new ParsingException(null, "[interval] must be 1 or greater for histogram aggregation [" + name() + "]: " + interval);
-            }
-
-            Rounding rounding = new Rounding.Interval(interval);
-            if (offset != 0) {
-                rounding = new Rounding.OffsetRounding(rounding, offset);
-            }
-            return rounding;
-        }
-
         @Override
         protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                throws IOException {
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
             if (collectsFromSingleBucket == false) {
                 return asMultiBucketAggregator(this, aggregationContext, parent);
             }
-            Rounding rounding = createRounding();
-            // we need to round the bounds given by the user and we have to do it for every aggregator we create
+            // we need to round the bounds given by the user and we have to do it for every aggregator we crate
             // as the rounding is not necessarily an idempotent operation.
-            // todo we need to think of a better structure to the factory/agtor
-            // code so we won't need to do that
+            // todo we need to think of a better structure to the factory/agtor code so we won't need to do that
             ExtendedBounds roundedBounds = null;
             if (extendedBounds != null) {
                 // we need to process & validate here using the parser
@@ -283,241 +196,5 @@ public class HistogramAggregator extends BucketsAggregator {
                     config.formatter(), histogramFactory, aggregationContext, parent, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-
-            builder.field(Rounding.Interval.INTERVAL_FIELD.getPreferredName());
-            doXContentInterval(builder, params);
-            builder.field(Rounding.OffsetRounding.OFFSET_FIELD.getPreferredName(), offset);
-
-            if (order != null) {
-                builder.field(ORDER_FIELD.getPreferredName());
-                order.toXContent(builder, params);
-            }
-
-            builder.field(KEYED_FIELD.getPreferredName(), keyed);
-
-            builder.field(MIN_DOC_COUNT_FIELD.getPreferredName(), minDocCount);
-
-            if (extendedBounds != null) {
-                extendedBounds.toXContent(builder, params);
-            }
-
-            return builder;
-        }
-
-        protected XContentBuilder doXContentInterval(XContentBuilder builder, Params params) throws IOException {
-            builder.value(interval);
-            return builder;
-        }
-
-        @Override
-        public String getWriteableName() {
-            return InternalHistogram.TYPE.name();
-        }
-
-        @Override
-        protected AF innerReadFrom(String name, ValuesSourceType valuesSourceType, ValueType targetValueType, StreamInput in)
-                throws IOException {
-            Factory<AF> factory = createFactoryFromStream(name, in);
-            factory.interval = in.readVLong();
-            factory.offset = in.readVLong();
-            if (in.readBoolean()) {
-                factory.order = InternalOrder.Streams.readOrder(in);
-            }
-            factory.keyed = in.readBoolean();
-            factory.minDocCount = in.readVLong();
-            if (in.readBoolean()) {
-                factory.extendedBounds = ExtendedBounds.readFrom(in);
-            }
-            return (AF) factory;
-        }
-
-        protected Factory<AF> createFactoryFromStream(String name, StreamInput in)
-                throws IOException {
-            return new Factory<AF>(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            writeFactoryToStream(out);
-            out.writeVLong(interval);
-            out.writeVLong(offset);
-            boolean hasOrder = order != null;
-            out.writeBoolean(hasOrder);
-            if (hasOrder) {
-                InternalOrder.Streams.writeOrder(order, out);
-            }
-            out.writeBoolean(keyed);
-            out.writeVLong(minDocCount);
-            boolean hasExtendedBounds = extendedBounds != null;
-            out.writeBoolean(hasExtendedBounds);
-            if (hasExtendedBounds) {
-                extendedBounds.writeTo(out);
-            }
-        }
-
-        protected void writeFactoryToStream(StreamOutput out) throws IOException {
-            // Default impl does nothing
-    }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(histogramFactory, interval, offset, order, keyed, minDocCount, extendedBounds);
-    }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(histogramFactory, other.histogramFactory)
-                    && Objects.equals(interval, other.interval)
-                    && Objects.equals(offset, other.offset)
-                    && Objects.equals(order, other.order)
-                    && Objects.equals(keyed, other.keyed)
-                    && Objects.equals(minDocCount, other.minDocCount)
-                    && Objects.equals(extendedBounds, other.extendedBounds);
-        }
-    }
-
-    public static class DateHistogramFactory extends Factory<DateHistogramFactory> {
-
-        public static final DateHistogramFactory PROTOTYPE = new DateHistogramFactory("");
-        public static final Map<String, DateTimeUnit> DATE_FIELD_UNITS;
-
-        static {
-            Map<String, DateTimeUnit> dateFieldUnits = new HashMap<>();
-            dateFieldUnits.put("year", DateTimeUnit.YEAR_OF_CENTURY);
-            dateFieldUnits.put("1y", DateTimeUnit.YEAR_OF_CENTURY);
-            dateFieldUnits.put("quarter", DateTimeUnit.QUARTER);
-            dateFieldUnits.put("1q", DateTimeUnit.QUARTER);
-            dateFieldUnits.put("month", DateTimeUnit.MONTH_OF_YEAR);
-            dateFieldUnits.put("1M", DateTimeUnit.MONTH_OF_YEAR);
-            dateFieldUnits.put("week", DateTimeUnit.WEEK_OF_WEEKYEAR);
-            dateFieldUnits.put("1w", DateTimeUnit.WEEK_OF_WEEKYEAR);
-            dateFieldUnits.put("day", DateTimeUnit.DAY_OF_MONTH);
-            dateFieldUnits.put("1d", DateTimeUnit.DAY_OF_MONTH);
-            dateFieldUnits.put("hour", DateTimeUnit.HOUR_OF_DAY);
-            dateFieldUnits.put("1h", DateTimeUnit.HOUR_OF_DAY);
-            dateFieldUnits.put("minute", DateTimeUnit.MINUTES_OF_HOUR);
-            dateFieldUnits.put("1m", DateTimeUnit.MINUTES_OF_HOUR);
-            dateFieldUnits.put("second", DateTimeUnit.SECOND_OF_MINUTE);
-            dateFieldUnits.put("1s", DateTimeUnit.SECOND_OF_MINUTE);
-            DATE_FIELD_UNITS = unmodifiableMap(dateFieldUnits);
-        }
-
-        private DateHistogramInterval dateHistogramInterval;
-
-        public DateHistogramFactory(String name) {
-            super(name, InternalDateHistogram.HISTOGRAM_FACTORY);
-        }
-
-        /**
-         * Set the interval.
-         */
-        public DateHistogramFactory dateHistogramInterval(DateHistogramInterval dateHistogramInterval) {
-            this.dateHistogramInterval = dateHistogramInterval;
-            return this;
-        }
-
-        public DateHistogramFactory offset(String offset) {
-            return offset(parseStringOffset(offset));
-        }
-
-        protected static long parseStringOffset(String offset) {
-            if (offset.charAt(0) == '-') {
-                return -TimeValue.parseTimeValue(offset.substring(1), null, DateHistogramFactory.class.getSimpleName() + ".parseOffset")
-                        .millis();
-            }
-            int beginIndex = offset.charAt(0) == '+' ? 1 : 0;
-            return TimeValue.parseTimeValue(offset.substring(beginIndex), null, DateHistogramFactory.class.getSimpleName() + ".parseOffset")
-                    .millis();
-        }
-
-        public DateHistogramInterval dateHistogramInterval() {
-            return dateHistogramInterval;
-        }
-
-        @Override
-        protected Rounding createRounding() {
-            TimeZoneRounding.Builder tzRoundingBuilder;
-            if (dateHistogramInterval != null) {
-            DateTimeUnit dateTimeUnit = DATE_FIELD_UNITS.get(dateHistogramInterval.toString());
-            if (dateTimeUnit != null) {
-                tzRoundingBuilder = TimeZoneRounding.builder(dateTimeUnit);
-            } else {
-                // the interval is a time value?
-                tzRoundingBuilder = TimeZoneRounding.builder(TimeValue.parseTimeValue(dateHistogramInterval.toString(), null, getClass()
-                        .getSimpleName() + ".interval"));
-            }
-            } else {
-                // the interval is an integer time value in millis?
-                tzRoundingBuilder = TimeZoneRounding.builder(TimeValue.timeValueMillis(interval()));
-            }
-            if (timeZone() != null) {
-                tzRoundingBuilder.timeZone(timeZone());
-            }
-            Rounding rounding = tzRoundingBuilder.offset(offset()).build();
-            return rounding;
-        }
-
-        @Override
-        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-            return super.createUnmapped(aggregationContext, parent, pipelineAggregators, metaData);
-        }
-
-        @Override
-        protected Aggregator doCreateInternal(Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                throws IOException {
-            return super
-                    .doCreateInternal(valuesSource, aggregationContext, parent, collectsFromSingleBucket, pipelineAggregators, metaData);
-        }
-
-        @Override
-        public String getWriteableName() {
-            return InternalDateHistogram.TYPE.name();
-        }
-
-        @Override
-        protected XContentBuilder doXContentInterval(XContentBuilder builder, Params params) throws IOException {
-            if (dateHistogramInterval == null) {
-                super.doXContentInterval(builder, params);
-            } else {
-                builder.value(dateHistogramInterval.toString());
-            }
-            return builder;
-        }
-
-        @Override
-        protected DateHistogramFactory createFactoryFromStream(String name, StreamInput in)
-                throws IOException {
-            DateHistogramFactory factory = new DateHistogramFactory(name);
-            if (in.readBoolean()) {
-                factory.dateHistogramInterval = DateHistogramInterval.readFromStream(in);
-            }
-            return factory;
-        }
-
-        @Override
-        protected void writeFactoryToStream(StreamOutput out) throws IOException {
-            boolean hasDateInterval = dateHistogramInterval != null;
-            out.writeBoolean(hasDateInterval);
-            if (hasDateInterval) {
-                dateHistogramInterval.writeTo(out);
-            }
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(super.innerHashCode(), dateHistogramInterval);
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            DateHistogramFactory other = (DateHistogramFactory) obj;
-            return super.innerEquals(obj)
-                    && Objects.equals(dateHistogramInterval, other.dateHistogramInterval);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBuilder.java
new file mode 100644
index 0000000..064e046
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBuilder.java
@@ -0,0 +1,134 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.histogram;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Histogram} aggregation.
+ */
+public class HistogramBuilder extends ValuesSourceAggregationBuilder<HistogramBuilder> {
+
+    private Long interval;
+    private Histogram.Order order;
+    private Long minDocCount;
+    private Long extendedBoundsMin;
+    private Long extendedBoundsMax;
+    private Long offset;
+
+    /**
+     * Constructs a new histogram aggregation builder.
+     *
+     * @param name  The name of the aggregation (will serve as the unique identifier for the aggregation result in the response)
+     */
+    public HistogramBuilder(String name) {
+        super(name, InternalHistogram.TYPE.name());
+    }
+
+    /**
+     * Sets the interval for the histogram.
+     *
+     * @param interval  The interval for the histogram
+     * @return          This builder
+     */
+    public HistogramBuilder interval(long interval) {
+        this.interval = interval;
+        return this;
+    }
+
+    /**
+     * Sets the order by which the buckets will be returned.
+     *
+     * @param order The order by which the buckets will be returned
+     * @return      This builder
+     */
+    public HistogramBuilder order(Histogram.Order order) {
+        this.order = order;
+        return this;
+    }
+
+    /**
+     * Sets the minimum document count per bucket. Buckets with less documents than this min value will not be returned.
+     *
+     * @param minDocCount   The minimum document count per bucket
+     * @return              This builder
+     */
+    public HistogramBuilder minDocCount(long minDocCount) {
+        this.minDocCount = minDocCount;
+        return this;
+    }
+
+    /**
+     * Set extended bounds for the histogram. In case the lower value in the
+     * histogram would be greater than <code>min</code> or the upper value would
+     * be less than <code>max</code>, empty buckets will be generated.
+     */
+    public HistogramBuilder extendedBounds(Long min, Long max) {
+        extendedBoundsMin = min;
+        extendedBoundsMax = max;
+        return this;
+    }
+
+    /**
+     * Set the offset to apply to shift bucket boundaries.
+     */
+    public HistogramBuilder offset(long offset) {
+        this.offset = offset;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (interval == null) {
+            throw new SearchSourceBuilderException("[interval] must be defined for histogram aggregation [" + getName() + "]");
+        }
+        builder.field("interval", interval);
+
+        if (order != null) {
+            builder.field("order");
+            order.toXContent(builder, params);
+        }
+
+        if (offset != null) {
+            builder.field("offset", offset);
+        }
+
+        if (minDocCount != null) {
+            builder.field("min_doc_count", minDocCount);
+        }
+
+        if (extendedBoundsMin != null || extendedBoundsMax != null) {
+            builder.startObject(HistogramParser.EXTENDED_BOUNDS.getPreferredName());
+            if (extendedBoundsMin != null) {
+                builder.field("min", extendedBoundsMin);
+            }
+            if (extendedBoundsMax != null) {
+                builder.field("max", extendedBoundsMax);
+            }
+            builder.endObject();
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java
index 2dcf706..c738251 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java
@@ -19,31 +19,24 @@
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.rounding.Rounding;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.aggregations.support.format.ValueParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  * Parses the histogram request
  */
-public class HistogramParser extends NumericValuesSourceParser {
+public class HistogramParser implements Aggregator.Parser {
 
-    public HistogramParser() {
-        super(true, true, false);
-    }
-
-    protected HistogramParser(boolean timezoneAware) {
-        super(true, true, timezoneAware);
-    }
+    static final ParseField EXTENDED_BOUNDS = new ParseField("extended_bounds");
 
     @Override
     public String type() {
@@ -51,105 +44,100 @@ public class HistogramParser extends NumericValuesSourceParser {
     }
 
     @Override
-    protected HistogramAggregator.Factory<?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        HistogramAggregator.Factory factory = new HistogramAggregator.Factory(aggregationName);
-        Long interval = (Long) otherOptions.get(Rounding.Interval.INTERVAL_FIELD);
-        if (interval == null) {
-            throw new ParsingException(null, "Missing required field [interval] for histogram aggregation [" + aggregationName + "]");
-        } else {
-            factory.interval(interval);
-        }
-        Long offset = (Long) otherOptions.get(Rounding.OffsetRounding.OFFSET_FIELD);
-        if (offset != null) {
-            factory.offset(offset);
-        }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-        ExtendedBounds extendedBounds = (ExtendedBounds) otherOptions.get(ExtendedBounds.EXTENDED_BOUNDS_FIELD);
-        if (extendedBounds != null) {
-            factory.extendedBounds(extendedBounds);
-        }
-        Boolean keyed = (Boolean) otherOptions.get(HistogramAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        Long minDocCount = (Long) otherOptions.get(HistogramAggregator.MIN_DOC_COUNT_FIELD);
-        if (minDocCount != null) {
-            factory.minDocCount(minDocCount);
-        }
-        InternalOrder order = (InternalOrder) otherOptions.get(HistogramAggregator.ORDER_FIELD);
-        if (order != null) {
-            factory.order(order);
-        }
-        return factory;
-    }
+        ValuesSourceParser vsParser = ValuesSourceParser.numeric(aggregationName, InternalHistogram.TYPE, context)
+                .targetValueType(ValueType.NUMERIC)
+                .formattable(true)
+                .build();
 
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser, ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions)
-            throws IOException {
-        if (token.isValue()) {
-            if (parseFieldMatcher.match(currentFieldName, Rounding.Interval.INTERVAL_FIELD)) {
-                if (token == XContentParser.Token.VALUE_STRING) {
-                    otherOptions.put(Rounding.Interval.INTERVAL_FIELD, parseStringInterval(parser.text()));
-                    return true;
-                } else {
-                    otherOptions.put(Rounding.Interval.INTERVAL_FIELD, parser.longValue());
-                    return true;
-                }
-            } else if (parseFieldMatcher.match(currentFieldName, HistogramAggregator.MIN_DOC_COUNT_FIELD)) {
-                otherOptions.put(HistogramAggregator.MIN_DOC_COUNT_FIELD, parser.longValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, HistogramAggregator.KEYED_FIELD)) {
-                otherOptions.put(HistogramAggregator.KEYED_FIELD, parser.booleanValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, Rounding.OffsetRounding.OFFSET_FIELD)) {
-                if (token == XContentParser.Token.VALUE_STRING) {
-                    otherOptions.put(Rounding.OffsetRounding.OFFSET_FIELD, parseStringOffset(parser.text()));
-                    return true;
+        boolean keyed = false;
+        long minDocCount = 0;
+        InternalOrder order = (InternalOrder) InternalOrder.KEY_ASC;
+        long interval = -1;
+        ExtendedBounds extendedBounds = null;
+        long offset = 0;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token.isValue()) {
+                if ("interval".equals(currentFieldName)) {
+                    interval = parser.longValue();
+                } else if ("min_doc_count".equals(currentFieldName) || "minDocCount".equals(currentFieldName)) {
+                    minDocCount = parser.longValue();
+                } else if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else if ("offset".equals(currentFieldName)) {
+                    offset = parser.longValue();
                 } else {
-                    otherOptions.put(Rounding.OffsetRounding.OFFSET_FIELD, parser.longValue());
-                    return true;
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-            } else {
-                return false;
-            }
-        } else if (token == XContentParser.Token.START_OBJECT) {
-            if (parseFieldMatcher.match(currentFieldName, HistogramAggregator.ORDER_FIELD)) {
-                InternalOrder order = null;
-                while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                    if (token == XContentParser.Token.FIELD_NAME) {
-                        currentFieldName = parser.currentName();
-                    } else if (token == XContentParser.Token.VALUE_STRING) {
-                        String dir = parser.text();
-                        boolean asc = "asc".equals(dir);
-                        if (!asc && !"desc".equals(dir)) {
-                            throw new ParsingException(parser.getTokenLocation(), "Unknown order direction in aggregation ["
-                                    + aggregationName + "]: [" + dir
-                                    + "]. Should be either [asc] or [desc]");
+            } else if (token == XContentParser.Token.START_OBJECT) {
+                if ("order".equals(currentFieldName)) {
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            currentFieldName = parser.currentName();
+                        } else if (token == XContentParser.Token.VALUE_STRING) {
+                            String dir = parser.text();
+                            boolean asc = "asc".equals(dir);
+                            if (!asc && !"desc".equals(dir)) {
+                                throw new SearchParseException(context, "Unknown order direction [" + dir + "] in aggregation ["
+                                        + aggregationName + "]. Should be either [asc] or [desc]", parser.getTokenLocation());
+                            }
+                            order = resolveOrder(currentFieldName, asc);
                         }
-                        order = resolveOrder(currentFieldName, asc);
                     }
+                } else if (context.parseFieldMatcher().match(currentFieldName, EXTENDED_BOUNDS)) {
+                    extendedBounds = new ExtendedBounds();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            currentFieldName = parser.currentName();
+                        } else if (token.isValue()) {
+                            if ("min".equals(currentFieldName)) {
+                                extendedBounds.min = parser.longValue(true);
+                            } else if ("max".equals(currentFieldName)) {
+                                extendedBounds.max = parser.longValue(true);
+                            } else {
+                                throw new SearchParseException(context, "Unknown extended_bounds key for a " + token + " in aggregation ["
+                                        + aggregationName + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                            }
+                        }
+                    }
+
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-                otherOptions.put(HistogramAggregator.ORDER_FIELD, order);
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, ExtendedBounds.EXTENDED_BOUNDS_FIELD)) {
-                ExtendedBounds extendedBounds = ExtendedBounds.parse(parser, parseFieldMatcher, aggregationName);
-                otherOptions.put(ExtendedBounds.EXTENDED_BOUNDS_FIELD, extendedBounds);
-                return true;
             } else {
-                return false;
+                throw new SearchParseException(context, "Unexpected token " + token + " in aggregation [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
-        } else {
-            return false;
         }
-    }
 
-    protected Object parseStringInterval(String interval) {
-        return Long.valueOf(interval);
-    }
+        if (interval < 1) {
+            throw new SearchParseException(context,
+                    "Missing required field [interval] for histogram aggregation [" + aggregationName + "]", parser.getTokenLocation());
+        }
+
+        Rounding rounding = new Rounding.Interval(interval);
+        if (offset != 0) {
+            rounding = new Rounding.OffsetRounding((Rounding.Interval) rounding, offset);
+        }
+
+        if (extendedBounds != null) {
+            // with numeric histogram, we can process here and fail fast if the bounds are invalid
+            extendedBounds.processAndValidate(aggregationName, context, ValueParser.RAW);
+        }
+
+        return new HistogramAggregator.Factory(aggregationName, vsParser.config(), rounding, order, keyed, minDocCount, extendedBounds,
+                new InternalHistogram.Factory());
 
-    protected long parseStringOffset(String offset) throws IOException {
-        return Long.valueOf(offset);
     }
 
     static InternalOrder resolveOrder(String key, boolean asc) {
@@ -161,9 +149,4 @@ public class HistogramParser extends NumericValuesSourceParser {
         }
         return new InternalOrder.Aggregation(key, asc);
     }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return HistogramAggregator.Factory.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java
index 9808eed..1651886 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java
@@ -21,7 +21,6 @@ package org.elasticsearch.search.aggregations.bucket.histogram;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.InternalAggregations;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
@@ -31,7 +30,6 @@ import org.joda.time.DateTimeZone;
  */
 public class InternalDateHistogram {
 
-    public static final Factory HISTOGRAM_FACTORY = new Factory();
     final static Type TYPE = new Type("date_histogram", "dhisto");
 
     static class Bucket extends InternalHistogram.Bucket {
@@ -67,13 +65,8 @@ public class InternalDateHistogram {
         }
 
         @Override
-        public Type type() {
-            return TYPE;
-        }
-
-        @Override
-        public ValueType valueType() {
-            return ValueType.DATE;
+        public String type() {
+            return TYPE.name();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java
index c24ab0d..faca359 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java
@@ -34,7 +34,6 @@ import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;
 import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -52,7 +51,6 @@ import java.util.Map;
 public class InternalHistogram<B extends InternalHistogram.Bucket> extends InternalMultiBucketAggregation<InternalHistogram, B> implements
         Histogram {
 
-    public static final Factory<Bucket> HISTOGRAM_FACTORY = new Factory<Bucket>();
     final static Type TYPE = new Type("histogram", "histo");
 
     private final static AggregationStreams.Stream STREAM = new AggregationStreams.Stream() {
@@ -237,12 +235,8 @@ public class InternalHistogram<B extends InternalHistogram.Bucket> extends Inter
         protected Factory() {
         }
 
-        public Type type() {
-            return TYPE;
-        }
-
-        public ValueType valueType() {
-            return ValueType.NUMERIC;
+        public String type() {
+            return TYPE.name();
         }
 
         public InternalHistogram<B> create(String name, List<B> buckets, InternalOrder order, long minDocCount,
@@ -511,7 +505,7 @@ public class InternalHistogram<B extends InternalHistogram.Bucket> extends Inter
     }
 
     @SuppressWarnings("unchecked")
-    protected static <B extends InternalHistogram.Bucket> Factory<B> resolveFactory(String factoryType) {
+    private static <B extends InternalHistogram.Bucket> Factory<B> resolveFactory(String factoryType) {
         if (factoryType.equals(InternalDateHistogram.TYPE.name())) {
             return (Factory<B>) new InternalDateHistogram.Factory();
         } else if (factoryType.equals(TYPE.name())) {
@@ -523,7 +517,7 @@ public class InternalHistogram<B extends InternalHistogram.Bucket> extends Inter
 
     @Override
     protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(factory.type().name());
+        out.writeString(factory.type());
         InternalOrder.Streams.writeOrder(order, out);
         out.writeVLong(minDocCount);
         if (minDocCount == 0) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java
index d19a839..9d503a8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java
@@ -25,7 +25,6 @@ import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;
 
 import java.io.IOException;
 import java.util.Comparator;
-import java.util.Objects;
 
 /**
  * An internal {@link Histogram.Order} strategy which is identified by a unique id.
@@ -65,25 +64,6 @@ class InternalOrder extends Histogram.Order {
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         return builder.startObject().field(key, asc ? "asc" : "desc").endObject();
     }
-    
-    @Override
-    public int hashCode() {
-        return Objects.hash(id, key, asc);
-    }
-    
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        InternalOrder other = (InternalOrder) obj;
-        return Objects.equals(id, other.id)
-                && Objects.equals(key, other.key)
-                && Objects.equals(asc, other.asc);
-    }
 
     static class Aggregation extends InternalOrder {
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java
index 99868dd..1ae7341 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java
@@ -20,9 +20,6 @@ package org.elasticsearch.search.aggregations.bucket.missing;
 
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -31,10 +28,9 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 import java.io.IOException;
 import java.util.List;
@@ -85,10 +81,10 @@ public class MissingAggregator extends SingleBucketAggregator {
         return new InternalMissing(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource>  {
 
-        public Factory(String name, ValueType targetValueType) {
-            super(name, InternalMissing.TYPE, ValuesSourceType.ANY, targetValueType);
+        public Factory(String name, ValuesSourceConfig valueSourceConfig) {
+            super(name, InternalMissing.TYPE.name(), valueSourceConfig);
         }
 
         @Override
@@ -102,32 +98,6 @@ public class MissingAggregator extends SingleBucketAggregator {
                 boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
             return new MissingAggregator(name, factories, valuesSource, aggregationContext, parent, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new Factory(name, targetValueType);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingBuilder.java
new file mode 100644
index 0000000..9f51fd0
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingBuilder.java
@@ -0,0 +1,57 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.missing;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Missing} aggregation.
+ */
+public class MissingBuilder extends AggregationBuilder<MissingBuilder> {
+
+    private String field;
+
+    /**
+     * Sole constructor.
+     */
+    public MissingBuilder(String name) {
+        super(name, InternalMissing.TYPE.name());
+    }
+
+    /**
+     * Set the field to count missing values on.
+     */
+    public MissingBuilder field(String field) {
+        this.field = field;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (field != null) {
+            builder.field("field", field);
+        }
+        return builder.endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java
index c242a35..6ecdc12 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java
@@ -18,22 +18,19 @@
  */
 package org.elasticsearch.search.aggregations.bucket.missing;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
-public class MissingParser extends AnyValuesSourceParser {
-
-    public MissingParser() {
-        super(true, true);
-    }
+/**
+ *
+ */
+public class MissingParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -41,19 +38,25 @@ public class MissingParser extends AnyValuesSourceParser {
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected MissingAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new MissingAggregator.Factory(aggregationName, targetValueType);
-    }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new MissingAggregator.Factory(null, null);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, InternalMissing.TYPE, context)
+                .scriptable(false)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+        }
+
+        return new MissingAggregator.Factory(aggregationName, vsParser.config());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java
index eca5be4..fa23cf8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java
@@ -28,11 +28,7 @@ import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.search.join.BitSetProducer;
 import org.apache.lucene.util.BitSet;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
@@ -49,15 +45,12 @@ import org.elasticsearch.search.aggregations.support.AggregationContext;
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class NestedAggregator extends SingleBucketAggregator {
 
-    static final ParseField PATH_FIELD = new ParseField("path");
-
     private BitSetProducer parentFilter;
     private final Query childFilter;
 
@@ -127,7 +120,7 @@ public class NestedAggregator extends SingleBucketAggregator {
             }
         };
     }
-
+        
     @Override
     public InternalAggregation buildAggregation(long owningBucketOrdinal) throws IOException {
         return new InternalNested(name, bucketDocCount(owningBucketOrdinal), bucketAggregations(owningBucketOrdinal), pipelineAggregators(),
@@ -150,29 +143,15 @@ public class NestedAggregator extends SingleBucketAggregator {
         return null;
     }
 
-    public static class Factory extends AggregatorFactory<Factory> {
+    public static class Factory extends AggregatorFactory {
 
         private final String path;
 
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param path
-         *            the path to use for this nested aggregation. The path must
-         *            match the path to a nested object in the mappings.
-         */
         public Factory(String name, String path) {
-            super(name, InternalNested.TYPE);
+            super(name, InternalNested.TYPE.name());
             this.path = path;
         }
 
-        /**
-         * Get the path to use for this nested aggregation.
-         */
-        public String path() {
-            return path;
-        }
-
         @Override
         public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
                 List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
@@ -189,37 +168,6 @@ public class NestedAggregator extends SingleBucketAggregator {
             return new NestedAggregator(name, factories, objectMapper, context, parent, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            builder.field(PATH_FIELD.getPreferredName(), path);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected AggregatorFactory doReadFrom(String name, StreamInput in) throws IOException {
-            String path = in.readString();
-            Factory factory = new Factory(name, path);
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeString(path);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(path);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(path, other.path);
-        }
-
         private final static class Unmapped extends NonCollectingAggregator {
 
             public Unmapped(String name, AggregationContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedBuilder.java
new file mode 100644
index 0000000..b375f09
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedBuilder.java
@@ -0,0 +1,61 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.nested;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Nested} aggregation.
+ */
+public class NestedBuilder extends AggregationBuilder<NestedBuilder> {
+
+    private String path;
+
+    /**
+     * Sole constructor.
+     */
+    public NestedBuilder(String name) {
+        super(name, InternalNested.TYPE.name());
+    }
+
+    /**
+     * Set the path to use for this nested aggregation. The path must match
+     * the path to a nested object in the mappings. This parameter is
+     * compulsory.
+     */
+    public NestedBuilder path(String path) {
+        this.path = path;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (path == null) {
+            throw new SearchSourceBuilderException("nested path must be set on nested aggregation [" + getName() + "]");
+        }
+        builder.field("path", path);
+        return builder.endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java
index 00e7973..ddf6bf1 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java
@@ -18,11 +18,11 @@
  */
 package org.elasticsearch.search.aggregations.bucket.nested;
 
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -37,7 +37,7 @@ public class NestedParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorFactory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         String path = null;
 
         XContentParser.Token token;
@@ -46,27 +46,24 @@ public class NestedParser implements Aggregator.Parser {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.VALUE_STRING) {
-                if (context.parseFieldMatcher().match(currentFieldName, NestedAggregator.PATH_FIELD)) {
+                if ("path".equals(currentFieldName)) {
                     path = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + aggregationName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (path == null) {
             // "field" doesn't exist, so we fall back to the context of the ancestors
-            throw new ParsingException(parser.getTokenLocation(), "Missing [path] field for nested aggregation [" + aggregationName + "]");
+            throw new SearchParseException(context, "Missing [path] field for nested aggregation [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
         return new NestedAggregator.Factory(aggregationName, path);
     }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new NestedAggregator.Factory(null, null);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java
index a567a62..1b9363c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java
@@ -24,11 +24,7 @@ import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.join.BitSetProducer;
 import org.apache.lucene.util.BitSet;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
@@ -46,15 +42,12 @@ import org.elasticsearch.search.aggregations.support.AggregationContext;
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class ReverseNestedAggregator extends SingleBucketAggregator {
 
-    static final ParseField PATH_FIELD = new ParseField("path");
-
     private final Query parentFilter;
     private final BitSetProducer parentBitsetProducer;
 
@@ -125,29 +118,13 @@ public class ReverseNestedAggregator extends SingleBucketAggregator {
         return parentFilter;
     }
 
-    public static class Factory extends AggregatorFactory<Factory> {
-
-        private String path;
+    public static class Factory extends AggregatorFactory {
 
-        public Factory(String name) {
-            super(name, InternalReverseNested.TYPE);
-        }
+        private final String path;
 
-        /**
-         * Set the path to use for this nested aggregation. The path must match
-         * the path to a nested object in the mappings. If it is not specified
-         * then this aggregation will go back to the root document.
-         */
-        public Factory path(String path) {
+        public Factory(String name, String path) {
+            super(name, InternalReverseNested.TYPE.name());
             this.path = path;
-            return this;
-        }
-
-        /**
-         * Get the path to use for this nested aggregation.
-         */
-        public String path() {
-            return path;
         }
 
         @Override
@@ -175,39 +152,6 @@ public class ReverseNestedAggregator extends SingleBucketAggregator {
             return new ReverseNestedAggregator(name, factories, objectMapper, context, parent, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            if (path != null) {
-                builder.field(PATH_FIELD.getPreferredName(), path);
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected AggregatorFactory doReadFrom(String name, StreamInput in) throws IOException {
-            Factory factory = new Factory(name);
-            factory.path = in.readOptionalString();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(path);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(path);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(path, other.path);
-        }
-
         private final static class Unmapped extends NonCollectingAggregator {
 
             public Unmapped(String name, AggregationContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedBuilder.java
new file mode 100644
index 0000000..591655e
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedBuilder.java
@@ -0,0 +1,59 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.nested;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link ReverseNested} aggregation.
+ */
+public class ReverseNestedBuilder extends AggregationBuilder<ReverseNestedBuilder> {
+
+    private String path;
+
+    /**
+     * Sole constructor.
+     */
+    public ReverseNestedBuilder(String name) {
+        super(name, InternalReverseNested.TYPE.name());
+    }
+
+    /**
+     * Set the path to use for this nested aggregation. The path must match
+     * the path to a nested object in the mappings. If it is not specified
+     * then this aggregation will go back to the root document.
+     */
+    public ReverseNestedBuilder path(String path) {
+        this.path = path;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (path != null) {
+            builder.field("path", path);
+        }
+        return builder.endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedParser.java
index b01b639..80ab9f5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedParser.java
@@ -18,11 +18,11 @@
  */
 package org.elasticsearch.search.aggregations.bucket.nested;
 
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -37,7 +37,7 @@ public class ReverseNestedParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorFactory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         String path = null;
 
         XContentParser.Token token;
@@ -49,23 +49,15 @@ public class ReverseNestedParser implements Aggregator.Parser {
                 if ("path".equals(currentFieldName)) {
                     path = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + aggregationName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
 
-        ReverseNestedAggregator.Factory factory = new ReverseNestedAggregator.Factory(aggregationName);
-        if (path != null) {
-            factory.path(path);
-        }
-        return factory;
-    }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new ReverseNestedAggregator.Factory(null);
+        return new ReverseNestedAggregator.Factory(aggregationName, path);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeBuilder.java
new file mode 100644
index 0000000..c4f0c76
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeBuilder.java
@@ -0,0 +1,81 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.range;
+
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ *
+ */
+public abstract class AbstractRangeBuilder<B extends AbstractRangeBuilder<B>> extends ValuesSourceAggregationBuilder<B> {
+
+    protected static class Range implements ToXContent {
+
+        private String key;
+        private Object from;
+        private Object to;
+
+        public Range(String key, Object from, Object to) {
+            this.key = key;
+            this.from = from;
+            this.to = to;
+        }
+
+        @Override
+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+            builder.startObject();
+            if (key != null) {
+                builder.field("key", key);
+            }
+            if (from != null) {
+                builder.field("from", from);
+            }
+            if (to != null) {
+                builder.field("to", to);
+            }
+            return builder.endObject();
+        }
+    }
+
+    protected List<Range> ranges = new ArrayList<>();
+
+    protected AbstractRangeBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (ranges.isEmpty()) {
+            throw new SearchSourceBuilderException("at least one range must be defined for range aggregation [" + getName() + "]");
+        }
+        builder.startArray("ranges");
+        for (Range range : ranges) {
+            range.toXContent(builder, params);
+        }
+        return builder.endArray();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java
index d96e860..5303d7f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java
@@ -29,8 +29,6 @@ import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;
 import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -223,16 +221,8 @@ public class InternalRange<B extends InternalRange.Bucket, R extends InternalRan
 
     public static class Factory<B extends Bucket, R extends InternalRange<B, R>> {
 
-        public Type type() {
-            return TYPE;
-        }
-
-        public ValuesSourceType getValueSourceType() {
-            return ValuesSourceType.NUMERIC;
-        }
-
-        public ValueType getValueType() {
-            return ValueType.NUMERIC;
+        public String type() {
+            return TYPE.name();
         }
 
         public R create(String name, List<B> ranges, ValueFormatter formatter, boolean keyed, List<PipelineAggregator> pipelineAggregators,
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java
index 3550c9d..125fca4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java
@@ -20,14 +20,6 @@ package org.elasticsearch.search.aggregations.bucket.range;
 
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.InPlaceMergeSorter;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
@@ -39,10 +31,9 @@ import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueParser;
@@ -52,38 +43,21 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class RangeAggregator extends BucketsAggregator {
 
-    public static final ParseField RANGES_FIELD = new ParseField("ranges");
-    public static final ParseField KEYED_FIELD = new ParseField("keyed");
+    public static class Range {
 
-    public static class Range implements Writeable<Range>, ToXContent {
+        public String key;
+        public double from = Double.NEGATIVE_INFINITY;
+        String fromAsStr;
+        public double to = Double.POSITIVE_INFINITY;
+        String toAsStr;
 
-        public static final Range PROTOTYPE = new Range(null, -1, null, -1, null);
-        public static final ParseField KEY_FIELD = new ParseField("key");
-        public static final ParseField FROM_FIELD = new ParseField("from");
-        public static final ParseField TO_FIELD = new ParseField("to");
-
-        protected String key;
-        protected double from = Double.NEGATIVE_INFINITY;
-        protected String fromAsStr;
-        protected double to = Double.POSITIVE_INFINITY;
-        protected String toAsStr;
-
-        public Range(String key, Double from, Double to) {
-            this(key, from == null ? Double.NEGATIVE_INFINITY : from, null, to == null ? Double.POSITIVE_INFINITY : to, null);
-        }
-
-        public Range(String key, String from, String to) {
-            this(key, Double.NEGATIVE_INFINITY, from, Double.POSITIVE_INFINITY, to);
-        }
-
-        protected Range(String key, double from, String fromAsStr, double to, String toAsStr) {
+        public Range(String key, double from, String fromAsStr, double to, String toAsStr) {
             this.key = key;
             this.from = from;
             this.fromAsStr = fromAsStr;
@@ -109,99 +83,6 @@ public class RangeAggregator extends BucketsAggregator {
                 to = parser.parseDouble(toAsStr, context);
             }
         }
-
-        @Override
-        public Range readFrom(StreamInput in) throws IOException {
-            String key = in.readOptionalString();
-            String fromAsStr = in.readOptionalString();
-            String toAsStr = in.readOptionalString();
-            double from = in.readDouble();
-            double to = in.readDouble();
-            return new Range(key, from, fromAsStr, to, toAsStr);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(key);
-            out.writeOptionalString(fromAsStr);
-            out.writeOptionalString(toAsStr);
-            out.writeDouble(from);
-            out.writeDouble(to);
-        }
-
-        public Range fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-
-            XContentParser.Token token;
-            String currentFieldName = null;
-            double from = Double.NEGATIVE_INFINITY;
-            String fromAsStr = null;
-            double to = Double.POSITIVE_INFINITY;
-            String toAsStr = null;
-            String key = null;
-            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                if (token == XContentParser.Token.FIELD_NAME) {
-                    currentFieldName = parser.currentName();
-                } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                    if (parseFieldMatcher.match(currentFieldName, FROM_FIELD)) {
-                        from = parser.doubleValue();
-                    } else if (parseFieldMatcher.match(currentFieldName, TO_FIELD)) {
-                        to = parser.doubleValue();
-                    }
-                } else if (token == XContentParser.Token.VALUE_STRING) {
-                    if (parseFieldMatcher.match(currentFieldName, FROM_FIELD)) {
-                        fromAsStr = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, TO_FIELD)) {
-                        toAsStr = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, KEY_FIELD)) {
-                        key = parser.text();
-                    }
-                }
-            }
-            return new Range(key, from, fromAsStr, to, toAsStr);
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            if (key != null) {
-                builder.field(KEY_FIELD.getPreferredName(), key);
-            }
-            if (Double.isFinite(from)) {
-                builder.field(FROM_FIELD.getPreferredName(), from);
-            }
-            if (Double.isFinite(to)) {
-                builder.field(TO_FIELD.getPreferredName(), to);
-            }
-            if (fromAsStr != null) {
-                builder.field(FROM_FIELD.getPreferredName(), fromAsStr);
-            }
-            if (toAsStr != null) {
-                builder.field(TO_FIELD.getPreferredName(), toAsStr);
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(key, from, fromAsStr, to, toAsStr);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            Range other = (Range) obj;
-            return Objects.equals(key, other.key)
-                    && Objects.equals(from, other.from)
-                    && Objects.equals(fromAsStr, other.fromAsStr)
-                    && Objects.equals(to, other.to)
-                    && Objects.equals(toAsStr, other.toAsStr);
-        }
     }
 
     final ValuesSource.Numeric valuesSource;
@@ -213,7 +94,7 @@ public class RangeAggregator extends BucketsAggregator {
     final double[] maxTo;
 
     public RangeAggregator(String name, AggregatorFactories factories, ValuesSource.Numeric valuesSource, ValueFormat format,
-            InternalRange.Factory rangeFactory, List<? extends Range> ranges, boolean keyed, AggregationContext aggregationContext,
+            InternalRange.Factory rangeFactory, List<Range> ranges, boolean keyed, AggregationContext aggregationContext,
             Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
 
         super(name, factories, aggregationContext, parent, pipelineAggregators, metaData);
@@ -364,13 +245,12 @@ public class RangeAggregator extends BucketsAggregator {
 
     public static class Unmapped extends NonCollectingAggregator {
 
-        private final List<? extends RangeAggregator.Range> ranges;
+        private final List<RangeAggregator.Range> ranges;
         private final boolean keyed;
         private final InternalRange.Factory factory;
         private final ValueFormatter formatter;
 
-        public Unmapped(String name, List<? extends RangeAggregator.Range> ranges, boolean keyed, ValueFormat format,
-                AggregationContext context,
+        public Unmapped(String name, List<RangeAggregator.Range> ranges, boolean keyed, ValueFormat format, AggregationContext context,
                 Aggregator parent, InternalRange.Factory factory, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
                 throws IOException {
 
@@ -396,34 +276,17 @@ public class RangeAggregator extends BucketsAggregator {
         }
     }
 
-    public static abstract class AbstractFactory<AF extends AbstractFactory<AF, R>, R extends Range>
-            extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, AF> {
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric> {
 
         private final InternalRange.Factory rangeFactory;
-        private List<R> ranges = new ArrayList<>();
-        private boolean keyed = false;
+        private final List<Range> ranges;
+        private final boolean keyed;
 
-        protected AbstractFactory(String name, InternalRange.Factory rangeFactory) {
-            super(name, rangeFactory.type(), rangeFactory.getValueSourceType(), rangeFactory.getValueType());
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valueSourceConfig, InternalRange.Factory rangeFactory, List<Range> ranges, boolean keyed) {
+            super(name, rangeFactory.type(), valueSourceConfig);
             this.rangeFactory = rangeFactory;
-        }
-
-        public AF addRange(R range) {
-            ranges.add(range);
-            return (AF) this;
-        }
-
-        public List<R> ranges() {
-            return ranges;
-        }
-
-        public AF keyed(boolean keyed) {
+            this.ranges = ranges;
             this.keyed = keyed;
-            return (AF) this;
-        }
-
-        public boolean keyed() {
-            return keyed;
         }
 
         @Override
@@ -437,127 +300,6 @@ public class RangeAggregator extends BucketsAggregator {
                 boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
             return new RangeAggregator(name, factories, valuesSource, config.format(), rangeFactory, ranges, keyed, aggregationContext, parent, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(RANGES_FIELD.getPreferredName(), ranges);
-            builder.field(KEYED_FIELD.getPreferredName(), keyed);
-            return builder;
-        }
-
-        @Override
-        protected AF innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            AbstractFactory<AF, R> factory = createFactoryFromStream(name, in);
-            factory.keyed = in.readBoolean();
-            return (AF) factory;
-        }
-
-        protected abstract AbstractFactory<AF, R> createFactoryFromStream(String name, StreamInput in) throws IOException;
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(ranges.size());
-            for (Range range : ranges) {
-                range.writeTo(out);
-            }
-            out.writeBoolean(keyed);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(ranges, keyed);
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            AbstractFactory<AF, R> other = (AbstractFactory<AF, R>) obj;
-            return Objects.equals(ranges, other.ranges)
-                    && Objects.equals(keyed, other.keyed);
-        }
-    }
-
-    public static class Factory extends AbstractFactory<Factory, Range> {
-
-        public Factory(String name) {
-            super(name, InternalRange.FACTORY);
-        }
-
-        /**
-         * Add a new range to this aggregation.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param from
-         *            the lower bound on the distances, inclusive
-         * @param to
-         *            the upper bound on the distances, exclusive
-         */
-        public Factory addRange(String key, double from, double to) {
-            addRange(new Range(key, from, to));
-            return this;
-        }
-
-        /**
-         * Same as {@link #addRange(String, double, double)} but the key will be
-         * automatically generated based on <code>from</code> and
-         * <code>to</code>.
-         */
-        public Factory addRange(double from, double to) {
-            return addRange(null, from, to);
-        }
-
-        /**
-         * Add a new range with no lower bound.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param to
-         *            the upper bound on the distances, exclusive
-         */
-        public Factory addUnboundedTo(String key, double to) {
-            addRange(new Range(key, null, to));
-            return this;
-        }
-
-        /**
-         * Same as {@link #addUnboundedTo(String, double)} but the key will be
-         * computed automatically.
-         */
-        public Factory addUnboundedTo(double to) {
-            return addUnboundedTo(null, to);
-        }
-
-        /**
-         * Add a new range with no upper bound.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param from
-         *            the lower bound on the distances, inclusive
-         */
-        public Factory addUnboundedFrom(String key, double from) {
-            addRange(new Range(key, from, null));
-            return this;
-        }
-
-        /**
-         * Same as {@link #addUnboundedFrom(String, double)} but the key will be
-         * computed automatically.
-         */
-        public Factory addUnboundedFrom(double from) {
-            return addUnboundedFrom(null, from);
-        }
-
-        @Override
-        protected Factory createFactoryFromStream(String name, StreamInput in) throws IOException {
-            int size = in.readVInt();
-            Factory factory = new Factory(name);
-            for (int i = 0; i < size; i++) {
-                factory.addRange(Range.PROTOTYPE.readFrom(in));
-            }
-            return factory;
-        }
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeBuilder.java
new file mode 100644
index 0000000..c772397
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeBuilder.java
@@ -0,0 +1,115 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.bucket.range;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link org.elasticsearch.search.aggregations.bucket.range.AbstractRangeBuilder.Range} aggregation.
+ */
+public class RangeBuilder extends AbstractRangeBuilder<RangeBuilder> {
+
+    private String format;
+
+    /**
+     * Sole constructor.
+     */
+    public RangeBuilder(String name) {
+        super(name, InternalRange.TYPE.name());
+    }
+
+    /**
+     * Add a new range to this aggregation.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     * @param to   the upper bound on the distances, exclusive
+     */
+    public RangeBuilder addRange(String key, double from, double to) {
+        ranges.add(new Range(key, from, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addRange(String, double, double)} but the key will be
+     * automatically generated based on <code>from</code> and <code>to</code>.
+     */
+    public RangeBuilder addRange(double from, double to) {
+        return addRange(null, from, to);
+    }
+
+    /**
+     * Add a new range with no lower bound.
+     *
+     * @param key the key to use for this range in the response
+     * @param to  the upper bound on the distances, exclusive
+     */
+    public RangeBuilder addUnboundedTo(String key, double to) {
+        ranges.add(new Range(key, null, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedTo(String, double)} but the key will be
+     * computed automatically.
+     */
+    public RangeBuilder addUnboundedTo(double to) {
+        return addUnboundedTo(null, to);
+    }
+
+    /**
+     * Add a new range with no upper bound.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     */
+    public RangeBuilder addUnboundedFrom(String key, double from) {
+        ranges.add(new Range(key, from, null));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedFrom(String, double)} but the key will be
+     * computed automatically.
+     */
+    public RangeBuilder addUnboundedFrom(double from) {
+        return addUnboundedFrom(null, from);
+    }
+
+    /**
+     * Set the format to use to display values.
+     */
+    public RangeBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.doInternalXContent(builder, params);
+        if (format != null) {
+            builder.field("format", format);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java
index 710f420..e30b84b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java
@@ -18,33 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.bucket.range;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Map;
 
 /**
  *
  */
-public class RangeParser extends NumericValuesSourceParser {
-
-    public RangeParser() {
-        this(true, true, false);
-    }
-
-    protected RangeParser(boolean scriptable, boolean formattable, boolean timezoneAware) {
-        super(scriptable, formattable, timezoneAware);
-    }
+public class RangeParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -52,49 +41,75 @@ public class RangeParser extends NumericValuesSourceParser {
     }
 
     @Override
-    protected RangeAggregator.AbstractFactory<?, ?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        RangeAggregator.Factory factory = new RangeAggregator.Factory(aggregationName);
-        List<? extends Range> ranges = (List<? extends Range>) otherOptions.get(RangeAggregator.RANGES_FIELD);
-        for (Range range : ranges) {
-            factory.addRange(range);
-        }
-        Boolean keyed = (Boolean) otherOptions.get(RangeAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        return factory;
-    }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.START_ARRAY) {
-            if (parseFieldMatcher.match(currentFieldName, RangeAggregator.RANGES_FIELD)) {
-                List<Range> ranges = new ArrayList<>();
-                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                    Range range = parseRange(parser, parseFieldMatcher);
-                    ranges.add(range);
+        List<RangeAggregator.Range> ranges = null;
+        boolean keyed = false;
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalRange.TYPE, context)
+                .formattable(true)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if ("ranges".equals(currentFieldName)) {
+                    ranges = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        double from = Double.NEGATIVE_INFINITY;
+                        String fromAsStr = null;
+                        double to = Double.POSITIVE_INFINITY;
+                        String toAsStr = null;
+                        String key = null;
+                        String toOrFromOrKey = null;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                toOrFromOrKey = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    from = parser.doubleValue();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    to = parser.doubleValue();
+                                }
+                            } else if (token == XContentParser.Token.VALUE_STRING) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    fromAsStr = parser.text();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    toAsStr = parser.text();
+                                } else if ("key".equals(toOrFromOrKey)) {
+                                    key = parser.text();
+                                }
+                            }
+                        }
+                        ranges.add(new RangeAggregator.Range(key, from, fromAsStr, to, toAsStr));
+                    }
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-                otherOptions.put(RangeAggregator.RANGES_FIELD, ranges);
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, RangeAggregator.KEYED_FIELD)) {
-                boolean keyed = parser.booleanValue();
-                otherOptions.put(RangeAggregator.KEYED_FIELD, keyed);
-                return true;
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
-        return false;
-    }
 
-    protected Range parseRange(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-        return Range.PROTOTYPE.fromXContent(parser, parseFieldMatcher);
-    }
+        if (ranges == null) {
+            throw new SearchParseException(context, "Missing [ranges] in ranges aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
+        }
 
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new RangeAggregator.Factory(null);
+        return new RangeAggregator.Factory(aggregationName, vsParser.config(), InternalRange.FACTORY, ranges, keyed);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorFactory.java
deleted file mode 100644
index 87fef87..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorFactory.java
+++ /dev/null
@@ -1,254 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range.date;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.AbstractFactory;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.joda.time.DateTime;
-
-import java.io.IOException;
-
-public class DateRangeAggregatorFactory extends AbstractFactory<DateRangeAggregatorFactory, RangeAggregator.Range> {
-
-    public DateRangeAggregatorFactory(String name) {
-        super(name, InternalDateRange.FACTORY);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return InternalDateRange.TYPE.name();
-    }
-
-    /**
-     * Add a new range to this aggregation.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the dates, inclusive
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorFactory addRange(String key, String from, String to) {
-        addRange(new Range(key, from, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addRange(String, String, String)} but the key will be
-     * automatically generated based on <code>from</code> and <code>to</code>.
-     */
-    public DateRangeAggregatorFactory addRange(String from, String to) {
-        return addRange(null, from, to);
-    }
-
-    /**
-     * Add a new range with no lower bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorFactory addUnboundedTo(String key, String to) {
-        addRange(new Range(key, null, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedTo(String, String)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorFactory addUnboundedTo(String to) {
-        return addUnboundedTo(null, to);
-    }
-
-    /**
-     * Add a new range with no upper bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the distances, inclusive
-     */
-    public DateRangeAggregatorFactory addUnboundedFrom(String key, String from) {
-        addRange(new Range(key, from, null));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedFrom(String, String)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorFactory addUnboundedFrom(String from) {
-        return addUnboundedFrom(null, from);
-    }
-
-    /**
-     * Add a new range to this aggregation.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the dates, inclusive
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorFactory addRange(String key, double from, double to) {
-        addRange(new Range(key, from, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addRange(String, double, double)} but the key will be
-     * automatically generated based on <code>from</code> and <code>to</code>.
-     */
-    public DateRangeAggregatorFactory addRange(double from, double to) {
-        return addRange(null, from, to);
-    }
-
-    /**
-     * Add a new range with no lower bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorFactory addUnboundedTo(String key, double to) {
-        addRange(new Range(key, null, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedTo(String, double)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorFactory addUnboundedTo(double to) {
-        return addUnboundedTo(null, to);
-    }
-
-    /**
-     * Add a new range with no upper bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the distances, inclusive
-     */
-    public DateRangeAggregatorFactory addUnboundedFrom(String key, double from) {
-        addRange(new Range(key, from, null));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedFrom(String, double)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorFactory addUnboundedFrom(double from) {
-        return addUnboundedFrom(null, from);
-    }
-
-    /**
-     * Add a new range to this aggregation.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the dates, inclusive
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorFactory addRange(String key, DateTime from, DateTime to) {
-        addRange(new Range(key, convertDateTime(from), convertDateTime(to)));
-        return this;
-    }
-
-    private Double convertDateTime(DateTime dateTime) {
-        if (dateTime == null) {
-            return null;
-        } else {
-            return (double) dateTime.getMillis();
-        }
-    }
-
-    /**
-     * Same as {@link #addRange(String, DateTime, DateTime)} but the key will be
-     * automatically generated based on <code>from</code> and <code>to</code>.
-     */
-    public DateRangeAggregatorFactory addRange(DateTime from, DateTime to) {
-        return addRange(null, from, to);
-    }
-
-    /**
-     * Add a new range with no lower bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorFactory addUnboundedTo(String key, DateTime to) {
-        addRange(new Range(key, null, convertDateTime(to)));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedTo(String, DateTime)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorFactory addUnboundedTo(DateTime to) {
-        return addUnboundedTo(null, to);
-    }
-
-    /**
-     * Add a new range with no upper bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the distances, inclusive
-     */
-    public DateRangeAggregatorFactory addUnboundedFrom(String key, DateTime from) {
-        addRange(new Range(key, convertDateTime(from), null));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedFrom(String, DateTime)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorFactory addUnboundedFrom(DateTime from) {
-        return addUnboundedFrom(null, from);
-    }
-
-    @Override
-    protected DateRangeAggregatorFactory createFactoryFromStream(String name, StreamInput in) throws IOException {
-        int size = in.readVInt();
-        DateRangeAggregatorFactory factory = new DateRangeAggregatorFactory(name);
-        for (int i = 0; i < size; i++) {
-            factory.addRange(Range.PROTOTYPE.readFrom(in));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeBuilder.java
new file mode 100644
index 0000000..4bd5758
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeBuilder.java
@@ -0,0 +1,114 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.bucket.range.date;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.bucket.range.AbstractRangeBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@code DateRange} aggregation.
+ */
+public class DateRangeBuilder extends AbstractRangeBuilder<DateRangeBuilder> {
+
+    private String format;
+
+    /**
+     * Sole constructor.
+     */
+    public DateRangeBuilder(String name) {
+        super(name, InternalDateRange.TYPE.name());
+    }
+
+    /**
+     * Add a new range to this aggregation.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     * @param to   the upper bound on the distances, exclusive
+     */
+    public DateRangeBuilder addRange(String key, Object from, Object to) {
+        ranges.add(new Range(key, from, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addRange(String, Object, Object)} but the key will be
+     * automatically generated based on <code>from</code> and <code>to</code>.
+     */
+    public DateRangeBuilder addRange(Object from, Object to) {
+        return addRange(null, from, to);
+    }
+
+    /**
+     * Add a new range with no lower bound.
+     *
+     * @param key the key to use for this range in the response
+     * @param to  the upper bound on the distances, exclusive
+     */
+    public DateRangeBuilder addUnboundedTo(String key, Object to) {
+        ranges.add(new Range(key, null, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedTo(String, Object)} but the key will be
+     * computed automatically.
+     */
+    public DateRangeBuilder addUnboundedTo(Object to) {
+        return addUnboundedTo(null, to);
+    }
+
+    /**
+     * Add a new range with no upper bound.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     */
+    public DateRangeBuilder addUnboundedFrom(String key, Object from) {
+        ranges.add(new Range(key, from, null));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedFrom(String, Object)} but the key will be
+     * computed automatically.
+     */
+    public DateRangeBuilder addUnboundedFrom(Object from) {
+        return addUnboundedFrom(null, from);
+    }
+
+    /**
+     * Set the format to use to display values.
+     */
+    public DateRangeBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.doInternalXContent(builder, params);
+        if (format != null) {
+            builder.field("format", format);
+        }
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java
index 0e3e4d5..940e20a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java
@@ -18,25 +18,24 @@
  */
 package org.elasticsearch.search.aggregations.bucket.range.date;
 
-import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.bucket.range.RangeParser;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
+import java.io.IOException;
+import java.util.ArrayList;
 import java.util.List;
-import java.util.Map;
 
 /**
  *
  */
-public class DateRangeParser extends RangeParser {
-
-    public DateRangeParser() {
-        super(true, true, true);
-    }
+public class DateRangeParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -44,22 +43,78 @@ public class DateRangeParser extends RangeParser {
     }
 
     @Override
-    protected DateRangeAggregatorFactory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        DateRangeAggregatorFactory factory = new DateRangeAggregatorFactory(aggregationName);
-        List<Range> ranges = (List<Range>) otherOptions.get(RangeAggregator.RANGES_FIELD);
-        for (Range range : ranges) {
-            factory.addRange(range);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalDateRange.TYPE, context)
+                .targetValueType(ValueType.DATE)
+                .formattable(true)
+                .build();
+
+        List<RangeAggregator.Range> ranges = null;
+        boolean keyed = false;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if ("ranges".equals(currentFieldName)) {
+                    ranges = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        double from = Double.NEGATIVE_INFINITY;
+                        String fromAsStr = null;
+                        double to = Double.POSITIVE_INFINITY;
+                        String toAsStr = null;
+                        String key = null;
+                        String toOrFromOrKey = null;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                toOrFromOrKey = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    from = parser.doubleValue();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    to = parser.doubleValue();
+                                } else {
+                                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName
+                                            + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                                }
+                            } else if (token == XContentParser.Token.VALUE_STRING) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    fromAsStr = parser.text();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    toAsStr = parser.text();
+                                } else if ("key".equals(toOrFromOrKey)) {
+                                    key = parser.text();
+                                } else {
+                                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                                }
+                            }
+                        }
+                        ranges.add(new RangeAggregator.Range(key, from, fromAsStr, to, toAsStr));
+                    }
+                }
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
         }
-        Boolean keyed = (Boolean) otherOptions.get(RangeAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
+
+        if (ranges == null) {
+            throw new SearchParseException(context, "Missing [ranges] in ranges aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
-        return factory;
-    }
 
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new DateRangeAggregatorFactory(null);
+        return new RangeAggregator.Factory(aggregationName, vsParser.config(), InternalDateRange.FACTORY, ranges, keyed);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java
index 88568bc..ac2c18e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java
@@ -26,7 +26,6 @@ import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
@@ -116,13 +115,8 @@ public class InternalDateRange extends InternalRange<InternalDateRange.Bucket, I
     public static class Factory extends InternalRange.Factory<InternalDateRange.Bucket, InternalDateRange> {
 
         @Override
-        public Type type() {
-            return TYPE;
-        }
-
-        @Override
-        public ValueType getValueType() {
-            return ValueType.DATE;
+        public String type() {
+            return TYPE.name();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceBuilder.java
new file mode 100644
index 0000000..ae8fc22
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceBuilder.java
@@ -0,0 +1,260 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.range.geodistance;
+
+import org.elasticsearch.common.geo.GeoDistance;
+import org.elasticsearch.common.geo.GeoPoint;
+import org.elasticsearch.common.unit.DistanceUnit;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Locale;
+
+/**
+ * Builder for the {@link GeoDistance} aggregation.
+ */
+public class GeoDistanceBuilder extends AggregationBuilder<GeoDistanceBuilder> {
+
+    /**
+     * A range of values.
+     */
+    public static class Range implements ToXContent {
+
+        private String key;
+        private Double from;
+        private Double to;
+
+        /**
+         * Create a new range.
+         * @param key   the identifier of this range
+         * @param from  the lower bound (inclusive)
+         * @param to    the upper bound (exclusive)
+         */
+        public Range(String key, Double from, Double to) {
+            this.key = key;
+            this.from = from;
+            this.to = to;
+        }
+
+        @Override
+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+            builder.startObject();
+            if (from != null) {
+                builder.field("from", from.doubleValue());
+            }
+            if (to != null) {
+                builder.field("to", to.doubleValue());
+            }
+            if (key != null) {
+                builder.field("key", key);
+            }
+            return builder.endObject();
+        }
+
+    }
+
+    private String field;
+    private DistanceUnit unit;
+    private GeoDistance distanceType;
+    private GeoPoint point;
+
+    private List<Range> ranges = new ArrayList<>();
+
+    /**
+     * Sole constructor.
+     */
+    public GeoDistanceBuilder(String name) {
+        super(name, InternalGeoDistance.TYPE.name());
+    }
+
+    /**
+     * Set the field to use to compute distances.
+     */
+    public GeoDistanceBuilder field(String field) {
+        this.field = field;
+        return this;
+    }
+
+    /**
+     * Set the unit to use for distances, default is kilometers.
+     */
+    public GeoDistanceBuilder unit(DistanceUnit unit) {
+        this.unit = unit;
+        return this;
+    }
+
+    /**
+     * Set the {@link GeoDistance distance type} to use, defaults to
+     * {@link GeoDistance#SLOPPY_ARC}.
+     */
+    public GeoDistanceBuilder distanceType(GeoDistance distanceType) {
+        this.distanceType = distanceType;
+        return this;
+    }
+
+    /**
+     * Set the point to calculate distances from using a
+     * <code>lat,lon</code> notation or geohash.
+     */
+    public GeoDistanceBuilder point(String latLon) {
+        return point(GeoPoint.parseFromLatLon(latLon));
+    }
+
+    /**
+     * Set the point to calculate distances from.
+     */
+    public GeoDistanceBuilder point(GeoPoint point) {
+        this.point = point;
+        return this;
+    }
+
+    /**
+     * Set the point to calculate distances from using its geohash.
+     */
+    public GeoDistanceBuilder geohash(String geohash) {
+        if (this.point == null) {
+            this.point = new GeoPoint();
+        }
+        this.point.resetFromGeoHash(geohash);
+        return this;
+    }
+
+    /**
+     * Set the latitude of the point to calculate distances from.
+     */
+    public GeoDistanceBuilder lat(double lat) {
+        if (this.point == null) {
+            point = new GeoPoint();
+        }
+        point.resetLat(lat);
+        return this;
+    }
+
+    /**
+     * Set the longitude of the point to calculate distances from.
+     */
+    public GeoDistanceBuilder lon(double lon) {
+        if (this.point == null) {
+            point = new GeoPoint();
+        }
+        point.resetLon(lon);
+        return this;
+    }
+
+    /**
+     * Add a new range to this aggregation.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     * @param to   the upper bound on the distances, exclusive
+     */
+    public GeoDistanceBuilder addRange(String key, double from, double to) {
+        ranges.add(new Range(key, from, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addRange(String, double, double)} but the key will be
+     * automatically generated based on <code>from</code> and <code>to</code>.
+     */
+    public GeoDistanceBuilder addRange(double from, double to) {
+        return addRange(null, from, to);
+    }
+
+    /**
+     * Add a new range with no lower bound.
+     *
+     * @param key the key to use for this range in the response
+     * @param to  the upper bound on the distances, exclusive
+     */
+    public GeoDistanceBuilder addUnboundedTo(String key, double to) {
+        ranges.add(new Range(key, null, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedTo(String, double)} but the key will be
+     * computed automatically.
+     */
+    public GeoDistanceBuilder addUnboundedTo(double to) {
+        return addUnboundedTo(null, to);
+    }
+
+    /**
+     * Add a new range with no upper bound.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     */
+    public GeoDistanceBuilder addUnboundedFrom(String key, double from) {
+        ranges.add(new Range(key, from, null));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedFrom(String, double)} but the key will be
+     * computed automatically.
+     */
+    public GeoDistanceBuilder addUnboundedFrom(double from) {
+        return addUnboundedFrom(null, from);
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (ranges.isEmpty()) {
+            throw new SearchSourceBuilderException("at least one range must be defined for geo_distance aggregation [" + getName() + "]");
+        }
+        if (point == null) {
+            throw new SearchSourceBuilderException("center point must be defined for geo_distance aggregation [" + getName() + "]");
+        }
+
+        if (field != null) {
+            builder.field("field", field);
+        }
+
+        if (unit != null) {
+            builder.field("unit", unit);
+        }
+
+        if (distanceType != null) {
+            builder.field("distance_type", distanceType.name().toLowerCase(Locale.ROOT));
+        }
+
+        builder.startObject("center")
+                .field("lat", point.lat())
+                .field("lon", point.lon())
+                .endObject();
+
+        builder.startArray("ranges");
+        for (Range range : ranges) {
+            range.toXContent(builder, params);
+        }
+        builder.endArray();
+
+        return builder.endObject();
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java
index 4201071..e110cc1 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java
@@ -21,306 +21,168 @@ package org.elasticsearch.search.aggregations.bucket.range.geodistance;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.SortedNumericDocValues;
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.geo.GeoDistance;
 import org.elasticsearch.common.geo.GeoDistance.FixedSourceDistance;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.DistanceUnit;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.index.fielddata.SortedBinaryDocValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
 import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
 import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Unmapped;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.GeoPointValuesSourceParser;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
 import org.elasticsearch.search.aggregations.support.GeoPointParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
-public class GeoDistanceParser extends GeoPointValuesSourceParser {
+public class GeoDistanceParser implements Aggregator.Parser {
 
     private static final ParseField ORIGIN_FIELD = new ParseField("origin", "center", "point", "por");
-    private static final ParseField UNIT_FIELD = new ParseField("unit");
-    private static final ParseField DISTANCE_TYPE_FIELD = new ParseField("distance_type");
-
-    private GeoPointParser geoPointParser = new GeoPointParser(InternalGeoDistance.TYPE, ORIGIN_FIELD);
-
-    public GeoDistanceParser() {
-        super(true, false);
-    }
 
     @Override
     public String type() {
         return InternalGeoDistance.TYPE.name();
     }
 
-    public static class Range extends RangeAggregator.Range {
-
-        static final Range PROTOTYPE = new Range(null, null, null);
-
-        public Range(String key, Double from, Double to) {
-            super(key(key, from, to), from, to);
-        }
-
-        private static String key(String key, Double from, Double to) {
-            if (key != null) {
-                return key;
-            }
-            StringBuilder sb = new StringBuilder();
-            sb.append((from == null || from == 0) ? "*" : from);
-            sb.append("-");
-            sb.append((to == null || Double.isInfinite(to)) ? "*" : to);
-            return sb.toString();
-        }
-
-        @Override
-        public Range readFrom(StreamInput in) throws IOException {
-            String key = in.readOptionalString();
-            double from = in.readDouble();
-            double to = in.readDouble();
-            return new Range(key, from, to);
+    private static String key(String key, double from, double to) {
+        if (key != null) {
+            return key;
         }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(key);
-            out.writeDouble(from);
-            out.writeDouble(to);
-        }
-
+        StringBuilder sb = new StringBuilder();
+        sb.append(from == 0 ? "*" : from);
+        sb.append("-");
+        sb.append(Double.isInfinite(to) ? "*" : to);
+        return sb.toString();
     }
 
     @Override
-    protected GeoDistanceFactory createFactory(
-            String aggregationName, ValuesSourceType valuesSourceType, ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        GeoPoint origin = (GeoPoint) otherOptions.get(ORIGIN_FIELD);
-        GeoDistanceFactory factory = new GeoDistanceFactory(aggregationName, origin);
-        List<Range> ranges = (List<Range>) otherOptions.get(RangeAggregator.RANGES_FIELD);
-        for (Range range : ranges) {
-            factory.addRange(range);
-        }
-        Boolean keyed = (Boolean) otherOptions.get(RangeAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        DistanceUnit unit = (DistanceUnit) otherOptions.get(UNIT_FIELD);
-        if (unit != null) {
-            factory.unit(unit);
-        }
-        GeoDistance distanceType = (GeoDistance) otherOptions.get(DISTANCE_TYPE_FIELD);
-        if (distanceType != null) {
-            factory.distanceType(distanceType);
-        }
-        return factory;
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (geoPointParser.token(aggregationName, currentFieldName, token, parser, parseFieldMatcher, otherOptions)) {
-            return true;
-        } else if (token == XContentParser.Token.VALUE_STRING) {
-            if (parseFieldMatcher.match(currentFieldName, UNIT_FIELD)) {
-                DistanceUnit unit = DistanceUnit.fromString(parser.text());
-                otherOptions.put(UNIT_FIELD, unit);
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, DISTANCE_TYPE_FIELD)) {
-                GeoDistance distanceType = GeoDistance.fromString(parser.text());
-                otherOptions.put(DISTANCE_TYPE_FIELD, distanceType);
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, RangeAggregator.KEYED_FIELD)) {
-                boolean keyed = parser.booleanValue();
-                otherOptions.put(RangeAggregator.KEYED_FIELD, keyed);
-                return true;
-            }
-        } else if (token == XContentParser.Token.START_ARRAY) {
-            if (parseFieldMatcher.match(currentFieldName, RangeAggregator.RANGES_FIELD)) {
-                List<Range> ranges = new ArrayList<>();
-                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                    String fromAsStr = null;
-                    String toAsStr = null;
-                    double from = 0.0;
-                    double to = Double.POSITIVE_INFINITY;
-                    String key = null;
-                    String toOrFromOrKey = null;
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        if (token == XContentParser.Token.FIELD_NAME) {
-                            toOrFromOrKey = parser.currentName();
-                        } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                            if (parseFieldMatcher.match(toOrFromOrKey, Range.FROM_FIELD)) {
-                                from = parser.doubleValue();
-                            } else if (parseFieldMatcher.match(toOrFromOrKey, Range.TO_FIELD)) {
-                                to = parser.doubleValue();
-                            }
-                        } else if (token == XContentParser.Token.VALUE_STRING) {
-                            if (parseFieldMatcher.match(toOrFromOrKey, Range.KEY_FIELD)) {
-                                key = parser.text();
-                            } else if (parseFieldMatcher.match(toOrFromOrKey, Range.FROM_FIELD)) {
-                                fromAsStr = parser.text();
-                            } else if (parseFieldMatcher.match(toOrFromOrKey, Range.TO_FIELD)) {
-                                toAsStr = parser.text();
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.GeoPoint> vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoDistance.TYPE, context).build();
+
+        GeoPointParser geoPointParser = new GeoPointParser(aggregationName, InternalGeoDistance.TYPE, context, ORIGIN_FIELD);
+
+        List<RangeAggregator.Range> ranges = null;
+        DistanceUnit unit = DistanceUnit.DEFAULT;
+        GeoDistance distanceType = GeoDistance.DEFAULT;
+        boolean keyed = false;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (geoPointParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_STRING) {
+                if ("unit".equals(currentFieldName)) {
+                    unit = DistanceUnit.fromString(parser.text());
+                } else if ("distance_type".equals(currentFieldName) || "distanceType".equals(currentFieldName)) {
+                    distanceType = GeoDistance.fromString(parser.text());
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if ("ranges".equals(currentFieldName)) {
+                    ranges = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        String fromAsStr = null;
+                        String toAsStr = null;
+                        double from = 0.0;
+                        double to = Double.POSITIVE_INFINITY;
+                        String key = null;
+                        String toOrFromOrKey = null;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                toOrFromOrKey = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    from = parser.doubleValue();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    to = parser.doubleValue();
+                                }
+                            } else if (token == XContentParser.Token.VALUE_STRING) {
+                                if ("key".equals(toOrFromOrKey)) {
+                                    key = parser.text();
+                                } else if ("from".equals(toOrFromOrKey)) {
+                                    fromAsStr = parser.text();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    toAsStr = parser.text();
+                                }
                             }
                         }
+                        ranges.add(new RangeAggregator.Range(key(key, from, to), from, fromAsStr, to, toAsStr));
                     }
-                    if (fromAsStr != null || toAsStr != null) {
-                        ranges.add(new Range(key, Double.parseDouble(fromAsStr), Double.parseDouble(toAsStr)));
-                    } else {
-                        ranges.add(new Range(key, from, to));
-                    }
+                } else  {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-                otherOptions.put(RangeAggregator.RANGES_FIELD, ranges);
-                return true;
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
             }
         }
-        return false;
-    }
-
-    public static class GeoDistanceFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, GeoDistanceFactory> {
-
-        private final GeoPoint origin;
-        private final InternalRange.Factory rangeFactory;
-        private List<Range> ranges = new ArrayList<>();
-        private DistanceUnit unit = DistanceUnit.DEFAULT;
-        private GeoDistance distanceType = GeoDistance.DEFAULT;
-        private boolean keyed = false;
-
-        public GeoDistanceFactory(String name, GeoPoint origin) {
-            this(name, origin, InternalGeoDistance.FACTORY);
-        }
-
-        private GeoDistanceFactory(String name, GeoPoint origin, InternalRange.Factory rangeFactory) {
-            super(name, rangeFactory.type(), rangeFactory.getValueSourceType(), rangeFactory.getValueType());
-            this.origin = origin;
-            this.rangeFactory = rangeFactory;
-        }
-
-        public GeoDistanceFactory addRange(Range range) {
-            ranges.add(range);
-            return this;
-        }
 
-        /**
-         * Add a new range to this aggregation.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param from
-         *            the lower bound on the distances, inclusive
-         * @param to
-         *            the upper bound on the distances, exclusive
-         */
-        public GeoDistanceFactory addRange(String key, double from, double to) {
-            ranges.add(new Range(key, from, to));
-            return this;
+        if (ranges == null) {
+            throw new SearchParseException(context, "Missing [ranges] in geo_distance aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
-        /**
-         * Same as {@link #addRange(String, double, double)} but the key will be
-         * automatically generated based on <code>from</code> and
-         * <code>to</code>.
-         */
-        public GeoDistanceFactory addRange(double from, double to) {
-            return addRange(null, from, to);
+        GeoPoint origin = geoPointParser.geoPoint();
+        if (origin == null) {
+            throw new SearchParseException(context, "Missing [origin] in geo_distance aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
-        /**
-         * Add a new range with no lower bound.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param to
-         *            the upper bound on the distances, exclusive
-         */
-        public GeoDistanceFactory addUnboundedTo(String key, double to) {
-            ranges.add(new Range(key, null, to));
-            return this;
-        }
-
-        /**
-         * Same as {@link #addUnboundedTo(String, double)} but the key will be
-         * computed automatically.
-         */
-        public GeoDistanceFactory addUnboundedTo(double to) {
-            return addUnboundedTo(null, to);
-        }
-
-        /**
-         * Add a new range with no upper bound.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param from
-         *            the lower bound on the distances, inclusive
-         */
-        public GeoDistanceFactory addUnboundedFrom(String key, double from) {
-            addRange(new Range(key, from, null));
-            return this;
-        }
-
-        /**
-         * Same as {@link #addUnboundedFrom(String, double)} but the key will be
-         * computed automatically.
-         */
-        public GeoDistanceFactory addUnboundedFrom(double from) {
-            return addUnboundedFrom(null, from);
-        }
+        return new GeoDistanceFactory(aggregationName, vsParser.config(), InternalGeoDistance.FACTORY, origin, unit, distanceType, ranges, keyed);
+    }
 
-        public List<Range> range() {
-            return ranges;
-        }
+    private static class GeoDistanceFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint> {
 
-        @Override
-        public String getWriteableName() {
-            return InternalGeoDistance.TYPE.name();
-        }
+        private final GeoPoint origin;
+        private final DistanceUnit unit;
+        private final GeoDistance distanceType;
+        private final InternalRange.Factory rangeFactory;
+        private final List<RangeAggregator.Range> ranges;
+        private final boolean keyed;
 
-        public GeoDistanceFactory unit(DistanceUnit unit) {
+        public GeoDistanceFactory(String name, ValuesSourceConfig<ValuesSource.GeoPoint> valueSourceConfig,
+                                  InternalRange.Factory rangeFactory, GeoPoint origin, DistanceUnit unit, GeoDistance distanceType,
+                                  List<RangeAggregator.Range> ranges, boolean keyed) {
+            super(name, rangeFactory.type(), valueSourceConfig);
+            this.origin = origin;
             this.unit = unit;
-            return this;
-        }
-
-        public DistanceUnit unit() {
-            return unit;
-        }
-
-        public GeoDistanceFactory distanceType(GeoDistance distanceType) {
             this.distanceType = distanceType;
-            return this;
-        }
-
-        public GeoDistance distanceType() {
-            return distanceType;
-        }
-
-        public GeoDistanceFactory keyed(boolean keyed) {
+            this.rangeFactory = rangeFactory;
+            this.ranges = ranges;
             this.keyed = keyed;
-            return this;
-        }
-
-        public boolean keyed() {
-            return keyed;
         }
 
         @Override
@@ -341,59 +203,6 @@ public class GeoDistanceParser extends GeoPointValuesSourceParser {
                     pipelineAggregators, metaData);
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(ORIGIN_FIELD.getPreferredName(), origin);
-            builder.field(RangeAggregator.RANGES_FIELD.getPreferredName(), ranges);
-            builder.field(RangeAggregator.KEYED_FIELD.getPreferredName(), keyed);
-            builder.field(UNIT_FIELD.getPreferredName(), unit);
-            builder.field(DISTANCE_TYPE_FIELD.getPreferredName(), distanceType);
-            return builder;
-        }
-
-        @Override
-        protected GeoDistanceFactory innerReadFrom(
-                String name, ValuesSourceType valuesSourceType, ValueType targetValueType, StreamInput in) throws IOException {
-            GeoPoint origin = new GeoPoint(in.readDouble(), in.readDouble());
-            int size = in.readVInt();
-            GeoDistanceFactory factory = new GeoDistanceFactory(name, origin);
-            for (int i = 0; i < size; i++) {
-                factory.addRange(Range.PROTOTYPE.readFrom(in));
-            }
-            factory.keyed = in.readBoolean();
-            factory.distanceType = GeoDistance.readGeoDistanceFrom(in);
-            factory.unit = DistanceUnit.readDistanceUnit(in);
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDouble(origin.lat());
-            out.writeDouble(origin.lon());
-            out.writeVInt(ranges.size());
-            for (Range range : ranges) {
-                range.writeTo(out);
-            }
-            out.writeBoolean(keyed);
-            distanceType.writeTo(out);
-            DistanceUnit.writeDistanceUnit(out, unit);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(origin, ranges, keyed, distanceType, unit);
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            GeoDistanceFactory other = (GeoDistanceFactory) obj;
-            return Objects.equals(origin, other.origin)
-                    && Objects.equals(ranges, other.ranges)
-                    && Objects.equals(keyed, other.keyed)
-                    && Objects.equals(distanceType, other.distanceType)
-                    && Objects.equals(unit, other.unit);
-        }
-
         private static class DistanceSource extends ValuesSource.Numeric {
 
             private final ValuesSource.GeoPoint source;
@@ -435,9 +244,4 @@ public class GeoDistanceParser extends GeoPointValuesSourceParser {
 
     }
 
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new GeoDistanceFactory(null, null);
-    }
-
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java
index 2c16d93..c5c67df 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java
@@ -26,8 +26,6 @@ import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -106,18 +104,8 @@ public class InternalGeoDistance extends InternalRange<InternalGeoDistance.Bucke
     public static class Factory extends InternalRange.Factory<InternalGeoDistance.Bucket, InternalGeoDistance> {
 
         @Override
-        public Type type() {
-            return TYPE;
-        }
-
-        @Override
-        public ValuesSourceType getValueSourceType() {
-            return ValuesSourceType.GEOPOINT;
-        }
-
-        @Override
-        public ValueType getValueType() {
-            return ValueType.GEOPOINT;
+        public String type() {
+            return TYPE.name();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeAggregatorFactory.java
deleted file mode 100644
index 4eddc32..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeAggregatorFactory.java
+++ /dev/null
@@ -1,268 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range.ipv4;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.network.Cidrs;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.AbstractFactory;
-
-import java.io.IOException;
-import java.util.Objects;
-
-public class IPv4RangeAggregatorFactory extends AbstractFactory<IPv4RangeAggregatorFactory, IPv4RangeAggregatorFactory.Range> {
-
-    public IPv4RangeAggregatorFactory(String name) {
-        super(name, InternalIPv4Range.FACTORY);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return InternalIPv4Range.TYPE.name();
-    }
-
-    /**
-     * Add a new range to this aggregation.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the distances, inclusive
-     * @param to
-     *            the upper bound on the distances, exclusive
-     */
-    public IPv4RangeAggregatorFactory addRange(String key, String from, String to) {
-        addRange(new Range(key, from, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addMaskRange(String, String)} but uses the mask itself as
-     * a key.
-     */
-    public IPv4RangeAggregatorFactory addMaskRange(String key, String mask) {
-        return addRange(new Range(key, mask));
-    }
-
-    /**
-     * Same as {@link #addMaskRange(String, String)} but uses the mask itself as
-     * a key.
-     */
-    public IPv4RangeAggregatorFactory addMaskRange(String mask) {
-        return addRange(new Range(mask, mask));
-    }
-
-    /**
-     * Same as {@link #addRange(String, String, String)} but the key will be
-     * automatically generated.
-     */
-    public IPv4RangeAggregatorFactory addRange(String from, String to) {
-        return addRange(null, from, to);
-    }
-
-    /**
-     * Same as {@link #addRange(String, String, String)} but there will be no
-     * lower bound.
-     */
-    public IPv4RangeAggregatorFactory addUnboundedTo(String key, String to) {
-        addRange(new Range(key, null, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedTo(String, String)} but the key will be
-     * generated automatically.
-     */
-    public IPv4RangeAggregatorFactory addUnboundedTo(String to) {
-        return addUnboundedTo(null, to);
-    }
-
-    /**
-     * Same as {@link #addRange(String, String, String)} but there will be no
-     * upper bound.
-     */
-    public IPv4RangeAggregatorFactory addUnboundedFrom(String key, String from) {
-        addRange(new Range(key, from, null));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedFrom(String, String)} but the key will be
-     * generated automatically.
-     */
-    public IPv4RangeAggregatorFactory addUnboundedFrom(String from) {
-        return addUnboundedFrom(null, from);
-    }
-
-    @Override
-    protected IPv4RangeAggregatorFactory createFactoryFromStream(String name, StreamInput in) throws IOException {
-        int size = in.readVInt();
-        IPv4RangeAggregatorFactory factory = new IPv4RangeAggregatorFactory(name);
-        for (int i = 0; i < size; i++) {
-            factory.addRange(Range.PROTOTYPE.readFrom(in));
-        }
-        return factory;
-    }
-
-    public static class Range extends RangeAggregator.Range {
-
-        static final Range PROTOTYPE = new Range(null, -1, null, -1, null, null);
-        static final ParseField MASK_FIELD = new ParseField("mask");
-
-        private String cidr;
-
-        public Range(String key, double from, double to) {
-            super(key, from, to);
-        }
-
-        public Range(String key, String from, String to) {
-            super(key, from, to);
-        }
-
-        public Range(String key, String cidr) {
-            super(key, -1, null, -1, null);
-            this.cidr = cidr;
-            if (cidr != null) {
-                parseMaskRange();
-            }
-        }
-
-        private Range(String key, double from, String fromAsStr, double to, String toAsStr, String cidr) {
-            super(key, from, fromAsStr, to, toAsStr);
-            this.cidr = cidr;
-            if (cidr != null) {
-                parseMaskRange();
-            }
-        }
-
-        public String mask() {
-            return cidr;
-        }
-
-        private void parseMaskRange() throws IllegalArgumentException {
-            long[] fromTo = Cidrs.cidrMaskToMinMax(cidr);
-            from = fromTo[0] == 0 ? Double.NEGATIVE_INFINITY : fromTo[0];
-            to = fromTo[1] == InternalIPv4Range.MAX_IP ? Double.POSITIVE_INFINITY : fromTo[1];
-            if (key == null) {
-                key = cidr;
-            }
-        }
-
-        @Override
-        public Range fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-
-            XContentParser.Token token;
-            String currentFieldName = null;
-            double from = Double.NEGATIVE_INFINITY;
-            String fromAsStr = null;
-            double to = Double.POSITIVE_INFINITY;
-            String toAsStr = null;
-            String key = null;
-            String cidr = null;
-            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                if (token == XContentParser.Token.FIELD_NAME) {
-                    currentFieldName = parser.currentName();
-                } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                    if (parseFieldMatcher.match(currentFieldName, FROM_FIELD)) {
-                        from = parser.doubleValue();
-                    } else if (parseFieldMatcher.match(currentFieldName, TO_FIELD)) {
-                        to = parser.doubleValue();
-                    }
-                } else if (token == XContentParser.Token.VALUE_STRING) {
-                    if (parseFieldMatcher.match(currentFieldName, FROM_FIELD)) {
-                        fromAsStr = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, TO_FIELD)) {
-                        toAsStr = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, KEY_FIELD)) {
-                        key = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, MASK_FIELD)) {
-                        cidr = parser.text();
-                    }
-                }
-            }
-            return new Range(key, from, fromAsStr, to, toAsStr, cidr);
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            if (key != null) {
-                builder.field(KEY_FIELD.getPreferredName(), key);
-            }
-            if (cidr != null) {
-                builder.field(MASK_FIELD.getPreferredName(), cidr);
-            } else {
-                if (Double.isFinite(from)) {
-                    builder.field(FROM_FIELD.getPreferredName(), from);
-                }
-                if (Double.isFinite(to)) {
-                    builder.field(TO_FIELD.getPreferredName(), to);
-                }
-                if (fromAsStr != null) {
-                    builder.field(FROM_FIELD.getPreferredName(), fromAsStr);
-                }
-                if (toAsStr != null) {
-                    builder.field(TO_FIELD.getPreferredName(), toAsStr);
-                }
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        public Range readFrom(StreamInput in) throws IOException {
-            String key = in.readOptionalString();
-            String fromAsStr = in.readOptionalString();
-            String toAsStr = in.readOptionalString();
-            double from = in.readDouble();
-            double to = in.readDouble();
-            String mask = in.readOptionalString();
-            return new Range(key, from, fromAsStr, to, toAsStr, mask);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(key);
-            out.writeOptionalString(fromAsStr);
-            out.writeOptionalString(toAsStr);
-            out.writeDouble(from);
-            out.writeDouble(to);
-            out.writeOptionalString(cidr);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(super.hashCode(), cidr);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            return super.equals(obj)
-                    && Objects.equals(cidr, ((Range) obj).cidr);
-        }
-
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeBuilder.java
new file mode 100644
index 0000000..5ac3f2a
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeBuilder.java
@@ -0,0 +1,110 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.range.ipv4;
+
+import org.elasticsearch.common.network.Cidrs;
+import org.elasticsearch.search.aggregations.bucket.range.AbstractRangeBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+/**
+ * Builder for the {@code IPv4Range} aggregation.
+ */
+public class IPv4RangeBuilder extends AbstractRangeBuilder<IPv4RangeBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public IPv4RangeBuilder(String name) {
+        super(name, InternalIPv4Range.TYPE.name());
+    }
+
+    /**
+     * Add a new range to this aggregation.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     * @param to   the upper bound on the distances, exclusive
+     */
+    public IPv4RangeBuilder addRange(String key, String from, String to) {
+        ranges.add(new Range(key, from, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addMaskRange(String, String)} but uses the mask itself as a key.
+     */
+    public IPv4RangeBuilder addMaskRange(String mask) {
+        return addMaskRange(mask, mask);
+    }
+
+    /**
+     * Add a range based on a CIDR mask.
+     */
+    public IPv4RangeBuilder addMaskRange(String key, String mask) {
+        long[] fromTo;
+        try {
+            fromTo = Cidrs.cidrMaskToMinMax(mask);
+        } catch (IllegalArgumentException e) {
+            throw new SearchSourceBuilderException("invalid CIDR mask [" + mask + "] in ip_range aggregation [" + getName() + "]", e);
+        }
+        ranges.add(new Range(key, fromTo[0] == 0 ? null : fromTo[0], fromTo[1] == InternalIPv4Range.MAX_IP ? null : fromTo[1]));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addRange(String, String, String)} but the key will be
+     * automatically generated.
+     */
+    public IPv4RangeBuilder addRange(String from, String to) {
+        return addRange(null, from, to);
+    }
+
+    /**
+     * Same as {@link #addRange(String, String, String)} but there will be no lower bound.
+     */
+    public IPv4RangeBuilder addUnboundedTo(String key, String to) {
+        ranges.add(new Range(key, null, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedTo(String, String)} but the key will be
+     * generated automatically.
+     */
+    public IPv4RangeBuilder addUnboundedTo(String to) {
+        return addUnboundedTo(null, to);
+    }
+
+    /**
+     * Same as {@link #addRange(String, String, String)} but there will be no upper bound.
+     */
+    public IPv4RangeBuilder addUnboundedFrom(String key, String from) {
+        ranges.add(new Range(key, from, null));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedFrom(String, String)} but the key will be
+     * generated automatically.
+     */
+    public IPv4RangeBuilder addUnboundedFrom(String from) {
+        return addUnboundedFrom(null, from);
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java
index a6c3ed3..e20d1ac 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java
@@ -26,7 +26,6 @@ import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -112,13 +111,8 @@ public class InternalIPv4Range extends InternalRange<InternalIPv4Range.Bucket, I
     public static class Factory extends InternalRange.Factory<InternalIPv4Range.Bucket, InternalIPv4Range> {
 
         @Override
-        public Type type() {
-            return TYPE;
-        }
-
-        @Override
-        public ValueType getValueType() {
-            return ValueType.IP;
+        public String type() {
+            return TYPE.name();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java
index 82fe569..dc1e2a6 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java
@@ -18,28 +18,25 @@
  */
 package org.elasticsearch.search.aggregations.bucket.range.ipv4;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.network.Cidrs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.bucket.range.RangeParser;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.List;
-import java.util.Map;
 
 /**
  *
  */
-public class IpRangeParser extends RangeParser {
-
-    public IpRangeParser() {
-        super(true, false, false);
-    }
+public class IpRangeParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -47,29 +44,99 @@ public class IpRangeParser extends RangeParser {
     }
 
     @Override
-    protected Range parseRange(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-        return IPv4RangeAggregatorFactory.Range.PROTOTYPE.fromXContent(parser, parseFieldMatcher);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalIPv4Range.TYPE, context)
+                .targetValueType(ValueType.IP)
+                .formattable(false)
+                .build();
+
+        List<RangeAggregator.Range> ranges = null;
+        boolean keyed = false;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if ("ranges".equals(currentFieldName)) {
+                    ranges = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        double from = Double.NEGATIVE_INFINITY;
+                        String fromAsStr = null;
+                        double to = Double.POSITIVE_INFINITY;
+                        String toAsStr = null;
+                        String key = null;
+                        String mask = null;
+                        String toOrFromOrMaskOrKey = null;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                toOrFromOrMaskOrKey = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if ("from".equals(toOrFromOrMaskOrKey)) {
+                                    from = parser.doubleValue();
+                                } else if ("to".equals(toOrFromOrMaskOrKey)) {
+                                    to = parser.doubleValue();
+                                }
+                            } else if (token == XContentParser.Token.VALUE_STRING) {
+                                if ("from".equals(toOrFromOrMaskOrKey)) {
+                                    fromAsStr = parser.text();
+                                } else if ("to".equals(toOrFromOrMaskOrKey)) {
+                                    toAsStr = parser.text();
+                                } else if ("key".equals(toOrFromOrMaskOrKey)) {
+                                    key = parser.text();
+                                } else if ("mask".equals(toOrFromOrMaskOrKey)) {
+                                    mask = parser.text();
+                                }
+                            }
+                        }
+                        RangeAggregator.Range range = new RangeAggregator.Range(key, from, fromAsStr, to, toAsStr);
+                        if (mask != null) {
+                            parseMaskRange(mask, range, aggregationName, context);
+                        }
+                        ranges.add(range);
+                    }
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
+        }
 
-    @Override
-    protected IPv4RangeAggregatorFactory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        IPv4RangeAggregatorFactory factory = new IPv4RangeAggregatorFactory(aggregationName);
-        List<IPv4RangeAggregatorFactory.Range> ranges = (List<IPv4RangeAggregatorFactory.Range>) otherOptions
-                .get(RangeAggregator.RANGES_FIELD);
-        for (IPv4RangeAggregatorFactory.Range range : ranges) {
-            factory.addRange(range);
+        if (ranges == null) {
+            throw new SearchParseException(context, "Missing [ranges] in ranges aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
-        Boolean keyed = (Boolean) otherOptions.get(RangeAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
+
+        return new RangeAggregator.Factory(aggregationName, vsParser.config(), InternalIPv4Range.FACTORY, ranges, keyed);
+    }
+
+    private static void parseMaskRange(String cidr, RangeAggregator.Range range, String aggregationName, SearchContext ctx) {
+        long[] fromTo;
+        try {
+            fromTo = Cidrs.cidrMaskToMinMax(cidr);
+        } catch (IllegalArgumentException e) {
+            throw new SearchParseException(ctx, "invalid CIDR mask [" + cidr + "] in aggregation [" + aggregationName + "]",
+                    null, e);
         }
-        return factory;
+        range.from = fromTo[0] == 0 ? Double.NEGATIVE_INFINITY : fromTo[0];
+        range.to = fromTo[1] == InternalIPv4Range.MAX_IP ? Double.POSITIVE_INFINITY : fromTo[1];
+        if (range.key == null) {
+            range.key = cidr;
         }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new IPv4RangeAggregatorFactory(null);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedSamplerParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedSamplerParser.java
deleted file mode 100644
index 93ced0b..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedSamplerParser.java
+++ /dev/null
@@ -1,94 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations.bucket.sampler;
-
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
-
-/**
- *
- */
-public class DiversifiedSamplerParser extends AnyValuesSourceParser {
-
-    public DiversifiedSamplerParser() {
-        super(true, false);
-    }
-
-    @Override
-    public String type() {
-        return SamplerAggregator.DiversifiedFactory.TYPE.name();
-    }
-
-    @Override
-    protected SamplerAggregator.DiversifiedFactory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        SamplerAggregator.DiversifiedFactory factory = new SamplerAggregator.DiversifiedFactory(aggregationName);
-        Integer shardSize = (Integer) otherOptions.get(SamplerAggregator.SHARD_SIZE_FIELD);
-        if (shardSize != null) {
-            factory.shardSize(shardSize);
-        }
-        Integer maxDocsPerValue = (Integer) otherOptions.get(SamplerAggregator.MAX_DOCS_PER_VALUE_FIELD);
-        if (maxDocsPerValue != null) {
-            factory.maxDocsPerValue(maxDocsPerValue);
-        }
-        String executionHint = (String) otherOptions.get(SamplerAggregator.EXECUTION_HINT_FIELD);
-        if (executionHint != null) {
-            factory.executionHint(executionHint);
-        }
-        return factory;
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.VALUE_NUMBER) {
-            if (parseFieldMatcher.match(currentFieldName, SamplerAggregator.SHARD_SIZE_FIELD)) {
-                int shardSize = parser.intValue();
-                otherOptions.put(SamplerAggregator.SHARD_SIZE_FIELD, shardSize);
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SamplerAggregator.MAX_DOCS_PER_VALUE_FIELD)) {
-                int maxDocsPerValue = parser.intValue();
-                otherOptions.put(SamplerAggregator.MAX_DOCS_PER_VALUE_FIELD, maxDocsPerValue);
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_STRING) {
-            if (parseFieldMatcher.match(currentFieldName, SamplerAggregator.EXECUTION_HINT_FIELD)) {
-                String executionHint = parser.text();
-                otherOptions.put(SamplerAggregator.EXECUTION_HINT_FIELD, executionHint);
-                return true;
-            }
-        }
-        return false;
-    }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new SamplerAggregator.DiversifiedFactory(null);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregationBuilder.java
new file mode 100644
index 0000000..a623735
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregationBuilder.java
@@ -0,0 +1,80 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.sampler;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Sampler} aggregation.
+ */
+public class SamplerAggregationBuilder extends ValuesSourceAggregationBuilder<SamplerAggregationBuilder> {
+
+    private int shardSize = SamplerParser.DEFAULT_SHARD_SAMPLE_SIZE;
+
+    int maxDocsPerValue = SamplerParser.MAX_DOCS_PER_VALUE_DEFAULT;
+    String executionHint = null;
+
+    /**
+     * Sole constructor.
+     */
+    public SamplerAggregationBuilder(String name) {
+        super(name, InternalSampler.TYPE.name());
+    }
+
+    /**
+     * Set the max num docs to be returned from each shard.
+     */
+    public SamplerAggregationBuilder shardSize(int shardSize) {
+        this.shardSize = shardSize;
+        return this;
+    }
+
+    public SamplerAggregationBuilder maxDocsPerValue(int maxDocsPerValue) {
+        this.maxDocsPerValue = maxDocsPerValue;
+        return this;
+    }
+
+    public SamplerAggregationBuilder executionHint(String executionHint) {
+        this.executionHint = executionHint;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        // builder.startObject();
+        if (shardSize != SamplerParser.DEFAULT_SHARD_SAMPLE_SIZE) {
+            builder.field(SamplerParser.SHARD_SIZE_FIELD.getPreferredName(), shardSize);
+        }
+
+        if (maxDocsPerValue != SamplerParser.MAX_DOCS_PER_VALUE_DEFAULT) {
+            builder.field(SamplerParser.MAX_DOCS_PER_VALUE_FIELD.getPreferredName(), maxDocsPerValue);
+        }
+        if (executionHint != null) {
+            builder.field(SamplerParser.EXECUTION_HINT_FIELD.getPreferredName(), executionHint);
+        }
+
+        return builder;
+    }
+
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java
index 9c3454e..8cb9809 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java
@@ -21,16 +21,12 @@ package org.elasticsearch.search.aggregations.bucket.sampler;
 import org.apache.lucene.index.LeafReaderContext;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.BestDocsDeferringCollector;
@@ -38,16 +34,14 @@ import org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Aggregate on only the top-scoring docs on a shard.
@@ -61,10 +55,6 @@ import java.util.Objects;
  */
 public class SamplerAggregator extends SingleBucketAggregator {
 
-    public static final ParseField SHARD_SIZE_FIELD = new ParseField("shard_size");
-    public static final ParseField MAX_DOCS_PER_VALUE_FIELD = new ParseField("max_docs_per_value");
-    public static final ParseField EXECUTION_HINT_FIELD = new ParseField("execution_hint");
-
 
     public enum ExecutionMode {
 
@@ -190,29 +180,13 @@ public class SamplerAggregator extends SingleBucketAggregator {
         return new InternalSampler(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends AggregatorFactory<Factory> {
-
-        public static final int DEFAULT_SHARD_SAMPLE_SIZE = 100;
+    public static class Factory extends AggregatorFactory {
 
-        private int shardSize = DEFAULT_SHARD_SAMPLE_SIZE;
-
-        public Factory(String name) {
-            super(name, InternalSampler.TYPE);
-        }
+        private int shardSize;
 
-        /**
-         * Set the max num docs to be returned from each shard.
-         */
-        public Factory shardSize(int shardSize) {
+        public Factory(String name, int shardSize) {
+            super(name, InternalSampler.TYPE.name());
             this.shardSize = shardSize;
-            return this;
-        }
-
-        /**
-         * Get the max num docs to be returned from each shard.
-         */
-        public int shardSize() {
-            return shardSize;
         }
 
         @Override
@@ -221,96 +195,19 @@ public class SamplerAggregator extends SingleBucketAggregator {
             return new SamplerAggregator(name, shardSize, factories, context, parent, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            builder.field(SHARD_SIZE_FIELD.getPreferredName(), shardSize);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected AggregatorFactory doReadFrom(String name, StreamInput in) throws IOException {
-            Factory factory = new Factory(name);
-            factory.shardSize = in.readVInt();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(shardSize);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(shardSize);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(shardSize, other.shardSize);
-        }
-
     }
 
-    public static class DiversifiedFactory extends ValuesSourceAggregatorFactory<ValuesSource, DiversifiedFactory> {
-
-        public static final Type TYPE = new Type("diversified_sampler");
+    public static class DiversifiedFactory extends ValuesSourceAggregatorFactory<ValuesSource> {
 
-        public static final int MAX_DOCS_PER_VALUE_DEFAULT = 1;
-
-        private int shardSize = Factory.DEFAULT_SHARD_SAMPLE_SIZE;
-        private int maxDocsPerValue = MAX_DOCS_PER_VALUE_DEFAULT;
-        private String executionHint = null;
-
-        public DiversifiedFactory(String name) {
-            super(name, TYPE, ValuesSourceType.ANY, null);
-        }
+        private int shardSize;
+        private int maxDocsPerValue;
+        private String executionHint;
 
-        /**
-         * Set the max num docs to be returned from each shard.
-         */
-        public DiversifiedFactory shardSize(int shardSize) {
+        public DiversifiedFactory(String name, int shardSize, String executionHint, ValuesSourceConfig vsConfig, int maxDocsPerValue) {
+            super(name, InternalSampler.TYPE.name(), vsConfig);
             this.shardSize = shardSize;
-            return this;
-        }
-
-        /**
-         * Get the max num docs to be returned from each shard.
-         */
-        public int shardSize() {
-            return shardSize;
-        }
-
-        /**
-         * Set the max num docs to be returned per value.
-         */
-        public DiversifiedFactory maxDocsPerValue(int maxDocsPerValue) {
             this.maxDocsPerValue = maxDocsPerValue;
-            return this;
-        }
-
-        /**
-         * Get the max num docs to be returned per value.
-         */
-        public int maxDocsPerValue() {
-            return maxDocsPerValue;
-        }
-
-        /**
-         * Set the execution hint.
-         */
-        public DiversifiedFactory executionHint(String executionHint) {
             this.executionHint = executionHint;
-            return this;
-        }
-
-        /**
-         * Get the execution hint.
-         */
-        public String executionHint() {
-            return executionHint;
         }
 
         @Override
@@ -359,45 +256,6 @@ public class SamplerAggregator extends SingleBucketAggregator {
             };
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(SHARD_SIZE_FIELD.getPreferredName(), shardSize);
-            builder.field(MAX_DOCS_PER_VALUE_FIELD.getPreferredName(), maxDocsPerValue);
-            if (executionHint != null) {
-                builder.field(EXECUTION_HINT_FIELD.getPreferredName(), executionHint);
-            }
-            return builder;
-        }
-
-        @Override
-        protected DiversifiedFactory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            DiversifiedFactory factory = new DiversifiedFactory(name);
-            factory.shardSize = in.readVInt();
-            factory.maxDocsPerValue = in.readVInt();
-            factory.executionHint = in.readOptionalString();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(shardSize);
-            out.writeVInt(maxDocsPerValue);
-            out.writeOptionalString(executionHint);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(shardSize, maxDocsPerValue, executionHint);
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            DiversifiedFactory other = (DiversifiedFactory) obj;
-            return Objects.equals(shardSize, other.shardSize)
-                    && Objects.equals(maxDocsPerValue, other.maxDocsPerValue)
-                    && Objects.equals(executionHint, other.executionHint);
-        }
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java
index 3259c75..498a7cb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java
@@ -18,12 +18,14 @@
  */
 package org.elasticsearch.search.aggregations.bucket.sampler;
 
-
-import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -32,44 +34,73 @@ import java.io.IOException;
  */
 public class SamplerParser implements Aggregator.Parser {
 
+    public static final int DEFAULT_SHARD_SAMPLE_SIZE = 100;
+    public static final ParseField SHARD_SIZE_FIELD = new ParseField("shard_size");
+    public static final ParseField MAX_DOCS_PER_VALUE_FIELD = new ParseField("max_docs_per_value");
+    public static final ParseField EXECUTION_HINT_FIELD = new ParseField("execution_hint");
+    public static final boolean DEFAULT_USE_GLOBAL_ORDINALS = false;
+    public static final int MAX_DOCS_PER_VALUE_DEFAULT = 1;
+
+
     @Override
     public String type() {
         return InternalSampler.TYPE.name();
     }
 
     @Override
-    public AggregatorFactory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
         XContentParser.Token token;
         String currentFieldName = null;
-        Integer shardSize = null;
+        String executionHint = null;
+        int shardSize = DEFAULT_SHARD_SAMPLE_SIZE;
+        int maxDocsPerValue = MAX_DOCS_PER_VALUE_DEFAULT;
+        ValuesSourceParser vsParser = null;
+        boolean diversityChoiceMade = false;
+
+        vsParser = ValuesSourceParser.any(aggregationName, InternalSampler.TYPE, context).scriptable(true).formattable(false).build();
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
             } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                if (context.parseFieldMatcher().match(currentFieldName, SamplerAggregator.SHARD_SIZE_FIELD)) {
+                if (context.parseFieldMatcher().match(currentFieldName, SHARD_SIZE_FIELD)) {
                     shardSize = parser.intValue();
+                } else if (context.parseFieldMatcher().match(currentFieldName, MAX_DOCS_PER_VALUE_FIELD)) {
+                    diversityChoiceMade = true;
+                    maxDocsPerValue = parser.intValue();
+                } else {
+                    throw new SearchParseException(context, "Unsupported property \"" + currentFieldName + "\" for aggregation \""
+                            + aggregationName, parser.getTokenLocation());
+                }
+            } else if (!vsParser.token(currentFieldName, token, parser)) {
+                if (context.parseFieldMatcher().match(currentFieldName, EXECUTION_HINT_FIELD)) {
+                    executionHint = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unsupported property \"" + currentFieldName + "\" for aggregation \"" + aggregationName);
+                    throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                            parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unsupported property \"" + currentFieldName + "\" for aggregation \"" + aggregationName);
+                throw new SearchParseException(context, "Unsupported property \"" + currentFieldName + "\" for aggregation \""
+                        + aggregationName, parser.getTokenLocation());
             }
         }
 
-        SamplerAggregator.Factory factory = new SamplerAggregator.Factory(aggregationName);
-        if (shardSize != null) {
-            factory.shardSize(shardSize);
+        ValuesSourceConfig vsConfig = vsParser.config();
+        if (vsConfig.valid()) {
+            return new SamplerAggregator.DiversifiedFactory(aggregationName, shardSize, executionHint, vsConfig, maxDocsPerValue);
+        } else {
+            if (diversityChoiceMade) {
+                throw new SearchParseException(context, "Sampler aggregation has " + MAX_DOCS_PER_VALUE_FIELD.getPreferredName()
+                        + " setting but no \"field\" or \"script\" setting to provide values for aggregation \"" + aggregationName + "\"",
+                        parser.getTokenLocation());
+
+            }
+            return new SamplerAggregator.Factory(aggregationName, shardSize);
         }
-        return factory;
     }
 
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new SamplerAggregator.Factory(null);
-    }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java
index 226c6d5..9e8ad33 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java
@@ -228,7 +228,7 @@ public class SignificantLongTerms extends InternalSignificantTerms<SignificantLo
         out.writeVLong(minDocCount);
         out.writeVLong(subsetSize);
         out.writeVLong(supersetSize);
-        SignificanceHeuristicStreams.writeTo(significanceHeuristic, out);
+        significanceHeuristic.writeTo(out);
         out.writeVInt(buckets.size());
         for (InternalSignificantTerms.Bucket bucket : buckets) {
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java
index b047947..6c1ca0a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java
@@ -214,7 +214,7 @@ public class SignificantStringTerms extends InternalSignificantTerms<Significant
         out.writeVLong(minDocCount);
         out.writeVLong(subsetSize);
         out.writeVLong(supersetSize);
-        SignificanceHeuristicStreams.writeTo(significanceHeuristic, out);
+        significanceHeuristic.writeTo(out);
         out.writeVInt(buckets.size());
         for (InternalSignificantTerms.Bucket bucket : buckets) {
             bucket.writeTo(out);
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java
index 3bb61d7..399e857 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java
@@ -26,51 +26,33 @@ import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.lucene.index.FilterableTermsEnum;
 import org.elasticsearch.common.lucene.index.FreqTermsEnum;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.bucket.BucketUtils;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicStreams;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
-public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource, SignificantTermsAggregatorFactory>
-        implements Releasable {
-
-    static final ParseField BACKGROUND_FILTER = new ParseField("background_filter");
-    static final ParseField HEURISTIC = new ParseField("significance_heuristic");
-
-    static final TermsAggregator.BucketCountThresholds DEFAULT_BUCKET_COUNT_THRESHOLDS = new TermsAggregator.BucketCountThresholds(
-            3, 0, 10, -1);
+public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource> implements Releasable {
 
     public SignificanceHeuristic getSignificanceHeuristic() {
         return significanceHeuristic;
@@ -116,8 +98,9 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
                     List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
                 final IncludeExclude.OrdinalsFilter filter = includeExclude == null ? null : includeExclude.convertToOrdinalsFilter();
                 return new GlobalOrdinalsSignificantTermsAggregator.WithHash(name, factories,
-                        (ValuesSource.Bytes.WithOrdinals.FieldData) valuesSource, bucketCountThresholds, filter, aggregationContext, parent,
-                        termsAggregatorFactory, pipelineAggregators, metaData);
+                        (ValuesSource.Bytes.WithOrdinals.FieldData) valuesSource, bucketCountThresholds, filter,
+ aggregationContext,
+                        parent, termsAggregatorFactory, pipelineAggregators, metaData);
             }
         };
 
@@ -146,132 +129,33 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
             return parseField.getPreferredName();
         }
     }
-
-    private IncludeExclude includeExclude = null;
-    private String executionHint = null;
+    private final IncludeExclude includeExclude;
+    private final String executionHint;
     private String indexedFieldName;
     private MappedFieldType fieldType;
     private FilterableTermsEnum termsEnum;
     private int numberOfAggregatorsCreated = 0;
-    private QueryBuilder<?> filterBuilder = null;
-    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new BucketCountThresholds(DEFAULT_BUCKET_COUNT_THRESHOLDS);
-    private SignificanceHeuristic significanceHeuristic = JLHScore.PROTOTYPE;
+    private final Query filter;
+    private final TermsAggregator.BucketCountThresholds bucketCountThresholds;
+    private final SignificanceHeuristic significanceHeuristic;
 
     protected TermsAggregator.BucketCountThresholds getBucketCountThresholds() {
         return new TermsAggregator.BucketCountThresholds(bucketCountThresholds);
     }
 
-    public SignificantTermsAggregatorFactory(String name, ValueType valueType) {
-        super(name, SignificantStringTerms.TYPE, ValuesSourceType.ANY, valueType);
-    }
+    public SignificantTermsAggregatorFactory(String name, ValuesSourceConfig valueSourceConfig, TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
+                                             String executionHint, Query filter, SignificanceHeuristic significanceHeuristic) {
 
-    public TermsAggregator.BucketCountThresholds bucketCountThresholds() {
-        return bucketCountThresholds;
-    }
-
-    public SignificantTermsAggregatorFactory bucketCountThresholds(TermsAggregator.BucketCountThresholds bucketCountThresholds) {
+        super(name, SignificantStringTerms.TYPE.name(), valueSourceConfig);
         this.bucketCountThresholds = bucketCountThresholds;
-        return this;
-    }
-
-    /**
-     * Sets the size - indicating how many term buckets should be returned
-     * (defaults to 10)
-     */
-    public SignificantTermsAggregatorFactory size(int size) {
-        bucketCountThresholds.setRequiredSize(size);
-        return this;
-    }
-
-    /**
-     * Sets the shard_size - indicating the number of term buckets each shard
-     * will return to the coordinating node (the node that coordinates the
-     * search execution). The higher the shard size is, the more accurate the
-     * results are.
-     */
-    public SignificantTermsAggregatorFactory shardSize(int shardSize) {
-        bucketCountThresholds.setShardSize(shardSize);
-        return this;
-    }
-
-    /**
-     * Set the minimum document count terms should have in order to appear in
-     * the response.
-     */
-    public SignificantTermsAggregatorFactory minDocCount(long minDocCount) {
-        bucketCountThresholds.setMinDocCount(minDocCount);
-        return this;
-    }
-
-    /**
-     * Set the minimum document count terms should have on the shard in order to
-     * appear in the response.
-     */
-    public SignificantTermsAggregatorFactory shardMinDocCount(long shardMinDocCount) {
-        bucketCountThresholds.setShardMinDocCount(shardMinDocCount);
-        return this;
-    }
-
-    /**
-     * Expert: sets an execution hint to the aggregation.
-     */
-    public SignificantTermsAggregatorFactory executionHint(String executionHint) {
-        this.executionHint = executionHint;
-        return this;
-    }
-
-    /**
-     * Expert: gets an execution hint to the aggregation.
-     */
-    public String executionHint() {
-        return executionHint;
-    }
-
-    public SignificantTermsAggregatorFactory backgroundFilter(QueryBuilder<?> filterBuilder) {
-        this.filterBuilder = filterBuilder;
-        return this;
-    }
-
-    public QueryBuilder<?> backgroundFilter() {
-        return filterBuilder;
-    }
-
-    /**
-     * Set terms to include and exclude from the aggregation results
-     */
-    public SignificantTermsAggregatorFactory includeExclude(IncludeExclude includeExclude) {
         this.includeExclude = includeExclude;
-        return this;
-    }
-
-    /**
-     * Get terms to include and exclude from the aggregation results
-     */
-    public IncludeExclude includeExclude() {
-        return includeExclude;
-    }
-
-    public SignificantTermsAggregatorFactory significanceHeuristic(SignificanceHeuristic significanceHeuristic) {
+        this.executionHint = executionHint;
         this.significanceHeuristic = significanceHeuristic;
-        return this;
-    }
-
-    public SignificanceHeuristic significanceHeuristic() {
-        return significanceHeuristic;
-    }
-
-    @Override
-    public void doInit(AggregationContext context) {
-        super.doInit(context);
-        setFieldInfo();
-        significanceHeuristic.initialize(context.searchContext());
-    }
-
-    private void setFieldInfo() {
-        if (!config.unmapped()) {
+        if (!valueSourceConfig.unmapped()) {
             this.indexedFieldName = config.fieldContext().field();
             fieldType = SearchContext.current().smartNameFieldType(indexedFieldName);
         }
+        this.filter = filter;
     }
 
     @Override
@@ -297,18 +181,6 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
         }
 
         numberOfAggregatorsCreated++;
-        BucketCountThresholds bucketCountThresholds = new BucketCountThresholds(this.bucketCountThresholds);
-        if (bucketCountThresholds.getShardSize() == DEFAULT_BUCKET_COUNT_THRESHOLDS.getShardSize()) {
-            //The user has not made a shardSize selection .
-            //Use default heuristic to avoid any wrong-ranking caused by distributed counting
-            //but request double the usual amount.
-            //We typically need more than the number of "top" terms requested by other aggregations
-            //as the significance algorithm is in less of a position to down-select at shard-level -
-            //some of the things we want to find have only one occurrence on each shard and as
-            // such are impossible to differentiate from non-significant terms at that early stage.
-            bucketCountThresholds.setShardSize(2 * BucketUtils.suggestShardSideQueueSize(bucketCountThresholds.getRequiredSize(),
-                    aggregationContext.searchContext().numberOfShards()));
-        }
 
         if (valuesSource instanceof ValuesSource.Bytes) {
             ExecutionMode execution = null;
@@ -365,14 +237,6 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
         }
         SearchContext searchContext = context.searchContext();
         IndexReader reader = searchContext.searcher().getIndexReader();
-        Query filter = null;
-        try {
-            if (filterBuilder != null) {
-                filter = filterBuilder.toFilter(context.searchContext().indexShard().getQueryShardContext());
-            }
-        } catch (IOException e) {
-            throw new ElasticsearchException("failed to create filter: " + filterBuilder.toString(), e);
-        }
         try {
             if (numberOfAggregatorsCreated == 1) {
                 // Setup a termsEnum for sole use by one aggregator
@@ -417,68 +281,4 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
             termsEnum = null;
         }
     }
-
-    @Override
-    protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        bucketCountThresholds.toXContent(builder, params);
-        if (executionHint != null) {
-            builder.field(TermsAggregatorFactory.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
-        }
-        if (filterBuilder != null) {
-            builder.field(BACKGROUND_FILTER.getPreferredName(), filterBuilder);
-        }
-        if (includeExclude != null) {
-            includeExclude.toXContent(builder, params);
-        }
-        significanceHeuristic.toXContent(builder, params);
-        return builder;
-    }
-
-    @Override
-    protected SignificantTermsAggregatorFactory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        SignificantTermsAggregatorFactory factory = new SignificantTermsAggregatorFactory(name, targetValueType);
-        factory.bucketCountThresholds = BucketCountThresholds.readFromStream(in);
-        factory.executionHint = in.readOptionalString();
-        if (in.readBoolean()) {
-            factory.filterBuilder = in.readQuery();
-        }
-        if (in.readBoolean()) {
-            factory.includeExclude = IncludeExclude.readFromStream(in);
-        }
-        factory.significanceHeuristic = SignificanceHeuristicStreams.read(in);
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        bucketCountThresholds.writeTo(out);
-        out.writeOptionalString(executionHint);
-        boolean hasfilterBuilder = filterBuilder != null;
-        out.writeBoolean(hasfilterBuilder);
-        if (hasfilterBuilder) {
-            out.writeQuery(filterBuilder);
-        }
-        boolean hasIncExc = includeExclude != null;
-        out.writeBoolean(hasIncExc);
-        if (hasIncExc) {
-            includeExclude.writeTo(out);
-        }
-        SignificanceHeuristicStreams.writeTo(significanceHeuristic, out);
-    }
-
-    @Override
-    protected int innerHashCode() {
-        return Objects.hash(bucketCountThresholds, executionHint, filterBuilder, includeExclude, significanceHeuristic);
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        SignificantTermsAggregatorFactory other = (SignificantTermsAggregatorFactory) obj;
-        return Objects.equals(bucketCountThresholds, other.bucketCountThresholds)
-                && Objects.equals(executionHint, other.executionHint)
-                && Objects.equals(filterBuilder, other.filterBuilder)
-                && Objects.equals(includeExclude, other.includeExclude)
-                && Objects.equals(significanceHeuristic, other.significanceHeuristic);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java
new file mode 100644
index 0000000..b67ce2a
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java
@@ -0,0 +1,277 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.significant;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicBuilder;
+import org.elasticsearch.search.aggregations.bucket.terms.AbstractTermsParametersParser;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
+
+import java.io.IOException;
+
+/**
+ * Creates an aggregation that finds interesting or unusual occurrences of terms in a result set.
+ * <p>
+ * This feature is marked as experimental, and may be subject to change in the future.  If you
+ * use this feature, please let us know your experience with it!
+ */
+public class SignificantTermsBuilder extends AggregationBuilder<SignificantTermsBuilder> {
+
+    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new TermsAggregator.BucketCountThresholds(-1, -1, -1, -1);
+
+    private String field;
+    private String executionHint;
+    private String includePattern;
+    private int includeFlags;
+    private String excludePattern;
+    private int excludeFlags;
+    private String[] includeTerms = null;
+    private String[] excludeTerms = null;
+    private QueryBuilder filterBuilder;
+    private SignificanceHeuristicBuilder significanceHeuristicBuilder;
+
+    /**
+     * Sole constructor.
+     */
+    public SignificantTermsBuilder(String name) {
+        super(name, SignificantStringTerms.TYPE.name());
+    }
+
+    /**
+     * Set the field to fetch significant terms from.
+     */
+    public SignificantTermsBuilder field(String field) {
+        this.field = field;
+        return this;
+    }
+
+    /**
+     * Set the number of significant terms to retrieve.
+     */
+    public SignificantTermsBuilder size(int requiredSize) {
+        bucketCountThresholds.setRequiredSize(requiredSize);
+        return this;
+    }
+
+    /**
+     * Expert: Set the number of significant terms to retrieve on each shard.
+     */
+    public SignificantTermsBuilder shardSize(int shardSize) {
+        bucketCountThresholds.setShardSize(shardSize);
+        return this;
+    }
+
+    /**
+     * Only return significant terms that belong to at least <code>minDocCount</code> documents.
+     */
+    public SignificantTermsBuilder minDocCount(int minDocCount) {
+        bucketCountThresholds.setMinDocCount(minDocCount);
+        return this;
+    }
+    
+    /**
+     * Set the background filter to compare to. Defaults to the whole index.
+     */
+    public SignificantTermsBuilder backgroundFilter(QueryBuilder filter) {
+        this.filterBuilder = filter;
+        return this;
+    }
+    
+    /**
+     * Expert: set the minimum number of documents that a term should match to
+     * be retrieved from a shard.
+     */
+    public SignificantTermsBuilder shardMinDocCount(int shardMinDocCount) {
+        bucketCountThresholds.setShardMinDocCount(shardMinDocCount);
+        return this;
+    }
+
+    /**
+     * Expert: give an execution hint to this aggregation.
+     */
+    public SignificantTermsBuilder executionHint(String executionHint) {
+        this.executionHint = executionHint;
+        return this;
+    }
+
+    /**
+     * Define a regular expression that will determine what terms should be aggregated. The regular expression is based
+     * on the {@link java.util.regex.Pattern} class.
+     *
+     * @see #include(String, int)
+     */
+    public SignificantTermsBuilder include(String regex) {
+        return include(regex, 0);
+    }
+
+    /**
+     * Define a regular expression that will determine what terms should be aggregated. The regular expression is based
+     * on the {@link java.util.regex.Pattern} class.
+     *
+     * @see java.util.regex.Pattern#compile(String, int)
+     */
+    public SignificantTermsBuilder include(String regex, int flags) {
+        if (includeTerms != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of strings or a regex, not both");
+        }
+        this.includePattern = regex;
+        this.includeFlags = flags;
+        return this;
+    }
+    
+    /**
+     * Define a set of terms that should be aggregated.
+     */
+    public SignificantTermsBuilder include(String [] terms) {
+        if (includePattern != null) {
+            throw new IllegalArgumentException("include clause must be an array of exact values or a regex, not both");
+        }
+        this.includeTerms = terms;
+        return this;
+    }    
+    
+    /**
+     * Define a set of terms that should be aggregated.
+     */
+    public SignificantTermsBuilder include(long [] terms) {
+        if (includePattern != null) {
+            throw new IllegalArgumentException("include clause must be an array of exact values or a regex, not both");
+        }
+        this.includeTerms = longsArrToStringArr(terms);
+        return this;
+    }     
+    
+    private String[] longsArrToStringArr(long[] terms) {
+        String[] termsAsString = new String[terms.length];
+        for (int i = 0; i < terms.length; i++) {
+            termsAsString[i] = Long.toString(terms[i]);
+        }
+        return termsAsString;
+    }      
+    
+
+    /**
+     * Define a regular expression that will filter out terms that should be excluded from the aggregation. The regular
+     * expression is based on the {@link java.util.regex.Pattern} class.
+     *
+     * @see #exclude(String, int)
+     */
+    public SignificantTermsBuilder exclude(String regex) {
+        return exclude(regex, 0);
+    }
+
+    /**
+     * Define a regular expression that will filter out terms that should be excluded from the aggregation. The regular
+     * expression is based on the {@link java.util.regex.Pattern} class.
+     *
+     * @see java.util.regex.Pattern#compile(String, int)
+     */
+    public SignificantTermsBuilder exclude(String regex, int flags) {
+        if (excludeTerms != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of strings or a regex, not both");
+        }
+        this.excludePattern = regex;
+        this.excludeFlags = flags;
+        return this;
+    }
+    
+    /**
+     * Define a set of terms that should not be aggregated.
+     */
+    public SignificantTermsBuilder exclude(String [] terms) {
+        if (excludePattern != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of strings or a regex, not both");
+        }
+        this.excludeTerms = terms;
+        return this;
+    }    
+    
+    
+    /**
+     * Define a set of terms that should not be aggregated.
+     */
+    public SignificantTermsBuilder exclude(long [] terms) {
+        if (excludePattern != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of longs or a regex, not both");
+        }
+        this.excludeTerms = longsArrToStringArr(terms);
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (field != null) {
+            builder.field("field", field);
+        }
+        bucketCountThresholds.toXContent(builder);
+        if (executionHint != null) {
+            builder.field(AbstractTermsParametersParser.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
+        }
+        if (includePattern != null) {
+            if (includeFlags == 0) {
+                builder.field("include", includePattern);
+            } else {
+                builder.startObject("include")
+                        .field("pattern", includePattern)
+                        .field("flags", includeFlags)
+                        .endObject();
+            }
+        }
+        if (includeTerms != null) {
+            builder.array("include", includeTerms);
+        }
+        
+        if (excludePattern != null) {
+            if (excludeFlags == 0) {
+                builder.field("exclude", excludePattern);
+            } else {
+                builder.startObject("exclude")
+                        .field("pattern", excludePattern)
+                        .field("flags", excludeFlags)
+                        .endObject();
+            }
+        }
+        if (excludeTerms != null) {
+            builder.array("exclude", excludeTerms);
+        }
+        
+        if (filterBuilder != null) {
+            builder.field(SignificantTermsParametersParser.BACKGROUND_FILTER.getPreferredName());
+            filterBuilder.toXContent(builder, params); 
+        }
+        if (significanceHeuristicBuilder != null) {
+            significanceHeuristicBuilder.toXContent(builder, params);
+        }
+
+        return builder.endObject();
+    }
+
+    /**
+     * Expert: set the {@link SignificanceHeuristic} to use.
+     */
+    public SignificantTermsBuilder significanceHeuristic(SignificanceHeuristicBuilder significanceHeuristicBuilder) {
+        this.significanceHeuristicBuilder = significanceHeuristicBuilder;
+        return this;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParametersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParametersParser.java
new file mode 100644
index 0000000..0202298
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParametersParser.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.search.aggregations.bucket.significant;
+
+import org.apache.lucene.search.Query;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParser;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParserMapper;
+import org.elasticsearch.search.aggregations.bucket.terms.AbstractTermsParametersParser;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+
+
+public class SignificantTermsParametersParser extends AbstractTermsParametersParser {
+
+    private static final TermsAggregator.BucketCountThresholds DEFAULT_BUCKET_COUNT_THRESHOLDS = new TermsAggregator.BucketCountThresholds(3, 0, 10, -1);
+    private final SignificanceHeuristicParserMapper significanceHeuristicParserMapper;
+
+    public SignificantTermsParametersParser(SignificanceHeuristicParserMapper significanceHeuristicParserMapper) {
+        this.significanceHeuristicParserMapper = significanceHeuristicParserMapper;
+    }
+
+    public Query getFilter() {
+        return filter;
+    }
+
+    private Query filter = null;
+
+    private SignificanceHeuristic significanceHeuristic;
+
+    @Override
+    public TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds() {
+        return new TermsAggregator.BucketCountThresholds(DEFAULT_BUCKET_COUNT_THRESHOLDS);
+    }
+
+    static final ParseField BACKGROUND_FILTER = new ParseField("background_filter");
+
+    @Override
+    public void parseSpecial(String aggregationName, XContentParser parser, SearchContext context, XContentParser.Token token, String currentFieldName) throws IOException {
+
+        if (token == XContentParser.Token.START_OBJECT) {
+            SignificanceHeuristicParser significanceHeuristicParser = significanceHeuristicParserMapper.get(currentFieldName);
+            if (significanceHeuristicParser != null) {
+                significanceHeuristic = significanceHeuristicParser.parse(parser, context.parseFieldMatcher(), context);
+            } else if (context.parseFieldMatcher().match(currentFieldName, BACKGROUND_FILTER)) {
+                filter = context.indexShard().getQueryShardContext().parseInnerFilter(parser).query();
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
+            }
+        } else {
+            throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName
+                    + "].", parser.getTokenLocation());
+        }
+    }
+
+    public SignificanceHeuristic getSignificanceHeuristic() {
+        return significanceHeuristic;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java
index 54f3d3a..28e0fb5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java
@@ -18,41 +18,31 @@
  */
 package org.elasticsearch.search.aggregations.bucket.significant;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.BucketUtils;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParser;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParserMapper;
-import org.elasticsearch.search.aggregations.bucket.terms.AbstractTermsParser;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  *
  */
-public class SignificantTermsParser extends AbstractTermsParser {
+public class SignificantTermsParser implements Aggregator.Parser {
+
     private final SignificanceHeuristicParserMapper significanceHeuristicParserMapper;
-    private final IndicesQueriesRegistry queriesRegistry;
 
     @Inject
-    public SignificantTermsParser(SignificanceHeuristicParserMapper significanceHeuristicParserMapper,
-            IndicesQueriesRegistry queriesRegistry) {
+    public SignificantTermsParser(SignificanceHeuristicParserMapper significanceHeuristicParserMapper) {
         this.significanceHeuristicParserMapper = significanceHeuristicParserMapper;
-        this.queriesRegistry = queriesRegistry;
     }
 
     @Override
@@ -61,58 +51,32 @@ public class SignificantTermsParser extends AbstractTermsParser {
     }
 
     @Override
-    protected SignificantTermsAggregatorFactory doCreateFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, BucketCountThresholds bucketCountThresholds, SubAggCollectionMode collectMode, String executionHint,
-            IncludeExclude incExc, Map<ParseField, Object> otherOptions) {
-        SignificantTermsAggregatorFactory factory = new SignificantTermsAggregatorFactory(aggregationName, targetValueType);
-        if (bucketCountThresholds != null) {
-            factory.bucketCountThresholds(bucketCountThresholds);
-        }
-        if (executionHint != null) {
-            factory.executionHint(executionHint);
-        }
-        if (incExc != null) {
-            factory.includeExclude(incExc);
-        }
-        QueryBuilder<?> backgroundFilter = (QueryBuilder<?>) otherOptions.get(SignificantTermsAggregatorFactory.BACKGROUND_FILTER);
-        if (backgroundFilter != null) {
-            factory.backgroundFilter(backgroundFilter);
-        }
-        SignificanceHeuristic significanceHeuristic = (SignificanceHeuristic) otherOptions.get(SignificantTermsAggregatorFactory.HEURISTIC);
-        if (significanceHeuristic != null) {
-            factory.significanceHeuristic(significanceHeuristic);
-        }
-        return factory;
-    }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        SignificantTermsParametersParser aggParser = new SignificantTermsParametersParser(significanceHeuristicParserMapper);
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, SignificantStringTerms.TYPE, context)
+                .scriptable(false)
+                .formattable(true)
+                .build();
+        IncludeExclude.Parser incExcParser = new IncludeExclude.Parser();
+        aggParser.parse(aggregationName, parser, context, vsParser, incExcParser);
 
-    @Override
-    public boolean parseSpecial(String aggregationName, XContentParser parser, ParseFieldMatcher parseFieldMatcher, Token token,
-            String currentFieldName, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.START_OBJECT) {
-            SignificanceHeuristicParser significanceHeuristicParser = significanceHeuristicParserMapper.get(currentFieldName);
-            if (significanceHeuristicParser != null) {
-                SignificanceHeuristic significanceHeuristic = significanceHeuristicParser.parse(parser, parseFieldMatcher);
-                otherOptions.put(SignificantTermsAggregatorFactory.HEURISTIC, significanceHeuristic);
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SignificantTermsAggregatorFactory.BACKGROUND_FILTER)) {
-                QueryParseContext queryParseContext = new QueryParseContext(queriesRegistry);
-                queryParseContext.reset(parser);
-                queryParseContext.parseFieldMatcher(parseFieldMatcher);
-                QueryBuilder<?> filter = queryParseContext.parseInnerQueryBuilder();
-                otherOptions.put(SignificantTermsAggregatorFactory.BACKGROUND_FILTER, filter);
-                return true;
-            }
+        TermsAggregator.BucketCountThresholds bucketCountThresholds = aggParser.getBucketCountThresholds();
+        if (bucketCountThresholds.getShardSize() == aggParser.getDefaultBucketCountThresholds().getShardSize()) {
+            //The user has not made a shardSize selection .
+            //Use default heuristic to avoid any wrong-ranking caused by distributed counting
+            //but request double the usual amount.
+            //We typically need more than the number of "top" terms requested by other aggregations
+            //as the significance algorithm is in less of a position to down-select at shard-level -
+            //some of the things we want to find have only one occurrence on each shard and as
+            // such are impossible to differentiate from non-significant terms at that early stage.
+            bucketCountThresholds.setShardSize(2 * BucketUtils.suggestShardSideQueueSize(bucketCountThresholds.getRequiredSize(), context.numberOfShards()));
         }
-        return false;
-    }
 
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new SignificantTermsAggregatorFactory(null, null);
-    }
-
-    @Override
-    protected BucketCountThresholds getDefaultBucketCountThresholds() {
-        return new TermsAggregator.BucketCountThresholds(SignificantTermsAggregatorFactory.DEFAULT_BUCKET_COUNT_THRESHOLDS);
+        bucketCountThresholds.ensureValidity();
+        SignificanceHeuristic significanceHeuristic = aggParser.getSignificanceHeuristic();
+        if (significanceHeuristic == null) {
+            significanceHeuristic = JLHScore.INSTANCE;
+        }
+        return new SignificantTermsAggregatorFactory(aggregationName, vsParser.config(), bucketCountThresholds, aggParser.getIncludeExclude(), aggParser.getExecutionHint(), aggParser.getFilter(), significanceHeuristic);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java
index bcad058..c7569db 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java
@@ -58,9 +58,9 @@ public class UnmappedSignificantTerms extends InternalSignificantTerms<UnmappedS
     UnmappedSignificantTerms() {} // for serialization
 
     public UnmappedSignificantTerms(String name, int requiredSize, long minDocCount, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) {
-        //We pass zero for index/subset sizes because for the purpose of significant term analysis
-        // we assume an unmapped index's size is irrelevant to the proceedings.
-        super(0, 0, name, requiredSize, minDocCount, JLHScore.PROTOTYPE, BUCKETS, pipelineAggregators, metaData);
+        //We pass zero for index/subset sizes because for the purpose of significant term analysis 
+        // we assume an unmapped index's size is irrelevant to the proceedings. 
+        super(0, 0, name, requiredSize, minDocCount, JLHScore.INSTANCE, BUCKETS, pipelineAggregators, metaData);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java
index c68e47a..cc9303a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java
@@ -23,14 +23,13 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
 public class ChiSquare extends NXYSignificanceHeuristic {
 
-    static final ChiSquare PROTOTYPE = new ChiSquare(false, false);
-
     protected static final ParseField NAMES_FIELD = new ParseField("chi_square");
 
     public ChiSquare(boolean includeNegatives, boolean backgroundIsSuperset) {
@@ -52,6 +51,18 @@ public class ChiSquare extends NXYSignificanceHeuristic {
         return result;
     }
 
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return new ChiSquare(in.readBoolean(), in.readBoolean());
+        }
+
+        @Override
+        public String getName() {
+            return NAMES_FIELD.getPreferredName();
+        }
+    };
+
     /**
      * Calculates Chi^2
      * see "Information Retrieval", Manning et al., Eq. 13.19
@@ -69,21 +80,9 @@ public class ChiSquare extends NXYSignificanceHeuristic {
     }
 
     @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return new ChiSquare(in.readBoolean(), in.readBoolean());
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName());
-        super.build(builder);
-        builder.endObject();
-        return builder;
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
+        super.writeTo(out);
     }
 
     public static class ChiSquareParser extends NXYParser {
@@ -107,7 +106,7 @@ public class ChiSquare extends NXYSignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName());
+            builder.startObject(STREAM.getName());
             super.build(builder);
             builder.endObject();
             return builder;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
index 6da05ed..99ee7c7 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
@@ -29,13 +29,12 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
 public class GND extends NXYSignificanceHeuristic {
 
-    static final GND PROTOTYPE = new GND(false);
-
     protected static final ParseField NAMES_FIELD = new ParseField("gnd");
 
     public GND(boolean backgroundIsSuperset) {
@@ -58,6 +57,18 @@ public class GND extends NXYSignificanceHeuristic {
         return result;
     }
 
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return new GND(in.readBoolean());
+        }
+
+        @Override
+        public String getName() {
+            return NAMES_FIELD.getPreferredName();
+        }
+    };
+
     /**
      * Calculates Google Normalized Distance, as described in "The Google Similarity Distance", Cilibrasi and Vitanyi, 2007
      * link: http://arxiv.org/pdf/cs/0412098v3.pdf
@@ -87,28 +98,11 @@ public class GND extends NXYSignificanceHeuristic {
     }
 
     @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return new GND(in.readBoolean());
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
         out.writeBoolean(backgroundIsSuperset);
     }
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName());
-        builder.field(BACKGROUND_IS_SUPERSET.getPreferredName(), backgroundIsSuperset);
-        builder.endObject();
-        return builder;
-    }
-
     public static class GNDParser extends NXYParser {
 
         @Override
@@ -122,7 +116,7 @@ public class GND extends NXYSignificanceHeuristic {
         }
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             String givenName = parser.currentName();
             boolean backgroundIsSuperset = true;
@@ -149,7 +143,7 @@ public class GND extends NXYSignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName());
+            builder.startObject(STREAM.getName());
             builder.field(BACKGROUND_IS_SUPERSET.getPreferredName(), backgroundIsSuperset);
             builder.endObject();
             return builder;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
index c4327cd..97264e7 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
@@ -22,43 +22,38 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
 public class JLHScore extends SignificanceHeuristic {
 
-    public static final JLHScore PROTOTYPE = new JLHScore();
+    public static final JLHScore INSTANCE = new JLHScore();
 
-    protected static final ParseField NAMES_FIELD = new ParseField("jlh");
+    protected static final String[] NAMES = {"jlh"};
 
-    public JLHScore() {
-    }
+    private JLHScore() {}
 
-    @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return PROTOTYPE;
-    }
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return readFrom(in);
+        }
 
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-    }
+        @Override
+        public String getName() {
+            return NAMES[0];
+        }
+    };
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
-        return builder;
+    public static SignificanceHeuristic readFrom(StreamInput in) throws IOException {
+        return INSTANCE;
     }
 
     /**
@@ -106,21 +101,26 @@ public class JLHScore extends SignificanceHeuristic {
         return absoluteProbabilityChange * relativeProbabilityChange;
     }
 
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
+    }
+
     public static class JLHScoreParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             // move to the closing bracket
             if (!parser.nextToken().equals(XContentParser.Token.END_OBJECT)) {
-                throw new ElasticsearchParseException("failed to parse [jlh] significance heuristic. expected an empty object, but found [{}] instead", parser.currentToken());
+                throw new ElasticsearchParseException("failed to parse [jhl] significance heuristic. expected an empty object, but found [{}] instead", parser.currentToken());
             }
-            return PROTOTYPE;
+            return new JLHScore();
         }
 
         @Override
         public String[] getNames() {
-            return NAMES_FIELD.getAllNamesIncludedDeprecated();
+            return NAMES;
         }
     }
 
@@ -128,7 +128,7 @@ public class JLHScore extends SignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
+            builder.startObject(STREAM.getName()).endObject();
             return builder;
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java
index d20b5f3..b4529b8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java
@@ -23,14 +23,13 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
 public class MutualInformation extends NXYSignificanceHeuristic {
 
-    static final MutualInformation PROTOTYPE = new MutualInformation(false, false);
-
     protected static final ParseField NAMES_FIELD = new ParseField("mutual_information");
 
     private static final double log2 = Math.log(2.0);
@@ -54,6 +53,18 @@ public class MutualInformation extends NXYSignificanceHeuristic {
         return result;
     }
 
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return new MutualInformation(in.readBoolean(), in.readBoolean());
+        }
+
+        @Override
+        public String getName() {
+            return NAMES_FIELD.getPreferredName();
+        }
+    };
+
     /**
      * Calculates mutual information
      * see "Information Retrieval", Manning et al., Eq. 13.17
@@ -102,21 +113,9 @@ public class MutualInformation extends NXYSignificanceHeuristic {
     }
 
     @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return new MutualInformation(in.readBoolean(), in.readBoolean());
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName());
-        super.build(builder);
-        builder.endObject();
-        return builder;
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
+        super.writeTo(out);
     }
 
     public static class MutualInformationParser extends NXYParser {
@@ -140,7 +139,7 @@ public class MutualInformation extends NXYSignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName());
+            builder.startObject(STREAM.getName());
             super.build(builder);
             builder.endObject();
             return builder;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
index d3f98f2..c6a6924 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
@@ -28,6 +28,7 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -135,15 +136,10 @@ public abstract class NXYSignificanceHeuristic extends SignificanceHeuristic {
         }
     }
 
-    protected void build(XContentBuilder builder) throws IOException {
-        builder.field(INCLUDE_NEGATIVES_FIELD.getPreferredName(), includeNegatives).field(BACKGROUND_IS_SUPERSET.getPreferredName(),
-                backgroundIsSuperset);
-    }
-
     public static abstract class NXYParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             String givenName = parser.currentName();
             boolean includeNegatives = false;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
index 648c126..aceae8c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
@@ -22,43 +22,38 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
 public class PercentageScore extends SignificanceHeuristic {
 
-    public static final PercentageScore PROTOTYPE = new PercentageScore();
+    public static final PercentageScore INSTANCE = new PercentageScore();
 
-    protected static final ParseField NAMES_FIELD = new ParseField("percentage");
+    protected static final String[] NAMES = {"percentage"};
 
-    public PercentageScore() {
-    }
+    private PercentageScore() {}
 
-    @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return PROTOTYPE;
-    }
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return readFrom(in);
+        }
 
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-    }
+        @Override
+        public String getName() {
+            return NAMES[0];
+        }
+    };
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
-        return builder;
+    public static SignificanceHeuristic readFrom(StreamInput in) throws IOException {
+        return INSTANCE;
     }
 
     /**
@@ -75,21 +70,26 @@ public class PercentageScore extends SignificanceHeuristic {
         return (double) subsetFreq / (double) supersetFreq;
    }
 
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
+    }
+
     public static class PercentageScoreParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             // move to the closing bracket
             if (!parser.nextToken().equals(XContentParser.Token.END_OBJECT)) {
                 throw new ElasticsearchParseException("failed to parse [percentage] significance heuristic. expected an empty object, but got [{}] instead", parser.currentToken());
             }
-            return PROTOTYPE;
+            return new PercentageScore();
         }
 
         @Override
         public String[] getNames() {
-            return NAMES_FIELD.getAllNamesIncludedDeprecated();
+            return NAMES;
         }
     }
 
@@ -97,7 +97,7 @@ public class PercentageScore extends SignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
+            builder.startObject(STREAM.getName()).endObject();
             return builder;
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
index 2956a2d..9efea00 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
@@ -22,7 +22,6 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.HasContextAndHeaders;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -45,12 +44,9 @@ import java.io.IOException;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
-import java.util.Objects;
 
 public class ScriptHeuristic extends SignificanceHeuristic {
 
-    static final ScriptHeuristic PROTOTYPE = new ScriptHeuristic(null);
-
     protected static final ParseField NAMES_FIELD = new ParseField("script_heuristic");
     private final LongAccessor subsetSizeHolder;
     private final LongAccessor supersetSizeHolder;
@@ -59,11 +55,31 @@ public class ScriptHeuristic extends SignificanceHeuristic {
     ExecutableScript searchScript = null;
     Script script;
 
-    public ScriptHeuristic(Script script) {
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            Script script = Script.readScript(in);
+            return new ScriptHeuristic(null, script);
+        }
+
+        @Override
+        public String getName() {
+            return NAMES_FIELD.getPreferredName();
+        }
+    };
+
+    public ScriptHeuristic(ExecutableScript searchScript, Script script) {
         subsetSizeHolder = new LongAccessor();
         supersetSizeHolder = new LongAccessor();
         subsetDfHolder = new LongAccessor();
         supersetDfHolder = new LongAccessor();
+        this.searchScript = searchScript;
+        if (searchScript != null) {
+            searchScript.setNextVar("_subset_freq", subsetDfHolder);
+            searchScript.setNextVar("_subset_size", subsetSizeHolder);
+            searchScript.setNextVar("_superset_freq", supersetDfHolder);
+            searchScript.setNextVar("_superset_size", supersetSizeHolder);
+        }
         this.script = script;
 
 
@@ -71,16 +87,7 @@ public class ScriptHeuristic extends SignificanceHeuristic {
 
     @Override
     public void initialize(InternalAggregation.ReduceContext context) {
-        initialize(context.scriptService(), context);
-    }
-
-    @Override
-    public void initialize(SearchContext context) {
-        initialize(context.scriptService(), context);
-    }
-
-    public void initialize(ScriptService scriptService, HasContextAndHeaders hasContextAndHeaders) {
-        searchScript = scriptService.executable(script, ScriptContext.Standard.AGGS, hasContextAndHeaders, Collections.emptyMap());
+        searchScript = context.scriptService().executable(script, ScriptContext.Standard.AGGS, context, Collections.emptyMap());
         searchScript.setNextVar("_subset_freq", subsetDfHolder);
         searchScript.setNextVar("_subset_size", subsetSizeHolder);
         searchScript.setNextVar("_superset_freq", supersetDfHolder);
@@ -114,54 +121,20 @@ public class ScriptHeuristic extends SignificanceHeuristic {
     }
 
     @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        Script script = Script.readScript(in);
-        return new ScriptHeuristic(script);
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
         script.writeTo(out);
     }
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params builderParams) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName());
-        builder.field(ScriptField.SCRIPT.getPreferredName());
-        script.toXContent(builder, builderParams);
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(script);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        ScriptHeuristic other = (ScriptHeuristic) obj;
-        return Objects.equals(script, other.script);
-    }
-
     public static class ScriptHeuristicParser implements SignificanceHeuristicParser {
+        private final ScriptService scriptService;
 
-        public ScriptHeuristicParser() {
+        public ScriptHeuristicParser(ScriptService scriptService) {
+            this.scriptService = scriptService;
         }
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             String heuristicName = parser.currentName();
             Script script = null;
@@ -200,7 +173,13 @@ public class ScriptHeuristic extends SignificanceHeuristic {
             if (script == null) {
                 throw new ElasticsearchParseException("failed to parse [{}] significance heuristic. no script found in script_heuristic", heuristicName);
             }
-            return new ScriptHeuristic(script);
+            ExecutableScript searchScript;
+            try {
+                searchScript = scriptService.executable(script, ScriptContext.Standard.AGGS, context, Collections.emptyMap());
+            } catch (Exception e) {
+                throw new ElasticsearchParseException("failed to parse [{}] significance heuristic. the script [{}] could not be loaded", e, script, heuristicName);
+            }
+            return new ScriptHeuristic(searchScript, script);
         }
 
         @Override
@@ -220,7 +199,7 @@ public class ScriptHeuristic extends SignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params builderParams) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName());
+            builder.startObject(STREAM.getName());
             builder.field(ScriptField.SCRIPT.getPreferredName());
             script.toXContent(builder, builderParams);
             builder.endObject();
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java
index 972696b..4f12277 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java
@@ -20,12 +20,12 @@
 package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 
-import org.elasticsearch.common.io.stream.NamedWriteable;
-import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.internal.SearchContext;
 
-public abstract class SignificanceHeuristic implements NamedWriteable<SignificanceHeuristic>, ToXContent {
+import java.io.IOException;
+
+public abstract class SignificanceHeuristic {
     /**
      * @param subsetFreq   The frequency of the term in the selected sample
      * @param subsetSize   The size of the selected sample (typically number of docs)
@@ -35,6 +35,8 @@ public abstract class SignificanceHeuristic implements NamedWriteable<Significan
      */
     public abstract double getScore(long subsetFreq, long subsetSize, long supersetFreq, long supersetSize);
 
+    abstract public void writeTo(StreamOutput out) throws IOException;
+
     protected void checkFrequencyValidity(long subsetFreq, long subsetSize, long supersetFreq, long supersetSize, String scoreFunctionName) {
         if (subsetFreq < 0 || subsetSize < 0 || supersetFreq < 0 || supersetSize < 0) {
             throw new IllegalArgumentException("Frequencies of subset and superset must be positive in " + scoreFunctionName + ".getScore()");
@@ -50,8 +52,4 @@ public abstract class SignificanceHeuristic implements NamedWriteable<Significan
     public void initialize(InternalAggregation.ReduceContext reduceContext) {
 
     }
-
-    public void initialize(SearchContext context) {
-
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java
index aa430dc..92baa43 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java
@@ -23,12 +23,13 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
 public interface SignificanceHeuristicParser {
 
-    SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException,
+    SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context) throws IOException,
             ParsingException;
 
     String[] getNames();
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParserMapper.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParserMapper.java
index a9b385c..07e74ec 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParserMapper.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParserMapper.java
@@ -21,6 +21,8 @@
 package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.script.ScriptService;
+
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
@@ -31,14 +33,14 @@ public class SignificanceHeuristicParserMapper {
     protected final Map<String, SignificanceHeuristicParser> significanceHeuristicParsers;
 
     @Inject
-    public SignificanceHeuristicParserMapper(Set<SignificanceHeuristicParser> parsers) {
+    public SignificanceHeuristicParserMapper(Set<SignificanceHeuristicParser> parsers, ScriptService scriptService) {
         Map<String, SignificanceHeuristicParser> map = new HashMap<>();
         add(map, new JLHScore.JLHScoreParser());
         add(map, new PercentageScore.PercentageScoreParser());
         add(map, new MutualInformation.MutualInformationParser());
         add(map, new ChiSquare.ChiSquareParser());
         add(map, new GND.GNDParser());
-        add(map, new ScriptHeuristic.ScriptHeuristicParser());
+        add(map, new ScriptHeuristic.ScriptHeuristicParser(scriptService));
         for (SignificanceHeuristicParser parser : parsers) {
             add(map, parser);
         }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicStreams.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicStreams.java
index 2ffe5ec..198f129 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicStreams.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicStreams.java
@@ -19,7 +19,6 @@
 package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 
 import java.io.IOException;
 import java.util.Collections;
@@ -32,26 +31,21 @@ import java.util.Map;
  */
 public class SignificanceHeuristicStreams {
 
-    private static Map<String, SignificanceHeuristic> STREAMS = Collections.emptyMap();
+    private static Map<String, Stream> STREAMS = Collections.emptyMap();
 
     static {
-        HashMap<String, SignificanceHeuristic> map = new HashMap<>();
-        map.put(JLHScore.NAMES_FIELD.getPreferredName(), JLHScore.PROTOTYPE);
-        map.put(PercentageScore.NAMES_FIELD.getPreferredName(), PercentageScore.PROTOTYPE);
-        map.put(MutualInformation.NAMES_FIELD.getPreferredName(), MutualInformation.PROTOTYPE);
-        map.put(GND.NAMES_FIELD.getPreferredName(), GND.PROTOTYPE);
-        map.put(ChiSquare.NAMES_FIELD.getPreferredName(), ChiSquare.PROTOTYPE);
-        map.put(ScriptHeuristic.NAMES_FIELD.getPreferredName(), ScriptHeuristic.PROTOTYPE);
+        HashMap<String, Stream> map = new HashMap<>();
+        map.put(JLHScore.STREAM.getName(), JLHScore.STREAM);
+        map.put(PercentageScore.STREAM.getName(), PercentageScore.STREAM);
+        map.put(MutualInformation.STREAM.getName(), MutualInformation.STREAM);
+        map.put(GND.STREAM.getName(), GND.STREAM);
+        map.put(ChiSquare.STREAM.getName(), ChiSquare.STREAM);
+        map.put(ScriptHeuristic.STREAM.getName(), ScriptHeuristic.STREAM);
         STREAMS = Collections.unmodifiableMap(map);
     }
 
     public static SignificanceHeuristic read(StreamInput in) throws IOException {
-        return stream(in.readString()).readFrom(in);
-    }
-
-    public static void writeTo(SignificanceHeuristic significanceHeuristic, StreamOutput out) throws IOException {
-        out.writeString(significanceHeuristic.getWriteableName());
-        significanceHeuristic.writeTo(out);
+        return stream(in.readString()).readResult(in);
     }
 
     /**
@@ -65,18 +59,17 @@ public class SignificanceHeuristicStreams {
     }
 
     /**
-     * Registers the given prototype.
+     * Registers the given stream and associate it with the given types.
      *
-     * @param prototype
-     *            The prototype to register
+     * @param stream The stream to register
      */
-    public static synchronized void registerPrototype(SignificanceHeuristic prototype) {
-        if (STREAMS.containsKey(prototype.getWriteableName())) {
-            throw new IllegalArgumentException("Can't register stream with name [" + prototype.getWriteableName() + "] more than once");
+    public static synchronized void registerStream(Stream stream) {
+        if (STREAMS.containsKey(stream.getName())) {
+            throw new IllegalArgumentException("Can't register stream with name [" + stream.getName() + "] more than once");
         }
-        HashMap<String, SignificanceHeuristic> map = new HashMap<>();
+        HashMap<String, Stream> map = new HashMap<>();
         map.putAll(STREAMS);
-        map.put(prototype.getWriteableName(), prototype);
+        map.put(stream.getName(), stream);
         STREAMS = Collections.unmodifiableMap(map);
     }
 
@@ -86,7 +79,7 @@ public class SignificanceHeuristicStreams {
      * @param name The given name
      * @return The associated stream
      */
-    private static synchronized SignificanceHeuristic stream(String name) {
+    private static synchronized Stream stream(String name) {
         return STREAMS.get(name);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java
new file mode 100644
index 0000000..891526c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java
@@ -0,0 +1,111 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.search.aggregations.bucket.terms;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
+import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+
+public abstract class AbstractTermsParametersParser {
+
+    public static final ParseField EXECUTION_HINT_FIELD_NAME = new ParseField("execution_hint");
+    public static final ParseField SHARD_SIZE_FIELD_NAME = new ParseField("shard_size");
+    public static final ParseField MIN_DOC_COUNT_FIELD_NAME = new ParseField("min_doc_count");
+    public static final ParseField SHARD_MIN_DOC_COUNT_FIELD_NAME = new ParseField("shard_min_doc_count");
+    public static final ParseField REQUIRED_SIZE_FIELD_NAME = new ParseField("size");
+    public static final ParseField SHOW_TERM_DOC_COUNT_ERROR = new ParseField("show_term_doc_count_error");
+    
+
+    //These are the results of the parsing.
+    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new TermsAggregator.BucketCountThresholds();
+
+    private String executionHint = null;
+    
+    private SubAggCollectionMode collectMode = SubAggCollectionMode.DEPTH_FIRST;
+
+
+    IncludeExclude includeExclude;
+
+    public TermsAggregator.BucketCountThresholds getBucketCountThresholds() {return bucketCountThresholds;}
+
+    //These are the results of the parsing.
+
+    public String getExecutionHint() {
+        return executionHint;
+    }
+
+    public IncludeExclude getIncludeExclude() {
+        return includeExclude;
+    }
+    
+    public SubAggCollectionMode getCollectionMode() {
+        return collectMode;
+    }
+
+    public void parse(String aggregationName, XContentParser parser, SearchContext context, ValuesSourceParser vsParser, IncludeExclude.Parser incExcParser) throws IOException {
+        bucketCountThresholds = getDefaultBucketCountThresholds();
+        XContentParser.Token token;
+        String currentFieldName = null;
+
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (incExcParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_STRING) {
+                if (context.parseFieldMatcher().match(currentFieldName, EXECUTION_HINT_FIELD_NAME)) {
+                    executionHint = parser.text();
+                } else if(context.parseFieldMatcher().match(currentFieldName, SubAggCollectionMode.KEY)){
+                    collectMode = SubAggCollectionMode.parse(parser.text(), context.parseFieldMatcher());
+                } else if (context.parseFieldMatcher().match(currentFieldName, REQUIRED_SIZE_FIELD_NAME)) {
+                    bucketCountThresholds.setRequiredSize(parser.intValue());
+                } else {
+                    parseSpecial(aggregationName, parser, context, token, currentFieldName);
+                }
+            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                if (context.parseFieldMatcher().match(currentFieldName, REQUIRED_SIZE_FIELD_NAME)) {
+                    bucketCountThresholds.setRequiredSize(parser.intValue());
+                } else if (context.parseFieldMatcher().match(currentFieldName, SHARD_SIZE_FIELD_NAME)) {
+                    bucketCountThresholds.setShardSize(parser.intValue());
+                } else if (context.parseFieldMatcher().match(currentFieldName, MIN_DOC_COUNT_FIELD_NAME)) {
+                    bucketCountThresholds.setMinDocCount(parser.intValue());
+                } else if (context.parseFieldMatcher().match(currentFieldName, SHARD_MIN_DOC_COUNT_FIELD_NAME)) {
+                    bucketCountThresholds.setShardMinDocCount(parser.longValue());
+                } else {
+                    parseSpecial(aggregationName, parser, context, token, currentFieldName);
+                }
+            } else {
+                parseSpecial(aggregationName, parser, context, token, currentFieldName);
+            }
+        }
+        includeExclude = incExcParser.includeExclude();
+    }
+
+    public abstract void parseSpecial(String aggregationName, XContentParser parser, SearchContext context, XContentParser.Token token, String currentFieldName) throws IOException;
+
+    protected abstract TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds();
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParser.java
deleted file mode 100644
index 5097dd7..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParser.java
+++ /dev/null
@@ -1,130 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.terms;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
-
-public abstract class AbstractTermsParser extends AnyValuesSourceParser {
-
-    public static final ParseField EXECUTION_HINT_FIELD_NAME = new ParseField("execution_hint");
-    public static final ParseField SHARD_SIZE_FIELD_NAME = new ParseField("shard_size");
-    public static final ParseField MIN_DOC_COUNT_FIELD_NAME = new ParseField("min_doc_count");
-    public static final ParseField SHARD_MIN_DOC_COUNT_FIELD_NAME = new ParseField("shard_min_doc_count");
-    public static final ParseField REQUIRED_SIZE_FIELD_NAME = new ParseField("size");
-
-    public IncludeExclude.Parser incExcParser = new IncludeExclude.Parser();
-
-    protected AbstractTermsParser() {
-        super(true, true);
-    }
-
-    @Override
-    protected final ValuesSourceAggregatorFactory<ValuesSource, ?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        BucketCountThresholds bucketCountThresholds = getDefaultBucketCountThresholds();
-        Integer requiredSize = (Integer) otherOptions.get(REQUIRED_SIZE_FIELD_NAME);
-        if (requiredSize != null && requiredSize != -1) {
-            bucketCountThresholds.setRequiredSize(requiredSize);
-        }
-        Integer shardSize = (Integer) otherOptions.get(SHARD_SIZE_FIELD_NAME);
-        if (shardSize != null && shardSize != -1) {
-            bucketCountThresholds.setShardSize(shardSize);
-        }
-        Long minDocCount = (Long) otherOptions.get(MIN_DOC_COUNT_FIELD_NAME);
-        if (minDocCount != null && minDocCount != -1) {
-            bucketCountThresholds.setMinDocCount(minDocCount);
-        }
-        Long shardMinDocCount = (Long) otherOptions.get(SHARD_MIN_DOC_COUNT_FIELD_NAME);
-        if (shardMinDocCount != null && shardMinDocCount != -1) {
-            bucketCountThresholds.setShardMinDocCount(shardMinDocCount);
-        }
-        SubAggCollectionMode collectMode = (SubAggCollectionMode) otherOptions.get(SubAggCollectionMode.KEY);
-        String executionHint = (String) otherOptions.get(EXECUTION_HINT_FIELD_NAME);
-        IncludeExclude incExc = incExcParser.createIncludeExclude(otherOptions);
-        return doCreateFactory(aggregationName, valuesSourceType, targetValueType, bucketCountThresholds, collectMode, executionHint,
-                incExc,
-                otherOptions);
-    }
-
-    protected abstract ValuesSourceAggregatorFactory<ValuesSource, ?> doCreateFactory(String aggregationName,
-            ValuesSourceType valuesSourceType,
-            ValueType targetValueType, BucketCountThresholds bucketCountThresholds, SubAggCollectionMode collectMode, String executionHint,
-            IncludeExclude incExc, Map<ParseField, Object> otherOptions);
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (incExcParser.token(currentFieldName, token, parser, parseFieldMatcher, otherOptions)) {
-            return true;
-        } else if (token == XContentParser.Token.VALUE_STRING) {
-            if (parseFieldMatcher.match(currentFieldName, EXECUTION_HINT_FIELD_NAME)) {
-                otherOptions.put(EXECUTION_HINT_FIELD_NAME, parser.text());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SubAggCollectionMode.KEY)) {
-                otherOptions.put(SubAggCollectionMode.KEY, SubAggCollectionMode.parse(parser.text(), parseFieldMatcher));
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, REQUIRED_SIZE_FIELD_NAME)) {
-                otherOptions.put(REQUIRED_SIZE_FIELD_NAME, parser.intValue());
-                return true;
-            } else if (parseSpecial(aggregationName, parser, parseFieldMatcher, token, currentFieldName, otherOptions)) {
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_NUMBER) {
-            if (parseFieldMatcher.match(currentFieldName, REQUIRED_SIZE_FIELD_NAME)) {
-                otherOptions.put(REQUIRED_SIZE_FIELD_NAME, parser.intValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SHARD_SIZE_FIELD_NAME)) {
-                otherOptions.put(SHARD_SIZE_FIELD_NAME, parser.intValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, MIN_DOC_COUNT_FIELD_NAME)) {
-                otherOptions.put(MIN_DOC_COUNT_FIELD_NAME, parser.longValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SHARD_MIN_DOC_COUNT_FIELD_NAME)) {
-                otherOptions.put(SHARD_MIN_DOC_COUNT_FIELD_NAME, parser.longValue());
-                return true;
-            } else if (parseSpecial(aggregationName, parser, parseFieldMatcher, token, currentFieldName, otherOptions)) {
-                return true;
-            }
-        } else if (parseSpecial(aggregationName, parser, parseFieldMatcher, token, currentFieldName, otherOptions)) {
-            return true;
-        }
-        return false;
-    }
-
-    public abstract boolean parseSpecial(String aggregationName, XContentParser parser, ParseFieldMatcher parseFieldMatcher,
-            XContentParser.Token token, String currentFieldName, Map<ParseField, Object> otherOptions) throws IOException;
-
-    protected abstract TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds();
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/GlobalOrdinalsStringTermsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/GlobalOrdinalsStringTermsAggregator.java
index 91e949e..1e7a004 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/GlobalOrdinalsStringTermsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/GlobalOrdinalsStringTermsAggregator.java
@@ -347,7 +347,7 @@ public class GlobalOrdinalsStringTermsAggregator extends AbstractStringTermsAggr
                 Map<String, Object> metaData) throws IOException {
             super(name, factories, valuesSource, order, bucketCountThresholds, null, aggregationContext, parent, collectionMode,
                     showTermDocCountError, pipelineAggregators, metaData);
-            assert factories == null || factories.countAggregators() == 0;
+            assert factories == null || factories.count() == 0;
             this.segmentDocCounts = context.bigArrays().newIntArray(1, true);
         }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java
index f3f87c0..4e3e28a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java
@@ -38,7 +38,6 @@ import java.util.Comparator;
 import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.List;
-import java.util.Objects;
 
 /**
  *
@@ -96,7 +95,7 @@ class InternalOrder extends Terms.Order {
     public static boolean isCountDesc(Terms.Order order) {
         if (order == COUNT_DESC) {
             return true;
-        } else if (order instanceof CompoundOrder) {
+        }else if (order instanceof CompoundOrder) {
             // check if its a compound order with count desc and the tie breaker (term asc)
             CompoundOrder compoundOrder = (CompoundOrder) order;
             if (compoundOrder.orderElements.size() == 2 && compoundOrder.orderElements.get(0) == COUNT_DESC && compoundOrder.orderElements.get(1) == TERM_ASC) {
@@ -106,23 +105,6 @@ class InternalOrder extends Terms.Order {
         return false;
     }
 
-    public static boolean isTermOrder(Terms.Order order) {
-        if (order == TERM_ASC) {
-            return true;
-        } else if (order == TERM_DESC) {
-            return true;
-        } else if (order instanceof CompoundOrder) {
-            // check if its a compound order with only a single element ordering
-            // by term
-            CompoundOrder compoundOrder = (CompoundOrder) order;
-            if (compoundOrder.orderElements.size() == 1 && compoundOrder.orderElements.get(0) == TERM_ASC
-                    || compoundOrder.orderElements.get(0) == TERM_DESC) {
-                return true;
-            }
-        }
-        return false;
-    }
-
     final byte id;
     final String key;
     final boolean asc;
@@ -281,23 +263,6 @@ class InternalOrder extends Terms.Order {
             return new CompoundOrderComparator(orderElements, aggregator);
         }
 
-        @Override
-        public int hashCode() {
-            return Objects.hash(orderElements);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            CompoundOrder other = (CompoundOrder) obj;
-            return Objects.equals(orderElements, other.orderElements);
-        }
-
         public static class CompoundOrderComparator implements Comparator<Terms.Bucket> {
 
             private List<Terms.Order> compoundOrder;
@@ -341,7 +306,7 @@ class InternalOrder extends Terms.Order {
         }
 
         public static Terms.Order readOrder(StreamInput in) throws IOException {
-            return readOrder(in, false);
+            return readOrder(in, true);
         }
 
         public static Terms.Order readOrder(StreamInput in, boolean absoluteOrder) throws IOException {
@@ -367,22 +332,4 @@ class InternalOrder extends Terms.Order {
             }
         }
     }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(id, asc);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        InternalOrder other = (InternalOrder) obj;
-        return Objects.equals(id, other.id)
-                && Objects.equals(asc, other.asc);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java
index 6e5aaac..31285d2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java
@@ -188,7 +188,7 @@ public abstract class InternalTerms<A extends InternalTerms, B extends InternalT
             }
             otherDocCount += terms.getSumOfOtherDocCounts();
             final long thisAggDocCountError;
-            if (terms.buckets.size() < this.shardSize || InternalOrder.isTermOrder(order)) {
+            if (terms.buckets.size() < this.shardSize || this.order == InternalOrder.TERM_ASC || this.order == InternalOrder.TERM_DESC) {
                 thisAggDocCountError = 0;
             } else if (InternalOrder.isCountDesc(this.order)) {
                 thisAggDocCountError = terms.buckets.get(terms.buckets.size() - 1).docCount;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java
index 16fb7f1..224e42c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java
@@ -82,7 +82,7 @@ public interface Terms extends MultiBucketsAggregation {
      * Get the bucket for the given term, or null if there is no such bucket.
      */
     Bucket getBucketByKey(String term);
-
+    
     /**
      * Get an upper bound of the error on document counts in this aggregation.
      */
@@ -166,11 +166,5 @@ public interface Terms extends MultiBucketsAggregation {
 
         abstract byte id();
 
-        @Override
-        public abstract int hashCode();
-
-        @Override
-        public abstract boolean equals(Object obj);
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java
index 7ea88c9..7971d1f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java
@@ -21,10 +21,7 @@
 package org.elasticsearch.search.aggregations.bucket.terms;
 
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.Explicit;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
@@ -39,136 +36,99 @@ import java.io.IOException;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.Set;
 
 public abstract class TermsAggregator extends BucketsAggregator {
 
-    public static class BucketCountThresholds implements Writeable<BucketCountThresholds>, ToXContent {
-
-        private static final BucketCountThresholds PROTOTYPE = new BucketCountThresholds(-1, -1, -1, -1);
-
-        private long minDocCount;
-        private long shardMinDocCount;
-        private int requiredSize;
-        private int shardSize;
-
-        public static BucketCountThresholds readFromStream(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
-        }
+    public static class BucketCountThresholds {
+        private Explicit<Long> minDocCount;
+        private Explicit<Long> shardMinDocCount;
+        private Explicit<Integer> requiredSize;
+        private Explicit<Integer> shardSize;
 
         public BucketCountThresholds(long minDocCount, long shardMinDocCount, int requiredSize, int shardSize) {
-            this.minDocCount = minDocCount;
-            this.shardMinDocCount = shardMinDocCount;
-            this.requiredSize = requiredSize;
-            this.shardSize = shardSize;
+            this.minDocCount = new Explicit<>(minDocCount, false);
+            this.shardMinDocCount =  new Explicit<>(shardMinDocCount, false);
+            this.requiredSize = new Explicit<>(requiredSize, false);
+            this.shardSize = new Explicit<>(shardSize, false);
+        }
+        public BucketCountThresholds() {
+            this(-1, -1, -1, -1);
         }
 
         public BucketCountThresholds(BucketCountThresholds bucketCountThresholds) {
-            this(bucketCountThresholds.minDocCount, bucketCountThresholds.shardMinDocCount, bucketCountThresholds.requiredSize,
-                    bucketCountThresholds.shardSize);
+            this(bucketCountThresholds.minDocCount.value(), bucketCountThresholds.shardMinDocCount.value(), bucketCountThresholds.requiredSize.value(), bucketCountThresholds.shardSize.value());
         }
 
         public void ensureValidity() {
 
-            if (shardSize == 0) {
+            if (shardSize.value() == 0) {
                 setShardSize(Integer.MAX_VALUE);
             }
 
-            if (requiredSize == 0) {
+            if (requiredSize.value() == 0) {
                 setRequiredSize(Integer.MAX_VALUE);
             }
             // shard_size cannot be smaller than size as we need to at least fetch <size> entries from every shards in order to return <size>
-            if (shardSize < requiredSize) {
-                setShardSize(requiredSize);
+            if (shardSize.value() < requiredSize.value()) {
+                setShardSize(requiredSize.value());
             }
 
             // shard_min_doc_count should not be larger than min_doc_count because this can cause buckets to be removed that would match the min_doc_count criteria
-            if (shardMinDocCount > minDocCount) {
-                setShardMinDocCount(minDocCount);
+            if (shardMinDocCount.value() > minDocCount.value()) {
+                setShardMinDocCount(minDocCount.value());
             }
 
-            if (requiredSize < 0 || minDocCount < 0) {
+            if (requiredSize.value() < 0 || minDocCount.value() < 0) {
                 throw new ElasticsearchException("parameters [requiredSize] and [minDocCount] must be >=0 in terms aggregation.");
             }
         }
 
         public long getShardMinDocCount() {
-            return shardMinDocCount;
+            return shardMinDocCount.value();
         }
 
         public void setShardMinDocCount(long shardMinDocCount) {
-            this.shardMinDocCount = shardMinDocCount;
+            this.shardMinDocCount = new Explicit<>(shardMinDocCount, true);
         }
 
         public long getMinDocCount() {
-            return minDocCount;
+            return minDocCount.value();
         }
 
         public void setMinDocCount(long minDocCount) {
-            this.minDocCount = minDocCount;
+            this.minDocCount = new Explicit<>(minDocCount, true);
         }
 
         public int getRequiredSize() {
-            return requiredSize;
+            return requiredSize.value();
         }
 
         public void setRequiredSize(int requiredSize) {
-            this.requiredSize = requiredSize;
+            this.requiredSize = new Explicit<>(requiredSize, true);
         }
 
         public int getShardSize() {
-            return shardSize;
+            return shardSize.value();
         }
 
         public void setShardSize(int shardSize) {
-            this.shardSize = shardSize;
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(TermsAggregatorFactory.REQUIRED_SIZE_FIELD_NAME.getPreferredName(), requiredSize);
-            builder.field(TermsAggregatorFactory.SHARD_SIZE_FIELD_NAME.getPreferredName(), shardSize);
-            builder.field(TermsAggregatorFactory.MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), minDocCount);
-            builder.field(TermsAggregatorFactory.SHARD_MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), shardMinDocCount);
-            return builder;
+            this.shardSize = new Explicit<>(shardSize, true);
         }
 
-        @Override
-        public BucketCountThresholds readFrom(StreamInput in) throws IOException {
-            int requiredSize = in.readInt();
-            int shardSize = in.readInt();
-            long minDocCount = in.readLong();
-            long shardMinDocCount = in.readLong();
-            return new BucketCountThresholds(minDocCount, shardMinDocCount, requiredSize, shardSize);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeInt(requiredSize);
-            out.writeInt(shardSize);
-            out.writeLong(minDocCount);
-            out.writeLong(shardMinDocCount);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(requiredSize, shardSize, minDocCount, shardMinDocCount);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
+        public void toXContent(XContentBuilder builder) throws IOException {
+            if (requiredSize.explicit()) {
+                builder.field(AbstractTermsParametersParser.REQUIRED_SIZE_FIELD_NAME.getPreferredName(), requiredSize.value());
+            }
+            if (shardSize.explicit()) {
+                builder.field(AbstractTermsParametersParser.SHARD_SIZE_FIELD_NAME.getPreferredName(), shardSize.value());
+            }
+            if (minDocCount.explicit()) {
+                builder.field(AbstractTermsParametersParser.MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), minDocCount.value());
             }
-            if (getClass() != obj.getClass()) {
-                return false;
+            if (shardMinDocCount.explicit()) {
+                builder.field(AbstractTermsParametersParser.SHARD_MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), shardMinDocCount.value());
             }
-            BucketCountThresholds other = (BucketCountThresholds) obj;
-            return Objects.equals(requiredSize, other.requiredSize)
-                    && Objects.equals(shardSize, other.shardSize)
-                    && Objects.equals(minDocCount, other.minDocCount)
-                    && Objects.equals(shardMinDocCount, other.shardMinDocCount);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java
index 37e5046..270dc00 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java
@@ -21,49 +21,27 @@ package org.elasticsearch.search.aggregations.bucket.terms;
 import org.apache.lucene.search.IndexSearcher;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.bucket.BucketUtils;
-import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
-/**
- *
- */
-public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource, TermsAggregatorFactory> {
-
-    public static final ParseField EXECUTION_HINT_FIELD_NAME = new ParseField("execution_hint");
-    public static final ParseField SHARD_SIZE_FIELD_NAME = new ParseField("shard_size");
-    public static final ParseField MIN_DOC_COUNT_FIELD_NAME = new ParseField("min_doc_count");
-    public static final ParseField SHARD_MIN_DOC_COUNT_FIELD_NAME = new ParseField("shard_min_doc_count");
-    public static final ParseField REQUIRED_SIZE_FIELD_NAME = new ParseField("size");
-
-    static final TermsAggregator.BucketCountThresholds DEFAULT_BUCKET_COUNT_THRESHOLDS = new TermsAggregator.BucketCountThresholds(1, 0, 10,
-            -1);
-    public static final ParseField SHOW_TERM_DOC_COUNT_ERROR = new ParseField("show_term_doc_count_error");
-    public static final ParseField ORDER_FIELD = new ParseField("order");
+public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource> {
 
     public enum ExecutionMode {
 
@@ -133,7 +111,7 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
                     AggregationContext aggregationContext, Aggregator parent, SubAggCollectionMode subAggCollectMode,
                     boolean showTermDocCountError, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
                     throws IOException {
-                if (includeExclude != null || factories.countAggregators() > 0
+                if (includeExclude != null || factories.count() > 0
                         // we need the FieldData impl to be able to extract the
                         // segment to global ord mapping
                         || valuesSource.getClass() != ValuesSource.Bytes.FieldData.class) {
@@ -180,146 +158,23 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
         }
     }
 
-    private Terms.Order order = Terms.Order.compound(Terms.Order.count(false), Terms.Order.term(true));
-    private IncludeExclude includeExclude = null;
-    private String executionHint = null;
-    private SubAggCollectionMode collectMode = SubAggCollectionMode.DEPTH_FIRST;
-    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new TermsAggregator.BucketCountThresholds(
-            DEFAULT_BUCKET_COUNT_THRESHOLDS);
-    private boolean showTermDocCountError = false;
-
-    public TermsAggregatorFactory(String name, ValueType valueType) {
-        super(name, StringTerms.TYPE, ValuesSourceType.ANY, valueType);
-    }
-
-    public TermsAggregator.BucketCountThresholds bucketCountThresholds() {
-        return bucketCountThresholds;
-    }
-
-    public TermsAggregatorFactory bucketCountThresholds(TermsAggregator.BucketCountThresholds bucketCountThresholds) {
-        this.bucketCountThresholds = bucketCountThresholds;
-        return this;
-    }
-
-    /**
-     * Sets the size - indicating how many term buckets should be returned
-     * (defaults to 10)
-     */
-    public TermsAggregatorFactory size(int size) {
-        bucketCountThresholds.setRequiredSize(size);
-        return this;
-    }
-
-    /**
-     * Sets the shard_size - indicating the number of term buckets each shard
-     * will return to the coordinating node (the node that coordinates the
-     * search execution). The higher the shard size is, the more accurate the
-     * results are.
-     */
-    public TermsAggregatorFactory shardSize(int shardSize) {
-        bucketCountThresholds.setShardSize(shardSize);
-        return this;
-    }
-
-    /**
-     * Set the minimum document count terms should have in order to appear in
-     * the response.
-     */
-    public TermsAggregatorFactory minDocCount(long minDocCount) {
-        bucketCountThresholds.setMinDocCount(minDocCount);
-        return this;
-    }
-
-    /**
-     * Set the minimum document count terms should have on the shard in order to
-     * appear in the response.
-     */
-    public TermsAggregatorFactory shardMinDocCount(long shardMinDocCount) {
-        bucketCountThresholds.setShardMinDocCount(shardMinDocCount);
-        return this;
-    }
-
-    /**
-     * Sets the order in which the buckets will be returned.
-     */
-    public TermsAggregatorFactory order(Terms.Order order) {
+    private final Terms.Order order;
+    private final IncludeExclude includeExclude;
+    private final String executionHint;
+    private final SubAggCollectionMode collectMode;
+    private final TermsAggregator.BucketCountThresholds bucketCountThresholds;
+    private final boolean showTermDocCountError;
+
+    public TermsAggregatorFactory(String name, ValuesSourceConfig config, Terms.Order order,
+            TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude, String executionHint,
+            SubAggCollectionMode executionMode, boolean showTermDocCountError) {
+        super(name, StringTerms.TYPE.name(), config);
         this.order = order;
-        return this;
-    }
-
-    /**
-     * Sets the order in which the buckets will be returned.
-     */
-    public TermsAggregatorFactory order(List<Terms.Order> orders) {
-        order(Terms.Order.compound(orders));
-        return this;
-    }
-
-    /**
-     * Gets the order in which the buckets will be returned.
-     */
-    public Terms.Order order() {
-        return order;
-    }
-
-    /**
-     * Expert: sets an execution hint to the aggregation.
-     */
-    public TermsAggregatorFactory executionHint(String executionHint) {
-        this.executionHint = executionHint;
-        return this;
-    }
-
-    /**
-     * Expert: gets an execution hint to the aggregation.
-     */
-    public String executionHint() {
-        return executionHint;
-    }
-
-    /**
-     * Expert: set the collection mode.
-     */
-    public TermsAggregatorFactory collectMode(SubAggCollectionMode mode) {
-        this.collectMode = mode;
-        return this;
-    }
-
-    /**
-     * Expert: get the collection mode.
-     */
-    public SubAggCollectionMode collectMode() {
-        return collectMode;
-    }
-
-    /**
-     * Set terms to include and exclude from the aggregation results
-     */
-    public TermsAggregatorFactory includeExclude(IncludeExclude includeExclude) {
         this.includeExclude = includeExclude;
-        return this;
-    }
-
-    /**
-     * Get terms to include and exclude from the aggregation results
-     */
-    public IncludeExclude includeExclude() {
-        return includeExclude;
-    }
-
-    /**
-     * Get whether doc count error will be return for individual terms
-     */
-    public boolean showTermDocCountError() {
-        return showTermDocCountError;
-    }
-
-    /**
-     * Set whether doc count error will be return for individual terms
-     */
-    public TermsAggregatorFactory showTermDocCountError(boolean showTermDocCountError) {
+        this.executionHint = executionHint;
+        this.bucketCountThresholds = bucketCountThresholds;
+        this.collectMode = executionMode;
         this.showTermDocCountError = showTermDocCountError;
-        return this;
     }
 
     @Override
@@ -340,20 +195,6 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
         };
     }
 
-    private Order resolveOrder(List<Order> orders) {
-        Terms.Order order;
-        if (orders.size() == 1 && (orders.get(0) == InternalOrder.TERM_ASC || orders.get(0) == InternalOrder.TERM_DESC)) {
-            // If order is only terms order then we don't need compound
-            // ordering
-            order = orders.get(0);
-        } else {
-            // for all other cases we need compound order so term order asc
-            // can be added to make the order deterministic
-            order = Order.compound(orders);
-        }
-        return order;
-    }
-
     @Override
     protected Aggregator doCreateInternal(ValuesSource valuesSource, AggregationContext aggregationContext, Aggregator parent,
             boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
@@ -361,16 +202,6 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
         if (collectsFromSingleBucket == false) {
             return asMultiBucketAggregator(this, aggregationContext, parent);
         }
-        BucketCountThresholds bucketCountThresholds = new BucketCountThresholds(this.bucketCountThresholds);
-        if (!(order == InternalOrder.TERM_ASC || order == InternalOrder.TERM_DESC)
-                && bucketCountThresholds.getShardSize() == DEFAULT_BUCKET_COUNT_THRESHOLDS.getShardSize()) {
-            // The user has not made a shardSize selection. Use default
-            // heuristic to avoid any wrong-ranking caused by distributed
-            // counting
-            bucketCountThresholds.setShardSize(BucketUtils.suggestShardSideQueueSize(bucketCountThresholds.getRequiredSize(),
-                    aggregationContext.searchContext().numberOfShards()));
-        }
-        bucketCountThresholds.ensureValidity();
         if (valuesSource instanceof ValuesSource.Bytes) {
             ExecutionMode execution = null;
             if (executionHint != null) {
@@ -450,65 +281,4 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
                 + "]. It can only be applied to numeric or string fields.");
     }
 
-    @Override
-    protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        bucketCountThresholds.toXContent(builder, params);
-        builder.field(SHOW_TERM_DOC_COUNT_ERROR.getPreferredName(), showTermDocCountError);
-        if (executionHint != null) {
-            builder.field(TermsAggregatorFactory.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
-        }
-        builder.field(ORDER_FIELD.getPreferredName());
-        order.toXContent(builder, params);
-        builder.field(SubAggCollectionMode.KEY.getPreferredName(), collectMode.parseField().getPreferredName());
-        if (includeExclude != null) {
-            includeExclude.toXContent(builder, params);
-        }
-        return builder;
-    }
-
-    @Override
-    protected TermsAggregatorFactory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        TermsAggregatorFactory factory = new TermsAggregatorFactory(name, targetValueType);
-        factory.bucketCountThresholds = BucketCountThresholds.readFromStream(in);
-        factory.collectMode = SubAggCollectionMode.BREADTH_FIRST.readFrom(in);
-        factory.executionHint = in.readOptionalString();
-        if (in.readBoolean()) {
-            factory.includeExclude = IncludeExclude.readFromStream(in);
-        }
-        factory.order = InternalOrder.Streams.readOrder(in);
-        factory.showTermDocCountError = in.readBoolean();
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        bucketCountThresholds.writeTo(out);
-        collectMode.writeTo(out);
-        out.writeOptionalString(executionHint);
-        boolean hasIncExc = includeExclude != null;
-        out.writeBoolean(hasIncExc);
-        if (hasIncExc) {
-            includeExclude.writeTo(out);
-        }
-        InternalOrder.Streams.writeOrder(order, out);
-        out.writeBoolean(showTermDocCountError);
-    }
-
-    @Override
-    protected int innerHashCode() {
-        return Objects.hash(bucketCountThresholds, collectMode, executionHint, includeExclude, order, showTermDocCountError);
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        TermsAggregatorFactory other = (TermsAggregatorFactory) obj;
-        return Objects.equals(bucketCountThresholds, other.bucketCountThresholds)
-                && Objects.equals(collectMode, other.collectMode)
-                && Objects.equals(executionHint, other.executionHint)
-                && Objects.equals(includeExclude, other.includeExclude)
-                && Objects.equals(order, other.order)
-                && Objects.equals(showTermDocCountError, other.showTermDocCountError);
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java
new file mode 100644
index 0000000..9bc1f7a
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java
@@ -0,0 +1,276 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.terms;
+
+import org.apache.lucene.util.automaton.RegExp;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+
+import java.io.IOException;
+import java.util.Locale;
+
+/**
+ * Builder for the {@link Terms} aggregation.
+ */
+public class TermsBuilder extends ValuesSourceAggregationBuilder<TermsBuilder> {
+
+    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new TermsAggregator.BucketCountThresholds(-1, -1, -1, -1);
+
+    private Terms.ValueType valueType;
+    private Terms.Order order;
+    private String includePattern;
+    private String excludePattern;
+    private String executionHint;
+    private SubAggCollectionMode collectionMode;
+    private Boolean showTermDocCountError;
+    private String[] includeTerms = null;
+    private String[] excludeTerms = null;
+
+    /**
+     * Sole constructor.
+     */
+    public TermsBuilder(String name) {
+        super(name, "terms");
+    }
+
+    /**
+     * Sets the size - indicating how many term buckets should be returned (defaults to 10)
+     */
+    public TermsBuilder size(int size) {
+        bucketCountThresholds.setRequiredSize(size);
+        return this;
+    }
+
+    /**
+     * Sets the shard_size - indicating the number of term buckets each shard will return to the coordinating node (the
+     * node that coordinates the search execution). The higher the shard size is, the more accurate the results are.
+     */
+    public TermsBuilder shardSize(int shardSize) {
+        bucketCountThresholds.setShardSize(shardSize);
+        return this;
+    }
+
+    /**
+     * Set the minimum document count terms should have in order to appear in the response.
+     */
+    public TermsBuilder minDocCount(long minDocCount) {
+        bucketCountThresholds.setMinDocCount(minDocCount);
+        return this;
+    }
+
+    /**
+     * Set the minimum document count terms should have on the shard in order to appear in the response.
+     */
+    public TermsBuilder shardMinDocCount(long shardMinDocCount) {
+        bucketCountThresholds.setShardMinDocCount(shardMinDocCount);
+        return this;
+    }
+
+    /**
+     * Define a regular expression that will determine what terms should be aggregated. The regular expression is based
+     * on the {@link RegExp} class.
+     *
+     * @see RegExp#RegExp(String)
+     */
+    public TermsBuilder include(String regex) {
+        if (includeTerms != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of strings or a regex, not both");
+        }
+        this.includePattern = regex;
+        return this;
+    }
+    
+    /**
+     * Define a set of terms that should be aggregated.
+     */
+    public TermsBuilder include(String [] terms) {
+        if (includePattern != null) {
+            throw new IllegalArgumentException("include clause must be an array of exact values or a regex, not both");
+        }
+        this.includeTerms = terms;
+        return this;
+    }    
+    
+    /**
+     * Define a set of terms that should be aggregated.
+     */
+    public TermsBuilder include(long [] terms) {
+        if (includePattern != null) {
+            throw new IllegalArgumentException("include clause must be an array of exact values or a regex, not both");
+        }
+        this.includeTerms = longsArrToStringArr(terms);
+        return this;
+    }     
+    
+    private String[] longsArrToStringArr(long[] terms) {
+        String[] termsAsString = new String[terms.length];
+        for (int i = 0; i < terms.length; i++) {
+            termsAsString[i] = Long.toString(terms[i]);
+        }
+        return termsAsString;
+    }      
+    
+
+    /**
+     * Define a set of terms that should be aggregated.
+     */
+    public TermsBuilder include(double [] terms) {
+        if (includePattern != null) {
+            throw new IllegalArgumentException("include clause must be an array of exact values or a regex, not both");
+        }
+        this.includeTerms = doubleArrToStringArr(terms);
+        return this;
+    }
+
+    private String[] doubleArrToStringArr(double[] terms) {
+        String[] termsAsString = new String[terms.length];
+        for (int i = 0; i < terms.length; i++) {
+            termsAsString[i] = Double.toString(terms[i]);
+        }
+        return termsAsString;
+    }    
+
+    /**
+     * Define a regular expression that will filter out terms that should be excluded from the aggregation. The regular
+     * expression is based on the {@link RegExp} class.
+     *
+     * @see RegExp#RegExp(String)
+     */
+    public TermsBuilder exclude(String regex) {
+        if (excludeTerms != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of exact values or a regex, not both");
+        }
+        this.excludePattern = regex;
+        return this;
+    }
+    
+    /**
+     * Define a set of terms that should not be aggregated.
+     */
+    public TermsBuilder exclude(String [] terms) {
+        if (excludePattern != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of exact values or a regex, not both");
+        }
+        this.excludeTerms = terms;
+        return this;
+    }    
+    
+    
+    /**
+     * Define a set of terms that should not be aggregated.
+     */
+    public TermsBuilder exclude(long [] terms) {
+        if (excludePattern != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of exact values or a regex, not both");
+        }
+        this.excludeTerms = longsArrToStringArr(terms);
+        return this;
+    }
+
+    /**
+     * Define a set of terms that should not be aggregated.
+     */
+    public TermsBuilder exclude(double [] terms) {
+        if (excludePattern != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of exact values or a regex, not both");
+        }
+        this.excludeTerms = doubleArrToStringArr(terms);
+        return this;
+    }    
+    
+    
+
+    /**
+     * When using scripts, the value type indicates the types of the values the script is generating.
+     */
+    public TermsBuilder valueType(Terms.ValueType valueType) {
+        this.valueType = valueType;
+        return this;
+    }
+
+    /**
+     * Defines the order in which the buckets will be returned.
+     */
+    public TermsBuilder order(Terms.Order order) {
+        this.order = order;
+        return this;
+    }
+
+    /**
+     * Expert: provide an execution hint to the aggregation.
+     */
+    public TermsBuilder executionHint(String executionHint) {
+        this.executionHint = executionHint;
+        return this;
+    }
+
+    /**
+     * Expert: set the collection mode.
+     */
+    public TermsBuilder collectMode(SubAggCollectionMode mode) {
+        this.collectionMode = mode;
+        return this;
+    }
+
+    /**
+     * Expert: return document count errors per term in the response.
+     */
+    public TermsBuilder showTermDocCountError(boolean showTermDocCountError) {
+        this.showTermDocCountError = showTermDocCountError;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+
+        bucketCountThresholds.toXContent(builder);
+
+        if (showTermDocCountError != null) {
+            builder.field(AbstractTermsParametersParser.SHOW_TERM_DOC_COUNT_ERROR.getPreferredName(), showTermDocCountError);
+        }
+        if (executionHint != null) {
+            builder.field(AbstractTermsParametersParser.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
+        }
+        if (valueType != null) {
+            builder.field("value_type", valueType.name().toLowerCase(Locale.ROOT));
+        }
+        if (order != null) {
+            builder.field("order");
+            order.toXContent(builder, params);
+        }
+        if (collectionMode != null) {
+            builder.field(SubAggCollectionMode.KEY.getPreferredName(), collectionMode.parseField().getPreferredName());
+        }
+        if (includeTerms != null) {
+            builder.array("include", includeTerms);
+        }
+        if (includePattern != null) {
+            builder.field("include", includePattern);
+        }
+        if (excludeTerms != null) {
+            builder.array("exclude", excludeTerms);
+        }
+        if (excludePattern != null) {
+            builder.field("exclude", excludePattern);
+        }
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParametersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParametersParser.java
new file mode 100644
index 0000000..c8138b7
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParametersParser.java
@@ -0,0 +1,144 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.search.aggregations.bucket.terms;
+
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+
+
+public class TermsParametersParser extends AbstractTermsParametersParser {
+
+    private static final TermsAggregator.BucketCountThresholds DEFAULT_BUCKET_COUNT_THRESHOLDS = new TermsAggregator.BucketCountThresholds(1, 0, 10, -1);
+
+    public List<OrderElement> getOrderElements() {
+        return orderElements;
+    }
+    
+    public boolean showTermDocCountError() {
+        return showTermDocCountError;
+    }
+
+    List<OrderElement> orderElements;
+    private boolean showTermDocCountError = false;
+
+    public TermsParametersParser() {
+        orderElements = new ArrayList<>(1);
+        orderElements.add(new OrderElement("_count", false));
+    }
+
+    @Override
+    public void parseSpecial(String aggregationName, XContentParser parser, SearchContext context, XContentParser.Token token, String currentFieldName) throws IOException {
+        if (token == XContentParser.Token.START_OBJECT) {
+            if ("order".equals(currentFieldName)) {
+                this.orderElements = Collections.singletonList(parseOrderParam(aggregationName, parser, context));
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
+            }
+        } else if (token == XContentParser.Token.START_ARRAY) {
+            if ("order".equals(currentFieldName)) {
+                orderElements = new ArrayList<>();
+                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                    if (token == XContentParser.Token.START_OBJECT) {
+                        OrderElement orderParam = parseOrderParam(aggregationName, parser, context);
+                        orderElements.add(orderParam);
+                    } else {
+                        throw new SearchParseException(context, "Order elements must be of type object in [" + aggregationName + "].",
+                                parser.getTokenLocation());
+                    }
+                }
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
+            }
+        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+            if (context.parseFieldMatcher().match(currentFieldName, SHOW_TERM_DOC_COUNT_ERROR)) {
+                showTermDocCountError = parser.booleanValue();
+            }
+        } else {
+            throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName
+                    + "].", parser.getTokenLocation());
+        }
+    }
+
+    private OrderElement parseOrderParam(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        XContentParser.Token token;
+        OrderElement orderParam = null;
+        String orderKey = null;
+        boolean orderAsc = false;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                orderKey = parser.currentName();
+            } else if (token == XContentParser.Token.VALUE_STRING) {
+                String dir = parser.text();
+                if ("asc".equalsIgnoreCase(dir)) {
+                    orderAsc = true;
+                } else if ("desc".equalsIgnoreCase(dir)) {
+                    orderAsc = false;
+                } else {
+                    throw new SearchParseException(context, "Unknown terms order direction [" + dir + "] in terms aggregation ["
+                            + aggregationName + "]", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " for [order] in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+        }
+        if (orderKey == null) {
+            throw new SearchParseException(context, "Must specify at least one field for [order] in [" + aggregationName + "].",
+                    parser.getTokenLocation());
+        } else {
+            orderParam = new OrderElement(orderKey, orderAsc);
+        }
+        return orderParam;
+    }
+
+    static class OrderElement {
+        private final String key;
+        private final boolean asc;
+
+        public OrderElement(String key, boolean asc) {
+            this.key = key;
+            this.asc = asc;
+        }
+
+        public String key() {
+            return key;
+        }
+
+        public boolean asc() {
+            return asc;
+        }
+
+        
+    }
+
+    @Override
+    public TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds() {
+        return new TermsAggregator.BucketCountThresholds(DEFAULT_BUCKET_COUNT_THRESHOLDS);
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java
index 7e300af..478309d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java
@@ -18,30 +18,24 @@
  */
 package org.elasticsearch.search.aggregations.bucket.terms;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.BucketUtils;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsParametersParser.OrderElement;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
-import java.util.Map;
 
 /**
  *
  */
-public class TermsParser extends AbstractTermsParser {
-
+public class TermsParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -49,123 +43,37 @@ public class TermsParser extends AbstractTermsParser {
     }
 
     @Override
-    protected TermsAggregatorFactory doCreateFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, BucketCountThresholds bucketCountThresholds, SubAggCollectionMode collectMode, String executionHint,
-            IncludeExclude incExc, Map<ParseField, Object> otherOptions) {
-        TermsAggregatorFactory factory = new TermsAggregatorFactory(aggregationName, targetValueType);
-        List<OrderElement> orderElements = (List<OrderElement>) otherOptions.get(TermsAggregatorFactory.ORDER_FIELD);
-        if (orderElements != null) {
-            List<Terms.Order> orders = new ArrayList<>(orderElements.size());
-            for (OrderElement orderElement : orderElements) {
-                orders.add(resolveOrder(orderElement.key(), orderElement.asc()));
-            }
-            factory.order(orders);
-        }
-        if (bucketCountThresholds != null) {
-            factory.bucketCountThresholds(bucketCountThresholds);
-        }
-        if (collectMode != null) {
-            factory.collectMode(collectMode);
-        }
-        if (executionHint != null) {
-            factory.executionHint(executionHint);
-        }
-        if (incExc != null) {
-            factory.includeExclude(incExc);
-        }
-        Boolean showTermDocCountError = (Boolean) otherOptions.get(TermsAggregatorFactory.SHOW_TERM_DOC_COUNT_ERROR);
-        if (showTermDocCountError != null) {
-            factory.showTermDocCountError(showTermDocCountError);
-        }
-        return factory;
-    }
-
-    @Override
-    public boolean parseSpecial(String aggregationName, XContentParser parser, ParseFieldMatcher parseFieldMatcher, Token token,
-            String currentFieldName, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.START_OBJECT) {
-            if (parseFieldMatcher.match(currentFieldName, TermsAggregatorFactory.ORDER_FIELD)) {
-                otherOptions.put(TermsAggregatorFactory.ORDER_FIELD, Collections.singletonList(parseOrderParam(aggregationName, parser)));
-                return true;
-            }
-        } else if (token == XContentParser.Token.START_ARRAY) {
-            if (parseFieldMatcher.match(currentFieldName, TermsAggregatorFactory.ORDER_FIELD)) {
-                List<OrderElement> orderElements = new ArrayList<>();
-                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                    if (token == XContentParser.Token.START_OBJECT) {
-                        OrderElement orderParam = parseOrderParam(aggregationName, parser);
-                        orderElements.add(orderParam);
-                    } else {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Order elements must be of type object in [" + aggregationName + "] found token of type [" + token + "].");
-                    }
-                }
-                otherOptions.put(TermsAggregatorFactory.ORDER_FIELD, orderElements);
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, TermsAggregatorFactory.SHOW_TERM_DOC_COUNT_ERROR)) {
-                otherOptions.put(TermsAggregatorFactory.SHOW_TERM_DOC_COUNT_ERROR, parser.booleanValue());
-                return true;
-            }
-        }
-        return false;
-    }
-
-    private OrderElement parseOrderParam(String aggregationName, XContentParser parser) throws IOException {
-        XContentParser.Token token;
-        OrderElement orderParam = null;
-        String orderKey = null;
-        boolean orderAsc = false;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                orderKey = parser.currentName();
-            } else if (token == XContentParser.Token.VALUE_STRING) {
-                String dir = parser.text();
-                if ("asc".equalsIgnoreCase(dir)) {
-                    orderAsc = true;
-                } else if ("desc".equalsIgnoreCase(dir)) {
-                    orderAsc = false;
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown terms order direction [" + dir + "] in terms aggregation [" + aggregationName + "]");
-                }
-            } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " for [order] in [" + aggregationName + "].");
-            }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        TermsParametersParser aggParser = new TermsParametersParser();
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, StringTerms.TYPE, context).scriptable(true).formattable(true).build();
+        IncludeExclude.Parser incExcParser = new IncludeExclude.Parser();
+        aggParser.parse(aggregationName, parser, context, vsParser, incExcParser);
+
+        List<OrderElement> orderElements = aggParser.getOrderElements();
+        List<Terms.Order> orders = new ArrayList<>(orderElements.size());
+        for (OrderElement orderElement : orderElements) {
+            orders.add(resolveOrder(orderElement.key(), orderElement.asc()));
         }
-        if (orderKey == null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Must specify at least one field for [order] in [" + aggregationName + "].");
-        } else {
-            orderParam = new OrderElement(orderKey, orderAsc);
+        Terms.Order order;
+        if (orders.size() == 1 && (orders.get(0) == InternalOrder.TERM_ASC || orders.get(0) == InternalOrder.TERM_DESC))
+        {
+            // If order is only terms order then we don't need compound ordering
+            order = orders.get(0);
         }
-        return orderParam;
-    }
-
-    static class OrderElement {
-        private final String key;
-        private final boolean asc;
-
-        public OrderElement(String key, boolean asc) {
-            this.key = key;
-            this.asc = asc;
+        else
+        {
+            // for all other cases we need compound order so term order asc can be added to make the order deterministic
+            order = Order.compound(orders);
         }
-
-        public String key() {
-            return key;
+        TermsAggregator.BucketCountThresholds bucketCountThresholds = aggParser.getBucketCountThresholds();
+        if (!(order == InternalOrder.TERM_ASC || order == InternalOrder.TERM_DESC)
+                && bucketCountThresholds.getShardSize() == aggParser.getDefaultBucketCountThresholds().getShardSize()) {
+            // The user has not made a shardSize selection. Use default heuristic to avoid any wrong-ranking caused by distributed counting
+            bucketCountThresholds.setShardSize(BucketUtils.suggestShardSideQueueSize(bucketCountThresholds.getRequiredSize(),
+                    context.numberOfShards()));
         }
-
-        public boolean asc() {
-            return asc;
-        }
-
-    }
-
-    @Override
-    public TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds() {
-        return new TermsAggregator.BucketCountThresholds(TermsAggregatorFactory.DEFAULT_BUCKET_COUNT_THRESHOLDS);
+        bucketCountThresholds.ensureValidity();
+        return new TermsAggregatorFactory(aggregationName, vsParser.config(), order, bucketCountThresholds, aggParser.getIncludeExclude(), aggParser.getExecutionHint(), aggParser.getCollectionMode(), aggParser.showTermDocCountError());
     }
 
     static Terms.Order resolveOrder(String key, boolean asc) {
@@ -178,9 +86,4 @@ public class TermsParser extends AbstractTermsParser {
         return Order.aggregation(key, asc);
     }
 
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new TermsAggregatorFactory(null, null);
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java
index eee9d4c..9c33a98 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java
@@ -20,7 +20,6 @@ package org.elasticsearch.search.aggregations.bucket.terms.support;
 
 import com.carrotsearch.hppc.LongHashSet;
 import com.carrotsearch.hppc.LongSet;
-
 import org.apache.lucene.index.RandomAccessOrds;
 import org.apache.lucene.index.SortedSetDocValues;
 import org.apache.lucene.index.Terms;
@@ -35,22 +34,12 @@ import org.apache.lucene.util.automaton.CompiledAutomaton;
 import org.apache.lucene.util.automaton.Operations;
 import org.apache.lucene.util.automaton.RegExp;
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Bytes.WithOrdinals;
 
 import java.io.IOException;
-import java.util.Collections;
 import java.util.HashSet;
-import java.util.Map;
-import java.util.Objects;
 import java.util.Set;
 import java.util.SortedSet;
 import java.util.TreeSet;
@@ -59,16 +48,7 @@ import java.util.TreeSet;
  * Defines the include/exclude regular expression filtering for string terms aggregation. In this filtering logic,
  * exclusion has precedence, where the {@code include} is evaluated first and then the {@code exclude}.
  */
-public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
-
-    private static final IncludeExclude PROTOTYPE = new IncludeExclude(Collections.emptySortedSet(), Collections.emptySortedSet());
-    private static final ParseField INCLUDE_FIELD = new ParseField("include");
-    private static final ParseField EXCLUDE_FIELD = new ParseField("exclude");
-    private static final ParseField PATTERN_FIELD = new ParseField("pattern");
-
-    public static IncludeExclude readFromStream(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
+public class IncludeExclude {
 
     // The includeValue and excludeValue ByteRefs which are the result of the parsing
     // process are converted into a LongFilter when used on numeric fields
@@ -227,10 +207,6 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
         this.excludeValues = null;
     }
 
-    public IncludeExclude(String include, String exclude) {
-        this(include == null ? null : new RegExp(include), exclude == null ? null : new RegExp(exclude));
-    }
-
     /**
      * @param includeValues   The terms to be included
      * @param excludeValues   The terms to be excluded
@@ -245,51 +221,6 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
         this.excludeValues = excludeValues;
     }
 
-    public IncludeExclude(String[] includeValues, String[] excludeValues) {
-        this(convertToBytesRefSet(includeValues), convertToBytesRefSet(excludeValues));
-    }
-
-    public IncludeExclude(double[] includeValues, double[] excludeValues) {
-        this(convertToBytesRefSet(includeValues), convertToBytesRefSet(excludeValues));
-    }
-
-    public IncludeExclude(long[] includeValues, long[] excludeValues) {
-        this(convertToBytesRefSet(includeValues), convertToBytesRefSet(excludeValues));
-    }
-
-    private static SortedSet<BytesRef> convertToBytesRefSet(String[] values) {
-        SortedSet<BytesRef> returnSet = null;
-        if (values != null) {
-            returnSet = new TreeSet<>();
-            for (String value : values) {
-                returnSet.add(new BytesRef(value));
-            }
-        }
-        return returnSet;
-    }
-
-    private static SortedSet<BytesRef> convertToBytesRefSet(double[] values) {
-        SortedSet<BytesRef> returnSet = null;
-        if (values != null) {
-            returnSet = new TreeSet<>();
-            for (double value : values) {
-                returnSet.add(new BytesRef(String.valueOf(value)));
-            }
-        }
-        return returnSet;
-    }
-
-    private static SortedSet<BytesRef> convertToBytesRefSet(long[] values) {
-        SortedSet<BytesRef> returnSet = null;
-        if (values != null) {
-            returnSet = new TreeSet<>();
-            for (long value : values) {
-                returnSet.add(new BytesRef(String.valueOf(value)));
-            }
-        }
-        return returnSet;
-    }
-
     /**
      * Terms adapter around doc values.
      */
@@ -352,14 +283,18 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
 
     public static class Parser {
 
-        public boolean token(String currentFieldName, XContentParser.Token token, XContentParser parser,
-                ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
+        String include = null;
+        String exclude = null;
+        SortedSet<BytesRef> includeValues;
+        SortedSet<BytesRef> excludeValues;
+
+        public boolean token(String currentFieldName, XContentParser.Token token, XContentParser parser) throws IOException {
 
             if (token == XContentParser.Token.VALUE_STRING) {
-                if (parseFieldMatcher.match(currentFieldName, INCLUDE_FIELD)) {
-                    otherOptions.put(INCLUDE_FIELD, parser.text());
-                } else if (parseFieldMatcher.match(currentFieldName, EXCLUDE_FIELD)) {
-                    otherOptions.put(EXCLUDE_FIELD, parser.text());
+                if ("include".equals(currentFieldName)) {
+                    include = parser.text();
+                } else if ("exclude".equals(currentFieldName)) {
+                    exclude = parser.text();
                 } else {
                     return false;
                 }
@@ -367,35 +302,35 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
             }
 
             if (token == XContentParser.Token.START_ARRAY) {
-                if (parseFieldMatcher.match(currentFieldName, INCLUDE_FIELD)) {
-                    otherOptions.put(INCLUDE_FIELD, new TreeSet<>(parseArrayToSet(parser)));
+                if ("include".equals(currentFieldName)) {
+                     includeValues = new TreeSet<>(parseArrayToSet(parser));
                      return true;
                 }
-                if (parseFieldMatcher.match(currentFieldName, EXCLUDE_FIELD)) {
-                    otherOptions.put(EXCLUDE_FIELD, new TreeSet<>(parseArrayToSet(parser)));
+                if ("exclude".equals(currentFieldName)) {
+                      excludeValues = new TreeSet<>(parseArrayToSet(parser));
                       return true;
                 }
                 return false;
             }
 
             if (token == XContentParser.Token.START_OBJECT) {
-                if (parseFieldMatcher.match(currentFieldName, INCLUDE_FIELD)) {
+                if ("include".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                         if (token == XContentParser.Token.FIELD_NAME) {
                             currentFieldName = parser.currentName();
                         } else if (token == XContentParser.Token.VALUE_STRING) {
-                            if (parseFieldMatcher.match(currentFieldName, PATTERN_FIELD)) {
-                                otherOptions.put(INCLUDE_FIELD, parser.text());
+                            if ("pattern".equals(currentFieldName)) {
+                                include = parser.text();
                             }
                         }
                     }
-                } else if (parseFieldMatcher.match(currentFieldName, EXCLUDE_FIELD)) {
+                } else if ("exclude".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                         if (token == XContentParser.Token.FIELD_NAME) {
                             currentFieldName = parser.currentName();
                         } else if (token == XContentParser.Token.VALUE_STRING) {
-                            if (parseFieldMatcher.match(currentFieldName, PATTERN_FIELD)) {
-                                otherOptions.put(EXCLUDE_FIELD, parser.text());
+                            if ("pattern".equals(currentFieldName)) {
+                                exclude = parser.text();
                             }
                         }
                     }
@@ -407,7 +342,6 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
 
             return false;
         }
-
         private Set<BytesRef> parseArrayToSet(XContentParser parser) throws IOException {
             final Set<BytesRef> set = new HashSet<>();
             if (parser.currentToken() != XContentParser.Token.START_ARRAY) {
@@ -422,27 +356,7 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
             return set;
         }
 
-        public IncludeExclude createIncludeExclude(Map<ParseField, Object> otherOptions) {
-            Object includeObject = otherOptions.get(INCLUDE_FIELD);
-            String include = null;
-            SortedSet<BytesRef> includeValues = null;
-            if (includeObject != null) {
-                if (includeObject instanceof String) {
-                    include = (String) includeObject;
-                } else if (includeObject instanceof SortedSet) {
-                    includeValues = (SortedSet<BytesRef>) includeObject;
-                }
-            }
-            Object excludeObject = otherOptions.get(EXCLUDE_FIELD);
-            String exclude = null;
-            SortedSet<BytesRef> excludeValues = null;
-            if (excludeObject != null) {
-                if (excludeObject instanceof String) {
-                    exclude = (String) excludeObject;
-                } else if (excludeObject instanceof SortedSet) {
-                    excludeValues = (SortedSet<BytesRef>) excludeObject;
-                }
-            }
+        public IncludeExclude includeExclude() {
             RegExp includePattern =  include != null ? new RegExp(include) : null;
             RegExp excludePattern = exclude != null ? new RegExp(exclude) : null;
             if (includePattern != null || excludePattern != null) {
@@ -530,111 +444,4 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
         return result;
     }
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        if (include != null) {
-            builder.field(INCLUDE_FIELD.getPreferredName(), include.getOriginalString());
-        }
-        if (includeValues != null) {
-            builder.startArray(INCLUDE_FIELD.getPreferredName());
-            for (BytesRef value : includeValues) {
-                builder.value(value.utf8ToString());
-            }
-            builder.endArray();
-        }
-        if (exclude != null) {
-            builder.field(EXCLUDE_FIELD.getPreferredName(), exclude.getOriginalString());
-        }
-        if (excludeValues != null) {
-            builder.startArray(EXCLUDE_FIELD.getPreferredName());
-            for (BytesRef value : excludeValues) {
-                builder.value(value.utf8ToString());
-            }
-            builder.endArray();
-        }
-        return builder;
-    }
-
-    @Override
-    public IncludeExclude readFrom(StreamInput in) throws IOException {
-        if (in.readBoolean()) {
-            String includeString = in.readOptionalString();
-            RegExp include = null;
-            if (includeString != null) {
-                include = new RegExp(includeString);
-            }
-            String excludeString = in.readOptionalString();
-            RegExp exclude = null;
-            if (excludeString != null) {
-                exclude = new RegExp(excludeString);
-            }
-            return new IncludeExclude(include, exclude);
-        } else {
-            SortedSet<BytesRef> includes = null;
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                includes = new TreeSet<>();
-                for (int i = 0; i < size; i++) {
-                    includes.add(in.readBytesRef());
-                }
-            }
-            SortedSet<BytesRef> excludes = null;
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                excludes = new TreeSet<>();
-                for (int i = 0; i < size; i++) {
-                    excludes.add(in.readBytesRef());
-                }
-            }
-            return new IncludeExclude(includes, excludes);
-        }
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        boolean regexBased = isRegexBased();
-        out.writeBoolean(regexBased);
-        if (regexBased) {
-            out.writeOptionalString(include == null ? null : include.getOriginalString());
-            out.writeOptionalString(exclude == null ? null : exclude.getOriginalString());
-        } else {
-            boolean hasIncludes = includeValues != null;
-            out.writeBoolean(hasIncludes);
-            if (hasIncludes) {
-                out.writeVInt(includeValues.size());
-                for (BytesRef value : includeValues) {
-                    out.writeBytesRef(value);
-                }
-            }
-            boolean hasExcludes = excludeValues != null;
-            out.writeBoolean(hasExcludes);
-            if (hasExcludes) {
-                out.writeVInt(excludeValues.size());
-                for (BytesRef value : excludeValues) {
-                    out.writeBytesRef(value);
-                }
-            }
-        }
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(include == null ? null : include.getOriginalString(), exclude == null ? null : exclude.getOriginalString(),
-                includeValues, excludeValues);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        } if (getClass() != obj.getClass()) {
-            return false;
-        }
-        IncludeExclude other = (IncludeExclude) obj;
-        return Objects.equals(include == null ? null : include.getOriginalString(), other.include == null ? null : other.include.getOriginalString())
-                && Objects.equals(exclude == null ? null : exclude.getOriginalString(), other.exclude == null ? null : other.exclude.getOriginalString())
-                && Objects.equals(includeValues, other.includeValues)
-                && Objects.equals(excludeValues, other.excludeValues);
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/MetricsAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/MetricsAggregationBuilder.java
new file mode 100644
index 0000000..56ae24b
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/MetricsAggregationBuilder.java
@@ -0,0 +1,44 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Base builder for metrics aggregations.
+ */
+public abstract class MetricsAggregationBuilder<B extends MetricsAggregationBuilder<B>> extends AbstractAggregationBuilder {
+
+    public MetricsAggregationBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    @Override
+    public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(getName()).startObject(type);
+        internalXContent(builder, params);
+        return builder.endObject().endObject();
+    }
+
+    protected abstract void internalXContent(XContentBuilder builder, Params params) throws IOException;
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/NumericValuesSourceMetricsAggregatorParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/NumericValuesSourceMetricsAggregatorParser.java
new file mode 100644
index 0000000..6847a9a
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/NumericValuesSourceMetricsAggregatorParser.java
@@ -0,0 +1,70 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.metrics;
+
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+
+/**
+ *
+ */
+public abstract class NumericValuesSourceMetricsAggregatorParser<S extends InternalNumericMetricsAggregation> implements Aggregator.Parser {
+
+    protected final InternalAggregation.Type aggType;
+
+    protected NumericValuesSourceMetricsAggregatorParser(InternalAggregation.Type aggType) {
+        this.aggType = aggType;
+    }
+
+    @Override
+    public String type() {
+        return aggType.name();
+    }
+
+    @Override
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, aggType, context).formattable(true)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (!vsParser.token(currentFieldName, token, parser)) {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+        }
+
+        return createFactory(aggregationName, vsParser.config());
+    }
+
+    protected abstract AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config);
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/ValuesSourceMetricsAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/ValuesSourceMetricsAggregationBuilder.java
new file mode 100644
index 0000000..e675548
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/ValuesSourceMetricsAggregationBuilder.java
@@ -0,0 +1,88 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+
+import java.io.IOException;
+
+/**
+ *
+ */
+public abstract class ValuesSourceMetricsAggregationBuilder<B extends ValuesSourceMetricsAggregationBuilder<B>> extends MetricsAggregationBuilder<B> {
+
+    private String field;
+    private Script script;
+    private String format;
+    private Object missing;
+
+    protected ValuesSourceMetricsAggregationBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    @SuppressWarnings("unchecked")
+    public B field(String field) {
+        this.field = field;
+        return (B) this;
+    }
+
+    /**
+     * The script to use for this aggregation
+     */
+    @SuppressWarnings("unchecked")
+    public B script(Script script) {
+        this.script = script;
+        return (B) this;
+    }
+
+    @SuppressWarnings("unchecked")
+    public B format(String format) {
+        this.format = format;
+        return (B) this;
+    }
+
+    /**
+     * Configure the value to use when documents miss a value.
+     */
+    public B missing(Object missingValue) {
+        this.missing = missingValue;
+        return (B) this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (field != null) {
+            builder.field("field", field);
+        }
+
+        if (script != null) {
+            builder.field("script", script);
+        }
+
+        if (format != null) {
+            builder.field("format", format);
+        }
+
+        if (missing != null) {
+            builder.field("missing", missing);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java
index fbada14..67a6f19 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java
@@ -19,13 +19,10 @@
 package org.elasticsearch.search.aggregations.metrics.avg;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -34,10 +31,9 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -117,10 +113,10 @@ public class AvgAggregator extends NumericMetricsAggregator.SingleValue {
         return new InternalAvg(name, 0.0, 0l, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public Factory(String name) {
-            super(name, InternalAvg.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, String type, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, type, valuesSourceConfig);
         }
 
         @Override
@@ -136,32 +132,6 @@ public class AvgAggregator extends NumericMetricsAggregator.SingleValue {
                 throws IOException {
             return new AvgAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new AvgAggregator.Factory(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgBuilder.java
new file mode 100644
index 0000000..143f39b
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.avg;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Avg} aggregation.
+ */
+public class AvgBuilder extends ValuesSourceMetricsAggregationBuilder<AvgBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public AvgBuilder(String name) {
+        super(name, InternalAvg.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java
index 70ebbf5..1c2b2be 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java
@@ -18,46 +18,23 @@
  */
 package org.elasticsearch.search.aggregations.metrics.avg;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class AvgParser extends NumericValuesSourceParser {
+public class AvgParser extends NumericValuesSourceMetricsAggregatorParser<InternalAvg> {
 
     public AvgParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalAvg.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected AvgAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new AvgAggregator.Factory(aggregationName);
+        super(InternalAvg.TYPE);
     }
 
     @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new AvgAggregator.Factory(null);
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new AvgAggregator.Factory(aggregationName, type(), config);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
index 2ea7793..1b2d5fc 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
@@ -19,62 +19,29 @@
 
 package org.elasticsearch.search.aggregations.metrics.cardinality;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
-public final class CardinalityAggregatorFactory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource, CardinalityAggregatorFactory> {
+final class CardinalityAggregatorFactory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource> {
 
-    public static final ParseField PRECISION_THRESHOLD_FIELD = new ParseField("precision_threshold");
+    private final long precisionThreshold;
 
-    private Long precisionThreshold = null;
-
-    public CardinalityAggregatorFactory(String name, ValueType targetValueType) {
-        super(name, InternalCardinality.TYPE, ValuesSourceType.ANY, targetValueType);
-    }
-
-    /**
-     * Set a precision threshold. Higher values improve accuracy but also
-     * increase memory usage.
-     */
-    public CardinalityAggregatorFactory precisionThreshold(long precisionThreshold) {
+    CardinalityAggregatorFactory(String name, ValuesSourceConfig config, long precisionThreshold) {
+        super(name, InternalCardinality.TYPE.name(), config);
         this.precisionThreshold = precisionThreshold;
-        return this;
-    }
-
-    /**
-     * Get the precision threshold. Higher values improve accuracy but also
-     * increase memory usage. Will return <code>null</code> if the
-     * precisionThreshold has not been set yet.
-     */
-    public Long precisionThreshold() {
-        return precisionThreshold;
-    }
-
-    /**
-     * @deprecated no replacement - values will always be rehashed
-     */
-    @Deprecated
-    public void rehash(boolean rehash) {
-        // Deprecated all values are already rehashed so do nothing
     }
 
     private int precision(Aggregator parent) {
-        return precisionThreshold == null ? defaultPrecision(parent) : HyperLogLogPlusPlus.precisionFromThreshold(precisionThreshold);
+        return precisionThreshold < 0 ? defaultPrecision(parent) : HyperLogLogPlusPlus.precisionFromThreshold(precisionThreshold);
     }
 
     @Override
@@ -90,44 +57,6 @@ public final class CardinalityAggregatorFactory extends ValuesSourceAggregatorFa
                 metaData);
     }
 
-    @Override
-    protected CardinalityAggregatorFactory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        CardinalityAggregatorFactory factory = new CardinalityAggregatorFactory(name, targetValueType);
-        if (in.readBoolean()) {
-            factory.precisionThreshold = in.readLong();
-        }
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        boolean hasPrecisionThreshold = precisionThreshold != null;
-        out.writeBoolean(hasPrecisionThreshold);
-        if (hasPrecisionThreshold) {
-            out.writeLong(precisionThreshold);
-        }
-    }
-
-    @Override
-    public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        if (precisionThreshold != null) {
-            builder.field(PRECISION_THRESHOLD_FIELD.getPreferredName(), precisionThreshold);
-        }
-        return builder;
-    }
-
-    @Override
-    protected int innerHashCode() {
-        return Objects.hash(precisionThreshold);
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        CardinalityAggregatorFactory other = (CardinalityAggregatorFactory) obj;
-        return Objects.equals(precisionThreshold, other.precisionThreshold);
-    }
-
     /*
      * If one of the parent aggregators is a MULTI_BUCKET one, we might want to lower the precision
      * because otherwise it might be memory-intensive. On the other hand, for top-level aggregators
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityBuilder.java
new file mode 100644
index 0000000..7680aa0
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityBuilder.java
@@ -0,0 +1,71 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.cardinality;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Cardinality} aggregation.
+ */
+public class CardinalityBuilder extends ValuesSourceMetricsAggregationBuilder<CardinalityBuilder> {
+
+    private Long precisionThreshold;
+    private Boolean rehash;
+
+    /**
+     * Sole constructor.
+     */
+    public CardinalityBuilder(String name) {
+        super(name, InternalCardinality.TYPE.name());
+    }
+
+    /**
+     * Set a precision threshold. Higher values improve accuracy but also
+     * increase memory usage.
+     */
+    public CardinalityBuilder precisionThreshold(long precisionThreshold) {
+        this.precisionThreshold = precisionThreshold;
+        return this;
+    }
+
+    /**
+     * Expert: set to false in case values of this field can already be treated
+     * as 64-bits hash values.
+     */
+    public CardinalityBuilder rehash(boolean rehash) {
+        this.rehash = rehash;
+        return this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.internalXContent(builder, params);
+        if (precisionThreshold != null) {
+            builder.field("precision_threshold", precisionThreshold);
+        }
+        if (rehash != null) {
+            builder.field("rehash", rehash);
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java
index 3317857..6833945 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java
@@ -20,59 +20,56 @@
 package org.elasticsearch.search.aggregations.metrics.cardinality;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 
-public class CardinalityParser extends AnyValuesSourceParser {
+public class CardinalityParser implements Aggregator.Parser {
 
+    private static final ParseField PRECISION_THRESHOLD = new ParseField("precision_threshold");
     private static final ParseField REHASH = new ParseField("rehash").withAllDeprecated("no replacement - values will always be rehashed");
 
-    public CardinalityParser() {
-        super(true, false);
-    }
-
     @Override
     public String type() {
         return InternalCardinality.TYPE.name();
     }
 
     @Override
-    protected CardinalityAggregatorFactory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        CardinalityAggregatorFactory factory = new CardinalityAggregatorFactory(aggregationName, targetValueType);
-        Long precisionThreshold = (Long) otherOptions.get(CardinalityAggregatorFactory.PRECISION_THRESHOLD_FIELD);
-        if (precisionThreshold != null) {
-            factory.precisionThreshold(precisionThreshold);
-        }
-        return factory;
-    }
+    public AggregatorFactory parse(String name, XContentParser parser, SearchContext context) throws IOException {
 
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token.isValue()) {
-            if (parseFieldMatcher.match(currentFieldName, CardinalityAggregatorFactory.PRECISION_THRESHOLD_FIELD)) {
-                otherOptions.put(CardinalityAggregatorFactory.PRECISION_THRESHOLD_FIELD, parser.longValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, REHASH)) {
-                // ignore
-                return true;
+        ValuesSourceParser<?> vsParser = ValuesSourceParser.any(name, InternalCardinality.TYPE, context).formattable(false).build();
+
+        long precisionThreshold = -1;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token.isValue()) {
+                if (context.parseFieldMatcher().match(currentFieldName, REHASH)) {
+                    // ignore
+                } else if (context.parseFieldMatcher().match(currentFieldName, PRECISION_THRESHOLD)) {
+                    precisionThreshold = parser.longValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + name + "]: [" + currentFieldName
+                            + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + name + "].", parser.getTokenLocation());
             }
         }
-        return false;
-    }
 
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new CardinalityAggregatorFactory(null, null);
+        return new CardinalityAggregatorFactory(name, vsParser.config(), precisionThreshold);
+
     }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java
index 2f8b7ae..8c6159e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java
@@ -20,14 +20,10 @@
 package org.elasticsearch.search.aggregations.metrics.geobounds;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -36,20 +32,16 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.MetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 public final class GeoBoundsAggregator extends MetricsAggregator {
 
-    static final ParseField WRAP_LONGITUDE_FIELD = new ParseField("wrap_longitude");
-
     private final ValuesSource.GeoPoint valuesSource;
     private final boolean wrapLongitude;
     DoubleArray tops;
@@ -168,33 +160,19 @@ public final class GeoBoundsAggregator extends MetricsAggregator {
         return new InternalGeoBounds(name, Double.NEGATIVE_INFINITY, Double.POSITIVE_INFINITY, Double.POSITIVE_INFINITY,
                 Double.NEGATIVE_INFINITY, Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY, wrapLongitude, pipelineAggregators(), metaData());
     }
-
+    
     @Override
     public void doClose() {
         Releasables.close(tops, bottoms, posLefts, posRights, negLefts, negRights);
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, Factory> {
-
-        private boolean wrapLongitude = true;
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint> {
 
-        public Factory(String name) {
-            super(name, InternalGeoBounds.TYPE, ValuesSourceType.GEOPOINT, ValueType.GEOPOINT);
-        }
+        private final boolean wrapLongitude;
 
-        /**
-         * Set whether to wrap longitudes. Defaults to true.
-         */
-        public Factory wrapLongitude(boolean wrapLongitude) {
+        protected Factory(String name, ValuesSourceConfig<ValuesSource.GeoPoint> config, boolean wrapLongitude) {
+            super(name, InternalGeoBounds.TYPE.name(), config);
             this.wrapLongitude = wrapLongitude;
-            return this;
-        }
-
-        /**
-         * Get whether to wrap longitudes.
-         */
-        public boolean wrapLongitude() {
-            return wrapLongitude;
         }
 
         @Override
@@ -210,35 +188,5 @@ public final class GeoBoundsAggregator extends MetricsAggregator {
             return new GeoBoundsAggregator(name, aggregationContext, parent, valuesSource, wrapLongitude, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            Factory factory = new Factory(name);
-            factory.wrapLongitude = in.readBoolean();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeBoolean(wrapLongitude);
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(WRAP_LONGITUDE_FIELD.getPreferredName(), wrapLongitude);
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(wrapLongitude);
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(wrapLongitude, other.wrapLongitude);
-        }
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsBuilder.java
new file mode 100644
index 0000000..582ab8c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsBuilder.java
@@ -0,0 +1,57 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.geobounds;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link GeoBounds} aggregation.
+ */
+public class GeoBoundsBuilder extends ValuesSourceAggregationBuilder<GeoBoundsBuilder> {
+
+    private Boolean wrapLongitude;
+
+    /**
+     * Sole constructor.
+     */
+    public GeoBoundsBuilder(String name) {
+        super(name, InternalGeoBounds.TYPE.name());
+    }
+
+    /**
+     * Set whether to wrap longitudes. Defaults to true.
+     */
+    public GeoBoundsBuilder wrapLongitude(boolean wrapLongitude) {
+        this.wrapLongitude = wrapLongitude;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (wrapLongitude != null) {
+            builder.field("wrap_longitude", wrapLongitude);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsParser.java
index cf09286..de1fea2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsParser.java
@@ -19,23 +19,18 @@
 
 package org.elasticsearch.search.aggregations.metrics.geobounds;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.GeoPointValuesSourceParser;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource.GeoPoint;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
-public class GeoBoundsParser extends GeoPointValuesSourceParser {
-
-    public GeoBoundsParser() {
-        super(false, false);
-    }
+public class GeoBoundsParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -43,31 +38,33 @@ public class GeoBoundsParser extends GeoPointValuesSourceParser {
     }
 
     @Override
-    protected GeoBoundsAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        GeoBoundsAggregator.Factory factory = new GeoBoundsAggregator.Factory(aggregationName);
-        Boolean wrapLongitude = (Boolean) otherOptions.get(GeoBoundsAggregator.WRAP_LONGITUDE_FIELD);
-        if (wrapLongitude != null) {
-            factory.wrapLongitude(wrapLongitude);
-        }
-        return factory;
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, GeoBoundsAggregator.WRAP_LONGITUDE_FIELD)) {
-                otherOptions.put(GeoBoundsAggregator.WRAP_LONGITUDE_FIELD, parser.booleanValue());
-                return true;
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        ValuesSourceParser<GeoPoint> vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoBounds.TYPE, context)
+                .targetValueType(ValueType.GEOPOINT)
+                .formattable(true)
+                .build();
+        boolean wrapLongitude = true;
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+                
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("wrap_longitude".equals(currentFieldName) || "wrapLongitude".equals(currentFieldName)) {
+                    wrapLongitude = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
             }
         }
-        return false;
-    }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new GeoBoundsAggregator.Factory(null);
+        return new GeoBoundsAggregator.Factory(aggregationName, vsParser.config(), wrapLongitude);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregator.java
index 39aa6bc..b99db25 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregator.java
@@ -22,24 +22,21 @@ package org.elasticsearch.search.aggregations.metrics.geocentroid;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.GeoUtils;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.MetricsAggregator;
+import org.elasticsearch.search.aggregations.metrics.geobounds.InternalGeoBounds;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 import java.io.IOException;
 import java.util.List;
@@ -125,10 +122,9 @@ public final class GeoCentroidAggregator extends MetricsAggregator {
         Releasables.close(centroids, counts);
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.GeoPoint, Factory> {
-
-        public Factory(String name) {
-            super(name, InternalGeoCentroid.TYPE, ValuesSourceType.GEOPOINT, ValueType.GEOPOINT);
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.GeoPoint> {
+        protected Factory(String name, ValuesSourceConfig<ValuesSource.GeoPoint> config) {
+            super(name, InternalGeoBounds.TYPE.name(), config);
         }
 
         @Override
@@ -143,31 +139,5 @@ public final class GeoCentroidAggregator extends MetricsAggregator {
                 throws IOException {
             return new GeoCentroidAggregator(name, aggregationContext, parent, valuesSource, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            return new Factory(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidBuilder.java
new file mode 100644
index 0000000..9d6823c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidBuilder.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.geocentroid;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder class for {@link org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator}
+ */
+public class GeoCentroidBuilder extends ValuesSourceMetricsAggregationBuilder<GeoCentroidBuilder> {
+
+    public GeoCentroidBuilder(String name) {
+        super(name, InternalGeoCentroid.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidParser.java
index f69913c..49a7bc8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidParser.java
@@ -19,26 +19,21 @@
 
 package org.elasticsearch.search.aggregations.metrics.geocentroid;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.GeoPointValuesSourceParser;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  * Parser class for {@link org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator}
  */
-public class GeoCentroidParser extends GeoPointValuesSourceParser {
-
-    public GeoCentroidParser() {
-        super(true, false);
-    }
+public class GeoCentroidParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -46,19 +41,23 @@ public class GeoCentroidParser extends GeoPointValuesSourceParser {
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected GeoCentroidAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new GeoCentroidAggregator.Factory(aggregationName);
-    }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new GeoCentroidAggregator.Factory(null);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        ValuesSourceParser<ValuesSource.GeoPoint> vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoCentroid.TYPE, context)
+                .targetValueType(ValueType.GEOPOINT)
+                .formattable(true)
+                .build();
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
+            }
+        }
+        return new GeoCentroidAggregator.Factory(aggregationName, vsParser.config());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java
index cd28dbb..e70fc7f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java
@@ -19,12 +19,9 @@
 package org.elasticsearch.search.aggregations.metrics.max;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.NumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.MultiValueMode;
@@ -35,10 +32,9 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -118,10 +114,10 @@ public class MaxAggregator extends NumericMetricsAggregator.SingleValue {
         return new InternalMax(name, Double.NEGATIVE_INFINITY, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public Factory(String name) {
-            super(name, InternalMax.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalMax.TYPE.name(), valuesSourceConfig);
         }
 
         @Override
@@ -136,33 +132,6 @@ public class MaxAggregator extends NumericMetricsAggregator.SingleValue {
                 throws IOException {
             return new MaxAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new MaxAggregator.Factory(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
-
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxBuilder.java
new file mode 100644
index 0000000..b433573
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.max;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Max} aggregation.
+ */
+public class MaxBuilder extends ValuesSourceMetricsAggregationBuilder<MaxBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public MaxBuilder(String name) {
+        super(name, InternalMax.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java
index e8d8b0d..a5cdac5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java
@@ -18,46 +18,23 @@
  */
 package org.elasticsearch.search.aggregations.metrics.max;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class MaxParser extends NumericValuesSourceParser {
+public class MaxParser extends NumericValuesSourceMetricsAggregatorParser<InternalMax> {
 
     public MaxParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalMax.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected MaxAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new MaxAggregator.Factory(aggregationName);
+        super(InternalMax.TYPE);
     }
 
     @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new MaxAggregator.Factory(null);
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new MaxAggregator.Factory(aggregationName, config);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java
index 5242422..3a9bd40 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java
@@ -19,12 +19,9 @@
 package org.elasticsearch.search.aggregations.metrics.min;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.NumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.MultiValueMode;
@@ -35,10 +32,9 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -117,10 +113,10 @@ public class MinAggregator extends NumericMetricsAggregator.SingleValue {
         return new InternalMin(name, Double.POSITIVE_INFINITY, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public Factory(String name) {
-            super(name, InternalMin.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalMin.TYPE.name(), valuesSourceConfig);
         }
 
         @Override
@@ -135,32 +131,6 @@ public class MinAggregator extends NumericMetricsAggregator.SingleValue {
                 throws IOException {
             return new MinAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new MinAggregator.Factory(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinBuilder.java
new file mode 100644
index 0000000..e5342de
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.min;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Min} aggregation.
+ */
+public class MinBuilder extends ValuesSourceMetricsAggregationBuilder<MinBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public MinBuilder(String name) {
+        super(name, InternalMin.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java
index 75cd9e8..e7dde9c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java
@@ -18,46 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.metrics.min;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class MinParser extends NumericValuesSourceParser {
+public class MinParser extends NumericValuesSourceMetricsAggregatorParser<InternalMin> {
 
     public MinParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalMin.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected MinAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new MinAggregator.Factory(aggregationName);
+        super(InternalMin.TYPE);
     }
 
     @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new MinAggregator.Factory(null);
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new MinAggregator.Factory(aggregationName, config);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesBuilder.java
new file mode 100644
index 0000000..c587d64
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesBuilder.java
@@ -0,0 +1,87 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.percentiles;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+import java.io.IOException;
+
+abstract class AbstractPercentilesBuilder<PB extends AbstractPercentilesBuilder<PB>> extends
+        ValuesSourceMetricsAggregationBuilder<PB> {
+
+    private Double compression;
+    private PercentilesMethod method;
+    private Integer numberOfSignificantValueDigits;
+
+    public AbstractPercentilesBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    /**
+     * Expert: Set the method to use to compute the percentiles.
+     */
+    public PB method(PercentilesMethod method) {
+        this.method = method;
+        return (PB) this;
+    }
+
+    /**
+     * Expert: set the compression. Higher values improve accuracy but also
+     * memory usage. Only relevant when using {@link PercentilesMethod#TDIGEST}.
+     */
+    public PB compression(double compression) {
+        this.compression = compression;
+        return (PB) this;
+    }
+
+    /**
+     * Expert: set the number of significant digits in the values. Only relevant
+     * when using {@link PercentilesMethod#HDR}.
+     */
+    public PB numberOfSignificantValueDigits(int numberOfSignificantValueDigits) {
+        this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
+        return (PB) this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.internalXContent(builder, params);
+
+        doInternalXContent(builder, params);
+
+        if (method != null) {
+            builder.startObject(method.getName());
+
+            if (compression != null) {
+                builder.field(AbstractPercentilesParser.COMPRESSION_FIELD.getPreferredName(), compression);
+            }
+
+            if (numberOfSignificantValueDigits != null) {
+                builder.field(AbstractPercentilesParser.NUMBER_SIGNIFICANT_DIGITS_FIELD.getPreferredName(), numberOfSignificantValueDigits);
+            }
+
+            builder.endObject();
+        }
+    }
+
+    protected abstract void doInternalXContent(XContentBuilder builder, Params params) throws IOException;
+
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java
index 707f5fb..0b30040 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java
@@ -21,19 +21,21 @@ package org.elasticsearch.search.aggregations.metrics.percentiles;
 
 import com.carrotsearch.hppc.DoubleArrayList;
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
+import java.util.Arrays;
 
-public abstract class AbstractPercentilesParser extends NumericValuesSourceParser {
+public abstract class AbstractPercentilesParser implements Aggregator.Parser {
 
     public static final ParseField KEYED_FIELD = new ParseField("keyed");
     public static final ParseField METHOD_FIELD = new ParseField("method");
@@ -43,95 +45,139 @@ public abstract class AbstractPercentilesParser extends NumericValuesSourceParse
     private boolean formattable;
 
     public AbstractPercentilesParser(boolean formattable) {
-        super(true, formattable, false);
+        this.formattable = formattable;
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.START_ARRAY) {
-            if (parseFieldMatcher.match(currentFieldName, keysField())) {
-                DoubleArrayList values = new DoubleArrayList(10);
-                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                    double value = parser.doubleValue();
-                    values.add(value);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalTDigestPercentiles.TYPE, context)
+                .formattable(formattable).build();
+
+        double[] keys = null;
+        boolean keyed = true;
+        Double compression = null;
+        Integer numberOfSignificantValueDigits = null;
+        PercentilesMethod method = null;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if (context.parseFieldMatcher().match(currentFieldName, keysField())) {
+                    DoubleArrayList values = new DoubleArrayList(10);
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        double value = parser.doubleValue();
+                        values.add(value);
+                    }
+                    keys = values.toArray();
+                    Arrays.sort(keys);
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-                double[] keys = values.toArray();
-                otherOptions.put(keysField(), keys);
-                return true;
-            } else {
-                return false;
-            }
-        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, KEYED_FIELD)) {
-                boolean keyed = parser.booleanValue();
-                otherOptions.put(KEYED_FIELD, keyed);
-                return true;
-            } else {
-                return false;
-            }
-        } else if (token == XContentParser.Token.START_OBJECT) {
-            PercentilesMethod method = PercentilesMethod.resolveFromName(currentFieldName);
-            if (method == null) {
-                return false;
-            } else {
-                otherOptions.put(METHOD_FIELD, method);
-                switch (method) {
-                case TDIGEST:
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        if (token == XContentParser.Token.FIELD_NAME) {
-                            currentFieldName = parser.currentName();
-                        } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                            if (parseFieldMatcher.match(currentFieldName, COMPRESSION_FIELD)) {
-                                double compression = parser.doubleValue();
-                                otherOptions.put(COMPRESSION_FIELD, compression);
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if (context.parseFieldMatcher().match(currentFieldName, KEYED_FIELD)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.START_OBJECT) {
+                if (method != null) {
+                    throw new SearchParseException(context, "Found multiple methods in [" + aggregationName + "]: [" + currentFieldName
+                            + "]. only one of [" + PercentilesMethod.TDIGEST.getName() + "] and [" + PercentilesMethod.HDR.getName()
+                            + "] may be used.", parser.getTokenLocation());
+                }
+                method = PercentilesMethod.resolveFromName(currentFieldName);
+                if (method == null) {
+                    throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                            parser.getTokenLocation());
+                } else {
+                    switch (method) {
+                    case TDIGEST:
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                currentFieldName = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if (context.parseFieldMatcher().match(currentFieldName, COMPRESSION_FIELD)) {
+                                    compression = parser.doubleValue();
+                                } else {
+                                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName
+                                            + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                                }
                             } else {
-                                return false;
+                                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                        + currentFieldName + "].", parser.getTokenLocation());
                             }
-                        } else {
-                            return false;
                         }
-                    }
-                    break;
-                case HDR:
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        if (token == XContentParser.Token.FIELD_NAME) {
-                            currentFieldName = parser.currentName();
-                        } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                            if (parseFieldMatcher.match(currentFieldName, NUMBER_SIGNIFICANT_DIGITS_FIELD)) {
-                                int numberOfSignificantValueDigits = parser.intValue();
-                                otherOptions.put(NUMBER_SIGNIFICANT_DIGITS_FIELD, numberOfSignificantValueDigits);
+                        break;
+                    case HDR:
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                currentFieldName = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if (context.parseFieldMatcher().match(currentFieldName, NUMBER_SIGNIFICANT_DIGITS_FIELD)) {
+                                    numberOfSignificantValueDigits = parser.intValue();
+                                } else {
+                                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName
+                                            + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                                }
                             } else {
-                                return false;
+                                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                        + currentFieldName + "].", parser.getTokenLocation());
                             }
-                        } else {
-                            return false;
                         }
+                        break;
                     }
-                    break;
                 }
-                return true;
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
-        return false;
-    }
 
-    @Override
-    protected ValuesSourceAggregatorFactory<Numeric, ?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        PercentilesMethod method = (PercentilesMethod) otherOptions.getOrDefault(METHOD_FIELD, PercentilesMethod.TDIGEST);
-
-        double[] cdfValues = (double[]) otherOptions.get(keysField());
-        Double compression = (Double) otherOptions.get(COMPRESSION_FIELD);
-        Integer numberOfSignificantValueDigits = (Integer) otherOptions.get(NUMBER_SIGNIFICANT_DIGITS_FIELD);
-        Boolean keyed = (Boolean) otherOptions.get(KEYED_FIELD);
-        return buildFactory(aggregationName, cdfValues, method, compression, numberOfSignificantValueDigits, keyed);
+        if (method == null) {
+            method = PercentilesMethod.TDIGEST;
+        }
+
+        switch (method) {
+        case TDIGEST:
+            if (numberOfSignificantValueDigits != null) {
+                throw new SearchParseException(context, "[number_of_significant_value_digits] cannot be used with method [tdigest] in ["
+                        + aggregationName + "].", parser.getTokenLocation());
+            }
+            if (compression == null) {
+                compression = 100.0;
+            }
+            break;
+        case HDR:
+            if (compression != null) {
+                throw new SearchParseException(context, "[compression] cannot be used with method [hdr] in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+            if (numberOfSignificantValueDigits == null) {
+                numberOfSignificantValueDigits = 3;
+            }
+            break;
+
+        default:
+            // Shouldn't get here but if we do, throw a parse exception for
+            // invalid method
+            throw new SearchParseException(context, "Unknown value for [" + currentFieldName + "] in [" + aggregationName + "]: [" + method
+                    + "].", parser.getTokenLocation());
+        }
+
+        return buildFactory(context, aggregationName, vsParser.config(), keys, method, compression,
+                numberOfSignificantValueDigits, keyed);
     }
 
-    protected abstract ValuesSourceAggregatorFactory<Numeric, ?> buildFactory(String aggregationName, double[] cdfValues,
-            PercentilesMethod method,
-            Double compression,
-            Integer numberOfSignificantValueDigits, Boolean keyed);
+    protected abstract AggregatorFactory buildFactory(SearchContext context, String aggregationName, ValuesSourceConfig<Numeric> config,
+            double[] cdfValues, PercentilesMethod method, Double compression, Integer numberOfSignificantValueDigits, boolean keyed);
 
     protected abstract ParseField keysField();
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksAggregatorFactory.java
deleted file mode 100644
index 27db589..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksAggregatorFactory.java
+++ /dev/null
@@ -1,230 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentileRanksAggregator;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentileRanks;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentileRanksAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory.LeafOnly;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Map;
-import java.util.Objects;
-
-public class PercentileRanksAggregatorFactory extends LeafOnly<ValuesSource.Numeric, PercentileRanksAggregatorFactory> {
-
-    private double[] values;
-    private PercentilesMethod method = PercentilesMethod.TDIGEST;
-    private int numberOfSignificantValueDigits = 3;
-    private double compression = 100.0;
-    private boolean keyed = false;
-
-    public PercentileRanksAggregatorFactory(String name) {
-        super(name, InternalTDigestPercentileRanks.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-    }
-
-    /**
-     * Set the values to compute percentiles from.
-     */
-    public PercentileRanksAggregatorFactory values(double... values) {
-        double[] sortedValues = Arrays.copyOf(values, values.length);
-        Arrays.sort(sortedValues);
-        this.values = sortedValues;
-        return this;
-    }
-
-    /**
-     * Get the values to compute percentiles from.
-     */
-    public double[] values() {
-        return values;
-    }
-
-    /**
-     * Set whether the XContent response should be keyed
-     */
-    public PercentileRanksAggregatorFactory keyed(boolean keyed) {
-        this.keyed = keyed;
-        return this;
-    }
-
-    /**
-     * Get whether the XContent response should be keyed
-     */
-    public boolean keyed() {
-        return keyed;
-    }
-
-    /**
-     * Expert: set the number of significant digits in the values. Only relevant
-     * when using {@link PercentilesMethod#HDR}.
-     */
-    public PercentileRanksAggregatorFactory numberOfSignificantValueDigits(int numberOfSignificantValueDigits) {
-        this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
-        return this;
-    }
-
-    /**
-     * Expert: get the number of significant digits in the values. Only relevant
-     * when using {@link PercentilesMethod#HDR}.
-     */
-    public int numberOfSignificantValueDigits() {
-        return numberOfSignificantValueDigits;
-    }
-
-    /**
-     * Expert: set the compression. Higher values improve accuracy but also
-     * memory usage. Only relevant when using {@link PercentilesMethod#TDIGEST}.
-     */
-    public PercentileRanksAggregatorFactory compression(double compression) {
-        this.compression = compression;
-        return this;
-    }
-
-    /**
-     * Expert: get the compression. Higher values improve accuracy but also
-     * memory usage. Only relevant when using {@link PercentilesMethod#TDIGEST}.
-     */
-    public double compression() {
-        return compression;
-    }
-
-    public PercentileRanksAggregatorFactory method(PercentilesMethod method) {
-        this.method = method;
-        return this;
-    }
-
-    public PercentilesMethod method() {
-        return method;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        switch (method) {
-        case TDIGEST:
-            return new TDigestPercentileRanksAggregator(name, null, aggregationContext, parent, values, compression, keyed,
-                    config.formatter(),
-                    pipelineAggregators, metaData);
-        case HDR:
-            return new HDRPercentileRanksAggregator(name, null, aggregationContext, parent, values, numberOfSignificantValueDigits, keyed,
-                    config.formatter(), pipelineAggregators, metaData);
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        switch (method) {
-        case TDIGEST:
-            return new TDigestPercentileRanksAggregator(name, valuesSource, aggregationContext, parent, values, compression, keyed,
-                    config.formatter(), pipelineAggregators, metaData);
-        case HDR:
-        return new HDRPercentileRanksAggregator(name, valuesSource, aggregationContext, parent, values, numberOfSignificantValueDigits,
-                keyed, config.formatter(), pipelineAggregators, metaData);
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-    }
-
-    @Override
-    protected PercentileRanksAggregatorFactory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        PercentileRanksAggregatorFactory factory = new PercentileRanksAggregatorFactory(name);
-        factory.values = in.readDoubleArray();
-        factory.keyed = in.readBoolean();
-        factory.numberOfSignificantValueDigits = in.readVInt();
-        factory.compression = in.readDouble();
-        factory.method = PercentilesMethod.TDIGEST.readFrom(in);
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        out.writeDoubleArray(values);
-        out.writeBoolean(keyed);
-        out.writeVInt(numberOfSignificantValueDigits);
-        out.writeDouble(compression);
-        method.writeTo(out);
-    }
-
-    @Override
-    protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        builder.field(PercentileRanksParser.VALUES_FIELD.getPreferredName(), values);
-        builder.field(AbstractPercentilesParser.KEYED_FIELD.getPreferredName(), keyed);
-        builder.startObject(method.getName());
-        if (method == PercentilesMethod.TDIGEST) {
-            builder.field(AbstractPercentilesParser.COMPRESSION_FIELD.getPreferredName(), compression);
-        } else {
-            builder.field(AbstractPercentilesParser.NUMBER_SIGNIFICANT_DIGITS_FIELD.getPreferredName(), numberOfSignificantValueDigits);
-        }
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        PercentileRanksAggregatorFactory other = (PercentileRanksAggregatorFactory) obj;
-        if (!Objects.equals(method, other.method)) {
-            return false;
-        }
-        boolean equalSettings = false;
-        switch (method) {
-        case HDR:
-            equalSettings = Objects.equals(numberOfSignificantValueDigits, other.numberOfSignificantValueDigits);
-            break;
-        case TDIGEST:
-            equalSettings = Objects.equals(compression, other.compression);
-            break;
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-        return equalSettings
-                && Objects.deepEquals(values, other.values)
-                && Objects.equals(keyed, other.keyed)
-                && Objects.equals(method, other.method);
-    }
-
-    @Override
-    protected int innerHashCode() {
-        switch (method) {
-        case HDR:
-            return Objects.hash(Arrays.hashCode(values), keyed, numberOfSignificantValueDigits, method);
-        case TDIGEST:
-            return Objects.hash(Arrays.hashCode(values), keyed, compression, method);
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksBuilder.java
new file mode 100644
index 0000000..abb9bc5
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksBuilder.java
@@ -0,0 +1,54 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.metrics.percentiles;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link PercentileRanks} aggregation.
+ */
+public class PercentileRanksBuilder extends AbstractPercentilesBuilder<PercentileRanksBuilder> {
+
+    private double[] values;
+
+    /**
+     * Sole constructor.
+     */
+    public PercentileRanksBuilder(String name) {
+        super(name, PercentileRanks.TYPE_NAME);
+    }
+
+    /**
+     * Set the values to compute percentiles from.
+     */
+    public PercentileRanksBuilder percentiles(double... values) {
+        this.values = values;
+        return this;
+    }
+
+    @Override
+    protected void doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+
+        if (values != null) {
+            builder.field(PercentileRanksParser.VALUES_FIELD.getPreferredName(), values);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java
index d0de4f5..51e900a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java
@@ -19,10 +19,14 @@
 package org.elasticsearch.search.aggregations.metrics.percentiles;
 
 import org.elasticsearch.common.ParseField;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentileRanksAggregator;
 import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentileRanks;
+import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentileRanksAggregator;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.internal.SearchContext;
 
 /**
  *
@@ -46,30 +50,19 @@ public class PercentileRanksParser extends AbstractPercentilesParser {
     }
 
     @Override
-    protected ValuesSourceAggregatorFactory<Numeric, ?> buildFactory(String aggregationName, double[] keys, PercentilesMethod method,
-            Double compression, Integer numberOfSignificantValueDigits, Boolean keyed) {
-        PercentileRanksAggregatorFactory factory = new PercentileRanksAggregatorFactory(aggregationName);
-        if (keys != null) {
-            factory.values(keys);
+    protected AggregatorFactory buildFactory(SearchContext context, String aggregationName, ValuesSourceConfig<Numeric> valuesSourceConfig,
+            double[] keys, PercentilesMethod method, Double compression, Integer numberOfSignificantValueDigits, boolean keyed) {
+        if (keys == null) {
+            throw new SearchParseException(context, "Missing token values in [" + aggregationName + "].", null);
         }
-        if (method != null) {
-            factory.method(method);
+        if (method == PercentilesMethod.TDIGEST) {
+            return new TDigestPercentileRanksAggregator.Factory(aggregationName, valuesSourceConfig, keys, compression, keyed);
+        } else if (method == PercentilesMethod.HDR) {
+            return new HDRPercentileRanksAggregator.Factory(aggregationName, valuesSourceConfig, keys, numberOfSignificantValueDigits,
+                    keyed);
+        } else {
+            throw new AssertionError();
         }
-        if (compression != null) {
-            factory.compression(compression);
-        }
-        if (numberOfSignificantValueDigits != null) {
-            factory.numberOfSignificantValueDigits(numberOfSignificantValueDigits);
-        }
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        return factory;
-    }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new PercentileRanksAggregatorFactory(null);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesAggregatorFactory.java
deleted file mode 100644
index 3492fb4..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesAggregatorFactory.java
+++ /dev/null
@@ -1,230 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentilesAggregator;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentilesAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory.LeafOnly;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Map;
-import java.util.Objects;
-
-public class PercentilesAggregatorFactory extends LeafOnly<ValuesSource.Numeric, PercentilesAggregatorFactory> {
-
-    private double[] percents = PercentilesParser.DEFAULT_PERCENTS;
-    private PercentilesMethod method = PercentilesMethod.TDIGEST;
-    private int numberOfSignificantValueDigits = 3;
-    private double compression = 100.0;
-    private boolean keyed = false;
-
-    public PercentilesAggregatorFactory(String name) {
-        super(name, InternalTDigestPercentiles.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-    }
-
-    /**
-     * Set the values to compute percentiles from.
-     */
-    public PercentilesAggregatorFactory percentiles(double... percents) {
-        double[] sortedPercents = Arrays.copyOf(percents, percents.length);
-        Arrays.sort(sortedPercents);
-        this.percents = sortedPercents;
-        return this;
-    }
-
-    /**
-     * Get the values to compute percentiles from.
-     */
-    public double[] percentiles() {
-        return percents;
-    }
-
-    /**
-     * Set whether the XContent response should be keyed
-     */
-    public PercentilesAggregatorFactory keyed(boolean keyed) {
-        this.keyed = keyed;
-        return this;
-    }
-
-    /**
-     * Get whether the XContent response should be keyed
-     */
-    public boolean keyed() {
-        return keyed;
-    }
-
-    /**
-     * Expert: set the number of significant digits in the values. Only relevant
-     * when using {@link PercentilesMethod#HDR}.
-     */
-    public PercentilesAggregatorFactory numberOfSignificantValueDigits(int numberOfSignificantValueDigits) {
-        this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
-        return this;
-    }
-
-    /**
-     * Expert: get the number of significant digits in the values. Only relevant
-     * when using {@link PercentilesMethod#HDR}.
-     */
-    public int numberOfSignificantValueDigits() {
-        return numberOfSignificantValueDigits;
-    }
-
-    /**
-     * Expert: set the compression. Higher values improve accuracy but also
-     * memory usage. Only relevant when using {@link PercentilesMethod#TDIGEST}.
-     */
-    public PercentilesAggregatorFactory compression(double compression) {
-        this.compression = compression;
-        return this;
-    }
-
-    /**
-     * Expert: get the compression. Higher values improve accuracy but also
-     * memory usage. Only relevant when using {@link PercentilesMethod#TDIGEST}.
-     */
-    public double compression() {
-        return compression;
-    }
-
-    public PercentilesAggregatorFactory method(PercentilesMethod method) {
-        this.method = method;
-        return this;
-    }
-
-    public PercentilesMethod method() {
-        return method;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        switch (method) {
-        case TDIGEST:
-            return new TDigestPercentilesAggregator(name, null, aggregationContext, parent, percents, compression, keyed,
-                    config.formatter(),
-                    pipelineAggregators, metaData);
-        case HDR:
-            return new HDRPercentilesAggregator(name, null, aggregationContext, parent, percents, numberOfSignificantValueDigits, keyed,
-                    config.formatter(), pipelineAggregators, metaData);
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-            throws IOException {
-        switch (method) {
-        case TDIGEST:
-            return new TDigestPercentilesAggregator(name, valuesSource, aggregationContext, parent, percents, compression, keyed,
-                    config.formatter(), pipelineAggregators, metaData);
-        case HDR:
-            return new HDRPercentilesAggregator(name, valuesSource, aggregationContext, parent, percents, numberOfSignificantValueDigits,
-                keyed, config.formatter(), pipelineAggregators, metaData);
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-    }
-
-    @Override
-    protected PercentilesAggregatorFactory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        PercentilesAggregatorFactory factory = new PercentilesAggregatorFactory(name);
-        factory.percents = in.readDoubleArray();
-        factory.keyed = in.readBoolean();
-        factory.numberOfSignificantValueDigits = in.readVInt();
-        factory.compression = in.readDouble();
-        factory.method = PercentilesMethod.TDIGEST.readFrom(in);
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        out.writeDoubleArray(percents);
-        out.writeBoolean(keyed);
-        out.writeVInt(numberOfSignificantValueDigits);
-        out.writeDouble(compression);
-        method.writeTo(out);
-    }
-
-    @Override
-    protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        builder.field(PercentilesParser.PERCENTS_FIELD.getPreferredName(), percents);
-        builder.field(AbstractPercentilesParser.KEYED_FIELD.getPreferredName(), keyed);
-        builder.startObject(method.getName());
-        if (method == PercentilesMethod.TDIGEST) {
-            builder.field(AbstractPercentilesParser.COMPRESSION_FIELD.getPreferredName(), compression);
-        } else {
-            builder.field(AbstractPercentilesParser.NUMBER_SIGNIFICANT_DIGITS_FIELD.getPreferredName(), numberOfSignificantValueDigits);
-        }
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        PercentilesAggregatorFactory other = (PercentilesAggregatorFactory) obj;
-        if (!Objects.equals(method, other.method)) {
-            return false;
-        }
-        boolean equalSettings = false;
-        switch (method) {
-        case HDR:
-            equalSettings = Objects.equals(numberOfSignificantValueDigits, other.numberOfSignificantValueDigits);
-            break;
-        case TDIGEST:
-            equalSettings = Objects.equals(compression, other.compression);
-            break;
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-        return equalSettings
-                && Objects.deepEquals(percents, other.percents)
-                && Objects.equals(keyed, other.keyed)
-                && Objects.equals(method, other.method);
-    }
-
-    @Override
-    protected int innerHashCode() {
-        switch (method) {
-        case HDR:
-            return Objects.hash(Arrays.hashCode(percents), keyed, numberOfSignificantValueDigits, method);
-        case TDIGEST:
-            return Objects.hash(Arrays.hashCode(percents), keyed, compression, method);
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesBuilder.java
new file mode 100644
index 0000000..399f9ea
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesBuilder.java
@@ -0,0 +1,60 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.metrics.percentiles;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
+
+
+/**
+ * Builder for the {@link Percentiles} aggregation.
+ */
+public class PercentilesBuilder extends AbstractPercentilesBuilder<PercentilesBuilder> {
+
+    double[] percentiles;
+    /**
+     * Sole constructor.
+     */
+    public PercentilesBuilder(String name) {
+        super(name, Percentiles.TYPE_NAME);
+    }
+
+    /**
+     * Set the percentiles to compute.
+     */
+    public PercentilesBuilder percentiles(double... percentiles) {
+        for (int i = 0; i < percentiles.length; i++) {
+            if (percentiles[i] < 0 || percentiles[i] > 100) {
+                throw new IllegalArgumentException("the percents in the percentiles aggregation [" +
+                        getName() + "] must be in the [0, 100] range");
+            }
+        }
+        this.percentiles = percentiles;
+        return this;
+    }
+
+    @Override
+    protected void doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (percentiles != null) {
+            builder.field(PercentilesParser.PERCENTS_FIELD.getPreferredName(), percentiles);
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethod.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethod.java
index 44316b6..c593b1f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethod.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethod.java
@@ -19,16 +19,11 @@
 
 package org.elasticsearch.search.aggregations.metrics.percentiles;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
 
 /**
  * An enum representing the methods for calculating percentiles
  */
-public enum PercentilesMethod implements Writeable<PercentilesMethod> {
+public enum PercentilesMethod {
     /**
      * The TDigest method for calculating percentiles
      */
@@ -51,20 +46,6 @@ public enum PercentilesMethod implements Writeable<PercentilesMethod> {
         return name;
     }
 
-    @Override
-    public PercentilesMethod readFrom(StreamInput in) throws IOException {
-        int ordinal = in.readVInt();
-        if (ordinal < 0 || ordinal >= values().length) {
-            throw new IOException("Unknown PercentilesMethod ordinal [" + ordinal + "]");
-        }
-        return values()[ordinal];
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(ordinal());
-    }
-
     /**
      * Returns the {@link PercentilesMethod} for this method name. returns
      * <code>null</code> if no {@link PercentilesMethod} exists for the name.
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java
index 4079f9e..6fbb2cc 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java
@@ -20,9 +20,12 @@ package org.elasticsearch.search.aggregations.metrics.percentiles;
 
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentilesAggregator;
 import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
+import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentilesAggregator;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.internal.SearchContext;
 
 /**
  *
@@ -35,7 +38,7 @@ public class PercentilesParser extends AbstractPercentilesParser {
         super(true);
     }
 
-    public final static double[] DEFAULT_PERCENTS = new double[] { 1, 5, 25, 50, 75, 95, 99 };
+    private final static double[] DEFAULT_PERCENTS = new double[] { 1, 5, 25, 50, 75, 95, 99 };
 
     @Override
     public String type() {
@@ -48,30 +51,18 @@ public class PercentilesParser extends AbstractPercentilesParser {
     }
 
     @Override
-    protected ValuesSourceAggregatorFactory<Numeric, ?> buildFactory(String aggregationName, double[] keys, PercentilesMethod method,
-            Double compression, Integer numberOfSignificantValueDigits, Boolean keyed) {
-        PercentilesAggregatorFactory factory = new PercentilesAggregatorFactory(aggregationName);
-        if (keys != null) {
-            factory.percentiles(keys);
+    protected AggregatorFactory buildFactory(SearchContext context, String aggregationName, ValuesSourceConfig<Numeric> valuesSourceConfig,
+            double[] keys, PercentilesMethod method, Double compression, Integer numberOfSignificantValueDigits, boolean keyed) {
+        if (keys == null) {
+            keys = DEFAULT_PERCENTS;
         }
-        if (method != null) {
-            factory.method(method);
+        if (method == PercentilesMethod.TDIGEST) {
+            return new TDigestPercentilesAggregator.Factory(aggregationName, valuesSourceConfig, keys, compression, keyed);
+        } else if (method == PercentilesMethod.HDR) {
+            return new HDRPercentilesAggregator.Factory(aggregationName, valuesSourceConfig, keys, numberOfSignificantValueDigits, keyed);
+        } else {
+            throw new AssertionError();
         }
-        if (compression != null) {
-            factory.compression(compression);
-        }
-        if (numberOfSignificantValueDigits != null) {
-            factory.numberOfSignificantValueDigits(numberOfSignificantValueDigits);
-        }
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        return factory;
-    }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new PercentilesAggregatorFactory(null);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java
index 603871e..d132fdb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java
@@ -23,7 +23,10 @@ import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -70,4 +73,34 @@ public class HDRPercentileRanksAggregator extends AbstractHDRPercentilesAggregat
             return InternalHDRPercentileRanks.percentileRank(state, Double.valueOf(name));
         }
     }
+
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
+
+        private final double[] values;
+        private final int numberOfSignificantValueDigits;
+        private final boolean keyed;
+
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig, double[] values,
+                int numberOfSignificantValueDigits, boolean keyed) {
+            super(name, InternalHDRPercentiles.TYPE.name(), valuesSourceConfig);
+            this.values = values;
+            this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
+            this.keyed = keyed;
+        }
+
+        @Override
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new HDRPercentileRanksAggregator(name, null, aggregationContext, parent, values, numberOfSignificantValueDigits, keyed,
+                    config.formatter(), pipelineAggregators, metaData);
+        }
+
+        @Override
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new HDRPercentileRanksAggregator(name, valuesSource, aggregationContext, parent, values, numberOfSignificantValueDigits,
+                    keyed, config.formatter(), pipelineAggregators, metaData);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java
index f51769d..d1c0a62 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java
@@ -21,9 +21,13 @@ package org.elasticsearch.search.aggregations.metrics.percentiles.hdr;
 import org.HdrHistogram.DoubleHistogram;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -71,4 +75,34 @@ public class HDRPercentilesAggregator extends AbstractHDRPercentilesAggregator {
                 keyed,
                 formatter, pipelineAggregators(), metaData());
     }
+
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
+
+        private final double[] percents;
+        private final int numberOfSignificantValueDigits;
+        private final boolean keyed;
+
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig, double[] percents,
+                int numberOfSignificantValueDigits, boolean keyed) {
+            super(name, InternalTDigestPercentiles.TYPE.name(), valuesSourceConfig);
+            this.percents = percents;
+            this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
+            this.keyed = keyed;
+        }
+
+        @Override
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new HDRPercentilesAggregator(name, null, aggregationContext, parent, percents, numberOfSignificantValueDigits, keyed,
+                    config.formatter(), pipelineAggregators, metaData);
+        }
+
+        @Override
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new HDRPercentilesAggregator(name, valuesSource, aggregationContext, parent, percents, numberOfSignificantValueDigits,
+                    keyed, config.formatter(), pipelineAggregators, metaData);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java
index 3db68ca..95c9f06 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java
@@ -22,7 +22,10 @@ import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -65,4 +68,34 @@ public class TDigestPercentileRanksAggregator extends AbstractTDigestPercentiles
             return InternalTDigestPercentileRanks.percentileRank(state, Double.valueOf(name));
         }
     }
+
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
+
+        private final double[] values;
+        private final double compression;
+        private final boolean keyed;
+
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig,
+                double[] values, double compression, boolean keyed) {
+            super(name, InternalTDigestPercentiles.TYPE.name(), valuesSourceConfig);
+            this.values = values;
+            this.compression = compression;
+            this.keyed = keyed;
+        }
+
+        @Override
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new TDigestPercentileRanksAggregator(name, null, aggregationContext, parent, values, compression, keyed, config.formatter(),
+                    pipelineAggregators, metaData);
+        }
+
+        @Override
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new TDigestPercentileRanksAggregator(name, valuesSource, aggregationContext, parent, values, compression, keyed,
+                    config.formatter(), pipelineAggregators, metaData);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java
index 3bf7cff..43800bf 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java
@@ -22,7 +22,10 @@ import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -65,4 +68,34 @@ public class TDigestPercentilesAggregator extends AbstractTDigestPercentilesAggr
     public InternalAggregation buildEmptyAggregation() {
         return new InternalTDigestPercentiles(name, keys, new TDigestState(compression), keyed, formatter, pipelineAggregators(), metaData());
     }
+
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
+
+        private final double[] percents;
+        private final double compression;
+        private final boolean keyed;
+
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig,
+                double[] percents, double compression, boolean keyed) {
+            super(name, InternalTDigestPercentiles.TYPE.name(), valuesSourceConfig);
+            this.percents = percents;
+            this.compression = compression;
+            this.keyed = keyed;
+        }
+
+        @Override
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new TDigestPercentilesAggregator(name, null, aggregationContext, parent, percents, compression, keyed, config.formatter(),
+                    pipelineAggregators, metaData);
+        }
+
+        @Override
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new TDigestPercentilesAggregator(name, valuesSource, aggregationContext, parent, percents, compression, keyed,
+                    config.formatter(), pipelineAggregators, metaData);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java
index 97852b4..6603c62 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java
@@ -20,9 +20,6 @@
 package org.elasticsearch.search.aggregations.metrics.scripted;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.script.ExecutableScript;
 import org.elasticsearch.script.LeafSearchScript;
 import org.elasticsearch.script.Script;
@@ -47,7 +44,6 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
-import java.util.Objects;
 
 public class ScriptedMetricAggregator extends MetricsAggregator {
 
@@ -110,7 +106,7 @@ public class ScriptedMetricAggregator extends MetricsAggregator {
         return new InternalScriptedMetric(name, null, reduceScript, pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends AggregatorFactory<Factory> {
+    public static class Factory extends AggregatorFactory {
 
         private Script initScript;
         private Script mapScript;
@@ -118,85 +114,14 @@ public class ScriptedMetricAggregator extends MetricsAggregator {
         private Script reduceScript;
         private Map<String, Object> params;
 
-        public Factory(String name) {
-            super(name, InternalScriptedMetric.TYPE);
-        }
-
-        /**
-         * Set the <tt>init</tt> script.
-         */
-        public Factory initScript(Script initScript) {
+        public Factory(String name, Script initScript, Script mapScript, Script combineScript, Script reduceScript,
+                Map<String, Object> params) {
+            super(name, InternalScriptedMetric.TYPE.name());
             this.initScript = initScript;
-            return this;
-        }
-
-        /**
-         * Get the <tt>init</tt> script.
-         */
-        public Script initScript() {
-            return initScript;
-        }
-
-        /**
-         * Set the <tt>map</tt> script.
-         */
-        public Factory mapScript(Script mapScript) {
             this.mapScript = mapScript;
-            return this;
-        }
-
-        /**
-         * Get the <tt>map</tt> script.
-         */
-        public Script mapScript() {
-            return mapScript;
-        }
-
-        /**
-         * Set the <tt>combine</tt> script.
-         */
-        public Factory combineScript(Script combineScript) {
             this.combineScript = combineScript;
-            return this;
-        }
-
-        /**
-         * Get the <tt>combine</tt> script.
-         */
-        public Script combineScript() {
-            return combineScript;
-        }
-
-        /**
-         * Set the <tt>reduce</tt> script.
-         */
-        public Factory reduceScript(Script reduceScript) {
             this.reduceScript = reduceScript;
-            return this;
-        }
-
-        /**
-         * Get the <tt>reduce</tt> script.
-         */
-        public Script reduceScript() {
-            return reduceScript;
-        }
-
-        /**
-         * Set parameters that will be available in the <tt>init</tt>,
-         * <tt>map</tt> and <tt>combine</tt> phases.
-         */
-        public Factory params(Map<String, Object> params) {
             this.params = params;
-            return this;
-        }
-
-        /**
-         * Get parameters that will be available in the <tt>init</tt>,
-         * <tt>map</tt> and <tt>combine</tt> phases.
-         */
-        public Map<String, Object> params() {
-            return params;
         }
 
         @Override
@@ -264,73 +189,6 @@ public class ScriptedMetricAggregator extends MetricsAggregator {
             return clone;
         }
 
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
-            builder.startObject();
-            if (initScript != null) {
-                builder.field(ScriptedMetricParser.INIT_SCRIPT_FIELD.getPreferredName(), initScript);
-            }
-
-            if (mapScript != null) {
-                builder.field(ScriptedMetricParser.MAP_SCRIPT_FIELD.getPreferredName(), mapScript);
-            }
-
-            if (combineScript != null) {
-                builder.field(ScriptedMetricParser.COMBINE_SCRIPT_FIELD.getPreferredName(), combineScript);
-            }
-
-            if (reduceScript != null) {
-                builder.field(ScriptedMetricParser.REDUCE_SCRIPT_FIELD.getPreferredName(), reduceScript);
-            }
-            if (params != null) {
-                builder.field(ScriptedMetricParser.PARAMS_FIELD.getPreferredName());
-                builder.map(params);
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected AggregatorFactory doReadFrom(String name, StreamInput in) throws IOException {
-            Factory factory = new Factory(name);
-            factory.initScript = in.readOptionalStreamable(Script.SUPPLIER);
-            factory.mapScript = in.readOptionalStreamable(Script.SUPPLIER);
-            factory.combineScript = in.readOptionalStreamable(Script.SUPPLIER);
-            factory.reduceScript = in.readOptionalStreamable(Script.SUPPLIER);
-            if (in.readBoolean()) {
-                factory.params = in.readMap();
-            }
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalStreamable(initScript);
-            out.writeOptionalStreamable(mapScript);
-            out.writeOptionalStreamable(combineScript);
-            out.writeOptionalStreamable(reduceScript);
-            boolean hasParams = params != null;
-            out.writeBoolean(hasParams);
-            if (hasParams) {
-                out.writeMap(params);
-            }
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(initScript, mapScript, combineScript, reduceScript, params);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(initScript, other.initScript)
-                    && Objects.equals(mapScript, other.mapScript)
-                    && Objects.equals(combineScript, other.combineScript)
-                    && Objects.equals(reduceScript, other.reduceScript)
-                    && Objects.equals(params, other.params);
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java
new file mode 100644
index 0000000..803123f
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java
@@ -0,0 +1,112 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.scripted;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.search.aggregations.metrics.MetricsAggregationBuilder;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * Builder for the {@link ScriptedMetric} aggregation.
+ */
+public class ScriptedMetricBuilder extends MetricsAggregationBuilder<ScriptedMetricBuilder> {
+
+    private Script initScript = null;
+    private Script mapScript = null;
+    private Script combineScript = null;
+    private Script reduceScript = null;
+    private Map<String, Object> params = null;
+
+    /**
+     * Sole constructor.
+     */
+    public ScriptedMetricBuilder(String name) {
+        super(name, InternalScriptedMetric.TYPE.name());
+    }
+
+    /**
+     * Set the <tt>init</tt> script.
+     */
+    public ScriptedMetricBuilder initScript(Script initScript) {
+        this.initScript = initScript;
+        return this;
+    }
+
+    /**
+     * Set the <tt>map</tt> script.
+     */
+    public ScriptedMetricBuilder mapScript(Script mapScript) {
+        this.mapScript = mapScript;
+        return this;
+    }
+
+    /**
+     * Set the <tt>combine</tt> script.
+     */
+    public ScriptedMetricBuilder combineScript(Script combineScript) {
+        this.combineScript = combineScript;
+        return this;
+    }
+
+    /**
+     * Set the <tt>reduce</tt> script.
+     */
+    public ScriptedMetricBuilder reduceScript(Script reduceScript) {
+        this.reduceScript = reduceScript;
+        return this;
+    }
+
+    /**
+     * Set parameters that will be available in the <tt>init</tt>, <tt>map</tt>
+     * and <tt>combine</tt> phases.
+     */
+    public ScriptedMetricBuilder params(Map<String, Object> params) {
+        this.params = params;
+        return this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
+
+        if (initScript != null) {
+            builder.field(ScriptedMetricParser.INIT_SCRIPT_FIELD.getPreferredName(), initScript);
+        }
+
+        if (mapScript != null) {
+            builder.field(ScriptedMetricParser.MAP_SCRIPT_FIELD.getPreferredName(), mapScript);
+        }
+
+        if (combineScript != null) {
+            builder.field(ScriptedMetricParser.COMBINE_SCRIPT_FIELD.getPreferredName(), combineScript);
+        }
+
+        if (reduceScript != null) {
+            builder.field(ScriptedMetricParser.REDUCE_SCRIPT_FIELD.getPreferredName(), reduceScript);
+        }
+        if (params != null) {
+            builder.field(ScriptedMetricParser.PARAMS_FIELD.getPreferredName());
+            builder.map(params);
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java
index 66dd65f..528078c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java
@@ -20,14 +20,14 @@
 package org.elasticsearch.search.aggregations.metrics.scripted;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptParameterParser;
 import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.HashSet;
@@ -35,7 +35,7 @@ import java.util.Map;
 import java.util.Set;
 
 public class ScriptedMetricParser implements Aggregator.Parser {
-
+    
     public static final String INIT_SCRIPT = "init_script";
     public static final String MAP_SCRIPT = "map_script";
     public static final String COMBINE_SCRIPT = "combine_script";
@@ -54,7 +54,7 @@ public class ScriptedMetricParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorFactory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         Script initScript = null;
         Script mapScript = null;
         Script combineScript = null;
@@ -87,16 +87,17 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, REDUCE_PARAMS_FIELD)) {
                   reduceParams = parser.map();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token.isValue()) {
                 if (!scriptParameterParser.token(currentFieldName, token, parser, context.parseFieldMatcher())) {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + aggregationName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
 
@@ -106,8 +107,10 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 initScript = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), params);
             }
         } else if (initScript.getParams() != null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "init_script params are not supported. Parameters for the init_script must be specified in the params field on the scripted_metric aggregator not inside the init_script object");
+            throw new SearchParseException(
+                    context,
+                    "init_script params are not supported. Parameters for the init_script must be specified in the params field on the scripted_metric aggregator not inside the init_script object",
+                    parser.getTokenLocation());
         }
 
         if (mapScript == null) { // Didn't find anything using the new API so try using the old one instead
@@ -116,8 +119,10 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 mapScript = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), params);
             }
         } else if (mapScript.getParams() != null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "map_script params are not supported. Parameters for the map_script must be specified in the params field on the scripted_metric aggregator not inside the map_script object");
+            throw new SearchParseException(
+                    context,
+                    "map_script params are not supported. Parameters for the map_script must be specified in the params field on the scripted_metric aggregator not inside the map_script object",
+                    parser.getTokenLocation());
         }
 
         if (combineScript == null) { // Didn't find anything using the new API so try using the old one instead
@@ -126,8 +131,10 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 combineScript = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), params);
             }
         } else if (combineScript.getParams() != null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "combine_script params are not supported. Parameters for the combine_script must be specified in the params field on the scripted_metric aggregator not inside the combine_script object");
+            throw new SearchParseException(
+                    context,
+                    "combine_script params are not supported. Parameters for the combine_script must be specified in the params field on the scripted_metric aggregator not inside the combine_script object",
+                    parser.getTokenLocation());
         }
 
         if (reduceScript == null) { // Didn't find anything using the new API so try using the old one instead
@@ -136,23 +143,11 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 reduceScript = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), reduceParams);
             }
         }
-
+        
         if (mapScript == null) {
-            throw new ParsingException(parser.getTokenLocation(), "map_script field is required in [" + aggregationName + "].");
+            throw new SearchParseException(context, "map_script field is required in [" + aggregationName + "].", parser.getTokenLocation());
         }
-
-        ScriptedMetricAggregator.Factory factory = new ScriptedMetricAggregator.Factory(aggregationName);
-        factory.initScript(initScript);
-        factory.mapScript(mapScript);
-        factory.combineScript(combineScript);
-        factory.reduceScript(reduceScript);
-        factory.params(params);
-        return factory;
-    }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new ScriptedMetricAggregator.Factory(null);
+        return new ScriptedMetricAggregator.Factory(aggregationName, initScript, mapScript, combineScript, reduceScript, params);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java
index 0e4e24a..6e648cb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java
@@ -19,13 +19,10 @@
 package org.elasticsearch.search.aggregations.metrics.stats;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -34,10 +31,9 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -159,10 +155,10 @@ public class StatsAggregator extends NumericMetricsAggregator.MultiValue {
         return new InternalStats(name, 0, 0, Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public Factory(String name) {
-            super(name, InternalStats.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalStats.TYPE.name(), valuesSourceConfig);
         }
 
         @Override
@@ -177,32 +173,6 @@ public class StatsAggregator extends NumericMetricsAggregator.MultiValue {
                 throws IOException {
             return new StatsAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new StatsAggregator.Factory(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsBuilder.java
new file mode 100644
index 0000000..3b6e549
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.stats;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Stats} aggregation.
+ */
+public class StatsBuilder extends ValuesSourceMetricsAggregationBuilder<StatsBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public StatsBuilder(String name) {
+        super(name, InternalStats.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java
index b0bbeca..86c85e4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java
@@ -18,45 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.metrics.stats;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class StatsParser extends NumericValuesSourceParser {
+public class StatsParser extends NumericValuesSourceMetricsAggregatorParser<InternalStats> {
 
     public StatsParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalStats.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected StatsAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new StatsAggregator.Factory(aggregationName);
+        super(InternalStats.TYPE);
     }
 
     @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new StatsAggregator.Factory(null);
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new StatsAggregator.Factory(aggregationName, config);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java
index 3ab11a7..86a6481 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java
@@ -19,14 +19,10 @@
 package org.elasticsearch.search.aggregations.metrics.stats.extended;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -35,24 +31,20 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class ExtendedStatsAggregator extends NumericMetricsAggregator.MultiValue {
 
-    public static final ParseField SIGMA_FIELD = new ParseField("sigma");
-
     final ValuesSource.Numeric valuesSource;
     final ValueFormatter formatter;
     final double sigma;
@@ -196,21 +188,14 @@ public class ExtendedStatsAggregator extends NumericMetricsAggregator.MultiValue
         Releasables.close(counts, maxes, mins, sumOfSqrs, sums);
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        private double sigma = 2.0;
+        private final double sigma;
 
-        public Factory(String name) {
-            super(name, InternalExtendedStats.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-        }
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig, double sigma) {
+            super(name, InternalExtendedStats.TYPE.name(), valuesSourceConfig);
 
-        public Factory sigma(double sigma) {
             this.sigma = sigma;
-            return this;
-        }
-
-        public double sigma() {
-            return sigma;
         }
 
         @Override
@@ -227,35 +212,5 @@ public class ExtendedStatsAggregator extends NumericMetricsAggregator.MultiValue
             return new ExtendedStatsAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, sigma,
                     pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            ExtendedStatsAggregator.Factory factory = new ExtendedStatsAggregator.Factory(name);
-            factory.sigma = in.readDouble();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDouble(sigma);
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(SIGMA_FIELD.getPreferredName(), sigma);
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(sigma);
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(sigma, other.sigma);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsBuilder.java
new file mode 100644
index 0000000..28f4d73
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsBuilder.java
@@ -0,0 +1,55 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.stats.extended;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link ExtendedStats} aggregation.
+ */
+public class ExtendedStatsBuilder extends ValuesSourceMetricsAggregationBuilder<ExtendedStatsBuilder> {
+
+    private Double sigma;
+
+    /**
+     * Sole constructor.
+     */
+    public ExtendedStatsBuilder(String name) {
+        super(name, InternalExtendedStats.TYPE.name());
+    }
+
+    public ExtendedStatsBuilder sigma(double sigma) {
+        this.sigma = sigma;
+        return this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.internalXContent(builder, params);
+
+        if (sigma != null) {
+            builder.field("sigma", sigma);
+        }
+
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java
index ebe8b84..b29ce08 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java
@@ -19,55 +19,65 @@
 package org.elasticsearch.search.aggregations.metrics.stats.extended;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  *
  */
-public class ExtendedStatsParser extends NumericValuesSourceParser {
+public class ExtendedStatsParser  implements Aggregator.Parser {
 
-    public ExtendedStatsParser() {
-        super(true, true, false);
-    }
+    static final ParseField SIGMA = new ParseField("sigma");
 
     @Override
     public String type() {
         return InternalExtendedStats.TYPE.name();
     }
 
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config, double sigma) {
+        return new ExtendedStatsAggregator.Factory(aggregationName, config, sigma);
+    }
+
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (parseFieldMatcher.match(currentFieldName, ExtendedStatsAggregator.SIGMA_FIELD)) {
-            if (token.isValue()) {
-                otherOptions.put(ExtendedStatsAggregator.SIGMA_FIELD, parser.doubleValue());
-                return true;
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalExtendedStats.TYPE, context).formattable(true)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        double sigma = 2.0;
+
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                if (context.parseFieldMatcher().match(currentFieldName, SIGMA)) {
+                    sigma = parser.doubleValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
-        return false;
-    }
 
-    @Override
-    protected ExtendedStatsAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        ExtendedStatsAggregator.Factory factory = new ExtendedStatsAggregator.Factory(aggregationName);
-        Double sigma = (Double) otherOptions.get(ExtendedStatsAggregator.SIGMA_FIELD);
-        if (sigma != null) {
-            factory.sigma(sigma);
+        if (sigma < 0) {
+            throw new SearchParseException(context, "[sigma] must not be negative. Value provided was" + sigma, parser.getTokenLocation());
         }
-        return factory;
-    }
 
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new ExtendedStatsAggregator.Factory(null);
+        return createFactory(aggregationName, vsParser.config(), sigma);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java
index 441c6be..8a6b40c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java
@@ -19,12 +19,9 @@
 package org.elasticsearch.search.aggregations.metrics.sum;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -33,11 +30,9 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -110,10 +105,10 @@ public class SumAggregator extends NumericMetricsAggregator.SingleValue {
         return new InternalSum(name, 0.0, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public Factory(String name) {
-            super(name, InternalSum.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalSum.TYPE.name(), valuesSourceConfig);
         }
 
         @Override
@@ -128,32 +123,6 @@ public class SumAggregator extends NumericMetricsAggregator.SingleValue {
                 throws IOException {
             return new SumAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected ValuesSourceAggregatorFactory<Numeric, Factory> innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new SumAggregator.Factory(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumBuilder.java
new file mode 100644
index 0000000..53e8780
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.sum;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Sum} aggregation.
+ */
+public class SumBuilder extends ValuesSourceMetricsAggregationBuilder<SumBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public SumBuilder(String name) {
+        super(name, InternalSum.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java
index 2e40ddf..b43285a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java
@@ -18,45 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.metrics.sum;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class SumParser extends NumericValuesSourceParser {
+public class SumParser extends NumericValuesSourceMetricsAggregatorParser<InternalSum> {
 
     public SumParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalSum.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected SumAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new SumAggregator.Factory(aggregationName);
+        super(InternalSum.TYPE);
     }
 
     @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new SumAggregator.Factory(null);
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new SumAggregator.Factory(aggregationName, config);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java
index 2bbb44b..82dea48 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java
@@ -30,23 +30,9 @@ import org.apache.lucene.search.TopDocsCollector;
 import org.apache.lucene.search.TopFieldCollector;
 import org.apache.lucene.search.TopFieldDocs;
 import org.apache.lucene.search.TopScoreDocCollector;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.util.LongObjectPagedHashMap;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentLocation;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.SearchScript;
 import org.elasticsearch.search.aggregations.AggregationInitializationException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
@@ -57,29 +43,15 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.MetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.builder.SearchSourceBuilder.ScriptField;
 import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsContext;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsContext.FieldDataField;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsFetchSubPhase;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
-import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.internal.InternalSearchHit;
 import org.elasticsearch.search.internal.InternalSearchHits;
 import org.elasticsearch.search.internal.SubSearchContext;
-import org.elasticsearch.search.sort.SortBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.search.sort.SortParseElement;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  */
@@ -209,598 +181,26 @@ public class TopHitsAggregator extends MetricsAggregator {
         Releasables.close(topDocsCollectors);
     }
 
-    public static class Factory extends AggregatorFactory<Factory> {
+    public static class Factory extends AggregatorFactory {
 
-        private static final SortParseElement sortParseElement = new SortParseElement();
-        private int from = 0;
-        private int size = 3;
-        private boolean explain = false;
-        private boolean version = false;
-        private boolean trackScores = false;
-        private List<BytesReference> sorts = null;
-        private HighlightBuilder highlightBuilder;
-        private List<String> fieldNames;
-        private List<String> fieldDataFields;
-        private List<ScriptField> scriptFields;
-        private FetchSourceContext fetchSourceContext;
+        private final FetchPhase fetchPhase;
+        private final SubSearchContext subSearchContext;
 
-        public Factory(String name) {
-            super(name, InternalTopHits.TYPE);
-        }
-
-        /**
-         * From index to start the search from. Defaults to <tt>0</tt>.
-         */
-        public Factory from(int from) {
-            this.from = from;
-            return this;
-        }
-
-        /**
-         * Gets the from index to start the search from.
-         **/
-        public int from() {
-            return from;
-        }
-
-        /**
-         * The number of search hits to return. Defaults to <tt>10</tt>.
-         */
-        public Factory size(int size) {
-            this.size = size;
-            return this;
-        }
-
-        /**
-         * Gets the number of search hits to return.
-         */
-        public int size() {
-            return size;
-        }
-
-        /**
-         * Adds a sort against the given field name and the sort ordering.
-         *
-         * @param name
-         *            The name of the field
-         * @param order
-         *            The sort ordering
-         */
-        public Factory sort(String name, SortOrder order) {
-            sort(SortBuilders.fieldSort(name).order(order));
-            return this;
-        }
-
-        /**
-         * Add a sort against the given field name.
-         *
-         * @param name
-         *            The name of the field to sort by
-         */
-        public Factory sort(String name) {
-            sort(SortBuilders.fieldSort(name));
-            return this;
-        }
-
-        /**
-         * Adds a sort builder.
-         */
-        public Factory sort(SortBuilder sort) {
-            try {
-                if (sorts == null) {
-                    sorts = new ArrayList<>();
-                }
-                // NORELEASE when sort has been refactored and made writeable
-                // add the sortBuilcer to the List directly instead of
-                // serialising to XContent
-                XContentBuilder builder = XContentFactory.jsonBuilder();
-                builder.startObject();
-                sort.toXContent(builder, EMPTY_PARAMS);
-                builder.endObject();
-                sorts.add(builder.bytes());
-            } catch (IOException e) {
-                throw new RuntimeException(e);
-            }
-            return this;
-        }
-
-        /**
-         * Adds a sort builder.
-         */
-        public Factory sorts(List<BytesReference> sorts) {
-            if (this.sorts == null) {
-                this.sorts = new ArrayList<>();
-            }
-            for (BytesReference sort : sorts) {
-                this.sorts.add(sort);
-            }
-            return this;
-        }
-
-        /**
-         * Gets the bytes representing the sort builders for this request.
-         */
-        public List<BytesReference> sorts() {
-            return sorts;
-        }
-
-        /**
-         * Adds highlight to perform as part of the search.
-         */
-        public Factory highlighter(HighlightBuilder highlightBuilder) {
-            this.highlightBuilder = highlightBuilder;
-            return this;
-        }
-
-        /**
-         * Gets the hightlighter builder for this request.
-         */
-        public HighlightBuilder highlighter() {
-            return highlightBuilder;
-        }
-
-        /**
-         * Indicates whether the response should contain the stored _source for
-         * every hit
-         */
-        public Factory fetchSource(boolean fetch) {
-            if (this.fetchSourceContext == null) {
-                this.fetchSourceContext = new FetchSourceContext(fetch);
-            } else {
-                this.fetchSourceContext.fetchSource(fetch);
-            }
-            return this;
-        }
-
-        /**
-         * Indicate that _source should be returned with every hit, with an
-         * "include" and/or "exclude" set which can include simple wildcard
-         * elements.
-         *
-         * @param include
-         *            An optional include (optionally wildcarded) pattern to
-         *            filter the returned _source
-         * @param exclude
-         *            An optional exclude (optionally wildcarded) pattern to
-         *            filter the returned _source
-         */
-        public Factory fetchSource(@Nullable String include, @Nullable String exclude) {
-            fetchSource(include == null ? Strings.EMPTY_ARRAY : new String[] { include },
-                    exclude == null ? Strings.EMPTY_ARRAY : new String[] { exclude });
-            return this;
-        }
-
-        /**
-         * Indicate that _source should be returned with every hit, with an
-         * "include" and/or "exclude" set which can include simple wildcard
-         * elements.
-         *
-         * @param includes
-         *            An optional list of include (optionally wildcarded)
-         *            pattern to filter the returned _source
-         * @param excludes
-         *            An optional list of exclude (optionally wildcarded)
-         *            pattern to filter the returned _source
-         */
-        public Factory fetchSource(@Nullable String[] includes, @Nullable String[] excludes) {
-            fetchSourceContext = new FetchSourceContext(includes, excludes);
-            return this;
-        }
-
-        /**
-         * Indicate how the _source should be fetched.
-         */
-        public Factory fetchSource(@Nullable FetchSourceContext fetchSourceContext) {
-            this.fetchSourceContext = fetchSourceContext;
-            return this;
-        }
-
-        /**
-         * Gets the {@link FetchSourceContext} which defines how the _source
-         * should be fetched.
-         */
-        public FetchSourceContext fetchSource() {
-            return fetchSourceContext;
-        }
-
-        /**
-         * Adds a field to load and return (note, it must be stored) as part of
-         * the search request. If none are specified, the source of the document
-         * will be return.
-         */
-        public Factory field(String name) {
-            if (fieldNames == null) {
-                fieldNames = new ArrayList<>();
-            }
-            fieldNames.add(name);
-            return this;
-        }
-
-        /**
-         * Sets the fields to load and return as part of the search request. If
-         * none are specified, the source of the document will be returned.
-         */
-        public Factory fields(List<String> fields) {
-            this.fieldNames = fields;
-            return this;
-        }
-
-        /**
-         * Sets no fields to be loaded, resulting in only id and type to be
-         * returned per field.
-         */
-        public Factory noFields() {
-            this.fieldNames = Collections.emptyList();
-            return this;
-        }
-
-        /**
-         * Gets the fields to load and return as part of the search request.
-         */
-        public List<String> fields() {
-            return fieldNames;
-        }
-
-        /**
-         * Adds a field to load from the field data cache and return as part of
-         * the search request.
-         */
-        public Factory fieldDataField(String name) {
-            if (fieldDataFields == null) {
-                fieldDataFields = new ArrayList<>();
-            }
-            fieldDataFields.add(name);
-            return this;
-        }
-
-        /**
-         * Adds fields to load from the field data cache and return as part of
-         * the search request.
-         */
-        public Factory fieldDataFields(List<String> names) {
-            if (fieldDataFields == null) {
-                fieldDataFields = new ArrayList<>();
-            }
-            fieldDataFields.addAll(names);
-            return this;
-        }
-
-        /**
-         * Gets the field-data fields.
-         */
-        public List<String> fieldDataFields() {
-            return fieldDataFields;
-        }
-
-        /**
-         * Adds a script field under the given name with the provided script.
-         *
-         * @param name
-         *            The name of the field
-         * @param script
-         *            The script
-         */
-        public Factory scriptField(String name, Script script) {
-            scriptField(name, script, false);
-            return this;
-        }
-
-        /**
-         * Adds a script field under the given name with the provided script.
-         *
-         * @param name
-         *            The name of the field
-         * @param script
-         *            The script
-         */
-        public Factory scriptField(String name, Script script, boolean ignoreFailure) {
-            if (scriptFields == null) {
-                scriptFields = new ArrayList<>();
-            }
-            scriptFields.add(new ScriptField(name, script, ignoreFailure));
-            return this;
-        }
-
-        public Factory scriptFields(List<ScriptField> scriptFields) {
-            if (this.scriptFields == null) {
-                this.scriptFields = new ArrayList<>();
-            }
-            this.scriptFields.addAll(scriptFields);
-            return this;
-        }
-
-        /**
-         * Gets the script fields.
-         */
-        public List<ScriptField> scriptFields() {
-            return scriptFields;
-        }
-
-        /**
-         * Should each {@link org.elasticsearch.search.SearchHit} be returned
-         * with an explanation of the hit (ranking).
-         */
-        public Factory explain(boolean explain) {
-            this.explain = explain;
-            return this;
-        }
-
-        /**
-         * Indicates whether each search hit will be returned with an
-         * explanation of the hit (ranking)
-         */
-        public boolean explain() {
-            return explain;
-        }
-
-        /**
-         * Should each {@link org.elasticsearch.search.SearchHit} be returned
-         * with a version associated with it.
-         */
-        public Factory version(boolean version) {
-            this.version = version;
-            return this;
-        }
-
-        /**
-         * Indicates whether the document's version will be included in the
-         * search hits.
-         */
-        public boolean version() {
-            return version;
-        }
-
-        /**
-         * Applies when sorting, and controls if scores will be tracked as well.
-         * Defaults to <tt>false</tt>.
-         */
-        public Factory trackScores(boolean trackScores) {
-            this.trackScores = trackScores;
-            return this;
-        }
-
-        /**
-         * Indicates whether scores will be tracked for this request.
-         */
-        public boolean trackScores() {
-            return trackScores;
+        public Factory(String name, FetchPhase fetchPhase, SubSearchContext subSearchContext) {
+            super(name, InternalTopHits.TYPE.name());
+            this.fetchPhase = fetchPhase;
+            this.subSearchContext = subSearchContext;
         }
 
         @Override
         public Aggregator createInternal(AggregationContext aggregationContext, Aggregator parent, boolean collectsFromSingleBucket,
                 List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-            SubSearchContext subSearchContext = new SubSearchContext(aggregationContext.searchContext());
-            subSearchContext.explain(explain);
-            subSearchContext.version(version);
-            subSearchContext.trackScores(trackScores);
-            subSearchContext.from(from);
-            subSearchContext.size(size);
-            if (sorts != null) {
-                XContentParser completeSortParser = null;
-                try {
-                    XContentBuilder completeSortBuilder = XContentFactory.jsonBuilder();
-                    completeSortBuilder.startObject();
-                    completeSortBuilder.startArray("sort");
-                    for (BytesReference sort : sorts) {
-                        XContentParser parser = XContentFactory.xContent(sort).createParser(sort);
-                        parser.nextToken();
-                        completeSortBuilder.copyCurrentStructure(parser);
-                    }
-                    completeSortBuilder.endArray();
-                    completeSortBuilder.endObject();
-                    BytesReference completeSortBytes = completeSortBuilder.bytes();
-                    completeSortParser = XContentFactory.xContent(completeSortBytes).createParser(completeSortBytes);
-                    completeSortParser.nextToken();
-                    completeSortParser.nextToken();
-                    completeSortParser.nextToken();
-                    sortParseElement.parse(completeSortParser, subSearchContext);
-                } catch (Exception e) {
-                    XContentLocation location = completeSortParser != null ? completeSortParser.getTokenLocation() : null;
-                    throw new ParsingException(location, "failed to parse sort source in aggregation [" + name + "]", e);
-                }
-            }
-            if (fieldNames != null) {
-                subSearchContext.fieldNames().addAll(fieldNames);
-            }
-            if (fieldDataFields != null) {
-                FieldDataFieldsContext fieldDataFieldsContext = subSearchContext
-                        .getFetchSubPhaseContext(FieldDataFieldsFetchSubPhase.CONTEXT_FACTORY);
-                for (String field : fieldDataFields) {
-                    fieldDataFieldsContext.add(new FieldDataField(field));
-                }
-                fieldDataFieldsContext.setHitExecutionNeeded(true);
-            }
-            if (scriptFields != null) {
-                for (ScriptField field : scriptFields) {
-                    SearchScript searchScript = subSearchContext.scriptService().search(subSearchContext.lookup(), field.script(),
-                            ScriptContext.Standard.SEARCH, Collections.emptyMap());
-                    subSearchContext.scriptFields().add(new org.elasticsearch.search.fetch.script.ScriptFieldsContext.ScriptField(
-                            field.fieldName(), searchScript, field.ignoreFailure()));
-                }
-            }
-            if (fetchSourceContext != null) {
-                subSearchContext.fetchSourceContext(fetchSourceContext);
-            }
-            if (highlightBuilder != null) {
-                subSearchContext.highlight(highlightBuilder.build(aggregationContext.searchContext().indexShard().getQueryShardContext()));
-            }
-            return new TopHitsAggregator(aggregationContext.searchContext().fetchPhase(), subSearchContext, name, aggregationContext,
-                    parent, pipelineAggregators, metaData);
+            return new TopHitsAggregator(fetchPhase, subSearchContext, name, aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        public Factory subFactories(AggregatorFactories subFactories) {
+        public AggregatorFactory subFactories(AggregatorFactories subFactories) {
             throw new AggregationInitializationException("Aggregator [" + name + "] of type [" + type + "] cannot accept sub-aggregations");
         }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            builder.field(SearchSourceBuilder.FROM_FIELD.getPreferredName(), from);
-            builder.field(SearchSourceBuilder.SIZE_FIELD.getPreferredName(), size);
-            builder.field(SearchSourceBuilder.VERSION_FIELD.getPreferredName(), version);
-            builder.field(SearchSourceBuilder.EXPLAIN_FIELD.getPreferredName(), explain);
-            if (fetchSourceContext != null) {
-                builder.field(SearchSourceBuilder._SOURCE_FIELD.getPreferredName(), fetchSourceContext);
-            }
-            if (fieldNames != null) {
-                if (fieldNames.size() == 1) {
-                    builder.field(SearchSourceBuilder.FIELDS_FIELD.getPreferredName(), fieldNames.get(0));
-                } else {
-                    builder.startArray(SearchSourceBuilder.FIELDS_FIELD.getPreferredName());
-                    for (String fieldName : fieldNames) {
-                        builder.value(fieldName);
-                    }
-                    builder.endArray();
-                }
-            }
-            if (fieldDataFields != null) {
-                builder.startArray(SearchSourceBuilder.FIELDDATA_FIELDS_FIELD.getPreferredName());
-                for (String fieldDataField : fieldDataFields) {
-                    builder.value(fieldDataField);
-                }
-                builder.endArray();
-            }
-            if (scriptFields != null) {
-                builder.startObject(SearchSourceBuilder.SCRIPT_FIELDS_FIELD.getPreferredName());
-                for (ScriptField scriptField : scriptFields) {
-                    scriptField.toXContent(builder, params);
-                }
-                builder.endObject();
-            }
-            if (sorts != null) {
-                builder.startArray(SearchSourceBuilder.SORT_FIELD.getPreferredName());
-                for (BytesReference sort : sorts) {
-                    XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(sort);
-                    parser.nextToken();
-                    builder.copyCurrentStructure(parser);
-                }
-                builder.endArray();
-            }
-            if (trackScores) {
-                builder.field(SearchSourceBuilder.TRACK_SCORES_FIELD.getPreferredName(), true);
-            }
-            if (highlightBuilder != null) {
-                this.highlightBuilder.toXContent(builder, params);
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected AggregatorFactory doReadFrom(String name, StreamInput in) throws IOException {
-            Factory factory = new Factory(name);
-            factory.explain = in.readBoolean();
-            factory.fetchSourceContext = FetchSourceContext.optionalReadFromStream(in);
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<String> fieldDataFields = new ArrayList<>(size);
-                for (int i = 0; i < size; i++) {
-                    fieldDataFields.add(in.readString());
-                }
-                factory.fieldDataFields = fieldDataFields;
-            }
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<String> fieldNames = new ArrayList<>(size);
-                for (int i = 0; i < size; i++) {
-                    fieldNames.add(in.readString());
-                }
-                factory.fieldNames = fieldNames;
-            }
-            factory.from = in.readVInt();
-            if (in.readBoolean()) {
-                factory.highlightBuilder = HighlightBuilder.PROTOTYPE.readFrom(in);
-            }
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<ScriptField> scriptFields = new ArrayList<>(size);
-                for (int i = 0; i < size; i++) {
-                    scriptFields.add(ScriptField.PROTOTYPE.readFrom(in));
-                }
-                factory.scriptFields = scriptFields;
-            }
-            factory.size = in.readVInt();
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<BytesReference> sorts = new ArrayList<>();
-                for (int i = 0; i < size; i++) {
-                    sorts.add(in.readBytesReference());
-                }
-                factory.sorts = sorts;
-            }
-            factory.trackScores = in.readBoolean();
-            factory.version = in.readBoolean();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeBoolean(explain);
-            FetchSourceContext.optionalWriteToStream(fetchSourceContext, out);
-            boolean hasFieldDataFields = fieldDataFields != null;
-            out.writeBoolean(hasFieldDataFields);
-            if (hasFieldDataFields) {
-                out.writeVInt(fieldDataFields.size());
-                for (String fieldName : fieldDataFields) {
-                    out.writeString(fieldName);
-                }
-            }
-            boolean hasFieldNames = fieldNames != null;
-            out.writeBoolean(hasFieldNames);
-            if (hasFieldNames) {
-                out.writeVInt(fieldNames.size());
-                for (String fieldName : fieldNames) {
-                    out.writeString(fieldName);
-                }
-            }
-            out.writeVInt(from);
-            boolean hasHighlighter = highlightBuilder != null;
-            out.writeBoolean(hasHighlighter);
-            if (hasHighlighter) {
-                highlightBuilder.writeTo(out);
-            }
-            boolean hasScriptFields = scriptFields != null;
-            out.writeBoolean(hasScriptFields);
-            if (hasScriptFields) {
-                out.writeVInt(scriptFields.size());
-                for (ScriptField scriptField : scriptFields) {
-                    scriptField.writeTo(out);
-                }
-            }
-            out.writeVInt(size);
-            boolean hasSorts = sorts != null;
-            out.writeBoolean(hasSorts);
-            if (hasSorts) {
-                out.writeVInt(sorts.size());
-                for (BytesReference sort : sorts) {
-                    out.writeBytesReference(sort);
-                }
-            }
-            out.writeBoolean(trackScores);
-            out.writeBoolean(version);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(explain, fetchSourceContext, fieldDataFields, fieldNames, from, highlightBuilder, scriptFields, size, sorts,
-                    trackScores, version);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(explain, other.explain)
-                    && Objects.equals(fetchSourceContext, other.fetchSourceContext)
-                    && Objects.equals(fieldDataFields, other.fieldDataFields)
-                    && Objects.equals(fieldNames, other.fieldNames)
-                    && Objects.equals(from, other.from)
-                    && Objects.equals(highlightBuilder, other.highlightBuilder)
-                    && Objects.equals(scriptFields, other.scriptFields)
-                    && Objects.equals(size, other.size)
-                    && Objects.equals(sorts, other.sorts)
-                    && Objects.equals(trackScores, other.trackScores)
-                    && Objects.equals(version, other.version);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
new file mode 100644
index 0000000..1efd4a7
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
@@ -0,0 +1,204 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.metrics.tophits;
+
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.search.highlight.HighlightBuilder;
+import org.elasticsearch.search.sort.SortBuilder;
+import org.elasticsearch.search.sort.SortOrder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link TopHits} aggregation.
+ */
+public class TopHitsBuilder extends AbstractAggregationBuilder {
+
+    private SearchSourceBuilder sourceBuilder;
+
+    /**
+     * Sole constructor.
+     */
+    public TopHitsBuilder(String name) {
+        super(name, InternalTopHits.TYPE.name());
+    }
+
+    /**
+     * The index to start to return hits from. Defaults to <tt>0</tt>.
+     */
+    public TopHitsBuilder setFrom(int from) {
+        sourceBuilder().from(from);
+        return this;
+    }
+
+
+    /**
+     * The number of search hits to return. Defaults to <tt>10</tt>.
+     */
+    public TopHitsBuilder setSize(int size) {
+        sourceBuilder().size(size);
+        return this;
+    }
+
+    /**
+     * Applies when sorting, and controls if scores will be tracked as well. Defaults to
+     * <tt>false</tt>.
+     */
+    public TopHitsBuilder setTrackScores(boolean trackScores) {
+        sourceBuilder().trackScores(trackScores);
+        return this;
+    }
+
+    /**
+     * Should each {@link org.elasticsearch.search.SearchHit} be returned with an
+     * explanation of the hit (ranking).
+     */
+    public TopHitsBuilder setExplain(boolean explain) {
+        sourceBuilder().explain(explain);
+        return this;
+    }
+
+    /**
+     * Should each {@link org.elasticsearch.search.SearchHit} be returned with its
+     * version.
+     */
+    public TopHitsBuilder setVersion(boolean version) {
+        sourceBuilder().version(version);
+        return this;
+    }
+
+    /**
+     * Adds a field to loaded and returned.
+     */
+    public TopHitsBuilder addField(String field) {
+        sourceBuilder().field(field);
+        return this;
+    }
+
+    /**
+     * Sets no fields to be loaded, resulting in only id and type to be returned per field.
+     */
+    public TopHitsBuilder setNoFields() {
+        sourceBuilder().noFields();
+        return this;
+    }
+
+    /**
+     * Indicates whether the response should contain the stored _source for every hit
+     */
+    public TopHitsBuilder setFetchSource(boolean fetch) {
+        sourceBuilder().fetchSource(fetch);
+        return this;
+    }
+
+    /**
+     * Indicate that _source should be returned with every hit, with an "include" and/or "exclude" set which can include simple wildcard
+     * elements.
+     *
+     * @param include An optional include (optionally wildcarded) pattern to filter the returned _source
+     * @param exclude An optional exclude (optionally wildcarded) pattern to filter the returned _source
+     */
+    public TopHitsBuilder setFetchSource(@Nullable String include, @Nullable String exclude) {
+        sourceBuilder().fetchSource(include, exclude);
+        return this;
+    }
+
+    /**
+     * Indicate that _source should be returned with every hit, with an "include" and/or "exclude" set which can include simple wildcard
+     * elements.
+     *
+     * @param includes An optional list of include (optionally wildcarded) pattern to filter the returned _source
+     * @param excludes An optional list of exclude (optionally wildcarded) pattern to filter the returned _source
+     */
+    public TopHitsBuilder setFetchSource(@Nullable String[] includes, @Nullable String[] excludes) {
+        sourceBuilder().fetchSource(includes, excludes);
+        return this;
+    }
+
+    /**
+     * Adds a field data based field to load and return. The field does not have to be stored,
+     * but its recommended to use non analyzed or numeric fields.
+     *
+     * @param name The field to get from the field data cache
+     */
+    public TopHitsBuilder addFieldDataField(String name) {
+        sourceBuilder().fieldDataField(name);
+        return this;
+    }
+
+    /**
+     * Adds a script based field to load and return. The field does not have to be stored,
+     * but its recommended to use non analyzed or numeric fields.
+     *
+     * @param name   The name that will represent this value in the return hit
+     * @param script The script to use
+     */
+    public TopHitsBuilder addScriptField(String name, Script script) {
+        sourceBuilder().scriptField(name, script);
+        return this;
+    }
+
+    /**
+     * Adds a sort against the given field name and the sort ordering.
+     *
+     * @param field The name of the field
+     * @param order The sort ordering
+     */
+    public TopHitsBuilder addSort(String field, SortOrder order) {
+        sourceBuilder().sort(field, order);
+        return this;
+    }
+
+    /**
+     * Adds a generic sort builder.
+     *
+     * @see org.elasticsearch.search.sort.SortBuilders
+     */
+    public TopHitsBuilder addSort(SortBuilder sort) {
+        sourceBuilder().sort(sort);
+        return this;
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(getName()).field(type);
+        sourceBuilder().toXContent(builder, params);
+        return builder.endObject();
+    }
+
+    private SearchSourceBuilder sourceBuilder() {
+        if (sourceBuilder == null) {
+            sourceBuilder = new SearchSourceBuilder();
+        }
+        return sourceBuilder;
+    }
+
+    public HighlightBuilder highlighter() {
+        return sourceBuilder().highlighter();
+    }
+
+    public TopHitsBuilder highlighter(HighlightBuilder highlightBuilder) {
+        sourceBuilder().highlighter(highlightBuilder);
+        return this;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java
index 7f03816..50b3482 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java
@@ -18,36 +18,30 @@
  */
 package org.elasticsearch.search.aggregations.metrics.tophits;
 
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.script.Script;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.builder.SearchSourceBuilder.ScriptField;
+import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FieldsParseElement;
 import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
 import org.elasticsearch.search.fetch.script.ScriptFieldsParseElement;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
 import org.elasticsearch.search.fetch.source.FetchSourceParseElement;
-import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.highlight.HighlighterParseElement;
+import org.elasticsearch.search.internal.SearchContext;
+import org.elasticsearch.search.internal.SubSearchContext;
 import org.elasticsearch.search.sort.SortParseElement;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
 
 /**
  *
  */
 public class TopHitsParser implements Aggregator.Parser {
 
+    private final FetchPhase fetchPhase;
     private final SortParseElement sortParseElement;
     private final FetchSourceParseElement sourceParseElement;
     private final HighlighterParseElement highlighterParseElement;
@@ -56,9 +50,10 @@ public class TopHitsParser implements Aggregator.Parser {
     private final FieldsParseElement fieldsParseElement;
 
     @Inject
-    public TopHitsParser(SortParseElement sortParseElement, FetchSourceParseElement sourceParseElement,
+    public TopHitsParser(FetchPhase fetchPhase, SortParseElement sortParseElement, FetchSourceParseElement sourceParseElement,
             HighlighterParseElement highlighterParseElement, FieldDataFieldsParseElement fieldDataFieldsParseElement,
             ScriptFieldsParseElement scriptFieldsParseElement, FieldsParseElement fieldsParseElement) {
+        this.fetchPhase = fetchPhase;
         this.sortParseElement = sortParseElement;
         this.sourceParseElement = sourceParseElement;
         this.highlighterParseElement = highlighterParseElement;
@@ -73,140 +68,74 @@ public class TopHitsParser implements Aggregator.Parser {
     }
 
     @Override
-    public TopHitsAggregator.Factory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
-        TopHitsAggregator.Factory factory = new TopHitsAggregator.Factory(aggregationName);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        SubSearchContext subSearchContext = new SubSearchContext(context);
         XContentParser.Token token;
         String currentFieldName = null;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token.isValue()) {
-                if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.FROM_FIELD)) {
-                    factory.from(parser.intValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SIZE_FIELD)) {
-                    factory.size(parser.intValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.VERSION_FIELD)) {
-                    factory.version(parser.booleanValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.EXPLAIN_FIELD)) {
-                    factory.explain(parser.booleanValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.TRACK_SCORES_FIELD)) {
-                    factory.trackScores(parser.booleanValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder._SOURCE_FIELD)) {
-                    factory.fetchSource(FetchSourceContext.parse(parser, context));
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.FIELDS_FIELD)) {
-                    List<String> fieldNames = new ArrayList<>();
-                    fieldNames.add(parser.text());
-                    factory.fields(fieldNames);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SORT_FIELD)) {
-                    factory.sort(parser.text());
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else if (token == XContentParser.Token.START_OBJECT) {
-                if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder._SOURCE_FIELD)) {
-                    factory.fetchSource(FetchSourceContext.parse(parser, context));
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SCRIPT_FIELDS_FIELD)) {
-                    List<ScriptField> scriptFields = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        String scriptFieldName = parser.currentName();
-                        token = parser.nextToken();
-                        if (token == XContentParser.Token.START_OBJECT) {
-                            Script script = null;
-                            boolean ignoreFailure = false;
-                            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                                if (token == XContentParser.Token.FIELD_NAME) {
-                                    currentFieldName = parser.currentName();
-                                } else if (token.isValue()) {
-                                    if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SCRIPT_FIELD)) {
-                                        script = Script.parse(parser, context.parseFieldMatcher());
-                                    } else if (context.parseFieldMatcher().match(currentFieldName,
-                                            SearchSourceBuilder.IGNORE_FAILURE_FIELD)) {
-                                        ignoreFailure = parser.booleanValue();
-                                    } else {
-                                        throw new ParsingException(parser.getTokenLocation(),
-                                                "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                                parser.getTokenLocation());
-                                    }
-                                } else if (token == XContentParser.Token.START_OBJECT) {
-                                    if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SCRIPT_FIELD)) {
-                                        script = Script.parse(parser, context.parseFieldMatcher());
-                                    } else {
-                                        throw new ParsingException(parser.getTokenLocation(),
-                                                "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                                parser.getTokenLocation());
-                                    }
-                                } else {
-                                    throw new ParsingException(parser.getTokenLocation(),
-                                            "Unknown key for a " + token + " in [" + currentFieldName + "].", parser.getTokenLocation());
-                                }
-                            }
-                            scriptFields.add(new ScriptField(scriptFieldName, script, ignoreFailure));
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.START_OBJECT
-                                    + "] in [" + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
+        try {
+            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                if (token == XContentParser.Token.FIELD_NAME) {
+                    currentFieldName = parser.currentName();
+                } else if ("sort".equals(currentFieldName)) {
+                    sortParseElement.parse(parser, subSearchContext);
+                } else if ("_source".equals(currentFieldName)) {
+                    sourceParseElement.parse(parser, subSearchContext);
+                } else if ("fields".equals(currentFieldName)) {
+                    fieldsParseElement.parse(parser, subSearchContext);
+                } else if (token.isValue()) {
+                    switch (currentFieldName) {
+                        case "from":
+                            subSearchContext.from(parser.intValue());
+                            break;
+                        case "size":
+                            subSearchContext.size(parser.intValue());
+                            break;
+                        case "track_scores":
+                        case "trackScores":
+                            subSearchContext.trackScores(parser.booleanValue());
+                            break;
+                        case "version":
+                            subSearchContext.version(parser.booleanValue());
+                            break;
+                        case "explain":
+                            subSearchContext.explain(parser.booleanValue());
+                            break;
+                        default:
+                        throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                + currentFieldName + "].", parser.getTokenLocation());
                     }
-                    factory.scriptFields(scriptFields);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.HIGHLIGHT_FIELD)) {
-                    factory.highlighter(HighlightBuilder.PROTOTYPE.fromXContent(context));
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SORT_FIELD)) {
-                    List<BytesReference> sorts = new ArrayList<>();
-                    XContentBuilder xContentBuilder = XContentFactory.jsonBuilder().copyCurrentStructure(parser);
-                    sorts.add(xContentBuilder.bytes());
-                    factory.sorts(sorts);
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else if (token == XContentParser.Token.START_ARRAY) {
-
-                if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.FIELDS_FIELD)) {
-                    List<String> fieldNames = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            fieldNames.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING
-                                    + "] in [" + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
-                    }
-                    factory.fields(fieldNames);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.FIELDDATA_FIELDS_FIELD)) {
-                    List<String> fieldDataFields = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            fieldDataFields.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING
-                                    + "] in [" + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
+                } else if (token == XContentParser.Token.START_OBJECT) {
+                    switch (currentFieldName) {
+                        case "highlight":
+                            highlighterParseElement.parse(parser, subSearchContext);
+                            break;
+                        case "scriptFields":
+                        case "script_fields":
+                            scriptFieldsParseElement.parse(parser, subSearchContext);
+                            break;
+                        default:
+                        throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                + currentFieldName + "].", parser.getTokenLocation());
                     }
-                    factory.fieldDataFields(fieldDataFields);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SORT_FIELD)) {
-                    List<BytesReference> sorts = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        XContentBuilder xContentBuilder = XContentFactory.jsonBuilder().copyCurrentStructure(parser);
-                        sorts.add(xContentBuilder.bytes());
+                } else if (token == XContentParser.Token.START_ARRAY) {
+                    switch (currentFieldName) {
+                        case "fielddataFields":
+                        case "fielddata_fields":
+                            fieldDataFieldsParseElement.parse(parser, subSearchContext);
+                            break;
+                        default:
+                        throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                + currentFieldName + "].", parser.getTokenLocation());
                     }
-                    factory.sorts(sorts);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder._SOURCE_FIELD)) {
-                    factory.fetchSource(FetchSourceContext.parse(parser, context));
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
+                    throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
                             parser.getTokenLocation());
                 }
-            } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                        parser.getTokenLocation());
             }
+        } catch (Exception e) {
+            throw ExceptionsHelper.convertToElastic(e);
         }
-        return factory;
-    }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new TopHitsAggregator.Factory(null);
+        return new TopHitsAggregator.Factory(aggregationName, fetchPhase, subSearchContext);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java
index 3a3a413..867d876 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java
@@ -19,12 +19,9 @@
 package org.elasticsearch.search.aggregations.metrics.valuecount;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedBinaryDocValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -33,10 +30,9 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -112,10 +108,10 @@ public class ValueCountAggregator extends NumericMetricsAggregator.SingleValue {
         Releasables.close(counts);
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource, Factory> {
+    public static class Factory<VS extends ValuesSource> extends ValuesSourceAggregatorFactory.LeafOnly<VS> {
 
-        public Factory(String name, ValueType targetValueType) {
-            super(name, InternalValueCount.TYPE, ValuesSourceType.ANY, targetValueType);
+        public Factory(String name, ValuesSourceConfig<VS> config) {
+            super(name, InternalValueCount.TYPE.name(), config);
         }
 
         @Override
@@ -125,39 +121,13 @@ public class ValueCountAggregator extends NumericMetricsAggregator.SingleValue {
         }
 
         @Override
-        protected Aggregator doCreateInternal(ValuesSource valuesSource, AggregationContext aggregationContext, Aggregator parent,
+        protected Aggregator doCreateInternal(VS valuesSource, AggregationContext aggregationContext, Aggregator parent,
                 boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
                 throws IOException {
             return new ValueCountAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators,
                     metaData);
         }
 
-        @Override
-        protected ValuesSourceAggregatorFactory<ValuesSource, Factory> innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new ValueCountAggregator.Factory(name, targetValueType);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountBuilder.java
new file mode 100644
index 0000000..1d37914
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountBuilder.java
@@ -0,0 +1,36 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.valuecount;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link ValueCount} aggregation.
+ */
+public class ValueCountBuilder extends ValuesSourceMetricsAggregationBuilder<ValueCountBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public ValueCountBuilder(String name) {
+        super(name, InternalValueCount.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java
index 1ea0765..764f6ce 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java
@@ -18,27 +18,19 @@
  */
 package org.elasticsearch.search.aggregations.metrics.valuecount;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  *
  */
-public class ValueCountParser extends AnyValuesSourceParser {
-
-    public ValueCountParser() {
-        super(true, true);
-    }
+public class ValueCountParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -46,19 +38,22 @@ public class ValueCountParser extends AnyValuesSourceParser {
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected ValuesSourceAggregatorFactory<ValuesSource, ValueCountAggregator.Factory> createFactory(String aggregationName,
-            ValuesSourceType valuesSourceType, ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new ValueCountAggregator.Factory(aggregationName, targetValueType);
-    }
-
-    @Override
-    public AggregatorFactory<?> getFactoryPrototypes() {
-        return new ValueCountAggregator.Factory(null, null);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, InternalValueCount.TYPE, context)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (!vsParser.token(currentFieldName, token, parser)) {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+        }
+
+        return new ValueCountAggregator.Factory(aggregationName, vsParser.config());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java
index 24d2913..881a8e4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java
@@ -20,17 +20,17 @@
 package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentLocation;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;
 import org.elasticsearch.search.aggregations.InvalidAggregationPathException;
 import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;
 import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativeParser;
 import org.elasticsearch.search.aggregations.support.AggregationPath;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -62,7 +62,7 @@ public class BucketHelpers {
          * @param text    GapPolicy in string format (e.g. "ignore")
          * @return        GapPolicy enum
          */
-        public static GapPolicy parse(QueryParseContext context, String text, XContentLocation tokenLocation) {
+        public static GapPolicy parse(SearchContext context, String text, XContentLocation tokenLocation) {
             GapPolicy result = null;
             for (GapPolicy policy : values()) {
                 if (context.parseFieldMatcher().match(text, policy.parseField)) {
@@ -79,7 +79,7 @@ public class BucketHelpers {
                 for (GapPolicy policy : values()) {
                     validNames.add(policy.getName());
                 }
-                throw new ParsingException(tokenLocation, "Invalid gap policy: [" + text + "], accepted values: " + validNames);
+                throw new SearchParseException(context, "Invalid gap policy: [" + text + "], accepted values: " + validNames, tokenLocation);
             }
             return result;
         }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java
index a3c1805..b2ee037 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java
@@ -25,10 +25,10 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.Map;
@@ -38,7 +38,7 @@ public abstract class PipelineAggregator implements Streamable {
     /**
      * Parses the pipeline aggregation request and creates the appropriate
      * pipeline aggregator factory for it.
-     *
+     * 
      * @see PipelineAggregatorFactory
      */
     public static interface Parser {
@@ -56,7 +56,7 @@ public abstract class PipelineAggregator implements Streamable {
         /**
          * Returns the pipeline aggregator factory with which this parser is
          * associated.
-         *
+         * 
          * @param pipelineAggregatorName
          *            The name of the pipeline aggregation
          * @param parser
@@ -67,13 +67,7 @@ public abstract class PipelineAggregator implements Streamable {
          * @throws java.io.IOException
          *             When parsing fails
          */
-        PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context) throws IOException;
-
-        /**
-         * @return an empty {@link PipelineAggregatorFactory} instance for this
-         *         parser that can be used for deserialization
-         */
-        PipelineAggregatorFactory getFactoryPrototype();
+        PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException;
 
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilder.java
new file mode 100644
index 0000000..56ae321
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilder.java
@@ -0,0 +1,84 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * A base class for all pipeline aggregator builders.
+ */
+public abstract class PipelineAggregatorBuilder<B extends PipelineAggregatorBuilder<B>> extends AbstractAggregationBuilder {
+
+    private String[] bucketsPaths;
+    private Map<String, Object> metaData;
+
+    /**
+     * Sole constructor, typically used by sub-classes.
+     */
+    protected PipelineAggregatorBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    /**
+     * Sets the paths to the buckets to use for this pipeline aggregator
+     */
+    public B setBucketsPaths(String... bucketsPaths) {
+        this.bucketsPaths = bucketsPaths;
+        return (B) this;
+    }
+
+    /**
+     * Sets the meta data to be included in the pipeline aggregator's response
+     */
+    public B setMetaData(Map<String, Object> metaData) {
+        this.metaData = metaData;
+        return (B)this;
+    }
+
+    @Override
+    public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(getName());
+
+        if (this.metaData != null) {
+            builder.field("meta", this.metaData);
+        }
+        builder.startObject(type);
+
+        if (bucketsPaths != null) {
+            builder.startArray(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName());
+            for (String path : bucketsPaths) {
+                builder.value(path);
+            }
+            builder.endArray();
+        }
+
+        internalXContent(builder, params);
+
+        builder.endObject();
+
+        return builder.endObject();
+    }
+
+    protected abstract XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException;
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilders.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilders.java
index adaba99..6fbc6f8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilders.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilders.java
@@ -19,87 +19,74 @@
 
 package org.elasticsearch.search.aggregations.pipeline;
 
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg.AvgBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max.MaxBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.StatsBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended.ExtendedStatsBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum.SumBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.cumulativesum.CumulativeSumPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativePipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.having.BucketSelectorPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.movavg.MovAvgPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.serialdiff.SerialDiffPipelineAggregator;
-
-import java.util.Map;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg.AvgBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max.MaxBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.StatsBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended.ExtendedStatsBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum.SumBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptBuilder;
+import org.elasticsearch.search.aggregations.pipeline.cumulativesum.CumulativeSumBuilder;
+import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativeBuilder;
+import org.elasticsearch.search.aggregations.pipeline.having.BucketSelectorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.movavg.MovAvgBuilder;
+import org.elasticsearch.search.aggregations.pipeline.serialdiff.SerialDiffBuilder;
 
 public final class PipelineAggregatorBuilders {
 
     private PipelineAggregatorBuilders() {
     }
 
-    public static final DerivativePipelineAggregator.Factory derivative(String name, String bucketsPath) {
-        return new DerivativePipelineAggregator.Factory(name, bucketsPath);
+    public static final DerivativeBuilder derivative(String name) {
+        return new DerivativeBuilder(name);
     }
 
-    public static final MaxBucketPipelineAggregator.Factory maxBucket(String name, String bucketsPath) {
-        return new MaxBucketPipelineAggregator.Factory(name, bucketsPath);
+    public static final MaxBucketBuilder maxBucket(String name) {
+        return new MaxBucketBuilder(name);
     }
 
-    public static final MinBucketPipelineAggregator.Factory minBucket(String name, String bucketsPath) {
-        return new MinBucketPipelineAggregator.Factory(name, bucketsPath);
+    public static final MinBucketBuilder minBucket(String name) {
+        return new MinBucketBuilder(name);
     }
 
-    public static final AvgBucketPipelineAggregator.Factory avgBucket(String name, String bucketsPath) {
-        return new AvgBucketPipelineAggregator.Factory(name, bucketsPath);
+    public static final AvgBucketBuilder avgBucket(String name) {
+        return new AvgBucketBuilder(name);
     }
 
-    public static final SumBucketPipelineAggregator.Factory sumBucket(String name, String bucketsPath) {
-        return new SumBucketPipelineAggregator.Factory(name, bucketsPath);
+    public static final SumBucketBuilder sumBucket(String name) {
+        return new SumBucketBuilder(name);
     }
 
-    public static final StatsBucketPipelineAggregator.Factory statsBucket(String name, String bucketsPath) {
-        return new StatsBucketPipelineAggregator.Factory(name, bucketsPath);
+    public static final StatsBucketBuilder statsBucket(String name) {
+        return new StatsBucketBuilder(name);
     }
 
-    public static final ExtendedStatsBucketPipelineAggregator.Factory extendedStatsBucket(String name, String bucketsPath) {
-        return new ExtendedStatsBucketPipelineAggregator.Factory(name, bucketsPath);
+    public static final ExtendedStatsBucketBuilder extendedStatsBucket(String name) {
+        return new ExtendedStatsBucketBuilder(name);
     }
 
-    public static final PercentilesBucketPipelineAggregator.Factory percentilesBucket(String name, String bucketsPath) {
-        return new PercentilesBucketPipelineAggregator.Factory(name, bucketsPath);
+    public static final PercentilesBucketBuilder percentilesBucket(String name) {
+        return new PercentilesBucketBuilder(name);
     }
 
-    public static final MovAvgPipelineAggregator.Factory movingAvg(String name, String bucketsPath) {
-        return new MovAvgPipelineAggregator.Factory(name, bucketsPath);
+    public static final MovAvgBuilder movingAvg(String name) {
+        return new MovAvgBuilder(name);
     }
 
-    public static final BucketScriptPipelineAggregator.Factory bucketScript(String name, Map<String, String> bucketsPathsMap,
-            Script script) {
-        return new BucketScriptPipelineAggregator.Factory(name, bucketsPathsMap, script);
+    public static final BucketScriptBuilder bucketScript(String name) {
+        return new BucketScriptBuilder(name);
     }
 
-    public static final BucketScriptPipelineAggregator.Factory bucketScript(String name, Script script, String... bucketsPaths) {
-        return new BucketScriptPipelineAggregator.Factory(name, script, bucketsPaths);
+    public static final BucketSelectorBuilder having(String name) {
+        return new BucketSelectorBuilder(name);
     }
 
-    public static final BucketSelectorPipelineAggregator.Factory bucketSelector(String name, Map<String, String> bucketsPathsMap,
-            Script script) {
-        return new BucketSelectorPipelineAggregator.Factory(name, bucketsPathsMap, script);
+    public static final CumulativeSumBuilder cumulativeSum(String name) {
+        return new CumulativeSumBuilder(name);
     }
 
-    public static final BucketSelectorPipelineAggregator.Factory bucketSelector(String name, Script script, String... bucketsPaths) {
-        return new BucketSelectorPipelineAggregator.Factory(name, script, bucketsPaths);
-    }
-
-    public static final CumulativeSumPipelineAggregator.Factory cumulativeSum(String name, String bucketsPath) {
-        return new CumulativeSumPipelineAggregator.Factory(name, bucketsPath);
-    }
-
-    public static final SerialDiffPipelineAggregator.Factory diff(String name, String bucketsPath) {
-        return new SerialDiffPipelineAggregator.Factory(name, bucketsPath);
+    public static final SerialDiffBuilder diff(String name) {
+        return new SerialDiffBuilder(name);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java
index 87d0780..6fc0185 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java
@@ -18,25 +18,17 @@
  */
 package org.elasticsearch.search.aggregations.pipeline;
 
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.NamedWriteable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 
 import java.io.IOException;
-import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * A factory that knows how to create an {@link PipelineAggregator} of a
  * specific type.
  */
-public abstract class PipelineAggregatorFactory extends ToXContentToBytes implements NamedWriteable<PipelineAggregatorFactory>, ToXContent {
+public abstract class PipelineAggregatorFactory {
 
     protected String name;
     protected String type;
@@ -61,10 +53,6 @@ public abstract class PipelineAggregatorFactory extends ToXContentToBytes implem
         return name;
     }
 
-    public String type() {
-        return type;
-    }
-
     /**
      * Validates the state of this factory (makes sure the factory is properly
      * configured)
@@ -102,91 +90,4 @@ public abstract class PipelineAggregatorFactory extends ToXContentToBytes implem
         return bucketsPaths;
     }
 
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(name);
-        out.writeStringArray(bucketsPaths);
-        doWriteTo(out);
-        out.writeMap(metaData);
-    }
-
-    protected abstract void doWriteTo(StreamOutput out) throws IOException;
-
-    @Override
-    public String getWriteableName() {
-        return type;
-    }
-
-    @Override
-    public PipelineAggregatorFactory readFrom(StreamInput in) throws IOException {
-        String name = in.readString();
-        String[] bucketsPaths = in.readStringArray();
-        PipelineAggregatorFactory factory = doReadFrom(name, bucketsPaths, in);
-        factory.metaData = in.readMap();
-        return factory;
-    }
-
-    protected abstract PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException;
-
-    @Override
-    public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(getName());
-
-        if (this.metaData != null) {
-            builder.field("meta", this.metaData);
-        }
-        builder.startObject(type);
-
-        if (!overrideBucketsPath() && bucketsPaths != null) {
-            builder.startArray(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName());
-            for (String path : bucketsPaths) {
-                builder.value(path);
-            }
-            builder.endArray();
-        }
-
-        internalXContent(builder, params);
-
-        builder.endObject();
-
-        return builder.endObject();
-    }
-
-    /**
-     * @return <code>true</code> if the {@link PipelineAggregatorFactory}
-     *         overrides the XContent rendering of the bucketPath option.
-     */
-    protected boolean overrideBucketsPath() {
-        return false;
-    }
-
-    protected abstract XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException;
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(Arrays.hashCode(bucketsPaths), metaData, name, type, doHashCode());
-    }
-
-    protected abstract int doHashCode();
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null)
-            return false;
-        if (getClass() != obj.getClass())
-            return false;
-        PipelineAggregatorFactory other = (PipelineAggregatorFactory) obj;
-        if (!Objects.equals(name, other.name))
-            return false;
-        if (!Objects.equals(type, other.type))
-            return false;
-        if (!Objects.deepEquals(bucketsPaths, other.bucketsPaths))
-            return false;
-        if (!Objects.equals(metaData, other.metaData))
-            return false;
-        return doEquals(obj);
-    }
-
-    protected abstract boolean doEquals(Object obj);
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsBuilder.java
new file mode 100644
index 0000000..88a1f42
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsBuilder.java
@@ -0,0 +1,67 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketParser;
+import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativeParser;
+
+import java.io.IOException;
+
+/**
+ * A builder for building requests for a {@link BucketMetricsPipelineAggregator}
+ */
+public abstract class BucketMetricsBuilder<B extends BucketMetricsBuilder<B>> extends PipelineAggregatorBuilder<B> {
+
+    private String format;
+    private GapPolicy gapPolicy;
+
+    public BucketMetricsBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    public B format(String format) {
+        this.format = format;
+        return (B) this;
+    }
+
+    public B gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return (B) this;
+    }
+
+    @Override
+    protected final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (format != null) {
+            builder.field(MinBucketParser.FORMAT.getPreferredName(), format);
+        }
+        if (gapPolicy != null) {
+            builder.field(DerivativeParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        doInternalXContent(builder, params);
+        return builder;
+    }
+
+    protected void doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsFactory.java
deleted file mode 100644
index 73096ef..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsFactory.java
+++ /dev/null
@@ -1,146 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
-import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-import java.util.Objects;
-
-public abstract class BucketMetricsFactory<AF extends BucketMetricsFactory<AF>> extends PipelineAggregatorFactory {
-
-    private String format = null;
-    private GapPolicy gapPolicy = GapPolicy.SKIP;
-
-    public BucketMetricsFactory(String name, String type, String[] bucketsPaths) {
-        super(name, type, bucketsPaths);
-    }
-
-    /**
-     * Sets the format to use on the output of this aggregation.
-     */
-    public AF format(String format) {
-        this.format = format;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the format to use on the output of this aggregation.
-     */
-    public String format() {
-        return format;
-    }
-
-    protected ValueFormatter formatter() {
-        if (format != null) {
-            return ValueFormat.Patternable.Number.format(format).formatter();
-        } else {
-            return ValueFormatter.RAW;
-        }
-    }
-
-    /**
-     * Sets the gap policy to use for this aggregation.
-     */
-    public AF gapPolicy(GapPolicy gapPolicy) {
-        this.gapPolicy = gapPolicy;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the gap policy to use for this aggregation.
-     */
-    public GapPolicy gapPolicy() {
-        return gapPolicy;
-    }
-
-    @Override
-    protected abstract PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException;
-
-    @Override
-    public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
-            List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
-        if (bucketsPaths.length != 1) {
-            throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
-                    + " must contain a single entry for aggregation [" + name + "]");
-        }
-    }
-
-    @Override
-    protected final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-        if (format != null) {
-            builder.field(BucketMetricsParser.FORMAT.getPreferredName(), format);
-        }
-        if (gapPolicy != null) {
-            builder.field(BucketMetricsParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-        }
-        doXContentBody(builder, params);
-        return builder;
-    }
-
-    protected abstract XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException;
-
-    @Override
-    protected final PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-        BucketMetricsFactory factory = innerReadFrom(name, bucketsPaths, in);
-        factory.format = in.readOptionalString();
-        factory.gapPolicy = GapPolicy.readFrom(in);
-        return factory;
-    }
-
-    protected abstract BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException;
-
-    @Override
-    protected final void doWriteTo(StreamOutput out) throws IOException {
-        innerWriteTo(out);
-        out.writeOptionalString(format);
-        gapPolicy.writeTo(out);
-    }
-
-    protected abstract void innerWriteTo(StreamOutput out) throws IOException;
-
-    @Override
-    protected final int doHashCode() {
-        return Objects.hash(format, gapPolicy, innerHashCode());
-    }
-
-    protected abstract int innerHashCode();
-
-    @Override
-    protected final boolean doEquals(Object obj) {
-        BucketMetricsFactory other = (BucketMetricsFactory) obj;
-        return Objects.equals(format, other.format)
-                && Objects.equals(gapPolicy, other.gapPolicy)
-                && innerEquals(other);
-    }
-
-    protected abstract boolean innerEquals(BucketMetricsFactory other);
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java
index db1f62b..3cf084b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java
@@ -20,12 +20,14 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.text.ParseException;
@@ -46,13 +48,12 @@ public abstract class BucketMetricsParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public final PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context)
-            throws IOException {
+    public final PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
         String format = null;
-        GapPolicy gapPolicy = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
         Map<String, Object> leftover = new HashMap<>(5);
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -85,34 +86,34 @@ public abstract class BucketMetricsParser implements PipelineAggregator.Parser {
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Missing required field [" + BUCKETS_PATH.getPreferredName() + "] for aggregation [" + pipelineAggregatorName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for aggregation [" + pipelineAggregatorName + "]", parser.getTokenLocation());
         }
 
-        BucketMetricsFactory factory = null;
+        ValueFormatter formatter = null;
+        if (format != null) {
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
+        }
+
+        PipelineAggregatorFactory factory = null;
         try {
-            factory = buildFactory(pipelineAggregatorName, bucketsPaths[0], leftover);
-            if (format != null) {
-                factory.format(format);
-            }
-            if (gapPolicy != null) {
-                factory.gapPolicy(gapPolicy);
-            }
+            factory = buildFactory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter, leftover);
         } catch (ParseException exception) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Could not parse settings for aggregation [" + pipelineAggregatorName + "].", exception);
+            throw new SearchParseException(context, "Could not parse settings for aggregation ["
+                    + pipelineAggregatorName + "].", null, exception);
         }
 
         if (leftover.size() > 0) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Unexpected tokens " + leftover.keySet() + " in [" + pipelineAggregatorName + "].");
+            throw new SearchParseException(context, "Unexpected tokens " + leftover.keySet() + " in [" + pipelineAggregatorName + "].", null);
         }
         assert(factory != null);
 
         return factory;
     }
 
-    protected abstract BucketMetricsFactory buildFactory(String pipelineAggregatorName, String bucketsPaths,
-            Map<String, Object> unparsedParams) throws ParseException;
+    protected abstract PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) throws ParseException;
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketBuilder.java
new file mode 100644
index 0000000..627cded
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketBuilder.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg;
+
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+public class AvgBucketBuilder extends BucketMetricsBuilder<AvgBucketBuilder> {
+
+    public AvgBucketBuilder(String name) {
+        super(name, AvgBucketPipelineAggregator.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketParser.java
index 431be6d..658284f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg;
 
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -32,11 +33,8 @@ public class AvgBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String bucketsPath, Map<String, Object> unparsedParams) {
-        return new AvgBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPath);
-    }
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new AvgBucketPipelineAggregator.Factory(null, null);
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new AvgBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketPipelineAggregator.java
index 58b26df..3ab134c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -30,7 +28,6 @@ import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
@@ -89,19 +86,20 @@ public class AvgBucketPipelineAggregator extends BucketMetricsPipelineAggregator
         return new InternalSimpleValue(name(), avgValue, formatter, pipelineAggregators, metadata);
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
 
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new AvgBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new AvgBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
@@ -112,31 +110,6 @@ public class AvgBucketPipelineAggregator extends BucketMetricsPipelineAggregator
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            return new Factory(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketBuilder.java
new file mode 100644
index 0000000..8b214d3
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketBuilder.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max;
+
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+public class MaxBucketBuilder extends BucketMetricsBuilder<MaxBucketBuilder> {
+
+    public MaxBucketBuilder(String name) {
+        super(name, MaxBucketPipelineAggregator.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketParser.java
index c75927e..4cd584a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max;
 
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -33,13 +34,9 @@ public class MaxBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String bucketsPath, Map<String, Object> unparsedParams) {
-        return new MaxBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPath);
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new MaxBucketPipelineAggregator.Factory(null, null);
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+                                                     ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new MaxBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketPipelineAggregator.java
index 739aef8..95a70af 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -29,7 +27,6 @@ import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.InternalBucketMetricValue;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
@@ -96,19 +93,20 @@ public class MaxBucketPipelineAggregator extends BucketMetricsPipelineAggregator
         return new InternalBucketMetricValue(name(), keys, maxValue, formatter, Collections.emptyList(), metaData());
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
 
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new MaxBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new MaxBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
@@ -120,31 +118,6 @@ public class MaxBucketPipelineAggregator extends BucketMetricsPipelineAggregator
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            return new Factory(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketBuilder.java
new file mode 100644
index 0000000..327bf4e
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketBuilder.java
@@ -0,0 +1,31 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min;
+
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+
+public class MinBucketBuilder extends BucketMetricsBuilder<MinBucketBuilder> {
+
+    public MinBucketBuilder(String name) {
+        super(name, MinBucketPipelineAggregator.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketParser.java
index 3e6e7e1..db7bc9b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min;
 
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -32,14 +33,9 @@ public class MinBucketParser extends BucketMetricsParser {
         return MinBucketPipelineAggregator.TYPE.name();
     }
 
-    @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String bucketsPath, Map<String, Object> unparsedParams) {
-        return new MinBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPath);
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new MinBucketPipelineAggregator.Factory(null, null);
-    }
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new MinBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
+    };
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketPipelineAggregator.java
index dfbe3e3..755b206 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -29,7 +27,6 @@ import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.InternalBucketMetricValue;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
@@ -97,19 +94,20 @@ public class MinBucketPipelineAggregator extends BucketMetricsPipelineAggregator
         return new InternalBucketMetricValue(name(), keys, minValue, formatter, Collections.emptyList(), metaData());
     };
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
 
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new MinBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new MinBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
@@ -121,31 +119,6 @@ public class MinBucketPipelineAggregator extends BucketMetricsPipelineAggregator
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            return new Factory(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketBuilder.java
new file mode 100644
index 0000000..9293e14
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketBuilder.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+import java.io.IOException;
+
+public class PercentilesBucketBuilder extends BucketMetricsBuilder<PercentilesBucketBuilder> {
+
+    Double[] percents;
+
+    public PercentilesBucketBuilder(String name) {
+        super(name, PercentilesBucketPipelineAggregator.TYPE.name());
+    }
+
+    public PercentilesBucketBuilder percents(Double[] percents) {
+        this.percents = percents;
+        return this;
+    }
+
+    @Override
+    protected void doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (percents != null) {
+            builder.field(PercentilesBucketParser.PERCENTS.getPreferredName(), (Object[])percents);
+        }
+    }
+
+
+}
+
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketParser.java
index fabe718..7c9da5c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketParser.java
@@ -21,13 +21,15 @@ package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile;
 
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.text.ParseException;
 import java.util.List;
 import java.util.Map;
 
+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+
 
 public class PercentilesBucketParser extends BucketMetricsParser {
 
@@ -39,10 +41,10 @@ public class PercentilesBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String bucketsPath, Map<String, Object> unparsedParams)
-            throws ParseException {
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+                                                     ValueFormatter formatter, Map<String, Object> unparsedParams) throws ParseException {
 
-        double[] percents = null;
+        double[] percents = new double[] { 1.0, 5.0, 25.0, 50.0, 75.0, 95.0, 99.0 };
         int counter = 0;
         Object percentParam = unparsedParams.get(PERCENTS.getPreferredName());
 
@@ -65,16 +67,6 @@ public class PercentilesBucketParser extends BucketMetricsParser {
             }
         }
 
-        PercentilesBucketPipelineAggregator.Factory factory = new PercentilesBucketPipelineAggregator.Factory(pipelineAggregatorName,
-                bucketsPath);
-        if (percents != null) {
-            factory.percents(percents);
-        }
-        return factory;
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new PercentilesBucketPipelineAggregator.Factory(null, null);
+        return new PercentilesBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter, percents);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregator.java
index d45078a..24e8204 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregator.java
@@ -19,33 +19,28 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile;
 
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
+
+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 
 public class PercentilesBucketPipelineAggregator extends BucketMetricsPipelineAggregator {
 
     public final static Type TYPE = new Type("percentiles_bucket");
-    public final ParseField PERCENTS_FIELD = new ParseField("percents");
 
     public final static PipelineAggregatorStreams.Stream STREAM = new PipelineAggregatorStreams.Stream() {
         @Override
@@ -124,36 +119,22 @@ public class PercentilesBucketPipelineAggregator extends BucketMetricsPipelineAg
         out.writeDoubleArray(percents);
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
-
-        private double[] percents = new double[] { 1.0, 5.0, 25.0, 50.0, 75.0, 95.0, 99.0 };
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
+        private final double[] percents;
 
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter, double[] percents) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Get the percentages to calculate percentiles for in this aggregation
-         */
-        public double[] percents() {
-            return percents;
-        }
-
-        /**
-         * Set the percentages to calculate percentiles for in this aggregation
-         */
-        public Factory percents(double[] percents) {
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
             this.percents = percents;
-            return this;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new PercentilesBucketPipelineAggregator(name, percents, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new PercentilesBucketPipelineAggregator(name, percents, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
@@ -172,37 +153,6 @@ public class PercentilesBucketPipelineAggregator extends BucketMetricsPipelineAg
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            if (percents != null) {
-                builder.field(PercentilesBucketParser.PERCENTS.getPreferredName(), percents);
-            }
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.percents = in.readDoubleArray();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDoubleArray(percents);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Arrays.hashCode(percents);
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory obj) {
-            Factory other = (Factory) obj;
-            return Objects.deepEquals(percents, other.percents);
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketBuilder.java
new file mode 100644
index 0000000..a8c19db
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketBuilder.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats;
+
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+public class StatsBucketBuilder extends BucketMetricsBuilder<StatsBucketBuilder> {
+
+    public StatsBucketBuilder(String name) {
+        super(name, StatsBucketPipelineAggregator.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketParser.java
index 41a742c..b250447 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats;
 
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -32,12 +33,8 @@ public class StatsBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String bucketsPath, Map<String, Object> unparsedParams) {
-        return new StatsBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPath);
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new StatsBucketPipelineAggregator.Factory(null, null);
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new StatsBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketPipelineAggregator.java
index 1b63ada..66726ce 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -29,7 +27,6 @@ import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
@@ -95,19 +92,20 @@ public class StatsBucketPipelineAggregator extends BucketMetricsPipelineAggregat
         return new InternalStatsBucket(name(), count, sum, min, max, formatter, pipelineAggregators, metadata);
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
 
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new StatsBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new StatsBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
@@ -119,31 +117,6 @@ public class StatsBucketPipelineAggregator extends BucketMetricsPipelineAggregat
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            return new Factory(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketBuilder.java
new file mode 100644
index 0000000..25880bd
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketBuilder.java
@@ -0,0 +1,47 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+import java.io.IOException;
+
+public class ExtendedStatsBucketBuilder extends BucketMetricsBuilder<ExtendedStatsBucketBuilder> {
+
+    Double sigma;
+
+    public ExtendedStatsBucketBuilder(String name) {
+        super(name, ExtendedStatsBucketPipelineAggregator.TYPE.name());
+    }
+
+    public ExtendedStatsBucketBuilder sigma(Double sigma) {
+        this.sigma = sigma;
+        return this;
+    }
+
+    @Override
+    protected void doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (sigma != null) {
+            builder.field(ExtendedStatsBucketParser.SIGMA.getPreferredName(), sigma);
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java
index 89457a9..b4d1f18 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java
@@ -20,9 +20,10 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended;
 
 import org.elasticsearch.common.ParseField;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.text.ParseException;
 import java.util.Map;
@@ -36,10 +37,10 @@ public class ExtendedStatsBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String bucketsPath, Map<String, Object> unparsedParams)
-            throws ParseException {
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) throws ParseException {
 
-        Double sigma = null;
+        double sigma = 2.0;
         Object param = unparsedParams.get(SIGMA.getPreferredName());
 
         if (param != null) {
@@ -51,16 +52,6 @@ public class ExtendedStatsBucketParser extends BucketMetricsParser {
                         + param.getClass().getSimpleName() + "` provided instead", 0);
             }
         }
-        ExtendedStatsBucketPipelineAggregator.Factory factory = new ExtendedStatsBucketPipelineAggregator.Factory(pipelineAggregatorName,
-                bucketsPath);
-        if (sigma != null) {
-            factory.sigma(sigma);
-        }
-        return factory;
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new ExtendedStatsBucketPipelineAggregator.Factory(null, null);
+        return new ExtendedStatsBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, sigma, gapPolicy, formatter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketPipelineAggregator.java
index f58d03c..6a7f2be 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -29,14 +27,12 @@ import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 public class ExtendedStatsBucketPipelineAggregator extends BucketMetricsPipelineAggregator {
 
@@ -101,38 +97,22 @@ public class ExtendedStatsBucketPipelineAggregator extends BucketMetricsPipeline
         return new InternalExtendedStatsBucket(name(), count, sum, min, max, sumOfSqrs, sigma, formatter, pipelineAggregators, metadata);
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        private double sigma = 2.0;
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
+        private final double sigma;
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
-
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, double sigma, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Set the value of sigma to use when calculating the standard deviation
-         * bounds
-         */
-        public Factory sigma(double sigma) {
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
             this.sigma = sigma;
-            return this;
-        }
-
-        /**
-         * Get the value of sigma to use when calculating the standard deviation
-         * bounds
-         */
-        public double sigma() {
-            return sigma;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new ExtendedStatsBucketPipelineAggregator(name, bucketsPaths, sigma, gapPolicy(), formatter(), metaData);
+            return new ExtendedStatsBucketPipelineAggregator(name, bucketsPaths, sigma, gapPolicy, formatter, metaData);
         }
 
         @Override
@@ -149,35 +129,6 @@ public class ExtendedStatsBucketPipelineAggregator extends BucketMetricsPipeline
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(ExtendedStatsBucketParser.SIGMA.getPreferredName(), sigma);
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.sigma = in.readDouble();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDouble(sigma);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(sigma);
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(sigma, other.sigma);
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketBuilder.java
new file mode 100644
index 0000000..5b2201c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketBuilder.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum;
+
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+public class SumBucketBuilder extends BucketMetricsBuilder<SumBucketBuilder> {
+
+    public SumBucketBuilder(String name) {
+        super(name, SumBucketPipelineAggregator.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketParser.java
index bcf08b3..3fad95d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum;
 
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -32,12 +33,8 @@ public class SumBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String bucketsPath, Map<String, Object> unparsedParams) {
-        return new SumBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPath);
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new SumBucketPipelineAggregator.Factory(null, null);
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new SumBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketPipelineAggregator.java
index 36f6c5a..138bd63 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -30,7 +28,6 @@ import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
@@ -85,19 +82,20 @@ public class SumBucketPipelineAggregator extends BucketMetricsPipelineAggregator
         return new InternalSimpleValue(name(), sum, formatter, pipelineAggregators, metadata);
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
 
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new SumBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new SumBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
@@ -109,31 +107,6 @@ public class SumBucketPipelineAggregator extends BucketMetricsPipelineAggregator
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            return new Factory(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptBuilder.java
new file mode 100644
index 0000000..ee5fa94
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptBuilder.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketscript;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+
+import java.io.IOException;
+import java.util.Map;
+
+public class BucketScriptBuilder extends PipelineAggregatorBuilder<BucketScriptBuilder> {
+
+    private String format;
+    private GapPolicy gapPolicy;
+    private Script script;
+    private Map<String, String> bucketsPathsMap;
+
+    public BucketScriptBuilder(String name) {
+        super(name, BucketScriptPipelineAggregator.TYPE.name());
+    }
+
+    public BucketScriptBuilder script(Script script) {
+        this.script = script;
+        return this;
+    }
+
+    public BucketScriptBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    public BucketScriptBuilder gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return this;
+    }
+
+    /**
+     * Sets the paths to the buckets to use for this pipeline aggregator
+     */
+    public BucketScriptBuilder setBucketsPathsMap(Map<String, String> bucketsPathsMap) {
+        this.bucketsPathsMap = bucketsPathsMap;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
+        if (script != null) {
+            builder.field(ScriptField.SCRIPT.getPreferredName(), script);
+        }
+        if (format != null) {
+            builder.field(BucketScriptParser.FORMAT.getPreferredName(), format);
+        }
+        if (gapPolicy != null) {
+            builder.field(BucketScriptParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        if (bucketsPathsMap != null) {
+            builder.field(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName(), bucketsPathsMap);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java
index d97d51a..05ff7e9 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java
@@ -20,18 +20,19 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketscript;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -48,13 +49,13 @@ public class BucketScriptParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, QueryParseContext context) throws IOException {
+    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         Script script = null;
         String currentFieldName = null;
         Map<String, String> bucketsPathsMap = null;
         String format = null;
-        GapPolicy gapPolicy = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -70,8 +71,8 @@ public class BucketScriptParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
                     script = Script.parse(parser, context.parseFieldMatcher());
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -85,8 +86,8 @@ public class BucketScriptParser implements PipelineAggregator.Parser {
                         bucketsPathsMap.put("_value" + i, paths.get(i));
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
@@ -98,37 +99,33 @@ public class BucketScriptParser implements PipelineAggregator.Parser {
                         bucketsPathsMap.put(entry.getKey(), String.valueOf(entry.getValue()));
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + reducerName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + reducerName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPathsMap == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for series_arithmetic aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for series_arithmetic aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
         if (script == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + ScriptField.SCRIPT.getPreferredName()
-                    + "] for series_arithmetic aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + ScriptField.SCRIPT.getPreferredName()
+                    + "] for series_arithmetic aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
-        BucketScriptPipelineAggregator.Factory factory = new BucketScriptPipelineAggregator.Factory(reducerName, bucketsPathsMap, script);
+        ValueFormatter formatter = null;
         if (format != null) {
-            factory.format(format);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
         }
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
-        }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new BucketScriptPipelineAggregator.Factory(null, Collections.emptyMap(), null);
+        return new BucketScriptPipelineAggregator.Factory(reducerName, bucketsPathsMap, script, formatter, gapPolicy);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java
index 7e487a9..76cb15e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java
@@ -21,11 +21,9 @@ package org.elasticsearch.search.aggregations.pipeline.bucketscript;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.script.CompiledScript;
 import org.elasticsearch.script.ExecutableScript;
 import org.elasticsearch.script.Script;
-import org.elasticsearch.script.Script.ScriptField;
 import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -39,7 +37,6 @@ import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -49,8 +46,6 @@ import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
@@ -162,124 +157,22 @@ public class BucketScriptPipelineAggregator extends PipelineAggregator {
 
     public static class Factory extends PipelineAggregatorFactory {
 
-        private final Script script;
-        private final Map<String, String> bucketsPathsMap;
-        private String format = null;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
+        private Script script;
+        private final ValueFormatter formatter;
+        private GapPolicy gapPolicy;
+        private Map<String, String> bucketsPathsMap;
 
-        public Factory(String name, Map<String, String> bucketsPathsMap, Script script) {
+        public Factory(String name, Map<String, String> bucketsPathsMap, Script script, ValueFormatter formatter, GapPolicy gapPolicy) {
             super(name, TYPE.name(), bucketsPathsMap.values().toArray(new String[bucketsPathsMap.size()]));
             this.bucketsPathsMap = bucketsPathsMap;
             this.script = script;
-        }
-
-        public Factory(String name, Script script, String... bucketsPaths) {
-            this(name, convertToBucketsPathMap(bucketsPaths), script);
-        }
-
-        private static Map<String, String> convertToBucketsPathMap(String[] bucketsPaths) {
-            Map<String, String> bucketsPathsMap = new HashMap<>();
-            for (int i = 0; i < bucketsPaths.length; i++) {
-                bucketsPathsMap.put("_value" + i, bucketsPaths[i]);
-            }
-            return bucketsPathsMap;
-        }
-
-        /**
-         * Sets the format to use on the output of this aggregation.
-         */
-        public Factory format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        /**
-         * Gets the format to use on the output of this aggregation.
-         */
-        public String format() {
-            return format;
-        }
-
-        protected ValueFormatter formatter() {
-            if (format != null) {
-                return ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                return ValueFormatter.RAW;
-            }
-        }
-
-        /**
-         * Sets the gap policy to use for this aggregation.
-         */
-        public Factory gapPolicy(GapPolicy gapPolicy) {
+            this.formatter = formatter;
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        /**
-         * Gets the gap policy to use for this aggregation.
-         */
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new BucketScriptPipelineAggregator(name, bucketsPathsMap, script, formatter(), gapPolicy, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(BucketScriptParser.BUCKETS_PATH.getPreferredName(), bucketsPathsMap);
-            builder.field(ScriptField.SCRIPT.getPreferredName(), script);
-            if (format != null) {
-                builder.field(BucketScriptParser.FORMAT.getPreferredName(), format);
-            }
-            builder.field(BucketScriptParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            return builder;
-        }
-
-        @Override
-        protected boolean overrideBucketsPath() {
-            return true;
-        }
-
-        @Override
-        protected PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Map<String, String> bucketsPathsMap = new HashMap<String, String>();
-            int mapSize = in.readVInt();
-            for (int i = 0; i < mapSize; i++) {
-                bucketsPathsMap.put(in.readString(), in.readString());
-            }
-            Script script = Script.readScript(in);
-            Factory factory = new Factory(name, bucketsPathsMap, script);
-            factory.format = in.readOptionalString();
-            factory.gapPolicy = GapPolicy.readFrom(in);
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(bucketsPathsMap.size());
-            for (Entry<String, String> e : bucketsPathsMap.entrySet()) {
-                out.writeString(e.getKey());
-                out.writeString(e.getValue());
-            }
-            script.writeTo(out);
-            out.writeOptionalString(format);
-            gapPolicy.writeTo(out);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(bucketsPathsMap, script, format, gapPolicy);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(bucketsPathsMap, other.bucketsPathsMap) && Objects.equals(script, other.script)
-                    && Objects.equals(format, other.format) && Objects.equals(gapPolicy, other.gapPolicy);
+            return new BucketScriptPipelineAggregator(name, bucketsPathsMap, script, formatter, gapPolicy, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumBuilder.java
new file mode 100644
index 0000000..282ded8
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumBuilder.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.cumulativesum;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+
+import java.io.IOException;
+
+public class CumulativeSumBuilder extends PipelineAggregatorBuilder<CumulativeSumBuilder> {
+
+    private String format;
+
+    public CumulativeSumBuilder(String name) {
+        super(name, CumulativeSumPipelineAggregator.TYPE.name());
+    }
+
+    public CumulativeSumBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (format != null) {
+            builder.field(CumulativeSumParser.FORMAT.getPreferredName(), format);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java
index 30a6b21..f3e2ead 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java
@@ -20,11 +20,13 @@
 package org.elasticsearch.search.aggregations.pipeline.cumulativesum;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -42,8 +44,7 @@ public class CumulativeSumParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context)
-            throws IOException {
+    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
@@ -58,8 +59,8 @@ public class CumulativeSumParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
                     bucketsPaths = new String[] { parser.text() };
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -70,31 +71,28 @@ public class CumulativeSumParser implements PipelineAggregator.Parser {
                     }
                     bucketsPaths = paths.toArray(new String[paths.size()]);
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " in [" + pipelineAggregatorName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + pipelineAggregatorName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for derivative aggregation [" + pipelineAggregatorName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for derivative aggregation [" + pipelineAggregatorName + "]", parser.getTokenLocation());
         }
 
-        CumulativeSumPipelineAggregator.Factory factory = new CumulativeSumPipelineAggregator.Factory(pipelineAggregatorName,
-                bucketsPaths[0]);
+        ValueFormatter formatter = null;
         if (format != null) {
-            factory.format(format);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
         }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new CumulativeSumPipelineAggregator.Factory(null, null);
+        return new CumulativeSumPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, formatter);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java
index 5ecc681..49c6f4f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java
@@ -21,7 +21,6 @@ package org.elasticsearch.search.aggregations.pipeline.cumulativesum;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
@@ -34,8 +33,6 @@ import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -43,7 +40,6 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
@@ -113,42 +109,16 @@ public class CumulativeSumPipelineAggregator extends PipelineAggregator {
 
     public static class Factory extends PipelineAggregatorFactory {
 
-        private String format;
+        private final ValueFormatter formatter;
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
-
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Sets the format to use on the output of this aggregation.
-         */
-        public Factory format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        /**
-         * Gets the format to use on the output of this aggregation.
-         */
-        public String format() {
-            return format;
-        }
-
-        protected ValueFormatter formatter() {
-            if (format != null) {
-                return ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                return ValueFormatter.RAW;
-            }
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new CumulativeSumPipelineAggregator(name, bucketsPaths, formatter(), metaData);
+            return new CumulativeSumPipelineAggregator(name, bucketsPaths, formatter, metaData);
         }
 
         @Override
@@ -169,35 +139,5 @@ public class CumulativeSumPipelineAggregator extends PipelineAggregator {
             }
         }
 
-        @Override
-        protected final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (format != null) {
-                builder.field(BucketMetricsParser.FORMAT.getPreferredName(), format);
-            }
-            return builder;
-        }
-
-        @Override
-        protected final PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.format = in.readOptionalString();
-            return factory;
-        }
-
-        @Override
-        protected final void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(format);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(format);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(format, other.format);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeBuilder.java
new file mode 100644
index 0000000..50b4578
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeBuilder.java
@@ -0,0 +1,78 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.derivative;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+
+import java.io.IOException;
+
+public class DerivativeBuilder extends PipelineAggregatorBuilder<DerivativeBuilder> {
+
+    private String format;
+    private GapPolicy gapPolicy;
+    private String unit;
+
+    public DerivativeBuilder(String name) {
+        super(name, DerivativePipelineAggregator.TYPE.name());
+    }
+
+    public DerivativeBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    public DerivativeBuilder gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return this;
+    }
+
+    public DerivativeBuilder unit(String unit) {
+        this.unit = unit;
+        return this;
+    }
+
+    /**
+     * Sets the unit using the provided {@link DateHistogramInterval}. This
+     * method is only useful when calculating the derivative using a
+     * `date_histogram`
+     */
+    public DerivativeBuilder unit(DateHistogramInterval unit) {
+        this.unit = unit.toString();
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (format != null) {
+            builder.field(DerivativeParser.FORMAT.getPreferredName(), format);
+        }
+        if (gapPolicy != null) {
+            builder.field(DerivativeParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        if (unit != null) {
+            builder.field(DerivativeParser.UNIT.getPreferredName(), unit);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java
index 7d6abdc..f413987 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java
@@ -20,12 +20,17 @@
 package org.elasticsearch.search.aggregations.pipeline.derivative;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.rounding.DateTimeUnit;
+import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramParser;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -43,14 +48,13 @@ public class DerivativeParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context)
-            throws IOException {
+    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
         String format = null;
         String units = null;
-        GapPolicy gapPolicy = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -65,8 +69,8 @@ public class DerivativeParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, UNIT)) {
                     units = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -77,36 +81,41 @@ public class DerivativeParser implements PipelineAggregator.Parser {
                     }
                     bucketsPaths = paths.toArray(new String[paths.size()]);
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " in [" + pipelineAggregatorName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + pipelineAggregatorName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for derivative aggregation [" + pipelineAggregatorName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for derivative aggregation [" + pipelineAggregatorName + "]", parser.getTokenLocation());
         }
 
-        DerivativePipelineAggregator.Factory factory = new DerivativePipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths[0]);
+        ValueFormatter formatter = null;
         if (format != null) {
-            factory.format(format);
-        }
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
         }
+
+        Long xAxisUnits = null;
         if (units != null) {
-            factory.unit(units);
+            DateTimeUnit dateTimeUnit = DateHistogramParser.DATE_FIELD_UNITS.get(units);
+            if (dateTimeUnit != null) {
+                xAxisUnits = dateTimeUnit.field().getDurationField().getUnitMillis();
+            } else {
+                TimeValue timeValue = TimeValue.parseTimeValue(units, null, getClass().getSimpleName() + ".unit");
+                if (timeValue != null) {
+                    xAxisUnits = timeValue.getMillis();
+                }
+            }
         }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new DerivativePipelineAggregator.Factory(null, null);
+        return new DerivativePipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, formatter, gapPolicy, xAxisUnits);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java
index 1ea4cc5..855fea8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java
@@ -21,23 +21,18 @@ package org.elasticsearch.search.aggregations.pipeline.derivative;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.rounding.DateTimeUnit;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.InternalAggregations;
-import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
 import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 import org.joda.time.DateTime;
@@ -46,7 +41,6 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
@@ -160,70 +154,19 @@ public class DerivativePipelineAggregator extends PipelineAggregator {
 
     public static class Factory extends PipelineAggregatorFactory {
 
-        private String format;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
-        private String units;
+        private final ValueFormatter formatter;
+        private GapPolicy gapPolicy;
+        private Long xAxisUnits;
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
-
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, ValueFormatter formatter, GapPolicy gapPolicy, Long xAxisUnits) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        public Factory format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        public String format() {
-            return format;
-        }
-
-        public Factory gapPolicy(GapPolicy gapPolicy) {
+            this.formatter = formatter;
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
-        }
-
-        public Factory unit(String units) {
-            this.units = units;
-            return this;
-        }
-
-        public Factory unit(DateHistogramInterval units) {
-            this.units = units.toString();
-            return this;
-        }
-
-        public String unit() {
-            return units;
+            this.xAxisUnits = xAxisUnits;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            ValueFormatter formatter;
-            if (format != null) {
-                formatter = ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                formatter = ValueFormatter.RAW;
-            }
-            Long xAxisUnits = null;
-            if (units != null) {
-                DateTimeUnit dateTimeUnit = HistogramAggregator.DateHistogramFactory.DATE_FIELD_UNITS.get(units);
-                if (dateTimeUnit != null) {
-                    xAxisUnits = dateTimeUnit.field().getDurationField().getUnitMillis();
-                } else {
-                    TimeValue timeValue = TimeValue.parseTimeValue(units, null, getClass().getSimpleName() + ".unit");
-                    if (timeValue != null) {
-                        xAxisUnits = timeValue.getMillis();
-                    }
-                }
-            }
             return new DerivativePipelineAggregator(name, bucketsPaths, formatter, gapPolicy, xAxisUnits, metaData);
         }
 
@@ -245,61 +188,5 @@ public class DerivativePipelineAggregator extends PipelineAggregator {
             }
         }
 
-        @Override
-        protected PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.format = in.readOptionalString();
-            if (in.readBoolean()) {
-                factory.gapPolicy = GapPolicy.readFrom(in);
-            }
-            factory.units = in.readOptionalString();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(format);
-            boolean hasGapPolicy = gapPolicy != null;
-            out.writeBoolean(hasGapPolicy);
-            if (hasGapPolicy) {
-                gapPolicy.writeTo(out);
-            }
-            out.writeOptionalString(units);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (format != null) {
-                builder.field(DerivativeParser.FORMAT.getPreferredName(), format);
-            }
-            if (gapPolicy != null) {
-                builder.field(DerivativeParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            }
-            if (units != null) {
-                builder.field(DerivativeParser.UNIT.getPreferredName(), units);
-            }
-            return builder;
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            if (!Objects.equals(format, other.format)) {
-                return false;
-            }
-            if (!Objects.equals(gapPolicy, other.gapPolicy)) {
-                return false;
-            }
-            if (!Objects.equals(units, other.units)) {
-                return false;
-            }
-            return true;
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(format, gapPolicy, units);
-        }
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorBuilder.java
new file mode 100644
index 0000000..c291c63
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorBuilder.java
@@ -0,0 +1,76 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.having;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+
+import java.io.IOException;
+import java.util.Map;
+
+public class BucketSelectorBuilder extends PipelineAggregatorBuilder<BucketSelectorBuilder> {
+
+    private GapPolicy gapPolicy;
+    private Script script;
+    private Map<String, String> bucketsPathsMap;
+
+    public BucketSelectorBuilder(String name) {
+        super(name, BucketSelectorPipelineAggregator.TYPE.name());
+    }
+
+    public BucketSelectorBuilder script(Script script) {
+        this.script = script;
+        return this;
+    }
+
+    public BucketSelectorBuilder gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return this;
+    }
+
+    /**
+     * Sets the paths to the buckets to use for this pipeline aggregator. The
+     * map given to this method must contain script variable name as keys with
+     * bucket paths values to the metrics to use for each variable.
+     */
+    public BucketSelectorBuilder setBucketsPathsMap(Map<String, String> bucketsPathsMap) {
+        this.bucketsPathsMap = bucketsPathsMap;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
+        if (script != null) {
+            builder.field(ScriptField.SCRIPT.getPreferredName(), script);
+        }
+        if (gapPolicy != null) {
+            builder.field(BucketSelectorParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        if (bucketsPathsMap != null) {
+            builder.field(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName(), bucketsPathsMap);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorParser.java
index cca0166..e2623b5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorParser.java
@@ -20,18 +20,17 @@
 package org.elasticsearch.search.aggregations.pipeline.having;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -48,12 +47,12 @@ public class BucketSelectorParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, QueryParseContext context) throws IOException {
+    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         Script script = null;
         String currentFieldName = null;
         Map<String, String> bucketsPathsMap = null;
-        GapPolicy gapPolicy = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -67,8 +66,8 @@ public class BucketSelectorParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
                     script = Script.parse(parser, context.parseFieldMatcher());
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -82,8 +81,8 @@ public class BucketSelectorParser implements PipelineAggregator.Parser {
                         bucketsPathsMap.put("_value" + i, paths.get(i));
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
@@ -95,36 +94,26 @@ public class BucketSelectorParser implements PipelineAggregator.Parser {
                         bucketsPathsMap.put(entry.getKey(), String.valueOf(entry.getValue()));
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + reducerName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + reducerName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPathsMap == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for bucket_selector aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for bucket_selector aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
         if (script == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + ScriptField.SCRIPT.getPreferredName()
-                    + "] for bucket_selector aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + ScriptField.SCRIPT.getPreferredName()
+                    + "] for bucket_selector aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
-        BucketSelectorPipelineAggregator.Factory factory = new BucketSelectorPipelineAggregator.Factory(reducerName, bucketsPathsMap,
-                script);
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
-        }
-        return factory;
-
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new BucketSelectorPipelineAggregator.Factory(null, Collections.emptyMap(), null);
+        return new BucketSelectorPipelineAggregator.Factory(reducerName, bucketsPathsMap, script, gapPolicy);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java
index dfb4789..edc3b4e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java
@@ -22,11 +22,9 @@ package org.elasticsearch.search.aggregations.pipeline.having;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.script.CompiledScript;
 import org.elasticsearch.script.ExecutableScript;
 import org.elasticsearch.script.Script;
-import org.elasticsearch.script.Script.ScriptField;
 import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
@@ -37,7 +35,6 @@ import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptParser;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -45,8 +42,6 @@ import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Objects;
 
 import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;
 
@@ -142,96 +137,20 @@ public class BucketSelectorPipelineAggregator extends PipelineAggregator {
     public static class Factory extends PipelineAggregatorFactory {
 
         private Script script;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
+        private GapPolicy gapPolicy;
         private Map<String, String> bucketsPathsMap;
 
-        public Factory(String name, Map<String, String> bucketsPathsMap, Script script) {
+        public Factory(String name, Map<String, String> bucketsPathsMap, Script script, GapPolicy gapPolicy) {
             super(name, TYPE.name(), bucketsPathsMap.values().toArray(new String[bucketsPathsMap.size()]));
             this.bucketsPathsMap = bucketsPathsMap;
             this.script = script;
-        }
-
-        public Factory(String name, Script script, String... bucketsPaths) {
-            this(name, convertToBucketsPathMap(bucketsPaths), script);
-        }
-
-        private static Map<String, String> convertToBucketsPathMap(String[] bucketsPaths) {
-            Map<String, String> bucketsPathsMap = new HashMap<>();
-            for (int i = 0; i < bucketsPaths.length; i++) {
-                bucketsPathsMap.put("_value" + i, bucketsPaths[i]);
-            }
-            return bucketsPathsMap;
-        }
-
-        /**
-         * Sets the gap policy to use for this aggregation.
-         */
-        public Factory gapPolicy(GapPolicy gapPolicy) {
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        /**
-         * Gets the gap policy to use for this aggregation.
-         */
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
             return new BucketSelectorPipelineAggregator(name, bucketsPathsMap, script, gapPolicy, metaData);
         }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(BucketScriptParser.BUCKETS_PATH.getPreferredName(), bucketsPathsMap);
-            builder.field(ScriptField.SCRIPT.getPreferredName(), script);
-            builder.field(BucketScriptParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            return builder;
-        }
-
-        @Override
-        protected boolean overrideBucketsPath() {
-            return true;
-        }
-
-        @Override
-        protected PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Map<String, String> bucketsPathsMap = new HashMap<String, String>();
-            int mapSize = in.readVInt();
-            for (int i = 0; i < mapSize; i++) {
-                bucketsPathsMap.put(in.readString(), in.readString());
-            }
-            Script script = Script.readScript(in);
-            Factory factory = new Factory(name, bucketsPathsMap, script);
-            factory.gapPolicy = GapPolicy.readFrom(in);
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(bucketsPathsMap.size());
-            for (Entry<String, String> e : bucketsPathsMap.entrySet()) {
-                out.writeString(e.getKey());
-                out.writeString(e.getValue());
-            }
-            script.writeTo(out);
-            gapPolicy.writeTo(out);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(bucketsPathsMap, script, gapPolicy);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(bucketsPathsMap, other.bucketsPathsMap) && Objects.equals(script, other.script)
-                    && Objects.equals(gapPolicy, other.gapPolicy);
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgBuilder.java
new file mode 100644
index 0000000..b2dc718
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgBuilder.java
@@ -0,0 +1,148 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.movavg;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelBuilder;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * A builder to create MovingAvg pipeline aggregations
+ */
+public class MovAvgBuilder extends PipelineAggregatorBuilder<MovAvgBuilder> {
+
+    private String format;
+    private GapPolicy gapPolicy;
+    private MovAvgModelBuilder modelBuilder;
+    private Integer window;
+    private Integer predict;
+    private Boolean minimize;
+    private Map<String, Object> settings;
+
+    public MovAvgBuilder(String name) {
+        super(name, MovAvgPipelineAggregator.TYPE.name());
+    }
+
+    public MovAvgBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    /**
+     * Defines what should be done when a gap in the series is discovered
+     *
+     * @param gapPolicy A GapPolicy enum defining the selected policy
+     * @return Returns the builder to continue chaining
+     */
+    public MovAvgBuilder gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return this;
+    }
+
+    /**
+     * Sets a MovAvgModelBuilder for the Moving Average.  The model builder is used to
+     * define what type of moving average you want to use on the series
+     *
+     * @param modelBuilder A MovAvgModelBuilder which has been prepopulated with settings
+     * @return Returns the builder to continue chaining
+     */
+    public MovAvgBuilder modelBuilder(MovAvgModelBuilder modelBuilder) {
+        this.modelBuilder = modelBuilder;
+        return this;
+    }
+
+    /**
+     * Sets the window size for the moving average.  This window will "slide" across the
+     * series, and the values inside that window will be used to calculate the moving avg value
+     *
+     * @param window Size of window
+     * @return Returns the builder to continue chaining
+     */
+    public MovAvgBuilder window(int window) {
+        this.window = window;
+        return this;
+    }
+
+    /**
+     * Sets the number of predictions that should be returned.  Each prediction will be spaced at
+     * the intervals specified in the histogram.  E.g "predict: 2" will return two new buckets at the
+     * end of the histogram with the predicted values.
+     *
+     * @param numPredictions Number of predictions to make
+     * @return Returns the builder to continue chaining
+     */
+    public MovAvgBuilder predict(int numPredictions) {
+        this.predict = numPredictions;
+        return this;
+    }
+
+    /**
+     * Determines if the model should be fit to the data using a cost
+     * minimizing algorithm.
+     *
+     * @param minimize If the model should be fit to the underlying data
+     * @return Returns the builder to continue chaining
+     */
+    public MovAvgBuilder minimize(boolean minimize) {
+        this.minimize = minimize;
+        return this;
+    }
+
+    /**
+     * The hash of settings that should be provided to the model when it is
+     * instantiated
+     */
+    public MovAvgBuilder settings(Map<String, Object> settings) {
+        this.settings = settings;
+        return this;
+    }
+
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (format != null) {
+            builder.field(MovAvgParser.FORMAT.getPreferredName(), format);
+        }
+        if (gapPolicy != null) {
+            builder.field(MovAvgParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        if (modelBuilder != null) {
+            modelBuilder.toXContent(builder, params);
+        }
+        if (window != null) {
+            builder.field(MovAvgParser.WINDOW.getPreferredName(), window);
+        }
+        if (predict != null) {
+            builder.field(MovAvgParser.PREDICT.getPreferredName(), predict);
+        }
+        if (minimize != null) {
+            builder.field(MovAvgParser.MINIMIZE.getPreferredName(), minimize);
+        }
+        if (settings != null) {
+            builder.field(MovAvgParser.SETTINGS.getPreferredName(), settings);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java
index 62bcf61..5856735 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java
@@ -20,15 +20,17 @@
 package org.elasticsearch.search.aggregations.pipeline.movavg;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModel;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelParserMapper;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.text.ParseException;
@@ -57,18 +59,17 @@ public class MovAvgParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context)
-            throws IOException {
+    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
         String format = null;
 
-        GapPolicy gapPolicy = null;
-        Integer window = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
+        int window = 5;
         Map<String, Object> settings = null;
-        String model = null;
-        Integer predict = null;
+        String model = "simple";
+        int predict = 0;
         Boolean minimize = null;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -78,18 +79,20 @@ public class MovAvgParser implements PipelineAggregator.Parser {
                 if (context.parseFieldMatcher().match(currentFieldName, WINDOW)) {
                     window = parser.intValue();
                     if (window <= 0) {
-                        throw new ParsingException(parser.getTokenLocation(), "[" + currentFieldName + "] value must be a positive, "
-                                + "non-zero integer.  Value supplied was [" + predict + "] in [" + pipelineAggregatorName + "].");
+                        throw new SearchParseException(context, "[" + currentFieldName + "] value must be a positive, "
+                                + "non-zero integer.  Value supplied was [" + predict + "] in [" + pipelineAggregatorName + "].",
+                                parser.getTokenLocation());
                     }
                 } else if (context.parseFieldMatcher().match(currentFieldName, PREDICT)) {
                     predict = parser.intValue();
                     if (predict <= 0) {
-                        throw new ParsingException(parser.getTokenLocation(), "[" + currentFieldName + "] value must be a positive integer."
-                                + "  Value supplied was [" + predict + "] in [" + pipelineAggregatorName + "].");
+                        throw new SearchParseException(context, "[" + currentFieldName + "] value must be a positive, "
+                                + "non-zero integer.  Value supplied was [" + predict + "] in [" + pipelineAggregatorName + "].",
+                                parser.getTokenLocation());
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.VALUE_STRING) {
                 if (context.parseFieldMatcher().match(currentFieldName, FORMAT)) {
@@ -101,8 +104,8 @@ public class MovAvgParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, MODEL)) {
                     model = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -113,71 +116,66 @@ public class MovAvgParser implements PipelineAggregator.Parser {
                     }
                     bucketsPaths = paths.toArray(new String[paths.size()]);
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (context.parseFieldMatcher().match(currentFieldName, SETTINGS)) {
                     settings = parser.map();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
                 if (context.parseFieldMatcher().match(currentFieldName, MINIMIZE)) {
                     minimize = parser.booleanValue();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " in [" + pipelineAggregatorName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + pipelineAggregatorName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for movingAvg aggregation [" + pipelineAggregatorName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for movingAvg aggregation [" + pipelineAggregatorName + "]", parser.getTokenLocation());
         }
 
-        MovAvgPipelineAggregator.Factory factory = new MovAvgPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths[0]);
+        ValueFormatter formatter = null;
         if (format != null) {
-            factory.format(format);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
         }
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
-        }
-        if (window != null) {
-            factory.window(window);
-        }
-        if (predict != null) {
-            factory.predict(predict);
+
+        MovAvgModel.AbstractModelParser modelParser = movAvgModelParserMapper.get(model);
+        if (modelParser == null) {
+            throw new SearchParseException(context, "Unknown model [" + model + "] specified.  Valid options are:"
+                    + movAvgModelParserMapper.getAllNames().toString(), parser.getTokenLocation());
         }
-        if (model != null) {
-            MovAvgModel.AbstractModelParser modelParser = movAvgModelParserMapper.get(model);
-            if (modelParser == null) {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unknown model [" + model + "] specified.  Valid options are:" + movAvgModelParserMapper.getAllNames().toString());
-            }
 
-            MovAvgModel movAvgModel;
-            try {
-                movAvgModel = modelParser.parse(settings, pipelineAggregatorName, window, context.parseFieldMatcher());
-            } catch (ParseException exception) {
-                throw new ParsingException(parser.getTokenLocation(), "Could not parse settings for model [" + model + "].", exception);
-            }
-            factory.model(movAvgModel);
+        MovAvgModel movAvgModel;
+        try {
+            movAvgModel = modelParser.parse(settings, pipelineAggregatorName, window, context.parseFieldMatcher());
+        } catch (ParseException exception) {
+            throw new SearchParseException(context, "Could not parse settings for model [" + model + "].", null, exception);
         }
-        if (minimize != null) {
-            factory.minimize(minimize);
+
+        // If the user doesn't set a preference for cost minimization, ask what the model prefers
+        if (minimize == null) {
+            minimize = movAvgModel.minimizeByDefault();
+        } else if (minimize && !movAvgModel.canBeMinimized()) {
+            // If the user asks to minimize, but this model doesn't support it, throw exception
+            throw new SearchParseException(context, "The [" + model + "] model cannot be minimized.", null);
         }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new MovAvgPipelineAggregator.Factory(null, null);
+
+        return new MovAvgPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, formatter, gapPolicy, window, predict,
+                movAvgModel, minimize);
     }
 
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
index dcb7ae7..4f7034b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
@@ -22,7 +22,6 @@ package org.elasticsearch.search.aggregations.pipeline.movavg;
 import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -37,10 +36,7 @@ import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelBuilder;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelStreams;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.SimpleModel;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 import org.joda.time.DateTime;
@@ -50,7 +46,6 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.ListIterator;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
@@ -281,176 +276,32 @@ public class MovAvgPipelineAggregator extends PipelineAggregator {
 
     public static class Factory extends PipelineAggregatorFactory {
 
-        private String format;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
-        private int window = 5;
-        private MovAvgModel model = new SimpleModel();
-        private int predict = 0;
-        private Boolean minimize;
+        private final ValueFormatter formatter;
+        private GapPolicy gapPolicy;
+        private int window;
+        private MovAvgModel model;
+        private int predict;
+        private boolean minimize;
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
-
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, ValueFormatter formatter, GapPolicy gapPolicy,
+                       int window, int predict, MovAvgModel model, boolean minimize) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Sets the format to use on the output of this aggregation.
-         */
-        public Factory format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        /**
-         * Gets the format to use on the output of this aggregation.
-         */
-        public String format() {
-            return format;
-        }
-
-        /**
-         * Sets the GapPolicy to use on the output of this aggregation.
-         */
-        public Factory gapPolicy(GapPolicy gapPolicy) {
+            this.formatter = formatter;
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        /**
-         * Gets the GapPolicy to use on the output of this aggregation.
-         */
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
-        }
-
-        protected ValueFormatter formatter() {
-            if (format != null) {
-                return ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                return ValueFormatter.RAW;
-            }
-        }
-
-        /**
-         * Sets the window size for the moving average. This window will "slide"
-         * across the series, and the values inside that window will be used to
-         * calculate the moving avg value
-         *
-         * @param window
-         *            Size of window
-         */
-        public Factory window(int window) {
             this.window = window;
-            return this;
-        }
-
-        /**
-         * Gets the window size for the moving average. This window will "slide"
-         * across the series, and the values inside that window will be used to
-         * calculate the moving avg value
-         */
-        public int window() {
-            return window;
-        }
-
-        /**
-         * Sets a MovAvgModel for the Moving Average. The model is used to
-         * define what type of moving average you want to use on the series
-         *
-         * @param model
-         *            A MovAvgModel which has been prepopulated with settings
-         */
-        public Factory modelBuilder(MovAvgModelBuilder model) {
-            this.model = model.build();
-            return this;
-        }
-
-        /**
-         * Sets a MovAvgModel for the Moving Average. The model is used to
-         * define what type of moving average you want to use on the series
-         *
-         * @param model
-         *            A MovAvgModel which has been prepopulated with settings
-         */
-        public Factory model(MovAvgModel model) {
             this.model = model;
-            return this;
-        }
-
-        /**
-         * Gets a MovAvgModel for the Moving Average. The model is used to
-         * define what type of moving average you want to use on the series
-         */
-        public MovAvgModel model() {
-            return model;
-        }
-
-        /**
-         * Sets the number of predictions that should be returned. Each
-         * prediction will be spaced at the intervals specified in the
-         * histogram. E.g "predict: 2" will return two new buckets at the end of
-         * the histogram with the predicted values.
-         *
-         * @param predict
-         *            Number of predictions to make
-         */
-        public Factory predict(int predict) {
-            if (predict <= 0) {
-                throw new IllegalArgumentException("predict must be greater than 0. Found [" + predict + "] in [" + name + "]");
-            }
             this.predict = predict;
-            return this;
-        }
-
-        /**
-         * Gets the number of predictions that should be returned. Each
-         * prediction will be spaced at the intervals specified in the
-         * histogram. E.g "predict: 2" will return two new buckets at the end of
-         * the histogram with the predicted values.
-         */
-        public int predict() {
-            return predict;
-        }
-
-        /**
-         * Sets whether the model should be fit to the data using a cost
-         * minimizing algorithm.
-         *
-         * @param minimize
-         *            If the model should be fit to the underlying data
-         */
-        public Factory minimize(boolean minimize) {
             this.minimize = minimize;
-            return this;
-        }
-
-        /**
-         * Gets whether the model should be fit to the data using a cost
-         * minimizing algorithm.
-         */
-        public Boolean minimize() {
-            return minimize;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            // If the user doesn't set a preference for cost minimization, ask
-            // what the model prefers
-            boolean minimize = this.minimize == null ? model.minimizeByDefault() : this.minimize;
-            return new MovAvgPipelineAggregator(name, bucketsPaths, formatter(), gapPolicy, window, predict, model, minimize, metaData);
+            return new MovAvgPipelineAggregator(name, bucketsPaths, formatter, gapPolicy, window, predict, model, minimize, metaData);
         }
 
         @Override
         public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
                 List<PipelineAggregatorFactory> pipelineAggregatoractories) {
-            if (minimize != null && minimize && !model.canBeMinimized()) {
-                // If the user asks to minimize, but this model doesn't support
-                // it, throw exception
-                throw new IllegalStateException("The [" + model + "] model cannot be minimized for aggregation [" + name + "]");
-            }
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
@@ -467,60 +318,5 @@ public class MovAvgPipelineAggregator extends PipelineAggregator {
             }
         }
 
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (format != null) {
-                builder.field(MovAvgParser.FORMAT.getPreferredName(), format);
-            }
-            builder.field(MovAvgParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            model.toXContent(builder, params);
-            builder.field(MovAvgParser.WINDOW.getPreferredName(), window);
-            if (predict > 0) {
-                builder.field(MovAvgParser.PREDICT.getPreferredName(), predict);
-            }
-            if (minimize != null) {
-                builder.field(MovAvgParser.MINIMIZE.getPreferredName(), minimize);
-            }
-            return builder;
-        }
-
-        @Override
-        protected PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.format = in.readOptionalString();
-            factory.gapPolicy = GapPolicy.readFrom(in);
-            factory.window = in.readVInt();
-            factory.model = MovAvgModelStreams.read(in);
-            factory.predict = in.readVInt();
-            factory.minimize = in.readOptionalBoolean();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(format);
-            gapPolicy.writeTo(out);
-            out.writeVInt(window);
-            model.writeTo(out);
-            out.writeVInt(predict);
-            out.writeOptionalBoolean(minimize);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(format, gapPolicy, window, model, predict, minimize);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(format, other.format)
-                    && Objects.equals(gapPolicy, other.gapPolicy)
-                    && Objects.equals(window, other.window)
-                    && Objects.equals(model, other.model)
-                    && Objects.equals(predict, other.predict)
-                    && Objects.equals(minimize, other.minimize);
-        }
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java
index edbfa66..84de794 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java
@@ -32,16 +32,13 @@ import java.text.ParseException;
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Calculate a exponentially weighted moving average
  */
 public class EwmaModel extends MovAvgModel {
 
-    private static final EwmaModel PROTOTYPE = new EwmaModel();
     protected static final ParseField NAME_FIELD = new ParseField("ewma");
-    public static final double DEFAULT_ALPHA = 0.3;
 
     /**
      * Controls smoothing of data.  Also known as "level" value.
@@ -51,10 +48,6 @@ public class EwmaModel extends MovAvgModel {
      */
     private final double alpha;
 
-    public EwmaModel() {
-        this(DEFAULT_ALPHA);
-    }
-
     public EwmaModel(double alpha) {
         this.alpha = alpha;
     }
@@ -104,7 +97,7 @@ public class EwmaModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            return new EwmaModel(in.readDouble());
         }
 
         @Override
@@ -114,42 +107,11 @@ public class EwmaModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-        builder.field("alpha", alpha);
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new EwmaModel(in.readDouble());
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
         out.writeDouble(alpha);
     }
 
-    @Override
-    public int hashCode() {
-        return Objects.hash(alpha);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        EwmaModel other = (EwmaModel) obj;
-        return Objects.equals(alpha, other.alpha);
-    }
-
     public static class SingleExpModelParser extends AbstractModelParser {
 
         @Override
@@ -161,7 +123,7 @@ public class EwmaModel extends MovAvgModel {
         public MovAvgModel parse(@Nullable Map<String, Object> settings, String pipelineName, int windowSize,
                                  ParseFieldMatcher parseFieldMatcher) throws ParseException {
 
-            double alpha = parseDoubleParam(settings, "alpha", DEFAULT_ALPHA);
+            double alpha = parseDoubleParam(settings, "alpha", 0.3);
             checkUnrecognizedParams(settings);
             return new EwmaModel(alpha);
         }
@@ -170,7 +132,7 @@ public class EwmaModel extends MovAvgModel {
 
     public static class EWMAModelBuilder implements MovAvgModelBuilder {
 
-        private double alpha = DEFAULT_ALPHA;
+        private Double alpha;
 
         /**
          * Alpha controls the smoothing of the data.  Alpha = 1 retains no memory of past values
@@ -190,16 +152,13 @@ public class EwmaModel extends MovAvgModel {
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
             builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
             builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-            builder.field("alpha", alpha);
+            if (alpha != null) {
+                builder.field("alpha", alpha);
+            }
 
             builder.endObject();
             return builder;
         }
-
-        @Override
-        public MovAvgModel build() {
-            return new EwmaModel(alpha);
-        }
     }
 }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java
index 06ce050..fe0321b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java
@@ -31,17 +31,13 @@ import java.io.IOException;
 import java.text.ParseException;
 import java.util.Collection;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Calculate a doubly exponential weighted moving average
  */
 public class HoltLinearModel extends MovAvgModel {
 
-    private static final HoltLinearModel PROTOTYPE = new HoltLinearModel();
     protected static final ParseField NAME_FIELD = new ParseField("holt");
-    public static final double DEFAULT_ALPHA = 0.3;
-    public static final double DEFAULT_BETA = 0.1;
 
     /**
      * Controls smoothing of data.  Also known as "level" value.
@@ -59,10 +55,6 @@ public class HoltLinearModel extends MovAvgModel {
      */
     private final double beta;
 
-    public HoltLinearModel() {
-        this(DEFAULT_ALPHA, DEFAULT_BETA);
-    }
-
     public HoltLinearModel(double alpha, double beta) {
         this.alpha = alpha;
         this.beta = beta;
@@ -165,7 +157,7 @@ public class HoltLinearModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            return new HoltLinearModel(in.readDouble(), in.readDouble());
         }
 
         @Override
@@ -175,45 +167,12 @@ public class HoltLinearModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-        builder.field("alpha", alpha);
-        builder.field("beta", beta);
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new HoltLinearModel(in.readDouble(), in.readDouble());
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
         out.writeDouble(alpha);
         out.writeDouble(beta);
     }
 
-    @Override
-    public int hashCode() {
-        return Objects.hash(alpha, beta);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        HoltLinearModel other = (HoltLinearModel) obj;
-        return Objects.equals(alpha, other.alpha)
-                && Objects.equals(beta, other.beta);
-    }
-
     public static class DoubleExpModelParser extends AbstractModelParser {
 
         @Override
@@ -225,8 +184,8 @@ public class HoltLinearModel extends MovAvgModel {
         public MovAvgModel parse(@Nullable Map<String, Object> settings, String pipelineName, int windowSize,
                                  ParseFieldMatcher parseFieldMatcher) throws ParseException {
 
-            double alpha = parseDoubleParam(settings, "alpha", DEFAULT_ALPHA);
-            double beta = parseDoubleParam(settings, "beta", DEFAULT_BETA);
+            double alpha = parseDoubleParam(settings, "alpha", 0.3);
+            double beta = parseDoubleParam(settings, "beta", 0.1);
             checkUnrecognizedParams(settings);
             return new HoltLinearModel(alpha, beta);
         }
@@ -235,8 +194,8 @@ public class HoltLinearModel extends MovAvgModel {
     public static class HoltLinearModelBuilder implements MovAvgModelBuilder {
 
 
-        private double alpha = DEFAULT_ALPHA;
-        private double beta = DEFAULT_BETA;
+        private Double alpha;
+        private Double beta;
 
         /**
          * Alpha controls the smoothing of the data.  Alpha = 1 retains no memory of past values
@@ -268,17 +227,18 @@ public class HoltLinearModel extends MovAvgModel {
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
             builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
             builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-            builder.field("alpha", alpha);
-            builder.field("beta", beta);
+
+            if (alpha != null) {
+                builder.field("alpha", alpha);
+            }
+
+            if (beta != null) {
+                builder.field("beta", beta);
+            }
 
             builder.endObject();
             return builder;
         }
-
-        @Override
-        public MovAvgModel build() {
-            return new HoltLinearModel(alpha, beta);
-        }
     }
 }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java
index be8d2fb..55cf6be 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java
@@ -37,7 +37,6 @@ import java.util.Arrays;
 import java.util.Collection;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Calculate a triple exponential weighted moving average
@@ -45,13 +44,6 @@ import java.util.Objects;
 public class HoltWintersModel extends MovAvgModel {
 
     protected static final ParseField NAME_FIELD = new ParseField("holt_winters");
-    public static final double DEFAULT_ALPHA = 0.3;
-    public static final double DEFAULT_BETA = 0.1;
-    public static final double DEFAULT_GAMMA = 0.3;
-    public static final int DEFAULT_PERIOD = 1;
-    public static final SeasonalityType DEFAULT_SEASONALITY_TYPE = SeasonalityType.ADDITIVE;
-    public static final boolean DEFAULT_PAD = false;
-    private static final HoltWintersModel PROTOTYPE = new HoltWintersModel();
 
     /**
      * Controls smoothing of data.  Also known as "level" value.
@@ -167,9 +159,6 @@ public class HoltWintersModel extends MovAvgModel {
         }
     }
 
-    public HoltWintersModel() {
-        this(DEFAULT_ALPHA, DEFAULT_BETA, DEFAULT_GAMMA, DEFAULT_PERIOD, DEFAULT_SEASONALITY_TYPE, DEFAULT_PAD);
-    }
 
     public HoltWintersModel(double alpha, double beta, double gamma, int period, SeasonalityType seasonalityType, boolean pad) {
         this.alpha = alpha;
@@ -284,8 +273,8 @@ public class HoltWintersModel extends MovAvgModel {
             s += vs[i];
             b += (vs[i + period] - vs[i]) / period;
         }
-        s /= period;
-        b /= period;
+        s /= (double) period;
+        b /= (double) period;
         last_s = s;
 
         // Calculate first seasonal
@@ -335,7 +324,14 @@ public class HoltWintersModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            double alpha = in.readDouble();
+            double beta = in.readDouble();
+            double gamma = in.readDouble();
+            int period = in.readVInt();
+            SeasonalityType type = SeasonalityType.readFrom(in);
+            boolean pad = in.readBoolean();
+
+            return new HoltWintersModel(alpha, beta, gamma, period, type, pad);
         }
 
         @Override
@@ -345,26 +341,6 @@ public class HoltWintersModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-        builder.field("alpha", alpha);
-        builder.field("beta", beta);
-        builder.field("gamma", gamma);
-        builder.field("period", period);
-        builder.field("pad", pad);
-        builder.field("type", seasonalityType.getName());
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new HoltWintersModel(in.readDouble(), in.readDouble(), in.readDouble(), in.readVInt(), SeasonalityType.readFrom(in),
-                in.readBoolean());
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
         out.writeDouble(alpha);
@@ -375,28 +351,6 @@ public class HoltWintersModel extends MovAvgModel {
         out.writeBoolean(pad);
     }
 
-    @Override
-    public int hashCode() {
-        return Objects.hash(alpha, beta, gamma, period, seasonalityType, pad);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        HoltWintersModel other = (HoltWintersModel) obj;
-        return Objects.equals(alpha, other.alpha)
-                && Objects.equals(beta, other.beta)
-                && Objects.equals(gamma, other.gamma)
-                && Objects.equals(period, other.period)
-                && Objects.equals(seasonalityType, other.seasonalityType)
-                && Objects.equals(pad, other.pad);
-    }
-
     public static class HoltWintersModelParser extends AbstractModelParser {
 
         @Override
@@ -408,10 +362,10 @@ public class HoltWintersModel extends MovAvgModel {
         public MovAvgModel parse(@Nullable Map<String, Object> settings, String pipelineName, int windowSize,
                                  ParseFieldMatcher parseFieldMatcher) throws ParseException {
 
-            double alpha = parseDoubleParam(settings, "alpha", DEFAULT_ALPHA);
-            double beta = parseDoubleParam(settings, "beta", DEFAULT_BETA);
-            double gamma = parseDoubleParam(settings, "gamma", DEFAULT_GAMMA);
-            int period = parseIntegerParam(settings, "period", DEFAULT_PERIOD);
+            double alpha = parseDoubleParam(settings, "alpha", 0.3);
+            double beta = parseDoubleParam(settings, "beta", 0.1);
+            double gamma = parseDoubleParam(settings, "gamma", 0.3);
+            int period = parseIntegerParam(settings, "period", 1);
 
             if (windowSize < 2 * period) {
                 throw new ParseException("Field [window] must be at least twice as large as the period when " +
@@ -419,7 +373,7 @@ public class HoltWintersModel extends MovAvgModel {
                         + (2 * period), 0);
             }
 
-            SeasonalityType seasonalityType = DEFAULT_SEASONALITY_TYPE;
+            SeasonalityType seasonalityType = SeasonalityType.ADDITIVE;
 
             if (settings != null) {
                 Object value = settings.get("type");
@@ -443,12 +397,12 @@ public class HoltWintersModel extends MovAvgModel {
 
     public static class HoltWintersModelBuilder implements MovAvgModelBuilder {
 
-        private double alpha = DEFAULT_ALPHA;
-        private double beta = DEFAULT_BETA;
-        private double gamma = DEFAULT_GAMMA;
-        private int period = DEFAULT_PERIOD;
-        private SeasonalityType seasonalityType = DEFAULT_SEASONALITY_TYPE;
-        private Boolean pad = null;
+        private Double alpha;
+        private Double beta;
+        private Double gamma;
+        private Integer period;
+        private SeasonalityType seasonalityType;
+        private Boolean pad;
 
         /**
          * Alpha controls the smoothing of the data.  Alpha = 1 retains no memory of past values
@@ -500,24 +454,34 @@ public class HoltWintersModel extends MovAvgModel {
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
             builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
             builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-            builder.field("alpha", alpha);
-            builder.field("beta", beta);
-            builder.field("gamma", gamma);
-            builder.field("period", period);
+
+            if (alpha != null) {
+                builder.field("alpha", alpha);
+            }
+
+            if (beta != null) {
+                builder.field("beta", beta);
+            }
+
+            if (gamma != null) {
+                builder.field("gamma", gamma);
+            }
+
+            if (period != null) {
+                builder.field("period", period);
+            }
+
             if (pad != null) {
                 builder.field("pad", pad);
             }
-            builder.field("type", seasonalityType.getName());
+
+            if (seasonalityType != null) {
+                builder.field("type", seasonalityType.getName());
+            }
 
             builder.endObject();
             return builder;
         }
-
-        @Override
-        public MovAvgModel build() {
-            boolean pad = this.pad == null ? (seasonalityType == SeasonalityType.MULTIPLICATIVE) : this.pad;
-            return new HoltWintersModel(alpha, beta, gamma, period, seasonalityType, pad);
-        }
     }
 }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java
index bc3de8b..264a425 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java
@@ -40,7 +40,6 @@ import java.util.Map;
  */
 public class LinearModel extends MovAvgModel {
 
-    private static final LinearModel PROTOTYPE = new LinearModel();
     protected static final ParseField NAME_FIELD = new ParseField("linear");
 
 
@@ -86,7 +85,7 @@ public class LinearModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            return new LinearModel();
         }
 
         @Override
@@ -96,17 +95,6 @@ public class LinearModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new LinearModel();
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
     }
@@ -132,26 +120,5 @@ public class LinearModel extends MovAvgModel {
             builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
             return builder;
         }
-
-        @Override
-        public MovAvgModel build() {
-            return new LinearModel();
-        }
-    }
-
-    @Override
-    public int hashCode() {
-        return 0;
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        return true;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java
index 92f4615..4bfac9d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java
@@ -22,8 +22,6 @@ package org.elasticsearch.search.aggregations.pipeline.movavg.models;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
 
 import java.io.IOException;
 import java.text.ParseException;
@@ -31,7 +29,7 @@ import java.util.Arrays;
 import java.util.Collection;
 import java.util.Map;
 
-public abstract class MovAvgModel implements Writeable<MovAvgModel>, ToXContent {
+public abstract class MovAvgModel {
 
     /**
      * Should this model be fit to the data via a cost minimizing algorithm by default?
@@ -118,21 +116,13 @@ public abstract class MovAvgModel implements Writeable<MovAvgModel>, ToXContent
      *
      * @param out   Output stream
      */
-    @Override
     public abstract void writeTo(StreamOutput out) throws IOException;
 
     /**
      * Clone the model, returning an exact copy
      */
-    @Override
     public abstract MovAvgModel clone();
 
-    @Override
-    public abstract int hashCode();
-
-    @Override
-    public abstract boolean equals(Object obj);
-
     /**
      * Abstract class which also provides some concrete parsing functionality.
      */
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelBuilder.java
index 759c493..e491d12 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelBuilder.java
@@ -26,6 +26,4 @@ import org.elasticsearch.common.xcontent.ToXContent;
  * average models are used by the MovAvg aggregation
  */
 public interface MovAvgModelBuilder extends ToXContent {
-
-    public MovAvgModel build();
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java
index 61f8c66..e0c7781 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java
@@ -38,7 +38,6 @@ import java.util.Map;
  */
 public class SimpleModel extends MovAvgModel {
 
-    private static final SimpleModel PROTOTYPE = new SimpleModel();
     protected static final ParseField NAME_FIELD = new ParseField("simple");
 
 
@@ -79,7 +78,7 @@ public class SimpleModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            return new SimpleModel();
         }
 
         @Override
@@ -89,17 +88,6 @@ public class SimpleModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new SimpleModel();
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
     }
@@ -125,26 +113,5 @@ public class SimpleModel extends MovAvgModel {
             builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
             return builder;
         }
-
-        @Override
-        public MovAvgModel build() {
-            return new SimpleModel();
-        }
-    }
-
-    @Override
-    public int hashCode() {
-        return 0;
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        return true;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffBuilder.java
new file mode 100644
index 0000000..052f3f0
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffBuilder.java
@@ -0,0 +1,67 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.serialdiff;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+
+import java.io.IOException;
+
+public class SerialDiffBuilder extends PipelineAggregatorBuilder<SerialDiffBuilder> {
+
+    private String format;
+    private GapPolicy gapPolicy;
+    private Integer lag;
+
+    public SerialDiffBuilder(String name) {
+        super(name, SerialDiffPipelineAggregator.TYPE.name());
+    }
+
+    public SerialDiffBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    public SerialDiffBuilder gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return this;
+    }
+
+    public SerialDiffBuilder lag(Integer lag) {
+        this.lag = lag;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (format != null) {
+            builder.field(SerialDiffParser.FORMAT.getPreferredName(), format);
+        }
+        if (gapPolicy != null) {
+            builder.field(SerialDiffParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        if (lag != null) {
+            builder.field(SerialDiffParser.LAG.getPreferredName(), lag);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java
index fd9abc2..109cbcc 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java
@@ -20,17 +20,20 @@
 package org.elasticsearch.search.aggregations.pipeline.serialdiff;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+
 public class SerialDiffParser implements PipelineAggregator.Parser {
 
     public static final ParseField FORMAT = new ParseField("format");
@@ -43,13 +46,13 @@ public class SerialDiffParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, QueryParseContext context) throws IOException {
+    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
         String format = null;
-        GapPolicy gapPolicy = null;
-        Integer lag = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
+        int lag = 1;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -62,21 +65,20 @@ public class SerialDiffParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, GAP_POLICY)) {
                     gapPolicy = GapPolicy.parse(context, parser.text(), parser.getTokenLocation());
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.VALUE_NUMBER) {
                 if (context.parseFieldMatcher().match(currentFieldName, LAG)) {
                     lag = parser.intValue(true);
                     if (lag <= 0) {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Lag must be a positive, non-zero integer.  Value supplied was" +
+                        throw new SearchParseException(context, "Lag must be a positive, non-zero integer.  Value supplied was" +
                                 lag + " in [" + reducerName + "]: ["
-                                        + currentFieldName + "].");
+                                + currentFieldName + "].", parser.getTokenLocation());
                     }
                 }  else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -87,36 +89,28 @@ public class SerialDiffParser implements PipelineAggregator.Parser {
                     }
                     bucketsPaths = paths.toArray(new String[paths.size()]);
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + reducerName + "].",
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + reducerName + "].",
                         parser.getTokenLocation());
             }
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Missing required field [" + BUCKETS_PATH.getPreferredName() + "] for derivative aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for derivative aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
-        SerialDiffPipelineAggregator.Factory factory = new SerialDiffPipelineAggregator.Factory(reducerName, bucketsPaths[0]);
-        if (lag != null) {
-            factory.lag(lag);
-        }
+        ValueFormatter formatter;
         if (format != null) {
-            factory.format(format);
-        }
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        }  else {
+            formatter = ValueFormatter.RAW;
         }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new SerialDiffPipelineAggregator.Factory(null, null);
+        return new SerialDiffPipelineAggregator.Factory(reducerName, bucketsPaths, formatter, gapPolicy, lag);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
index 1bf4f82..5df97d3 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
@@ -23,18 +23,15 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.InternalAggregations;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -42,10 +39,10 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;
 
 public class SerialDiffPipelineAggregator extends PipelineAggregator {
@@ -147,112 +144,20 @@ public class SerialDiffPipelineAggregator extends PipelineAggregator {
 
     public static class Factory extends PipelineAggregatorFactory {
 
-        private String format;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
-        private int lag = 1;
+        private final ValueFormatter formatter;
+        private GapPolicy gapPolicy;
+        private int lag;
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
-
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, @Nullable ValueFormatter formatter, GapPolicy gapPolicy, int lag) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Sets the lag to use when calculating the serial difference.
-         */
-        public Factory lag(int lag) {
-            this.lag = lag;
-            return this;
-        }
-
-        /**
-         * Gets the lag to use when calculating the serial difference.
-         */
-        public int lag() {
-            return lag;
-        }
-
-        /**
-         * Sets the format to use on the output of this aggregation.
-         */
-        public Factory format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        /**
-         * Gets the format to use on the output of this aggregation.
-         */
-        public String format() {
-            return format;
-        }
-
-        /**
-         * Sets the GapPolicy to use on the output of this aggregation.
-         */
-        public Factory gapPolicy(GapPolicy gapPolicy) {
+            this.formatter = formatter;
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        /**
-         * Gets the GapPolicy to use on the output of this aggregation.
-         */
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
-        }
-
-        protected ValueFormatter formatter() {
-            if (format != null) {
-                return ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                return ValueFormatter.RAW;
-            }
+            this.lag = lag;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new SerialDiffPipelineAggregator(name, bucketsPaths, formatter(), gapPolicy, lag, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (format != null) {
-                builder.field(SerialDiffParser.FORMAT.getPreferredName(), format);
-            }
-            builder.field(SerialDiffParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            builder.field(SerialDiffParser.LAG.getPreferredName(), lag);
-            return builder;
-        }
-
-        @Override
-        protected PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.format = in.readOptionalString();
-            factory.gapPolicy = GapPolicy.readFrom(in);
-            factory.lag = in.readVInt();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(format);
-            gapPolicy.writeTo(out);
-            out.writeVInt(lag);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(format, gapPolicy, lag);
-        }
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(format, other.format)
-                    && Objects.equals(gapPolicy, other.gapPolicy)
-                    && Objects.equals(lag, other.lag);
+            return new SerialDiffPipelineAggregator(name, bucketsPaths, formatter, gapPolicy, lag, metaData);
         }
 
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/AbstractValuesSourceParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/AbstractValuesSourceParser.java
deleted file mode 100644
index 12bae3b..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/AbstractValuesSourceParser.java
+++ /dev/null
@@ -1,208 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.support;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.Script.ScriptField;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.joda.time.DateTimeZone;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- *
- */
-public abstract class AbstractValuesSourceParser<VS extends ValuesSource>
-        implements Aggregator.Parser {
-    static final ParseField TIME_ZONE = new ParseField("time_zone");
-
-    public abstract static class AnyValuesSourceParser extends AbstractValuesSourceParser<ValuesSource> {
-
-        protected AnyValuesSourceParser(boolean scriptable, boolean formattable) {
-            super(scriptable, formattable, false, ValuesSourceType.ANY, null);
-        }
-    }
-
-    public abstract static class NumericValuesSourceParser extends AbstractValuesSourceParser<ValuesSource.Numeric> {
-
-        protected NumericValuesSourceParser(boolean scriptable, boolean formattable, boolean timezoneAware) {
-            super(scriptable, formattable, timezoneAware, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-        }
-    }
-
-    public abstract static class BytesValuesSourceParser extends AbstractValuesSourceParser<ValuesSource.Bytes> {
-
-        protected BytesValuesSourceParser(boolean scriptable, boolean formattable) {
-            super(scriptable, formattable, false, ValuesSourceType.BYTES, ValueType.STRING);
-        }
-    }
-
-    public abstract static class GeoPointValuesSourceParser extends AbstractValuesSourceParser<ValuesSource.GeoPoint> {
-
-        protected GeoPointValuesSourceParser(boolean scriptable, boolean formattable) {
-            super(scriptable, formattable, false, ValuesSourceType.GEOPOINT, ValueType.GEOPOINT);
-        }
-    }
-
-    private boolean scriptable = true;
-    private boolean formattable = false;
-    private boolean timezoneAware = false;
-    private ValuesSourceType valuesSourceType = null;
-    private ValueType targetValueType = null;
-
-    private AbstractValuesSourceParser(boolean scriptable, boolean formattable, boolean timezoneAware, ValuesSourceType valuesSourceType,
-            ValueType targetValueType) {
-        this.timezoneAware = timezoneAware;
-        this.valuesSourceType = valuesSourceType;
-        this.targetValueType = targetValueType;
-        this.scriptable = scriptable;
-        this.formattable = formattable;
-    }
-
-    @Override
-    public final ValuesSourceAggregatorFactory<VS, ?> parse(String aggregationName, XContentParser parser, QueryParseContext context)
-            throws IOException {
-
-        String field = null;
-        Script script = null;
-        ValueType valueType = null;
-        String format = null;
-        Object missing = null;
-        DateTimeZone timezone = null;
-        Map<ParseField, Object> otherOptions = new HashMap<>();
-
-        XContentParser.Token token;
-        String currentFieldName = null;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if ("missing".equals(currentFieldName) && token.isValue()) {
-                missing = parser.objectText();
-            } else if (timezoneAware && context.parseFieldMatcher().match(currentFieldName, TIME_ZONE)) {
-                if (token == XContentParser.Token.VALUE_STRING) {
-                    timezone = DateTimeZone.forID(parser.text());
-                } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                    timezone = DateTimeZone.forOffsetHours(parser.intValue());
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-                }
-            } else if (token == XContentParser.Token.VALUE_STRING) {
-                if ("field".equals(currentFieldName)) {
-                    field = parser.text();
-                } else if (formattable && "format".equals(currentFieldName)) {
-                    format = parser.text();
-                } else if (scriptable) {
-                    if ("value_type".equals(currentFieldName) || "valueType".equals(currentFieldName)) {
-                        valueType = ValueType.resolveForScript(parser.text());
-                        if (targetValueType != null && valueType.isNotA(targetValueType)) {
-                            throw new ParsingException(parser.getTokenLocation(),
-                                    type() + " aggregation [" + aggregationName + "] was configured with an incompatible value type ["
-                                            + valueType + "]. [" + type() + "] aggregation can only work on value of type ["
-                                            + targetValueType + "]");
-                        }
-                    } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-                    }
-                } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-                }
-            } else if (scriptable && token == XContentParser.Token.START_OBJECT) {
-                if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
-                    script = Script.parse(parser, context.parseFieldMatcher());
-                } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-                }
-            } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-            }
-        }
-
-        ValuesSourceAggregatorFactory<VS, ?> factory = createFactory(aggregationName, this.valuesSourceType, this.targetValueType,
-                otherOptions);
-        factory.field(field);
-        factory.script(script);
-        factory.valueType(valueType);
-        factory.format(format);
-        factory.missing(missing);
-        factory.timeZone(timezone);
-        return factory;
-    }
-
-    /**
-     * Creates a {@link ValuesSourceAggregatorFactory} from the information
-     * gathered by the subclass. Options parsed in
-     * {@link AbstractValuesSourceParser} itself will be added to the factory
-     * after it has been returned by this method.
-     *
-     * @param aggregationName
-     *            the name of the aggregation
-     * @param valuesSourceType
-     *            the type of the {@link ValuesSource}
-     * @param targetValueType
-     *            the target type of the final value output by the aggregation
-     * @param otherOptions
-     *            a {@link Map} containing the extra options parsed by the
-     *            {@link #token(String, String, org.elasticsearch.common.xcontent.XContentParser.Token, XContentParser, ParseFieldMatcher, Map)}
-     *            method
-     * @return the created factory
-     */
-    protected abstract ValuesSourceAggregatorFactory<VS, ?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions);
-
-    /**
-     * Allows subclasses of {@link AbstractValuesSourceParser} to parse extra
-     * parameters and store them in a {@link Map} which will later be passed to
-     * {@link #createFactory(String, ValuesSourceType, ValueType, Map)}.
-     *
-     * @param aggregationName
-     *            the name of the aggregation
-     * @param currentFieldName
-     *            the name of the current field being parsed
-     * @param token
-     *            the current token for the parser
-     * @param parser
-     *            the parser
-     * @param parseFieldMatcher
-     *            the {@link ParseFieldMatcher} to use to match field names
-     * @param otherOptions
-     *            a {@link Map} of options to be populated by successive calls
-     *            to this method which will then be passed to the
-     *            {@link #createFactory(String, ValuesSourceType, ValueType, Map)}
-     *            method
-     * @return <code>true</code> if the current token was correctly parsed,
-     *         <code>false</code> otherwise
-     * @throws IOException
-     *             if an error occurs whilst parsing
-     */
-    protected abstract boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException;
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java
index 14e8481..ee91782 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java
@@ -70,11 +70,13 @@ public class AggregationContext {
             if (config.missing == null) {
                 // otherwise we will have values because of the missing value
                 vs = null;
-            } else if (config.valueSourceType == ValuesSourceType.NUMERIC) {
+            } else if (ValuesSource.Numeric.class.isAssignableFrom(config.valueSourceType)) {
                 vs = (VS) ValuesSource.Numeric.EMPTY;
-            } else if (config.valueSourceType == ValuesSourceType.GEOPOINT) {
+            } else if (ValuesSource.GeoPoint.class.isAssignableFrom(config.valueSourceType)) {
                 vs = (VS) ValuesSource.GeoPoint.EMPTY;
-            } else if (config.valueSourceType == ValuesSourceType.ANY || config.valueSourceType == ValuesSourceType.BYTES) {
+            } else if (ValuesSource.class.isAssignableFrom(config.valueSourceType)
+                    || ValuesSource.Bytes.class.isAssignableFrom(config.valueSourceType)
+                    || ValuesSource.Bytes.WithOrdinals.class.isAssignableFrom(config.valueSourceType)) {
                 vs = (VS) ValuesSource.Bytes.EMPTY;
             } else {
                 throw new SearchParseException(searchContext, "Can't deal with unmapped ValuesSource type " + config.valueSourceType, null);
@@ -130,20 +132,19 @@ public class AggregationContext {
      */
     private <VS extends ValuesSource> VS originalValuesSource(ValuesSourceConfig<VS> config) throws IOException {
         if (config.fieldContext == null) {
-            if (config.valueSourceType == ValuesSourceType.NUMERIC) {
+            if (ValuesSource.Numeric.class.isAssignableFrom(config.valueSourceType)) {
                 return (VS) numericScript(config);
             }
-            if (config.valueSourceType == ValuesSourceType.BYTES) {
+            if (ValuesSource.Bytes.class.isAssignableFrom(config.valueSourceType)) {
                 return (VS) bytesScript(config);
             }
-            throw new AggregationExecutionException("value source of type [" + config.valueSourceType.name()
-                    + "] is not supported by scripts");
+            throw new AggregationExecutionException("value source of type [" + config.valueSourceType.getSimpleName() + "] is not supported by scripts");
         }
 
-        if (config.valueSourceType == ValuesSourceType.NUMERIC) {
+        if (ValuesSource.Numeric.class.isAssignableFrom(config.valueSourceType)) {
             return (VS) numericField(config);
         }
-        if (config.valueSourceType == ValuesSourceType.GEOPOINT) {
+        if (ValuesSource.GeoPoint.class.isAssignableFrom(config.valueSourceType)) {
             return (VS) geoPointField(config);
         }
         // falling back to bytes values
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java
index fd2f363..3dfab20 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java
@@ -19,39 +19,41 @@
 
 package org.elasticsearch.search.aggregations.support;
 
-
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  *
  */
 public class GeoPointParser {
 
+    private final String aggName;
     private final InternalAggregation.Type aggType;
+    private final SearchContext context;
     private final ParseField field;
 
-    public GeoPointParser(InternalAggregation.Type aggType, ParseField field) {
+    GeoPoint point;
+
+    public GeoPointParser(String aggName, InternalAggregation.Type aggType, SearchContext context, ParseField field) {
+        this.aggName = aggName;
         this.aggType = aggType;
+        this.context = context;
         this.field = field;
     }
 
-    public boolean token(String aggName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (!parseFieldMatcher.match(currentFieldName, field)) {
+    public boolean token(String currentFieldName, XContentParser.Token token, XContentParser parser) throws IOException {
+        if (!context.parseFieldMatcher().match(currentFieldName, field)) {
             return false;
         }
         if (token == XContentParser.Token.VALUE_STRING) {
-            GeoPoint point = new GeoPoint();
+            point = new GeoPoint();
             point.resetFromString(parser.text());
-            otherOptions.put(field, point);
             return true;
         }
         if (token == XContentParser.Token.START_ARRAY) {
@@ -63,12 +65,12 @@ public class GeoPointParser {
                 } else if (Double.isNaN(lat)) {
                     lat = parser.doubleValue();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(), "malformed [" + currentFieldName + "] geo point array in ["
-                            + aggName + "] " + aggType + " aggregation. a geo point array must be of the form [lon, lat]");
+                    throw new SearchParseException(context, "malformed [" + currentFieldName + "] geo point array in [" +
+                            aggName + "] " + aggType + " aggregation. a geo point array must be of the form [lon, lat]", 
+                            parser.getTokenLocation());
                 }
             }
-            GeoPoint point = new GeoPoint(lat, lon);
-            otherOptions.put(field, point);
+            point = new GeoPoint(lat, lon);
             return true;
         }
         if (token == XContentParser.Token.START_OBJECT) {
@@ -86,15 +88,17 @@ public class GeoPointParser {
                 }
             }
             if (Double.isNaN(lat) || Double.isNaN(lon)) {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "malformed [" + currentFieldName + "] geo point object. either [lat] or [lon] (or both) are " + "missing in ["
-                                + aggName + "] " + aggType + " aggregation");
+                throw new SearchParseException(context, "malformed [" + currentFieldName + "] geo point object. either [lat] or [lon] (or both) are " +
+                        "missing in [" + aggName + "] " + aggType + " aggregation", parser.getTokenLocation());
             }
-            GeoPoint point = new GeoPoint(lat, lon);
-            otherOptions.put(field, point);
+            point = new GeoPoint(lat, lon);
             return true;
         }
         return false;
     }
 
+    public GeoPoint geoPoint() {
+        return point;
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java
index 688e447..0ae99b4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java
@@ -19,35 +19,26 @@
 
 package org.elasticsearch.search.aggregations.support;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.index.fielddata.IndexFieldData;
 import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
 import org.elasticsearch.index.fielddata.IndexNumericFieldData;
 import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 
-import java.io.IOException;
-
 /**
  *
  */
-public enum ValueType implements Writeable<ValueType> {
+public enum ValueType {
 
     @Deprecated
-    ANY((byte) 0, "any", "any", ValuesSourceType.ANY, IndexFieldData.class, ValueFormat.RAW), 
-    STRING((byte) 1, "string", "string", ValuesSourceType.BYTES,
-            IndexFieldData.class,
+    ANY("any", ValuesSource.class, IndexFieldData.class, ValueFormat.RAW), STRING("string", ValuesSource.Bytes.class, IndexFieldData.class,
             ValueFormat.RAW),
-    LONG((byte) 2, "byte|short|integer|long", "long",
-                    ValuesSourceType.NUMERIC,
-            IndexNumericFieldData.class, ValueFormat.RAW) {
+    LONG("byte|short|integer|long", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    DOUBLE((byte) 3, "float|double", "double", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.RAW) {
+    DOUBLE("float|double", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isNumeric() {
             return true;
@@ -58,31 +49,31 @@ public enum ValueType implements Writeable<ValueType> {
             return true;
         }
     },
-    NUMBER((byte) 4, "number", "number", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.RAW) {
+    NUMBER("number", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    DATE((byte) 5, "date", "date", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.DateTime.DEFAULT) {
+    DATE("date", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.DateTime.DEFAULT) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    IP((byte) 6, "ip", "ip", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.IPv4) {
+    IP("ip", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.IPv4) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    NUMERIC((byte) 7, "numeric", "numeric", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.RAW) {
+    NUMERIC("numeric", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    GEOPOINT((byte) 8, "geo_point", "geo_point", ValuesSourceType.GEOPOINT, IndexGeoPointFieldData.class, ValueFormat.RAW) {
+    GEOPOINT("geo_point", ValuesSource.GeoPoint.class, IndexGeoPointFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isGeoPoint() {
             return true;
@@ -90,17 +81,12 @@ public enum ValueType implements Writeable<ValueType> {
     };
 
     final String description;
-    final ValuesSourceType valuesSourceType;
+    final Class<? extends ValuesSource> valuesSourceType;
     final Class<? extends IndexFieldData> fieldDataType;
     final ValueFormat defaultFormat;
-    private final byte id;
-    private String preferredName;
 
-    private ValueType(byte id, String description, String preferredName, ValuesSourceType valuesSourceType, Class<? extends IndexFieldData> fieldDataType,
-            ValueFormat defaultFormat) {
-        this.id = id;
+    private ValueType(String description, Class<? extends ValuesSource> valuesSourceType, Class<? extends IndexFieldData> fieldDataType, ValueFormat defaultFormat) {
         this.description = description;
-        this.preferredName = preferredName;
         this.valuesSourceType = valuesSourceType;
         this.fieldDataType = fieldDataType;
         this.defaultFormat = defaultFormat;
@@ -110,11 +96,7 @@ public enum ValueType implements Writeable<ValueType> {
         return description;
     }
 
-    public String getPreferredName() {
-        return preferredName;
-    }
-    
-    public ValuesSourceType getValuesSourceType() {
+    public Class<? extends ValuesSource> getValuesSourceType() {
         return valuesSourceType;
     }
 
@@ -123,7 +105,7 @@ public enum ValueType implements Writeable<ValueType> {
     }
 
     public boolean isA(ValueType valueType) {
-        return valueType.valuesSourceType == valuesSourceType &&
+        return valueType.valuesSourceType.isAssignableFrom(valuesSourceType) &&
                 valueType.fieldDataType.isAssignableFrom(fieldDataType);
     }
 
@@ -167,20 +149,4 @@ public enum ValueType implements Writeable<ValueType> {
     public String toString() {
         return description;
     }
-
-    @Override
-    public ValueType readFrom(StreamInput in) throws IOException {
-        byte id = in.readByte();
-        for (ValueType valueType : values()) {
-            if (id == valueType.id) {
-                return valueType;
-            }
-        }
-        throw new IOException("No valueType found for id [" + id + "]");
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeByte(id);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java
index cdd3cf0..d0eaec2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java
@@ -18,172 +18,40 @@
  */
 package org.elasticsearch.search.aggregations.support;
 
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
-import org.elasticsearch.index.fielddata.IndexNumericFieldData;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.core.BooleanFieldMapper;
-import org.elasticsearch.index.mapper.core.DateFieldMapper;
-import org.elasticsearch.index.mapper.core.NumberFieldMapper;
-import org.elasticsearch.index.mapper.ip.IpFieldMapper;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.SearchScript;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.AggregationInitializationException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormat;
-import org.elasticsearch.search.internal.SearchContext;
-import org.joda.time.DateTimeZone;
 
 import java.io.IOException;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
-public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource, AF extends ValuesSourceAggregatorFactory<VS, AF>>
-        extends AggregatorFactory<AF> {
+public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource> extends AggregatorFactory {
 
-    public static abstract class LeafOnly<VS extends ValuesSource, AF extends ValuesSourceAggregatorFactory<VS, AF>>
-            extends ValuesSourceAggregatorFactory<VS, AF> {
+    public static abstract class LeafOnly<VS extends ValuesSource> extends ValuesSourceAggregatorFactory<VS> {
 
-        protected LeafOnly(String name, Type type, ValuesSourceType valuesSourceType, ValueType targetValueType) {
-            super(name, type, valuesSourceType, targetValueType);
+        protected LeafOnly(String name, String type, ValuesSourceConfig<VS> valuesSourceConfig) {
+            super(name, type, valuesSourceConfig);
         }
 
         @Override
-        public AF subFactories(AggregatorFactories subFactories) {
+        public AggregatorFactory subFactories(AggregatorFactories subFactories) {
             throw new AggregationInitializationException("Aggregator [" + name + "] of type [" + type + "] cannot accept sub-aggregations");
         }
     }
 
-    private final ValuesSourceType valuesSourceType;
-    private final ValueType targetValueType;
-    private String field = null;
-    private Script script = null;
-    private ValueType valueType = null;
-    private String format = null;
-    private Object missing = null;
-    private DateTimeZone timeZone;
     protected ValuesSourceConfig<VS> config;
 
-    protected ValuesSourceAggregatorFactory(String name, Type type, ValuesSourceType valuesSourceType, ValueType targetValueType) {
+    protected ValuesSourceAggregatorFactory(String name, String type, ValuesSourceConfig<VS> config) {
         super(name, type);
-        this.valuesSourceType = valuesSourceType;
-        this.targetValueType = targetValueType;
-    }
-
-    /**
-     * Sets the field to use for this aggregation.
-     */
-    public AF field(String field) {
-        this.field = field;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the field to use for this aggregation.
-     */
-    public String field() {
-        return field;
-    }
-
-    /**
-     * Sets the script to use for this aggregation.
-     */
-    public AF script(Script script) {
-        this.script = script;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the script to use for this aggregation.
-     */
-    public Script script() {
-        return script;
-    }
-
-    /**
-     * Sets the {@link ValueType} for the value produced by this aggregation
-     */
-    public AF valueType(ValueType valueType) {
-        this.valueType = valueType;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the {@link ValueType} for the value produced by this aggregation
-     */
-    public ValueType valueType() {
-        return valueType;
-    }
-
-    /**
-     * Sets the format to use for the output of the aggregation.
-     */
-    public AF format(String format) {
-        this.format = format;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the format to use for the output of the aggregation.
-     */
-    public String format() {
-        return format;
-    }
-
-    /**
-     * Sets the value to use when the aggregation finds a missing value in a
-     * document
-     */
-    public AF missing(Object missing) {
-        this.missing = missing;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the value to use when the aggregation finds a missing value in a
-     * document
-     */
-    public Object missing() {
-        return missing;
-    }
-
-    /**
-     * Sets the time zone to use for this aggregation
-     */
-    public AF timeZone(DateTimeZone timeZone) {
-        this.timeZone = timeZone;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the time zone to use for this aggregation
-     */
-    public DateTimeZone timeZone() {
-        return timeZone;
-    }
-
-    @Override
-    public void doInit(AggregationContext context) {
-        this.config = config(context);
-        if (config == null || !config.valid()) {
-            resolveValuesSourceConfigFromAncestors(name, this.parent, config.valueSourceType());
-        }
-
+        this.config = config;
     }
 
     @Override
@@ -198,101 +66,9 @@ public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource, AF
 
     @Override
     public void doValidate() {
-    }
-
-    public ValuesSourceConfig<VS> config(AggregationContext context) {
-
-        ValueType valueType = this.valueType != null ? this.valueType : targetValueType;
-
-        if (field == null) {
-            if (script == null) {
-                ValuesSourceConfig<VS> config = new ValuesSourceConfig(ValuesSourceType.ANY);
-                config.format = resolveFormat(null, valueType);
-                return config;
-            }
-            ValuesSourceType valuesSourceType = valueType != null ? valueType.getValuesSourceType() : this.valuesSourceType;
-            if (valuesSourceType == null || valuesSourceType == ValuesSourceType.ANY) {
-                // the specific value source type is undefined, but for scripts,
-                // we need to have a specific value source
-                // type to know how to handle the script values, so we fallback
-                // on Bytes
-                valuesSourceType = ValuesSourceType.BYTES;
-            }
-            ValuesSourceConfig<VS> config = new ValuesSourceConfig<VS>(valuesSourceType);
-            config.missing = missing;
-            config.format = resolveFormat(format, valueType);
-            config.script = createScript(script, context.searchContext());
-            config.scriptValueType = valueType;
-            return config;
-        }
-
-        MappedFieldType fieldType = context.searchContext().smartNameFieldType(field);
-        if (fieldType == null) {
-            ValuesSourceType valuesSourceType = valueType != null ? valueType.getValuesSourceType() : this.valuesSourceType;
-            ValuesSourceConfig<VS> config = new ValuesSourceConfig<>(valuesSourceType);
-            config.missing = missing;
-            config.format = resolveFormat(format, valueType);
-            config.unmapped = true;
-            if (valueType != null) {
-                // todo do we really need this for unmapped?
-                config.scriptValueType = valueType;
-            }
-            return config;
-        }
-
-        IndexFieldData<?> indexFieldData = context.searchContext().fieldData().getForField(fieldType);
-
-        ValuesSourceConfig config;
-        if (valuesSourceType == ValuesSourceType.ANY) {
-            if (indexFieldData instanceof IndexNumericFieldData) {
-                config = new ValuesSourceConfig<>(ValuesSourceType.NUMERIC);
-            } else if (indexFieldData instanceof IndexGeoPointFieldData) {
-                config = new ValuesSourceConfig<>(ValuesSourceType.GEOPOINT);
-            } else {
-                config = new ValuesSourceConfig<>(ValuesSourceType.BYTES);
-            }
-        } else {
-            config = new ValuesSourceConfig(valuesSourceType);
-        }
-
-        config.fieldContext = new FieldContext(field, indexFieldData, fieldType);
-        config.missing = missing;
-        config.script = createScript(script, context.searchContext());
-        config.format = resolveFormat(format, this.timeZone, fieldType);
-        return config;
-    }
-
-    private SearchScript createScript(Script script, SearchContext context) {
-        return script == null ? null
-                : context.scriptService().search(context.lookup(), script, ScriptContext.Standard.AGGS, Collections.emptyMap());
-    }
-
-    private static ValueFormat resolveFormat(@Nullable String format, @Nullable ValueType valueType) {
-        if (valueType == null) {
-            return ValueFormat.RAW; // we can't figure it out
-        }
-        ValueFormat valueFormat = valueType.defaultFormat;
-        if (valueFormat != null && valueFormat instanceof ValueFormat.Patternable && format != null) {
-            return ((ValueFormat.Patternable) valueFormat).create(format);
-        }
-        return valueFormat;
-    }
-
-    private static ValueFormat resolveFormat(@Nullable String format, @Nullable DateTimeZone timezone, MappedFieldType fieldType) {
-        if (fieldType instanceof DateFieldMapper.DateFieldType) {
-            return format != null ? ValueFormat.DateTime.format(format, timezone) : ValueFormat.DateTime.mapper(
-                    (DateFieldMapper.DateFieldType) fieldType, timezone);
-        }
-        if (fieldType instanceof IpFieldMapper.IpFieldType) {
-            return ValueFormat.IPv4;
-        }
-        if (fieldType instanceof BooleanFieldMapper.BooleanFieldType) {
-            return ValueFormat.BOOLEAN;
-        }
-        if (fieldType instanceof NumberFieldMapper.NumberFieldType) {
-            return format != null ? ValueFormat.Number.format(format) : ValueFormat.RAW;
+        if (config == null || !config.valid()) {
+            resolveValuesSourceConfigFromAncestors(name, parent, config.valueSourceType());
         }
-        return ValueFormat.RAW;
     }
 
     protected abstract Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
@@ -302,18 +78,16 @@ public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource, AF
             boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
             throws IOException;
 
-    private void resolveValuesSourceConfigFromAncestors(String aggName, AggregatorFactory parent, ValuesSourceType requiredValuesSourceType) {
+    private void resolveValuesSourceConfigFromAncestors(String aggName, AggregatorFactory parent, Class<VS> requiredValuesSourceType) {
         ValuesSourceConfig config;
         while (parent != null) {
             if (parent instanceof ValuesSourceAggregatorFactory) {
                 config = ((ValuesSourceAggregatorFactory) parent).config;
                 if (config != null && config.valid()) {
-                    if (requiredValuesSourceType == null || requiredValuesSourceType == ValuesSourceType.ANY
-                            || requiredValuesSourceType == config.valueSourceType) {
+                    if (requiredValuesSourceType == null || requiredValuesSourceType.isAssignableFrom(config.valueSourceType)) {
                         ValueFormat format = config.format;
                         this.config = config;
-                        // if the user explicitly defined a format pattern,
-                        // we'll do our best to keep it even when we inherit the
+                        // if the user explicitly defined a format pattern, we'll do our best to keep it even when we inherit the
                         // value source form one of the ancestor aggregations
                         if (this.config.formatPattern != null && format != null && format instanceof ValueFormat.Patternable) {
                             this.config.format = ((ValueFormat.Patternable) format).create(this.config.formatPattern);
@@ -326,121 +100,4 @@ public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource, AF
         }
         throw new AggregationExecutionException("could not find the appropriate value context to perform aggregation [" + aggName + "]");
     }
-
-    @Override
-    protected final void doWriteTo(StreamOutput out) throws IOException {
-        valuesSourceType.writeTo(out);
-        boolean hasTargetValueType = targetValueType != null;
-        out.writeBoolean(hasTargetValueType);
-        if (hasTargetValueType) {
-            targetValueType.writeTo(out);
-        }
-        innerWriteTo(out);
-        out.writeOptionalString(field);
-        boolean hasScript = script != null;
-        out.writeBoolean(hasScript);
-        if (hasScript) {
-            script.writeTo(out);
-        }
-        boolean hasValueType = valueType != null;
-        out.writeBoolean(hasValueType);
-        if (hasValueType) {
-            valueType.writeTo(out);
-        }
-        out.writeOptionalString(format);
-        out.writeGenericValue(missing);
-        boolean hasTimeZone = timeZone != null;
-        out.writeBoolean(hasTimeZone);
-        if (hasTimeZone) {
-            out.writeString(timeZone.getID());
-        }
-    }
-
-    protected abstract void innerWriteTo(StreamOutput out) throws IOException;
-
-    @Override
-    protected final ValuesSourceAggregatorFactory<VS, AF> doReadFrom(String name, StreamInput in) throws IOException {
-        ValuesSourceType valuesSourceType = ValuesSourceType.ANY.readFrom(in);
-        ValueType targetValueType = null;
-        if (in.readBoolean()) {
-            targetValueType = ValueType.STRING.readFrom(in);
-        }
-        ValuesSourceAggregatorFactory<VS, AF> factory = innerReadFrom(name, valuesSourceType, targetValueType, in);
-        factory.field = in.readOptionalString();
-        if (in.readBoolean()) {
-            factory.script = Script.readScript(in);
-        }
-        if (in.readBoolean()) {
-            factory.valueType = ValueType.STRING.readFrom(in);
-        }
-        factory.format = in.readOptionalString();
-        factory.missing = in.readGenericValue();
-        if (in.readBoolean()) {
-            factory.timeZone = DateTimeZone.forID(in.readString());
-        }
-        return factory;
-    }
-
-    protected abstract ValuesSourceAggregatorFactory<VS, AF> innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException;
-
-    @Override
-    public final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        if (field != null) {
-            builder.field("field", field);
-        }
-        if (script != null) {
-            builder.field("script", script);
-        }
-        if (missing != null) {
-            builder.field("missing", missing);
-        }
-        if (format != null) {
-            builder.field("format", format);
-        }
-        if (timeZone != null) {
-            builder.field("time_zone", timeZone);
-        }
-        if (valueType != null) {
-            builder.field("value_type", valueType.getPreferredName());
-        }
-        doXContentBody(builder, params);
-        builder.endObject();
-        return builder;
-    }
-
-    protected abstract XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException;
-
-    @Override
-    protected final int doHashCode() {
-        return Objects.hash(field, format, missing, script, targetValueType, timeZone, valueType, valuesSourceType,
-                innerHashCode());
-    }
-
-    protected abstract int innerHashCode();
-
-    @Override
-    protected final boolean doEquals(Object obj) {
-        ValuesSourceAggregatorFactory<?, ?> other = (ValuesSourceAggregatorFactory<?, ?>) obj;
-        if (!Objects.equals(field, other.field))
-            return false;
-        if (!Objects.equals(format, other.format))
-            return false;
-        if (!Objects.equals(missing, other.missing))
-            return false;
-        if (!Objects.equals(script, other.script))
-            return false;
-        if (!Objects.equals(targetValueType, other.targetValueType))
-            return false;
-        if (!Objects.equals(timeZone, other.timeZone))
-            return false;
-        if (!Objects.equals(valueType, other.valueType))
-            return false;
-        if (!Objects.equals(valuesSourceType, other.valuesSourceType))
-            return false;
-        return innerEquals(obj);
-    }
-
-    protected abstract boolean innerEquals(Object obj);
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java
index 35e7293..a831f2f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java
@@ -28,7 +28,7 @@ import org.elasticsearch.search.aggregations.support.format.ValueParser;
  */
 public class ValuesSourceConfig<VS extends ValuesSource> {
 
-    final ValuesSourceType valueSourceType;
+    final Class<VS> valueSourceType;
     FieldContext fieldContext;
     SearchScript script;
     ValueType scriptValueType;
@@ -37,11 +37,11 @@ public class ValuesSourceConfig<VS extends ValuesSource> {
     ValueFormat format = ValueFormat.RAW;
     Object missing;
 
-    public ValuesSourceConfig(ValuesSourceType valueSourceType) {
+    public ValuesSourceConfig(Class<VS> valueSourceType) {
         this.valueSourceType = valueSourceType;
     }
 
-    public ValuesSourceType valueSourceType() {
+    public Class<VS> valueSourceType() {
         return valueSourceType;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java
new file mode 100644
index 0000000..fced5fd
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java
@@ -0,0 +1,297 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.support;
+
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.fielddata.IndexFieldData;
+import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
+import org.elasticsearch.index.fielddata.IndexNumericFieldData;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.core.BooleanFieldMapper;
+import org.elasticsearch.index.mapper.core.DateFieldMapper;
+import org.elasticsearch.index.mapper.core.NumberFieldMapper;
+import org.elasticsearch.index.mapper.ip.IpFieldMapper;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.script.ScriptContext;
+import org.elasticsearch.script.ScriptParameterParser;
+import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
+import org.elasticsearch.script.SearchScript;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.internal.SearchContext;
+import org.joda.time.DateTimeZone;
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ *
+ */
+public class ValuesSourceParser<VS extends ValuesSource> {
+
+    static final ParseField TIME_ZONE = new ParseField("time_zone");
+
+    public static Builder any(String aggName, InternalAggregation.Type aggType, SearchContext context) {
+        return new Builder<>(aggName, aggType, context, ValuesSource.class);
+    }
+
+    public static Builder<ValuesSource.Numeric> numeric(String aggName, InternalAggregation.Type aggType, SearchContext context) {
+        return new Builder<>(aggName, aggType, context, ValuesSource.Numeric.class).targetValueType(ValueType.NUMERIC);
+    }
+
+    public static Builder<ValuesSource.Bytes> bytes(String aggName, InternalAggregation.Type aggType, SearchContext context) {
+        return new Builder<>(aggName, aggType, context, ValuesSource.Bytes.class).targetValueType(ValueType.STRING);
+    }
+
+    public static Builder<ValuesSource.GeoPoint> geoPoint(String aggName, InternalAggregation.Type aggType, SearchContext context) {
+        return new Builder<>(aggName, aggType, context, ValuesSource.GeoPoint.class).targetValueType(ValueType.GEOPOINT).scriptable(false);
+    }
+
+    public static class Input {
+        private String field = null;
+        private Script script = null;
+        @Deprecated
+        private Map<String, Object> params = null; // TODO Remove in 3.0
+        private ValueType valueType = null;
+        private String format = null;
+        private Object missing = null;
+        private DateTimeZone timezone = DateTimeZone.UTC;
+
+        public DateTimeZone timezone() {
+            return this.timezone;
+        }
+    }
+
+    private final String aggName;
+    private final InternalAggregation.Type aggType;
+    private final SearchContext context;
+    private final Class<VS> valuesSourceType;
+
+    private boolean scriptable = true;
+    private boolean formattable = false;
+    private boolean timezoneAware = false;
+    private ValueType targetValueType = null;
+    private ScriptParameterParser scriptParameterParser = new ScriptParameterParser();
+
+    private Input input = new Input();
+
+    private ValuesSourceParser(String aggName, InternalAggregation.Type aggType, SearchContext context, Class<VS> valuesSourceType) {
+        this.aggName = aggName;
+        this.aggType = aggType;
+        this.context = context;
+        this.valuesSourceType = valuesSourceType;
+    }
+
+    public boolean token(String currentFieldName, XContentParser.Token token, XContentParser parser) throws IOException {
+        if ("missing".equals(currentFieldName) && token.isValue()) {
+            input.missing = parser.objectText();
+            return true;
+        }
+        if (token == XContentParser.Token.VALUE_STRING) {
+            if ("field".equals(currentFieldName)) {
+                input.field = parser.text();
+            } else if (formattable && "format".equals(currentFieldName)) {
+                input.format = parser.text();
+            } else if (timezoneAware && context.parseFieldMatcher().match(currentFieldName, TIME_ZONE)) {
+                input.timezone = DateTimeZone.forID(parser.text());
+            } else if (scriptable) {
+                if ("value_type".equals(currentFieldName) || "valueType".equals(currentFieldName)) {
+                    input.valueType = ValueType.resolveForScript(parser.text());
+                    if (targetValueType != null && input.valueType.isNotA(targetValueType)) {
+                        throw new SearchParseException(context, aggType.name() + " aggregation [" + aggName +
+                                "] was configured with an incompatible value type [" + input.valueType + "]. [" + aggType +
+                                "] aggregation can only work on value of type [" + targetValueType + "]",
+                                parser.getTokenLocation());
+                    }
+                } else if (!scriptParameterParser.token(currentFieldName, token, parser, context.parseFieldMatcher())) {
+                    return false;
+                }
+                return true;
+            } else {
+                return false;
+            }
+            return true;
+        }
+        if (token == XContentParser.Token.VALUE_NUMBER) {
+            if (timezoneAware && context.parseFieldMatcher().match(currentFieldName, TIME_ZONE)) {
+                input.timezone = DateTimeZone.forOffsetHours(parser.intValue());
+            } else {
+                return false;
+            }
+            return true;
+        }
+        if (scriptable && token == XContentParser.Token.START_OBJECT) {
+            if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
+                input.script = Script.parse(parser, context.parseFieldMatcher());
+                return true;
+            } else if ("params".equals(currentFieldName)) {
+                input.params = parser.map();
+                return true;
+            }
+            return false;
+        }
+
+        return false;
+    }
+
+    public ValuesSourceConfig<VS> config() {
+
+        if (input.script == null) { // Didn't find anything using the new API so try using the old one instead
+            ScriptParameterValue scriptValue = scriptParameterParser.getDefaultScriptParameterValue();
+            if (scriptValue != null) {
+                if (input.params == null) {
+                    input.params = new HashMap<>();
+                }
+                input.script = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), input.params);
+            }
+        }
+
+        ValueType valueType = input.valueType != null ? input.valueType : targetValueType;
+
+        if (input.field == null) {
+            if (input.script == null) {
+                ValuesSourceConfig<VS> config = new ValuesSourceConfig(ValuesSource.class);
+                config.format = resolveFormat(null, valueType);
+                return config;
+            }
+            Class valuesSourceType = valueType != null ? (Class<VS>) valueType.getValuesSourceType() : this.valuesSourceType;
+            if (valuesSourceType == null || valuesSourceType == ValuesSource.class) {
+                // the specific value source type is undefined, but for scripts, we need to have a specific value source
+                // type to know how to handle the script values, so we fallback on Bytes
+                valuesSourceType = ValuesSource.Bytes.class;
+            }
+            ValuesSourceConfig<VS> config = new ValuesSourceConfig<VS>(valuesSourceType);
+            config.missing = input.missing;
+            config.format = resolveFormat(input.format, valueType);
+            config.script = createScript();
+            config.scriptValueType = valueType;
+            return config;
+        }
+
+        MappedFieldType fieldType = context.smartNameFieldType(input.field);
+        if (fieldType == null) {
+            Class<VS> valuesSourceType = valueType != null ? (Class<VS>) valueType.getValuesSourceType() : this.valuesSourceType;
+            ValuesSourceConfig<VS> config = new ValuesSourceConfig<>(valuesSourceType);
+            config.missing = input.missing;
+            config.format = resolveFormat(input.format, valueType);
+            config.unmapped = true;
+            if (valueType != null) {
+                // todo do we really need this for unmapped?
+                config.scriptValueType = valueType;
+            }
+            return config;
+        }
+
+        IndexFieldData<?> indexFieldData = context.fieldData().getForField(fieldType);
+
+        ValuesSourceConfig config;
+        if (valuesSourceType == ValuesSource.class) {
+            if (indexFieldData instanceof IndexNumericFieldData) {
+                config = new ValuesSourceConfig<>(ValuesSource.Numeric.class);
+            } else if (indexFieldData instanceof IndexGeoPointFieldData) {
+                config = new ValuesSourceConfig<>(ValuesSource.GeoPoint.class);
+            } else {
+                config = new ValuesSourceConfig<>(ValuesSource.Bytes.class);
+            }
+        } else {
+            config = new ValuesSourceConfig(valuesSourceType);
+        }
+
+        config.fieldContext = new FieldContext(input.field, indexFieldData, fieldType);
+        config.missing = input.missing;
+        config.script = createScript();
+        config.format = resolveFormat(input.format, input.timezone, fieldType);
+        return config;
+    }
+
+    private SearchScript createScript() {
+        return input.script == null ? null : context.scriptService().search(context.lookup(), input.script, ScriptContext.Standard.AGGS, Collections.emptyMap());
+    }
+
+    private static ValueFormat resolveFormat(@Nullable String format, @Nullable ValueType valueType) {
+        if (valueType == null) {
+            return ValueFormat.RAW; // we can't figure it out
+        }
+        ValueFormat valueFormat = valueType.defaultFormat;
+        if (valueFormat != null && valueFormat instanceof ValueFormat.Patternable && format != null) {
+            return ((ValueFormat.Patternable) valueFormat).create(format);
+        }
+        return valueFormat;
+    }
+
+    private static ValueFormat resolveFormat(@Nullable String format, @Nullable DateTimeZone timezone,  MappedFieldType fieldType) {
+        if (fieldType instanceof  DateFieldMapper.DateFieldType) {
+            return format != null ? ValueFormat.DateTime.format(format, timezone) : ValueFormat.DateTime.mapper((DateFieldMapper.DateFieldType) fieldType, timezone);
+        }
+        if (fieldType instanceof IpFieldMapper.IpFieldType) {
+            return ValueFormat.IPv4;
+        }
+        if (fieldType instanceof BooleanFieldMapper.BooleanFieldType) {
+            return ValueFormat.BOOLEAN;
+        }
+        if (fieldType instanceof NumberFieldMapper.NumberFieldType) {
+            return format != null ? ValueFormat.Number.format(format) : ValueFormat.RAW;
+        }
+        return ValueFormat.RAW;
+    }
+
+    public Input input() {
+        return this.input;
+    }
+
+    public static class Builder<VS extends ValuesSource> {
+
+        private final ValuesSourceParser<VS> parser;
+
+        private Builder(String aggName, InternalAggregation.Type aggType, SearchContext context, Class<VS> valuesSourceType) {
+            parser = new ValuesSourceParser<>(aggName, aggType, context, valuesSourceType);
+        }
+
+        public Builder<VS> scriptable(boolean scriptable) {
+            parser.scriptable = scriptable;
+            return this;
+        }
+
+        public Builder<VS> formattable(boolean formattable) {
+            parser.formattable = formattable;
+            return this;
+        }
+
+        public Builder<VS> timezoneAware(boolean timezoneAware) {
+            parser.timezoneAware = timezoneAware;
+            return this;
+        }
+
+        public Builder<VS> targetValueType(ValueType valueType) {
+            parser.targetValueType = valueType;
+            return this;
+        }
+
+        public ValuesSourceParser<VS> build() {
+            return parser;
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceType.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceType.java
deleted file mode 100644
index 46b5698..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceType.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.support;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
-
-/*
- * The ordinal values for this class are tested in ValuesSourceTypeTests to
- * ensure that the ordinal for each value does not change and break bwc
- */
-public enum ValuesSourceType implements Writeable<ValuesSourceType> {
-
-    ANY,
-    NUMERIC,
-    BYTES,
-    GEOPOINT;
-
-    @Override
-    public ValuesSourceType readFrom(StreamInput in) throws IOException {
-        int ordinal = in.readVInt();
-        if (ordinal < 0 || ordinal >= values().length) {
-            throw new IOException("Unknown ValuesSourceType ordinal [" + ordinal + "]");
-        }
-        return values()[ordinal];
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(ordinal());
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
index 59ffb0a..a5e1334 100644
--- a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
@@ -41,13 +41,13 @@ import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.rescore.RescoreBuilder;
+import org.elasticsearch.search.rescore.RescoreBuilder;
 import org.elasticsearch.search.sort.SortBuilder;
 import org.elasticsearch.search.sort.SortBuilders;
 import org.elasticsearch.search.sort.SortOrder;
@@ -153,7 +153,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
 
     private BytesReference innerHitsBuilder;
 
-    private List<BytesReference> rescoreBuilders;
+    private List<RescoreBuilder<?>> rescoreBuilders;
 
     private ObjectFloatHashMap<String> indexBoost = null;
 
@@ -384,26 +384,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     /**
      * Add an aggregation to perform as part of the search.
      */
-    public SearchSourceBuilder aggregation(AggregatorFactory<?> aggregation) {
-        try {
-            if (aggregations == null) {
-                aggregations = new ArrayList<>();
-            }
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            aggregation.toXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            aggregations.add(builder.bytes());
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
-        }
-    }
-
-    /**
-     * Add an aggregation to perform as part of the search.
-     */
-    public SearchSourceBuilder aggregation(PipelineAggregatorFactory aggregation) {
+    public SearchSourceBuilder aggregation(AbstractAggregationBuilder aggregation) {
         try {
             if (aggregations == null) {
                 aggregations = new ArrayList<>();
@@ -479,20 +460,12 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         return suggestBuilder;
     }
 
-    public SearchSourceBuilder addRescorer(RescoreBuilder rescoreBuilder) {
-        try {
+    public SearchSourceBuilder addRescorer(RescoreBuilder<?> rescoreBuilder) {
             if (rescoreBuilders == null) {
                 rescoreBuilders = new ArrayList<>();
             }
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            rescoreBuilder.toXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            rescoreBuilders.add(builder.bytes());
+            rescoreBuilders.add(rescoreBuilder);
             return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
-        }
     }
 
     public SearchSourceBuilder clearRescorers() {
@@ -519,7 +492,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     /**
      * Gets the bytes representing the rescore builders for this request.
      */
-    public List<BytesReference> rescores() {
+    public List<RescoreBuilder<?>> rescores() {
         return rescoreBuilders;
     }
 
@@ -899,10 +872,9 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
                     }
                     builder.sorts = sorts;
                 } else if (context.parseFieldMatcher().match(currentFieldName, RESCORE_FIELD)) {
-                    List<BytesReference> rescoreBuilders = new ArrayList<>();
+                    List<RescoreBuilder<?>> rescoreBuilders = new ArrayList<>();
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        XContentBuilder xContentBuilder = XContentFactory.jsonBuilder().copyCurrentStructure(parser);
-                        rescoreBuilders.add(xContentBuilder.bytes());
+                        rescoreBuilders.add(RescoreBuilder.parseFromXContent(context));
                     }
                     builder.rescoreBuilders = rescoreBuilders;
                 } else if (context.parseFieldMatcher().match(currentFieldName, STATS_FIELD)) {
@@ -1069,10 +1041,8 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
 
         if (rescoreBuilders != null) {
             builder.startArray(RESCORE_FIELD.getPreferredName());
-            for (BytesReference rescoreBuilder : rescoreBuilders) {
-                XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(rescoreBuilder);
-                parser.nextToken();
-                builder.copyCurrentStructure(parser);
+            for (RescoreBuilder<?> rescoreBuilder : rescoreBuilders) {
+                rescoreBuilder.toXContent(builder, params);
             }
             builder.endArray();
         }
@@ -1101,7 +1071,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
             this(fieldName, script, false);
         }
 
-        public ScriptField(String fieldName, Script script, boolean ignoreFailure) {
+        private ScriptField(String fieldName, Script script, boolean ignoreFailure) {
             this.fieldName = fieldName;
             this.script = script;
             this.ignoreFailure = ignoreFailure;
@@ -1218,9 +1188,9 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         }
         if (in.readBoolean()) {
             int size = in.readVInt();
-            List<BytesReference> rescoreBuilders = new ArrayList<>();
+            List<RescoreBuilder<?>> rescoreBuilders = new ArrayList<>();
             for (int i = 0; i < size; i++) {
-                rescoreBuilders.add(in.readBytesReference());
+                rescoreBuilders.add(in.readRescorer());
             }
             builder.rescoreBuilders = rescoreBuilders;
         }
@@ -1334,8 +1304,8 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         out.writeBoolean(hasRescoreBuilders);
         if (hasRescoreBuilders) {
             out.writeVInt(rescoreBuilders.size());
-            for (BytesReference rescoreBuilder : rescoreBuilders) {
-                out.writeBytesReference(rescoreBuilder);
+            for (RescoreBuilder<?> rescoreBuilder : rescoreBuilders) {
+                out.writeRescorer(rescoreBuilder);
             }
         }
         boolean hasScriptFields = scriptFields != null;
diff --git a/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
index 6159f67..e5113bb 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
@@ -57,7 +57,6 @@ import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -149,16 +148,16 @@ public class DefaultSearchContext extends SearchContext {
 
     private final Map<String, FetchSubPhaseContext> subPhaseContexts = new HashMap<>();
     private final Map<Class<?>, Collector> queryCollectors = new HashMap<>();
-    private FetchPhase fetchPhase;
 
-    public DefaultSearchContext(long id, ShardSearchRequest request, SearchShardTarget shardTarget, Engine.Searcher engineSearcher,
-            IndexService indexService, IndexShard indexShard, ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
-            BigArrays bigArrays, Counter timeEstimateCounter, ParseFieldMatcher parseFieldMatcher, TimeValue timeout,
-            FetchPhase fetchPhase) {
+    public DefaultSearchContext(long id, ShardSearchRequest request, SearchShardTarget shardTarget,
+                                Engine.Searcher engineSearcher, IndexService indexService, IndexShard indexShard,
+                                ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
+                                BigArrays bigArrays, Counter timeEstimateCounter, ParseFieldMatcher parseFieldMatcher,
+                                TimeValue timeout
+    ) {
         super(parseFieldMatcher, request);
         this.id = id;
         this.request = request;
-        this.fetchPhase = fetchPhase;
         this.searchType = request.searchType();
         this.shardTarget = shardTarget;
         this.engineSearcher = engineSearcher;
@@ -729,11 +728,6 @@ public class DefaultSearchContext extends SearchContext {
     }
 
     @Override
-    public FetchPhase fetchPhase() {
-        return fetchPhase;
-    }
-
-    @Override
     public FetchSearchResult fetchResult() {
         return fetchResult;
     }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
index eb87a37..eaa1493 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
@@ -40,7 +40,6 @@ import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -493,11 +492,6 @@ public abstract class FilteredSearchContext extends SearchContext {
     }
 
     @Override
-    public FetchPhase fetchPhase() {
-        return in.fetchPhase();
-    }
-
-    @Override
     public MappedFieldType smartNameFieldType(String name) {
         return in.smartNameFieldType(name);
     }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
index af9d108..76164b5 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
@@ -47,7 +47,6 @@ import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -305,8 +304,6 @@ public abstract class SearchContext extends DelegatingHasContextAndHeaders imple
 
     public abstract QuerySearchResult queryResult();
 
-    public abstract FetchPhase fetchPhase();
-
     public abstract FetchSearchResult fetchResult();
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/search/rescore/QueryRescorer.java b/core/src/main/java/org/elasticsearch/search/rescore/QueryRescorer.java
index 7f95ff1..3190556 100644
--- a/core/src/main/java/org/elasticsearch/search/rescore/QueryRescorer.java
+++ b/core/src/main/java/org/elasticsearch/search/rescore/QueryRescorer.java
@@ -27,7 +27,7 @@ import org.apache.lucene.search.TopDocs;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.xcontent.ObjectParser;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.ParsedQuery;
+import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.search.internal.ContextIndexSearcher;
 import org.elasticsearch.search.internal.SearchContext;
 
@@ -120,17 +120,17 @@ public final class QueryRescorer implements Rescorer {
         }
     }
 
-    private static final ObjectParser<QueryRescoreContext, SearchContext> RESCORE_PARSER = new ObjectParser<>("query", null);
+    private static final ObjectParser<QueryRescoreContext, QueryShardContext> RESCORE_PARSER = new ObjectParser<>("query", null);
 
     static {
-        RESCORE_PARSER.declareObject(QueryRescoreContext::setParsedQuery, (p, c) -> c.indexShard().getQueryShardContext().parse(p), new ParseField("rescore_query"));
+        RESCORE_PARSER.declareObject(QueryRescoreContext::setQuery, (p, c) -> c.parse(p).query(), new ParseField("rescore_query"));
         RESCORE_PARSER.declareFloat(QueryRescoreContext::setQueryWeight, new ParseField("query_weight"));
         RESCORE_PARSER.declareFloat(QueryRescoreContext::setRescoreQueryWeight, new ParseField("rescore_query_weight"));
         RESCORE_PARSER.declareString(QueryRescoreContext::setScoreMode, new ParseField("score_mode"));
     }
 
     @Override
-    public RescoreSearchContext parse(XContentParser parser, SearchContext context) throws IOException {
+    public RescoreSearchContext parse(XContentParser parser, QueryShardContext context) throws IOException {
         return RESCORE_PARSER.parse(parser, new QueryRescoreContext(this), context);
     }
 
@@ -178,22 +178,24 @@ public final class QueryRescorer implements Rescorer {
 
     public static class QueryRescoreContext extends RescoreSearchContext {
 
+        static final int DEFAULT_WINDOW_SIZE = 10;
+
         public QueryRescoreContext(QueryRescorer rescorer) {
-            super(NAME, 10, rescorer);
+            super(NAME, DEFAULT_WINDOW_SIZE, rescorer);
             this.scoreMode = QueryRescoreMode.Total;
         }
 
-        private ParsedQuery parsedQuery;
+        private Query query;
         private float queryWeight = 1.0f;
         private float rescoreQueryWeight = 1.0f;
         private QueryRescoreMode scoreMode;
 
-        public void setParsedQuery(ParsedQuery parsedQuery) {
-            this.parsedQuery = parsedQuery;
+        public void setQuery(Query query) {
+            this.query = query;
         }
 
         public Query query() {
-            return parsedQuery.query();
+            return query;
         }
 
         public float queryWeight() {
diff --git a/core/src/main/java/org/elasticsearch/search/rescore/QueryRescorerBuilder.java b/core/src/main/java/org/elasticsearch/search/rescore/QueryRescorerBuilder.java
new file mode 100644
index 0000000..10c727a
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/rescore/QueryRescorerBuilder.java
@@ -0,0 +1,242 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.rescore;
+
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.ObjectParser;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.query.MatchAllQueryBuilder;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.search.rescore.QueryRescorer.QueryRescoreContext;
+
+import java.io.IOException;
+import java.util.Locale;
+import java.util.Objects;
+
+public class QueryRescorerBuilder extends RescoreBuilder<QueryRescorerBuilder> {
+
+    public static final String NAME = "query";
+
+    public static final QueryRescorerBuilder PROTOTYPE = new QueryRescorerBuilder(new MatchAllQueryBuilder());
+
+    public static final float DEFAULT_RESCORE_QUERYWEIGHT = 1.0f;
+    public static final float DEFAULT_QUERYWEIGHT = 1.0f;
+    public static final QueryRescoreMode DEFAULT_SCORE_MODE = QueryRescoreMode.Total;
+    private final QueryBuilder<?> queryBuilder;
+    private float rescoreQueryWeight = DEFAULT_RESCORE_QUERYWEIGHT;
+    private float queryWeight = DEFAULT_QUERYWEIGHT;
+    private QueryRescoreMode scoreMode = DEFAULT_SCORE_MODE;
+
+    private static ParseField RESCORE_QUERY_FIELD = new ParseField("rescore_query");
+    private static ParseField QUERY_WEIGHT_FIELD = new ParseField("query_weight");
+    private static ParseField RESCORE_QUERY_WEIGHT_FIELD = new ParseField("rescore_query_weight");
+    private static ParseField SCORE_MODE_FIELD = new ParseField("score_mode");
+
+    private static final ObjectParser<InnerBuilder, QueryParseContext> QUERY_RESCORE_PARSER = new ObjectParser<>(NAME, null);
+
+    static {
+        QUERY_RESCORE_PARSER.declareObject(InnerBuilder::setQueryBuilder, (p, c) -> {
+            try {
+                return c.parseInnerQueryBuilder();
+            } catch (IOException e) {
+                throw new ParsingException(p.getTokenLocation(), "Could not parse inner query", e);
+            }
+        } , RESCORE_QUERY_FIELD);
+        QUERY_RESCORE_PARSER.declareFloat(InnerBuilder::setQueryWeight, QUERY_WEIGHT_FIELD);
+        QUERY_RESCORE_PARSER.declareFloat(InnerBuilder::setRescoreQueryWeight, RESCORE_QUERY_WEIGHT_FIELD);
+        QUERY_RESCORE_PARSER.declareString((struct, value) ->  struct.setScoreMode(QueryRescoreMode.fromString(value)), SCORE_MODE_FIELD);
+    }
+
+    /**
+     * Creates a new {@link QueryRescorerBuilder} instance
+     * @param builder the query builder to build the rescore query from
+     */
+    public QueryRescorerBuilder(QueryBuilder<?> builder) {
+        this.queryBuilder = builder;
+    }
+
+    /**
+     * @return the query used for this rescore query
+     */
+    public QueryBuilder<?> getRescoreQuery() {
+        return this.queryBuilder;
+    }
+
+    /**
+     * Sets the original query weight for rescoring. The default is <tt>1.0</tt>
+     */
+    public QueryRescorerBuilder setQueryWeight(float queryWeight) {
+        this.queryWeight = queryWeight;
+        return this;
+    }
+
+
+    /**
+     * Gets the original query weight for rescoring. The default is <tt>1.0</tt>
+     */
+    public float getQueryWeight() {
+        return this.queryWeight;
+    }
+
+    /**
+     * Sets the original query weight for rescoring. The default is <tt>1.0</tt>
+     */
+    public QueryRescorerBuilder setRescoreQueryWeight(float rescoreQueryWeight) {
+        this.rescoreQueryWeight = rescoreQueryWeight;
+        return this;
+    }
+
+    /**
+     * Gets the original query weight for rescoring. The default is <tt>1.0</tt>
+     */
+    public float getRescoreQueryWeight() {
+        return this.rescoreQueryWeight;
+    }
+
+    /**
+     * Sets the original query score mode. The default is {@link QueryRescoreMode#Total}.
+     */
+    public QueryRescorerBuilder setScoreMode(QueryRescoreMode scoreMode) {
+        this.scoreMode = scoreMode;
+        return this;
+    }
+
+    /**
+     * Gets the original query score mode. The default is <tt>total</tt>
+     */
+    public QueryRescoreMode getScoreMode() {
+        return this.scoreMode;
+    }
+
+    @Override
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(NAME);
+        builder.field(RESCORE_QUERY_FIELD.getPreferredName(), queryBuilder);
+        builder.field(QUERY_WEIGHT_FIELD.getPreferredName(), queryWeight);
+        builder.field(RESCORE_QUERY_WEIGHT_FIELD.getPreferredName(), rescoreQueryWeight);
+        builder.field(SCORE_MODE_FIELD.getPreferredName(), scoreMode.name().toLowerCase(Locale.ROOT));
+        builder.endObject();
+    }
+
+    public QueryRescorerBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+            InnerBuilder innerBuilder = QUERY_RESCORE_PARSER.parse(parseContext.parser(), new InnerBuilder(), parseContext);
+            return innerBuilder.build();
+    }
+
+    @Override
+    public QueryRescoreContext build(QueryShardContext context) throws IOException {
+        org.elasticsearch.search.rescore.QueryRescorer rescorer = new org.elasticsearch.search.rescore.QueryRescorer();
+        QueryRescoreContext queryRescoreContext = new QueryRescoreContext(rescorer);
+        queryRescoreContext.setQuery(this.queryBuilder.toQuery(context));
+        queryRescoreContext.setQueryWeight(this.queryWeight);
+        queryRescoreContext.setRescoreQueryWeight(this.rescoreQueryWeight);
+        queryRescoreContext.setScoreMode(this.scoreMode);
+        if (this.windowSize != null) {
+            queryRescoreContext.setWindowSize(this.windowSize);
+        }
+        return queryRescoreContext;
+    }
+
+    @Override
+    public final int hashCode() {
+        int result = super.hashCode();
+        return 31 * result + Objects.hash(scoreMode, queryWeight, rescoreQueryWeight, queryBuilder);
+    }
+
+    @Override
+    public final boolean equals(Object obj) {
+        if (this == obj) {
+            return true;
+        }
+        if (obj == null || getClass() != obj.getClass()) {
+            return false;
+        }
+        QueryRescorerBuilder other = (QueryRescorerBuilder) obj;
+        return super.equals(obj) &&
+               Objects.equals(scoreMode, other.scoreMode) &&
+               Objects.equals(queryWeight, other.queryWeight) &&
+               Objects.equals(rescoreQueryWeight, other.rescoreQueryWeight) &&
+               Objects.equals(queryBuilder, other.queryBuilder);
+    }
+
+    @Override
+    public QueryRescorerBuilder doReadFrom(StreamInput in) throws IOException {
+        QueryRescorerBuilder rescorer = new QueryRescorerBuilder(in.readQuery());
+        rescorer.setScoreMode(QueryRescoreMode.PROTOTYPE.readFrom(in));
+        rescorer.setRescoreQueryWeight(in.readFloat());
+        rescorer.setQueryWeight(in.readFloat());
+        return rescorer;
+    }
+
+    @Override
+    public void doWriteTo(StreamOutput out) throws IOException {
+        out.writeQuery(queryBuilder);
+        scoreMode.writeTo(out);
+        out.writeFloat(rescoreQueryWeight);
+        out.writeFloat(queryWeight);
+    }
+
+    @Override
+    public String getWriteableName() {
+        return NAME;
+    }
+
+    /**
+     * Helper to be able to use {@link ObjectParser}, since we need the inner query builder
+     * for the constructor of {@link QueryRescorerBuilder}, but {@link ObjectParser} only
+     * allows filling properties of an already constructed value.
+     */
+    private class InnerBuilder {
+
+        private QueryBuilder<?> queryBuilder;
+        private float rescoreQueryWeight = DEFAULT_RESCORE_QUERYWEIGHT;
+        private float queryWeight = DEFAULT_QUERYWEIGHT;
+        private QueryRescoreMode scoreMode = DEFAULT_SCORE_MODE;
+
+        void setQueryBuilder(QueryBuilder<?> builder) {
+            this.queryBuilder = builder;
+        }
+
+        QueryRescorerBuilder build() {
+            QueryRescorerBuilder queryRescoreBuilder = new QueryRescorerBuilder(queryBuilder);
+            queryRescoreBuilder.setQueryWeight(queryWeight);
+            queryRescoreBuilder.setRescoreQueryWeight(rescoreQueryWeight);
+            queryRescoreBuilder.setScoreMode(scoreMode);
+            return queryRescoreBuilder;
+        }
+
+        void setQueryWeight(float queryWeight) {
+            this.queryWeight = queryWeight;
+        }
+
+        void setRescoreQueryWeight(float rescoreQueryWeight) {
+            this.rescoreQueryWeight = rescoreQueryWeight;
+        }
+
+        void setScoreMode(QueryRescoreMode scoreMode) {
+            this.scoreMode = scoreMode;
+        }
+    }
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/search/rescore/RescoreBuilder.java b/core/src/main/java/org/elasticsearch/search/rescore/RescoreBuilder.java
index 7510d24..8dad07a 100644
--- a/core/src/main/java/org/elasticsearch/search/rescore/RescoreBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/rescore/RescoreBuilder.java
@@ -20,255 +20,140 @@
 package org.elasticsearch.search.rescore;
 
 import org.elasticsearch.ExceptionsHelper;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.io.stream.NamedWriteable;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.MatchAllQueryBuilder;
+import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.search.rescore.QueryRescorer.QueryRescoreContext;
 
 import java.io.IOException;
-import java.util.Locale;
 import java.util.Objects;
 
-public class RescoreBuilder implements ToXContent, Writeable<RescoreBuilder> {
-
-    private Rescorer rescorer;
-    private Integer windowSize;
-    public static final RescoreBuilder PROTOYPE = new RescoreBuilder(new QueryRescorer(new MatchAllQueryBuilder()));
+/**
+ * The abstract base builder for instances of {@link RescoreBuilder}.
+ */
+public abstract class RescoreBuilder<RB extends RescoreBuilder<RB>> implements ToXContent, NamedWriteable<RB> {
 
-    public RescoreBuilder(Rescorer rescorer) {
-        if (rescorer == null) {
-            throw new IllegalArgumentException("rescorer cannot be null");
-        }
-        this.rescorer = rescorer;
-    }
+    protected Integer windowSize;
 
-    public Rescorer rescorer() {
-        return this.rescorer;
-    }
+    private static ParseField WINDOW_SIZE_FIELD = new ParseField("window_size");
 
-    public RescoreBuilder windowSize(int windowSize) {
+    @SuppressWarnings("unchecked")
+    public RB windowSize(int windowSize) {
         this.windowSize = windowSize;
-        return this;
+        return (RB) this;
     }
 
     public Integer windowSize() {
         return windowSize;
     }
 
+    public static RescoreBuilder<?> parseFromXContent(QueryParseContext parseContext) throws IOException {
+        XContentParser parser = parseContext.parser();
+        String fieldName = null;
+        RescoreBuilder<?> rescorer = null;
+        Integer windowSize = null;
+        XContentParser.Token token;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                fieldName = parser.currentName();
+            } else if (token.isValue()) {
+                if (parseContext.parseFieldMatcher().match(fieldName, WINDOW_SIZE_FIELD)) {
+                    windowSize = parser.intValue();
+                } else {
+                    throw new ParsingException(parser.getTokenLocation(), "rescore doesn't support [" + fieldName + "]");
+                }
+            } else if (token == XContentParser.Token.START_OBJECT) {
+                // we only have QueryRescorer at this point
+                if (QueryRescorerBuilder.NAME.equals(fieldName)) {
+                    rescorer = QueryRescorerBuilder.PROTOTYPE.fromXContent(parseContext);
+                } else {
+                    throw new ParsingException(parser.getTokenLocation(), "rescore doesn't support rescorer with name [" + fieldName + "]");
+                }
+            } else {
+                throw new ParsingException(parser.getTokenLocation(), "unexpected token [" + token + "] after [" + fieldName + "]");
+            }
+        }
+        if (rescorer == null) {
+            throw new ParsingException(parser.getTokenLocation(), "missing rescore type");
+        }
+        if (windowSize != null) {
+            rescorer.windowSize(windowSize.intValue());
+        }
+        return rescorer;
+    }
+
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
         if (windowSize != null) {
             builder.field("window_size", windowSize);
         }
-        rescorer.toXContent(builder, params);
+        doXContent(builder, params);
+        builder.endObject();
         return builder;
     }
 
-    public static QueryRescorer queryRescorer(QueryBuilder<?> queryBuilder) {
-        return new QueryRescorer(queryBuilder);
+    protected abstract void doXContent(XContentBuilder builder, Params params) throws IOException;
+
+    public abstract QueryRescoreContext build(QueryShardContext context) throws IOException;
+
+    public static QueryRescorerBuilder queryRescorer(QueryBuilder<?> queryBuilder) {
+        return new QueryRescorerBuilder(queryBuilder);
     }
 
     @Override
-    public final int hashCode() {
-        return Objects.hash(windowSize, rescorer);
+    public int hashCode() {
+        return Objects.hash(windowSize);
     }
 
     @Override
-    public final boolean equals(Object obj) {
+    public boolean equals(Object obj) {
         if (this == obj) {
             return true;
         }
         if (obj == null || getClass() != obj.getClass()) {
             return false;
         }
+        @SuppressWarnings("rawtypes")
         RescoreBuilder other = (RescoreBuilder) obj;
-        return Objects.equals(windowSize, other.windowSize) &&
-               Objects.equals(rescorer, other.rescorer);
+        return Objects.equals(windowSize, other.windowSize);
     }
 
     @Override
-    public RescoreBuilder readFrom(StreamInput in) throws IOException {
-        RescoreBuilder builder = new RescoreBuilder(in.readRescorer());
-        Integer windowSize = in.readOptionalVInt();
-        if (windowSize != null) {
-            builder.windowSize(windowSize);
-        }
+    public RB readFrom(StreamInput in) throws IOException {
+        RB builder = doReadFrom(in);
+        builder.windowSize = in.readOptionalVInt();
         return builder;
     }
 
+    protected abstract RB doReadFrom(StreamInput in) throws IOException;
+
     @Override
     public void writeTo(StreamOutput out) throws IOException {
-        out.writeRescorer(rescorer);
+        doWriteTo(out);
         out.writeOptionalVInt(this.windowSize);
     }
 
+    protected abstract void doWriteTo(StreamOutput out) throws IOException;
+
     @Override
     public final String toString() {
         try {
             XContentBuilder builder = XContentFactory.jsonBuilder();
             builder.prettyPrint();
-            builder.startObject();
             toXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
             return builder.string();
         } catch (Exception e) {
             return "{ \"error\" : \"" + ExceptionsHelper.detailedMessage(e) + "\"}";
         }
     }
-
-    public static abstract class Rescorer implements ToXContent, NamedWriteable<Rescorer> {
-
-        private String name;
-
-        public Rescorer(String name) {
-            this.name = name;
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(name);
-            builder = innerToXContent(builder, params);
-            builder.endObject();
-            return builder;
-        }
-
-        protected abstract XContentBuilder innerToXContent(XContentBuilder builder, Params params) throws IOException;
-
-        @Override
-        public abstract int hashCode();
-
-        @Override
-        public abstract boolean equals(Object obj);
-    }
-
-    public static class QueryRescorer extends Rescorer {
-
-        private static final String NAME = "query";
-        public static final QueryRescorer PROTOTYPE = new QueryRescorer(new MatchAllQueryBuilder());
-        public static final float DEFAULT_RESCORE_QUERYWEIGHT = 1.0f;
-        public static final float DEFAULT_QUERYWEIGHT = 1.0f;
-        public static final QueryRescoreMode DEFAULT_SCORE_MODE = QueryRescoreMode.Total;
-        private final QueryBuilder<?> queryBuilder;
-        private float rescoreQueryWeight = DEFAULT_RESCORE_QUERYWEIGHT;
-        private float queryWeight = DEFAULT_QUERYWEIGHT;
-        private QueryRescoreMode scoreMode = DEFAULT_SCORE_MODE;
-
-        /**
-         * Creates a new {@link QueryRescorer} instance
-         * @param builder the query builder to build the rescore query from
-         */
-        public QueryRescorer(QueryBuilder<?> builder) {
-            super(NAME);
-            this.queryBuilder = builder;
-        }
-
-        /**
-         * @return the query used for this rescore query
-         */
-        public QueryBuilder<?> getRescoreQuery() {
-            return this.queryBuilder;
-        }
-
-        /**
-         * Sets the original query weight for rescoring. The default is <tt>1.0</tt>
-         */
-        public QueryRescorer setQueryWeight(float queryWeight) {
-            this.queryWeight = queryWeight;
-            return this;
-        }
-
-
-        /**
-         * Gets the original query weight for rescoring. The default is <tt>1.0</tt>
-         */
-        public float getQueryWeight() {
-            return this.queryWeight;
-        }
-
-        /**
-         * Sets the original query weight for rescoring. The default is <tt>1.0</tt>
-         */
-        public QueryRescorer setRescoreQueryWeight(float rescoreQueryWeight) {
-            this.rescoreQueryWeight = rescoreQueryWeight;
-            return this;
-        }
-
-        /**
-         * Gets the original query weight for rescoring. The default is <tt>1.0</tt>
-         */
-        public float getRescoreQueryWeight() {
-            return this.rescoreQueryWeight;
-        }
-
-        /**
-         * Sets the original query score mode. The default is {@link QueryRescoreMode#Total}.
-         */
-        public QueryRescorer setScoreMode(QueryRescoreMode scoreMode) {
-            this.scoreMode = scoreMode;
-            return this;
-        }
-
-        /**
-         * Gets the original query score mode. The default is <tt>total</tt>
-         */
-        public QueryRescoreMode getScoreMode() {
-            return this.scoreMode;
-        }
-
-        @Override
-        protected XContentBuilder innerToXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field("rescore_query", queryBuilder);
-            builder.field("query_weight", queryWeight);
-            builder.field("rescore_query_weight", rescoreQueryWeight);
-            builder.field("score_mode", scoreMode.name().toLowerCase(Locale.ROOT));
-            return builder;
-        }
-
-        @Override
-        public final int hashCode() {
-            return Objects.hash(getClass(), scoreMode, queryWeight, rescoreQueryWeight, queryBuilder);
-        }
-
-        @Override
-        public final boolean equals(Object obj) {
-            if (this == obj) {
-                return true;
-            }
-            if (obj == null || getClass() != obj.getClass()) {
-                return false;
-            }
-            QueryRescorer other = (QueryRescorer) obj;
-            return Objects.equals(scoreMode, other.scoreMode) &&
-                   Objects.equals(queryWeight, other.queryWeight) &&
-                   Objects.equals(rescoreQueryWeight, other.rescoreQueryWeight) &&
-                   Objects.equals(queryBuilder, other.queryBuilder);
-        }
-
-        @Override
-        public QueryRescorer readFrom(StreamInput in) throws IOException {
-            QueryRescorer rescorer = new QueryRescorer(in.readQuery());
-            rescorer.setScoreMode(QueryRescoreMode.PROTOTYPE.readFrom(in));
-            rescorer.setRescoreQueryWeight(in.readFloat());
-            rescorer.setQueryWeight(in.readFloat());
-            return rescorer;
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeQuery(queryBuilder);
-            scoreMode.writeTo(out);
-            out.writeFloat(rescoreQueryWeight);
-            out.writeFloat(queryWeight);
-        }
-
-        @Override
-        public String getWriteableName() {
-            return NAME;
-        }
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/rescore/RescoreParseElement.java b/core/src/main/java/org/elasticsearch/search/rescore/RescoreParseElement.java
index 7f9f272..149db6c 100644
--- a/core/src/main/java/org/elasticsearch/search/rescore/RescoreParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/rescore/RescoreParseElement.java
@@ -21,9 +21,12 @@ package org.elasticsearch.search.rescore;
 
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.internal.SearchContext;
 
+import java.io.IOException;
+
 /**
  *
  */
@@ -33,14 +36,14 @@ public class RescoreParseElement implements SearchParseElement {
     public void parse(XContentParser parser, SearchContext context) throws Exception {
         if (parser.currentToken() == XContentParser.Token.START_ARRAY) {
             while (parser.nextToken() != XContentParser.Token.END_ARRAY) {
-                parseSingleRescoreContext(parser, context);
+                context.addRescore(parseSingleRescoreContext(parser, context.indexShard().getQueryShardContext()));
             }
         } else {
-            parseSingleRescoreContext(parser, context);
+            context.addRescore(parseSingleRescoreContext(parser, context.indexShard().getQueryShardContext()));
         }
     }
 
-    public void parseSingleRescoreContext(XContentParser parser, SearchContext context) throws Exception {
+    public RescoreSearchContext parseSingleRescoreContext(XContentParser parser, QueryShardContext context) throws ElasticsearchParseException, IOException {
         String fieldName = null;
         RescoreSearchContext rescoreContext = null;
         Integer windowSize = null;
@@ -71,7 +74,7 @@ public class RescoreParseElement implements SearchParseElement {
         if (windowSize != null) {
             rescoreContext.setWindowSize(windowSize.intValue());
         }
-        context.addRescore(rescoreContext);
+        return rescoreContext;
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/rescore/Rescorer.java b/core/src/main/java/org/elasticsearch/search/rescore/Rescorer.java
index 3c90289..e3465a4 100644
--- a/core/src/main/java/org/elasticsearch/search/rescore/Rescorer.java
+++ b/core/src/main/java/org/elasticsearch/search/rescore/Rescorer.java
@@ -24,6 +24,7 @@ import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.TopDocs;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
@@ -68,11 +69,11 @@ public interface Rescorer {
      * Parses the {@link RescoreSearchContext} for this impelementation
      *
      * @param parser  the parser to read the context from
-     * @param context the current search context
+     * @param context the current shard context
      * @return the parsed {@link RescoreSearchContext}
      * @throws IOException if an {@link IOException} occurs while parsing the context
      */
-    public RescoreSearchContext parse(XContentParser parser, SearchContext context) throws IOException;
+    public RescoreSearchContext parse(XContentParser parser, QueryShardContext context) throws IOException;
 
     /**
      * Extracts all terms needed to exectue this {@link Rescorer}. This method
@@ -81,7 +82,7 @@ public interface Rescorer {
      * {@link SearchType#DFS_QUERY_THEN_FETCH}
      */
     public void extractTerms(SearchContext context, RescoreSearchContext rescoreContext, Set<Term> termsSet);
-    
+
     /*
      * TODO: At this point we only have one implemenation which modifies the
      * TopDocs given. Future implemenations might return actual resutls that
diff --git a/core/src/test/java/org/elasticsearch/indices/cache/query/IndicesRequestCacheIT.java b/core/src/test/java/org/elasticsearch/indices/cache/query/IndicesRequestCacheIT.java
index 65ba54d..46402c6 100644
--- a/core/src/test/java/org/elasticsearch/indices/cache/query/IndicesRequestCacheIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/cache/query/IndicesRequestCacheIT.java
@@ -26,7 +26,6 @@ import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInter
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.test.ESIntegTestCase;
-import org.joda.time.DateTimeZone;
 
 import java.util.List;
 
@@ -51,19 +50,15 @@ public class IndicesRequestCacheIT extends ESIntegTestCase {
         // which used to not work well with the query cache because of the handles stream output
         // see #9500
         final SearchResponse r1 = client().prepareSearch("index").setSize(0).setSearchType(SearchType.QUERY_THEN_FETCH)
-                .addAggregation(dateHistogram("histo").field("f").timeZone(DateTimeZone.forID("+01:00")).minDocCount(0)
-                        .dateHistogramInterval(DateHistogramInterval.MONTH))
-                .get();
+            .addAggregation(dateHistogram("histo").field("f").timeZone("+01:00").minDocCount(0).interval(DateHistogramInterval.MONTH)).get();
         assertSearchResponse(r1);
 
         // The cached is actually used
         assertThat(client().admin().indices().prepareStats("index").setRequestCache(true).get().getTotal().getRequestCache().getMemorySizeInBytes(), greaterThan(0l));
 
         for (int i = 0; i < 10; ++i) {
-            final SearchResponse r2 = client().prepareSearch("index").setSize(0)
-                    .setSearchType(SearchType.QUERY_THEN_FETCH).addAggregation(dateHistogram("histo").field("f")
-                            .timeZone(DateTimeZone.forID("+01:00")).minDocCount(0).dateHistogramInterval(DateHistogramInterval.MONTH))
-                    .get();
+            final SearchResponse r2 = client().prepareSearch("index").setSize(0).setSearchType(SearchType.QUERY_THEN_FETCH)
+                    .addAggregation(dateHistogram("histo").field("f").timeZone("+01:00").minDocCount(0).interval(DateHistogramInterval.MONTH)).get();
             assertSearchResponse(r2);
             Histogram h1 = r1.getAggregations().get("histo");
             Histogram h2 = r2.getAggregations().get("histo");
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java b/core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java
index ba503b4..eecc71f 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java
@@ -24,7 +24,6 @@ import org.apache.lucene.search.TermQuery;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.percolate.PercolateShardRequest;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.Index;
@@ -88,10 +87,8 @@ public class PercolateDocumentParserTests extends ESTestCase {
         queryShardContext = new QueryShardContext(indexSettings, null, null, null, mapperService, null, null, indicesQueriesRegistry);
 
         HighlightPhase highlightPhase = new HighlightPhase(Settings.EMPTY, new Highlighters());
-        AggregatorParsers aggregatorParsers = new AggregatorParsers(Collections.emptySet(), Collections.emptySet(),
-                new NamedWriteableRegistry());
-        AggregationPhase aggregationPhase = new AggregationPhase(new AggregationParseElement(aggregatorParsers, indicesQueriesRegistry),
-                new AggregationBinaryParseElement(aggregatorParsers, indicesQueriesRegistry));
+        AggregatorParsers aggregatorParsers = new AggregatorParsers(Collections.emptySet(), Collections.emptySet());
+        AggregationPhase aggregationPhase = new AggregationPhase(new AggregationParseElement(aggregatorParsers), new AggregationBinaryParseElement(aggregatorParsers));
         parser = new PercolateDocumentParser(highlightPhase, new SortParseElement(), aggregationPhase);
 
         request = Mockito.mock(PercolateShardRequest.class);
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java b/core/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java
index 7bac10d..b8ed2cc 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java
@@ -165,7 +165,7 @@ public class PercolatorAggregationsIT extends ESIntegTestCase {
                 percolateRequestBuilder.setOnlyCount(countOnly);
             }
 
-            percolateRequestBuilder.addAggregation(PipelineAggregatorBuilders.maxBucket("max_a", "a>_count"));
+            percolateRequestBuilder.addAggregation(PipelineAggregatorBuilders.maxBucket("max_a").setBucketsPaths("a>_count"));
 
             PercolateResponse response = percolateRequestBuilder.execute().actionGet();
             assertMatchCount(response, expectedCount[i % numUniqueQueries]);
@@ -245,7 +245,7 @@ public class PercolatorAggregationsIT extends ESIntegTestCase {
                 percolateRequestBuilder.setOnlyCount(countOnly);
             }
 
-            percolateRequestBuilder.addAggregation(PipelineAggregatorBuilders.maxBucket("max_terms", "terms>_count"));
+            percolateRequestBuilder.addAggregation(PipelineAggregatorBuilders.maxBucket("max_terms").setBucketsPaths("terms>_count"));
 
             PercolateResponse response = percolateRequestBuilder.execute().actionGet();
             assertMatchCount(response, numQueries);
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationCollectorTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationCollectorTests.java
index dd93b87..178c7ff 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationCollectorTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationCollectorTests.java
@@ -19,12 +19,9 @@
 
 package org.elasticsearch.search.aggregations;
 
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
 import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESSingleNodeTestCase;
@@ -37,7 +34,7 @@ public class AggregationCollectorTests extends ESSingleNodeTestCase {
         IndexService index = createIndex("idx");
         client().prepareIndex("idx", "type", "1").setSource("f", 5).execute().get();
         client().admin().indices().prepareRefresh("idx").get();
-
+        
         // simple field aggregation, no scores needed
         String fieldAgg = "{ \"my_terms\": {\"terms\": {\"field\": \"f\"}}}";
         assertFalse(needsScores(index, fieldAgg));
@@ -61,17 +58,12 @@ public class AggregationCollectorTests extends ESSingleNodeTestCase {
 
     private boolean needsScores(IndexService index, String agg) throws IOException {
         AggregatorParsers parser = getInstanceFromNode(AggregatorParsers.class);
-        IndicesQueriesRegistry queriesRegistry = getInstanceFromNode(IndicesQueriesRegistry.class);
         XContentParser aggParser = JsonXContent.jsonXContent.createParser(agg);
-        QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-        parseContext.reset(aggParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
         aggParser.nextToken();
         SearchContext searchContext = createSearchContext(index);
-        final AggregatorFactories factories = parser.parseAggregators(aggParser, parseContext);
+        final AggregatorFactories factories = parser.parseAggregators(aggParser, searchContext);
         AggregationContext aggregationContext = new AggregationContext(searchContext);
-        factories.init(aggregationContext);
-        final Aggregator[] aggregators = factories.createTopLevelAggregators();
+        final Aggregator[] aggregators = factories.createTopLevelAggregators(aggregationContext);
         assertEquals(1, aggregators.length);
         return aggregators[0].needsScores();
     }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java
new file mode 100644
index 0000000..80227ff
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java
@@ -0,0 +1,142 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations;
+
+import org.apache.lucene.util.LuceneTestCase.AwaitsFix;
+import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.client.Requests;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.json.JsonXContent;
+import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
+import org.elasticsearch.test.ESIntegTestCase;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.core.IsNull.notNullValue;
+
+@ESIntegTestCase.SuiteScopeTestCase
+@AwaitsFix(bugUrl = "needs fixing after the search request refactor. Do we need agg binary?")
+// NO RELEASE
+public class AggregationsBinaryIT extends ESIntegTestCase {
+
+    private static final String STRING_FIELD_NAME = "s_value";
+    private static final String INT_FIELD_NAME = "i_value";
+
+    @Override
+    public void setupSuiteScopeCluster() throws Exception {
+        createIndex("idx");
+        List<IndexRequestBuilder> builders = new ArrayList<>();
+        for (int i = 0; i < 5; i++) {
+            builders.add(client().prepareIndex("idx", "type").setSource(
+                    jsonBuilder().startObject().field(STRING_FIELD_NAME, "val" + i).field(INT_FIELD_NAME, i).endObject()));
+        }
+        indexRandom(true, builders);
+        ensureSearchable();
+    }
+
+    public void testAggregationsBinary() throws Exception {
+        TermsBuilder termsBuilder = AggregationBuilders.terms("terms").field(STRING_FIELD_NAME);
+        TermsBuilder subTerm = AggregationBuilders.terms("subterms").field(INT_FIELD_NAME);
+
+        // Create an XContentBuilder from sub aggregation
+        XContentBuilder subTermContentBuilder = JsonXContent.contentBuilder().startObject();
+        subTerm.toXContent(subTermContentBuilder, ToXContent.EMPTY_PARAMS);
+        subTermContentBuilder.endObject();
+
+        // Add sub aggregation as a XContentBuilder (binary_aggregation)
+        termsBuilder.subAggregation(subTermContentBuilder);
+
+        SearchResponse response = client().prepareSearch("idx").setTypes("type").addAggregation(termsBuilder).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("val" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(bucket.getKeyAsString(), equalTo("val" + i));
+            assertThat(bucket.getDocCount(), equalTo(1l));
+            Aggregations subAggs = bucket.getAggregations();
+            assertThat(subAggs, notNullValue());
+            assertThat(subAggs.asList().size(), equalTo(1));
+            Terms subTerms = subAggs.get("subterms");
+            assertThat(subTerms, notNullValue());
+            List<Bucket> subTermsBuckets = subTerms.getBuckets();
+            assertThat(subTermsBuckets, notNullValue());
+            assertThat(subTermsBuckets.size(), equalTo(1));
+            assertThat(((Number) subTermsBuckets.get(0).getKey()).intValue(), equalTo(i));
+            assertThat(subTermsBuckets.get(0).getDocCount(), equalTo(1l));
+        }
+    }
+
+    public void testAggregationsBinarySameContentType() throws Exception {
+        TermsBuilder termsBuilder = AggregationBuilders.terms("terms").field(STRING_FIELD_NAME);
+        TermsBuilder subTerm = AggregationBuilders.terms("subterms").field(INT_FIELD_NAME);
+
+        // Create an XContentBuilder from sub aggregation
+
+        XContentBuilder subTermContentBuilder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+        subTermContentBuilder.startObject();
+        subTerm.toXContent(subTermContentBuilder, ToXContent.EMPTY_PARAMS);
+        subTermContentBuilder.endObject();
+
+        // Add sub aggregation as a XContentBuilder (binary_aggregation)
+        termsBuilder.subAggregation(subTermContentBuilder);
+
+        SearchResponse response = client().prepareSearch("idx").setTypes("type").addAggregation(termsBuilder).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("val" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(bucket.getKeyAsString(), equalTo("val" + i));
+            assertThat(bucket.getDocCount(), equalTo(1l));
+            Aggregations subAggs = bucket.getAggregations();
+            assertThat(subAggs, notNullValue());
+            assertThat(subAggs.asList().size(), equalTo(1));
+            Terms subTerms = subAggs.get("subterms");
+            assertThat(subTerms, notNullValue());
+            List<Bucket> subTermsBuckets = subTerms.getBuckets();
+            assertThat(subTermsBuckets, notNullValue());
+            assertThat(subTermsBuckets.size(), equalTo(1));
+            assertThat(((Number) subTermsBuckets.get(0).getKey()).intValue(), equalTo(i));
+            assertThat(subTermsBuckets.get(0).getDocCount(), equalTo(1l));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java b/core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java
deleted file mode 100644
index 278db79..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java
+++ /dev/null
@@ -1,289 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations;
-
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsFilter;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.search.SearchModule;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.IndexSettingsModule;
-import org.elasticsearch.test.TestSearchContext;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public abstract class BaseAggregationTestCase<AF extends AggregatorFactory> extends ESTestCase {
-
-    protected static final String STRING_FIELD_NAME = "mapped_string";
-    protected static final String INT_FIELD_NAME = "mapped_int";
-    protected static final String DOUBLE_FIELD_NAME = "mapped_double";
-    protected static final String BOOLEAN_FIELD_NAME = "mapped_boolean";
-    protected static final String DATE_FIELD_NAME = "mapped_date";
-    protected static final String OBJECT_FIELD_NAME = "mapped_object";
-    protected static final String[] mappedFieldNames = new String[] { STRING_FIELD_NAME, INT_FIELD_NAME,
-            DOUBLE_FIELD_NAME, BOOLEAN_FIELD_NAME, DATE_FIELD_NAME, OBJECT_FIELD_NAME };
-
-    private static Injector injector;
-    private static Index index;
-
-    private static String[] currentTypes;
-
-    protected static String[] getCurrentTypes() {
-        return currentTypes;
-    }
-
-    private static NamedWriteableRegistry namedWriteableRegistry;
-
-    private static AggregatorParsers aggParsers;
-    private static IndicesQueriesRegistry queriesRegistry;
-    private static ParseFieldMatcher parseFieldMatcher;
-
-    protected abstract AF createTestAggregatorFactory();
-
-    /**
-     * Setup for the whole base test class.
-     */
-    @BeforeClass
-    public static void init() throws IOException {
-        Settings settings = Settings.settingsBuilder()
-                .put("name", BaseAggregationTestCase.class.toString())
-                .put("path.home", createTempDir())
-                .build();
-
-        namedWriteableRegistry =  new NamedWriteableRegistry();
-        index = new Index("test");
-        injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)),
-                new SettingsModule(settings, new SettingsFilter(settings)),
-                new ThreadPoolModule(new ThreadPool(settings)),
-                new ScriptModule(settings),
-                new IndicesModule() {
-
-                    @Override
-                    protected void configure() {
-                        bindMapperExtension();
-                    }
-                }, new SearchModule(settings, namedWriteableRegistry) {
-                    @Override
-                    protected void configureSearch() {
-                        // Skip me
-                    }
-                    @Override
-                    protected void configureSuggesters() {
-                        // Skip me
-                    }
-                },
-                new IndexSettingsModule(index, settings),
-
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        bind(ClusterService.class).toProvider(Providers.of((ClusterService) null));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                        bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry);
-                    }
-                }
-        ).createInjector();
-        aggParsers = injector.getInstance(AggregatorParsers.class);
-        //create some random type with some default field, those types will stick around for all of the subclasses
-        currentTypes = new String[randomIntBetween(0, 5)];
-        for (int i = 0; i < currentTypes.length; i++) {
-            String type = randomAsciiOfLengthBetween(1, 10);
-            currentTypes[i] = type;
-        }
-        queriesRegistry = injector.getInstance(IndicesQueriesRegistry.class);
-        parseFieldMatcher = ParseFieldMatcher.STRICT;
-    }
-
-    @AfterClass
-    public static void afterClass() throws Exception {
-        terminate(injector.getInstance(ThreadPool.class));
-        injector = null;
-        index = null;
-        aggParsers = null;
-        currentTypes = null;
-        namedWriteableRegistry = null;
-    }
-
-    @Before
-    public void beforeTest() {
-        //set some random types to be queried as part the search request, before each test
-        String[] types = getRandomTypes();
-        TestSearchContext testSearchContext = new TestSearchContext();
-        testSearchContext.setTypes(types);
-        SearchContext.setCurrent(testSearchContext);
-    }
-
-    @After
-    public void afterTest() {
-        SearchContext.removeCurrent();
-    }
-
-    /**
-     * Generic test that creates new AggregatorFactory from the test
-     * AggregatorFactory and checks both for equality and asserts equality on
-     * the two queries.
-     */
-    public void testFromXContent() throws IOException {
-        AF testAgg = createTestAggregatorFactory();
-        AggregatorFactories factories = AggregatorFactories.builder().addAggregator(testAgg).build();
-        String contentString = factories.toString();
-        XContentParser parser = XContentFactory.xContent(contentString).createParser(contentString);
-        QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-        parseContext.reset(parser);
-        parseContext.parseFieldMatcher(parseFieldMatcher);
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.FIELD_NAME, parser.nextToken());
-        assertEquals(testAgg.name, parser.currentName());
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.FIELD_NAME, parser.nextToken());
-        assertEquals(testAgg.type.name(), parser.currentName());
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        AggregatorFactory newAgg = aggParsers.parser(testAgg.getType()).parse(testAgg.name, parser, parseContext);
-        assertSame(XContentParser.Token.END_OBJECT, parser.currentToken());
-        assertSame(XContentParser.Token.END_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.END_OBJECT, parser.nextToken());
-        assertNull(parser.nextToken());
-        assertNotNull(newAgg);
-        assertNotSame(newAgg, testAgg);
-        assertEquals(testAgg, newAgg);
-        assertEquals(testAgg.hashCode(), newAgg.hashCode());
-    }
-
-    /**
-     * Test serialization and deserialization of the test AggregatorFactory.
-     */
-
-    public void testSerialization() throws IOException {
-        AF testAgg = createTestAggregatorFactory();
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            testAgg.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                AggregatorFactory prototype = (AggregatorFactory) namedWriteableRegistry.getPrototype(AggregatorFactory.class, testAgg.getWriteableName());
-                AggregatorFactory deserializedQuery = prototype.readFrom(in);
-                assertEquals(deserializedQuery, testAgg);
-                assertEquals(deserializedQuery.hashCode(), testAgg.hashCode());
-                assertNotSame(deserializedQuery, testAgg);
-            }
-        }
-    }
-
-
-    public void testEqualsAndHashcode() throws IOException {
-        AF firstAgg = createTestAggregatorFactory();
-        assertFalse("aggregation is equal to null", firstAgg.equals(null));
-        assertFalse("aggregation is equal to incompatible type", firstAgg.equals(""));
-        assertTrue("aggregation is not equal to self", firstAgg.equals(firstAgg));
-        assertThat("same aggregation's hashcode returns different values if called multiple times", firstAgg.hashCode(),
-                equalTo(firstAgg.hashCode()));
-
-        AF secondQuery = copyAggregation(firstAgg);
-        assertTrue("aggregation is not equal to self", secondQuery.equals(secondQuery));
-        assertTrue("aggregation is not equal to its copy", firstAgg.equals(secondQuery));
-        assertTrue("equals is not symmetric", secondQuery.equals(firstAgg));
-        assertThat("aggregation copy's hashcode is different from original hashcode", secondQuery.hashCode(), equalTo(firstAgg.hashCode()));
-
-        AF thirdQuery = copyAggregation(secondQuery);
-        assertTrue("aggregation is not equal to self", thirdQuery.equals(thirdQuery));
-        assertTrue("aggregation is not equal to its copy", secondQuery.equals(thirdQuery));
-        assertThat("aggregation copy's hashcode is different from original hashcode", secondQuery.hashCode(),
-                equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not transitive", firstAgg.equals(thirdQuery));
-        assertThat("aggregation copy's hashcode is different from original hashcode", firstAgg.hashCode(), equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not symmetric", thirdQuery.equals(secondQuery));
-        assertTrue("equals is not symmetric", thirdQuery.equals(firstAgg));
-    }
-
-    // we use the streaming infra to create a copy of the query provided as
-    // argument
-    private AF copyAggregation(AF agg) throws IOException {
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            agg.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                AggregatorFactory prototype = (AggregatorFactory) namedWriteableRegistry.getPrototype(AggregatorFactory.class, agg.getWriteableName());
-                @SuppressWarnings("unchecked")
-                AF secondAgg = (AF) prototype.readFrom(in);
-                return secondAgg;
-            }
-        }
-    }
-
-    protected String[] getRandomTypes() {
-        String[] types;
-        if (currentTypes.length > 0 && randomBoolean()) {
-            int numberOfQueryTypes = randomIntBetween(1, currentTypes.length);
-            types = new String[numberOfQueryTypes];
-            for (int i = 0; i < numberOfQueryTypes; i++) {
-                types[i] = randomFrom(currentTypes);
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[] { MetaData.ALL };
-            } else {
-                types = new String[0];
-            }
-        }
-        return types;
-    }
-
-    public String randomNumericField() {
-        int randomInt = randomInt(3);
-        switch (randomInt) {
-        case 0:
-            return DATE_FIELD_NAME;
-        case 1:
-            return DOUBLE_FIELD_NAME;
-        case 2:
-        default:
-            return INT_FIELD_NAME;
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/BasePipelineAggregationTestCase.java b/core/src/test/java/org/elasticsearch/search/aggregations/BasePipelineAggregationTestCase.java
deleted file mode 100644
index 30b6584..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/BasePipelineAggregationTestCase.java
+++ /dev/null
@@ -1,292 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations;
-
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsFilter;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.search.SearchModule;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.IndexSettingsModule;
-import org.elasticsearch.test.TestSearchContext;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public abstract class BasePipelineAggregationTestCase<AF extends PipelineAggregatorFactory> extends ESTestCase {
-
-    protected static final String STRING_FIELD_NAME = "mapped_string";
-    protected static final String INT_FIELD_NAME = "mapped_int";
-    protected static final String DOUBLE_FIELD_NAME = "mapped_double";
-    protected static final String BOOLEAN_FIELD_NAME = "mapped_boolean";
-    protected static final String DATE_FIELD_NAME = "mapped_date";
-    protected static final String OBJECT_FIELD_NAME = "mapped_object";
-    protected static final String[] mappedFieldNames = new String[] { STRING_FIELD_NAME, INT_FIELD_NAME,
-            DOUBLE_FIELD_NAME, BOOLEAN_FIELD_NAME, DATE_FIELD_NAME, OBJECT_FIELD_NAME };
-
-    private static Injector injector;
-    private static Index index;
-
-    private static String[] currentTypes;
-
-    protected static String[] getCurrentTypes() {
-        return currentTypes;
-    }
-
-    private static NamedWriteableRegistry namedWriteableRegistry;
-
-    private static AggregatorParsers aggParsers;
-    private static ParseFieldMatcher parseFieldMatcher;
-    private static IndicesQueriesRegistry queriesRegistry;
-
-    protected abstract AF createTestAggregatorFactory();
-
-    /**
-     * Setup for the whole base test class.
-     */
-    @BeforeClass
-    public static void init() throws IOException {
-        Settings settings = Settings.settingsBuilder()
-                .put("name", BasePipelineAggregationTestCase.class.toString())
-                .put("path.home", createTempDir())
-                .build();
-
-        namedWriteableRegistry = new NamedWriteableRegistry();
-        index = new Index("test");
-        injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)),
-                new SettingsModule(settings, new SettingsFilter(settings)),
-                new ThreadPoolModule(new ThreadPool(settings)),
-                new ScriptModule(settings),
-                new IndicesModule() {
-
-                    @Override
-                    protected void configure() {
-                        bindMapperExtension();
-                    }
-                }, new SearchModule(settings, namedWriteableRegistry) {
-                    @Override
-                    protected void configureSearch() {
-                        // Skip me
-                    }
-                    @Override
-                    protected void configureSuggesters() {
-                        // Skip me
-                    }
-                },
-                new IndexSettingsModule(index, settings),
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        bind(ClusterService.class).toProvider(Providers.of((ClusterService) null));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                        bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry);
-                    }
-                }
-        ).createInjector();
-        aggParsers = injector.getInstance(AggregatorParsers.class);
-        //create some random type with some default field, those types will stick around for all of the subclasses
-        currentTypes = new String[randomIntBetween(0, 5)];
-        for (int i = 0; i < currentTypes.length; i++) {
-            String type = randomAsciiOfLengthBetween(1, 10);
-            currentTypes[i] = type;
-        }
-        queriesRegistry = injector.getInstance(IndicesQueriesRegistry.class);
-        parseFieldMatcher = ParseFieldMatcher.STRICT;
-    }
-
-    @AfterClass
-    public static void afterClass() throws Exception {
-        terminate(injector.getInstance(ThreadPool.class));
-        injector = null;
-        index = null;
-        aggParsers = null;
-        currentTypes = null;
-        namedWriteableRegistry = null;
-    }
-
-    @Before
-    public void beforeTest() {
-        //set some random types to be queried as part the search request, before each test
-        String[] types = getRandomTypes();
-        TestSearchContext testSearchContext = new TestSearchContext();
-        testSearchContext.setTypes(types);
-        SearchContext.setCurrent(testSearchContext);
-    }
-
-    @After
-    public void afterTest() {
-        SearchContext.removeCurrent();
-    }
-
-    /**
-     * Generic test that creates new AggregatorFactory from the test
-     * AggregatorFactory and checks both for equality and asserts equality on
-     * the two queries.
-     */
-
-    public void testFromXContent() throws IOException {
-        AF testAgg = createTestAggregatorFactory();
-        AggregatorFactories factories = AggregatorFactories.builder().skipResolveOrder().addPipelineAggregator(testAgg).build();
-        String contentString = factories.toString();
-        System.out.println(contentString);
-        XContentParser parser = XContentFactory.xContent(contentString).createParser(contentString);
-        QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-        parseContext.reset(parser);
-        parseContext.parseFieldMatcher(parseFieldMatcher);
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.FIELD_NAME, parser.nextToken());
-        assertEquals(testAgg.name(), parser.currentName());
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.FIELD_NAME, parser.nextToken());
-        assertEquals(testAgg.type(), parser.currentName());
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        PipelineAggregatorFactory newAgg = aggParsers.pipelineAggregator(testAgg.getWriteableName()).parse(testAgg.name(), parser,
-                parseContext);
-        assertSame(XContentParser.Token.END_OBJECT, parser.currentToken());
-        assertSame(XContentParser.Token.END_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.END_OBJECT, parser.nextToken());
-        assertNull(parser.nextToken());
-        assertNotNull(newAgg);
-        assertNotSame(newAgg, testAgg);
-        assertEquals(testAgg, newAgg);
-        assertEquals(testAgg.hashCode(), newAgg.hashCode());
-    }
-
-    /**
-     * Test serialization and deserialization of the test AggregatorFactory.
-     */
-
-    public void testSerialization() throws IOException {
-        AF testAgg = createTestAggregatorFactory();
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            testAgg.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                PipelineAggregatorFactory prototype = aggParsers.pipelineAggregator(testAgg.getWriteableName()).getFactoryPrototype();
-                PipelineAggregatorFactory deserializedQuery = prototype.readFrom(in);
-                assertEquals(deserializedQuery, testAgg);
-                assertEquals(deserializedQuery.hashCode(), testAgg.hashCode());
-                assertNotSame(deserializedQuery, testAgg);
-            }
-        }
-    }
-
-
-    public void testEqualsAndHashcode() throws IOException {
-        AF firstAgg = createTestAggregatorFactory();
-        assertFalse("aggregation is equal to null", firstAgg.equals(null));
-        assertFalse("aggregation is equal to incompatible type", firstAgg.equals(""));
-        assertTrue("aggregation is not equal to self", firstAgg.equals(firstAgg));
-        assertThat("same aggregation's hashcode returns different values if called multiple times", firstAgg.hashCode(),
-                equalTo(firstAgg.hashCode()));
-
-        AF secondQuery = copyAggregation(firstAgg);
-        assertTrue("aggregation is not equal to self", secondQuery.equals(secondQuery));
-        assertTrue("aggregation is not equal to its copy", firstAgg.equals(secondQuery));
-        assertTrue("equals is not symmetric", secondQuery.equals(firstAgg));
-        assertThat("aggregation copy's hashcode is different from original hashcode", secondQuery.hashCode(), equalTo(firstAgg.hashCode()));
-
-        AF thirdQuery = copyAggregation(secondQuery);
-        assertTrue("aggregation is not equal to self", thirdQuery.equals(thirdQuery));
-        assertTrue("aggregation is not equal to its copy", secondQuery.equals(thirdQuery));
-        assertThat("aggregation copy's hashcode is different from original hashcode", secondQuery.hashCode(),
-                equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not transitive", firstAgg.equals(thirdQuery));
-        assertThat("aggregation copy's hashcode is different from original hashcode", firstAgg.hashCode(), equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not symmetric", thirdQuery.equals(secondQuery));
-        assertTrue("equals is not symmetric", thirdQuery.equals(firstAgg));
-    }
-
-    // we use the streaming infra to create a copy of the query provided as
-    // argument
-    private AF copyAggregation(AF agg) throws IOException {
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            agg.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                PipelineAggregatorFactory prototype = aggParsers.pipelineAggregator(agg.getWriteableName()).getFactoryPrototype();
-                @SuppressWarnings("unchecked")
-                AF secondAgg = (AF) prototype.readFrom(in);
-                return secondAgg;
-            }
-        }
-    }
-
-    protected String[] getRandomTypes() {
-        String[] types;
-        if (currentTypes.length > 0 && randomBoolean()) {
-            int numberOfQueryTypes = randomIntBetween(1, currentTypes.length);
-            types = new String[numberOfQueryTypes];
-            for (int i = 0; i < numberOfQueryTypes; i++) {
-                types[i] = randomFrom(currentTypes);
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[] { MetaData.ALL };
-            } else {
-                types = new String[0];
-            }
-        }
-        return types;
-    }
-
-    public String randomNumericField() {
-        int randomInt = randomInt(3);
-        switch (randomInt) {
-        case 0:
-            return DATE_FIELD_NAME;
-        case 1:
-            return DOUBLE_FIELD_NAME;
-        case 2:
-        default:
-            return INT_FIELD_NAME;
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/MissingValueIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/MissingValueIT.java
index 1821d09..63008bc 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/MissingValueIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/MissingValueIT.java
@@ -147,10 +147,7 @@ public class MissingValueIT extends ESIntegTestCase {
     }
 
     public void testDateHistogram() {
-        SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(
-                        dateHistogram("my_histogram").field("date").dateHistogramInterval(DateHistogramInterval.YEAR).missing("2014-05-07"))
-                .get();
+        SearchResponse response = client().prepareSearch("idx").addAggregation(dateHistogram("my_histogram").field("date").interval(DateHistogramInterval.YEAR).missing("2014-05-07")).get();
         assertSearchResponse(response);
         Histogram histogram = response.getAggregations().get("my_histogram");
         assertEquals(2, histogram.getBuckets().size());
@@ -159,10 +156,7 @@ public class MissingValueIT extends ESIntegTestCase {
         assertEquals("2015-01-01T00:00:00.000Z", histogram.getBuckets().get(1).getKeyAsString());
         assertEquals(1, histogram.getBuckets().get(1).getDocCount());
 
-        response = client().prepareSearch("idx")
-                .addAggregation(
-                        dateHistogram("my_histogram").field("date").dateHistogramInterval(DateHistogramInterval.YEAR).missing("2015-05-07"))
-                .get();
+        response = client().prepareSearch("idx").addAggregation(dateHistogram("my_histogram").field("date").interval(DateHistogramInterval.YEAR).missing("2015-05-07")).get();
         assertSearchResponse(response);
         histogram = response.getAggregations().get("my_histogram");
         assertEquals(1, histogram.getBuckets().size());
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/SubAggCollectionModeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/SubAggCollectionModeTests.java
deleted file mode 100644
index 131144d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/SubAggCollectionModeTests.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class SubAggCollectionModeTests extends ESTestCase {
-
-    public void testValidOrdinals() {
-        assertThat(SubAggCollectionMode.DEPTH_FIRST.ordinal(), equalTo(0));
-        assertThat(SubAggCollectionMode.BREADTH_FIRST.ordinal(), equalTo(1));
-    }
-
-    public void testwriteTo() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            SubAggCollectionMode.DEPTH_FIRST.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(0));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            SubAggCollectionMode.BREADTH_FIRST.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(1));
-            }
-        }
-    }
-
-    public void testReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(0);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(SubAggCollectionMode.BREADTH_FIRST.readFrom(in), equalTo(SubAggCollectionMode.DEPTH_FIRST));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(1);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(SubAggCollectionMode.BREADTH_FIRST.readFrom(in), equalTo(SubAggCollectionMode.BREADTH_FIRST));
-            }
-        }
-    }
-
-    public void testInvalidReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(randomIntBetween(2, Integer.MAX_VALUE));
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                SubAggCollectionMode.BREADTH_FIRST.readFrom(in);
-                fail("Expected IOException");
-            } catch(IOException e) {
-                assertThat(e.getMessage(), containsString("Unknown SubAggCollectionMode ordinal ["));
-            }
-
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java
index 868ca23..e0c7d23 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java
@@ -134,8 +134,7 @@ public class ChildrenIT extends ESIntegTestCase {
                 .setQuery(matchQuery("randomized", true))
                 .addAggregation(
                         terms("category").field("category").size(0).subAggregation(
-children("to_comment", "comment")
-                                .subAggregation(
+                                children("to_comment").childType("comment").subAggregation(
                                         terms("commenters").field("commenter").size(0).subAggregation(
                                                 topHits("top_comments")
                                         ))
@@ -176,7 +175,7 @@ children("to_comment", "comment")
                 .setQuery(matchQuery("randomized", false))
                 .addAggregation(
                         terms("category").field("category").size(0).subAggregation(
-                        children("to_comment", "comment").subAggregation(topHits("top_comments").sort("_uid", SortOrder.ASC))
+                                children("to_comment").childType("comment").subAggregation(topHits("top_comments").addSort("_uid", SortOrder.ASC))
                         )
                 ).get();
         assertSearchResponse(searchResponse);
@@ -251,7 +250,7 @@ children("to_comment", "comment")
 
         for (int i = 0; i < 10; i++) {
             SearchResponse searchResponse = client().prepareSearch(indexName)
-                    .addAggregation(children("children", "child").subAggregation(sum("counts").field("count")))
+                    .addAggregation(children("children").childType("child").subAggregation(sum("counts").field("count")))
                     .get();
 
             assertNoFailures(searchResponse);
@@ -280,7 +279,7 @@ children("to_comment", "comment")
     public void testNonExistingChildType() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("test")
                 .addAggregation(
-children("non-existing", "xyz")
+                        children("non-existing").childType("xyz")
                 ).get();
         assertSearchResponse(searchResponse);
 
@@ -320,7 +319,8 @@ children("non-existing", "xyz")
 
         SearchResponse response = client().prepareSearch(indexName).setTypes(masterType)
                 .setQuery(hasChildQuery(childType, termQuery("color", "orange")))
-.addAggregation(children("my-refinements", childType)
+                .addAggregation(children("my-refinements")
+                                .childType(childType)
                                 .subAggregation(terms("my-colors").field("color"))
                                 .subAggregation(terms("my-sizes").field("size"))
                 ).get();
@@ -371,7 +371,8 @@ children("non-existing", "xyz")
         SearchResponse response = client().prepareSearch(indexName)
                 .setQuery(matchQuery("name", "europe"))
                 .addAggregation(
-                children(parentType, parentType).subAggregation(children(childType, childType).subAggregation(
+                        children(parentType).childType(parentType).subAggregation(
+                                children(childType).childType(childType).subAggregation(
                                         terms("name").field("name")
                                 )
                         )
@@ -419,7 +420,7 @@ children("non-existing", "xyz")
             .setSize(0)
             .addAggregation(AggregationBuilders.terms("towns").field("town")
                 .subAggregation(AggregationBuilders.terms("parent_names").field("name")
-.subAggregation(AggregationBuilders.children("child_docs", "childType"))
+                    .subAggregation(AggregationBuilders.children("child_docs").childType("childType"))
                 )
             )
             .get();
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenTests.java
deleted file mode 100644
index 88e0fba..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenTests.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.children.ParentToChildrenAggregator;
-import org.elasticsearch.search.aggregations.bucket.children.ParentToChildrenAggregator.Factory;
-
-public class ChildrenTests extends BaseAggregationTestCase<ParentToChildrenAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String childType = randomAsciiOfLengthBetween(5, 40);
-        Factory factory = new Factory(name, childType);
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
index 15ac49a..ef784b6 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
@@ -38,7 +38,6 @@ import org.elasticsearch.script.ScriptModule;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.SearchScript;
 import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
@@ -161,7 +160,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testSingleValuedField() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -196,7 +195,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testSingleValuedFieldWithTimeZone() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(1).timeZone(DateTimeZone.forID("+01:00"))).execute()
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(1).timeZone("+01:00")).execute()
                 .actionGet();
         DateTimeZone tz = DateTimeZone.forID("+01:00");
         assertSearchResponse(response);
@@ -254,7 +253,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.KEY_ASC))
                 .execute().actionGet();
 
@@ -277,7 +276,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.KEY_DESC))
                 .execute().actionGet();
 
@@ -299,7 +298,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.COUNT_ASC))
                 .execute().actionGet();
 
@@ -321,7 +320,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.COUNT_DESC))
                 .execute().actionGet();
 
@@ -341,7 +340,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testSingleValuedFieldWithSubAggregation() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH)
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH)
                         .subAggregation(sum("sum").field("value")))
                 .execute().actionGet();
 
@@ -398,7 +397,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testSingleValuedFieldWithSubAggregationInherited() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH)
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH)
                         .subAggregation(max("max")))
                 .execute().actionGet();
 
@@ -445,7 +444,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.aggregation("sum", true))
                         .subAggregation(max("sum").field("value")))
                 .execute().actionGet();
@@ -468,7 +467,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.aggregation("sum", false))
                         .subAggregation(max("sum").field("value")))
                 .execute().actionGet();
@@ -491,7 +490,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.aggregation("stats", "sum", true))
                         .subAggregation(stats("stats").field("value")))
                 .execute().actionGet();
@@ -514,7 +513,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.aggregation("stats", "sum", false))
                         .subAggregation(stats("stats").field("value")))
                 .execute().actionGet();
@@ -538,7 +537,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .addAggregation(dateHistogram("histo")
                         .field("date")
                         .script(new Script("", ScriptType.INLINE, FieldValueScriptEngine.NAME, null))
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)).execute().actionGet();
+                        .interval(DateHistogramInterval.MONTH)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -582,7 +581,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testMultiValuedField() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").field("dates").dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").field("dates").interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -626,7 +625,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("dates")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.COUNT_DESC))
                 .execute().actionGet();
 
@@ -672,7 +671,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .addAggregation(dateHistogram("histo")
                         .field("dates")
                         .script(new Script("", ScriptType.INLINE, FieldValueScriptEngine.NAME, null))
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)).execute().actionGet();
+                        .interval(DateHistogramInterval.MONTH)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -726,7 +725,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .addAggregation(dateHistogram("histo")
                         .field("dates")
                         .script(new Script("", ScriptType.INLINE, FieldValueScriptEngine.NAME, null))
-                        .dateHistogramInterval(DateHistogramInterval.MONTH).subAggregation(max("max"))).execute().actionGet();
+                        .interval(DateHistogramInterval.MONTH).subAggregation(max("max"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -787,7 +786,7 @@ public class DateHistogramIT extends ESIntegTestCase {
      */
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").script(new Script("date", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").script(new Script("date", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -823,7 +822,7 @@ public class DateHistogramIT extends ESIntegTestCase {
     public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
-                        .script(new Script("date", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .script(new Script("date", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).interval(DateHistogramInterval.MONTH)
                         .subAggregation(max("max"))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -867,7 +866,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testScriptMultiValued() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").script(new Script("dates", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").script(new Script("dates", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -921,7 +920,7 @@ public class DateHistogramIT extends ESIntegTestCase {
     public void testScriptMultiValuedWithAggregatorInherited() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
-                        .script(new Script("dates", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .script(new Script("dates", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).interval(DateHistogramInterval.MONTH)
                         .subAggregation(max("max"))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -975,7 +974,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx_unmapped")
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -988,7 +987,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testPartiallyUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx", "idx_unmapped")
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -1058,8 +1057,8 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(dateHistogram("date_histo")
                         .field("date")
-                        .timeZone(DateTimeZone.forID("-02:00"))
-                        .dateHistogramInterval(DateHistogramInterval.DAY)
+                        .timeZone("-02:00")
+                        .interval(DateHistogramInterval.DAY)
                         .format("yyyy-MM-dd:HH-mm-ssZZ"))
                 .execute().actionGet();
 
@@ -1152,10 +1151,10 @@ public class DateHistogramIT extends ESIntegTestCase {
             response = client().prepareSearch("idx2")
                     .addAggregation(dateHistogram("histo")
                             .field("date")
-                            .dateHistogramInterval(DateHistogramInterval.days(interval))
+                            .interval(DateHistogramInterval.days(interval))
                             .minDocCount(0)
                                     // when explicitly specifying a format, the extended bounds should be defined by the same format
-                            .extendedBounds(new ExtendedBounds(format(boundsMin, pattern), format(boundsMax, pattern)))
+                            .extendedBounds(format(boundsMin, pattern), format(boundsMax, pattern))
                             .format(pattern))
                     .execute().actionGet();
 
@@ -1228,8 +1227,8 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .prepareSearch(index)
                 .setQuery(QueryBuilders.rangeQuery("date").from("now/d").to("now/d").includeLower(true).includeUpper(true).timeZone(timezone.getID()))
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.hours(1)).timeZone(timezone).minDocCount(0)
-                                .extendedBounds(new ExtendedBounds("now/d", "now/d+23h"))
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.hours(1)).timeZone(timezone.getID()).minDocCount(0)
+                                .extendedBounds("now/d", "now/d+23h")
                 ).execute().actionGet();
         assertSearchResponse(response);
 
@@ -1267,7 +1266,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(dateHistogram("date_histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.DAY))
+                        .interval(DateHistogramInterval.DAY))
                 .execute().actionGet();
 
         assertSearchHits(response, "0", "1", "2", "3", "4");
@@ -1286,7 +1285,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testIssue6965() {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").field("date").timeZone(DateTimeZone.forID("+01:00")).dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0))
+                .addAggregation(dateHistogram("histo").field("date").timeZone("+01:00").interval(DateHistogramInterval.MONTH).minDocCount(0))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -1327,7 +1326,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 client().prepareIndex("test9491", "type").setSource("d", "2014-11-08T13:00:00Z"));
         ensureSearchable("test9491");
         SearchResponse response = client().prepareSearch("test9491")
-                .addAggregation(dateHistogram("histo").field("d").dateHistogramInterval(DateHistogramInterval.YEAR).timeZone(DateTimeZone.forID("Asia/Jerusalem")))
+                .addAggregation(dateHistogram("histo").field("d").interval(DateHistogramInterval.YEAR).timeZone("Asia/Jerusalem"))
                 .execute().actionGet();
         assertSearchResponse(response);
         Histogram histo = response.getAggregations().get("histo");
@@ -1344,7 +1343,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 client().prepareIndex("test8209", "type").setSource("d", "2014-04-30T00:00:00Z"));
         ensureSearchable("test8209");
         SearchResponse response = client().prepareSearch("test8209")
-                .addAggregation(dateHistogram("histo").field("d").dateHistogramInterval(DateHistogramInterval.MONTH).timeZone(DateTimeZone.forID("CET"))
+                .addAggregation(dateHistogram("histo").field("d").interval(DateHistogramInterval.MONTH).timeZone("CET")
                         .minDocCount(0))
                 .execute().actionGet();
         assertSearchResponse(response);
@@ -1362,7 +1361,7 @@ public class DateHistogramIT extends ESIntegTestCase {
     }
 
     /**
-     * see issue #9634, negative dateHistogramInterval in date_histogram should raise exception
+     * see issue #9634, negative interval in date_histogram should raise exception
      */
     public void testExceptionOnNegativeInterval() {
         try {
@@ -1371,12 +1370,12 @@ public class DateHistogramIT extends ESIntegTestCase {
                     .actionGet();
             fail();
         } catch (SearchPhaseExecutionException e) {
-            assertThat(e.toString(), containsString("IllegalArgumentException"));
+            assertThat(e.toString(), containsString("ElasticsearchParseException"));
         }
     }
 
     public void testTimestampField() { // see #11692
-        SearchResponse response = client().prepareSearch("idx").addAggregation(dateHistogram("histo").field("_timestamp").dateHistogramInterval(randomFrom(DateHistogramInterval.DAY, DateHistogramInterval.MONTH))).get();
+        SearchResponse response = client().prepareSearch("idx").addAggregation(dateHistogram("histo").field("_timestamp").interval(randomFrom(DateHistogramInterval.DAY, DateHistogramInterval.MONTH))).get();
         assertSearchResponse(response);
         Histogram histo = response.getAggregations().get("histo");
         assertThat(histo.getBuckets().size(), greaterThan(0));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
index 70a6a56..75c0fc2 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
@@ -92,7 +92,7 @@ public class DateHistogramOffsetIT extends ESIntegTestCase {
                         .field("date")
                         .offset("2h")
                         .format(DATE_FORMAT)
-                        .dateHistogramInterval(DateHistogramInterval.DAY))
+                        .interval(DateHistogramInterval.DAY))
                 .execute().actionGet();
 
         assertThat(response.getHits().getTotalHits(), equalTo(5l));
@@ -114,7 +114,7 @@ public class DateHistogramOffsetIT extends ESIntegTestCase {
                         .field("date")
                         .offset("-2h")
                         .format(DATE_FORMAT)
-                        .dateHistogramInterval(DateHistogramInterval.DAY))
+                        .interval(DateHistogramInterval.DAY))
                 .execute().actionGet();
 
         assertThat(response.getHits().getTotalHits(), equalTo(5l));
@@ -141,7 +141,7 @@ public class DateHistogramOffsetIT extends ESIntegTestCase {
                         .offset("6h")
                         .minDocCount(0)
                         .format(DATE_FORMAT)
-                        .dateHistogramInterval(DateHistogramInterval.DAY))
+                        .interval(DateHistogramInterval.DAY))
                 .execute().actionGet();
 
         assertThat(response.getHits().getTotalHits(), equalTo(24l));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java
deleted file mode 100644
index bc076e8..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java
+++ /dev/null
@@ -1,110 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
-import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Order;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.DateHistogramFactory;
-
-public class DateHistogramTests extends BaseAggregationTestCase<DateHistogramFactory> {
-
-    @Override
-    protected DateHistogramFactory createTestAggregatorFactory() {
-        DateHistogramFactory factory = new DateHistogramFactory("foo");
-        factory.field(INT_FIELD_NAME);
-        if (randomBoolean()) {
-            factory.interval(randomIntBetween(1, 100000));
-        } else {
-            if (randomBoolean()) {
-                factory.dateHistogramInterval(randomFrom(DateHistogramInterval.YEAR, DateHistogramInterval.QUARTER,
-                        DateHistogramInterval.MONTH, DateHistogramInterval.WEEK, DateHistogramInterval.DAY, DateHistogramInterval.HOUR,
-                        DateHistogramInterval.MINUTE, DateHistogramInterval.SECOND));
-            } else {
-                int branch = randomInt(4);
-                switch (branch) {
-                case 0:
-                    factory.dateHistogramInterval(DateHistogramInterval.seconds(randomIntBetween(1, 1000)));
-                    break;
-                case 1:
-                    factory.dateHistogramInterval(DateHistogramInterval.minutes(randomIntBetween(1, 1000)));
-                    break;
-                case 2:
-                    factory.dateHistogramInterval(DateHistogramInterval.hours(randomIntBetween(1, 1000)));
-                    break;
-                case 3:
-                    factory.dateHistogramInterval(DateHistogramInterval.days(randomIntBetween(1, 1000)));
-                    break;
-                case 4:
-                    factory.dateHistogramInterval(DateHistogramInterval.weeks(randomIntBetween(1, 1000)));
-                    break;
-                default:
-                    throw new IllegalStateException("invalid branch: " + branch);
-                }
-            }
-        }
-        if (randomBoolean()) {
-            long extendedBoundsMin = randomIntBetween(-100000, 100000);
-            long extendedBoundsMax = randomIntBetween((int) extendedBoundsMin, 200000);
-            factory.extendedBounds(new ExtendedBounds(extendedBoundsMin, extendedBoundsMax));
-        }
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.minDocCount(randomIntBetween(0, 100));
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        if (randomBoolean()) {
-            factory.offset(randomIntBetween(0, 100000));
-        }
-        if (randomBoolean()) {
-            int branch = randomInt(5);
-            switch (branch) {
-            case 0:
-                factory.order(Order.COUNT_ASC);
-                break;
-            case 1:
-                factory.order(Order.COUNT_DESC);
-                break;
-            case 2:
-                factory.order(Order.KEY_ASC);
-                break;
-            case 3:
-                factory.order(Order.KEY_DESC);
-                break;
-            case 4:
-                factory.order(Order.aggregation("foo", true));
-                break;
-            case 5:
-                factory.order(Order.aggregation("foo", false));
-                break;
-            }
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeTests.java
deleted file mode 100644
index dc1bf43..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeTests.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.bucket.range.date.DateRangeAggregatorFactory;
-
-public class DateRangeTests extends BaseAggregationTestCase<DateRangeAggregatorFactory> {
-
-    @Override
-    protected DateRangeAggregatorFactory createTestAggregatorFactory() {
-        int numRanges = randomIntBetween(1, 10);
-        DateRangeAggregatorFactory factory = new DateRangeAggregatorFactory("foo");
-        for (int i = 0; i < numRanges; i++) {
-            String key = null;
-            if (randomBoolean()) {
-                key = randomAsciiOfLengthBetween(1, 20);
-            }
-            double from = randomBoolean() ? Double.NEGATIVE_INFINITY : randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE - 1000);
-            double to = randomBoolean() ? Double.POSITIVE_INFINITY
-                    : (Double.isInfinite(from) ? randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE)
-                            : randomIntBetween((int) from, Integer.MAX_VALUE));
-            if (randomBoolean()) {
-                factory.addRange(new Range(key, from, to));
-            } else {
-                String fromAsStr = Double.isInfinite(from) ? null : String.valueOf(from);
-                String toAsStr = Double.isInfinite(to) ? null : String.valueOf(to);
-                factory.addRange(new Range(key, fromAsStr, toAsStr));
-            }
-        }
-        factory.field(INT_FIELD_NAME);
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerIT.java
deleted file mode 100644
index f7cbfa7..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerIT.java
+++ /dev/null
@@ -1,238 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.action.admin.indices.refresh.RefreshRequest;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchType;
-import org.elasticsearch.index.query.TermQueryBuilder;
-import org.elasticsearch.search.aggregations.bucket.sampler.Sampler;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator.DiversifiedFactory;
-import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory;
-import org.elasticsearch.search.aggregations.metrics.max.Max;
-import org.elasticsearch.test.ESIntegTestCase;
-
-import java.util.Collection;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.max;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.sampler;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.greaterThanOrEqualTo;
-import static org.hamcrest.Matchers.lessThanOrEqualTo;
-
-/**
- * Tests the Sampler aggregation
- */
-@ESIntegTestCase.SuiteScopeTestCase
-public class DiversifiedSamplerIT extends ESIntegTestCase {
-
-    public static final int NUM_SHARDS = 2;
-
-    public String randomExecutionHint() {
-        return randomBoolean() ? null : randomFrom(SamplerAggregator.ExecutionMode.values()).toString();
-    }
-
-
-    @Override
-    public void setupSuiteScopeCluster() throws Exception {
-        assertAcked(prepareCreate("test").setSettings(SETTING_NUMBER_OF_SHARDS, NUM_SHARDS, SETTING_NUMBER_OF_REPLICAS, 0).addMapping(
-                "book", "author", "type=string,index=not_analyzed", "name", "type=string,index=analyzed", "genre",
-                "type=string,index=not_analyzed", "price", "type=float"));
-        createIndex("idx_unmapped");
-        // idx_unmapped_author is same as main index but missing author field
-        assertAcked(prepareCreate("idx_unmapped_author").setSettings(SETTING_NUMBER_OF_SHARDS, NUM_SHARDS, SETTING_NUMBER_OF_REPLICAS, 0)
-                .addMapping("book", "name", "type=string,index=analyzed", "genre", "type=string,index=not_analyzed", "price", "type=float"));
-
-        ensureGreen();
-        String data[] = {
-                // "id,cat,name,price,inStock,author_t,series_t,sequence_i,genre_s",
-                "0553573403,book,A Game of Thrones,7.99,true,George R.R. Martin,A Song of Ice and Fire,1,fantasy",
-                "0553579908,book,A Clash of Kings,7.99,true,George R.R. Martin,A Song of Ice and Fire,2,fantasy",
-                "055357342X,book,A Storm of Swords,7.99,true,George R.R. Martin,A Song of Ice and Fire,3,fantasy",
-                "0553293354,book,Foundation,17.99,true,Isaac Asimov,Foundation Novels,1,scifi",
-                "0812521390,book,The Black Company,6.99,false,Glen Cook,The Chronicles of The Black Company,1,fantasy",
-                "0812550706,book,Ender's Game,6.99,true,Orson Scott Card,Ender,1,scifi",
-                "0441385532,book,Jhereg,7.95,false,Steven Brust,Vlad Taltos,1,fantasy",
-                "0380014300,book,Nine Princes In Amber,6.99,true,Roger Zelazny,the Chronicles of Amber,1,fantasy",
-                "0805080481,book,The Book of Three,5.99,true,Lloyd Alexander,The Chronicles of Prydain,1,fantasy",
-                "080508049X,book,The Black Cauldron,5.99,true,Lloyd Alexander,The Chronicles of Prydain,2,fantasy"
-
-            };
-
-        for (int i = 0; i < data.length; i++) {
-            String[] parts = data[i].split(",");
-            client().prepareIndex("test", "book", "" + i).setSource("author", parts[5], "name", parts[2], "genre", parts[8], "price",Float.parseFloat(parts[3])).get();
-            client().prepareIndex("idx_unmapped_author", "book", "" + i).setSource("name", parts[2], "genre", parts[8],"price",Float.parseFloat(parts[3])).get();
-        }
-        client().admin().indices().refresh(new RefreshRequest("test")).get();
-    }
-
-    public void testIssue10719() throws Exception {
-        // Tests that we can refer to nested elements under a sample in a path
-        // statement
-        boolean asc = randomBoolean();
-        SearchResponse response = client().prepareSearch("test").setTypes("book").setSearchType(SearchType.QUERY_AND_FETCH)
-                .addAggregation(terms("genres")
-                        .field("genre")
-                        .order(Terms.Order.aggregation("sample>max_price.value", asc))
-                        .subAggregation(sampler("sample").shardSize(100)
-                                .subAggregation(max("max_price").field("price")))
-                ).execute().actionGet();
-        assertSearchResponse(response);
-        Terms genres = response.getAggregations().get("genres");
-        Collection<Bucket> genreBuckets = genres.getBuckets();
-        // For this test to be useful we need >1 genre bucket to compare
-        assertThat(genreBuckets.size(), greaterThan(1));
-        double lastMaxPrice = asc ? Double.MIN_VALUE : Double.MAX_VALUE;
-        for (Terms.Bucket genreBucket : genres.getBuckets()) {
-            Sampler sample = genreBucket.getAggregations().get("sample");
-            Max maxPriceInGenre = sample.getAggregations().get("max_price");
-            double price = maxPriceInGenre.getValue();
-            if (asc) {
-                assertThat(price, greaterThanOrEqualTo(lastMaxPrice));
-            } else {
-                assertThat(price, lessThanOrEqualTo(lastMaxPrice));
-            }
-            lastMaxPrice = price;
-        }
-
-    }
-
-    public void testSimpleDiversity() throws Exception {
-        int MAX_DOCS_PER_AUTHOR = 1;
-        DiversifiedFactory sampleAgg = new DiversifiedFactory("sample").shardSize(100);
-        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
-        sampleAgg.subAggregation(terms("authors").field("author"));
-        SearchResponse response = client().prepareSearch("test")
-                .setSearchType(SearchType.QUERY_AND_FETCH)
-                .setQuery(new TermQueryBuilder("genre", "fantasy"))
-                .setFrom(0).setSize(60)
-                .addAggregation(sampleAgg)
-                .execute()
-                .actionGet();
-        assertSearchResponse(response);
-        Sampler sample = response.getAggregations().get("sample");
-        Terms authors = sample.getAggregations().get("authors");
-        Collection<Bucket> testBuckets = authors.getBuckets();
-
-        for (Terms.Bucket testBucket : testBuckets) {
-            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
-        }
-    }
-
-    public void testNestedDiversity() throws Exception {
-        // Test multiple samples gathered under buckets made by a parent agg
-        int MAX_DOCS_PER_AUTHOR = 1;
-        TermsAggregatorFactory rootTerms = terms("genres").field("genre");
-
-        DiversifiedFactory sampleAgg = new DiversifiedFactory("sample").shardSize(100);
-        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
-        sampleAgg.subAggregation(terms("authors").field("author"));
-
-        rootTerms.subAggregation(sampleAgg);
-        SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH)
-                .addAggregation(rootTerms).execute().actionGet();
-        assertSearchResponse(response);
-        Terms genres = response.getAggregations().get("genres");
-        Collection<Bucket> genreBuckets = genres.getBuckets();
-        for (Terms.Bucket genreBucket : genreBuckets) {
-            Sampler sample = genreBucket.getAggregations().get("sample");
-            Terms authors = sample.getAggregations().get("authors");
-            Collection<Bucket> testBuckets = authors.getBuckets();
-
-            for (Terms.Bucket testBucket : testBuckets) {
-                assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
-            }
-        }
-    }
-
-    public void testNestedSamples() throws Exception {
-        // Test samples nested under samples
-        int MAX_DOCS_PER_AUTHOR = 1;
-        int MAX_DOCS_PER_GENRE = 2;
-        DiversifiedFactory rootSample = new DiversifiedFactory("genreSample").shardSize(100)
-                .field("genre")
-                .maxDocsPerValue(MAX_DOCS_PER_GENRE);
-
-        DiversifiedFactory sampleAgg = new DiversifiedFactory("sample").shardSize(100);
-        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
-        sampleAgg.subAggregation(terms("authors").field("author"));
-        sampleAgg.subAggregation(terms("genres").field("genre"));
-
-        rootSample.subAggregation(sampleAgg);
-        SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH).addAggregation(rootSample)
-                .execute().actionGet();
-        assertSearchResponse(response);
-        Sampler genreSample = response.getAggregations().get("genreSample");
-        Sampler sample = genreSample.getAggregations().get("sample");
-
-        Terms genres = sample.getAggregations().get("genres");
-        Collection<Bucket> testBuckets = genres.getBuckets();
-        for (Terms.Bucket testBucket : testBuckets) {
-            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_GENRE));
-        }
-
-        Terms authors = sample.getAggregations().get("authors");
-        testBuckets = authors.getBuckets();
-        for (Terms.Bucket testBucket : testBuckets) {
-            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
-        }
-    }
-
-    public void testPartiallyUnmappedDiversifyField() throws Exception {
-        // One of the indexes is missing the "author" field used for
-        // diversifying results
-        DiversifiedFactory sampleAgg = new DiversifiedFactory("sample").shardSize(100).field("author")
-                .maxDocsPerValue(1);
-        sampleAgg.subAggregation(terms("authors").field("author"));
-        SearchResponse response = client().prepareSearch("idx_unmapped_author", "test").setSearchType(SearchType.QUERY_AND_FETCH)
-                .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg)
-                .execute().actionGet();
-        assertSearchResponse(response);
-        Sampler sample = response.getAggregations().get("sample");
-        assertThat(sample.getDocCount(), greaterThan(0l));
-        Terms authors = sample.getAggregations().get("authors");
-        assertThat(authors.getBuckets().size(), greaterThan(0));
-    }
-
-    public void testWhollyUnmappedDiversifyField() throws Exception {
-        //All of the indices are missing the "author" field used for diversifying results
-        int MAX_DOCS_PER_AUTHOR = 1;
-        DiversifiedFactory sampleAgg = new DiversifiedFactory("sample").shardSize(100);
-        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
-        sampleAgg.subAggregation(terms("authors").field("author"));
-        SearchResponse response = client().prepareSearch("idx_unmapped", "idx_unmapped_author").setSearchType(SearchType.QUERY_AND_FETCH)
-                .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg).execute().actionGet();
-        assertSearchResponse(response);
-        Sampler sample = response.getAggregations().get("sample");
-        assertThat(sample.getDocCount(), equalTo(0l));
-        Terms authors = sample.getAggregations().get("authors");
-        assertNull(authors);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerTests.java
deleted file mode 100644
index c49ad73..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerTests.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator.ExecutionMode;
-
-public class DiversifiedSamplerTests extends BaseAggregationTestCase<SamplerAggregator.DiversifiedFactory> {
-
-    @Override
-    protected final SamplerAggregator.DiversifiedFactory createTestAggregatorFactory() {
-        SamplerAggregator.DiversifiedFactory factory = new SamplerAggregator.DiversifiedFactory("foo");
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            factory.maxDocsPerValue(randomIntBetween(1, 1000));
-        }
-        if (randomBoolean()) {
-            factory.shardSize(randomIntBetween(1, 1000));
-        }
-        if (randomBoolean()) {
-            factory.executionHint(randomFrom(ExecutionMode.values()).toString());
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FilterIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FilterIT.java
index be3dd83..6e97a33 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FilterIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FilterIT.java
@@ -93,7 +93,7 @@ public class FilterIT extends ESIntegTestCase {
 
     public void testSimple() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filter("tag1", termQuery("tag", "tag1")))
+                .addAggregation(filter("tag1").filter(termQuery("tag", "tag1")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -109,7 +109,7 @@ public class FilterIT extends ESIntegTestCase {
     // https://github.com/elasticsearch/elasticsearch/issues/8438
     public void testEmptyFilterDeclarations() throws Exception {
         QueryBuilder emptyFilter = new BoolQueryBuilder();
-        SearchResponse response = client().prepareSearch("idx").addAggregation(filter("tag1", emptyFilter)).execute().actionGet();
+        SearchResponse response = client().prepareSearch("idx").addAggregation(filter("tag1").filter(emptyFilter)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -120,7 +120,8 @@ public class FilterIT extends ESIntegTestCase {
 
     public void testWithSubAggregation() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filter("tag1", termQuery("tag", "tag1"))
+                .addAggregation(filter("tag1")
+                        .filter(termQuery("tag", "tag1"))
                         .subAggregation(avg("avg_value").field("value")))
                 .execute().actionGet();
 
@@ -149,7 +150,7 @@ public class FilterIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(
                         histogram("histo").field("value").interval(2L).subAggregation(
-                                filter("filter", matchAllQuery()))).get();
+                                filter("filter").filter(matchAllQuery()))).get();
 
         assertSearchResponse(response);
 
@@ -167,7 +168,8 @@ public class FilterIT extends ESIntegTestCase {
     public void testWithContextBasedSubAggregation() throws Exception {
         try {
             client().prepareSearch("idx")
-                    .addAggregation(filter("tag1", termQuery("tag", "tag1"))
+                    .addAggregation(filter("tag1")
+                            .filter(termQuery("tag", "tag1"))
                             .subAggregation(avg("avg_value")))
                     .execute().actionGet();
 
@@ -183,7 +185,7 @@ public class FilterIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1l).minDocCount(0)
-                        .subAggregation(filter("filter", matchAllQuery())))
+                        .subAggregation(filter("filter").filter(matchAllQuery())))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2l));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java
index c84c5f5..2235b00 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java
@@ -26,7 +26,6 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.BoolQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.bucket.filters.Filters;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.KeyedFilter;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -110,8 +109,11 @@ public class FiltersIT extends ESIntegTestCase {
     }
 
     public void testSimple() throws Exception {
-        SearchResponse response = client().prepareSearch("idx").addAggregation(
-                filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")), new KeyedFilter("tag2", termQuery("tag", "tag2"))))
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(
+                        filters("tags")
+                                .filter("tag1", termQuery("tag", "tag1"))
+                                .filter("tag2", termQuery("tag", "tag2")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -134,10 +136,10 @@ public class FiltersIT extends ESIntegTestCase {
     // See NullPointer issue when filters are empty:
     // https://github.com/elasticsearch/elasticsearch/issues/8438
     public void testEmptyFilterDeclarations() throws Exception {
-        QueryBuilder<?> emptyFilter = new BoolQueryBuilder();
+        QueryBuilder emptyFilter = new BoolQueryBuilder();
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", new KeyedFilter("all", emptyFilter), new KeyedFilter("tag1", termQuery("tag", "tag1"))))
-                .execute().actionGet();
+                .addAggregation(filters("tags").filter("all", emptyFilter).filter("tag1", termQuery("tag", "tag1"))).execute()
+                .actionGet();
 
         assertSearchResponse(response);
 
@@ -153,8 +155,11 @@ public class FiltersIT extends ESIntegTestCase {
 
     public void testWithSubAggregation() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")),
-                        new KeyedFilter("tag2", termQuery("tag", "tag2"))).subAggregation(avg("avg_value").field("value")))
+                .addAggregation(
+                        filters("tags")
+                                .filter("tag1", termQuery("tag", "tag1"))
+                                .filter("tag2", termQuery("tag", "tag2"))
+                                .subAggregation(avg("avg_value").field("value")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -205,7 +210,7 @@ public class FiltersIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(
                         histogram("histo").field("value").interval(2L).subAggregation(
-                                filters("filters", matchAllQuery()))).get();
+                                filters("filters").filter(matchAllQuery()))).get();
 
         assertSearchResponse(response);
 
@@ -227,7 +232,9 @@ public class FiltersIT extends ESIntegTestCase {
         try {
             client().prepareSearch("idx")
                     .addAggregation(
-                    filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")), new KeyedFilter("tag2", termQuery("tag", "tag2")))
+                            filters("tags")
+                                    .filter("tag1", termQuery("tag", "tag1"))
+                                    .filter("tag2", termQuery("tag", "tag2"))
                                     .subAggregation(avg("avg_value"))
                     )
                     .execute().actionGet();
@@ -244,7 +251,7 @@ public class FiltersIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1l).minDocCount(0)
-                        .subAggregation(filters("filters", new KeyedFilter("all", matchAllQuery()))))
+                        .subAggregation(filters("filters").filter("all", matchAllQuery())))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2l));
@@ -263,7 +270,11 @@ public class FiltersIT extends ESIntegTestCase {
 
     public void testSimpleNonKeyed() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", termQuery("tag", "tag1"), termQuery("tag", "tag2"))).execute().actionGet();
+                .addAggregation(
+                        filters("tags")
+                                .filter(termQuery("tag", "tag1"))
+                                .filter(termQuery("tag", "tag2")))
+                .execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -286,9 +297,12 @@ public class FiltersIT extends ESIntegTestCase {
     }
 
     public void testOtherBucket() throws Exception {
-        SearchResponse response = client().prepareSearch("idx").addAggregation(
-                filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")), new KeyedFilter("tag2", termQuery("tag", "tag2")))
-                        .otherBucket(true))
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        filters("tags").otherBucket(true)
+                        .filter("tag1", termQuery("tag", "tag1"))
+                        .filter("tag2", termQuery("tag", "tag2")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -313,9 +327,12 @@ public class FiltersIT extends ESIntegTestCase {
     }
 
     public void testOtherNamedBucket() throws Exception {
-        SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")),
-                        new KeyedFilter("tag2", termQuery("tag", "tag2"))).otherBucket(true).otherBucketKey("foobar"))
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        filters("tags").otherBucketKey("foobar")
+                        .filter("tag1", termQuery("tag", "tag1"))
+                        .filter("tag2", termQuery("tag", "tag2")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -341,8 +358,11 @@ public class FiltersIT extends ESIntegTestCase {
 
     public void testOtherNonKeyed() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", termQuery("tag", "tag1"), termQuery("tag", "tag2")).otherBucket(true)).execute()
-                .actionGet();
+                .addAggregation(
+                        filters("tags").otherBucket(true)
+                                .filter(termQuery("tag", "tag1"))
+                                .filter(termQuery("tag", "tag2")))
+                .execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -370,8 +390,10 @@ public class FiltersIT extends ESIntegTestCase {
 
     public void testOtherWithSubAggregation() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")),
-                        new KeyedFilter("tag2", termQuery("tag", "tag2"))).otherBucket(true)
+                .addAggregation(
+                        filters("tags").otherBucket(true)
+                                .filter("tag1", termQuery("tag", "tag1"))
+                                .filter("tag2", termQuery("tag", "tag2"))
                                 .subAggregation(avg("avg_value").field("value")))
                 .execute().actionGet();
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceIT.java
index f6ebbbc..57c4c8b 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceIT.java
@@ -22,7 +22,6 @@ import org.elasticsearch.Version;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -141,9 +140,10 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
     public void testSimple() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo(500)
                         .addRange(500, 1000)
                         .addUnboundedFrom(1000))
@@ -188,9 +188,10 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
     public void testSimpleWithCustomKeys() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo("ring1", 500)
                         .addRange("ring2", 500, 1000)
                         .addUnboundedFrom("ring3", 1000))
@@ -237,9 +238,10 @@ public class GeoDistanceIT extends ESIntegTestCase {
         client().admin().cluster().prepareHealth("idx_unmapped").setWaitForYellowStatus().execute().actionGet();
 
         SearchResponse response = client().prepareSearch("idx_unmapped")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo(500)
                         .addRange(500, 1000)
                         .addUnboundedFrom(1000))
@@ -284,9 +286,10 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
     public void testPartiallyUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx", "idx_unmapped")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo(500)
                         .addRange(500, 1000)
                         .addUnboundedFrom(1000))
@@ -331,9 +334,10 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
     public void testWithSubAggregation() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo(500)
                         .addRange(500, 1000)
                         .addUnboundedFrom(1000)
@@ -418,7 +422,7 @@ public class GeoDistanceIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1l).minDocCount(0)
-                        .subAggregation(geoDistance("geo_dist", new GeoPoint(52.3760, 4.894)).field("location").addRange("0-100", 0.0, 100.0)))
+                        .subAggregation(geoDistance("geo_dist").field("location").point("52.3760, 4.894").addRange("0-100", 0.0, 100.0)))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2l));
@@ -443,10 +447,11 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
     public void testMultiValues() throws Exception {
         SearchResponse response = client().prepareSearch("idx-multi")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
                         .distanceType(org.elasticsearch.common.geo.GeoDistance.ARC)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo(500)
                         .addRange(500, 1000)
                         .addUnboundedFrom(1000))
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceRangeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceRangeTests.java
deleted file mode 100644
index a3ac9c4..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceRangeTests.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.unit.DistanceUnit;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanceParser.GeoDistanceFactory;
-import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanceParser.Range;
-import org.elasticsearch.test.geo.RandomShapeGenerator;
-
-public class GeoDistanceRangeTests extends BaseAggregationTestCase<GeoDistanceFactory> {
-
-    @Override
-    protected GeoDistanceFactory createTestAggregatorFactory() {
-        int numRanges = randomIntBetween(1, 10);
-        GeoPoint origin = RandomShapeGenerator.randomPoint(getRandom());
-        GeoDistanceFactory factory = new GeoDistanceFactory("foo", origin);
-        for (int i = 0; i < numRanges; i++) {
-            String key = null;
-            if (randomBoolean()) {
-                key = randomAsciiOfLengthBetween(1, 20);
-            }
-            double from = randomBoolean() ? 0 : randomIntBetween(0, Integer.MAX_VALUE - 1000);
-            double to = randomBoolean() ? Double.POSITIVE_INFINITY
-                    : (Double.compare(from, 0) == 0 ? randomIntBetween(0, Integer.MAX_VALUE)
-                            : randomIntBetween((int) from, Integer.MAX_VALUE));
-            factory.addRange(new Range(key, from, to));
-        }
-        factory.field(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing("0, 0");
-        }
-        if (randomBoolean()) {
-            factory.unit(randomFrom(DistanceUnit.values()));
-        }
-        if (randomBoolean()) {
-            factory.distanceType(randomFrom(GeoDistance.values()));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java
index c8658cc..672447a 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java
@@ -204,7 +204,7 @@ public class GeoHashGridIT extends ESIntegTestCase {
         for (int precision = 1; precision <= GeoHashUtils.PRECISION; precision++) {
             SearchResponse response = client().prepareSearch("idx")
                     .addAggregation(
-                            AggregationBuilders.filter("filtered", bbox)
+                            AggregationBuilders.filter("filtered").filter(bbox)
                                     .subAggregation(
                                             geohashGrid("geohashgrid")
                                                     .field("location")
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java
deleted file mode 100644
index 8836ece..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridParser.GeoGridFactory;
-
-public class GeoHashGridTests extends BaseAggregationTestCase<GeoGridFactory> {
-
-    @Override
-    protected GeoGridFactory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        GeoGridFactory factory = new GeoGridFactory(name);
-        if (randomBoolean()) {
-            int precision = randomIntBetween(1, 12);
-            factory.precision(precision);
-        }
-        if (randomBoolean()) {
-            int size = randomInt(5);
-            switch (size) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                size = randomInt();
-                break;
-            }
-            factory.size(size);
-
-        }
-        if (randomBoolean()) {
-            int shardSize = randomInt(5);
-            switch (shardSize) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardSize = randomInt();
-                break;
-            }
-            factory.shardSize(shardSize);
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GlobalTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GlobalTests.java
deleted file mode 100644
index 6529e0b..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GlobalTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.global.GlobalAggregator;
-import org.elasticsearch.search.aggregations.bucket.global.GlobalAggregator.Factory;
-
-public class GlobalTests extends BaseAggregationTestCase<GlobalAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        return new GlobalAggregator.Factory(randomAsciiOfLengthBetween(3, 20));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java
deleted file mode 100644
index ed0b17b..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
-import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Order;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.Factory;
-
-public class HistogramTests extends BaseAggregationTestCase<HistogramAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        Factory factory = new Factory("foo");
-        factory.field(INT_FIELD_NAME);
-        factory.interval(randomIntBetween(1, 100000));
-        if (randomBoolean()) {
-            long extendedBoundsMin = randomIntBetween(-100000, 100000);
-            long extendedBoundsMax = randomIntBetween((int) extendedBoundsMin, 200000);
-            factory.extendedBounds(new ExtendedBounds(extendedBoundsMin, extendedBoundsMax));
-        }
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.minDocCount(randomIntBetween(0, 100));
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        if (randomBoolean()) {
-            factory.offset(randomIntBetween(0, 100000));
-        }
-        if (randomBoolean()) {
-            int branch = randomInt(5);
-            switch (branch) {
-            case 0:
-                factory.order(Order.COUNT_ASC);
-                break;
-            case 1:
-                factory.order(Order.COUNT_DESC);
-                break;
-            case 2:
-                factory.order(Order.KEY_ASC);
-                break;
-            case 3:
-                factory.order(Order.KEY_DESC);
-                break;
-            case 4:
-                factory.order(Order.aggregation("foo", true));
-                break;
-            case 5:
-                factory.order(Order.aggregation("foo", false));
-                break;
-            }
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java
deleted file mode 100644
index 54380a0..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.common.network.Cidrs;
-import org.elasticsearch.index.mapper.ip.IpFieldMapper;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.range.ipv4.IPv4RangeAggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.range.ipv4.IPv4RangeAggregatorFactory.Range;
-
-public class IPv4RangeTests extends BaseAggregationTestCase<IPv4RangeAggregatorFactory> {
-
-    @Override
-    protected IPv4RangeAggregatorFactory createTestAggregatorFactory() {
-        int numRanges = randomIntBetween(1, 10);
-        IPv4RangeAggregatorFactory factory = new IPv4RangeAggregatorFactory("foo");
-        for (int i = 0; i < numRanges; i++) {
-            String key = null;
-            if (randomBoolean()) {
-                key = randomAsciiOfLengthBetween(1, 20);
-            }
-            if (randomBoolean()) {
-                double from = randomBoolean() ? Double.NEGATIVE_INFINITY : randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE - 1000);
-                double to = randomBoolean() ? Double.POSITIVE_INFINITY
-                        : (Double.isInfinite(from) ? randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE)
-                                : randomIntBetween((int) from, Integer.MAX_VALUE));
-                if (randomBoolean()) {
-                    factory.addRange(new Range(key, from, to));
-                } else {
-                    String fromAsStr = Double.isInfinite(from) ? null : IpFieldMapper.longToIp((long) from);
-                    String toAsStr = Double.isInfinite(to) ? null : IpFieldMapper.longToIp((long) to);
-                    factory.addRange(new Range(key, fromAsStr, toAsStr));
-                }
-            } else {
-                int mask = randomInt(32);
-                long ipAsLong = randomIntBetween(0, Integer.MAX_VALUE);
-
-                long blockSize = 1L << (32 - mask);
-                ipAsLong = ipAsLong - (ipAsLong & (blockSize - 1));
-                String cidr = Cidrs.createCIDR(ipAsLong, mask);
-                factory.addRange(new Range(key, cidr));
-            }
-        }
-        factory.field(INT_FIELD_NAME);
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NaNSortingIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NaNSortingIT.java
index 465dbce..bfd6837 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NaNSortingIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NaNSortingIT.java
@@ -26,12 +26,9 @@ import org.elasticsearch.search.aggregations.Aggregation;
 import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.metrics.MetricsAggregationBuilder;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
-import org.elasticsearch.search.aggregations.metrics.avg.AvgAggregator;
 import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStats;
-import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStatsAggregator;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.test.ESIntegTestCase;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
@@ -48,10 +45,8 @@ public class NaNSortingIT extends ESIntegTestCase {
     private enum SubAggregation {
         AVG("avg") {
             @Override
-            public AvgAggregator.Factory builder() {
-                AvgAggregator.Factory factory = avg(name);
-                factory.field("numeric_field");
-                return factory;
+            public MetricsAggregationBuilder<?> builder() {
+                return avg(name).field("numeric_field");
             }
             @Override
             public double getValue(Aggregation aggregation) {
@@ -60,10 +55,8 @@ public class NaNSortingIT extends ESIntegTestCase {
         },
         VARIANCE("variance") {
             @Override
-            public ExtendedStatsAggregator.Factory builder() {
-                ExtendedStatsAggregator.Factory factory = extendedStats(name);
-                factory.field("numeric_field");
-                return factory;
+            public MetricsAggregationBuilder<?> builder() {
+                return extendedStats(name).field("numeric_field");
             }
             @Override
             public String sortKey() {
@@ -76,10 +69,8 @@ public class NaNSortingIT extends ESIntegTestCase {
         },
         STD_DEVIATION("std_deviation"){
             @Override
-            public ExtendedStatsAggregator.Factory builder() {
-                ExtendedStatsAggregator.Factory factory = extendedStats(name);
-                factory.field("numeric_field");
-                return factory;
+            public MetricsAggregationBuilder<?> builder() {
+                return extendedStats(name).field("numeric_field");
             }
             @Override
             public String sortKey() {
@@ -97,7 +88,7 @@ public class NaNSortingIT extends ESIntegTestCase {
 
         public String name;
 
-        public abstract ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, ? extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, ?>> builder();
+        public abstract MetricsAggregationBuilder<?> builder();
 
         public String sortKey() {
             return name;
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NestedIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NestedIT.java
index 12d6b85..349b61f 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NestedIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NestedIT.java
@@ -167,7 +167,7 @@ public class NestedIT extends ESIntegTestCase {
 
     public void testSimple() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(nested("nested", "nested")
+                .addAggregation(nested("nested").path("nested")
                         .subAggregation(stats("nested_value_stats").field("nested.value")))
                 .execute().actionGet();
 
@@ -205,7 +205,7 @@ public class NestedIT extends ESIntegTestCase {
 
     public void testNonExistingNestedField() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("idx")
-                .addAggregation(nested("nested", "value")
+                .addAggregation(nested("nested").path("value")
                         .subAggregation(stats("nested_value_stats").field("nested.value")))
                 .execute().actionGet();
 
@@ -217,7 +217,7 @@ public class NestedIT extends ESIntegTestCase {
 
     public void testNestedWithSubTermsAgg() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(nested("nested", "nested")
+                .addAggregation(nested("nested").path("nested")
                         .subAggregation(terms("values").field("nested.value").size(100)
                                 .collectMode(aggCollectionMode)))
                 .execute().actionGet();
@@ -270,7 +270,7 @@ public class NestedIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(terms("top_values").field("value").size(100)
                         .collectMode(aggCollectionMode)
-                        .subAggregation(nested("nested", "nested")
+                        .subAggregation(nested("nested").path("nested")
                                 .subAggregation(max("max_value").field("nested.value"))))
                 .execute().actionGet();
 
@@ -296,10 +296,10 @@ public class NestedIT extends ESIntegTestCase {
 
     public void testNestNestedAggs() throws Exception {
         SearchResponse response = client().prepareSearch("idx_nested_nested_aggs")
-                .addAggregation(nested("level1", "nested1")
+                .addAggregation(nested("level1").path("nested1")
                         .subAggregation(terms("a").field("nested1.a")
                                 .collectMode(aggCollectionMode)
-                        .subAggregation(nested("level2", "nested1.nested2")
+                                .subAggregation(nested("level2").path("nested1.nested2")
                                         .subAggregation(sum("sum").field("nested1.nested2.b")))))
                 .get();
         assertSearchResponse(response);
@@ -333,7 +333,7 @@ public class NestedIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1l).minDocCount(0)
-                        .subAggregation(nested("nested", "nested")))
+                        .subAggregation(nested("nested").path("nested")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2l));
@@ -352,7 +352,7 @@ public class NestedIT extends ESIntegTestCase {
         try {
             client().prepareSearch("idx")
                     .setQuery(matchAllQuery())
-                    .addAggregation(nested("object_field", "incorrect"))
+                    .addAggregation(nested("object_field").path("incorrect"))
                     .execute().actionGet();
             fail();
         } catch (SearchPhaseExecutionException e) {
@@ -407,10 +407,9 @@ public class NestedIT extends ESIntegTestCase {
                         terms("startDate").field("dates.month.start").subAggregation(
                                 terms("endDate").field("dates.month.end").subAggregation(
                                         terms("period").field("dates.month.label").subAggregation(
-                                                nested("ctxt_idfier_nested", "comments")
-                                                .subAggregation(filter("comment_filter", termQuery("comments.identifier", "29111"))
-                                                        .subAggregation(nested("nested_tags", "comments.tags")
-                                                                .subAggregation(
+                                                nested("ctxt_idfier_nested").path("comments").subAggregation(
+                                                        filter("comment_filter").filter(termQuery("comments.identifier", "29111")).subAggregation(
+                                                                nested("nested_tags").path("comments.tags").subAggregation(
                                                                         terms("tag").field("comments.tags.name")
                                                                 )
                                                         )
@@ -489,7 +488,7 @@ public class NestedIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("idx4").setTypes("product")
                 .addAggregation(terms("category").field("categories").subAggregation(
-                        nested("property", "property").subAggregation(
+                        nested("property").path("property").subAggregation(
                                 terms("property_id").field("property.id")
                         )
                 ))
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java
deleted file mode 100644
index d7b652f..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Factory;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-
-public class RangeTests extends BaseAggregationTestCase<RangeAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        int numRanges = randomIntBetween(1, 10);
-        Factory factory = new Factory("foo");
-        for (int i = 0; i < numRanges; i++) {
-            String key = null;
-            if (randomBoolean()) {
-                key = randomAsciiOfLengthBetween(1, 20);
-            }
-            double from = randomBoolean() ? Double.NEGATIVE_INFINITY : randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE - 1000);
-            double to = randomBoolean() ? Double.POSITIVE_INFINITY
-                    : (Double.isInfinite(from) ? randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE)
-                            : randomIntBetween((int) from, Integer.MAX_VALUE));
-            if (randomBoolean()) {
-                factory.addRange(new Range(key, from, to));
-            } else {
-                String fromAsStr = Double.isInfinite(from) ? null : String.valueOf(from);
-                String toAsStr = Double.isInfinite(to) ? null : String.valueOf(to);
-                factory.addRange(new Range(key, fromAsStr, toAsStr));
-            }
-        }
-        factory.field(INT_FIELD_NAME);
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java
index 438f9b4..8800063 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java
@@ -138,7 +138,7 @@ public class ReverseNestedIT extends ESIntegTestCase {
 
     public void testSimpleReverseNestedToRoot() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type1")
-                .addAggregation(nested("nested1", "nested1")
+                .addAggregation(nested("nested1").path("nested1")
                         .subAggregation(
                                 terms("field2").field("nested1.field2")
                                         .subAggregation(
@@ -326,10 +326,10 @@ public class ReverseNestedIT extends ESIntegTestCase {
 
     public void testSimpleNested1ToRootToNested2() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type2")
-                .addAggregation(nested("nested1", "nested1")
+                .addAggregation(nested("nested1").path("nested1")
                                 .subAggregation(
                                         reverseNested("nested1_to_root")
-                                                .subAggregation(nested("root_to_nested2", "nested1.nested2"))
+                                                .subAggregation(nested("root_to_nested2").path("nested1.nested2"))
                                         )
                                 )
                 .get();
@@ -348,7 +348,7 @@ public class ReverseNestedIT extends ESIntegTestCase {
 
     public void testSimpleReverseNestedToNested1() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type2")
-                .addAggregation(nested("nested1", "nested1.nested2")
+                .addAggregation(nested("nested1").path("nested1.nested2")
                                 .subAggregation(
                                         terms("field2").field("nested1.nested2.field2").order(Terms.Order.term(true))
                                                 .collectMode(randomFrom(SubAggCollectionMode.values()))
@@ -470,7 +470,7 @@ public class ReverseNestedIT extends ESIntegTestCase {
     public void testNonExistingNestedField() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(nested("nested2", "nested1.nested2").subAggregation(reverseNested("incorrect").path("nested3")))
+                .addAggregation(nested("nested2").path("nested1.nested2").subAggregation(reverseNested("incorrect").path("nested3")))
                 .execute().actionGet();
 
         Nested nested = searchResponse.getAggregations().get("nested2");
@@ -558,11 +558,11 @@ public class ReverseNestedIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("idx3")
                 .addAggregation(
-                        nested("nested_0", "category").subAggregation(
+                        nested("nested_0").path("category").subAggregation(
                                 terms("group_by_category").field("category.name").subAggregation(
                                         reverseNested("to_root").subAggregation(
-                                                nested("nested_1", "sku").subAggregation(
-                                                        filter("filter_by_sku", termQuery("sku.sku_type", "bar1")).subAggregation(
+                                                nested("nested_1").path("sku").subAggregation(
+                                                        filter("filter_by_sku").filter(termQuery("sku.sku_type", "bar1")).subAggregation(
                                                                 count("sku_count").field("sku.sku_type")
                                                         )
                                                 )
@@ -593,13 +593,13 @@ public class ReverseNestedIT extends ESIntegTestCase {
 
         response = client().prepareSearch("idx3")
                 .addAggregation(
-                        nested("nested_0", "category").subAggregation(
+                        nested("nested_0").path("category").subAggregation(
                                 terms("group_by_category").field("category.name").subAggregation(
                                         reverseNested("to_root").subAggregation(
-                                                nested("nested_1", "sku").subAggregation(
-                                                        filter("filter_by_sku", termQuery("sku.sku_type", "bar1")).subAggregation(
-                                                                nested("nested_2", "sku.colors").subAggregation(
-                                                                        filter("filter_sku_color", termQuery("sku.colors.name", "red")).subAggregation(
+                                                nested("nested_1").path("sku").subAggregation(
+                                                        filter("filter_by_sku").filter(termQuery("sku.sku_type", "bar1")).subAggregation(
+                                                                nested("nested_2").path("sku.colors").subAggregation(
+                                                                        filter("filter_sku_color").filter(termQuery("sku.colors.name", "red")).subAggregation(
                                                                                 reverseNested("reverse_to_sku").path("sku").subAggregation(
                                                                                         count("sku_count").field("sku.sku_type")
                                                                                 )
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerIT.java
index d0ed8bc..2535ca3 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerIT.java
@@ -23,9 +23,11 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.index.query.TermQueryBuilder;
 import org.elasticsearch.search.aggregations.bucket.sampler.Sampler;
+import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
 import org.elasticsearch.test.ESIntegTestCase;
 
@@ -38,8 +40,6 @@ import static org.elasticsearch.search.aggregations.AggregationBuilders.sampler;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.sampler;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.greaterThan;
 import static org.hamcrest.Matchers.greaterThanOrEqualTo;
@@ -123,9 +123,9 @@ public class SamplerIT extends ESIntegTestCase {
 
     }
 
-    public void testSimpleSampler() throws Exception {
-        SamplerAggregator.Factory sampleAgg = sampler("sample").shardSize(100);
-        sampleAgg.subAggregation(terms("authors").field("author"));
+    public void testNoDiversity() throws Exception {
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
         SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg).execute().actionGet();
         assertSearchResponse(response);
@@ -140,9 +140,89 @@ public class SamplerIT extends ESIntegTestCase {
         assertThat(maxBooksPerAuthor, equalTo(3l));
     }
 
+    public void testSimpleDiversity() throws Exception {
+        int MAX_DOCS_PER_AUTHOR = 1;
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+        SearchResponse response = client().prepareSearch("test")
+                .setSearchType(SearchType.QUERY_AND_FETCH)
+                .setQuery(new TermQueryBuilder("genre", "fantasy"))
+                .setFrom(0).setSize(60)
+                .addAggregation(sampleAgg)
+                .execute()
+                .actionGet();
+        assertSearchResponse(response);
+        Sampler sample = response.getAggregations().get("sample");
+        Terms authors = sample.getAggregations().get("authors");
+        Collection<Bucket> testBuckets = authors.getBuckets();
+
+        for (Terms.Bucket testBucket : testBuckets) {
+            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
+        }
+    }
+
+    public void testNestedDiversity() throws Exception {
+        // Test multiple samples gathered under buckets made by a parent agg
+        int MAX_DOCS_PER_AUTHOR = 1;
+        TermsBuilder rootTerms = new TermsBuilder("genres").field("genre");
+
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+
+        rootTerms.subAggregation(sampleAgg);
+        SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH)
+                .addAggregation(rootTerms).execute().actionGet();
+        assertSearchResponse(response);
+        Terms genres = response.getAggregations().get("genres");
+        Collection<Bucket> genreBuckets = genres.getBuckets();
+        for (Terms.Bucket genreBucket : genreBuckets) {
+            Sampler sample = genreBucket.getAggregations().get("sample");
+            Terms authors = sample.getAggregations().get("authors");
+            Collection<Bucket> testBuckets = authors.getBuckets();
+
+            for (Terms.Bucket testBucket : testBuckets) {
+                assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
+            }
+        }
+    }
+
+    public void testNestedSamples() throws Exception {
+        // Test samples nested under samples
+        int MAX_DOCS_PER_AUTHOR = 1;
+        int MAX_DOCS_PER_GENRE = 2;
+        SamplerAggregationBuilder rootSample = new SamplerAggregationBuilder("genreSample").shardSize(100).field("genre")
+                .maxDocsPerValue(MAX_DOCS_PER_GENRE);
+
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+        sampleAgg.subAggregation(new TermsBuilder("genres").field("genre"));
+
+        rootSample.subAggregation(sampleAgg);
+        SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH).addAggregation(rootSample)
+                .execute().actionGet();
+        assertSearchResponse(response);
+        Sampler genreSample = response.getAggregations().get("genreSample");
+        Sampler sample = genreSample.getAggregations().get("sample");
+
+        Terms genres = sample.getAggregations().get("genres");
+        Collection<Bucket> testBuckets = genres.getBuckets();
+        for (Terms.Bucket testBucket : testBuckets) {
+            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_GENRE));
+        }
+
+        Terms authors = sample.getAggregations().get("authors");
+        testBuckets = authors.getBuckets();
+        for (Terms.Bucket testBucket : testBuckets) {
+            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
+        }
+    }
+
     public void testUnmappedChildAggNoDiversity() throws Exception {
-        SamplerAggregator.Factory sampleAgg = sampler("sample").shardSize(100);
-        sampleAgg.subAggregation(terms("authors").field("author"));
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
         SearchResponse response = client().prepareSearch("idx_unmapped")
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("genre", "fantasy"))
@@ -158,8 +238,8 @@ public class SamplerIT extends ESIntegTestCase {
     }
 
     public void testPartiallyUnmappedChildAggNoDiversity() throws Exception {
-        SamplerAggregator.Factory sampleAgg = sampler("sample").shardSize(100);
-        sampleAgg.subAggregation(terms("authors").field("author"));
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
         SearchResponse response = client().prepareSearch("idx_unmapped", "test")
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("genre", "fantasy"))
@@ -174,4 +254,34 @@ public class SamplerIT extends ESIntegTestCase {
         assertThat(authors.getBuckets().size(), greaterThan(0));
     }
 
+    public void testPartiallyUnmappedDiversifyField() throws Exception {
+        // One of the indexes is missing the "author" field used for
+        // diversifying results
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100).field("author").maxDocsPerValue(1);
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+        SearchResponse response = client().prepareSearch("idx_unmapped_author", "test").setSearchType(SearchType.QUERY_AND_FETCH)
+                .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg)
+                .execute().actionGet();
+        assertSearchResponse(response);
+        Sampler sample = response.getAggregations().get("sample");
+        assertThat(sample.getDocCount(), greaterThan(0l));
+        Terms authors = sample.getAggregations().get("authors");
+        assertThat(authors.getBuckets().size(), greaterThan(0));
+    }
+
+    public void testWhollyUnmappedDiversifyField() throws Exception {
+        //All of the indices are missing the "author" field used for diversifying results
+        int MAX_DOCS_PER_AUTHOR = 1;
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+        SearchResponse response = client().prepareSearch("idx_unmapped", "idx_unmapped_author").setSearchType(SearchType.QUERY_AND_FETCH)
+                .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg).execute().actionGet();
+        assertSearchResponse(response);
+        Sampler sample = response.getAggregations().get("sample");
+        assertThat(sample.getDocCount(), equalTo(0l));
+        Terms authors = sample.getAggregations().get("authors");
+        assertNull(authors);
+    }
+
 }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerTests.java
deleted file mode 100644
index b591253..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerTests.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator;
-
-public class SamplerTests extends BaseAggregationTestCase<SamplerAggregator.Factory> {
-
-    @Override
-    protected final SamplerAggregator.Factory createTestAggregatorFactory() {
-        SamplerAggregator.Factory factory = new SamplerAggregator.Factory("foo");
-        if (randomBoolean()) {
-            factory.shardSize(randomIntBetween(1, 1000));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java
index 8c6f30c..d138c0c 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java
@@ -91,7 +91,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(global("global")
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -104,8 +104,8 @@ public class ShardReduceIT extends ESIntegTestCase {
     public void testFilter() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
-                .addAggregation(filter("filter", QueryBuilders.matchAllQuery())
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                .addAggregation(filter("filter").filter(QueryBuilders.matchAllQuery())
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -119,7 +119,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(missing("missing").field("foobar")
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -133,9 +133,9 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(global("global")
-                        .subAggregation(filter("filter", QueryBuilders.matchAllQuery())
+                        .subAggregation(filter("filter").filter(QueryBuilders.matchAllQuery())
                                 .subAggregation(missing("missing").field("foobar")
-                                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))))
+                                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -150,8 +150,8 @@ public class ShardReduceIT extends ESIntegTestCase {
     public void testNested() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
-                .addAggregation(nested("nested", "nested")
-                        .subAggregation(dateHistogram("histo").field("nested.date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                .addAggregation(nested("nested").path("nested")
+                        .subAggregation(dateHistogram("histo").field("nested.date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -166,7 +166,7 @@ public class ShardReduceIT extends ESIntegTestCase {
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(terms("terms").field("term-s")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -181,7 +181,7 @@ public class ShardReduceIT extends ESIntegTestCase {
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(terms("terms").field("term-l")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -196,7 +196,7 @@ public class ShardReduceIT extends ESIntegTestCase {
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(terms("terms").field("term-d")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -210,7 +210,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(range("range").field("value").addRange("r1", 0, 10)
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -224,7 +224,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(dateRange("range").field("date").addRange("r1", "2014-01-01", "2014-01-10")
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -238,7 +238,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(ipRange("range").field("ip").addRange("r1", "10.0.0.1", "10.0.0.10")
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -252,7 +252,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(histogram("topHisto").field("value").interval(5)
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -265,8 +265,8 @@ public class ShardReduceIT extends ESIntegTestCase {
     public void testDateHistogram() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
-                .addAggregation(dateHistogram("topHisto").field("date").dateHistogramInterval(DateHistogramInterval.MONTH)
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                .addAggregation(dateHistogram("topHisto").field("date").interval(DateHistogramInterval.MONTH)
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -281,7 +281,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(geohashGrid("grid").field("location")
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsIT.java
index 214a8e6..7582d75 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsIT.java
@@ -27,13 +27,14 @@ import org.elasticsearch.index.query.TermQueryBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms.Bucket;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorFactory.ExecutionMode;
+import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.ChiSquare;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.GND;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.MutualInformation;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.PercentageScore;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 
 import java.util.Arrays;
@@ -45,8 +46,6 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.significantTerms;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.hasSize;
 import static org.hamcrest.Matchers.is;
@@ -117,7 +116,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
                            .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -133,8 +132,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "paul"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
-                           .minDocCount(1).includeExclude(new IncludeExclude(null, excludeTerms)))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
+                           .minDocCount(1).exclude(excludeTerms))
                 .execute()
                 .actionGet();
         assertSearchResponse(response);
@@ -146,8 +145,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
     public void testIncludeExclude() throws Exception {
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(new TermQueryBuilder("_all", "weller"))
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
-                        .includeExclude(new IncludeExclude(null, "weller")))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                        .exclude("weller"))
                 .get();
         assertSearchResponse(response);
         SignificantTerms topTerms = response.getAggregations().get("mySignificantTerms");
@@ -165,8 +164,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
 
         response = client().prepareSearch("test")
                 .setQuery(new TermQueryBuilder("_all", "weller"))
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
-                        .includeExclude(new IncludeExclude("weller", null)))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                        .include("weller"))
                 .get();
         assertSearchResponse(response);
         topTerms = response.getAggregations().get("mySignificantTerms");
@@ -182,8 +181,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
         String []incExcTerms={"weller","nosuchterm"};
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(new TermQueryBuilder("_all", "weller"))
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
-                        .includeExclude(new IncludeExclude(null, incExcTerms)))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                        .exclude(incExcTerms))
                 .get();
         assertSearchResponse(response);
         SignificantTerms topTerms = response.getAggregations().get("mySignificantTerms");
@@ -195,8 +194,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
 
         response = client().prepareSearch("test")
                 .setQuery(new TermQueryBuilder("_all", "weller"))
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
-                        .includeExclude(new IncludeExclude(incExcTerms, null)))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                        .include(incExcTerms))
                 .get();
         assertSearchResponse(response);
         topTerms = response.getAggregations().get("mySignificantTerms");
@@ -213,7 +212,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
                         .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -227,7 +226,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
                            .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -241,7 +240,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint()).significanceHeuristic(new GND(true))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint()).significanceHeuristic(new GND.GNDBuilder(true))
                         .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -255,7 +254,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint()).significanceHeuristic(new ChiSquare(false,true))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint()).significanceHeuristic(new ChiSquare.ChiSquareBuilder(false,true))
                         .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -273,8 +272,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSize(60)
                 .setExplain(true)
                 .addAggregation(
-                        significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
-                                .significanceHeuristic(new PercentageScore()).minDocCount(2)).execute().actionGet();
+                        new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                                .significanceHeuristic(new PercentageScore.PercentageScoreBuilder()).minDocCount(2)).execute().actionGet();
         assertSearchResponse(response);
         SignificantTerms topTerms = response.getAggregations().get("mySignificantTerms");
         checkExpectedStringTermsFound(topTerms);
@@ -289,7 +288,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description")
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description")
                            .minDocCount(2).backgroundFilter(QueryBuilders.termQuery("fact_category", 1)))
                 .execute()
                 .actionGet();
@@ -313,7 +312,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "weller"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description")
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description")
                            .minDocCount(1).backgroundFilter(QueryBuilders.termsQuery("description",  "paul")))
                 .execute()
                 .actionGet();
@@ -336,9 +335,9 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 { "craig", "kelly", "terje", "haakonsen", "burton" }};
         SearchResponse response = client().prepareSearch("test")
                 .setSearchType(SearchType.QUERY_AND_FETCH)
-                .addAggregation(terms("myCategories").field("fact_category").minDocCount(2)
+                .addAggregation(new TermsBuilder("myCategories").field("fact_category").minDocCount(2)
                         .subAggregation(
-                                   significantTerms("mySignificantTerms").field("description")
+                                   new SignificantTermsBuilder("mySignificantTerms").field("description")
                                    .executionHint(randomExecutionHint())
                                    .minDocCount(2)))
                 .execute()
@@ -363,7 +362,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description")
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description")
                             .executionHint(randomExecutionHint())
                            .minDocCount(2))
                 .execute()
@@ -394,10 +393,10 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms")
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms")
                         .field("description")
                         .executionHint(randomExecutionHint())
-                        .significanceHeuristic(new JLHScore())
+                        .significanceHeuristic(new JLHScore.JLHScoreBuilder())
                         .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -411,10 +410,10 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms")
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms")
                         .field("description")
                         .executionHint(randomExecutionHint())
-                        .significanceHeuristic(new MutualInformation(false, true))
+                        .significanceHeuristic(new MutualInformation.MutualInformationBuilder(false, true))
                         .minDocCount(1))
                 .execute()
                 .actionGet();
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
index 33f17c3..6c1e7df 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
@@ -20,7 +20,6 @@ package org.elasticsearch.search.aggregations.bucket;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -35,11 +34,13 @@ import org.elasticsearch.script.ScriptModule;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.search.SearchModule;
 import org.elasticsearch.search.aggregations.Aggregation;
+import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.filter.InternalFilter;
 import org.elasticsearch.search.aggregations.bucket.script.NativeSignificanceScoreScriptNoParams;
 import org.elasticsearch.search.aggregations.bucket.script.NativeSignificanceScoreScriptWithParams;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.ChiSquare;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.GND;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.MutualInformation;
@@ -50,6 +51,8 @@ import org.elasticsearch.search.aggregations.bucket.significant.heuristics.Signi
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicStreams;
 import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
+import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.search.aggregations.bucket.SharedSignificantTermsTestMethods;
 
@@ -66,9 +69,6 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.filter;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.significantTerms;
 import static org.hamcrest.Matchers.closeTo;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.greaterThan;
@@ -99,12 +99,11 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         String settings = "{\"index.number_of_shards\": 1, \"index.number_of_replicas\": 0}";
         SharedSignificantTermsTestMethods.index01Docs(type, settings, this);
         SearchResponse response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(
-                        terms("class")
+                .addAggregation(new TermsBuilder("class")
                         .field(CLASS_FIELD)
-                                .subAggregation((significantTerms("sig_terms"))
+                        .subAggregation((new SignificantTermsBuilder("sig_terms"))
                                 .field(TEXT_FIELD)
-                                .significanceHeuristic(new SimpleHeuristic())
+                                .significanceHeuristic(new SimpleHeuristic.SimpleHeuristicBuilder())
                                 .minDocCount(1)
                         )
                 )
@@ -132,12 +131,11 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         // the reason is that this would trigger toXContent and we would like to check that this has no potential side effects
 
         response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(
-                        terms("class")
+                .addAggregation(new TermsBuilder("class")
                         .field(CLASS_FIELD)
-                                .subAggregation((significantTerms("sig_terms"))
+                        .subAggregation((new SignificantTermsBuilder("sig_terms"))
                                 .field(TEXT_FIELD)
-                                .significanceHeuristic(new SimpleHeuristic())
+                                .significanceHeuristic(new SimpleHeuristic.SimpleHeuristicBuilder())
                                 .minDocCount(1)
                         )
                 )
@@ -165,7 +163,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
     public static class CustomSignificanceHeuristicPlugin extends Plugin {
 
         static {
-            SignificanceHeuristicStreams.registerPrototype(SimpleHeuristic.PROTOTYPE);
+            SignificanceHeuristicStreams.registerStream(SimpleHeuristic.STREAM);
         }
 
         @Override
@@ -179,7 +177,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         }
 
         public void onModule(SearchModule significanceModule) {
-            significanceModule.registerHeuristicParser(new SimpleHeuristic.SimpleHeuristicParser());
+            significanceModule.registerHeuristicParser(SimpleHeuristic.SimpleHeuristicParser.class);
         }
         public void onModule(ScriptModule module) {
             module.registerScript(NativeSignificanceScoreScriptNoParams.NATIVE_SIGNIFICANCE_SCORE_SCRIPT_NO_PARAMS, NativeSignificanceScoreScriptNoParams.Factory.class);
@@ -189,46 +187,24 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
     public static class SimpleHeuristic extends SignificanceHeuristic {
 
-        static final SimpleHeuristic PROTOTYPE = new SimpleHeuristic();
+        protected static final String[] NAMES = {"simple"};
 
-        protected static final ParseField NAMES_FIELD = new ParseField("simple");
+        public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+            @Override
+            public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+                return readFrom(in);
+            }
 
-        @Override
-        public String getWriteableName() {
-            return NAMES_FIELD.getPreferredName();
-        }
+            @Override
+            public String getName() {
+                return NAMES[0];
+            }
+        };
 
-        @Override
-        public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
+        public static SignificanceHeuristic readFrom(StreamInput in) throws IOException {
             return new SimpleHeuristic();
         }
 
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
-            return builder;
-        }
-
-        @Override
-        public int hashCode() {
-            return 1;
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            return true;
-        }
-
         /**
          * @param subsetFreq   The frequency of the term in the selected sample
          * @param subsetSize   The size of the selected sample (typically number of docs)
@@ -241,10 +217,15 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
             return subsetFreq / subsetSize > supersetFreq / supersetSize ? 2.0 : 1.0;
         }
 
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            out.writeString(STREAM.getName());
+        }
+
         public static class SimpleHeuristicParser implements SignificanceHeuristicParser {
 
             @Override
-            public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+            public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                     throws IOException, QueryShardException {
                 parser.nextToken();
                 return new SimpleHeuristic();
@@ -252,7 +233,16 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
             @Override
             public String[] getNames() {
-                return NAMES_FIELD.getAllNamesIncludedDeprecated();
+                return NAMES;
+            }
+        }
+
+        public static class SimpleHeuristicBuilder implements SignificanceHeuristicBuilder {
+
+            @Override
+            public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+                builder.startObject(STREAM.getName()).endObject();
+                return builder;
             }
         }
     }
@@ -262,7 +252,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         String settings = "{\"index.number_of_shards\": 1, \"index.number_of_replicas\": 0}";
         SharedSignificantTermsTestMethods.index01Docs(type, settings, this);
         SearchResponse response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(terms("class").field(CLASS_FIELD).subAggregation(significantTerms("sig_terms").field(TEXT_FIELD)))
+                .addAggregation(new TermsBuilder("class").field(CLASS_FIELD).subAggregation(new SignificantTermsBuilder("sig_terms").field(TEXT_FIELD)))
                 .execute()
                 .actionGet();
         assertSearchResponse(response);
@@ -322,11 +312,10 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         indexRandom(true, false, indexRequestBuilderList);
 
         SearchResponse response1 = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(
-                        terms("class")
+                .addAggregation(new TermsBuilder("class")
                         .field(CLASS_FIELD)
                         .subAggregation(
-                                significantTerms("sig_terms")
+                                new SignificantTermsBuilder("sig_terms")
                                         .field(TEXT_FIELD)
                                         .minDocCount(1)))
                 .execute()
@@ -337,22 +326,22 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         String type = randomBoolean() ? "string" : "long";
         String settings = "{\"index.number_of_shards\": 1, \"index.number_of_replicas\": 0}";
         SharedSignificantTermsTestMethods.index01Docs(type, settings, this);
-        testBackgroundVsSeparateSet(new MutualInformation(true, true), new MutualInformation(true, false));
-        testBackgroundVsSeparateSet(new ChiSquare(true, true), new ChiSquare(true, false));
-        testBackgroundVsSeparateSet(new GND(true), new GND(false));
+        testBackgroundVsSeparateSet(new MutualInformation.MutualInformationBuilder(true, true), new MutualInformation.MutualInformationBuilder(true, false));
+        testBackgroundVsSeparateSet(new ChiSquare.ChiSquareBuilder(true, true), new ChiSquare.ChiSquareBuilder(true, false));
+        testBackgroundVsSeparateSet(new GND.GNDBuilder(true), new GND.GNDBuilder(false));
     }
 
     // compute significance score by
     // 1. terms agg on class and significant terms
     // 2. filter buckets and set the background to the other class and set is_background false
     // both should yield exact same result
-    public void testBackgroundVsSeparateSet(SignificanceHeuristic significanceHeuristicExpectingSuperset, SignificanceHeuristic significanceHeuristicExpectingSeparateSets) throws Exception {
+    public void testBackgroundVsSeparateSet(SignificanceHeuristicBuilder significanceHeuristicExpectingSuperset, SignificanceHeuristicBuilder significanceHeuristicExpectingSeparateSets) throws Exception {
 
         SearchResponse response1 = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(terms("class")
+                .addAggregation(new TermsBuilder("class")
                         .field(CLASS_FIELD)
                         .subAggregation(
-                                significantTerms("sig_terms")
+                                new SignificantTermsBuilder("sig_terms")
                                         .field(TEXT_FIELD)
                                         .minDocCount(1)
                                         .significanceHeuristic(
@@ -361,14 +350,16 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
                 .actionGet();
         assertSearchResponse(response1);
         SearchResponse response2 = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(filter("0", QueryBuilders.termQuery(CLASS_FIELD, "0"))
-                        .subAggregation(significantTerms("sig_terms")
+                .addAggregation((new FilterAggregationBuilder("0"))
+                        .filter(QueryBuilders.termQuery(CLASS_FIELD, "0"))
+                        .subAggregation(new SignificantTermsBuilder("sig_terms")
                                 .field(TEXT_FIELD)
                                 .minDocCount(1)
                                 .backgroundFilter(QueryBuilders.termQuery(CLASS_FIELD, "1"))
                                 .significanceHeuristic(significanceHeuristicExpectingSeparateSets)))
-                .addAggregation(filter("1", QueryBuilders.termQuery(CLASS_FIELD, "1"))
-                        .subAggregation(significantTerms("sig_terms")
+                .addAggregation((new FilterAggregationBuilder("1"))
+                        .filter(QueryBuilders.termQuery(CLASS_FIELD, "1"))
+                        .subAggregation(new SignificantTermsBuilder("sig_terms")
                                 .field(TEXT_FIELD)
                                 .minDocCount(1)
                                 .backgroundFilter(QueryBuilders.termQuery(CLASS_FIELD, "0"))
@@ -397,15 +388,15 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
     public void testScoresEqualForPositiveAndNegative() throws Exception {
         indexEqualTestData();
-        testScoresEqualForPositiveAndNegative(new MutualInformation(true, true));
-        testScoresEqualForPositiveAndNegative(new ChiSquare(true, true));
+        testScoresEqualForPositiveAndNegative(new MutualInformation.MutualInformationBuilder(true, true));
+        testScoresEqualForPositiveAndNegative(new ChiSquare.ChiSquareBuilder(true, true));
     }
 
-    public void testScoresEqualForPositiveAndNegative(SignificanceHeuristic heuristic) throws Exception {
+    public void testScoresEqualForPositiveAndNegative(SignificanceHeuristicBuilder heuristic) throws Exception {
 
         //check that results for both classes are the same with exclude negatives = false and classes are routing ids
         SearchResponse response = client().prepareSearch("test")
-                .addAggregation(terms("class").field("class").subAggregation(significantTerms("mySignificantTerms")
+                .addAggregation(new TermsBuilder("class").field("class").subAggregation(new SignificantTermsBuilder("mySignificantTerms")
                         .field("text")
                         .executionHint(randomExecutionHint())
                         .significanceHeuristic(heuristic)
@@ -462,14 +453,13 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
     public void testScriptScore() throws ExecutionException, InterruptedException, IOException {
         indexRandomFrequencies01(randomBoolean() ? "string" : "long");
-        ScriptHeuristic scriptHeuristic = getScriptSignificanceHeuristic();
+        ScriptHeuristic.ScriptHeuristicBuilder scriptHeuristicBuilder = getScriptSignificanceHeuristicBuilder();
         ensureYellow();
         SearchResponse response = client().prepareSearch(INDEX_NAME)
-                .addAggregation(terms("class").field(CLASS_FIELD)
-                        .subAggregation(significantTerms("mySignificantTerms")
+                .addAggregation(new TermsBuilder("class").field(CLASS_FIELD).subAggregation(new SignificantTermsBuilder("mySignificantTerms")
                         .field(TEXT_FIELD)
                         .executionHint(randomExecutionHint())
-                        .significanceHeuristic(scriptHeuristic)
+                        .significanceHeuristic(scriptHeuristicBuilder)
                         .minDocCount(1).shardSize(2).size(2)))
                 .execute()
                 .actionGet();
@@ -481,7 +471,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         }
     }
 
-    private ScriptHeuristic getScriptSignificanceHeuristic() throws IOException {
+    private ScriptHeuristic.ScriptHeuristicBuilder getScriptSignificanceHeuristicBuilder() throws IOException {
         Script script = null;
         if (randomBoolean()) {
             Map<String, Object> params = null;
@@ -491,9 +481,9 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         } else {
             script = new Script("native_significance_score_script_no_params", ScriptType.INLINE, "native", null);
         }
-        ScriptHeuristic scriptHeuristic = new ScriptHeuristic(script);
+        ScriptHeuristic.ScriptHeuristicBuilder builder = new ScriptHeuristic.ScriptHeuristicBuilder().setScript(script);
 
-        return scriptHeuristic;
+        return builder;
     }
 
     private void indexRandomFrequencies01(String type) throws ExecutionException, InterruptedException {
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsTests.java
deleted file mode 100644
index d0c9e61..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsTests.java
+++ /dev/null
@@ -1,224 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.automaton.RegExp;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.ChiSquare;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.GND;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.MutualInformation;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.PercentageScore;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.ExecutionMode;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import java.util.SortedSet;
-import java.util.TreeSet;
-
-public class SignificantTermsTests extends BaseAggregationTestCase<SignificantTermsAggregatorFactory> {
-
-    private static final String[] executionHints;
-
-    static {
-        ExecutionMode[] executionModes = ExecutionMode.values();
-        executionHints = new String[executionModes.length];
-        for (int i = 0; i < executionModes.length; i++) {
-            executionHints[i] = executionModes[i].toString();
-        }
-    }
-
-    @Override
-    protected SignificantTermsAggregatorFactory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        SignificantTermsAggregatorFactory factory = new SignificantTermsAggregatorFactory(name, null);
-        String field = randomAsciiOfLengthBetween(3, 20);
-        int randomFieldBranch = randomInt(2);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        default:
-            fail();
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            int size = randomInt(4);
-            switch (size) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                size = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setRequiredSize(size);
-
-        }
-        if (randomBoolean()) {
-            int shardSize = randomInt(4);
-            switch (shardSize) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardSize = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setShardSize(shardSize);
-        }
-        if (randomBoolean()) {
-            int minDocCount = randomInt(4);
-            switch (minDocCount) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                minDocCount = randomInt();
-                break;
-            }
-            factory.bucketCountThresholds().setMinDocCount(minDocCount);
-        }
-        if (randomBoolean()) {
-            int shardMinDocCount = randomInt(4);
-            switch (shardMinDocCount) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardMinDocCount = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setShardMinDocCount(shardMinDocCount);
-        }
-        if (randomBoolean()) {
-            factory.executionHint(randomFrom(executionHints));
-        }
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            IncludeExclude incExc = null;
-            switch (randomInt(5)) {
-            case 0:
-                incExc = new IncludeExclude(new RegExp("foobar"), null);
-                break;
-            case 1:
-                incExc = new IncludeExclude(null, new RegExp("foobaz"));
-                break;
-            case 2:
-                incExc = new IncludeExclude(new RegExp("foobar"), new RegExp("foobaz"));
-                break;
-            case 3:
-                SortedSet<BytesRef> includeValues = new TreeSet<>();
-                int numIncs = randomIntBetween(1, 20);
-                for (int i = 0; i < numIncs; i++) {
-                    includeValues.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                SortedSet<BytesRef> excludeValues = null;
-                incExc = new IncludeExclude(includeValues, excludeValues);
-                break;
-            case 4:
-                SortedSet<BytesRef> includeValues2 = null;
-                SortedSet<BytesRef> excludeValues2 = new TreeSet<>();
-                int numExcs2 = randomIntBetween(1, 20);
-                for (int i = 0; i < numExcs2; i++) {
-                    excludeValues2.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                incExc = new IncludeExclude(includeValues2, excludeValues2);
-                break;
-            case 5:
-                SortedSet<BytesRef> includeValues3 = new TreeSet<>();
-                int numIncs3 = randomIntBetween(1, 20);
-                for (int i = 0; i < numIncs3; i++) {
-                    includeValues3.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                SortedSet<BytesRef> excludeValues3 = new TreeSet<>();
-                int numExcs3 = randomIntBetween(1, 20);
-                for (int i = 0; i < numExcs3; i++) {
-                    excludeValues3.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                incExc = new IncludeExclude(includeValues3, excludeValues3);
-                break;
-            default:
-                fail();
-            }
-            factory.includeExclude(incExc);
-        }
-        if (randomBoolean()) {
-            SignificanceHeuristic significanceHeuristic = null;
-            switch (randomInt(5)) {
-            case 0:
-                significanceHeuristic = PercentageScore.PROTOTYPE;
-                break;
-            case 1:
-                significanceHeuristic = new ChiSquare(randomBoolean(), randomBoolean());
-                break;
-            case 2:
-                significanceHeuristic = new GND(randomBoolean());
-                break;
-            case 3:
-                significanceHeuristic = new MutualInformation(randomBoolean(), randomBoolean());
-                break;
-            case 4:
-                significanceHeuristic = new ScriptHeuristic(new Script("foo"));
-                break;
-            case 5:
-                significanceHeuristic = JLHScore.PROTOTYPE;
-                break;
-            default:
-                fail();
-            }
-            factory.significanceHeuristic(significanceHeuristic);
-        }
-        if (randomBoolean()) {
-            factory.backgroundFilter(QueryBuilders.termsQuery("foo", "bar"));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java
index 4d61f45..9a7b337 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java
@@ -21,10 +21,13 @@ package org.elasticsearch.search.aggregations.bucket;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.index.query.QueryBuilders;
+import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.filter.InternalFilter;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsBuilder;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 
 import java.util.ArrayList;
@@ -34,9 +37,6 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.filter;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.significantTerms;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.hamcrest.Matchers.equalTo;
 
 /**
@@ -71,8 +71,8 @@ public class TermsShardMinDocCountIT extends ESIntegTestCase {
         // first, check that indeed when not setting the shardMinDocCount parameter 0 terms are returned
         SearchResponse response = client().prepareSearch(index)
                 .addAggregation(
-                        (filter("inclass", QueryBuilders.termQuery("class", true)))
-                                .subAggregation(significantTerms("mySignificantTerms").field("text").minDocCount(2).size(2).executionHint(randomExecutionHint()))
+                        (new FilterAggregationBuilder("inclass").filter(QueryBuilders.termQuery("class", true)))
+                                .subAggregation(new SignificantTermsBuilder("mySignificantTerms").field("text").minDocCount(2).size(2).executionHint(randomExecutionHint()))
                 )
                 .execute()
                 .actionGet();
@@ -84,8 +84,8 @@ public class TermsShardMinDocCountIT extends ESIntegTestCase {
 
         response = client().prepareSearch(index)
                 .addAggregation(
-                        (filter("inclass", QueryBuilders.termQuery("class", true)))
-                                .subAggregation(significantTerms("mySignificantTerms").field("text").minDocCount(2).shardMinDocCount(2).size(2).executionHint(randomExecutionHint()))
+                        (new FilterAggregationBuilder("inclass").filter(QueryBuilders.termQuery("class", true)))
+                                .subAggregation(new SignificantTermsBuilder("mySignificantTerms").field("text").minDocCount(2).shardMinDocCount(2).size(2).executionHint(randomExecutionHint()))
                 )
                 .execute()
                 .actionGet();
@@ -127,7 +127,7 @@ public class TermsShardMinDocCountIT extends ESIntegTestCase {
         // first, check that indeed when not setting the shardMinDocCount parameter 0 terms are returned
         SearchResponse response = client().prepareSearch(index)
                 .addAggregation(
-                        terms("myTerms").field("text").minDocCount(2).size(2).executionHint(randomExecutionHint()).order(Terms.Order.term(true))
+                        new TermsBuilder("myTerms").field("text").minDocCount(2).size(2).executionHint(randomExecutionHint()).order(Terms.Order.term(true))
                 )
                 .execute()
                 .actionGet();
@@ -138,7 +138,7 @@ public class TermsShardMinDocCountIT extends ESIntegTestCase {
 
         response = client().prepareSearch(index)
                 .addAggregation(
-                        terms("myTerms").field("text").minDocCount(2).shardMinDocCount(2).size(2).executionHint(randomExecutionHint()).order(Terms.Order.term(true))
+                        new TermsBuilder("myTerms").field("text").minDocCount(2).shardMinDocCount(2).size(2).executionHint(randomExecutionHint()).order(Terms.Order.term(true))
                 )
                 .execute()
                 .actionGet();
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsTests.java
deleted file mode 100644
index 9591c3f..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsTests.java
+++ /dev/null
@@ -1,230 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.automaton.RegExp;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.ExecutionMode;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.SortedSet;
-import java.util.TreeSet;
-
-public class TermsTests extends BaseAggregationTestCase<TermsAggregatorFactory> {
-
-    private static final String[] executionHints;
-
-    static {
-        ExecutionMode[] executionModes = ExecutionMode.values();
-        executionHints = new String[executionModes.length];
-        for (int i = 0; i < executionModes.length; i++) {
-            executionHints[i] = executionModes[i].toString();
-        }
-    }
-
-    @Override
-    protected TermsAggregatorFactory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        TermsAggregatorFactory factory = new TermsAggregatorFactory(name, null);
-        String field = randomAsciiOfLengthBetween(3, 20);
-        int randomFieldBranch = randomInt(2);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        default:
-            fail();
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            int size = randomInt(4);
-            switch (size) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                size = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setRequiredSize(size);
-
-        }
-        if (randomBoolean()) {
-            int shardSize = randomInt(4);
-            switch (shardSize) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardSize = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setShardSize(shardSize);
-        }
-        if (randomBoolean()) {
-            int minDocCount = randomInt(4);
-            switch (minDocCount) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                minDocCount = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setMinDocCount(minDocCount);
-        }
-        if (randomBoolean()) {
-            int shardMinDocCount = randomInt(4);
-            switch (shardMinDocCount) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardMinDocCount = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setShardMinDocCount(shardMinDocCount);
-        }
-        if (randomBoolean()) {
-            factory.collectMode(randomFrom(SubAggCollectionMode.values()));
-        }
-        if (randomBoolean()) {
-            factory.executionHint(randomFrom(executionHints));
-        }
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            IncludeExclude incExc = null;
-            switch (randomInt(5)) {
-            case 0:
-                incExc = new IncludeExclude(new RegExp("foobar"), null);
-                break;
-            case 1:
-                incExc = new IncludeExclude(null, new RegExp("foobaz"));
-                break;
-            case 2:
-                incExc = new IncludeExclude(new RegExp("foobar"), new RegExp("foobaz"));
-                break;
-            case 3:
-                SortedSet<BytesRef> includeValues = new TreeSet<>();
-                int numIncs = randomIntBetween(1, 20);
-                for (int i = 0; i < numIncs; i++) {
-                    includeValues.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                SortedSet<BytesRef> excludeValues = null;
-                incExc = new IncludeExclude(includeValues, excludeValues);
-                break;
-            case 4:
-                SortedSet<BytesRef> includeValues2 = null;
-                SortedSet<BytesRef> excludeValues2 = new TreeSet<>();
-                int numExcs2 = randomIntBetween(1, 20);
-                for (int i = 0; i < numExcs2; i++) {
-                    excludeValues2.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                incExc = new IncludeExclude(includeValues2, excludeValues2);
-                break;
-            case 5:
-                SortedSet<BytesRef> includeValues3 = new TreeSet<>();
-                int numIncs3 = randomIntBetween(1, 20);
-                for (int i = 0; i < numIncs3; i++) {
-                    includeValues3.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                SortedSet<BytesRef> excludeValues3 = new TreeSet<>();
-                int numExcs3 = randomIntBetween(1, 20);
-                for (int i = 0; i < numExcs3; i++) {
-                    excludeValues3.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                incExc = new IncludeExclude(includeValues3, excludeValues3);
-                break;
-            default:
-                fail();
-            }
-            factory.includeExclude(incExc);
-        }
-        if (randomBoolean()) {
-            List<Terms.Order> order = randomOrder();
-            factory.order(order);
-        }
-        if (randomBoolean()) {
-            factory.showTermDocCountError(randomBoolean());
-        }
-        return factory;
-    }
-
-    private List<Terms.Order> randomOrder() {
-        List<Terms.Order> orders = new ArrayList<>();
-        switch (randomInt(4)) {
-        case 0:
-            orders.add(Terms.Order.term(randomBoolean()));
-            break;
-        case 1:
-            orders.add(Terms.Order.count(randomBoolean()));
-            break;
-        case 2:
-            orders.add(Terms.Order.aggregation(randomAsciiOfLengthBetween(3, 20), randomBoolean()));
-            break;
-        case 3:
-            orders.add(Terms.Order.aggregation(randomAsciiOfLengthBetween(3, 20), randomAsciiOfLengthBetween(3, 20), randomBoolean()));
-            break;
-        case 4:
-            int numOrders = randomIntBetween(1, 3);
-            for (int i = 0; i < numOrders; i++) {
-                orders.addAll(randomOrder());
-            }
-            break;
-        default:
-            fail();
-        }
-        return orders;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParserTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParserTests.java
index 4ffa860..cd7dadd 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParserTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParserTests.java
@@ -18,52 +18,40 @@
  */
 package org.elasticsearch.search.aggregations.bucket.geogrid;
 
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.TestSearchContext;
 
 public class GeoHashGridParserTests extends ESTestCase {
     public void testParseValidFromInts() throws Exception {
+        SearchContext searchContext = new TestSearchContext();
         int precision = randomIntBetween(1, 12);
         XContentParser stParser = JsonXContent.jsonXContent.createParser(
                 "{\"field\":\"my_loc\", \"precision\":" + precision + ", \"size\": 500, \"shard_size\": 550}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         // can create a factory
-        assertNotNull(parser.parse("geohash_grid", stParser, parseContext));
+        assertNotNull(parser.parse("geohash_grid", stParser, searchContext));
     }
 
     public void testParseValidFromStrings() throws Exception {
+        SearchContext searchContext = new TestSearchContext();
         int precision = randomIntBetween(1, 12);
         XContentParser stParser = JsonXContent.jsonXContent.createParser(
                 "{\"field\":\"my_loc\", \"precision\":\"" + precision + "\", \"size\": \"500\", \"shard_size\": \"550\"}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         // can create a factory
-        assertNotNull(parser.parse("geohash_grid", stParser, parseContext));
+        assertNotNull(parser.parse("geohash_grid", stParser, searchContext));
     }
 
     public void testParseErrorOnNonIntPrecision() throws Exception {
+        SearchContext searchContext = new TestSearchContext();
         XContentParser stParser = JsonXContent.jsonXContent.createParser("{\"field\":\"my_loc\", \"precision\":\"2.0\"}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         try {
-            parser.parse("geohash_grid", stParser, parseContext);
+            parser.parse("geohash_grid", stParser, searchContext);
             fail();
         } catch (NumberFormatException ex) {
             assertEquals("For input string: \"2.0\"", ex.getMessage());
@@ -71,31 +59,23 @@ public class GeoHashGridParserTests extends ESTestCase {
     }
 
     public void testParseErrorOnBooleanPrecision() throws Exception {
+        SearchContext searchContext = new TestSearchContext();
         XContentParser stParser = JsonXContent.jsonXContent.createParser("{\"field\":\"my_loc\", \"precision\":false}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         try {
-            parser.parse("geohash_grid", stParser, parseContext);
+            parser.parse("geohash_grid", stParser, searchContext);
             fail();
-        } catch (ParsingException ex) {
-            assertEquals("Unexpected token VALUE_BOOLEAN [precision] in [geohash_grid].", ex.getMessage());
+        } catch (SearchParseException ex) {
+            assertEquals("Unexpected token VALUE_BOOLEAN in [geohash_grid].", ex.getMessage());
         }
     }
 
     public void testParseErrorOnPrecisionOutOfRange() throws Exception {
+        SearchContext searchContext = new TestSearchContext();
         XContentParser stParser = JsonXContent.jsonXContent.createParser("{\"field\":\"my_loc\", \"precision\":\"13\"}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         try {
-            parser.parse("geohash_grid", stParser, parseContext);
+            parser.parse("geohash_grid", stParser, searchContext);
             fail();
         } catch (IllegalArgumentException ex) {
             assertEquals("Invalid geohash aggregation precision of 13. Must be between 1 and 12.", ex.getMessage());
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
index 3a90c8e..0cb799d 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
@@ -124,12 +124,10 @@ public class NestedAggregatorTests extends ESSingleNodeTestCase {
         AggregationContext context = new AggregationContext(searchContext);
 
         AggregatorFactories.Builder builder = AggregatorFactories.builder();
-        NestedAggregator.Factory factory = new NestedAggregator.Factory("test", "nested_field");
-        builder.addAggregator(factory);
+        builder.addAggregator(new NestedAggregator.Factory("test", "nested_field"));
         AggregatorFactories factories = builder.build();
         searchContext.aggregations(new SearchContextAggregations(factories));
-        factories.init(context);
-        Aggregator[] aggs = factories.createTopLevelAggregators();
+        Aggregator[] aggs = factories.createTopLevelAggregators(context);
         BucketCollector collector = BucketCollector.wrap(Arrays.asList(aggs));
         collector.preCollection();
         // A regular search always exclude nested docs, so we use NonNestedDocsFilter.INSTANCE here (otherwise MatchAllDocsQuery would be sufficient)
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedTests.java
deleted file mode 100644
index 59ceb4d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedTests.java
+++ /dev/null
@@ -1,32 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.nested;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.nested.NestedAggregator.Factory;
-
-public class NestedTests extends BaseAggregationTestCase<NestedAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        return new Factory(randomAsciiOfLengthBetween(1, 20), randomAsciiOfLengthBetween(3, 40));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedTests.java
deleted file mode 100644
index 7feecd8..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedTests.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.nested;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.nested.ReverseNestedAggregator.Factory;
-
-public class ReverseNestedTests extends BaseAggregationTestCase<ReverseNestedAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        Factory factory = new Factory(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.path(randomAsciiOfLengthBetween(3, 40));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java
index b2cd7b0..0fe9113 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java
@@ -21,17 +21,12 @@ package org.elasticsearch.search.aggregations.bucket.significant;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.Version;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.InputStreamStreamInput;
 import org.elasticsearch.common.io.stream.OutputStreamStreamOutput;
-import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.QueryParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregations;
@@ -41,6 +36,7 @@ import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHSc
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.MutualInformation;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.PercentageScore;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParser;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParserMapper;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
@@ -66,7 +62,6 @@ import static org.hamcrest.Matchers.greaterThan;
 import static org.hamcrest.Matchers.greaterThanOrEqualTo;
 import static org.hamcrest.Matchers.lessThan;
 import static org.hamcrest.Matchers.lessThanOrEqualTo;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.significantTerms;
 
 /**
  *
@@ -132,7 +127,7 @@ public class SignificanceHeuristicTests extends ESTestCase {
 
     SignificanceHeuristic getRandomSignificanceheuristic() {
         List<SignificanceHeuristic> heuristics = new ArrayList<>();
-        heuristics.add(JLHScore.PROTOTYPE);
+        heuristics.add(JLHScore.INSTANCE);
         heuristics.add(new MutualInformation(randomBoolean(), randomBoolean()));
         heuristics.add(new GND(randomBoolean()));
         heuristics.add(new ChiSquare(randomBoolean(), randomBoolean()));
@@ -195,7 +190,7 @@ public class SignificanceHeuristicTests extends ESTestCase {
     public void testBuilderAndParser() throws Exception {
 
         Set<SignificanceHeuristicParser> parsers = new HashSet<>();
-        SignificanceHeuristicParserMapper heuristicParserMapper = new SignificanceHeuristicParserMapper(parsers);
+        SignificanceHeuristicParserMapper heuristicParserMapper = new SignificanceHeuristicParserMapper(parsers, null);
         SearchContext searchContext = new SignificantTermsTestSearchContext();
 
         // test jlh with string
@@ -209,10 +204,10 @@ public class SignificanceHeuristicTests extends ESTestCase {
         assertThat(parseFromString(heuristicParserMapper, searchContext, "\"chi_square\":{\"include_negatives\": " + includeNegatives + ", \"background_is_superset\":" + backgroundIsSuperset + "}"), equalTo((SignificanceHeuristic) (new ChiSquare(includeNegatives, backgroundIsSuperset))));
 
         // test with builders
-        assertTrue(parseFromBuilder(heuristicParserMapper, searchContext, new JLHScore()) instanceof JLHScore);
-        assertTrue(parseFromBuilder(heuristicParserMapper, searchContext, new GND(backgroundIsSuperset)) instanceof GND);
-        assertThat(parseFromBuilder(heuristicParserMapper, searchContext, new MutualInformation(includeNegatives, backgroundIsSuperset)), equalTo((SignificanceHeuristic) new MutualInformation(includeNegatives, backgroundIsSuperset)));
-        assertThat(parseFromBuilder(heuristicParserMapper, searchContext, new ChiSquare(includeNegatives, backgroundIsSuperset)), equalTo((SignificanceHeuristic) new ChiSquare(includeNegatives, backgroundIsSuperset)));
+        assertTrue(parseFromBuilder(heuristicParserMapper, searchContext, new JLHScore.JLHScoreBuilder()) instanceof JLHScore);
+        assertTrue(parseFromBuilder(heuristicParserMapper, searchContext, new GND.GNDBuilder(backgroundIsSuperset)) instanceof GND);
+        assertThat(parseFromBuilder(heuristicParserMapper, searchContext, new MutualInformation.MutualInformationBuilder(includeNegatives, backgroundIsSuperset)), equalTo((SignificanceHeuristic) new MutualInformation(includeNegatives, backgroundIsSuperset)));
+        assertThat(parseFromBuilder(heuristicParserMapper, searchContext, new ChiSquare.ChiSquareBuilder(includeNegatives, backgroundIsSuperset)), equalTo((SignificanceHeuristic) new ChiSquare(includeNegatives, backgroundIsSuperset)));
 
         // test exceptions
         String faultyHeuristicdefinition = "\"mutual_information\":{\"include_negatives\": false, \"some_unknown_field\": false}";
@@ -232,41 +227,29 @@ public class SignificanceHeuristicTests extends ESTestCase {
         checkParseException(heuristicParserMapper, searchContext, faultyHeuristicdefinition, expectedError);
     }
 
-    protected void checkParseException(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext,
-            String faultyHeuristicDefinition, String expectedError) throws IOException {
-
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, new HashMap<String, QueryParser<?>>());
+    protected void checkParseException(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext, String faultyHeuristicDefinition, String expectedError) throws IOException {
         try {
             XContentParser stParser = JsonXContent.jsonXContent.createParser("{\"field\":\"text\", " + faultyHeuristicDefinition + ",\"min_doc_count\":200}");
-            QueryParseContext parseContext = new QueryParseContext(registry);
-            parseContext.reset(stParser);
-            parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
             stParser.nextToken();
-            new SignificantTermsParser(heuristicParserMapper, registry).parse("testagg", stParser, parseContext);
+            new SignificantTermsParser(heuristicParserMapper).parse("testagg", stParser, searchContext);
             fail();
         } catch (ElasticsearchParseException e) {
             assertTrue(e.getMessage().contains(expectedError));
         }
     }
 
-    protected SignificanceHeuristic parseFromBuilder(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext, SignificanceHeuristic significanceHeuristic) throws IOException {
-        SignificantTermsAggregatorFactory stBuilder = significantTerms("testagg");
-        stBuilder.significanceHeuristic(significanceHeuristic).field("text").minDocCount(200);
+    protected SignificanceHeuristic parseFromBuilder(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext, SignificanceHeuristicBuilder significanceHeuristicBuilder) throws IOException {
+        SignificantTermsBuilder stBuilder = new SignificantTermsBuilder("testagg");
+        stBuilder.significanceHeuristic(significanceHeuristicBuilder).field("text").minDocCount(200);
         XContentBuilder stXContentBuilder = XContentFactory.jsonBuilder();
         stBuilder.internalXContent(stXContentBuilder, null);
         XContentParser stParser = JsonXContent.jsonXContent.createParser(stXContentBuilder.string());
         return parseSignificanceHeuristic(heuristicParserMapper, searchContext, stParser);
     }
 
-    private SignificanceHeuristic parseSignificanceHeuristic(SignificanceHeuristicParserMapper heuristicParserMapper,
-            SearchContext searchContext, XContentParser stParser) throws IOException {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, new HashMap<String, QueryParser<?>>());
-        QueryParseContext parseContext = new QueryParseContext(registry);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
+    private SignificanceHeuristic parseSignificanceHeuristic(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext, XContentParser stParser) throws IOException {
         stParser.nextToken();
-        SignificantTermsAggregatorFactory aggregatorFactory = (SignificantTermsAggregatorFactory) new SignificantTermsParser(
-                heuristicParserMapper, registry).parse("testagg", stParser, parseContext);
+        SignificantTermsAggregatorFactory aggregatorFactory = (SignificantTermsAggregatorFactory) new SignificantTermsParser(heuristicParserMapper).parse("testagg", stParser, searchContext);
         stParser.nextToken();
         assertThat(aggregatorFactory.getBucketCountThresholds().getMinDocCount(), equalTo(200l));
         assertThat(stParser.currentToken(), equalTo(null));
@@ -382,14 +365,14 @@ public class SignificanceHeuristicTests extends ESTestCase {
         testBackgroundAssertions(new MutualInformation(true, true), new MutualInformation(true, false));
         testBackgroundAssertions(new ChiSquare(true, true), new ChiSquare(true, false));
         testBackgroundAssertions(new GND(true), new GND(false));
-        testAssertions(PercentageScore.PROTOTYPE);
-        testAssertions(JLHScore.PROTOTYPE);
+        testAssertions(PercentageScore.INSTANCE);
+        testAssertions(JLHScore.INSTANCE);
     }
 
     public void testBasicScoreProperties() {
-        basicScoreProperties(JLHScore.PROTOTYPE, true);
+        basicScoreProperties(JLHScore.INSTANCE, true);
         basicScoreProperties(new GND(true), true);
-        basicScoreProperties(PercentageScore.PROTOTYPE, true);
+        basicScoreProperties(PercentageScore.INSTANCE, true);
         basicScoreProperties(new MutualInformation(true, true), false);
         basicScoreProperties(new ChiSquare(true, true), false);
     }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AbstractNumericMetricTestCase.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AbstractNumericMetricTestCase.java
deleted file mode 100644
index f51a1a8..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AbstractNumericMetricTestCase.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-
-public abstract class AbstractNumericMetricTestCase<AF extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, AF>>
-        extends BaseAggregationTestCase<AF> {
-
-    @Override
-    protected final AF createTestAggregatorFactory() {
-        AF factory = doCreateTestAggregatorFactory();
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-    protected abstract AF doCreateTestAggregatorFactory();
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgTests.java
deleted file mode 100644
index fa51fb2..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.avg.AvgAggregator;
-
-public class AvgTests extends AbstractNumericMetricTestCase<AvgAggregator.Factory> {
-
-    @Override
-    protected AvgAggregator.Factory doCreateTestAggregatorFactory() {
-        return new AvgAggregator.Factory("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ExtendedStatsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ExtendedStatsTests.java
deleted file mode 100644
index 504254d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ExtendedStatsTests.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStatsAggregator;
-
-public class ExtendedStatsTests extends AbstractNumericMetricTestCase<ExtendedStatsAggregator.Factory> {
-
-    @Override
-    protected ExtendedStatsAggregator.Factory doCreateTestAggregatorFactory() {
-        ExtendedStatsAggregator.Factory factory = new ExtendedStatsAggregator.Factory("foo");
-        if (randomBoolean()) {
-            factory.sigma(randomDoubleBetween(0.0, 10.0, true));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FilterTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FilterTests.java
deleted file mode 100644
index 1072cdb..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FilterTests.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator;
-import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator.Factory;
-
-public class FilterTests extends BaseAggregationTestCase<FilterAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        Factory factory = new Factory(randomAsciiOfLengthBetween(1, 20),
-                QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20)));
-        // NORELEASE make RandomQueryBuilder work outside of the
-        // AbstractQueryTestCase
-        // builder.query(RandomQueryBuilder.createQuery(getRandom()));
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FiltersTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FiltersTests.java
deleted file mode 100644
index 6e0abf3..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FiltersTests.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.Factory;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.KeyedFilter;
-
-public class FiltersTests extends BaseAggregationTestCase<FiltersAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-
-        int size = randomIntBetween(1, 20);
-        Factory factory;
-        if (randomBoolean()) {
-            KeyedFilter[] filters = new KeyedFilter[size];
-            for (int i = 0; i < size; i++) {
-                // NORELEASE make RandomQueryBuilder work outside of the
-                // AbstractQueryTestCase
-                // builder.query(RandomQueryBuilder.createQuery(getRandom()));
-                filters[i] = new KeyedFilter(randomAsciiOfLengthBetween(1, 20),
-                        QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20)));
-            }
-            factory = new Factory(randomAsciiOfLengthBetween(1, 20), filters);
-        } else {
-            QueryBuilder<?>[] filters = new QueryBuilder<?>[size];
-            for (int i = 0; i < size; i++) {
-                // NORELEASE make RandomQueryBuilder work outside of the
-                // AbstractQueryTestCase
-                // builder.query(RandomQueryBuilder.createQuery(getRandom()));
-                filters[i] = QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20));
-            }
-            factory = new Factory(randomAsciiOfLengthBetween(1, 20), filters);
-        }
-        if (randomBoolean()) {
-            factory.otherBucket(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.otherBucketKey(randomAsciiOfLengthBetween(1, 20));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoBoundsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoBoundsTests.java
deleted file mode 100644
index e4bbffe..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoBoundsTests.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsAggregator;
-import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsAggregator.Factory;
-
-public class GeoBoundsTests extends BaseAggregationTestCase<GeoBoundsAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        Factory factory = new Factory(randomAsciiOfLengthBetween(1, 20));
-        String field = randomAsciiOfLengthBetween(3, 20);
-        factory.field(field);
-        if (randomBoolean()) {
-            factory.wrapLongitude(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing("0,0");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidTests.java
deleted file mode 100644
index 28c426a..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidTests.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator;
-import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator.Factory;
-
-public class GeoCentroidTests extends BaseAggregationTestCase<GeoCentroidAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        Factory factory = new Factory(randomAsciiOfLengthBetween(1, 20));
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("0,0");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MaxTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MaxTests.java
deleted file mode 100644
index cff4888..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MaxTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.max.MaxAggregator;
-
-public class MaxTests extends AbstractNumericMetricTestCase<MaxAggregator.Factory> {
-
-    @Override
-    protected MaxAggregator.Factory doCreateTestAggregatorFactory() {
-        return new MaxAggregator.Factory("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MinTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MinTests.java
deleted file mode 100644
index b24ccca..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MinTests.java
+++ /dev/null
@@ -1,32 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.min.MinAggregator;
-import org.elasticsearch.search.aggregations.metrics.min.MinAggregator.Factory;
-
-public class MinTests extends AbstractNumericMetricTestCase<MinAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory() {
-        return new MinAggregator.Factory("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MissingTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MissingTests.java
deleted file mode 100644
index 16f5aaf..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MissingTests.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.missing.MissingAggregator;
-
-public class MissingTests extends BaseAggregationTestCase<MissingAggregator.Factory> {
-
-    @Override
-    protected final MissingAggregator.Factory createTestAggregatorFactory() {
-        MissingAggregator.Factory factory = new MissingAggregator.Factory("foo", null);
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentileRanksTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentileRanksTests.java
deleted file mode 100644
index 37b7fa2..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentileRanksTests.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksAggregatorFactory;
-
-public class PercentileRanksTests extends BaseAggregationTestCase<PercentileRanksAggregatorFactory> {
-
-    @Override
-    protected PercentileRanksAggregatorFactory createTestAggregatorFactory() {
-        PercentileRanksAggregatorFactory factory = new PercentileRanksAggregatorFactory(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        int valuesSize = randomIntBetween(1, 20);
-        double[] values = new double[valuesSize];
-        for (int i = 0; i < valuesSize; i++) {
-            values[i] = randomDouble() * 100;
-        }
-        factory.values(values);
-        if (randomBoolean()) {
-            factory.numberOfSignificantValueDigits(randomIntBetween(0, 5));
-        }
-        if (randomBoolean()) {
-            factory.compression(randomIntBetween(1, 50000));
-        }
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentilesTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentilesTests.java
deleted file mode 100644
index 10b8581..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentilesTests.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesAggregatorFactory;
-
-public class PercentilesTests extends BaseAggregationTestCase<PercentilesAggregatorFactory> {
-
-    @Override
-    protected PercentilesAggregatorFactory createTestAggregatorFactory() {
-        PercentilesAggregatorFactory factory = new PercentilesAggregatorFactory(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            int percentsSize = randomIntBetween(1, 20);
-            double[] percents = new double[percentsSize];
-            for (int i = 0; i < percentsSize; i++) {
-                percents[i] = randomDouble() * 100;
-            }
-            factory.percentiles(percents);
-        }
-        if (randomBoolean()) {
-            factory.numberOfSignificantValueDigits(randomIntBetween(0, 5));
-        }
-        if (randomBoolean()) {
-            factory.compression(randomIntBetween(1, 50000));
-        }
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            factory.format("###.00");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricTests.java
deleted file mode 100644
index be0aec6..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricTests.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator;
-import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.Factory;
-
-import java.util.HashMap;
-import java.util.Map;
-
-public class ScriptedMetricTests extends BaseAggregationTestCase<ScriptedMetricAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        Factory factory = new Factory(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.initScript(randomScript("initScript"));
-        }
-        factory.mapScript(randomScript("mapScript"));
-        if (randomBoolean()) {
-            factory.combineScript(randomScript("combineScript"));
-        }
-        if (randomBoolean()) {
-            factory.reduceScript(randomScript("reduceScript"));
-        }
-        if (randomBoolean()) {
-            Map<String, Object> params = new HashMap<String, Object>();
-            params.put("foo", "bar");
-            factory.params(params);
-        }
-        return factory;
-    }
-
-    private Script randomScript(String script) {
-        if (randomBoolean()) {
-            return new Script(script);
-        } else {
-            return new Script(script, randomFrom(ScriptType.values()), randomFrom("my_lang", null), null);
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/StatsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/StatsTests.java
deleted file mode 100644
index a09958d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/StatsTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.stats.StatsAggregator;
-
-public class StatsTests extends AbstractNumericMetricTestCase<StatsAggregator.Factory> {
-
-    @Override
-    protected StatsAggregator.Factory doCreateTestAggregatorFactory() {
-        return new StatsAggregator.Factory("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumTests.java
deleted file mode 100644
index 0d8d61e..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.sum.SumAggregator;
-
-public class SumTests extends AbstractNumericMetricTestCase<SumAggregator.Factory> {
-
-    @Override
-    protected SumAggregator.Factory doCreateTestAggregatorFactory() {
-        return new SumAggregator.Factory("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
index 6c1ff6a..65e71fe 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
@@ -256,7 +256,7 @@ public class TopHitsIT extends ESIntegTestCase {
                         .executionHint(randomExecutionHint())
                         .field(TERMS_AGGS_FIELD)
                         .subAggregation(
-                                topHits("hits").sort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
+                                topHits("hits").addSort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
                         )
                 )
                 .get();
@@ -352,7 +352,7 @@ public class TopHitsIT extends ESIntegTestCase {
                         .executionHint(randomExecutionHint())
                         .collectMode(SubAggCollectionMode.BREADTH_FIRST)
                         .field(TERMS_AGGS_FIELD)
-                        .subAggregation(topHits("hits").size(3))
+                        .subAggregation(topHits("hits").setSize(3))
                 ).get();
 
         assertSearchResponse(response);
@@ -403,9 +403,9 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .executionHint(randomExecutionHint())
                                 .field(TERMS_AGGS_FIELD)
                                 .subAggregation(
-                                        topHits("hits").sort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
-                                                .from(from)
-                                                .size(size)
+                                        topHits("hits").addSort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
+                                                .setFrom(from)
+                                                .setSize(size)
                                 )
                 )
                 .get();
@@ -447,7 +447,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .field(TERMS_AGGS_FIELD)
                                 .order(Terms.Order.aggregation("max_sort", false))
                                 .subAggregation(
-                                        topHits("hits").sort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC)).trackScores(true)
+                                        topHits("hits").addSort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC)).setTrackScores(true)
                                 )
                                 .subAggregation(
                                         max("max_sort").field(SORT_FIELD)
@@ -487,7 +487,7 @@ public class TopHitsIT extends ESIntegTestCase {
                 .setQuery(matchQuery("text", "term rare"))
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field("group")
-                                .order(Terms.Order.aggregation("max_score", false)).subAggregation(topHits("hits").size(1))
+                                .order(Terms.Order.aggregation("max_score", false)).subAggregation(topHits("hits").setSize(1))
                                 .subAggregation(max("max_score").field("value"))).get();
         assertSearchResponse(response);
 
@@ -529,14 +529,14 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .executionHint(randomExecutionHint())
                                 .field(TERMS_AGGS_FIELD)
                                 .subAggregation(
-                                        topHits("hits").size(1)
+                                        topHits("hits").setSize(1)
                                             .highlighter(new HighlightBuilder().field("text"))
-                                            .explain(true)
-                                            .field("text")
-                                            .fieldDataField("field1")
-                                            .scriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()))
-                                            .fetchSource("text", null)
-                                            .version(true)
+                                            .setExplain(true)
+                                            .addField("text")
+                                            .addFieldDataField("field1")
+                                            .addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()))
+                                            .setFetchSource("text", null)
+                                            .setVersion(true)
                                 )
                 )
                 .get();
@@ -586,7 +586,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                     .executionHint(randomExecutionHint())
                                     .field(TERMS_AGGS_FIELD)
                                     .subAggregation(
-                                            topHits("hits").sort(SortBuilders.fieldSort("xyz").order(SortOrder.DESC))
+                                            topHits("hits").addSort(SortBuilders.fieldSort("xyz").order(SortOrder.DESC))
                                     )
                     ).get();
             fail();
@@ -650,9 +650,9 @@ public class TopHitsIT extends ESIntegTestCase {
                                     .field("group")
                                     .subAggregation(
                                             topHits("hits")
-                                                    .trackScores(trackScore)
-                                                    .size(1)
-                                                    .sort("_id", SortOrder.DESC)
+                                                    .setTrackScores(trackScore)
+                                                    .setSize(1)
+                                                    .addSort("_id", SortOrder.DESC)
                                     )
                     )
                     .get();
@@ -690,12 +690,13 @@ public class TopHitsIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("articles")
                 .setQuery(matchQuery("title", "title"))
                 .addAggregation(
-                        nested("to-comments", "comments")
+                        nested("to-comments")
+                                .path("comments")
                                 .subAggregation(
                                         terms("users")
                                                 .field("comments.user")
                                                 .subAggregation(
-                                                        topHits("top-comments").sort("comments.date", SortOrder.ASC)
+                                                        topHits("top-comments").addSort("comments.date", SortOrder.ASC)
                                                 )
                                 )
                 )
@@ -740,14 +741,15 @@ public class TopHitsIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("articles")
                 .setQuery(matchQuery("title", "title"))
                 .addAggregation(
-                        nested("to-comments", "comments")
+                        nested("to-comments")
+                                .path("comments")
                                 .subAggregation(
-                                    nested("to-reviewers", "comments.reviewers").subAggregation(
+                                    nested("to-reviewers").path("comments.reviewers").subAggregation(
                                             // Also need to sort on _doc because there are two reviewers with the same name
-                                            topHits("top-reviewers").sort("comments.reviewers.name", SortOrder.ASC).sort("_doc", SortOrder.DESC).size(7)
+                                            topHits("top-reviewers").addSort("comments.reviewers.name", SortOrder.ASC).addSort("_doc", SortOrder.DESC).setSize(7)
                                     )
                                 )
-                                .subAggregation(topHits("top-comments").sort("comments.date", SortOrder.DESC).size(4))
+                                .subAggregation(topHits("top-comments").addSort("comments.date", SortOrder.DESC).setSize(4))
                 ).get();
         assertNoFailures(searchResponse);
 
@@ -846,11 +848,11 @@ public class TopHitsIT extends ESIntegTestCase {
                 .prepareSearch("articles")
                 .setQuery(nestedQuery("comments", matchQuery("comments.message", "comment").queryName("test")))
                 .addAggregation(
-                        nested("to-comments", "comments").subAggregation(
-                                topHits("top-comments").size(1).highlighter(new HighlightBuilder().field(hlField)).explain(true)
-                                                .fieldDataField("comments.user")
-                                        .scriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap())).fetchSource("message", null)
-                                        .version(true).sort("comments.date", SortOrder.ASC))).get();
+                        nested("to-comments").path("comments").subAggregation(
+                                topHits("top-comments").setSize(1).highlighter(new HighlightBuilder().field(hlField)).setExplain(true)
+                                                .addFieldDataField("comments.user")
+                                        .addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap())).setFetchSource("message", null)
+                                        .setVersion(true).addSort("comments.date", SortOrder.ASC))).get();
         assertHitCount(searchResponse, 2);
         Nested nested = searchResponse.getAggregations().get("to-comments");
         assertThat(nested.getDocCount(), equalTo(4l));
@@ -895,10 +897,11 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .interval(5)
                                 .order(Histogram.Order.aggregation("to-comments", true))
                                 .subAggregation(
-                                        nested("to-comments", "comments")
+                                        nested("to-comments")
+                                                .path("comments")
                                                 .subAggregation(topHits("comments")
                                                         .highlighter(new HighlightBuilder().field(new HighlightBuilder.Field("comments.message").highlightQuery(matchQuery("comments.message", "text"))))
-                                                        .sort("comments.id", SortOrder.ASC))
+                                                        .addSort("comments.id", SortOrder.ASC))
                                 )
                 )
                 .get();
@@ -935,7 +938,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .executionHint(randomExecutionHint())
                                 .field(TERMS_AGGS_FIELD)
                                 .subAggregation(
-                                        topHits("hits").size(ArrayUtil.MAX_ARRAY_LENGTH - 1).sort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
+                                        topHits("hits").setSize(ArrayUtil.MAX_ARRAY_LENGTH - 1).addSort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
                                 )
                 )
                 .get();
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsTests.java
deleted file mode 100644
index f53c196..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsTests.java
+++ /dev/null
@@ -1,146 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.index.query.AbstractQueryTestCase;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.tophits.TopHitsAggregator;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
-import org.elasticsearch.search.highlight.HighlightBuilderTests;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
-
-import java.util.ArrayList;
-import java.util.List;;
-
-public class TopHitsTests extends BaseAggregationTestCase<TopHitsAggregator.Factory> {
-
-    @Override
-    protected final TopHitsAggregator.Factory createTestAggregatorFactory() {
-        TopHitsAggregator.Factory factory = new TopHitsAggregator.Factory("foo");
-        if (randomBoolean()) {
-            factory.from(randomIntBetween(0, 10000));
-        }
-        if (randomBoolean()) {
-            factory.size(randomIntBetween(0, 10000));
-        }
-        if (randomBoolean()) {
-            factory.explain(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.version(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.trackScores(randomBoolean());
-        }
-        if (randomBoolean()) {
-            int fieldsSize = randomInt(25);
-            List<String> fields = new ArrayList<>(fieldsSize);
-            for (int i = 0; i < fieldsSize; i++) {
-                fields.add(randomAsciiOfLengthBetween(5, 50));
-            }
-            factory.fields(fields);
-        }
-        if (randomBoolean()) {
-            int fieldDataFieldsSize = randomInt(25);
-            for (int i = 0; i < fieldDataFieldsSize; i++) {
-                factory.fieldDataField(randomAsciiOfLengthBetween(5, 50));
-            }
-        }
-        if (randomBoolean()) {
-            int scriptFieldsSize = randomInt(25);
-            for (int i = 0; i < scriptFieldsSize; i++) {
-                if (randomBoolean()) {
-                    factory.scriptField(randomAsciiOfLengthBetween(5, 50), new Script("foo"), randomBoolean());
-                } else {
-                    factory.scriptField(randomAsciiOfLengthBetween(5, 50), new Script("foo"));
-                }
-            }
-        }
-        if (randomBoolean()) {
-            FetchSourceContext fetchSourceContext;
-            int branch = randomInt(5);
-            String[] includes = new String[randomIntBetween(0, 20)];
-            for (int i = 0; i < includes.length; i++) {
-                includes[i] = randomAsciiOfLengthBetween(5, 20);
-            }
-            String[] excludes = new String[randomIntBetween(0, 20)];
-            for (int i = 0; i < excludes.length; i++) {
-                excludes[i] = randomAsciiOfLengthBetween(5, 20);
-            }
-            switch (branch) {
-            case 0:
-                fetchSourceContext = new FetchSourceContext(randomBoolean());
-                break;
-            case 1:
-                fetchSourceContext = new FetchSourceContext(includes, excludes);
-                break;
-            case 2:
-                fetchSourceContext = new FetchSourceContext(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20));
-                break;
-            case 3:
-                fetchSourceContext = new FetchSourceContext(true, includes, excludes);
-                break;
-            case 4:
-                fetchSourceContext = new FetchSourceContext(includes);
-                break;
-            case 5:
-                fetchSourceContext = new FetchSourceContext(randomAsciiOfLengthBetween(5, 20));
-                break;
-            default:
-                throw new IllegalStateException();
-            }
-            factory.fetchSource(fetchSourceContext);
-        }
-        if (randomBoolean()) {
-            int numSorts = randomIntBetween(1, 5);
-            for (int i = 0; i < numSorts; i++) {
-                int branch = randomInt(5);
-                switch (branch) {
-                case 0:
-                    factory.sort(SortBuilders.fieldSort(randomAsciiOfLengthBetween(5, 20)).order(randomFrom(SortOrder.values())));
-                    break;
-                case 1:
-                    factory.sort(SortBuilders.geoDistanceSort(randomAsciiOfLengthBetween(5, 20))
-                            .geohashes(AbstractQueryTestCase.randomGeohash(1, 12)).order(randomFrom(SortOrder.values())));
-                    break;
-                case 2:
-                    factory.sort(SortBuilders.scoreSort().order(randomFrom(SortOrder.values())));
-                    break;
-                case 3:
-                    factory.sort(SortBuilders.scriptSort(new Script("foo"), "number").order(randomFrom(SortOrder.values())));
-                    break;
-                case 4:
-                    factory.sort(randomAsciiOfLengthBetween(5, 20));
-                    break;
-                case 5:
-                    factory.sort(randomAsciiOfLengthBetween(5, 20), randomFrom(SortOrder.values()));
-                    break;
-                }
-            }
-        }
-        if (randomBoolean()) {
-            factory.highlighter(HighlightBuilderTests.randomHighlighterBuilder());
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountTests.java
deleted file mode 100644
index b400bff..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountTests.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCountAggregator;
-
-public class ValueCountTests extends BaseAggregationTestCase<ValueCountAggregator.Factory> {
-
-    @Override
-    protected final ValueCountAggregator.Factory createTestAggregatorFactory() {
-        ValueCountAggregator.Factory factory = new ValueCountAggregator.Factory("foo", null);
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityTests.java
deleted file mode 100644
index 5b2bbd1..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityTests.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.cardinality;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-
-public class CardinalityTests extends BaseAggregationTestCase<CardinalityAggregatorFactory> {
-
-    @Override
-    protected final CardinalityAggregatorFactory createTestAggregatorFactory() {
-        CardinalityAggregatorFactory factory = new CardinalityAggregatorFactory("foo", null);
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethodTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethodTests.java
deleted file mode 100644
index eb08e6f..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethodTests.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class PercentilesMethodTests extends ESTestCase {
-
-    public void testValidOrdinals() {
-        assertThat(PercentilesMethod.TDIGEST.ordinal(), equalTo(0));
-        assertThat(PercentilesMethod.HDR.ordinal(), equalTo(1));
-    }
-
-    public void testwriteTo() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            PercentilesMethod.TDIGEST.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(0));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            PercentilesMethod.HDR.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(1));
-            }
-        }
-    }
-
-    public void testReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(0);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(PercentilesMethod.TDIGEST.readFrom(in), equalTo(PercentilesMethod.TDIGEST));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(1);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(PercentilesMethod.TDIGEST.readFrom(in), equalTo(PercentilesMethod.HDR));
-            }
-        }
-    }
-
-    public void testInvalidReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(randomIntBetween(2, Integer.MAX_VALUE));
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                PercentilesMethod.TDIGEST.readFrom(in);
-                fail("Expected IOException");
-            } catch(IOException e) {
-                assertThat(e.getMessage(), containsString("Unknown PercentilesMethod ordinal ["));
-            }
-
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/AvgBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/AvgBucketIT.java
index 074ad41..ea0eb7f 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/AvgBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/AvgBucketIT.java
@@ -21,12 +21,10 @@ package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -94,8 +92,8 @@ public class AvgBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(avgBucket("avg_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(avgBucket("avg_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -132,8 +130,8 @@ public class AvgBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .subAggregation(avgBucket("avg_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(avgBucket("avg_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -175,7 +173,7 @@ public class AvgBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(avgBucket("avg_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(avgBucket("avg_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -214,9 +212,9 @@ public class AvgBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .subAggregation(avgBucket("avg_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(avgBucket("avg_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -267,9 +265,9 @@ public class AvgBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .subAggregation(avgBucket("avg_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(avgBucket("avg_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -313,9 +311,8 @@ public class AvgBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(avgBucket("avg_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(avgBucket("avg_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -340,9 +337,9 @@ public class AvgBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .subAggregation(avgBucket("avg_histo_bucket", "histo>_count")))
-                .addAggregation(avgBucket("avg_terms_bucket", "terms>avg_histo_bucket")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(avgBucket("avg_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(avgBucket("avg_terms_bucket").setBucketsPaths("terms>avg_histo_bucket")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptTests.java
deleted file mode 100644
index d9b5496..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptTests.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptPipelineAggregator.Factory;
-
-import java.util.HashMap;
-import java.util.Map;
-
-public class BucketScriptTests extends BasePipelineAggregationTestCase<BucketScriptPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        Map<String, String> bucketsPaths = new HashMap<>();
-        int numBucketPaths = randomIntBetween(1, 10);
-        for (int i = 0; i < numBucketPaths; i++) {
-            bucketsPaths.put(randomAsciiOfLengthBetween(1, 20), randomAsciiOfLengthBetween(1, 40));
-        }
-        Script script;
-        if (randomBoolean()) {
-            script = new Script("script");
-        } else {
-            Map<String, Object> params = null;
-            if (randomBoolean()) {
-                params = new HashMap<String, Object>();
-                params.put("foo", "bar");
-            }
-            script = new Script("script", randomFrom(ScriptType.values()), randomFrom("my_lang", null), params);
-        }
-        Factory factory = new Factory(name, bucketsPaths, script);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorTests.java
deleted file mode 100644
index 3d5f4d1..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorTests.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.having.BucketSelectorPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.having.BucketSelectorPipelineAggregator.Factory;
-
-import java.util.HashMap;
-import java.util.Map;
-
-public class BucketSelectorTests extends BasePipelineAggregationTestCase<BucketSelectorPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        Map<String, String> bucketsPaths = new HashMap<>();
-        int numBucketPaths = randomIntBetween(1, 10);
-        for (int i = 0; i < numBucketPaths; i++) {
-            bucketsPaths.put(randomAsciiOfLengthBetween(1, 20), randomAsciiOfLengthBetween(1, 40));
-        }
-        Script script;
-        if (randomBoolean()) {
-            script = new Script("script");
-        } else {
-            Map<String, Object> params = null;
-            if (randomBoolean()) {
-                params = new HashMap<String, Object>();
-                params.put("foo", "bar");
-            }
-            script = new Script("script", randomFrom(ScriptType.values()), randomFrom("my_lang", null), params);
-        }
-        Factory factory = new Factory(name, bucketsPaths, script);
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        return factory;
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumIT.java
index 906aa3d..6f10e5d 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumIT.java
@@ -21,7 +21,6 @@ package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
@@ -89,8 +88,8 @@ public class CumulativeSumIT extends ESIntegTestCase {
     public void testDocCount() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
-                                .subAggregation(cumulativeSum("cumulative_sum", "_count"))).execute().actionGet();
+                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
+                                .subAggregation(cumulativeSum("cumulative_sum").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -120,9 +119,9 @@ public class CumulativeSumIT extends ESIntegTestCase {
                 .prepareSearch("idx")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(cumulativeSum("cumulative_sum", "sum"))).execute().actionGet();
+                                .subAggregation(cumulativeSum("cumulative_sum").setBucketsPaths("sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -154,7 +153,7 @@ public class CumulativeSumIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(cumulativeSum("cumulative_sum", "sum"))).execute().actionGet();
+                                .subAggregation(cumulativeSum("cumulative_sum").setBucketsPaths("sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumTests.java
deleted file mode 100644
index 0e0aa2d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumTests.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.cumulativesum.CumulativeSumPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.cumulativesum.CumulativeSumPipelineAggregator.Factory;
-
-public class CumulativeSumTests extends BasePipelineAggregationTestCase<CumulativeSumPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String bucketsPath = randomAsciiOfLengthBetween(3, 20);
-        Factory factory = new Factory(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java
index e5b816d..3058d1f 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java
@@ -108,8 +108,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -150,8 +150,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count").unit(DateHistogramInterval.DAY))).execute()
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count").unit(DateHistogramInterval.DAY))).execute()
                 .actionGet();
 
         assertSearchResponse(response);
@@ -195,8 +195,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                            .subAggregation(sum("sum").field("value")).subAggregation(derivative("deriv", "sum")))
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("sum")).subAggregation(sum("sum").field("value")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -264,8 +264,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
-                        dateHistogram("histo").field("dates").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                        dateHistogram("histo").field("dates").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -319,8 +319,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx_unmapped")
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -334,8 +334,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx", "idx_unmapped")
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java
index f1ad081..d76f88a 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java
@@ -19,12 +19,13 @@
 
 package org.elasticsearch.search.aggregations.pipeline;
 
+import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.Bucket;
@@ -171,8 +172,8 @@ public class DerivativeIT extends ESIntegTestCase {
                 .prepareSearch("idx")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .subAggregation(derivative("deriv", "_count"))
-                                .subAggregation(derivative("2nd_deriv", "deriv"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))
+                                .subAggregation(derivative("2nd_deriv").setBucketsPaths("deriv"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -210,8 +211,8 @@ public class DerivativeIT extends ESIntegTestCase {
                 .prepareSearch("idx")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count").unit("1ms"))
-                                .subAggregation(derivative("2nd_deriv", "deriv").unit("10ms"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count").unit("1ms"))
+                                .subAggregation(derivative("2nd_deriv").setBucketsPaths("deriv").unit("10ms"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -249,7 +250,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(derivative("deriv", "sum"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -294,7 +295,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
                                 .subAggregation(stats("stats").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(derivative("deriv", "stats.sum"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("stats.sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -338,7 +339,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .prepareSearch("idx_unmapped")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -353,7 +354,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .prepareSearch("idx", "idx_unmapped")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -382,7 +383,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx));
 
@@ -410,8 +411,8 @@ public class DerivativeIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
-                                .extendedBounds(new ExtendedBounds(0l, (long) numBuckets_empty_rnd - 1))
-                                .subAggregation(derivative("deriv", "_count").gapPolicy(randomFrom(GapPolicy.values()))))
+                                .extendedBounds(0l, (long) numBuckets_empty_rnd - 1)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count").gapPolicy(randomFrom(GapPolicy.values()))))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx_rnd));
@@ -440,7 +441,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
-                                .subAggregation(derivative("deriv", "_count").gapPolicy(GapPolicy.INSERT_ZEROS))).execute()
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count").gapPolicy(GapPolicy.INSERT_ZEROS))).execute()
                                 .actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx));
@@ -470,7 +471,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(derivative("deriv", "sum"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("sum"))).execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx));
 
@@ -511,7 +512,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(derivative("deriv", "sum").gapPolicy(GapPolicy.INSERT_ZEROS))).execute()
+                                .subAggregation(derivative("deriv").setBucketsPaths("sum").gapPolicy(GapPolicy.INSERT_ZEROS))).execute()
                 .actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx));
@@ -549,9 +550,9 @@ public class DerivativeIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
-                                .extendedBounds(new ExtendedBounds(0l, (long) numBuckets_empty_rnd - 1))
+                                .extendedBounds(0l, (long) numBuckets_empty_rnd - 1)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(derivative("deriv", "sum").gapPolicy(gapPolicy))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("sum").gapPolicy(gapPolicy))).execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx_rnd));
 
@@ -593,15 +594,28 @@ public class DerivativeIT extends ESIntegTestCase {
                                     .field(SINGLE_VALUED_FIELD_NAME)
                                     .interval(interval)
                                     .subAggregation(
-                                            filters("filters", QueryBuilders.termQuery("tag", "foo")).subAggregation(
+                                            filters("filters").filter(QueryBuilders.termQuery("tag", "foo")).subAggregation(
                                                     sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                    .subAggregation(derivative("deriv", "filters>get>sum"))).execute().actionGet();
+                                    .subAggregation(derivative("deriv").setBucketsPaths("filters>get>sum"))).execute().actionGet();
             fail("Expected an Exception but didn't get one");
         } catch (Exception e) {
             Throwable cause = ExceptionsHelper.unwrapCause(e);
             if (cause == null) {
                 throw e;
-            } else if (!(cause instanceof IllegalArgumentException)) {
+            } else if (cause instanceof SearchPhaseExecutionException) {
+                ElasticsearchException[] rootCauses = ((SearchPhaseExecutionException) cause).guessRootCauses();
+                // If there is more than one root cause then something
+                // unexpected happened and we should re-throw the original
+                // exception
+                if (rootCauses.length > 1) {
+                    throw e;
+                }
+                ElasticsearchException rootCauseWrapper = rootCauses[0];
+                Throwable rootCause = rootCauseWrapper.getCause();
+                if (rootCause == null || !(rootCause instanceof IllegalArgumentException)) {
+                    throw e;
+                }
+            } else {
                 throw e;
             }
         }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeTests.java
deleted file mode 100644
index 74c58f6..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeTests.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativePipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativePipelineAggregator.Factory;
-
-public class DerivativeTests extends BasePipelineAggregationTestCase<DerivativePipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String bucketsPath = randomAsciiOfLengthBetween(3, 20);
-        Factory factory = new Factory(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                factory.unit(String.valueOf(randomInt()));
-            } else {
-                factory.unit(String.valueOf(randomIntBetween(1, 10) + randomFrom("s", "m", "h", "d", "w", "M", "y")));
-            }
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/ExtendedStatsBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/ExtendedStatsBucketIT.java
index 63a55cf..6c7ae23 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/ExtendedStatsBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/ExtendedStatsBucketIT.java
@@ -22,12 +22,10 @@ package org.elasticsearch.search.aggregations.pipeline;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended.ExtendedStatsBucket;
@@ -96,8 +94,8 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(extendedStatsBucket("extended_stats_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -143,8 +141,8 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(extendedStatsBucket("extended_stats_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -195,7 +193,7 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(extendedStatsBucket("extended_stats_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -243,9 +241,9 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(extendedStatsBucket("extended_stats_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -305,9 +303,9 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(extendedStatsBucket("extended_stats_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -360,9 +358,8 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(extendedStatsBucket("extended_stats_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -388,10 +385,10 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
                                     .order(Order.term(true))
                                     .subAggregation(
                                             histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                    .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                    .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                     .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                    .subAggregation(extendedStatsBucket("extended_stats_bucket", "histo>sum")
-                                            .sigma(-1.0))).execute().actionGet();
+                                    .subAggregation(extendedStatsBucket("extended_stats_bucket")
+                                            .setBucketsPaths("histo>sum").sigma(-1.0))).execute().actionGet();
             fail("Illegal sigma was provided but no exception was thrown.");
         } catch (SearchPhaseExecutionException exception) {
             // All good
@@ -407,9 +404,9 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(extendedStatsBucket("avg_histo_bucket", "histo>_count")))
-                .addAggregation(extendedStatsBucket("avg_terms_bucket", "terms>avg_histo_bucket.avg")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(extendedStatsBucket("avg_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(extendedStatsBucket("avg_terms_bucket").setBucketsPaths("terms>avg_histo_bucket.avg")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketIT.java
index 82ed6b9..81b5735 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketIT.java
@@ -22,12 +22,10 @@ package org.elasticsearch.search.aggregations.pipeline;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.InternalBucketMetricValue;
@@ -98,8 +96,8 @@ public class MaxBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(maxBucket("max_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(maxBucket("max_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -141,8 +139,8 @@ public class MaxBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(maxBucket("max_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(maxBucket("max_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -189,7 +187,7 @@ public class MaxBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(maxBucket("max_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(maxBucket("max_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -233,9 +231,9 @@ public class MaxBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(maxBucket("max_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(maxBucket("max_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -286,12 +284,13 @@ public class MaxBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
-                        filter("filter", termQuery("tag", "tag0"))
+                        filter("filter")
+                                .filter(termQuery("tag", "tag0"))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(maxBucket("max_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(maxBucket("max_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -338,9 +337,9 @@ public class MaxBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(maxBucket("max_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(maxBucket("max_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -388,9 +387,8 @@ public class MaxBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(maxBucket("max_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(maxBucket("max_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -416,9 +414,9 @@ public class MaxBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(maxBucket("max_histo_bucket", "histo>_count")))
-                .addAggregation(maxBucket("max_terms_bucket", "terms>max_histo_bucket")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(maxBucket("max_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(maxBucket("max_terms_bucket").setBucketsPaths("terms>max_histo_bucket")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MinBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MinBucketIT.java
index 3d4ecf1..f02a85f 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MinBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MinBucketIT.java
@@ -21,12 +21,10 @@ package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.InternalBucketMetricValue;
@@ -95,8 +93,8 @@ public class MinBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(minBucket("min_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(minBucket("min_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -138,8 +136,8 @@ public class MinBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(minBucket("min_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(minBucket("min_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -186,7 +184,7 @@ public class MinBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(minBucket("min_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(minBucket("min_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -230,9 +228,9 @@ public class MinBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(minBucket("min_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(minBucket("min_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -288,9 +286,9 @@ public class MinBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(minBucket("min_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(minBucket("min_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -338,9 +336,8 @@ public class MinBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(minBucket("min_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(minBucket("min_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -366,9 +363,9 @@ public class MinBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(minBucket("min_histo_bucket", "histo>_count")))
-                .addAggregation(minBucket("min_terms_bucket", "terms>min_histo_bucket")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(minBucket("min_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(minBucket("min_terms_bucket").setBucketsPaths("terms>min_histo_bucket")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PercentilesBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PercentilesBucketIT.java
index 4f03cce..c4dc267 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PercentilesBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PercentilesBucketIT.java
@@ -23,10 +23,8 @@ import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.percentiles.Percentile;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucket;
@@ -53,7 +51,7 @@ import static org.hamcrest.core.IsNull.notNullValue;
 public class PercentilesBucketIT extends ESIntegTestCase {
 
     private static final String SINGLE_VALUED_FIELD_NAME = "l_value";
-    private static final double[] PERCENTS = {1.0, 25.0, 50.0, 75.0, 99.0};
+    private static final Double[] PERCENTS = {1.0, 25.0, 50.0, 75.0, 99.0};
     static int numDocs;
     static int interval;
     static int minRandomValue;
@@ -98,8 +96,9 @@ public class PercentilesBucketIT extends ESIntegTestCase {
     public void testDocCountopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(percentilesBucket("percentiles_bucket", "histo>_count")
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(percentilesBucket("percentiles_bucket")
+                        .setBucketsPaths("histo>_count")
                         .percents(PERCENTS)).execute().actionGet();
 
         assertSearchResponse(response);
@@ -140,8 +139,9 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(percentilesBucket("percentiles_bucket", "histo>_count")
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(percentilesBucket("percentiles_bucket")
+                                        .setBucketsPaths("histo>_count")
                                         .percents(PERCENTS))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -186,7 +186,8 @@ public class PercentilesBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(percentilesBucket("percentiles_bucket", "terms>sum")
+                .addAggregation(percentilesBucket("percentiles_bucket")
+                        .setBucketsPaths("terms>sum")
                         .percents(PERCENTS)).execute().actionGet();
 
         assertSearchResponse(response);
@@ -223,7 +224,8 @@ public class PercentilesBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(percentilesBucket("percentiles_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(percentilesBucket("percentiles_bucket")
+                        .setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -265,9 +267,10 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(percentilesBucket("percentiles_bucket", "histo>sum")
+                                .subAggregation(percentilesBucket("percentiles_bucket")
+                                        .setBucketsPaths("histo>sum")
                                         .percents(PERCENTS))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -321,9 +324,10 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(percentilesBucket("percentiles_bucket", "histo>sum")
+                                .subAggregation(percentilesBucket("percentiles_bucket")
+                                        .setBucketsPaths("histo>sum")
                                         .gapPolicy(BucketHelpers.GapPolicy.INSERT_ZEROS)
                                         .percents(PERCENTS)))
                 .execute().actionGet();
@@ -371,9 +375,9 @@ public class PercentilesBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(percentilesBucket("percentiles_bucket", "terms>sum")
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(percentilesBucket("percentiles_bucket")
+                        .setBucketsPaths("terms>sum")
                         .percents(PERCENTS)).execute().actionGet();
 
         assertSearchResponse(response);
@@ -394,9 +398,9 @@ public class PercentilesBucketIT extends ESIntegTestCase {
 
     public void testWrongPercents() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(percentilesBucket("percentiles_bucket", "terms>sum")
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(percentilesBucket("percentiles_bucket")
+                        .setBucketsPaths("terms>sum")
                         .percents(PERCENTS)).execute().actionGet();
 
         assertSearchResponse(response);
@@ -420,12 +424,13 @@ public class PercentilesBucketIT extends ESIntegTestCase {
     }
 
     public void testBadPercents() throws Exception {
-        double[] badPercents = {-1.0, 110.0};
+        Double[] badPercents = {-1.0, 110.0};
 
         try {
             client().prepareSearch("idx")
                     .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                    .addAggregation(percentilesBucket("percentiles_bucket", "terms>sum")
+                    .addAggregation(percentilesBucket("percentiles_bucket")
+                            .setBucketsPaths("terms>sum")
                             .percents(badPercents)).execute().actionGet();
 
             fail("Illegal percent's were provided but no exception was thrown.");
@@ -439,7 +444,7 @@ public class PercentilesBucketIT extends ESIntegTestCase {
     }
 
     public void testBadPercents_asSubAgg() throws Exception {
-        double[] badPercents = {-1.0, 110.0};
+        Double[] badPercents = {-1.0, 110.0};
 
         try {
             client()
@@ -450,8 +455,9 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(percentilesBucket("percentiles_bucket", "histo>_count")
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(percentilesBucket("percentiles_bucket")
+                                        .setBucketsPaths("histo>_count")
                                         .percents(badPercents))).execute().actionGet();
 
             fail("Illegal percent's were provided but no exception was thrown.");
@@ -473,9 +479,10 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(percentilesBucket("percentile_histo_bucket", "histo>_count")))
-                .addAggregation(percentilesBucket("percentile_terms_bucket", "terms>percentile_histo_bucket.50")
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(percentilesBucket("percentile_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(percentilesBucket("percentile_terms_bucket")
+                        .setBucketsPaths("terms>percentile_histo_bucket.50")
                         .percents(PERCENTS)).execute().actionGet();
 
         assertSearchResponse(response);
@@ -529,7 +536,7 @@ public class PercentilesBucketIT extends ESIntegTestCase {
     }
 
     public void testNestedWithDecimal() throws Exception {
-        double[] percent = {99.9};
+        Double[] percent = {99.9};
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
@@ -538,10 +545,12 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(percentilesBucket("percentile_histo_bucket", "histo>_count")
-                                        .percents(percent)))
-                .addAggregation(percentilesBucket("percentile_terms_bucket", "terms>percentile_histo_bucket[99.9]")
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(percentilesBucket("percentile_histo_bucket")
+                                        .percents(percent)
+                                        .setBucketsPaths("histo>_count")))
+                .addAggregation(percentilesBucket("percentile_terms_bucket")
+                        .setBucketsPaths("terms>percentile_histo_bucket[99.9]")
                         .percents(percent)).execute().actionGet();
 
         assertSearchResponse(response);
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
index 9a6559a..e962e90 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
@@ -20,11 +20,11 @@
 package org.elasticsearch.search.aggregations.pipeline;
 
 
-import org.elasticsearch.search.aggregations.metrics.avg.AvgAggregator;
-import org.elasticsearch.search.aggregations.metrics.max.MaxAggregator;
-import org.elasticsearch.search.aggregations.metrics.min.MinAggregator;
-import org.elasticsearch.search.aggregations.metrics.sum.SumAggregator;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+import org.elasticsearch.search.aggregations.metrics.avg.AvgBuilder;
+import org.elasticsearch.search.aggregations.metrics.max.MaxBuilder;
+import org.elasticsearch.search.aggregations.metrics.min.MinBuilder;
+import org.elasticsearch.search.aggregations.metrics.sum.SumBuilder;
 import org.elasticsearch.test.ESTestCase;
 
 import java.util.ArrayList;
@@ -109,27 +109,27 @@ public class PipelineAggregationHelperTests extends ESTestCase {
      * @param values Array of values to compute metric for
      * @param metric A metric builder which defines what kind of metric should be returned for the values
      */
-    public static double calculateMetric(double[] values, ValuesSourceAggregatorFactory<?, ?> metric) {
+    public static double calculateMetric(double[] values, ValuesSourceMetricsAggregationBuilder metric) {
 
-        if (metric instanceof MinAggregator.Factory) {
+        if (metric instanceof MinBuilder) {
             double accumulator = Double.POSITIVE_INFINITY;
             for (double value : values) {
                 accumulator = Math.min(accumulator, value);
             }
             return accumulator;
-        } else if (metric instanceof MaxAggregator.Factory) {
+        } else if (metric instanceof MaxBuilder) {
             double accumulator = Double.NEGATIVE_INFINITY;
             for (double value : values) {
                 accumulator = Math.max(accumulator, value);
             }
             return accumulator;
-        } else if (metric instanceof SumAggregator.Factory) {
+        } else if (metric instanceof SumBuilder) {
             double accumulator = 0;
             for (double value : values) {
                 accumulator += value;
             }
             return accumulator;
-        } else if (metric instanceof AvgAggregator.Factory) {
+        } else if (metric instanceof AvgBuilder) {
             double accumulator = 0;
             for (double value : values) {
                 accumulator += value;
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SerialDifferenceTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SerialDifferenceTests.java
deleted file mode 100644
index 2d0306c..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SerialDifferenceTests.java
+++ /dev/null
@@ -1,46 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.serialdiff.SerialDiffPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.serialdiff.SerialDiffPipelineAggregator.Factory;
-
-public class SerialDifferenceTests extends BasePipelineAggregationTestCase<SerialDiffPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String bucketsPath = randomAsciiOfLengthBetween(3, 20);
-        Factory factory = new Factory(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        if (randomBoolean()) {
-            factory.lag(randomIntBetween(1, 1000));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/StatsBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/StatsBucketIT.java
index b60d4cc..92325cc 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/StatsBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/StatsBucketIT.java
@@ -21,12 +21,10 @@ package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.StatsBucket;
@@ -95,8 +93,8 @@ public class StatsBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(statsBucket("stats_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(statsBucket("stats_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -139,8 +137,8 @@ public class StatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(statsBucket("stats_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(statsBucket("stats_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -188,7 +186,7 @@ public class StatsBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(statsBucket("stats_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(statsBucket("stats_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -233,9 +231,9 @@ public class StatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(statsBucket("stats_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(statsBucket("stats_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -292,9 +290,9 @@ public class StatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(statsBucket("stats_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(statsBucket("stats_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -344,9 +342,8 @@ public class StatsBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(statsBucket("stats_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(statsBucket("stats_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -371,9 +368,9 @@ public class StatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(statsBucket("avg_histo_bucket", "histo>_count")))
-                .addAggregation(statsBucket("avg_terms_bucket", "terms>avg_histo_bucket.avg")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(statsBucket("avg_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(statsBucket("avg_terms_bucket").setBucketsPaths("terms>avg_histo_bucket.avg")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SumBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SumBucketIT.java
index fcbbf9b..ba13b55 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SumBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SumBucketIT.java
@@ -21,12 +21,10 @@ package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -94,8 +92,8 @@ public class SumBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(sumBucket("sum_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(sumBucket("sum_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -129,8 +127,8 @@ public class SumBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(sumBucket("sum_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(sumBucket("sum_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -169,7 +167,7 @@ public class SumBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(sumBucket("sum_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(sumBucket("sum_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -205,9 +203,9 @@ public class SumBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(sumBucket("sum_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(sumBucket("sum_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -255,9 +253,9 @@ public class SumBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(sumBucket("sum_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(sumBucket("sum_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -298,9 +296,8 @@ public class SumBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(sumBucket("sum_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(sumBucket("sum_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -325,9 +322,9 @@ public class SumBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(sumBucket("sum_histo_bucket", "histo>_count")))
-                .addAggregation(sumBucket("sum_terms_bucket", "terms>sum_histo_bucket")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(sumBucket("sum_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(sumBucket("sum_terms_bucket").setBucketsPaths("terms>sum_histo_bucket")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AbstractBucketMetricsTestCase.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AbstractBucketMetricsTestCase.java
deleted file mode 100644
index 5da6238..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AbstractBucketMetricsTestCase.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-
-public abstract class AbstractBucketMetricsTestCase<PAF extends BucketMetricsFactory> extends BasePipelineAggregationTestCase<PAF> {
-
-    @Override
-    protected final PAF createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String bucketsPath = randomAsciiOfLengthBetween(3, 20);
-        PAF factory = doCreateTestAggregatorFactory(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        return factory;
-    }
-
-    protected abstract PAF doCreateTestAggregatorFactory(String name, String bucketsPath);
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AvgBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AvgBucketTests.java
deleted file mode 100644
index 112556d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AvgBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg.AvgBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg.AvgBucketPipelineAggregator.Factory;
-
-public class AvgBucketTests extends AbstractBucketMetricsTestCase<AvgBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        return new Factory(name, bucketsPath);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/ExtendedStatsBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/ExtendedStatsBucketTests.java
deleted file mode 100644
index e467273..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/ExtendedStatsBucketTests.java
+++ /dev/null
@@ -1,37 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended.ExtendedStatsBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended.ExtendedStatsBucketPipelineAggregator.Factory;
-
-public class ExtendedStatsBucketTests extends AbstractBucketMetricsTestCase<ExtendedStatsBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        Factory factory = new Factory(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.sigma(randomDoubleBetween(0.0, 10.0, false));
-        }
-        return factory;
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MaxBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MaxBucketTests.java
deleted file mode 100644
index dd35fbb..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MaxBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max.MaxBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max.MaxBucketPipelineAggregator.Factory;
-
-public class MaxBucketTests extends AbstractBucketMetricsTestCase<MaxBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        return new Factory(name, bucketsPath);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MinBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MinBucketTests.java
deleted file mode 100644
index 745e061..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MinBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketPipelineAggregator.Factory;
-
-public class MinBucketTests extends AbstractBucketMetricsTestCase<MinBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        return new Factory(name, bucketsPath);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/PercentilesBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/PercentilesBucketTests.java
deleted file mode 100644
index 200cd26..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/PercentilesBucketTests.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucketPipelineAggregator.Factory;
-
-public class PercentilesBucketTests extends AbstractBucketMetricsTestCase<PercentilesBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        Factory factory = new Factory(name, bucketsPath);
-        if (randomBoolean()) {
-            int numPercents = randomIntBetween(1, 20);
-            double[] percents = new double[numPercents];
-            for (int i = 0; i < numPercents; i++) {
-                percents[i] = randomDoubleBetween(0.0, 100.0, false);
-            }
-            factory.percents(percents);
-        }
-        return factory;
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/StatsBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/StatsBucketTests.java
deleted file mode 100644
index 22a0774..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/StatsBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.StatsBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.StatsBucketPipelineAggregator.Factory;
-
-public class StatsBucketTests extends AbstractBucketMetricsTestCase<StatsBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        return new Factory(name, bucketsPath);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/SumBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/SumBucketTests.java
deleted file mode 100644
index 02448cd..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/SumBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum.SumBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum.SumBucketPipelineAggregator.Factory;
-
-public class SumBucketTests extends AbstractBucketMetricsTestCase<SumBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        return new Factory(name, bucketsPath);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
index e23df88..90d4437 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
@@ -23,10 +23,10 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.collect.EvictingQueue;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.Bucket;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregationHelperTests;
@@ -38,8 +38,6 @@ import org.elasticsearch.search.aggregations.pipeline.movavg.models.HoltWintersM
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.LinearModel;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelBuilder;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.SimpleModel;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
 
@@ -79,7 +77,7 @@ public class MovAvgIT extends ESIntegTestCase {
     static int period;
     static HoltWintersModel.SeasonalityType seasonalityType;
     static BucketHelpers.GapPolicy gapPolicy;
-    static ValuesSourceAggregatorFactory<? extends ValuesSource, ? extends ValuesSourceAggregatorFactory<?, ?>> metric;
+    static ValuesSourceMetricsAggregationBuilder metric;
     static List<PipelineAggregationHelperTests.MockBucket> mockHisto;
 
     static Map<String, ArrayList<Double>> testValues;
@@ -410,16 +408,18 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts","_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                        .gapPolicy(gapPolicy))
-                                .subAggregation(movingAvg("movavg_values","the_metric")
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -458,16 +458,18 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new LinearModel.LinearModelBuilder())
-                                        .gapPolicy(gapPolicy))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new LinearModel.LinearModelBuilder())
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -506,16 +508,18 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new EwmaModel.EWMAModelBuilder().alpha(alpha))
-                                        .gapPolicy(gapPolicy))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new EwmaModel.EWMAModelBuilder().alpha(alpha))
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -554,16 +558,18 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new HoltLinearModel.HoltLinearModelBuilder().alpha(alpha).beta(beta))
-                                        .gapPolicy(gapPolicy))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new HoltLinearModel.HoltLinearModelBuilder().alpha(alpha).beta(beta))
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -602,20 +608,22 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                 .alpha(alpha).beta(beta).gamma(gamma).period(period).seasonalityType(seasonalityType))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(false))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .minimize(false)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                 .alpha(alpha).beta(beta).gamma(gamma).period(period).seasonalityType(seasonalityType))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(false))
+                                        .minimize(false)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -660,8 +668,8 @@ public class MovAvgIT extends ESIntegTestCase {
                                 .interval(1)
                                 .subAggregation(avg("avg").field(VALUE_FIELD))
                                 .subAggregation(
-                                        movingAvg("movavg_values", "avg").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                                .gapPolicy(gapPolicy).predict(5))).execute().actionGet();
+                                        movingAvg("movavg_values").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
+                                                .gapPolicy(gapPolicy).predict(5).setBucketsPaths("avg"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -706,12 +714,13 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                    .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(0)
                                             .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                            .gapPolicy(gapPolicy))
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("the_metric"))
                     ).execute().actionGet();
             fail("MovingAvg should not accept a window that is zero");
         } catch (SearchPhaseExecutionException e) {
@@ -726,10 +735,11 @@ public class MovAvgIT extends ESIntegTestCase {
                     .addAggregation(
                             range("histo").field(INTERVAL_FIELD).addRange(0, 10)
                                     .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                    .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(0)
                                             .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                            .gapPolicy(gapPolicy))
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("the_metric"))
                     ).execute().actionGet();
             fail("MovingAvg should not accept non-histogram as parent");
 
@@ -744,12 +754,13 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                    .subAggregation(movingAvg("movavg_counts", "_count")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(-10)
                                             .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                            .gapPolicy(gapPolicy))
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("_count"))
                     ).execute().actionGet();
             fail("MovingAvg should not accept a window that is negative");
 
@@ -767,10 +778,11 @@ public class MovAvgIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field("test").interval(interval)
                                 .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -789,10 +801,11 @@ public class MovAvgIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field("test").interval(interval)
                                 .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new SimpleModel.SimpleModelBuilder())
                                         .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric")
                                         .predict(numPredictions))
                 ).execute().actionGet();
 
@@ -811,17 +824,18 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                    .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(windowSize)
                                             .modelBuilder(randomModelBuilder())
                                             .gapPolicy(gapPolicy)
-                                            .predict(0))
+                                            .predict(0)
+                                            .setBucketsPaths("the_metric"))
                     ).execute().actionGet();
             fail("MovingAvg should not accept a prediction size that is zero");
 
-        } catch (IllegalArgumentException exception) {
+        } catch (SearchPhaseExecutionException exception) {
             // All Good
         }
     }
@@ -832,17 +846,18 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                    .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(windowSize)
                                             .modelBuilder(randomModelBuilder())
                                             .gapPolicy(gapPolicy)
-                                            .predict(-10))
+                                            .predict(-10)
+                                            .setBucketsPaths("the_metric"))
                     ).execute().actionGet();
             fail("MovingAvg should not accept a prediction size that is negative");
 
-        } catch (IllegalArgumentException exception) {
+        } catch (SearchPhaseExecutionException exception) {
             // All Good
         }
     }
@@ -853,18 +868,20 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(metric)
-                                    .subAggregation(movingAvg("movavg_counts", "_count")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(10)
                                             .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                     .alpha(alpha).beta(beta).gamma(gamma).period(20).seasonalityType(seasonalityType))
-                                            .gapPolicy(gapPolicy))
-                                    .subAggregation(movingAvg("movavg_values", "the_metric")
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("_count"))
+                                    .subAggregation(movingAvg("movavg_values")
                                             .window(windowSize)
                                             .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                     .alpha(alpha).beta(beta).gamma(gamma).period(20).seasonalityType(seasonalityType))
-                                            .gapPolicy(gapPolicy))
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("the_metric"))
                     ).execute().actionGet();
         } catch (SearchPhaseExecutionException e) {
             // All good
@@ -881,13 +898,14 @@ public class MovAvgIT extends ESIntegTestCase {
                                 .field(INTERVAL_FIELD)
                                 .interval(1)
                                 .subAggregation(avg("avg").field(VALUE_FIELD))
-                                .subAggregation(derivative("deriv", "avg").gapPolicy(gapPolicy))
+                                .subAggregation(derivative("deriv")
+                                        .setBucketsPaths("avg").gapPolicy(gapPolicy))
                                 .subAggregation(
-                                        movingAvg("avg_movavg", "avg").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                                .gapPolicy(gapPolicy).predict(12))
+                                        movingAvg("avg_movavg").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
+                                                .gapPolicy(gapPolicy).predict(12).setBucketsPaths("avg"))
                                 .subAggregation(
-                                        movingAvg("deriv_movavg", "deriv").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                                .gapPolicy(gapPolicy).predict(12))
+                                        movingAvg("deriv_movavg").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
+                                                .gapPolicy(gapPolicy).predict(12).setBucketsPaths("deriv"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -989,12 +1007,13 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(metric)
-                                    .subAggregation(movingAvg("movavg_counts", "_count")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(10)
                                             .modelBuilder(randomModelBuilder(100))
-                                            .gapPolicy(gapPolicy))
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("_count"))
                     ).execute().actionGet();
         } catch (SearchPhaseExecutionException e) {
             // All good
@@ -1007,20 +1026,22 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                 .period(period).seasonalityType(seasonalityType))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(true))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .minimize(true)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                 .period(period).seasonalityType(seasonalityType))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(true))
+                                        .minimize(true)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -1093,18 +1114,20 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(numBuckets)
                                         .modelBuilder(new HoltLinearModel.HoltLinearModelBuilder().alpha(alpha).beta(beta))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(true))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .minimize(true)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(numBuckets)
                                         .modelBuilder(new HoltLinearModel.HoltLinearModelBuilder().alpha(alpha).beta(beta))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(true))
+                                        .minimize(true)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -1147,13 +1170,14 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(numBuckets)
                                         .modelBuilder(new SimpleModel.SimpleModelBuilder())
                                         .gapPolicy(gapPolicy)
-                                        .minimize(true))
+                                        .minimize(true)
+                                        .setBucketsPaths("_count"))
                 ).execute().actionGet();
             fail("Simple Model cannot be minimized, but an exception was not thrown");
         } catch (SearchPhaseExecutionException e) {
@@ -1165,13 +1189,14 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(metric)
-                                    .subAggregation(movingAvg("movavg_counts", "_count")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(numBuckets)
                                             .modelBuilder(new LinearModel.LinearModelBuilder())
                                             .gapPolicy(gapPolicy)
-                                            .minimize(true))
+                                            .minimize(true)
+                                            .setBucketsPaths("_count"))
                     ).execute().actionGet();
             fail("Linear Model cannot be minimized, but an exception was not thrown");
         } catch (SearchPhaseExecutionException e) {
@@ -1195,13 +1220,14 @@ public class MovAvgIT extends ESIntegTestCase {
                         .prepareSearch("idx").setTypes("type")
                         .addAggregation(
                                 histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                        .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                        .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                         .subAggregation(metric)
-                                        .subAggregation(movingAvg("movavg_counts", "_count")
+                                        .subAggregation(movingAvg("movavg_counts")
                                                 .window(numBuckets)
                                                 .modelBuilder(builder)
                                                 .gapPolicy(gapPolicy)
-                                                .minimize(true))
+                                                .minimize(true)
+                                                .setBucketsPaths("_count"))
                         ).execute().actionGet();
             } catch (SearchPhaseExecutionException e) {
                 fail("Model [" + builder.toString() + "] can be minimized, but an exception was thrown");
@@ -1209,6 +1235,43 @@ public class MovAvgIT extends ESIntegTestCase {
         }
     }
 
+    public void testUnrecognizedParams() {
+        MovAvgModelBuilder[] builders = new MovAvgModelBuilder[]{
+                new SimpleModel.SimpleModelBuilder(),
+                new LinearModel.LinearModelBuilder(),
+                new EwmaModel.EWMAModelBuilder(),
+                new HoltLinearModel.HoltLinearModelBuilder(),
+                new HoltWintersModel.HoltWintersModelBuilder()
+        };
+        Map<String, Object> badSettings = new HashMap<>(1);
+        badSettings.put("abc", 1.2);
+
+        for (MovAvgModelBuilder builder : builders) {
+            try {
+                SearchResponse response = client()
+                        .prepareSearch("idx").setTypes("type")
+                        .addAggregation(
+                                histogram("histo").field(INTERVAL_FIELD).interval(interval)
+                                        .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
+                                        .subAggregation(metric)
+                                        .subAggregation(movingAvg("movavg_counts")
+                                                .window(10)
+                                                .modelBuilder(builder)
+                                                .gapPolicy(gapPolicy)
+                                                .settings(badSettings)
+                                                .setBucketsPaths("_count"))
+                        ).execute().actionGet();
+            } catch (SearchPhaseExecutionException e) {
+                // All good
+            }
+        }
+
+
+
+
+    }
+
+
     private void assertValidIterators(Iterator expectedBucketIter, Iterator expectedCountsIter, Iterator expectedValuesIter) {
         if (!expectedBucketIter.hasNext()) {
             fail("`expectedBucketIter` iterator ended before `actual` iterator, size mismatch");
@@ -1292,8 +1355,7 @@ public class MovAvgIT extends ESIntegTestCase {
         }
     }
 
-    private ValuesSourceAggregatorFactory<? extends ValuesSource, ? extends ValuesSourceAggregatorFactory<?, ?>> randomMetric(String name,
-            String field) {
+    private ValuesSourceMetricsAggregationBuilder randomMetric(String name, String field) {
         int rand = randomIntBetween(0,3);
 
         switch (rand) {
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java
deleted file mode 100644
index d12466f..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java
+++ /dev/null
@@ -1,95 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.moving.avg;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.movavg.MovAvgPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.movavg.MovAvgPipelineAggregator.Factory;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.EwmaModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.HoltLinearModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.HoltWintersModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.HoltWintersModel.SeasonalityType;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.LinearModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.SimpleModel;;
-
-public class MovAvgTests extends BasePipelineAggregationTestCase<MovAvgPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String bucketsPath = randomAsciiOfLengthBetween(3, 20);
-        Factory factory = new Factory(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        if (randomBoolean()) {
-            switch (randomInt(4)) {
-            case 0:
-                factory.modelBuilder(new SimpleModel.SimpleModelBuilder());
-                factory.window(randomIntBetween(1, 100));
-                break;
-            case 1:
-                factory.modelBuilder(new LinearModel.LinearModelBuilder());
-                factory.window(randomIntBetween(1, 100));
-                break;
-            case 2:
-                if (randomBoolean()) {
-                    factory.modelBuilder(new EwmaModel.EWMAModelBuilder());
-                    factory.window(randomIntBetween(1, 100));
-                } else {
-                    factory.modelBuilder(new EwmaModel.EWMAModelBuilder().alpha(randomDouble()));
-                    factory.window(randomIntBetween(1, 100));
-                }
-                break;
-            case 3:
-                if (randomBoolean()) {
-                    factory.modelBuilder(new HoltLinearModel.HoltLinearModelBuilder());
-                    factory.window(randomIntBetween(1, 100));
-                } else {
-                    factory.modelBuilder(new HoltLinearModel.HoltLinearModelBuilder().alpha(randomDouble()).beta(randomDouble()));
-                    factory.window(randomIntBetween(1, 100));
-                }
-                break;
-            case 4:
-            default:
-                if (randomBoolean()) {
-                    factory.modelBuilder(new HoltWintersModel.HoltWintersModelBuilder());
-                    factory.window(randomIntBetween(2, 100));
-                } else {
-                    int period = randomIntBetween(1, 100);
-                    factory.modelBuilder(new HoltWintersModel.HoltWintersModelBuilder().alpha(randomDouble()).beta(randomDouble())
-                            .gamma(randomDouble()).period(period).seasonalityType(randomFrom(SeasonalityType.values())).pad(randomBoolean()));
-                    factory.window(randomIntBetween(2 * period, 200 * period));
-                }
-                break;
-            }
-        }
-        factory.predict(randomIntBetween(1, 50));
-        if (factory.model().canBeMinimized() && randomBoolean()) {
-            factory.minimize(randomBoolean());
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
index 89e5cae..aebd6a7 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
@@ -23,14 +23,12 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.collect.EvictingQueue;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregationHelperTests;
 import org.elasticsearch.search.aggregations.pipeline.SimpleValue;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
 
@@ -62,7 +60,7 @@ public class SerialDiffIT extends ESIntegTestCase {
     static int numBuckets;
     static int lag;
     static BucketHelpers.GapPolicy gapPolicy;
-    static ValuesSourceAggregatorFactory<? extends ValuesSource, ? extends ValuesSourceAggregatorFactory<?, ?>> metric;
+    static ValuesSourceMetricsAggregationBuilder metric;
     static List<PipelineAggregationHelperTests.MockBucket> mockHisto;
 
     static Map<String, ArrayList<Double>> testValues;
@@ -82,7 +80,7 @@ public class SerialDiffIT extends ESIntegTestCase {
         }
     }
 
-    private ValuesSourceAggregatorFactory<? extends ValuesSource, ? extends ValuesSourceAggregatorFactory<?, ?>> randomMetric(String name, String field) {
+    private ValuesSourceMetricsAggregationBuilder randomMetric(String name, String field) {
         int rand = randomIntBetween(0,3);
 
         switch (rand) {
@@ -233,14 +231,16 @@ public class SerialDiffIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(diff("diff_counts", "_count")
+                                .subAggregation(diff("diff_counts")
                                         .lag(lag)
-                                        .gapPolicy(gapPolicy))
-                                .subAggregation(diff("diff_values", "the_metric")
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(diff("diff_values")
                                         .lag(lag)
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -280,11 +280,12 @@ public class SerialDiffIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(diff("diff_counts", "_count")
+                                .subAggregation(diff("diff_counts")
                                         .lag(-1)
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
                 ).execute().actionGet();
         } catch (SearchPhaseExecutionException e) {
             assertThat(e.getMessage(), is("all shards failed"));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/support/ValuesSourceTypeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/support/ValuesSourceTypeTests.java
deleted file mode 100644
index a297181..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/support/ValuesSourceTypeTests.java
+++ /dev/null
@@ -1,109 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.support;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class ValuesSourceTypeTests extends ESTestCase {
-
-    public void testValidOrdinals() {
-        assertThat(ValuesSourceType.ANY.ordinal(), equalTo(0));
-        assertThat(ValuesSourceType.NUMERIC.ordinal(), equalTo(1));
-        assertThat(ValuesSourceType.BYTES.ordinal(), equalTo(2));
-        assertThat(ValuesSourceType.GEOPOINT.ordinal(), equalTo(3));
-    }
-
-    public void testwriteTo() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ValuesSourceType.ANY.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(0));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ValuesSourceType.NUMERIC.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(1));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ValuesSourceType.BYTES.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(2));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ValuesSourceType.GEOPOINT.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(3));
-            }
-        }
-    }
-
-    public void testReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(0);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ValuesSourceType.ANY.readFrom(in), equalTo(ValuesSourceType.ANY));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(1);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ValuesSourceType.ANY.readFrom(in), equalTo(ValuesSourceType.NUMERIC));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(2);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ValuesSourceType.ANY.readFrom(in), equalTo(ValuesSourceType.BYTES));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(3);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ValuesSourceType.ANY.readFrom(in), equalTo(ValuesSourceType.GEOPOINT));
-            }
-        }
-    }
-
-    public void testInvalidReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(randomIntBetween(4, Integer.MAX_VALUE));
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                ValuesSourceType.ANY.readFrom(in);
-                fail("Expected IOException");
-            } catch(IOException e) {
-                assertThat(e.getMessage(), containsString("Unknown ValuesSourceType ordinal ["));
-            }
-
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java b/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
index 38fb4ca..eedc4d9 100644
--- a/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
@@ -344,8 +344,8 @@ public class TransportTwoNodesSearchIT extends ESIntegTestCase {
                 .query(termQuery("multi", "test"))
                 .from(0).size(20).explain(true)
                 .aggregation(AggregationBuilders.global("global").subAggregation(
-                        AggregationBuilders.filter("all", termQuery("multi", "test"))))
-                .aggregation(AggregationBuilders.filter("test1", termQuery("name", "test1")));
+                        AggregationBuilders.filter("all").filter(termQuery("multi", "test"))))
+                .aggregation(AggregationBuilders.filter("test1").filter(termQuery("name", "test1")));
 
         SearchResponse searchResponse = client().search(searchRequest("test").source(sourceBuilder)).actionGet();
         assertNoFailures(searchResponse);
diff --git a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
index 62f3e22..46fbae9 100644
--- a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
@@ -19,11 +19,6 @@
 
 package org.elasticsearch.search.builder;
 
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.concurrent.TimeUnit;
-
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
@@ -58,7 +53,7 @@ import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder.InnerHit;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 import org.elasticsearch.search.highlight.HighlightBuilderTests;
-import org.elasticsearch.search.rescore.RescoreBuilder;
+import org.elasticsearch.search.rescore.QueryRescoreBuilderTests;
 import org.elasticsearch.search.sort.SortBuilders;
 import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.search.suggest.SuggestBuilder;
@@ -69,6 +64,11 @@ import org.elasticsearch.threadpool.ThreadPoolModule;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+
 import static org.hamcrest.Matchers.equalTo;
 
 public class SearchSourceBuilderTests extends ESTestCase {
@@ -94,7 +94,7 @@ public class SearchSourceBuilderTests extends ESTestCase {
                         // skip me so we don't need transport
                     }
                     @Override
-                    protected void configureAggs(IndicesQueriesRegistry indicesQueriesRegistry) {
+                    protected void configureAggs() {
                         // skip me so we don't need scripting
                     }
                     @Override
@@ -281,17 +281,14 @@ public class SearchSourceBuilderTests extends ESTestCase {
         if (randomBoolean()) {
             int numRescores = randomIntBetween(1, 5);
             for (int i = 0; i < numRescores; i++) {
-                // NORELEASE need a random rescore builder method
-                RescoreBuilder rescoreBuilder = new RescoreBuilder(RescoreBuilder.queryRescorer(QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20),
-                        randomAsciiOfLengthBetween(5, 20))));
-                builder.addRescorer(rescoreBuilder);
+                builder.addRescorer(QueryRescoreBuilderTests.randomRescoreBuilder());
             }
         }
         if (randomBoolean()) {
             // NORELEASE need a random aggregation builder method
             builder.aggregation(AggregationBuilders.avg(randomAsciiOfLengthBetween(5, 20)));
         }
-        if (randomBoolean()) {
+        if (true) {
             // NORELEASE need a method to randomly build content for ext
             XContentBuilder xContentBuilder = XContentFactory.jsonBuilder();
             xContentBuilder.startObject();
diff --git a/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java b/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
index 9bd8fcb..bd396cf 100644
--- a/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
@@ -429,7 +429,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
                 .prepareSearch("test")
                 .setQuery(hasChildQuery("child", boolQuery().should(termQuery("c_field", "red")).should(termQuery("c_field", "yellow"))))
                 .addAggregation(AggregationBuilders.global("global").subAggregation(
-                        AggregationBuilders.filter("filter", boolQuery().should(termQuery("c_field", "red")).should(termQuery("c_field", "yellow"))).subAggregation(
+                        AggregationBuilders.filter("filter").filter(boolQuery().should(termQuery("c_field", "red")).should(termQuery("c_field", "yellow"))).subAggregation(
                                 AggregationBuilders.terms("facet1").field("c_field")))).get();
         assertNoFailures(searchResponse);
         assertThat(searchResponse.getHits().totalHits(), equalTo(2l));
diff --git a/core/src/test/java/org/elasticsearch/search/functionscore/QueryRescorerIT.java b/core/src/test/java/org/elasticsearch/search/functionscore/QueryRescorerIT.java
index 5644f89..07f5177 100644
--- a/core/src/test/java/org/elasticsearch/search/functionscore/QueryRescorerIT.java
+++ b/core/src/test/java/org/elasticsearch/search/functionscore/QueryRescorerIT.java
@@ -37,9 +37,9 @@ import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHits;
-import org.elasticsearch.search.rescore.QueryRescoreMode;
 import org.elasticsearch.search.rescore.RescoreBuilder;
-import org.elasticsearch.search.rescore.RescoreBuilder.QueryRescorer;
+import org.elasticsearch.search.rescore.QueryRescoreMode;
+import org.elasticsearch.search.rescore.QueryRescorerBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 
 import java.util.Arrays;
@@ -538,7 +538,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         String[] scoreModes = new String[]{ "max", "min", "avg", "total", "multiply", "" };
         String[] descriptionModes = new String[]{ "max of:", "min of:", "avg of:", "sum of:", "product of:", "sum of:" };
         for (int innerMode = 0; innerMode < scoreModes.length; innerMode++) {
-            QueryRescorer innerRescoreQuery = RescoreBuilder.queryRescorer(QueryBuilders.matchQuery("field1", "the quick brown").boost(4.0f))
+            QueryRescorerBuilder innerRescoreQuery = RescoreBuilder.queryRescorer(QueryBuilders.matchQuery("field1", "the quick brown").boost(4.0f))
                 .setQueryWeight(0.5f).setRescoreQueryWeight(0.4f);
 
             if (!"".equals(scoreModes[innerMode])) {
@@ -561,7 +561,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
             }
 
             for (int outerMode = 0; outerMode < scoreModes.length; outerMode++) {
-                QueryRescorer outerRescoreQuery = RescoreBuilder.queryRescorer(QueryBuilders.matchQuery("field1", "the quick brown")
+                QueryRescorerBuilder outerRescoreQuery = RescoreBuilder.queryRescorer(QueryBuilders.matchQuery("field1", "the quick brown")
                         .boost(4.0f)).setQueryWeight(0.5f).setRescoreQueryWeight(0.4f);
 
                 if (!"".equals(scoreModes[outerMode])) {
@@ -572,7 +572,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                         .prepareSearch()
                         .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
                         .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
-                        .addRescorer(innerRescoreQuery, 5).addRescorer(outerRescoreQuery, 10)
+                        .addRescorer(innerRescoreQuery, 5).addRescorer(outerRescoreQuery.windowSize(10))
                         .setExplain(true).get();
                 assertHitCount(searchResponse, 3);
                 assertFirstHit(searchResponse, hasId("1"));
@@ -599,7 +599,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
             for (int i = 0; i < numDocs - 4; i++) {
                 String[] intToEnglish = new String[] { English.intToEnglish(i), English.intToEnglish(i + 1), English.intToEnglish(i + 2), English.intToEnglish(i + 3) };
 
-                QueryRescorer rescoreQuery = RescoreBuilder
+                QueryRescorerBuilder rescoreQuery = RescoreBuilder
                         .queryRescorer(
                                 QueryBuilders.boolQuery()
                                         .disableCoord(true)
@@ -682,10 +682,10 @@ public class QueryRescorerIT extends ESIntegTestCase {
 
     public void testMultipleRescores() throws Exception {
         int numDocs = indexRandomNumbers("keyword", 1, true);
-        QueryRescorer eightIsGreat = RescoreBuilder.queryRescorer(
+        QueryRescorerBuilder eightIsGreat = RescoreBuilder.queryRescorer(
                 QueryBuilders.functionScoreQuery(QueryBuilders.termQuery("field1", English.intToEnglish(8)),
                         ScoreFunctionBuilders.weightFactorFunction(1000.0f)).boostMode(CombineFunction.REPLACE)).setScoreMode(QueryRescoreMode.Total);
-        QueryRescorer sevenIsBetter = RescoreBuilder.queryRescorer(
+        QueryRescorerBuilder sevenIsBetter = RescoreBuilder.queryRescorer(
                 QueryBuilders.functionScoreQuery(QueryBuilders.termQuery("field1", English.intToEnglish(7)),
                         ScoreFunctionBuilders.weightFactorFunction(10000.0f)).boostMode(CombineFunction.REPLACE))
                 .setScoreMode(QueryRescoreMode.Total);
@@ -703,10 +703,10 @@ public class QueryRescorerIT extends ESIntegTestCase {
         // We have no idea what the second hit will be because we didn't get a chance to look for seven
 
         // Now use one rescore to drag the number we're looking for into the window of another
-        QueryRescorer ninetyIsGood = RescoreBuilder.queryRescorer(
+        QueryRescorerBuilder ninetyIsGood = RescoreBuilder.queryRescorer(
                 QueryBuilders.functionScoreQuery(QueryBuilders.queryStringQuery("*ninety*"), ScoreFunctionBuilders.weightFactorFunction(1000.0f))
                         .boostMode(CombineFunction.REPLACE)).setScoreMode(QueryRescoreMode.Total);
-        QueryRescorer oneToo = RescoreBuilder.queryRescorer(
+        QueryRescorerBuilder oneToo = RescoreBuilder.queryRescorer(
                 QueryBuilders.functionScoreQuery(QueryBuilders.queryStringQuery("*one*"), ScoreFunctionBuilders.weightFactorFunction(1000.0f))
                         .boostMode(CombineFunction.REPLACE)).setScoreMode(QueryRescoreMode.Total);
         request.clearRescorers().addRescorer(ninetyIsGood, numDocs).addRescorer(oneToo, 10);
diff --git a/core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java b/core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java
index 383bde0..7446f99 100644
--- a/core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java
@@ -19,13 +19,6 @@
 
 package org.elasticsearch.search.highlight;
 
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.ParseFieldMatcher;
@@ -64,6 +57,13 @@ import org.elasticsearch.test.IndexSettingsModule;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.not;
 
diff --git a/core/src/test/java/org/elasticsearch/search/rescore/QueryRescoreBuilderTests.java b/core/src/test/java/org/elasticsearch/search/rescore/QueryRescoreBuilderTests.java
index 2aa55f8..1305aa7 100644
--- a/core/src/test/java/org/elasticsearch/search/rescore/QueryRescoreBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/search/rescore/QueryRescoreBuilderTests.java
@@ -19,15 +19,38 @@
 
 package org.elasticsearch.search.rescore;
 
+import org.elasticsearch.ElasticsearchParseException;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.index.IndexSettings;
+import org.elasticsearch.index.mapper.ContentPath;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.Mapper;
+import org.elasticsearch.index.mapper.MapperBuilders;
+import org.elasticsearch.index.mapper.core.StringFieldMapper;
 import org.elasticsearch.index.query.MatchAllQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.search.rescore.RescoreBuilder.QueryRescorer;
-import org.elasticsearch.search.rescore.RescoreBuilder.Rescorer;
+import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.indices.query.IndicesQueriesRegistry;
+import org.elasticsearch.search.SearchModule;
+import org.elasticsearch.search.rescore.QueryRescorer.QueryRescoreContext;
 import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.IndexSettingsModule;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
@@ -40,6 +63,7 @@ public class QueryRescoreBuilderTests extends ESTestCase {
 
     private static final int NUMBER_OF_TESTBUILDERS = 20;
     private static NamedWriteableRegistry namedWriteableRegistry;
+    private static IndicesQueriesRegistry indicesQueriesRegistry;
 
     /**
      * setup for the whole base test class
@@ -47,13 +71,14 @@ public class QueryRescoreBuilderTests extends ESTestCase {
     @BeforeClass
     public static void init() {
         namedWriteableRegistry = new NamedWriteableRegistry();
-        namedWriteableRegistry.registerPrototype(Rescorer.class, org.elasticsearch.search.rescore.RescoreBuilder.QueryRescorer.PROTOTYPE);
-        namedWriteableRegistry.registerPrototype(QueryBuilder.class, new MatchAllQueryBuilder());
+        namedWriteableRegistry.registerPrototype(RescoreBuilder.class, QueryRescorerBuilder.PROTOTYPE);
+        indicesQueriesRegistry = new SearchModule(Settings.EMPTY, namedWriteableRegistry).buildQueryParserRegistry();
     }
 
     @AfterClass
     public static void afterClass() throws Exception {
         namedWriteableRegistry = null;
+        indicesQueriesRegistry = null;
     }
 
     /**
@@ -61,8 +86,8 @@ public class QueryRescoreBuilderTests extends ESTestCase {
      */
     public void testSerialization() throws IOException {
         for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) {
-            RescoreBuilder original = randomRescoreBuilder();
-            RescoreBuilder deserialized = serializedCopy(original);
+            RescoreBuilder<?> original = randomRescoreBuilder();
+            RescoreBuilder<?> deserialized = serializedCopy(original);
             assertEquals(deserialized, original);
             assertEquals(deserialized.hashCode(), original.hashCode());
             assertNotSame(deserialized, original);
@@ -74,7 +99,7 @@ public class QueryRescoreBuilderTests extends ESTestCase {
      */
     public void testEqualsAndHashcode() throws IOException {
         for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) {
-            RescoreBuilder firstBuilder = randomRescoreBuilder();
+            RescoreBuilder<?> firstBuilder = randomRescoreBuilder();
             assertFalse("rescore builder is equal to null", firstBuilder.equals(null));
             assertFalse("rescore builder is equal to incompatible type", firstBuilder.equals(""));
             assertTrue("rescore builder is not equal to self", firstBuilder.equals(firstBuilder));
@@ -82,13 +107,13 @@ public class QueryRescoreBuilderTests extends ESTestCase {
                     equalTo(firstBuilder.hashCode()));
             assertThat("different rescore builder should not be equal", mutate(firstBuilder), not(equalTo(firstBuilder)));
 
-            RescoreBuilder secondBuilder = serializedCopy(firstBuilder);
+            RescoreBuilder<?> secondBuilder = serializedCopy(firstBuilder);
             assertTrue("rescore builder is not equal to self", secondBuilder.equals(secondBuilder));
             assertTrue("rescore builder is not equal to its copy", firstBuilder.equals(secondBuilder));
             assertTrue("equals is not symmetric", secondBuilder.equals(firstBuilder));
             assertThat("rescore builder copy's hashcode is different from original hashcode", secondBuilder.hashCode(), equalTo(firstBuilder.hashCode()));
 
-            RescoreBuilder thirdBuilder = serializedCopy(secondBuilder);
+            RescoreBuilder<?> thirdBuilder = serializedCopy(secondBuilder);
             assertTrue("rescore builder is not equal to self", thirdBuilder.equals(thirdBuilder));
             assertTrue("rescore builder is not equal to its copy", secondBuilder.equals(thirdBuilder));
             assertThat("rescore builder copy's hashcode is different from original hashcode", secondBuilder.hashCode(), equalTo(thirdBuilder.hashCode()));
@@ -99,8 +124,162 @@ public class QueryRescoreBuilderTests extends ESTestCase {
         }
     }
 
-    private RescoreBuilder mutate(RescoreBuilder original) throws IOException {
-        RescoreBuilder mutation = serializedCopy(original);
+    /**
+     *  creates random rescorer, renders it to xContent and back to new instance that should be equal to original
+     */
+    public void testFromXContent() throws IOException {
+        QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
+        context.parseFieldMatcher(new ParseFieldMatcher(Settings.EMPTY));
+        for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) {
+            RescoreBuilder<?> rescoreBuilder = randomRescoreBuilder();
+
+            XContentParser parser = createParser(rescoreBuilder);
+            context.reset(parser);
+            parser.nextToken();
+            RescoreBuilder<?> secondRescoreBuilder = RescoreBuilder.parseFromXContent(context);
+            assertNotSame(rescoreBuilder, secondRescoreBuilder);
+            assertEquals(rescoreBuilder, secondRescoreBuilder);
+            assertEquals(rescoreBuilder.hashCode(), secondRescoreBuilder.hashCode());
+        }
+    }
+
+    private static XContentParser createParser(RescoreBuilder<?> rescoreBuilder) throws IOException {
+        XContentBuilder builder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));
+        if (randomBoolean()) {
+            builder.prettyPrint();
+        }
+        rescoreBuilder.toXContent(builder, ToXContent.EMPTY_PARAMS);
+        return XContentHelper.createParser(builder.bytes());
+    }
+
+    /**
+     * test that build() outputs a {@link RescoreSearchContext} that is similar to the one
+     * we would get when parsing the xContent the test rescore builder is rendering out
+     */
+    public void testBuildRescoreSearchContext() throws ElasticsearchParseException, IOException {
+        Settings indexSettings = Settings.settingsBuilder()
+                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
+        Index index = new Index(randomAsciiOfLengthBetween(1, 10));
+        IndexSettings idxSettings = IndexSettingsModule.newIndexSettings(index, indexSettings);
+        // shard context will only need indicesQueriesRegistry for building Query objects nested in query rescorer
+        QueryShardContext mockShardContext = new QueryShardContext(idxSettings, null, null, null, null, null, null, indicesQueriesRegistry) {
+            @Override
+            public MappedFieldType fieldMapper(String name) {
+                StringFieldMapper.Builder builder = MapperBuilders.stringField(name);
+                return builder.build(new Mapper.BuilderContext(idxSettings.getSettings(), new ContentPath(1))).fieldType();
+            }
+        };
+
+        for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) {
+            RescoreBuilder<?> rescoreBuilder = randomRescoreBuilder();
+            QueryRescoreContext rescoreContext = (QueryRescoreContext) rescoreBuilder.build(mockShardContext);
+            XContentParser parser = createParser(rescoreBuilder);
+
+            QueryRescoreContext parsedRescoreContext = (QueryRescoreContext) new RescoreParseElement().parseSingleRescoreContext(parser, mockShardContext);
+            assertNotSame(rescoreContext, parsedRescoreContext);
+            assertEquals(rescoreContext.window(), parsedRescoreContext.window());
+            assertEquals(rescoreContext.query(), parsedRescoreContext.query());
+            assertEquals(rescoreContext.queryWeight(), parsedRescoreContext.queryWeight(), Float.MIN_VALUE);
+            assertEquals(rescoreContext.rescoreQueryWeight(), parsedRescoreContext.rescoreQueryWeight(), Float.MIN_VALUE);
+            assertEquals(rescoreContext.scoreMode(), parsedRescoreContext.scoreMode());
+        }
+    }
+
+    /**
+     * test parsing exceptions for incorrect rescorer syntax
+     */
+    public void testUnknownFieldsExpection() throws IOException {
+        QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
+        context.parseFieldMatcher(new ParseFieldMatcher(Settings.EMPTY));
+
+        String rescoreElement = "{\n" +
+                "    \"window_size\" : 20,\n" +
+                "    \"bad_rescorer_name\" : { }\n" +
+                "}\n";
+        prepareContext(context, rescoreElement);
+        try {
+            RescoreBuilder.parseFromXContent(context);
+            fail("expected a parsing exception");
+        } catch (ParsingException e) {
+            assertEquals("rescore doesn't support rescorer with name [bad_rescorer_name]", e.getMessage());
+        }
+
+        rescoreElement = "{\n" +
+                "    \"bad_fieldName\" : 20\n" +
+                "}\n";
+        prepareContext(context, rescoreElement);
+        try {
+            RescoreBuilder.parseFromXContent(context);
+            fail("expected a parsing exception");
+        } catch (ParsingException e) {
+            assertEquals("rescore doesn't support [bad_fieldName]", e.getMessage());
+        }
+
+        rescoreElement = "{\n" +
+                "    \"window_size\" : 20,\n" +
+                "    \"query\" : [ ]\n" +
+                "}\n";
+        prepareContext(context, rescoreElement);
+        try {
+            RescoreBuilder.parseFromXContent(context);
+            fail("expected a parsing exception");
+        } catch (ParsingException e) {
+            assertEquals("unexpected token [START_ARRAY] after [query]", e.getMessage());
+        }
+
+        rescoreElement = "{ }";
+        prepareContext(context, rescoreElement);
+        try {
+            RescoreBuilder.parseFromXContent(context);
+            fail("expected a parsing exception");
+        } catch (ParsingException e) {
+            assertEquals("missing rescore type", e.getMessage());
+        }
+
+        rescoreElement = "{\n" +
+                "    \"window_size\" : 20,\n" +
+                "    \"query\" : { \"bad_fieldname\" : 1.0  } \n" +
+                "}\n";
+        prepareContext(context, rescoreElement);
+        try {
+            RescoreBuilder.parseFromXContent(context);
+            fail("expected a parsing exception");
+        } catch (IllegalArgumentException e) {
+            assertEquals("[query] unknown field [bad_fieldname], parser not found", e.getMessage());
+        }
+
+        rescoreElement = "{\n" +
+                "    \"window_size\" : 20,\n" +
+                "    \"query\" : { \"rescore_query\" : { \"unknown_queryname\" : { } } } \n" +
+                "}\n";
+        prepareContext(context, rescoreElement);
+        try {
+            RescoreBuilder.parseFromXContent(context);
+            fail("expected a parsing exception");
+        } catch (ParsingException e) {
+            assertEquals("[query] failed to parse field [rescore_query]", e.getMessage());
+        }
+
+        rescoreElement = "{\n" +
+                "    \"window_size\" : 20,\n" +
+                "    \"query\" : { \"rescore_query\" : { \"match_all\" : { } } } \n"
+                + "}\n";
+        prepareContext(context, rescoreElement);
+        RescoreBuilder.parseFromXContent(context);
+    }
+
+    /**
+     * create a new parser from the rescorer string representation and reset context with it
+     */
+    private static void prepareContext(QueryParseContext context, String rescoreElement) throws IOException {
+        XContentParser parser = XContentFactory.xContent(rescoreElement).createParser(rescoreElement);
+        context.reset(parser);
+        // move to first token, this is where the internal fromXContent
+        assertTrue(parser.nextToken() == XContentParser.Token.START_OBJECT);
+    }
+
+    private static RescoreBuilder<?> mutate(RescoreBuilder<?> original) throws IOException {
+        RescoreBuilder<?> mutation = serializedCopy(original);
         if (randomBoolean()) {
             Integer windowSize = original.windowSize();
             if (windowSize != null) {
@@ -109,7 +288,7 @@ public class QueryRescoreBuilderTests extends ESTestCase {
                 mutation.windowSize(randomIntBetween(0, 100));
             }
         } else {
-            QueryRescorer queryRescorer = (QueryRescorer) mutation.rescorer();
+            QueryRescorerBuilder queryRescorer = (QueryRescorerBuilder) mutation;
             switch (randomIntBetween(0, 3)) {
             case 0:
                 queryRescorer.setQueryWeight(queryRescorer.getQueryWeight() + 0.1f);
@@ -138,10 +317,10 @@ public class QueryRescoreBuilderTests extends ESTestCase {
     /**
      * create random shape that is put under test
      */
-    private static RescoreBuilder randomRescoreBuilder() {
+    public static org.elasticsearch.search.rescore.QueryRescorerBuilder randomRescoreBuilder() {
         QueryBuilder<MatchAllQueryBuilder> queryBuilder = new MatchAllQueryBuilder().boost(randomFloat()).queryName(randomAsciiOfLength(20));
-        org.elasticsearch.search.rescore.RescoreBuilder.QueryRescorer rescorer = new
-                org.elasticsearch.search.rescore.RescoreBuilder.QueryRescorer(queryBuilder);
+        org.elasticsearch.search.rescore.QueryRescorerBuilder rescorer = new
+                org.elasticsearch.search.rescore.QueryRescorerBuilder(queryBuilder);
         if (randomBoolean()) {
             rescorer.setQueryWeight(randomFloat());
         }
@@ -151,18 +330,17 @@ public class QueryRescoreBuilderTests extends ESTestCase {
         if (randomBoolean()) {
             rescorer.setScoreMode(randomFrom(QueryRescoreMode.values()));
         }
-        RescoreBuilder builder = new RescoreBuilder(rescorer);
         if (randomBoolean()) {
-            builder.windowSize(randomIntBetween(0, 100));
+            rescorer.windowSize(randomIntBetween(0, 100));
         }
-        return builder;
+        return rescorer;
     }
 
-    private static RescoreBuilder serializedCopy(RescoreBuilder original) throws IOException {
+    private static RescoreBuilder<?> serializedCopy(RescoreBuilder<?> original) throws IOException {
         try (BytesStreamOutput output = new BytesStreamOutput()) {
-            original.writeTo(output);
+            output.writeRescorer(original);
             try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                return RescoreBuilder.PROTOYPE.readFrom(in);
+                return in.readRescorer();
             }
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java b/core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java
index 7739124..1df9659 100644
--- a/core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java
+++ b/core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java
@@ -23,8 +23,10 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.search.aggregations.Aggregation;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms;
+import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsBuilder;
 import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Assert;
@@ -38,8 +40,6 @@ import static org.elasticsearch.test.ESIntegTestCase.client;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
 import static org.hamcrest.Matchers.equalTo;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.significantTerms;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 
 public class SharedSignificantTermsTestMethods {
     public static final String INDEX_NAME = "testidx";
@@ -57,9 +57,13 @@ public class SharedSignificantTermsTestMethods {
     }
 
     private static void checkSignificantTermsAggregationCorrect(ESIntegTestCase testCase) {
-        SearchResponse response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE).addAggregation(
-                terms("class").field(CLASS_FIELD).subAggregation(significantTerms("sig_terms").field(TEXT_FIELD)))
-                .execute().actionGet();
+
+        SearchResponse response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
+                .addAggregation(new TermsBuilder("class").field(CLASS_FIELD).subAggregation(
+                        new SignificantTermsBuilder("sig_terms")
+                                .field(TEXT_FIELD)))
+                .execute()
+                .actionGet();
         assertSearchResponse(response);
         StringTerms classes = response.getAggregations().get("class");
         Assert.assertThat(classes.getBuckets().size(), equalTo(2));
diff --git a/docs/plugins/ingest-geoip.asciidoc b/docs/plugins/ingest-geoip.asciidoc
index 539c299..2c0663b 100644
--- a/docs/plugins/ingest-geoip.asciidoc
+++ b/docs/plugins/ingest-geoip.asciidoc
@@ -4,7 +4,7 @@
 The GeoIP processor adds information about the geographical location of IP addresses, based on data from the Maxmind databases.
 This processor adds this information by default under the `geoip` field.
 
-The ingest plugin ships by default with the GeoLite2 City and GeoLite2 Country geoip2 databases from Maxmind made available
+The ingest-geoip plugin ships by default with the GeoLite2 City and GeoLite2 Country geoip2 databases from Maxmind made available
 under the CCA-ShareAlike 3.0 license. For more details see, http://dev.maxmind.com/geoip/geoip2/geolite2/
 
 The GeoIP processor can run with other geoip2 databases from Maxmind. The files must be copied into the geoip config directory
@@ -18,7 +18,7 @@ is located at `$ES_HOME/config/ingest/geoip` and holds the shipped databases too
 | Name                   | Required  | Default                                                                            | Description
 | `source_field`         | yes       | -                                                                                  | The field to get the ip address or hostname from for the geographical lookup.
 | `target_field`         | no        | geoip                                                                              | The field that will hold the geographical information looked up from the Maxmind database.
-| `database_file`        | no        | GeoLite2-City.mmdb                                                                 | The database filename in the geoip config directory. The ingest plugin ships with the GeoLite2-City.mmdb and GeoLite2-Country.mmdb files.
+| `database_file`        | no        | GeoLite2-City.mmdb                                                                 | The database filename in the geoip config directory. The ingest-geoip plugin ships with the GeoLite2-City.mmdb and GeoLite2-Country.mmdb files.
 | `fields`               | no        | [`continent_name`, `country_iso_code`, `region_name`, `city_name`, `location`] <1> | Controls what properties are added to the `target_field` based on the geoip lookup.
 |======
 
diff --git a/docs/reference/aggregations/bucket.asciidoc b/docs/reference/aggregations/bucket.asciidoc
index 66ce2d8..2d185dd 100644
--- a/docs/reference/aggregations/bucket.asciidoc
+++ b/docs/reference/aggregations/bucket.asciidoc
@@ -19,8 +19,6 @@ include::bucket/datehistogram-aggregation.asciidoc[]
 
 include::bucket/daterange-aggregation.asciidoc[]
 
-include::bucket/diversified-sampler-aggregation.asciidoc[]
-
 include::bucket/filter-aggregation.asciidoc[]
 
 include::bucket/filters-aggregation.asciidoc[]
diff --git a/docs/reference/aggregations/bucket/diversified-sampler-aggregation.asciidoc b/docs/reference/aggregations/bucket/diversified-sampler-aggregation.asciidoc
deleted file mode 100644
index 92effce..0000000
--- a/docs/reference/aggregations/bucket/diversified-sampler-aggregation.asciidoc
+++ /dev/null
@@ -1,154 +0,0 @@
-[[search-aggregations-bucket-sampler-aggregation]]
-=== Sampler Aggregation
-
-experimental[]
-
-A filtering aggregation used to limit any sub aggregations' processing to a sample of the top-scoring documents. Diversity settings are 
-used to limit the number of matches that share a common value such as an "author".
-
-.Example use cases:
-* Tightening the focus of analytics to high-relevance matches rather than the potentially very long tail of low-quality matches
-* Removing bias from analytics by ensuring fair representation of content from different sources
-* Reducing the running cost of aggregations that can produce useful results using only samples e.g. `significant_terms`
- 
-
-Example:
-
-[source,js]
---------------------------------------------------
-{
-    "query": {
-        "match": {
-            "text": "iphone"
-        }
-    },
-    "aggs": {
-        "sample": {
-            "sampler": {
-                "shard_size": 200,
-                "field" : "user.id"   
-            },
-            "aggs": {
-                "keywords": {
-                    "significant_terms": {
-                        "field": "text"
-                    }
-                }
-            }
-        }
-    }
-}
---------------------------------------------------
-
-Response:
-
-[source,js]
---------------------------------------------------
-{
-    ...
-        "aggregations": {
-        "sample": {
-            "doc_count": 1000,<1>
-            "keywords": {<2>
-                "doc_count": 1000,
-                "buckets": [
-                    ...
-                    {
-                        "key": "bend",
-                        "doc_count": 58,
-                        "score": 37.982536582524276,
-                        "bg_count": 103
-                    },
-                    ....
-}
---------------------------------------------------
-
-<1> 1000 documents were sampled in total becase we asked for a maximum of 200 from an index with 5 shards. The cost of performing the nested significant_terms aggregation was therefore limited rather than unbounded.
-<2> The results of the significant_terms aggregation are not skewed by any single over-active Twitter user because we asked for a maximum of one tweet from any one user in our sample.
-
-
-==== shard_size
-
-The `shard_size` parameter limits how many top-scoring documents are collected in the sample processed on each shard.
-The default value is 100.
-
-==== Controlling diversity
-=`field` or `script` and `max_docs_per_value` settings are used to control the maximum number of documents collected on any one shard which share a common value.
-The choice of value (e.g. `author`) is loaded from a regular `field` or derived dynamically by a `script`.
-
-The aggregation will throw an error if the choice of field or script produces multiple values for a document.
-It is currently not possible to offer this form of de-duplication using many values, primarily due to concerns over efficiency.
-
-NOTE: Any good market researcher will tell you that when working with samples of data it is important
-that the sample represents a healthy variety of opinions rather than being skewed by any single voice.
-The same is true with aggregations and sampling with these diversify settings can offer a way to remove the bias in your content (an over-populated geography, a large spike in a timeline or an over-active forum spammer).  
-
-==== Field
-
-Controlling diversity using a field:
-
-[source,js]
---------------------------------------------------
-{
-    "aggs" : {
-        "sample" : {
-            "diverisfied_sampler" : {
-                "field" : "author",
-                "max_docs_per_value" : 3
-            }
-        }
-    }
-}
---------------------------------------------------
-
-Note that the `max_docs_per_value` setting applies on a per-shard basis only for the purposes of shard-local sampling.
-It is not intended as a way of providing a global de-duplication feature on search results.
-
-
-
-==== Script
-
-Controlling diversity using a script:
-
-[source,js]
---------------------------------------------------
-{
-    "aggs" : {
-        "sample" : {
-            "diverisfied_sampler" : {
-                "script" : "doc['author'].value + '/' + doc['genre'].value"
-            }
-        }
-    }
-}
---------------------------------------------------
-Note in the above example we chose to use the default `max_docs_per_value` setting of 1 and combine author and genre fields to ensure 
-each shard sample has, at most, one match for an author/genre pair.
-
-
-==== execution_hint
-
-When using the settings to control diversity, the optional `execution_hint` setting can influence the management of the values used for de-duplication.
-Each option will hold up to `shard_size` values in memory while performing de-duplication but the type of value held can be controlled as follows:
- 
- - hold field values directly (`map`)
- - hold ordinals of the field as determined by the Lucene index (`global_ordinals`)
- - hold hashes of the field values - with potential for hash collisions (`bytes_hash`)
- 
-The default setting is to use `global_ordinals` if this information is available from the Lucene index and reverting to `map` if not.
-The `bytes_hash` setting may prove faster in some cases but introduces the possibility of false positives in de-duplication logic due to the possibility of hash collisions.
-Please note that Elasticsearch will ignore the choice of execution hint if it is not applicable and that there is no backward compatibility guarantee on these hints.
-
-==== Limitations
-
-===== Cannot be nested under `breadth_first` aggregations
-Being a quality-based filter the sampler aggregation needs access to the relevance score produced for each document.
-It therefore cannot be nested under a `terms` aggregation which has the `collect_mode` switched from the default `depth_first` mode to `breadth_first` as this discards scores.
-In this situation an error will be thrown.
-
-===== Limited de-dup logic.
-The de-duplication logic in the diversify settings applies only at a shard level so will not apply across shards.
-
-===== No specialized syntax for geo/date fields
-Currently the syntax for defining the diversifying values is defined by a choice of `field` or `script` - there is no added syntactical sugar for expressing geo or date units such as "1w" (1 week).
-This support may be added in a later release and users will currently have to create these sorts of values using a script.
\ No newline at end of file
diff --git a/docs/reference/aggregations/bucket/sampler-aggregation.asciidoc b/docs/reference/aggregations/bucket/sampler-aggregation.asciidoc
index 741edc8..2974270 100644
--- a/docs/reference/aggregations/bucket/sampler-aggregation.asciidoc
+++ b/docs/reference/aggregations/bucket/sampler-aggregation.asciidoc
@@ -4,9 +4,11 @@
 experimental[]
 
 A filtering aggregation used to limit any sub aggregations' processing to a sample of the top-scoring documents.
+Optionally, diversity settings can be used to limit the number of matches that share a common value such as an "author".
 
 .Example use cases:
 * Tightening the focus of analytics to high-relevance matches rather than the potentially very long tail of low-quality matches
+* Removing bias from analytics by ensuring fair representation of content from different sources
 * Reducing the running cost of aggregations that can produce useful results using only samples e.g. `significant_terms`
  
 
@@ -23,7 +25,8 @@ Example:
     "aggs": {
         "sample": {
             "sampler": {
-                "shard_size": 200
+                "shard_size": 200,
+                "field" : "user.id"   
             },
             "aggs": {
                 "keywords": {
@@ -60,7 +63,8 @@ Response:
 }
 --------------------------------------------------
 
-<1> 1000 documents were sampled in total because we asked for a maximum of 200 from an index with 5 shards. The cost of performing the nested significant_terms aggregation was therefore limited rather than unbounded.
+<1> 1000 documents were sampled in total becase we asked for a maximum of 200 from an index with 5 shards. The cost of performing the nested significant_terms aggregation was therefore limited rather than unbounded.
+<2> The results of the significant_terms aggregation are not skewed by any single over-active Twitter user because we asked for a maximum of one tweet from any one user in our sample.
 
 
 ==== shard_size
@@ -68,9 +72,83 @@ Response:
 The `shard_size` parameter limits how many top-scoring documents are collected in the sample processed on each shard.
 The default value is 100.
 
+==== Controlling diversity
+Optionally, you can use the `field` or `script` and `max_docs_per_value` settings to control the maximum number of documents collected on any one shard which share a common value.
+The choice of value (e.g. `author`) is loaded from a regular `field` or derived dynamically by a `script`.
+
+The aggregation will throw an error if the choice of field or script produces multiple values for a document.
+It is currently not possible to offer this form of de-duplication using many values, primarily due to concerns over efficiency.
+
+NOTE: Any good market researcher will tell you that when working with samples of data it is important
+that the sample represents a healthy variety of opinions rather than being skewed by any single voice.
+The same is true with aggregations and sampling with these diversify settings can offer a way to remove the bias in your content (an over-populated geography, a large spike in a timeline or an over-active forum spammer).  
+
+==== Field
+
+Controlling diversity using a field:
+
+[source,js]
+--------------------------------------------------
+{
+    "aggs" : {
+        "sample" : {
+            "sampler" : {
+                "field" : "author",
+                "max_docs_per_value" : 3
+            }
+        }
+    }
+}
+--------------------------------------------------
+
+Note that the `max_docs_per_value` setting applies on a per-shard basis only for the purposes of shard-local sampling.
+It is not intended as a way of providing a global de-duplication feature on search results.
+
+
+
+==== Script
+
+Controlling diversity using a script:
+
+[source,js]
+--------------------------------------------------
+{
+    "aggs" : {
+        "sample" : {
+            "sampler" : {
+                "script" : "doc['author'].value + '/' + doc['genre'].value"
+            }
+        }
+    }
+}
+--------------------------------------------------
+Note in the above example we chose to use the default `max_docs_per_value` setting of 1 and combine author and genre fields to ensure 
+each shard sample has, at most, one match for an author/genre pair.
+
+
+==== execution_hint
+
+When using the settings to control diversity, the optional `execution_hint` setting can influence the management of the values used for de-duplication.
+Each option will hold up to `shard_size` values in memory while performing de-duplication but the type of value held can be controlled as follows:
+ 
+ - hold field values directly (`map`)
+ - hold ordinals of the field as determined by the Lucene index (`global_ordinals`)
+ - hold hashes of the field values - with potential for hash collisions (`bytes_hash`)
+ 
+The default setting is to use `global_ordinals` if this information is available from the Lucene index and reverting to `map` if not.
+The `bytes_hash` setting may prove faster in some cases but introduces the possibility of false positives in de-duplication logic due to the possibility of hash collisions.
+Please note that Elasticsearch will ignore the choice of execution hint if it is not applicable and that there is no backward compatibility guarantee on these hints.
+
 ==== Limitations
 
 ===== Cannot be nested under `breadth_first` aggregations
 Being a quality-based filter the sampler aggregation needs access to the relevance score produced for each document.
 It therefore cannot be nested under a `terms` aggregation which has the `collect_mode` switched from the default `depth_first` mode to `breadth_first` as this discards scores.
-In this situation an error will be thrown.
\ No newline at end of file
+In this situation an error will be thrown.
+
+===== Limited de-dup logic.
+The de-duplication logic in the diversify settings applies only at a shard level so will not apply across shards.
+
+===== No specialized syntax for geo/date fields
+Currently the syntax for defining the diversifying values is defined by a choice of `field` or `script` - there is no added syntactical sugar for expressing geo or date units such as "1w" (1 week).
+This support may be added in a later release and users will currently have to create these sorts of values using a script.
\ No newline at end of file
diff --git a/docs/reference/ingest/ingest.asciidoc b/docs/reference/ingest/ingest.asciidoc
index 0c049f8..ee724d4 100644
--- a/docs/reference/ingest/ingest.asciidoc
+++ b/docs/reference/ingest/ingest.asciidoc
@@ -1,23 +1,22 @@
 [[ingest]]
-== Ingest Plugin
+== Ingest Node
 
-The ingest plugin can be used to pre-process documents before the actual indexing takes place.
-This pre-processing happens by the ingest plugin that intercepts bulk and index requests, applies the
+Ingest node can be used to pre-process documents before the actual indexing takes place.
+This pre-processing happens by an ingest node that intercepts bulk and index requests, applies the
 transformations and then passes the documents back to the index or bulk APIs.
 
-The ingest plugin is disabled by default. In order to enable the ingest plugin the following
+Ingest node is enabled by default. In order to disable ingest the following
 setting should be configured in the elasticsearch.yml file:
 
 [source,yaml]
 --------------------------------------------------
-node.ingest: true
+node.ingest: false
 --------------------------------------------------
 
-The ingest plugin can be installed and enabled on any node. It is possible to run ingest
-on an master and or data node or have dedicated client nodes that run with ingest.
+It is possible to enable ingest on any node or have dedicated ingest nodes.
 
 In order to pre-process document before indexing the `pipeline` parameter should be used
-on an index or bulk request to tell the ingest plugin what pipeline is going to be used.
+on an index or bulk request to tell Ingest what pipeline is going to be used.
 
 [source,js]
 --------------------------------------------------
@@ -606,9 +605,9 @@ The following example sets the id of a document to `1`:
 The following metadata fields are accessible by a processor: `_index`, `_type`, `_id`, `_routing`, `_parent`,
 `_timestamp` and `_ttl`.
 
-Beyond metadata fields and source fields, the ingest plugin also adds ingest metadata to documents being processed.
-These metadata properties are accessible under the `_ingest` key. Currently the ingest plugin adds the ingest timestamp
-under `_ingest.timestamp` key to the ingest metadata, which is the time the ingest plugin received the index or bulk
+Beyond metadata fields and source fields, ingest also adds ingest metadata to documents being processed.
+These metadata properties are accessible under the `_ingest` key. Currently ingest adds the ingest timestamp
+under `_ingest.timestamp` key to the ingest metadata, which is the time ES received the index or bulk
 request to pre-process. But any processor is free to add more ingest related metadata to it. Ingest metadata is transient
 and is lost after a document has been processed by the pipeline and thus ingest metadata won't be indexed.
 
@@ -626,7 +625,7 @@ The following example adds a field with the name `received` and the value is the
 
 As opposed to Elasticsearch metadata fields, the ingest metadata field name _ingest can be used as a valid field name
 in the source of a document. Use _source._ingest to refer to it, otherwise _ingest will be interpreted as ingest
-metadata fields by the ingest plugin.
+metadata fields.
 
 A number of processor settings also support templating. Settings that support templating can have zero or more
 template snippets. A template snippet begins with `{{` and ends with `}}`.
diff --git a/docs/reference/migration/migrate_3_0.asciidoc b/docs/reference/migration/migrate_3_0.asciidoc
index b4aa2d6..699805a 100644
--- a/docs/reference/migration/migrate_3_0.asciidoc
+++ b/docs/reference/migration/migrate_3_0.asciidoc
@@ -545,6 +545,10 @@ to index a document only if it doesn't already exist.
 
 `InternalLineStringBuilder` is removed in favour of `LineStringBuilder`, `InternalPolygonBuilder` in favour of PolygonBuilder` and `Ring` has been replaced with `LineStringBuilder`. Also the abstract base classes `BaseLineStringBuilder` and `BasePolygonBuilder` haven been merged with their corresponding implementations.
 
+==== RescoreBuilder
+
+`RecoreBuilder.Rescorer` was merged with `RescoreBuilder`, which now is an abstract superclass. QueryRescoreBuilder currently is its only implementation.
+
 [[breaking_30_cache_concurrency]]
 === Cache concurrency level settings removed
 
diff --git a/modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java b/modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java
index 5246d0d..a866f33 100644
--- a/modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java
+++ b/modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java
@@ -530,10 +530,9 @@ public class MoreExpressionTests extends ESIntegTestCase {
                                 .subAggregation(sum("twoSum").field("two"))
                                 .subAggregation(sum("threeSum").field("three"))
                                 .subAggregation(sum("fourSum").field("four"))
-                                .subAggregation(bucketScript("totalSum",
-                                    new Script("_value0 + _value1 + _value2", ScriptType.INLINE, ExpressionScriptEngineService.NAME, null),
-                                    "twoSum", "threeSum", "fourSum")))
-                .execute().actionGet();
+                                .subAggregation(
+                                        bucketScript("totalSum").setBucketsPaths("twoSum", "threeSum", "fourSum").script(
+                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, ExpressionScriptEngineService.NAME, null)))).execute().actionGet();
 
         InternalHistogram<Bucket> histogram = response.getAggregations().get("histogram");
         assertThat(histogram, notNullValue());
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketScriptTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketScriptTests.java
index 28549a1..06119fd 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketScriptTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketScriptTests.java
@@ -113,8 +113,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null)
-                                                , "field2Sum", "field3Sum", "field4Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -157,8 +157,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0 + _value1 / _value2", ScriptType.INLINE, null, null),
-                                                "field2Sum", "field3Sum", "field4Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("_value0 + _value1 / _value2", ScriptType.INLINE, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -199,8 +199,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .interval(interval)
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0", ScriptType.INLINE, null, null),
-                                                "field2Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum").script(
+                                                new Script("_value0", ScriptType.INLINE, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -241,7 +241,7 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", bucketsPathsMap, 
+                                        bucketScript("seriesArithmetic").setBucketsPathsMap(bucketsPathsMap ).script(
                                                 new Script("foo + bar + baz", ScriptType.INLINE, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -287,8 +287,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("(_value0 + _value1 + _value2) * factor", ScriptType.INLINE, null, params),
-                                                "field2Sum", "field3Sum", "field4Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("(_value0 + _value1 + _value2) * factor", ScriptType.INLINE, null, params)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -331,8 +331,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null),
-                                                "field2Sum", "field3Sum", "field4Sum").gapPolicy(GapPolicy.INSERT_ZEROS))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null)).gapPolicy(GapPolicy.INSERT_ZEROS))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -377,8 +377,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("my_script", ScriptType.INDEXED, null, null),
-                                                "field2Sum", "field3Sum", "field4Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("my_script", ScriptType.INDEXED, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -421,8 +421,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null),
-                                                "field2Sum", "field3Sum", "field4Sum")))
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null))))
                                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -444,8 +444,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null),
-                                                "field2Sum", "field3Sum", "field4Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketSelectorTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketSelectorTests.java
index c78e513..a1faea0 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketSelectorTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketSelectorTests.java
@@ -27,7 +27,6 @@ import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.Bucket;
@@ -46,8 +45,8 @@ import java.util.Map;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.histogram;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
-import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders.bucketSelector;
 import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders.derivative;
+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders.having;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.greaterThan;
@@ -126,8 +125,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)",
-                                                ScriptType.INLINE, null, null), "field2Sum", "field3Sum"))).execute()
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", ScriptType.INLINE,
+                                                        null, null)))).execute()
                 .actionGet();
 
         assertSearchResponse(response);
@@ -159,8 +159,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("Double.isNaN(_value0) ? true : (_value0 < 10000)", 
-                                                ScriptType.INLINE, null, null), "field2Sum", "field3Sum"))).execute()
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? true : (_value0 < 10000)", ScriptType.INLINE, null,
+                                                        null)))).execute()
                 .actionGet();
 
         assertSearchResponse(response);
@@ -192,8 +193,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("Double.isNaN(_value0) ? false : (_value0 > 10000)", 
-                                                ScriptType.INLINE, null, null), "field2Sum", "field3Sum"))).execute().actionGet();
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 > 10000)", ScriptType.INLINE, null,
+                                                        null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -214,8 +216,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("Double.isNaN(_value0) ? false : (_value0 < _value1)",
-                                                ScriptType.INLINE, null, null), "field2Sum", "field3Sum"))).execute().actionGet();
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 < _value1)", ScriptType.INLINE, null,
+                                                        null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -245,8 +248,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .interval(interval)
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("Double.isNaN(_value0) ? false : (_value0 > 100)", 
-                                                ScriptType.INLINE,null, null), "field2Sum"))).execute().actionGet();
+                                        having("having").setBucketsPaths("field2Sum")
+                                                .script(new Script("Double.isNaN(_value0) ? false : (_value0 > 100)", ScriptType.INLINE,
+                                                        null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -278,8 +282,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", bucketPathsMap, new Script("Double.isNaN(my_value1) ? false : (my_value1 + my_value2 > 100)",
-                                                ScriptType.INLINE, null, null)))).execute()
+                                        having("having").setBucketsPathsMap(bucketPathsMap).script(
+                                                new Script("Double.isNaN(my_value1) ? false : (my_value1 + my_value2 > 100)",
+                                                        ScriptType.INLINE, null, null)))).execute()
                 .actionGet();
 
         assertSearchResponse(response);
@@ -313,8 +318,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > threshold)",
-                                                ScriptType.INLINE, null, params), "field2Sum", "field3Sum"))).execute()
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > threshold)",
+                                                        ScriptType.INLINE, null, params)))).execute()
                 .actionGet();
 
         assertSearchResponse(response);
@@ -346,8 +352,8 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("_value0 + _value1 > 100", ScriptType.INLINE, null, null),
-                                                "field2Sum", "field3Sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").gapPolicy(GapPolicy.INSERT_ZEROS)
+                                                .script(new Script("_value0 + _value1 > 100", ScriptType.INLINE, null, null))))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -379,8 +385,8 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("my_script", ScriptType.INDEXED, null, null),
-                                                "field2Sum", "field3Sum"))).execute().actionGet();
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("my_script", ScriptType.INDEXED, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -411,8 +417,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", 
-                                                ScriptType.INLINE, null, null), "field2Sum", "field3Sum"))).execute()
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", ScriptType.INLINE,
+                                                        null, null)))).execute()
                 .actionGet();
 
         assertSearchResponse(response);
@@ -433,8 +440,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", 
-                                                ScriptType.INLINE, null, null), "field2Sum", "field3Sum"))).execute()
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", ScriptType.INLINE,
+                                                        null, null)))).execute()
                 .actionGet();
 
         assertSearchResponse(response);
@@ -459,8 +467,8 @@ public class BucketSelectorTests extends ESIntegTestCase {
     public void testEmptyBuckets() {
         SearchResponse response = client().prepareSearch("idx_with_gaps")
                 .addAggregation(histogram("histo").field(FIELD_1_NAME).interval(1)
-                        .subAggregation(histogram("inner_histo").field(FIELD_1_NAME).interval(1).extendedBounds(new ExtendedBounds(1l, 4l))
-                                .minDocCount(0).subAggregation(derivative("derivative", "_count").gapPolicy(GapPolicy.INSERT_ZEROS))))
+                        .subAggregation(histogram("inner_histo").field(FIELD_1_NAME).interval(1).extendedBounds(1l, 4l).minDocCount(0)
+                                .subAggregation(derivative("derivative").setBucketsPaths("_count").gapPolicy(GapPolicy.INSERT_ZEROS))))
                 .execute().actionGet();
 
         assertSearchResponse(response);
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DateRangeTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DateRangeTests.java
index 6a06e45..ba4ca38 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DateRangeTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DateRangeTests.java
@@ -26,7 +26,7 @@ import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.range.Range;
 import org.elasticsearch.search.aggregations.bucket.range.Range.Bucket;
-import org.elasticsearch.search.aggregations.bucket.range.date.DateRangeAggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.range.date.DateRangeBuilder;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
 import org.elasticsearch.search.aggregations.metrics.min.Min;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
@@ -112,7 +112,7 @@ public class DateRangeTests extends ESIntegTestCase {
     }
 
     public void testDateMath() throws Exception {
-        DateRangeAggregatorFactory rangeBuilder = dateRange("range");
+        DateRangeBuilder rangeBuilder = dateRange("range");
         if (randomBoolean()) {
             rangeBuilder.field("date");
         } else {
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java
index de6fc52..d0de4c7 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java
@@ -31,13 +31,11 @@ import org.elasticsearch.search.aggregations.bucket.AbstractTermsTestCase;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
 import org.elasticsearch.search.aggregations.metrics.stats.Stats;
 import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStats;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
 
@@ -315,7 +313,8 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
         SearchResponse response = client().prepareSearch("idx").setTypes("type")
                 .addAggregation(terms("terms")
                         .field(SINGLE_VALUED_FIELD_NAME)
-                        .includeExclude(new IncludeExclude(includes, excludes))
+                        .include(includes)
+                        .exclude(excludes)
                         .collectMode(randomFrom(SubAggCollectionMode.values())))
                 .execute().actionGet();
         assertSearchResponse(response);
@@ -711,7 +710,7 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
                 .addAggregation(terms("terms")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "']"))
-                        .valueType(ValueType.DOUBLE)
+                        .valueType(Terms.ValueType.DOUBLE)
                         .subAggregation(sum("sum")))
                 .execute().actionGet();
 
@@ -885,7 +884,7 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
                 .addAggregation(
                         terms("num_tags").field("num_tag").collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .order(Terms.Order.aggregation("filter", asc))
-                                .subAggregation(filter("filter", QueryBuilders.matchAllQuery()))).execute().actionGet();
+                                .subAggregation(filter("filter").filter(QueryBuilders.matchAllQuery()))).execute().actionGet();
 
 
         assertSearchResponse(response);
@@ -925,8 +924,8 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
                                 .collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .order(Terms.Order.aggregation("filter1>filter2>max", asc))
                                 .subAggregation(
-                                        filter("filter1", QueryBuilders.matchAllQuery()).subAggregation(
-                                                filter("filter2", QueryBuilders.matchAllQuery()).subAggregation(
+                                        filter("filter1").filter(QueryBuilders.matchAllQuery()).subAggregation(
+                                                filter("filter2").filter(QueryBuilders.matchAllQuery()).subAggregation(
                                                         max("max").field(SINGLE_VALUED_FIELD_NAME))))).execute().actionGet();
 
 
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/EquivalenceTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/EquivalenceTests.java
index 89a03d6..584a8d2 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/EquivalenceTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/EquivalenceTests.java
@@ -35,8 +35,8 @@ import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.range.Range;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
 import org.elasticsearch.search.aggregations.bucket.range.Range.Bucket;
+import org.elasticsearch.search.aggregations.bucket.range.RangeBuilder;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
@@ -122,7 +122,7 @@ public class EquivalenceTests extends ESIntegTestCase {
             }
         }
 
-        RangeAggregator.Factory query = range("range").field("values");
+        RangeBuilder query = range("range").field("values");
         for (int i = 0; i < ranges.length; ++i) {
             String key = Integer.toString(i);
             if (ranges[i][0] == Double.NEGATIVE_INFINITY) {
@@ -143,7 +143,7 @@ public class EquivalenceTests extends ESIntegTestCase {
             if (ranges[i][1] != Double.POSITIVE_INFINITY){
                 filter = filter.to(ranges[i][1]);
             }
-            reqBuilder = reqBuilder.addAggregation(filter("filter" + i, filter));
+            reqBuilder = reqBuilder.addAggregation(filter("filter" + i).filter(filter));
         }
 
         SearchResponse resp = reqBuilder.execute().actionGet();
@@ -354,7 +354,7 @@ public class EquivalenceTests extends ESIntegTestCase {
         indexRandom(true, client().prepareIndex("idx", "type").setSource("f", value));
         ensureYellow("idx"); // only one document let's make sure all shards have an active primary
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filter("filter", QueryBuilders.matchAllQuery())
+                .addAggregation(filter("filter").filter(QueryBuilders.matchAllQuery())
                 .subAggregation(range("range")
                         .field("f")
                         .addUnboundedTo(6)
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentileRanksTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentileRanksTests.java
index c5bf370..7e7e8cb 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentileRanksTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentileRanksTests.java
@@ -77,7 +77,7 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
             }
         }
         Arrays.sort(percents);
-        Loggers.getLogger(HDRPercentileRanksTests.class).info("Using values={}", Arrays.toString(percents));
+        Loggers.getLogger(HDRPercentileRanksTests.class).info("Using percentiles={}", Arrays.toString(percents));
         return percents;
     }
 
@@ -85,8 +85,8 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
         return randomIntBetween(0, 5);
     }
 
-    private void assertConsistent(double[] pcts, PercentileRanks values, long minValue, long maxValue, int numberSigDigits) {
-        final List<Percentile> percentileList = iterableAsArrayList(values);
+    private void assertConsistent(double[] pcts, PercentileRanks percentiles, long minValue, long maxValue, int numberSigDigits) {
+        final List<Percentile> percentileList = iterableAsArrayList(percentiles);
         assertEquals(pcts.length, percentileList.size());
         for (int i = 0; i < pcts.length; ++i) {
             final Percentile percentile = percentileList.get(i);
@@ -122,8 +122,7 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                                 .minDocCount(0)
                                 .subAggregation(
                                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR)
-                                        .numberOfSignificantValueDigits(sigDigits).values(10, 15)))
-                .execute().actionGet();
+                                                .numberOfSignificantValueDigits(sigDigits).percentiles(10, 15))).execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2l));
         Histogram histo = searchResponse.getAggregations().get("histo");
@@ -146,8 +145,7 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").values(0, 10, 15, 100))
-                .execute().actionGet();
+                                .field("value").percentiles(0, 10, 15, 100)).execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(0l));
 
@@ -169,13 +167,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").values(pcts))
-                .execute().actionGet();
+                                .field("value").percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue, sigDigits);
     }
 
     @Override
@@ -188,8 +185,7 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .addAggregation(
                         global("global").subAggregation(
                                 percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                                .field("value").values(pcts)))
-                .execute().actionGet();
+                                        .field("value").percentiles(pcts))).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
@@ -200,10 +196,10 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
         assertThat(global.getAggregations(), notNullValue());
         assertThat(global.getAggregations().asMap().size(), equalTo(1));
 
-        PercentileRanks values = global.getAggregations().get("percentile_ranks");
-        assertThat(values, notNullValue());
-        assertThat(values.getName(), equalTo("percentile_ranks"));
-        assertThat((PercentileRanks) global.getProperty("percentile_ranks"), sameInstance(values));
+        PercentileRanks percentiles = global.getAggregations().get("percentile_ranks");
+        assertThat(percentiles, notNullValue());
+        assertThat(percentiles.getName(), equalTo("percentile_ranks"));
+        assertThat((PercentileRanks) global.getProperty("percentile_ranks"), sameInstance(percentiles));
 
     }
 
@@ -215,13 +211,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").values(pcts))
-                .execute().actionGet();
+                                .field("value").percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue, sigDigits);
     }
 
     @Override
@@ -233,13 +228,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").values(pcts))
-                .execute().actionGet();
+                                .field("value").percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue, sigDigits);
     }
 
     @Override
@@ -251,13 +245,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").script(new Script("_value - 1")).values(pcts))
-                .execute().actionGet();
+                                .field("value").script(new Script("_value - 1")).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1, sigDigits);
     }
 
     @Override
@@ -271,13 +264,13 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").script(new Script("_value - dec", ScriptType.INLINE, null, params)).values(pcts))
+                                .field("value").script(new Script("_value - dec", ScriptType.INLINE, null, params)).percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1, sigDigits);
     }
 
     @Override
@@ -289,13 +282,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("values").values(pcts))
-                .execute().actionGet();
+                                .field("values").percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues, maxValues, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues, maxValues, sigDigits);
     }
 
     @Override
@@ -307,13 +299,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("values").script(new Script("_value - 1")).values(pcts))
-                .execute().actionGet();
+                                .field("values").script(new Script("_value - 1")).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1, sigDigits);
     }
 
     public void testMultiValuedFieldWithValueScriptReverse() throws Exception {
@@ -324,13 +315,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("values").script(new Script("20 - _value")).values(pcts))
-                .execute().actionGet();
+                                .field("values").script(new Script("20 - _value")).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, 20 - maxValues, 20 - minValues, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, 20 - maxValues, 20 - minValues, sigDigits);
     }
 
     @Override
@@ -344,13 +334,13 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("values").script(new Script("_value - dec", ScriptType.INLINE, null, params)).values(pcts))
+                                .field("values").script(new Script("_value - dec", ScriptType.INLINE, null, params)).percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1, sigDigits);
     }
 
     @Override
@@ -362,13 +352,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .script(new Script("doc['value'].value")).values(pcts))
-                .execute().actionGet();
+                                .script(new Script("doc['value'].value")).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue, sigDigits);
     }
 
     @Override
@@ -382,13 +371,13 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .script(new Script("doc['value'].value - dec", ScriptType.INLINE, null, params)).values(pcts))
+                                .script(new Script("doc['value'].value - dec", ScriptType.INLINE, null, params)).percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1, sigDigits);
     }
 
     @Override
@@ -400,13 +389,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .script(new Script("doc['values'].values")).values(pcts))
-                .execute().actionGet();
+                                .script(new Script("doc['values'].values")).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues, maxValues, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues, maxValues, sigDigits);
     }
 
     @Override
@@ -424,14 +412,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                                 .numberOfSignificantValueDigits(sigDigits)
                                 .script(new Script(
                                         "List values = doc['values'].values; double[] res = new double[values.size()]; for (int i = 0; i < res.length; i++) { res[i] = values.get(i) - dec; }; return res;",
-                                ScriptType.INLINE, null, params))
-                        .values(pcts))
-                .execute().actionGet();
+                                        ScriptType.INLINE, null, params)).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1, sigDigits);
     }
 
     public void testOrderBySubAggregation() {
@@ -444,7 +430,7 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                         histogram("histo").field("value").interval(2l)
                                 .subAggregation(
                                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR)
-                                .numberOfSignificantValueDigits(sigDigits).values(99))
+                                                .numberOfSignificantValueDigits(sigDigits).percentiles(99))
                                 .order(Order.aggregation("percentile_ranks", "99", asc))).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
@@ -452,8 +438,8 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
         Histogram histo = searchResponse.getAggregations().get("histo");
         double previous = asc ? Double.NEGATIVE_INFINITY : Double.POSITIVE_INFINITY;
         for (Histogram.Bucket bucket : histo.getBuckets()) {
-            PercentileRanks values = bucket.getAggregations().get("percentile_ranks");
-            double p99 = values.percent(99);
+            PercentileRanks percentiles = bucket.getAggregations().get("percentile_ranks");
+            double p99 = percentiles.percent(99);
             if (asc) {
                 assertThat(p99, greaterThanOrEqualTo(previous));
             } else {
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java
index 1b1fa92..b8c6f6d 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java
@@ -27,7 +27,6 @@ import org.elasticsearch.script.Script;
 import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
@@ -557,7 +556,7 @@ public class HistogramTests extends ESIntegTestCase {
         boolean asc = randomBoolean();
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval).order(Histogram.Order.aggregation("filter>max", asc))
-                        .subAggregation(filter("filter", matchAllQuery())
+                        .subAggregation(filter("filter").filter(matchAllQuery())
                                 .subAggregation(max("max").field(SINGLE_VALUED_FIELD_NAME))))
                 .execute().actionGet();
 
@@ -897,7 +896,7 @@ public class HistogramTests extends ESIntegTestCase {
                 .prepareSearch("idx", "idx_unmapped")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) -1 * 2 * interval, (long) valueCounts.length * interval))).execute().actionGet();
+                                .extendedBounds((long) -1 * 2 * interval, (long) valueCounts.length * interval)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -987,7 +986,7 @@ public class HistogramTests extends ESIntegTestCase {
                             .field(SINGLE_VALUED_FIELD_NAME)
                             .interval(interval)
                             .minDocCount(0)
-                            .extendedBounds(new ExtendedBounds(boundsMin, boundsMax)))
+                            .extendedBounds(boundsMin, boundsMax))
                     .execute().actionGet();
 
             if (invalidBoundsError) {
@@ -1030,7 +1029,7 @@ public class HistogramTests extends ESIntegTestCase {
                     .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(-1).minDocCount(0)).execute().actionGet();
             fail();
         } catch (SearchPhaseExecutionException e) {
-            assertThat(e.toString(), containsString("[interval] must be 1 or greater for histogram aggregation [histo]"));
+            assertThat(e.toString(), containsString("Missing required field [interval]"));
         }
     }
 }
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/LongTermsTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/LongTermsTests.java
index 0f79ca8..638d923 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/LongTermsTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/LongTermsTests.java
@@ -30,13 +30,11 @@ import org.elasticsearch.search.aggregations.bucket.AbstractTermsTestCase;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
 import org.elasticsearch.search.aggregations.metrics.stats.Stats;
 import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStats;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
 
@@ -291,7 +289,8 @@ public class LongTermsTests extends AbstractTermsTestCase {
         SearchResponse response = client().prepareSearch("idx").setTypes("type")
                 .addAggregation(terms("terms")
                         .field(SINGLE_VALUED_FIELD_NAME)
-                        .includeExclude(new IncludeExclude(includes, excludes))
+                        .include(includes)
+                        .exclude(excludes)
                         .collectMode(randomFrom(SubAggCollectionMode.values())))
                 .execute().actionGet();
         assertSearchResponse(response);
@@ -707,7 +706,7 @@ public class LongTermsTests extends AbstractTermsTestCase {
                 .addAggregation(terms("terms")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "']"))
-                        .valueType(ValueType.LONG)
+                        .valueType(Terms.ValueType.LONG)
                         .subAggregation(sum("sum")))
                 .execute().actionGet();
 
@@ -877,7 +876,7 @@ public class LongTermsTests extends AbstractTermsTestCase {
                         .field("num_tag")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
                         .order(Terms.Order.aggregation("filter", asc))
-.subAggregation(filter("filter", QueryBuilders.matchAllQuery()))
+                        .subAggregation(filter("filter").filter(QueryBuilders.matchAllQuery()))
                 ).execute().actionGet();
 
 
@@ -914,8 +913,8 @@ public class LongTermsTests extends AbstractTermsTestCase {
                         .field("num_tag")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
                         .order(Terms.Order.aggregation("filter1>filter2>max", asc))
-                .subAggregation(filter("filter1", QueryBuilders.matchAllQuery()).subAggregation(
-                        filter("filter2", QueryBuilders.matchAllQuery())
+                        .subAggregation(filter("filter1").filter(QueryBuilders.matchAllQuery())
+                                .subAggregation(filter("filter2").filter(QueryBuilders.matchAllQuery())
                                         .subAggregation(max("max").field(SINGLE_VALUED_FIELD_NAME))))
                 ).execute().actionGet();
 
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinDocCountTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinDocCountTests.java
index a348577..9b3e1a3 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinDocCountTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinDocCountTests.java
@@ -35,8 +35,7 @@ import org.elasticsearch.search.aggregations.bucket.AbstractTermsTestCase;
 import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
@@ -111,17 +110,17 @@ public class MinDocCountTests extends AbstractTermsTestCase {
     private enum Script {
         NO {
             @Override
-            TermsAggregatorFactory apply(TermsAggregatorFactory builder, String field) {
+            TermsBuilder apply(TermsBuilder builder, String field) {
                 return builder.field(field);
             }
         },
         YES {
             @Override
-            TermsAggregatorFactory apply(TermsAggregatorFactory builder, String field) {
+            TermsBuilder apply(TermsBuilder builder, String field) {
                 return builder.script(new org.elasticsearch.script.Script("doc['" + field + "'].values"));
             }
         };
-        abstract TermsAggregatorFactory apply(TermsAggregatorFactory builder, String field);
+        abstract TermsBuilder apply(TermsBuilder builder, String field);
     }
 
     // check that terms2 is a subset of terms1
@@ -298,7 +297,7 @@ public class MinDocCountTests extends AbstractTermsTestCase {
                             .executionHint(randomExecutionHint())
                             .order(order)
                             .size(size)
-                            .includeExclude(include == null ? null : new IncludeExclude(include, null))
+                            .include(include)
                             .shardSize(cardinality + randomInt(10))
                             .minDocCount(minDocCount)).request();
             final SearchResponse response = client().search(request).get();
@@ -378,7 +377,7 @@ public class MinDocCountTests extends AbstractTermsTestCase {
         final SearchResponse allResponse = client().prepareSearch("idx").setTypes("type")
                 .setSize(0)
                 .setQuery(QUERY)
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).order(order).minDocCount(0))
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).order(order).minDocCount(0))
                 .execute().actionGet();
 
         final Histogram allHisto = allResponse.getAggregations().get("histo");
@@ -387,7 +386,7 @@ public class MinDocCountTests extends AbstractTermsTestCase {
             final SearchResponse response = client().prepareSearch("idx").setTypes("type")
                     .setSize(0)
                     .setQuery(QUERY)
-                    .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).order(order).minDocCount(minDocCount))
+                    .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).order(order).minDocCount(minDocCount))
                     .execute().actionGet();
             assertSubset(allHisto, (Histogram) response.getAggregations().get("histo"), minDocCount);
         }
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/RangeTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/RangeTests.java
index 288d5ed..5d78b99 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/RangeTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/RangeTests.java
@@ -42,8 +42,8 @@ import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.avg;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.histogram;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.range;
+import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
 import static org.hamcrest.Matchers.equalTo;
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StringTermsTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StringTermsTests.java
index f4b7fc7..55672a0 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StringTermsTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StringTermsTests.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.messy.tests;
 
-import org.apache.lucene.util.automaton.RegExp;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
@@ -37,7 +36,6 @@ import org.elasticsearch.search.aggregations.bucket.filter.Filter;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.ExecutionMode;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.search.aggregations.metrics.stats.Stats;
 import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStats;
@@ -275,8 +273,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values())).includeExclude(new IncludeExclude("val00.+", null)))
-                .execute().actionGet();
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).include("val00.+")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -300,8 +297,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .includeExclude(new IncludeExclude("val00.+", "(val000|val001)")))
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).include("val00.+").exclude("(val000|val001)"))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -326,9 +322,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .includeExclude(new IncludeExclude(null, new RegExp("val0[1-9]+.+"))))
-                .execute().actionGet();
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).exclude("val0[1-9]+.+")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -353,8 +347,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values())).includeExclude(new IncludeExclude(incVals, null)))
-                .execute().actionGet();
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).include(incVals)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -381,8 +374,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values())).includeExclude(new IncludeExclude(incVals, excVals)))
-                .execute()
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).include(incVals).exclude(excVals)).execute()
                 .actionGet();
 
         assertSearchResponse(response);
@@ -405,8 +397,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values())).includeExclude(new IncludeExclude(null, excVals)))
-                .execute().actionGet();
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).exclude(excVals)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -916,7 +907,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .prepareSearch("idx")
                 .setTypes("type")
                 .addAggregation(
-                        filter("filter", termQuery(MULTI_VALUED_FIELD_NAME, "val3")).subAggregation(
+                        filter("filter").filter(termQuery(MULTI_VALUED_FIELD_NAME, "val3")).subAggregation(
                                 terms("terms").field(MULTI_VALUED_FIELD_NAME).collectMode(randomFrom(SubAggCollectionMode.values()))))
                 .execute().actionGet();
 
@@ -1023,7 +1014,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .addAggregation(
                         terms("tags").executionHint(randomExecutionHint()).field("tag")
                                 .collectMode(randomFrom(SubAggCollectionMode.values())).order(Terms.Order.aggregation("filter", asc))
-                                .subAggregation(filter("filter", QueryBuilders.matchAllQuery()))).execute().actionGet();
+                                .subAggregation(filter("filter").filter(QueryBuilders.matchAllQuery()))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -1063,8 +1054,8 @@ public class StringTermsTests extends AbstractTermsTestCase {
                                 .collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .order(Terms.Order.aggregation("filter1>filter2>stats.max", asc))
                                 .subAggregation(
-                                        filter("filter1", QueryBuilders.matchAllQuery()).subAggregation(
-                                                filter("filter2", QueryBuilders.matchAllQuery()).subAggregation(
+                                        filter("filter1").filter(QueryBuilders.matchAllQuery()).subAggregation(
+                                                filter("filter2").filter(QueryBuilders.matchAllQuery()).subAggregation(
                                                         stats("stats").field("i"))))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -1126,8 +1117,8 @@ public class StringTermsTests extends AbstractTermsTestCase {
                                 .collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .order(Terms.Order.aggregation("filter1>" + filter2Name + ">" + statsName + ".max", asc))
                                 .subAggregation(
-                                        filter("filter1", QueryBuilders.matchAllQuery()).subAggregation(
-                                                filter(filter2Name, QueryBuilders.matchAllQuery()).subAggregation(
+                                        filter("filter1").filter(QueryBuilders.matchAllQuery()).subAggregation(
+                                                filter(filter2Name).filter(QueryBuilders.matchAllQuery()).subAggregation(
                                                         stats(statsName).field("i"))))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -1189,8 +1180,8 @@ public class StringTermsTests extends AbstractTermsTestCase {
                                 .collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .order(Terms.Order.aggregation("filter1>" + filter2Name + ">" + statsName + "[max]", asc))
                                 .subAggregation(
-                                        filter("filter1", QueryBuilders.matchAllQuery()).subAggregation(
-                                                filter(filter2Name, QueryBuilders.matchAllQuery()).subAggregation(
+                                        filter("filter1").filter(QueryBuilders.matchAllQuery()).subAggregation(
+                                                filter(filter2Name).filter(QueryBuilders.matchAllQuery()).subAggregation(
                                                         stats(statsName).field("i"))))).execute().actionGet();
 
         assertSearchResponse(response);
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentileRanksTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentileRanksTests.java
index ae23856..20916c3 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentileRanksTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentileRanksTests.java
@@ -31,7 +31,7 @@ import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Order;
 import org.elasticsearch.search.aggregations.metrics.AbstractNumericTestCase;
 import org.elasticsearch.search.aggregations.metrics.percentiles.Percentile;
 import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanks;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksAggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksBuilder;
 
 import java.util.Arrays;
 import java.util.Collection;
@@ -78,19 +78,19 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
             }
         }
         Arrays.sort(percents);
-        Loggers.getLogger(TDigestPercentileRanksTests.class).info("Using values={}", Arrays.toString(percents));
+        Loggers.getLogger(TDigestPercentileRanksTests.class).info("Using percentiles={}", Arrays.toString(percents));
         return percents;
     }
 
-    private static PercentileRanksAggregatorFactory randomCompression(PercentileRanksAggregatorFactory builder) {
+    private static PercentileRanksBuilder randomCompression(PercentileRanksBuilder builder) {
         if (randomBoolean()) {
             builder.compression(randomIntBetween(20, 120) + randomDouble());
         }
         return builder;
     }
 
-    private void assertConsistent(double[] pcts, PercentileRanks values, long minValue, long maxValue) {
-        final List<Percentile> percentileList = CollectionUtils.iterableAsArrayList(values);
+    private void assertConsistent(double[] pcts, PercentileRanks percentiles, long minValue, long maxValue) {
+        final List<Percentile> percentileList = CollectionUtils.iterableAsArrayList(percentiles);
         assertEquals(pcts.length, percentileList.size());
         for (int i = 0; i < pcts.length; ++i) {
             final Percentile percentile = percentileList.get(i);
@@ -117,7 +117,7 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1l).minDocCount(0)
                         .subAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                                .values(10, 15)))
+                                .percentiles(10, 15)))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2l));
@@ -139,7 +139,7 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
                         .field("value")
-                        .values(0, 10, 15, 100))
+                        .percentiles(0, 10, 15, 100))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(0l));
@@ -160,13 +160,13 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
                         .field("value")
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue);
     }
 
     @Override
@@ -177,7 +177,7 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         global("global").subAggregation(
-                                randomCompression(percentileRanks("percentile_ranks")).field("value").values(pcts))).execute()
+                                randomCompression(percentileRanks("percentile_ranks")).field("value").percentiles(pcts))).execute()
                 .actionGet();
 
         assertHitCount(searchResponse, 10);
@@ -189,10 +189,10 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         assertThat(global.getAggregations(), notNullValue());
         assertThat(global.getAggregations().asMap().size(), equalTo(1));
 
-        PercentileRanks values = global.getAggregations().get("percentile_ranks");
-        assertThat(values, notNullValue());
-        assertThat(values.getName(), equalTo("percentile_ranks"));
-        assertThat((PercentileRanks) global.getProperty("percentile_ranks"), sameInstance(values));
+        PercentileRanks percentiles = global.getAggregations().get("percentile_ranks");
+        assertThat(percentiles, notNullValue());
+        assertThat(percentiles.getName(), equalTo("percentile_ranks"));
+        assertThat((PercentileRanks) global.getProperty("percentile_ranks"), sameInstance(percentiles));
 
     }
 
@@ -202,13 +202,13 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
                         .field("value")
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue);
     }
 
     @Override
@@ -218,13 +218,13 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
                         .field("value")
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue);
     }
 
     @Override
@@ -233,14 +233,14 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .field("value").script(new Script("_value - 1"))
-                        .values(pcts))
+.field("value").script(new Script("_value - 1"))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1);
     }
 
     @Override
@@ -251,15 +251,15 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .field("value")
+.field("value")
                                 .script(new Script("_value - dec", ScriptType.INLINE, null, params))
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1);
     }
 
     @Override
@@ -269,13 +269,13 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
                         .field("values")
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues, maxValues);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues, maxValues);
     }
 
     @Override
@@ -284,14 +284,14 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .field("values").script(new Script("_value - 1"))
-                        .values(pcts))
+.field("values").script(new Script("_value - 1"))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1);
     }
 
     public void testMultiValuedFieldWithValueScriptReverse() throws Exception {
@@ -299,14 +299,14 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .field("values").script(new Script("_value * -1"))
-                        .values(pcts))
+.field("values").script(new Script("_value * -1"))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, -maxValues, -minValues);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, -maxValues, -minValues);
     }
 
     @Override
@@ -317,15 +317,15 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .field("values")
+.field("values")
                                 .script(new Script("_value - dec", ScriptType.INLINE, null, params))
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1);
     }
 
     @Override
@@ -334,14 +334,14 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .script(new Script("doc['value'].value"))
-                        .values(pcts))
+.script(new Script("doc['value'].value"))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue);
     }
 
     @Override
@@ -352,15 +352,15 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .script(
+.script(
                                 new Script("doc['value'].value - dec", ScriptType.INLINE, null, params))
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1);
     }
 
     @Override
@@ -369,14 +369,14 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .script(new Script("doc['values'].values"))
-                        .values(pcts))
+.script(new Script("doc['values'].values"))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues, maxValues);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues, maxValues);
     }
 
     @Override
@@ -390,13 +390,13 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                                 .script(new Script(
                                         "List values = doc['values'].values; double[] res = new double[values.size()]; for (int i = 0; i < res.length; i++) { res[i] = values.get(i) - dec; }; return res;",
                                         ScriptType.INLINE, null, params))
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1);
     }
 
     public void testOrderBySubAggregation() {
@@ -405,7 +405,7 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field("value").interval(2l)
-                            .subAggregation(randomCompression(percentileRanks("percentile_ranks").values(99)))
+                            .subAggregation(randomCompression(percentileRanks("percentile_ranks").percentiles(99)))
                             .order(Order.aggregation("percentile_ranks", "99", asc)))
                 .execute().actionGet();
 
@@ -414,8 +414,8 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         Histogram histo = searchResponse.getAggregations().get("histo");
         double previous = asc ? Double.NEGATIVE_INFINITY : Double.POSITIVE_INFINITY;
         for (Histogram.Bucket bucket : histo.getBuckets()) {
-            PercentileRanks values = bucket.getAggregations().get("percentile_ranks");
-            double p99 = values.percent(99);
+            PercentileRanks percentiles = bucket.getAggregations().get("percentile_ranks");
+            double p99 = percentiles.percent(99);
             if (asc) {
                 assertThat(p99, greaterThanOrEqualTo(previous));
             } else {
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentilesTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentilesTests.java
index da1bfc0..d14638a 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentilesTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentilesTests.java
@@ -31,7 +31,8 @@ import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Order;
 import org.elasticsearch.search.aggregations.metrics.AbstractNumericTestCase;
 import org.elasticsearch.search.aggregations.metrics.percentiles.Percentile;
 import org.elasticsearch.search.aggregations.metrics.percentiles.Percentiles;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesAggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesBuilder;
+
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
@@ -80,7 +81,7 @@ public class TDigestPercentilesTests extends AbstractNumericTestCase {
         return percentiles;
     }
 
-    private static PercentilesAggregatorFactory randomCompression(PercentilesAggregatorFactory builder) {
+    private static PercentilesBuilder randomCompression(PercentilesBuilder builder) {
         if (randomBoolean()) {
             builder.compression(randomIntBetween(20, 120) + randomDouble());
         }
diff --git a/test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java b/test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java
index 1bc2ca0..796872b 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java
@@ -18,12 +18,6 @@
  */
 package org.elasticsearch.test;
 
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
 import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Query;
@@ -52,7 +46,6 @@ import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -71,7 +64,11 @@ import org.elasticsearch.search.rescore.RescoreSearchContext;
 import org.elasticsearch.search.suggest.SuggestionSearchContext;
 import org.elasticsearch.threadpool.ThreadPool;
 
-import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
 
 public class TestSearchContext extends SearchContext {
 
@@ -551,10 +548,6 @@ public class TestSearchContext extends SearchContext {
         return null;
     }
 
-    @Override
-    public FetchPhase fetchPhase() {
-        return null;
-    }
 
     @Override
     public MappedFieldType smartNameFieldType(String name) {
