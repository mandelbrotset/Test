diff --git a/core/src/main/java/org/apache/lucene/queries/MinDocQuery.java b/core/src/main/java/org/apache/lucene/queries/MinDocQuery.java
new file mode 100644
index 0000000..169c017
--- /dev/null
+++ b/core/src/main/java/org/apache/lucene/queries/MinDocQuery.java
@@ -0,0 +1,119 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.lucene.queries;
+
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.search.ConstantScoreScorer;
+import org.apache.lucene.search.ConstantScoreWeight;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.util.Bits;
+
+import java.io.IOException;
+
+/** A {@link Query} that only matches documents that are greater than or equal
+ *  to a configured doc ID. */
+public final class MinDocQuery extends Query {
+
+    private final int minDoc;
+
+    /** Sole constructor. */
+    public MinDocQuery(int minDoc) {
+        this.minDoc = minDoc;
+    }
+
+    @Override
+    public int hashCode() {
+        return 31 * super.hashCode() + minDoc;
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (super.equals(obj) == false) {
+            return false;
+        }
+        MinDocQuery that = (MinDocQuery) obj;
+        return minDoc == that.minDoc;
+    }
+
+    @Override
+    public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
+        return new ConstantScoreWeight(this) {
+            @Override
+            public Scorer scorer(LeafReaderContext context, final Bits acceptDocs) throws IOException {
+                final int maxDoc = context.reader().maxDoc();
+                if (context.docBase + maxDoc <= minDoc) {
+                    return null;
+                }
+                final int segmentMinDoc = Math.max(0, minDoc - context.docBase);
+                final DocIdSetIterator disi = new DocIdSetIterator() {
+
+                    int doc = -1;
+
+                    @Override
+                    public int docID() {
+                        return doc;
+                    }
+
+                    @Override
+                    public int nextDoc() throws IOException {
+                        return advance(doc + 1);
+                    }
+
+                    @Override
+                    public int advance(int target) throws IOException {
+                        assert target > doc;
+                        if (doc == -1) {
+                            // skip directly to minDoc
+                            doc = Math.max(target, segmentMinDoc);
+                        } else {
+                            doc = target;
+                        }
+                        while (doc < maxDoc) {
+                            if (acceptDocs == null || acceptDocs.get(doc)) {
+                                break;
+                            }
+                            doc += 1;
+                        }
+                        if (doc >= maxDoc) {
+                            doc = NO_MORE_DOCS;
+                        }
+                        return doc;
+                    }
+
+                    @Override
+                    public long cost() {
+                        return maxDoc - segmentMinDoc;
+                    }
+
+                };
+                return new ConstantScoreScorer(this, score(), disi);
+            }
+        };
+    }
+
+    @Override
+    public String toString(String field) {
+        return "MinDocQuery(minDoc=" + minDoc  + ")";
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ElasticsearchException.java b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
index a4def64..4f47805 100644
--- a/core/src/main/java/org/elasticsearch/ElasticsearchException.java
+++ b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
@@ -45,8 +45,8 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
 
     public static final String REST_EXCEPTION_SKIP_CAUSE = "rest.exception.cause.skip";
     public static final String REST_EXCEPTION_SKIP_STACK_TRACE = "rest.exception.stacktrace.skip";
-    private static final boolean REST_EXCEPTION_SKIP_STACK_TRACE_DEFAULT = false;
-    private static final boolean REST_EXCEPTION_SKIP_CAUSE_DEFAULT = false;
+    public static final boolean REST_EXCEPTION_SKIP_STACK_TRACE_DEFAULT = true;
+    public static final boolean REST_EXCEPTION_SKIP_CAUSE_DEFAULT = false;
     private static final String INDEX_HEADER_KEY = "es.index";
     private static final String SHARD_HEADER_KEY = "es.shard";
     private static final String RESOURCE_HEADER_TYPE_KEY = "es.resource.type";
diff --git a/core/src/main/java/org/elasticsearch/action/ActionRequestValidationException.java b/core/src/main/java/org/elasticsearch/action/ActionRequestValidationException.java
index d41900c..a1d0869 100644
--- a/core/src/main/java/org/elasticsearch/action/ActionRequestValidationException.java
+++ b/core/src/main/java/org/elasticsearch/action/ActionRequestValidationException.java
@@ -19,45 +19,7 @@
 
 package org.elasticsearch.action;
 
-import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.common.ValidationException;
 
-
-import java.util.ArrayList;
-import java.util.List;
-
-/**
- *
- */
-public class ActionRequestValidationException extends IllegalArgumentException {
-
-    private final List<String> validationErrors = new ArrayList<>();
-
-    public ActionRequestValidationException() {
-        super("validation failed");
-    }
-
-    public void addValidationError(String error) {
-        validationErrors.add(error);
-    }
-
-    public void addValidationErrors(Iterable<String> errors) {
-        for (String error : errors) {
-            validationErrors.add(error);
-        }
-    }
-
-    public List<String> validationErrors() {
-        return validationErrors;
-    }
-
-    @Override
-    public String getMessage() {
-        StringBuilder sb = new StringBuilder();
-        sb.append("Validation Failed: ");
-        int index = 0;
-        for (String error : validationErrors) {
-            sb.append(++index).append(": ").append(error).append(";");
-        }
-        return sb.toString();
-    }
+public class ActionRequestValidationException extends ValidationException {
 }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponse.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponse.java
index e227a97..7172c25 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponse.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.cluster.health;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Maps;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionResponse;
@@ -34,7 +35,6 @@ import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.rest.RestStatus;
 
 import java.io.IOException;
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Locale;
@@ -267,7 +267,7 @@ public class ClusterHealthResponse extends ActionResponse implements Iterable<Cl
         timedOut = in.readBoolean();
         size = in.readVInt();
         if (size == 0) {
-            validationFailures = Collections.emptyList();
+            validationFailures = ImmutableList.of();
         } else {
             for (int i = 0; i < size; i++) {
                 validationFailures.add(in.readString());
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterIndexHealth.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterIndexHealth.java
index 5637871..9184a71 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterIndexHealth.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterIndexHealth.java
@@ -19,10 +19,12 @@
 
 package org.elasticsearch.action.admin.cluster.health;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Maps;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.routing.IndexRoutingTable;
 import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
+import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
@@ -31,7 +33,6 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 
 import java.io.IOException;
-import java.util.Arrays;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Locale;
@@ -178,7 +179,7 @@ public class ClusterIndexHealth implements Iterable<ClusterShardHealth>, Streama
             ClusterShardHealth shardHealth = readClusterShardHealth(in);
             shards.put(shardHealth.getId(), shardHealth);
         }
-        validationFailures = Arrays.asList(in.readStringArray());
+        validationFailures = ImmutableList.copyOf(in.readStringArray());
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesResponse.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesResponse.java
index c933156..2d93030 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesResponse.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.cluster.repositories.get;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.cluster.metadata.RepositoryMetaData;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -26,8 +27,6 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.settings.Settings;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 
@@ -36,13 +35,13 @@ import java.util.List;
  */
 public class GetRepositoriesResponse extends ActionResponse implements Iterable<RepositoryMetaData> {
 
-    private List<RepositoryMetaData> repositories = Collections.emptyList();
+    private ImmutableList<RepositoryMetaData> repositories = ImmutableList.of();
 
 
     GetRepositoriesResponse() {
     }
 
-    GetRepositoriesResponse(List<RepositoryMetaData> repositories) {
+    GetRepositoriesResponse(ImmutableList<RepositoryMetaData> repositories) {
         this.repositories = repositories;
     }
 
@@ -60,7 +59,7 @@ public class GetRepositoriesResponse extends ActionResponse implements Iterable<
     public void readFrom(StreamInput in) throws IOException {
         super.readFrom(in);
         int size = in.readVInt();
-        List<RepositoryMetaData> repositoryListBuilder = new ArrayList<>();
+        ImmutableList.Builder<RepositoryMetaData> repositoryListBuilder = ImmutableList.builder();
         for (int j = 0; j < size; j++) {
             repositoryListBuilder.add(new RepositoryMetaData(
                     in.readString(),
@@ -68,7 +67,7 @@ public class GetRepositoriesResponse extends ActionResponse implements Iterable<
                     Settings.readSettingsFromStream(in))
             );
         }
-        repositories = Collections.unmodifiableList(repositoryListBuilder);
+        repositories = repositoryListBuilder.build();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java
index 1e2e2fd..bf7d7e4 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.cluster.repositories.get;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.master.TransportMasterNodeReadAction;
@@ -36,10 +37,6 @@ import org.elasticsearch.repositories.RepositoryMissingException;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-
 /**
  * Transport action for get repositories operation
  */
@@ -74,11 +71,11 @@ public class TransportGetRepositoriesAction extends TransportMasterNodeReadActio
             if (repositories != null) {
                 listener.onResponse(new GetRepositoriesResponse(repositories.repositories()));
             } else {
-                listener.onResponse(new GetRepositoriesResponse(Collections.<RepositoryMetaData>emptyList()));
+                listener.onResponse(new GetRepositoriesResponse(ImmutableList.<RepositoryMetaData>of()));
             }
         } else {
             if (repositories != null) {
-                List<RepositoryMetaData> repositoryListBuilder = new ArrayList<>();
+                ImmutableList.Builder<RepositoryMetaData> repositoryListBuilder = ImmutableList.builder();
                 for (String repository : request.repositories()) {
                     RepositoryMetaData repositoryMetaData = repositories.repository(repository);
                     if (repositoryMetaData == null) {
@@ -87,7 +84,7 @@ public class TransportGetRepositoriesAction extends TransportMasterNodeReadActio
                     }
                     repositoryListBuilder.add(repositoryMetaData);
                 }
-                listener.onResponse(new GetRepositoriesResponse(Collections.unmodifiableList(repositoryListBuilder)));
+                listener.onResponse(new GetRepositoriesResponse(repositoryListBuilder.build()));
             } else {
                 listener.onFailure(new RepositoryMissingException(request.repositories()[0]));
             }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsResponse.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsResponse.java
index 4ca88da..71b8fa3 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsResponse.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.cluster.snapshots.get;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -28,8 +29,6 @@ import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.snapshots.SnapshotInfo;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 
 /**
@@ -37,12 +36,12 @@ import java.util.List;
  */
 public class GetSnapshotsResponse extends ActionResponse implements ToXContent {
 
-    private List<SnapshotInfo> snapshots = Collections.emptyList();
+    private ImmutableList<SnapshotInfo> snapshots = ImmutableList.of();
 
     GetSnapshotsResponse() {
     }
 
-    GetSnapshotsResponse(List<SnapshotInfo> snapshots) {
+    GetSnapshotsResponse(ImmutableList<SnapshotInfo> snapshots) {
         this.snapshots = snapshots;
     }
 
@@ -59,11 +58,11 @@ public class GetSnapshotsResponse extends ActionResponse implements ToXContent {
     public void readFrom(StreamInput in) throws IOException {
         super.readFrom(in);
         int size = in.readVInt();
-        List<SnapshotInfo> builder = new ArrayList<>();
+        ImmutableList.Builder<SnapshotInfo> builder = ImmutableList.builder();
         for (int i = 0; i < size; i++) {
             builder.add(SnapshotInfo.readSnapshotInfo(in));
         }
-        snapshots = Collections.unmodifiableList(builder);
+        snapshots = builder.build();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java
index b21e16d..40a00c7 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.cluster.snapshots.get;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.master.TransportMasterNodeAction;
@@ -36,8 +37,6 @@ import org.elasticsearch.snapshots.SnapshotsService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 
 /**
@@ -72,7 +71,7 @@ public class TransportGetSnapshotsAction extends TransportMasterNodeAction<GetSn
     @Override
     protected void masterOperation(final GetSnapshotsRequest request, ClusterState state, final ActionListener<GetSnapshotsResponse> listener) {
         try {
-            List<SnapshotInfo> snapshotInfoBuilder = new ArrayList<>();
+            ImmutableList.Builder<SnapshotInfo> snapshotInfoBuilder = ImmutableList.builder();
             if (isAllSnapshots(request.snapshots())) {
                 List<Snapshot> snapshots = snapshotsService.snapshots(request.repository());
                 for (Snapshot snapshot : snapshots) {
@@ -89,7 +88,7 @@ public class TransportGetSnapshotsAction extends TransportMasterNodeAction<GetSn
                     snapshotInfoBuilder.add(new SnapshotInfo(snapshotsService.snapshot(snapshotId)));
                 }
             }
-            listener.onResponse(new GetSnapshotsResponse(Collections.unmodifiableList(snapshotInfoBuilder)));
+            listener.onResponse(new GetSnapshotsResponse(snapshotInfoBuilder.build()));
         } catch (Throwable t) {
             listener.onFailure(t);
         }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java
index 9d13bc9..d6a0cd5 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.cluster.snapshots.status;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.cluster.SnapshotsInProgress.State;
@@ -31,8 +32,6 @@ import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.common.xcontent.XContentFactory;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -49,7 +48,7 @@ public class SnapshotStatus implements ToXContent, Streamable {
 
     private State state;
 
-    private List<SnapshotIndexShardStatus> shards;
+    private ImmutableList<SnapshotIndexShardStatus> shards;
 
     private ImmutableMap<String, SnapshotIndexStatus> indicesStatus;
 
@@ -58,7 +57,7 @@ public class SnapshotStatus implements ToXContent, Streamable {
     private SnapshotStats stats;
 
 
-    SnapshotStatus(SnapshotId snapshotId, State state, List<SnapshotIndexShardStatus> shards) {
+    SnapshotStatus(SnapshotId snapshotId, State state, ImmutableList<SnapshotIndexShardStatus> shards) {
         this.snapshotId = snapshotId;
         this.state = state;
         this.shards = shards;
@@ -128,11 +127,11 @@ public class SnapshotStatus implements ToXContent, Streamable {
         snapshotId = SnapshotId.readSnapshotId(in);
         state = State.fromValue(in.readByte());
         int size = in.readVInt();
-        List<SnapshotIndexShardStatus> builder = new ArrayList<>();
+        ImmutableList.Builder<SnapshotIndexShardStatus> builder = ImmutableList.builder();
         for (int i = 0; i < size; i++) {
             builder.add(SnapshotIndexShardStatus.readShardSnapshotStatus(in));
         }
-        shards = Collections.unmodifiableList(builder);
+        shards = builder.build();
         updateShardStats();
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotsStatusResponse.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotsStatusResponse.java
index e569237..6191a45 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotsStatusResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotsStatusResponse.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.cluster.snapshots.status;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -27,21 +28,18 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
 
 /**
  * Snapshot status response
  */
 public class SnapshotsStatusResponse extends ActionResponse implements ToXContent {
 
-    private List<SnapshotStatus> snapshots = Collections.emptyList();
+    private ImmutableList<SnapshotStatus> snapshots = ImmutableList.of();
 
     SnapshotsStatusResponse() {
     }
 
-    SnapshotsStatusResponse(List<SnapshotStatus> snapshots) {
+    SnapshotsStatusResponse(ImmutableList<SnapshotStatus> snapshots) {
         this.snapshots = snapshots;
     }
 
@@ -50,7 +48,7 @@ public class SnapshotsStatusResponse extends ActionResponse implements ToXConten
      *
      * @return the list of snapshots
      */
-    public List<SnapshotStatus> getSnapshots() {
+    public ImmutableList<SnapshotStatus> getSnapshots() {
         return snapshots;
     }
 
@@ -58,11 +56,11 @@ public class SnapshotsStatusResponse extends ActionResponse implements ToXConten
     public void readFrom(StreamInput in) throws IOException {
         super.readFrom(in);
         int size = in.readVInt();
-        List<SnapshotStatus> builder = new ArrayList<>();
+        ImmutableList.Builder<SnapshotStatus> builder = ImmutableList.builder();
         for (int i = 0; i < size; i++) {
             builder.add(SnapshotStatus.readSnapshotStatus(in));
         }
-        snapshots = Collections.unmodifiableList(builder);
+        snapshots = builder.build();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java
index 12a8135..65ceaa2 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.cluster.snapshots.status;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.support.ActionFilters;
@@ -41,8 +42,6 @@ import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -139,7 +138,7 @@ public class TransportSnapshotsStatusAction extends TransportMasterNodeAction<Sn
     private SnapshotsStatusResponse buildResponse(SnapshotsStatusRequest request, List<SnapshotsInProgress.Entry> currentSnapshots,
                                                   TransportNodesSnapshotsStatus.NodesSnapshotStatus nodeSnapshotStatuses) throws IOException {
         // First process snapshot that are currently processed
-        List<SnapshotStatus> builder = new ArrayList<>();
+        ImmutableList.Builder<SnapshotStatus> builder = ImmutableList.builder();
         Set<SnapshotId> currentSnapshotIds = newHashSet();
         if (!currentSnapshots.isEmpty()) {
             Map<String, TransportNodesSnapshotsStatus.NodeSnapshotStatus> nodeSnapshotStatusMap;
@@ -151,7 +150,7 @@ public class TransportSnapshotsStatusAction extends TransportMasterNodeAction<Sn
 
             for (SnapshotsInProgress.Entry entry : currentSnapshots) {
                 currentSnapshotIds.add(entry.snapshotId());
-                List<SnapshotIndexShardStatus> shardStatusBuilder = new ArrayList<>();
+                ImmutableList.Builder<SnapshotIndexShardStatus> shardStatusBuilder = ImmutableList.builder();
                 for (ImmutableMap.Entry<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shardEntry : entry.shards().entrySet()) {
                     SnapshotsInProgress.ShardSnapshotStatus status = shardEntry.getValue();
                     if (status.nodeId() != null) {
@@ -190,7 +189,7 @@ public class TransportSnapshotsStatusAction extends TransportMasterNodeAction<Sn
                     SnapshotIndexShardStatus shardStatus = new SnapshotIndexShardStatus(shardEntry.getKey(), stage);
                     shardStatusBuilder.add(shardStatus);
                 }
-                builder.add(new SnapshotStatus(entry.snapshotId(), entry.state(), Collections.unmodifiableList(shardStatusBuilder)));
+                builder.add(new SnapshotStatus(entry.snapshotId(), entry.state(), shardStatusBuilder.build()));
             }
         }
         // Now add snapshots on disk that are not currently running
@@ -203,7 +202,7 @@ public class TransportSnapshotsStatusAction extends TransportMasterNodeAction<Sn
                         continue;
                     }
                     Snapshot snapshot = snapshotsService.snapshot(snapshotId);
-                    List<SnapshotIndexShardStatus> shardStatusBuilder = new ArrayList<>();
+                    ImmutableList.Builder<SnapshotIndexShardStatus> shardStatusBuilder = ImmutableList.builder();
                     if (snapshot.state().completed()) {
                         ImmutableMap<ShardId, IndexShardSnapshotStatus> shardStatues = snapshotsService.snapshotShards(snapshotId);
                         for (ImmutableMap.Entry<ShardId, IndexShardSnapshotStatus> shardStatus : shardStatues.entrySet()) {
@@ -223,13 +222,13 @@ public class TransportSnapshotsStatusAction extends TransportMasterNodeAction<Sn
                             default:
                                 throw new IllegalArgumentException("Unknown snapshot state " + snapshot.state());
                         }
-                        builder.add(new SnapshotStatus(snapshotId, state, Collections.unmodifiableList(shardStatusBuilder)));
+                        builder.add(new SnapshotStatus(snapshotId, state, shardStatusBuilder.build()));
                     }
                 }
             }
         }
 
-        return new SnapshotsStatusResponse(Collections.unmodifiableList(builder));
+        return new SnapshotsStatusResponse(builder.build());
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java
index a09ccf2..fd6819c 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.action.admin.indices.alias;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 
 import org.elasticsearch.action.ActionRequestValidationException;
@@ -180,9 +181,9 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
             if (expandAliasesWildcards()) {
                 //for DELETE we expand the aliases
                 String[] indexAsArray = {concreteIndex};
-                ImmutableOpenMap<String, List<AliasMetaData>> aliasMetaData = metaData.findAliases(aliases, indexAsArray);
+                ImmutableOpenMap<String, ImmutableList<AliasMetaData>> aliasMetaData = metaData.findAliases(aliases, indexAsArray);
                 List<String> finalAliases = new ArrayList<>();
-                for (ObjectCursor<List<AliasMetaData>> curAliases : aliasMetaData.values()) {
+                for (ObjectCursor<ImmutableList<AliasMetaData>> curAliases : aliasMetaData.values()) {
                     for (AliasMetaData aliasMeta: curAliases.value) {
                         finalAliases.add(aliasMeta.alias());
                     }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/alias/get/GetAliasesResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/alias/get/GetAliasesResponse.java
index e23faa1..106e864 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/alias/get/GetAliasesResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/alias/get/GetAliasesResponse.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.action.admin.indices.alias.get;
 
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.cluster.metadata.AliasMetaData;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
@@ -28,7 +29,6 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 
 /**
@@ -61,7 +61,7 @@ public class GetAliasesResponse extends ActionResponse {
             for (int j = 0; j < valueSize; j++) {
                 value.add(AliasMetaData.Builder.readFrom(in));
             }
-            aliasesBuilder.put(key, Collections.unmodifiableList(value));
+            aliasesBuilder.put(key, ImmutableList.copyOf(value));
         }
         aliases = aliasesBuilder.build();
     }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/alias/get/TransportGetAliasesAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/alias/get/TransportGetAliasesAction.java
index 7c7dfb0..496b8a3 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/alias/get/TransportGetAliasesAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/alias/get/TransportGetAliasesAction.java
@@ -64,7 +64,7 @@ public class TransportGetAliasesAction extends TransportMasterNodeReadAction<Get
     @Override
     protected void masterOperation(GetAliasesRequest request, ClusterState state, ActionListener<GetAliasesResponse> listener) {
         String[] concreteIndices = indexNameExpressionResolver.concreteIndices(state, request);
-        @SuppressWarnings("unchecked")
+        @SuppressWarnings("unchecked") // ImmutableList to List results incompatible type
                 ImmutableOpenMap<String, List<AliasMetaData>> result = (ImmutableOpenMap) state.metaData().findAliases(request.aliases(), concreteIndices);
         listener.onResponse(new GetAliasesResponse(result));
     }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java
index 0930f8f..3bc0ad0 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.action.admin.indices.get;
 
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+import com.google.common.collect.ImmutableList;
 
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.cluster.metadata.AliasMetaData;
@@ -31,24 +32,21 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
 
 /**
  * A response for a delete index action.
  */
 public class GetIndexResponse extends ActionResponse {
 
-    private ImmutableOpenMap<String, List<IndexWarmersMetaData.Entry>> warmers = ImmutableOpenMap.of();
+    private ImmutableOpenMap<String, ImmutableList<IndexWarmersMetaData.Entry>> warmers = ImmutableOpenMap.of();
     private ImmutableOpenMap<String, ImmutableOpenMap<String, MappingMetaData>> mappings = ImmutableOpenMap.of();
-    private ImmutableOpenMap<String, List<AliasMetaData>> aliases = ImmutableOpenMap.of();
+    private ImmutableOpenMap<String, ImmutableList<AliasMetaData>> aliases = ImmutableOpenMap.of();
     private ImmutableOpenMap<String, Settings> settings = ImmutableOpenMap.of();
     private String[] indices;
 
-    GetIndexResponse(String[] indices, ImmutableOpenMap<String, List<IndexWarmersMetaData.Entry>> warmers,
+    GetIndexResponse(String[] indices, ImmutableOpenMap<String, ImmutableList<IndexWarmersMetaData.Entry>> warmers,
             ImmutableOpenMap<String, ImmutableOpenMap<String, MappingMetaData>> mappings,
-            ImmutableOpenMap<String, List<AliasMetaData>> aliases, ImmutableOpenMap<String, Settings> settings) {
+            ImmutableOpenMap<String, ImmutableList<AliasMetaData>> aliases, ImmutableOpenMap<String, Settings> settings) {
         this.indices = indices;
         if (warmers != null) {
             this.warmers = warmers;
@@ -75,11 +73,11 @@ public class GetIndexResponse extends ActionResponse {
         return indices();
     }
 
-    public ImmutableOpenMap<String, List<IndexWarmersMetaData.Entry>> warmers() {
+    public ImmutableOpenMap<String, ImmutableList<IndexWarmersMetaData.Entry>> warmers() {
         return warmers;
     }
 
-    public ImmutableOpenMap<String, List<IndexWarmersMetaData.Entry>> getWarmers() {
+    public ImmutableOpenMap<String, ImmutableList<IndexWarmersMetaData.Entry>> getWarmers() {
         return warmers();
     }
 
@@ -91,11 +89,11 @@ public class GetIndexResponse extends ActionResponse {
         return mappings();
     }
 
-    public ImmutableOpenMap<String, List<AliasMetaData>> aliases() {
+    public ImmutableOpenMap<String, ImmutableList<AliasMetaData>> aliases() {
         return aliases;
     }
 
-    public ImmutableOpenMap<String, List<AliasMetaData>> getAliases() {
+    public ImmutableOpenMap<String, ImmutableList<AliasMetaData>> getAliases() {
         return aliases();
     }
 
@@ -112,11 +110,11 @@ public class GetIndexResponse extends ActionResponse {
         super.readFrom(in);
         this.indices = in.readStringArray();
         int warmersSize = in.readVInt();
-        ImmutableOpenMap.Builder<String, List<IndexWarmersMetaData.Entry>> warmersMapBuilder = ImmutableOpenMap.builder();
+        ImmutableOpenMap.Builder<String, ImmutableList<IndexWarmersMetaData.Entry>> warmersMapBuilder = ImmutableOpenMap.builder();
         for (int i = 0; i < warmersSize; i++) {
             String key = in.readString();
             int valueSize = in.readVInt();
-            List<IndexWarmersMetaData.Entry> warmerEntryBuilder = new ArrayList<>();
+            ImmutableList.Builder<IndexWarmersMetaData.Entry> warmerEntryBuilder = ImmutableList.builder();
             for (int j = 0; j < valueSize; j++) {
                 warmerEntryBuilder.add(new IndexWarmersMetaData.Entry(
                         in.readString(),
@@ -125,7 +123,7 @@ public class GetIndexResponse extends ActionResponse {
                         in.readBytesReference())
                 );
             }
-            warmersMapBuilder.put(key, Collections.unmodifiableList(warmerEntryBuilder));
+            warmersMapBuilder.put(key, warmerEntryBuilder.build());
         }
         warmers = warmersMapBuilder.build();
         int mappingsSize = in.readVInt();
@@ -141,15 +139,15 @@ public class GetIndexResponse extends ActionResponse {
         }
         mappings = mappingsMapBuilder.build();
         int aliasesSize = in.readVInt();
-        ImmutableOpenMap.Builder<String, List<AliasMetaData>> aliasesMapBuilder = ImmutableOpenMap.builder();
+        ImmutableOpenMap.Builder<String, ImmutableList<AliasMetaData>> aliasesMapBuilder = ImmutableOpenMap.builder();
         for (int i = 0; i < aliasesSize; i++) {
             String key = in.readString();
             int valueSize = in.readVInt();
-            List<AliasMetaData> aliasEntryBuilder = new ArrayList<>();
+            ImmutableList.Builder<AliasMetaData> aliasEntryBuilder = ImmutableList.builder();
             for (int j = 0; j < valueSize; j++) {
                 aliasEntryBuilder.add(AliasMetaData.Builder.readFrom(in));
             }
-            aliasesMapBuilder.put(key, Collections.unmodifiableList(aliasEntryBuilder));
+            aliasesMapBuilder.put(key, aliasEntryBuilder.build());
         }
         aliases = aliasesMapBuilder.build();
         int settingsSize = in.readVInt();
@@ -166,7 +164,7 @@ public class GetIndexResponse extends ActionResponse {
         super.writeTo(out);
         out.writeStringArray(indices);
         out.writeVInt(warmers.size());
-        for (ObjectObjectCursor<String, List<IndexWarmersMetaData.Entry>> indexEntry : warmers) {
+        for (ObjectObjectCursor<String, ImmutableList<IndexWarmersMetaData.Entry>> indexEntry : warmers) {
             out.writeString(indexEntry.key);
             out.writeVInt(indexEntry.value.size());
             for (IndexWarmersMetaData.Entry warmerEntry : indexEntry.value) {
@@ -186,7 +184,7 @@ public class GetIndexResponse extends ActionResponse {
             }
         }
         out.writeVInt(aliases.size());
-        for (ObjectObjectCursor<String, List<AliasMetaData>> indexEntry : aliases) {
+        for (ObjectObjectCursor<String, ImmutableList<AliasMetaData>> indexEntry : aliases) {
             out.writeString(indexEntry.key);
             out.writeVInt(indexEntry.value.size());
             for (AliasMetaData aliasEntry : indexEntry.value) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/get/TransportGetIndexAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/get/TransportGetIndexAction.java
index e398541..89360ce 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/get/TransportGetIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/get/TransportGetIndexAction.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.indices.get;
 
+import com.google.common.collect.ImmutableList;
 
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.admin.indices.get.GetIndexRequest.Feature;
@@ -40,8 +41,6 @@ import org.elasticsearch.search.warmer.IndexWarmersMetaData.Entry;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
-import java.util.List;
-
 /**
  * Get index action.
  */
@@ -72,9 +71,9 @@ public class TransportGetIndexAction extends TransportClusterInfoAction<GetIndex
     @Override
     protected void doMasterOperation(final GetIndexRequest request, String[] concreteIndices, final ClusterState state,
                                      final ActionListener<GetIndexResponse> listener) {
-        ImmutableOpenMap<String, List<Entry>> warmersResult = ImmutableOpenMap.of();
+        ImmutableOpenMap<String, ImmutableList<Entry>> warmersResult = ImmutableOpenMap.of();
         ImmutableOpenMap<String, ImmutableOpenMap<String, MappingMetaData>> mappingsResult = ImmutableOpenMap.of();
-        ImmutableOpenMap<String, List<AliasMetaData>> aliasesResult = ImmutableOpenMap.of();
+        ImmutableOpenMap<String, ImmutableList<AliasMetaData>> aliasesResult = ImmutableOpenMap.of();
         ImmutableOpenMap<String, Settings> settings = ImmutableOpenMap.of();
         Feature[] features = request.features();
         boolean doneAliases = false;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/segments/ShardSegments.java b/core/src/main/java/org/elasticsearch/action/admin/indices/segments/ShardSegments.java
index c901f53..6e754a2 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/segments/ShardSegments.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/segments/ShardSegments.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.indices.segments;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.action.support.broadcast.BroadcastShardResponse;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -27,7 +28,6 @@ import org.elasticsearch.index.engine.Segment;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 
@@ -93,7 +93,7 @@ public class ShardSegments extends BroadcastShardResponse implements Iterable<Se
         shardRouting = readShardRoutingEntry(in);
         int size = in.readVInt();
         if (size == 0) {
-            segments = Collections.emptyList();
+            segments = ImmutableList.of();
         } else {
             segments = new ArrayList<>(size);
             for (int i = 0; i < size; i++) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoresResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoresResponse.java
index 84b39d4..50d305e 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoresResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoresResponse.java
@@ -21,6 +21,7 @@ package org.elasticsearch.action.admin.indices.shards;
 
 import com.carrotsearch.hppc.cursors.IntObjectCursor;
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.action.ShardOperationFailedException;
@@ -37,7 +38,6 @@ import org.elasticsearch.common.xcontent.XContentBuilderString;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 
 import static org.elasticsearch.action.admin.indices.shards.IndicesShardStoresResponse.StoreStatus.*;
@@ -258,15 +258,15 @@ public class IndicesShardStoresResponse extends ActionResponse implements ToXCon
     }
 
     private ImmutableOpenMap<String, ImmutableOpenIntMap<List<StoreStatus>>> storeStatuses;
-    private List<Failure> failures;
+    private ImmutableList<Failure> failures;
 
-    public IndicesShardStoresResponse(ImmutableOpenMap<String, ImmutableOpenIntMap<List<StoreStatus>>> storeStatuses, List<Failure> failures) {
+    public IndicesShardStoresResponse(ImmutableOpenMap<String, ImmutableOpenIntMap<List<StoreStatus>>> storeStatuses, ImmutableList<Failure> failures) {
         this.storeStatuses = storeStatuses;
         this.failures = failures;
     }
 
     IndicesShardStoresResponse() {
-        this(ImmutableOpenMap.<String, ImmutableOpenIntMap<List<StoreStatus>>>of(), Collections.<Failure>emptyList());
+        this(ImmutableOpenMap.<String, ImmutableOpenIntMap<List<StoreStatus>>>of(), ImmutableList.<Failure>of());
     }
 
     /**
@@ -281,7 +281,7 @@ public class IndicesShardStoresResponse extends ActionResponse implements ToXCon
      * Returns node {@link Failure}s encountered
      * while executing the request
      */
-    public List<Failure> getFailures() {
+    public ImmutableList<Failure> getFailures() {
         return failures;
     }
 
@@ -306,12 +306,12 @@ public class IndicesShardStoresResponse extends ActionResponse implements ToXCon
             storeStatusesBuilder.put(index, shardEntries.build());
         }
         int numFailure = in.readVInt();
-        List<Failure> failureBuilder = new ArrayList<>();
+        ImmutableList.Builder<Failure> failureBuilder = ImmutableList.builder();
         for (int i = 0; i < numFailure; i++) {
             failureBuilder.add(Failure.readFailure(in));
         }
         storeStatuses = storeStatusesBuilder.build();
-        failures = Collections.unmodifiableList(failureBuilder);
+        failures = failureBuilder.build();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/shards/TransportIndicesShardStoresAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/shards/TransportIndicesShardStoresAction.java
index 01613d6..b783ce1 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/shards/TransportIndicesShardStoresAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/shards/TransportIndicesShardStoresAction.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.action.admin.indices.shards;
 
+import com.google.common.collect.ImmutableList;
 import org.apache.lucene.util.CollectionUtil;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.FailedNodeException;
@@ -33,11 +34,7 @@ import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.IndexRoutingTable;
-import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
-import org.elasticsearch.cluster.routing.RoutingNodes;
-import org.elasticsearch.cluster.routing.RoutingTable;
-import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.common.collect.ImmutableOpenIntMap;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.inject.Inject;
@@ -51,11 +48,7 @@ import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.Queue;
-import java.util.Set;
+import java.util.*;
 import java.util.concurrent.ConcurrentLinkedQueue;
 
 /**
@@ -164,7 +157,7 @@ public class TransportIndicesShardStoresAction extends TransportMasterNodeReadAc
 
             void finish() {
                 ImmutableOpenMap.Builder<String, ImmutableOpenIntMap<java.util.List<IndicesShardStoresResponse.StoreStatus>>> indicesStoreStatusesBuilder = ImmutableOpenMap.builder();
-                java.util.List<IndicesShardStoresResponse.Failure> failureBuilder = new ArrayList<>();
+                ImmutableList.Builder<IndicesShardStoresResponse.Failure> failureBuilder = ImmutableList.builder();
                 for (Response fetchResponse : fetchResponses) {
                     ImmutableOpenIntMap<java.util.List<IndicesShardStoresResponse.StoreStatus>> indexStoreStatuses = indicesStoreStatusesBuilder.get(fetchResponse.shardId.getIndex());
                     final ImmutableOpenIntMap.Builder<java.util.List<IndicesShardStoresResponse.StoreStatus>> indexShardsBuilder;
@@ -190,7 +183,7 @@ public class TransportIndicesShardStoresAction extends TransportMasterNodeReadAc
                         failureBuilder.add(new IndicesShardStoresResponse.Failure(failure.nodeId(), fetchResponse.shardId.getIndex(), fetchResponse.shardId.id(), failure.getCause()));
                     }
                 }
-                listener.onResponse(new IndicesShardStoresResponse(indicesStoreStatusesBuilder.build(), Collections.unmodifiableList(failureBuilder)));
+                listener.onResponse(new IndicesShardStoresResponse(indicesStoreStatusesBuilder.build(), failureBuilder.build()));
             }
 
             private IndicesShardStoresResponse.StoreStatus.Allocation getAllocation(String index, int shardID, DiscoveryNode node) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryResponse.java
index 2d3c0a0..3d1ef78 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryResponse.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.indices.validate.query;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.action.ShardOperationFailedException;
 import org.elasticsearch.action.support.broadcast.BroadcastResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -26,7 +27,6 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 
 import static org.elasticsearch.action.admin.indices.validate.query.QueryExplanation.readQueryExplanation;
@@ -51,7 +51,7 @@ public class ValidateQueryResponse extends BroadcastResponse {
         this.valid = valid;
         this.queryExplanations = queryExplanations;
         if (queryExplanations == null) {
-            this.queryExplanations = Collections.emptyList();
+            this.queryExplanations = ImmutableList.of();
         }
     }
 
@@ -67,7 +67,7 @@ public class ValidateQueryResponse extends BroadcastResponse {
      */
     public List<? extends QueryExplanation> getQueryExplanation() {
         if (queryExplanations == null) {
-            return Collections.emptyList();
+            return ImmutableList.of();
         }
         return queryExplanations;
     }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java
index 3ed444c..cb45d36 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.action.admin.indices.warmer.get;
 
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.bytes.BytesReference;
@@ -29,9 +30,6 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
 
 /**
  * Holds a warmer-name to a list of {@link IndexWarmersMetaData} mapping for each warmer specified
@@ -40,20 +38,20 @@ import java.util.List;
  */
 public class GetWarmersResponse extends ActionResponse {
 
-    private ImmutableOpenMap<String, List<IndexWarmersMetaData.Entry>> warmers = ImmutableOpenMap.of();
+    private ImmutableOpenMap<String, ImmutableList<IndexWarmersMetaData.Entry>> warmers = ImmutableOpenMap.of();
 
-    GetWarmersResponse(ImmutableOpenMap<String, List<IndexWarmersMetaData.Entry>> warmers) {
+    GetWarmersResponse(ImmutableOpenMap<String, ImmutableList<IndexWarmersMetaData.Entry>> warmers) {
         this.warmers = warmers;
     }
 
     GetWarmersResponse() {
     }
 
-    public ImmutableOpenMap<String, List<IndexWarmersMetaData.Entry>> warmers() {
+    public ImmutableOpenMap<String, ImmutableList<IndexWarmersMetaData.Entry>> warmers() {
         return warmers;
     }
 
-    public ImmutableOpenMap<String, List<IndexWarmersMetaData.Entry>> getWarmers() {
+    public ImmutableOpenMap<String, ImmutableList<IndexWarmersMetaData.Entry>> getWarmers() {
         return warmers();
     }
 
@@ -61,11 +59,11 @@ public class GetWarmersResponse extends ActionResponse {
     public void readFrom(StreamInput in) throws IOException {
         super.readFrom(in);
         int size = in.readVInt();
-        ImmutableOpenMap.Builder<String, List<IndexWarmersMetaData.Entry>> indexMapBuilder = ImmutableOpenMap.builder();
+        ImmutableOpenMap.Builder<String, ImmutableList<IndexWarmersMetaData.Entry>> indexMapBuilder = ImmutableOpenMap.builder();
         for (int i = 0; i < size; i++) {
             String key = in.readString();
             int valueSize = in.readVInt();
-            List<IndexWarmersMetaData.Entry> warmerEntryBuilder = new ArrayList<>();
+            ImmutableList.Builder<IndexWarmersMetaData.Entry> warmerEntryBuilder = ImmutableList.builder();
             for (int j = 0; j < valueSize; j++) {
                 String name = in.readString();
                 String[] types = in.readStringArray();
@@ -79,7 +77,7 @@ public class GetWarmersResponse extends ActionResponse {
                                 source)
                 );
             }
-            indexMapBuilder.put(key, Collections.unmodifiableList(warmerEntryBuilder));
+            indexMapBuilder.put(key, warmerEntryBuilder.build());
         }
         warmers = indexMapBuilder.build();
     }
@@ -88,7 +86,7 @@ public class GetWarmersResponse extends ActionResponse {
     public void writeTo(StreamOutput out) throws IOException {
         super.writeTo(out);
         out.writeVInt(warmers.size());
-        for (ObjectObjectCursor<String, List<IndexWarmersMetaData.Entry>> indexEntry : warmers) {
+        for (ObjectObjectCursor<String, ImmutableList<IndexWarmersMetaData.Entry>> indexEntry : warmers) {
             out.writeString(indexEntry.key);
             out.writeVInt(indexEntry.value.size());
             for (IndexWarmersMetaData.Entry warmerEntry : indexEntry.value) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/TransportGetWarmersAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/TransportGetWarmersAction.java
index 50d972b..0504e32 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/TransportGetWarmersAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/TransportGetWarmersAction.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.indices.warmer.get;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.master.info.TransportClusterInfoAction;
@@ -34,8 +35,6 @@ import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
-import java.util.List;
-
 /**
  * Internal Actions executed on the master fetching the warmer from the cluster state metadata.
  *
@@ -67,7 +66,7 @@ public class TransportGetWarmersAction extends TransportClusterInfoAction<GetWar
 
     @Override
     protected void doMasterOperation(final GetWarmersRequest request, String[] concreteIndices, final ClusterState state, final ActionListener<GetWarmersResponse> listener) {
-        ImmutableOpenMap<String, List<IndexWarmersMetaData.Entry>> result = state.metaData().findWarmers(
+        ImmutableOpenMap<String, ImmutableList<IndexWarmersMetaData.Entry>> result = state.metaData().findWarmers(
                 concreteIndices, request.types(), request.warmers()
         );
         listener.onResponse(new GetWarmersResponse(result));
diff --git a/core/src/main/java/org/elasticsearch/action/percolate/PercolateShardResponse.java b/core/src/main/java/org/elasticsearch/action/percolate/PercolateShardResponse.java
index 5416e2f..c626cda 100644
--- a/core/src/main/java/org/elasticsearch/action/percolate/PercolateShardResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/percolate/PercolateShardResponse.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.action.percolate;
 
+import com.google.common.collect.ImmutableList;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.action.support.broadcast.BroadcastShardResponse;
 import org.elasticsearch.common.bytes.BytesReference;
@@ -34,7 +35,6 @@ import org.elasticsearch.search.query.QuerySearchResult;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -45,7 +45,7 @@ public class PercolateShardResponse extends BroadcastShardResponse {
 
     private static final BytesRef[] EMPTY_MATCHES = new BytesRef[0];
     private static final float[] EMPTY_SCORES = new float[0];
-    private static final List<Map<String, HighlightField>> EMPTY_HL = Collections.emptyList();
+    private static final List<Map<String, HighlightField>> EMPTY_HL = ImmutableList.of();
 
     private long count;
     private float[] scores;
diff --git a/core/src/main/java/org/elasticsearch/action/search/SearchType.java b/core/src/main/java/org/elasticsearch/action/search/SearchType.java
index 432e816..6d91e40 100644
--- a/core/src/main/java/org/elasticsearch/action/search/SearchType.java
+++ b/core/src/main/java/org/elasticsearch/action/search/SearchType.java
@@ -54,7 +54,9 @@ public enum SearchType {
     /**
      * Performs scanning of the results which executes the search without any sorting.
      * It will automatically start scrolling the result set.
+     * @deprecated will be removed in 3.0, you should do a regular scroll instead, ordered by `_doc`
      */
+    @Deprecated
     SCAN((byte) 4),
     /**
      * Only counts the results, will still execute aggregations and the like.
@@ -69,6 +71,7 @@ public enum SearchType {
     public static final SearchType DEFAULT = QUERY_THEN_FETCH;
 
     private static final ParseField COUNT_VALUE = new ParseField("count").withAllDeprecated("query_then_fetch");
+    private static final ParseField SCAN_VALUE = new ParseField("scan").withAllDeprecated("query_then_fetch sorting on `_doc`");
 
     private byte id;
 
@@ -121,7 +124,7 @@ public enum SearchType {
             return SearchType.QUERY_THEN_FETCH;
         } else if ("query_and_fetch".equals(searchType)) {
             return SearchType.QUERY_AND_FETCH;
-        } else if ("scan".equals(searchType)) {
+        } else if (parseFieldMatcher.match(searchType, SCAN_VALUE)) {
             return SearchType.SCAN;
         } else if (parseFieldMatcher.match(searchType, COUNT_VALUE)) {
             return SearchType.COUNT;
diff --git a/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchScanAction.java b/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchScanAction.java
index 6edaf8f..c5ea867 100644
--- a/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchScanAction.java
+++ b/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchScanAction.java
@@ -40,6 +40,7 @@ import org.elasticsearch.threadpool.ThreadPool;
 
 import static org.elasticsearch.action.search.type.TransportSearchHelper.buildScrollId;
 
+@Deprecated // remove in 3.0
 public class TransportSearchScanAction extends TransportSearchTypeAction {
 
     @Inject
diff --git a/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java b/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
index c284044..1c77bed 100644
--- a/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
+++ b/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.update;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.ActionRunnable;
@@ -57,7 +58,6 @@ import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
-import java.util.Collections;
 import java.util.Map;
 
 /**
@@ -153,10 +153,10 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
         ShardRouting shard;
         while ((shard = shardIterator.nextOrNull()) != null) {
             if (shard.primary()) {
-                return new PlainShardIterator(shardIterator.shardId(), Collections.singletonList(shard));
+                return new PlainShardIterator(shardIterator.shardId(), ImmutableList.of(shard));
             }
         }
-        return new PlainShardIterator(shardIterator.shardId(), Collections.<ShardRouting>emptyList());
+        return new PlainShardIterator(shardIterator.shardId(), ImmutableList.<ShardRouting>of());
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
index d1143a5..bf37fc2 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
@@ -21,15 +21,12 @@ package org.elasticsearch.bootstrap;
 
 import org.apache.lucene.util.Constants;
 import org.apache.lucene.util.StringHelper;
-import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.PidFile;
 import org.elasticsearch.common.SuppressForbidden;
 import org.elasticsearch.common.cli.CliTool;
 import org.elasticsearch.common.cli.Terminal;
 import org.elasticsearch.common.collect.Tuple;
-import org.elasticsearch.common.inject.CreationException;
-import org.elasticsearch.common.inject.spi.Message;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
@@ -44,16 +41,14 @@ import org.elasticsearch.node.NodeBuilder;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 
 import java.util.Locale;
-import java.util.Set;
 import java.util.concurrent.CountDownLatch;
 
-import static com.google.common.collect.Sets.newHashSet;
 import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
 
 /**
- * A main entry point when starting from the command line.
+ * Internal startup code.
  */
-public class Bootstrap {
+final class Bootstrap {
 
     private static volatile Bootstrap INSTANCE;
 
@@ -137,10 +132,6 @@ public class Bootstrap {
         OsProbe.getInstance();
     }
 
-    public static boolean isMemoryLocked() {
-        return Natives.isMemoryLocked();
-    }
-
     private void setup(boolean addShutdownHook, Settings settings, Environment environment) throws Exception {
         initializeNatives(settings.getAsBoolean("bootstrap.mlockall", false),
                 settings.getAsBoolean("bootstrap.ctrlhandler", true));
@@ -222,7 +213,11 @@ public class Bootstrap {
         }
     }
 
-    public static void main(String[] args) throws Throwable {
+    /**
+     * This method is invoked by {@link Elasticsearch#main(String[])}
+     * to startup elasticsearch.
+     */
+    static void init(String[] args) throws Throwable {
         BootstrapCLIParser bootstrapCLIParser = new BootstrapCLIParser();
         CliTool.ExitStatus status = bootstrapCLIParser.execute(args);
 
@@ -277,11 +272,19 @@ public class Bootstrap {
                 closeSysError();
             }
         } catch (Throwable e) {
+            // disable console logging, so user does not see the exception twice (jvm will show it already)
+            if (foreground) {
+                Loggers.disableConsoleLogging();
+            }
             ESLogger logger = Loggers.getLogger(Bootstrap.class);
             if (INSTANCE.node != null) {
                 logger = Loggers.getLogger(Bootstrap.class, INSTANCE.node.settings().get("name"));
             }
             logger.error("Exception", e);
+            // re-enable it if appropriate, so they can see any logging during the shutdown process
+            if (foreground) {
+                Loggers.enableConsoleLogging();
+            }
             
             throw e;
         }
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/BootstrapCLIParser.java b/core/src/main/java/org/elasticsearch/bootstrap/BootstrapCLIParser.java
index 1e3f839..f22c6d1 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/BootstrapCLIParser.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/BootstrapCLIParser.java
@@ -38,7 +38,7 @@ import java.util.Properties;
 import static org.elasticsearch.common.cli.CliToolConfig.Builder.cmd;
 import static org.elasticsearch.common.cli.CliToolConfig.Builder.optionBuilder;
 
-public class BootstrapCLIParser extends CliTool {
+final class BootstrapCLIParser extends CliTool {
 
     private static final CliToolConfig CONFIG = CliToolConfig.config("elasticsearch", BootstrapCLIParser.class)
             .cmds(Start.CMD, Version.CMD)
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/BootstrapInfo.java b/core/src/main/java/org/elasticsearch/bootstrap/BootstrapInfo.java
new file mode 100644
index 0000000..829c45f
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/bootstrap/BootstrapInfo.java
@@ -0,0 +1,46 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.bootstrap;
+
+/** 
+ * Exposes system startup information 
+ */
+public final class BootstrapInfo {
+
+    /** no instantiation */
+    private BootstrapInfo() {}
+    
+    /** 
+     * Returns true if we successfully loaded native libraries.
+     * <p>
+     * If this returns false, then native operations such as locking
+     * memory did not work.
+     */
+    public static boolean isNativesAvailable() {
+        return Natives.JNA_AVAILABLE;
+    }
+    
+    /** 
+     * Returns true if we were able to lock the process's address space.
+     */
+    public static boolean isMemoryLocked() {
+        return Natives.isMemoryLocked();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java b/core/src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java
index 21910e5..f3a07f0 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java
@@ -20,11 +20,23 @@
 package org.elasticsearch.bootstrap;
 
 /**
- * A wrapper around {@link Bootstrap} just so the process will look nicely on things like jps.
+ * This class starts elasticsearch.
  */
-public class Elasticsearch extends Bootstrap {
+public final class Elasticsearch {
 
-    public static void main(String[] args) throws Throwable {
-        Bootstrap.main(args);
+    /** no instantiation */
+    private Elasticsearch() {}
+
+    /**
+     * Main entry point for starting elasticsearch
+     */
+    public static void main(String[] args) throws StartupError {
+        try {
+            Bootstrap.init(args);
+        } catch (Throwable t) {
+            // format exceptions to the console in a special way
+            // to avoid 2MB stacktraces from guice, etc.
+            throw new StartupError(t);
+        }
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/JNACLibrary.java b/core/src/main/java/org/elasticsearch/bootstrap/JNACLibrary.java
index 226fea6..972d1a1 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/JNACLibrary.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/JNACLibrary.java
@@ -59,7 +59,7 @@ final class JNACLibrary {
         public long rlim_max = 0;
         
         @Override
-        protected List getFieldOrder() {
+        protected List<String> getFieldOrder() {
             return Arrays.asList(new String[] { "rlim_cur", "rlim_max" });
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/JNAKernel32Library.java b/core/src/main/java/org/elasticsearch/bootstrap/JNAKernel32Library.java
index 1792343..4dcbd6c 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/JNAKernel32Library.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/JNAKernel32Library.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.bootstrap;
 
+import com.google.common.collect.ImmutableList;
 import com.sun.jna.*;
 import com.sun.jna.win32.StdCallLibrary;
 
@@ -28,14 +29,13 @@ import org.elasticsearch.common.logging.Loggers;
 
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.List;
 
 
 /**
  * Library for Windows/Kernel32
  */
-class JNAKernel32Library {
+final class JNAKernel32Library {
 
     private static final ESLogger logger = Loggers.getLogger(JNAKernel32Library.class);
 
@@ -85,8 +85,8 @@ class JNAKernel32Library {
         return result;
     }
 
-    List<Object> getCallbacks() {
-        return Collections.<Object>unmodifiableList(callbacks);
+    ImmutableList<Object> getCallbacks() {
+        return ImmutableList.builder().addAll(callbacks).build();
     }
 
     /**
@@ -148,7 +148,7 @@ class JNAKernel32Library {
         public NativeLong Type;
 
         @Override
-        protected List getFieldOrder() {
+        protected List<String> getFieldOrder() {
             return Arrays.asList(new String[]{"BaseAddress", "AllocationBase", "AllocationProtect", "RegionSize", "State", "Protect", "Type"});
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java b/core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java
index ba6eef1..7a76d37 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java
@@ -34,10 +34,13 @@ import static org.elasticsearch.bootstrap.JNAKernel32Library.SizeT;
  */
 class JNANatives {
 
+    /** no instantiation */
+    private JNANatives() {}
+
     private static final ESLogger logger = Loggers.getLogger(JNANatives.class);
 
     // Set to true, in case native mlockall call was successful
-    public static boolean LOCAL_MLOCKALL = false;
+    static boolean LOCAL_MLOCKALL = false;
 
     static void tryMlockall() {
         int errno = Integer.MIN_VALUE;
@@ -72,16 +75,18 @@ class JNANatives {
         }
 
         // mlockall failed for some reason
-        logger.warn("Unable to lock JVM Memory: error=" + errno + ",reason=" + errMsg + ". This can result in part of the JVM being swapped out.");
+        logger.warn("Unable to lock JVM Memory: error=" + errno + ",reason=" + errMsg);
+        logger.warn("This can result in part of the JVM being swapped out.");
         if (errno == JNACLibrary.ENOMEM) {
             if (rlimitSuccess) {
                 logger.warn("Increase RLIMIT_MEMLOCK, soft limit: " + rlimitToString(softLimit) + ", hard limit: " + rlimitToString(hardLimit));
                 if (Constants.LINUX) {
                     // give specific instructions for the linux case to make it easy
+                    String user = System.getProperty("user.name");
                     logger.warn("These can be adjusted by modifying /etc/security/limits.conf, for example: \n" +
-                                "\t# allow user 'esuser' mlockall\n" +
-                                "\tesuser soft memlock unlimited\n" +
-                                "\tesuser hard memlock unlimited"
+                                "\t# allow user '" + user + "' mlockall\n" +
+                                "\t" + user + " soft memlock unlimited\n" +
+                                "\t" + user + " hard memlock unlimited"
                                );
                     logger.warn("If you are logged in interactively, you will have to re-login for the new limits to take effect.");
                 }
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/JVMCheck.java b/core/src/main/java/org/elasticsearch/bootstrap/JVMCheck.java
index db9e2ba..5c402bd 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/JVMCheck.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/JVMCheck.java
@@ -29,6 +29,8 @@ import java.util.Map;
 
 /** Checks that the JVM is ok and won't cause index corruption */
 final class JVMCheck {
+    /** no instantiation */
+    private JVMCheck() {}
     
     /**
      * URL with latest JVM recommendations
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/JarHell.java b/core/src/main/java/org/elasticsearch/bootstrap/JarHell.java
index 1f6e334..9062140 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/JarHell.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/JarHell.java
@@ -37,15 +37,30 @@ import java.util.Arrays;
 import java.util.Enumeration;
 import java.util.HashMap;
 import java.util.HashSet;
+import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 import java.util.jar.JarEntry;
 import java.util.jar.JarFile;
 import java.util.jar.Manifest;
 
-/** Simple check for duplicate class files across the classpath */
+/**
+ * Simple check for duplicate class files across the classpath.
+ * <p>
+ * This class checks for incompatibilities in the following ways:
+ * <ul>
+ *   <li>Checks that class files are not duplicated across jars.</li>
+ *   <li>Checks any {@code X-Compile-Target-JDK} value in the jar
+ *       manifest is compatible with current JRE</li>
+ *   <li>Checks any {@code X-Compile-Elasticsearch-Version} value in
+ *       the jar manifest is compatible with the current ES</li>
+ * </ul>
+ */
 public class JarHell {
 
+    /** no instantiation */
+    private JarHell() {}
+
     /** Simple driver class, can be used eg. from builds. Returns non-zero on jar-hell */
     @SuppressForbidden(reason = "command line tool")
     public static void main(String args[]) throws Exception {
@@ -69,7 +84,7 @@ public class JarHell {
             logger.debug("sun.boot.class.path: {}", System.getProperty("sun.boot.class.path"));
             logger.debug("classloader urls: {}", Arrays.toString(((URLClassLoader)loader).getURLs()));
         }
-        checkJarHell(((URLClassLoader)loader).getURLs());
+        checkJarHell(((URLClassLoader) loader).getURLs());
     }
 
     /**
@@ -141,6 +156,7 @@ public class JarHell {
         // give a nice error if jar requires a newer java version
         String targetVersion = manifest.getMainAttributes().getValue("X-Compile-Target-JDK");
         if (targetVersion != null) {
+            checkVersionFormat(targetVersion);
             checkJavaVersion(jar.toString(), targetVersion);
         }
 
@@ -153,23 +169,34 @@ public class JarHell {
         }
     }
 
+    public static void checkVersionFormat(String targetVersion) {
+        if (!JavaVersion.isValid(targetVersion)) {
+            throw new IllegalStateException(
+                    String.format(
+                            Locale.ROOT,
+                            "version string must be a sequence of nonnegative decimal integers separated by \".\"'s and may have leading zeros but was %s",
+                            targetVersion
+                    )
+            );
+        }
+    }
+
     /**
      * Checks that the java specification version {@code targetVersion}
      * required by {@code resource} is compatible with the current installation.
      */
     public static void checkJavaVersion(String resource, String targetVersion) {
-        String systemVersion = System.getProperty("java.specification.version");
-        float current = Float.POSITIVE_INFINITY;
-        float target = Float.NEGATIVE_INFINITY;
-        try {
-            current = Float.parseFloat(systemVersion);
-            target = Float.parseFloat(targetVersion);
-        } catch (NumberFormatException e) {
-            // some spec changed, time for a more complex parser
-        }
-        if (current < target) {
-            throw new IllegalStateException(resource + " requires Java " + targetVersion
-                    + ", your system: " + systemVersion);
+        JavaVersion version = JavaVersion.parse(targetVersion);
+        if (JavaVersion.current().compareTo(version) < 0) {
+            throw new IllegalStateException(
+                    String.format(
+                            Locale.ROOT,
+                            "%s requires Java %s:, your system: %s",
+                            resource,
+                            targetVersion,
+                            JavaVersion.current().toString()
+                    )
+            );
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/JavaVersion.java b/core/src/main/java/org/elasticsearch/bootstrap/JavaVersion.java
new file mode 100644
index 0000000..bf8e996
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/bootstrap/JavaVersion.java
@@ -0,0 +1,84 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.bootstrap;
+
+import org.elasticsearch.common.Strings;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+
+class JavaVersion implements Comparable<JavaVersion> {
+    private final List<Integer> version;
+
+    public List<Integer> getVersion() {
+        return Collections.unmodifiableList(version);
+    }
+
+    private JavaVersion(List<Integer> version) {
+        this.version = version;
+    }
+
+    public static JavaVersion parse(String value) {
+        if (value == null) {
+            throw new NullPointerException("value");
+        }
+        if ("".equals(value)) {
+            throw new IllegalArgumentException("value");
+        }
+
+        List<Integer> version = new ArrayList<>();
+        String[] components = value.split("\\.");
+        for (String component : components) {
+            version.add(Integer.valueOf(component));
+        }
+
+        return new JavaVersion(version);
+    }
+
+    public static boolean isValid(String value) {
+        return value.matches("^0*[0-9]+(\\.[0-9]+)*$");
+    }
+
+    private final static JavaVersion CURRENT = parse(System.getProperty("java.specification.version"));
+
+    public static JavaVersion current() {
+        return CURRENT;
+    }
+
+    @Override
+    public int compareTo(JavaVersion o) {
+        int len = Math.max(version.size(), o.version.size());
+        for (int i = 0; i < len; i++) {
+            int d = (i < version.size() ? version.get(i) : 0);
+            int s = (i < o.version.size() ? o.version.get(i) : 0);
+            if (s < d)
+                return 1;
+            if (s > d)
+                return -1;
+        }
+        return 0;
+    }
+
+    @Override
+    public String toString() {
+        return Strings.collectionToDelimitedString(version, ".");
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Natives.java b/core/src/main/java/org/elasticsearch/bootstrap/Natives.java
index 2048895..05b0088 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Natives.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Natives.java
@@ -26,27 +26,32 @@ import org.elasticsearch.common.logging.Loggers;
  * The Natives class is a wrapper class that checks if the classes necessary for calling native methods are available on
  * startup. If they are not available, this class will avoid calling code that loads these classes.
  */
-class Natives {
+final class Natives {
+    /** no instantiation */
+    private Natives() {}
+
     private static final ESLogger logger = Loggers.getLogger(Natives.class);
 
     // marker to determine if the JNA class files are available to the JVM
-    private static boolean jnaAvailable = false;
+    static final boolean JNA_AVAILABLE;
 
     static {
+        boolean v = false;
         try {
             // load one of the main JNA classes to see if the classes are available. this does not ensure that all native
             // libraries are available, only the ones necessary by JNA to function
             Class.forName("com.sun.jna.Native");
-            jnaAvailable = true;
+            v = true;
         } catch (ClassNotFoundException e) {
             logger.warn("JNA not found. native methods will be disabled.", e);
         } catch (UnsatisfiedLinkError e) {
             logger.warn("unable to load JNA native support library, native methods will be disabled.", e);
         }
+        JNA_AVAILABLE = v;
     }
 
     static void tryMlockall() {
-        if (!jnaAvailable) {
+        if (!JNA_AVAILABLE) {
             logger.warn("cannot mlockall because JNA is not available");
             return;
         }
@@ -54,7 +59,7 @@ class Natives {
     }
 
     static boolean definitelyRunningAsRoot() {
-        if (!jnaAvailable) {
+        if (!JNA_AVAILABLE) {
             logger.warn("cannot check if running as root because JNA is not available");
             return false;
         }
@@ -62,7 +67,7 @@ class Natives {
     }
 
     static void tryVirtualLock() {
-        if (!jnaAvailable) {
+        if (!JNA_AVAILABLE) {
             logger.warn("cannot mlockall because JNA is not available");
             return;
         }
@@ -70,7 +75,7 @@ class Natives {
     }
 
     static void addConsoleCtrlHandler(ConsoleCtrlHandler handler) {
-        if (!jnaAvailable) {
+        if (!JNA_AVAILABLE) {
             logger.warn("cannot register console handler because JNA is not available");
             return;
         }
@@ -78,7 +83,7 @@ class Natives {
     }
 
     static boolean isMemoryLocked() {
-        if (!jnaAvailable) {
+        if (!JNA_AVAILABLE) {
             return false;
         }
         return JNANatives.LOCAL_MLOCKALL;
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Security.java b/core/src/main/java/org/elasticsearch/bootstrap/Security.java
index 8f421f4..249ff5b 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Security.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Security.java
@@ -38,15 +38,59 @@ import java.util.Map;
 import java.util.regex.Pattern;
 
 /** 
- * Initializes securitymanager with necessary permissions.
+ * Initializes SecurityManager with necessary permissions.
  * <p>
- * We use a template file (the one we test with), and add additional 
- * permissions based on the environment (data paths, etc)
+ * <h1>Initialization</h1>
+ * The JVM is not initially started with security manager enabled,
+ * instead we turn it on early in the startup process. This is a tradeoff
+ * between security and ease of use:
+ * <ul>
+ *   <li>Assigns file permissions to user-configurable paths that can
+ *       be specified from the command-line or {@code elasticsearch.yml}.</li>
+ *   <li>Allows for some contained usage of native code that would not
+ *       otherwise be permitted.</li>
+ * </ul>
+ * <p>
+ * <h1>Permissions</h1>
+ * Permissions use a policy file packaged as a resource, this file is
+ * also used in tests. File permissions are generated dynamically and
+ * combined with this policy file.
+ * <p>
+ * For each configured path, we ensure it exists and is accessible before
+ * granting permissions, otherwise directory creation would require
+ * permissions to parent directories.
+ * <p>
+ * In some exceptional cases, permissions are assigned to specific jars only,
+ * when they are so dangerous that general code should not be granted the
+ * permission, but there are extenuating circumstances.
+ * <p>
+ * Groovy scripts are assigned no permissions. This does not provide adequate
+ * sandboxing, as these scripts still have access to ES classes, and could
+ * modify members, etc that would cause bad things to happen later on their
+ * behalf (no package protections are yet in place, this would need some
+ * cleanups to the scripting apis). But still it can provide some defense for users
+ * that enable dynamic scripting without being fully aware of the consequences.
+ * <p>
+ * <h1>Disabling Security</h1>
+ * SecurityManager can be disabled completely with this setting:
+ * <pre>
+ * es.security.manager.enabled = false
+ * </pre>
+ * <p>
+ * <h1>Debugging Security</h1>
+ * A good place to start when there is a problem is to turn on security debugging:
+ * <pre>
+ * JAVA_OPTS="-Djava.security.debug=access:failure" bin/elasticsearch
+ * </pre>
+ * See <a href="https://docs.oracle.com/javase/7/docs/technotes/guides/security/troubleshooting-security.html">
+ * Troubleshooting Security</a> for information.
  */
 final class Security {
+    /** no instantiation */
+    private Security() {}
        
     /** 
-     * Initializes securitymanager for the environment
+     * Initializes SecurityManager for the environment
      * Can only happen once!
      */
     static void configure(Environment environment) throws Exception {
@@ -118,25 +162,25 @@ final class Security {
     static Permissions createPermissions(Environment environment) throws IOException {
         Permissions policy = new Permissions();
         // read-only dirs
-        addPath(policy, environment.binFile(), "read,readlink");
-        addPath(policy, environment.libFile(), "read,readlink");
-        addPath(policy, environment.pluginsFile(), "read,readlink");
-        addPath(policy, environment.configFile(), "read,readlink");
-        addPath(policy, environment.scriptsFile(), "read,readlink");
+        addPath(policy, "path.home", environment.binFile(), "read,readlink");
+        addPath(policy, "path.home", environment.libFile(), "read,readlink");
+        addPath(policy, "path.plugins", environment.pluginsFile(), "read,readlink");
+        addPath(policy, "path.conf", environment.configFile(), "read,readlink");
+        addPath(policy, "path.scripts", environment.scriptsFile(), "read,readlink");
         // read-write dirs
-        addPath(policy, environment.tmpFile(), "read,readlink,write,delete");
-        addPath(policy, environment.logsFile(), "read,readlink,write,delete");
+        addPath(policy, "java.io.tmpdir", environment.tmpFile(), "read,readlink,write,delete");
+        addPath(policy, "path.logs", environment.logsFile(), "read,readlink,write,delete");
         if (environment.sharedDataFile() != null) {
-            addPath(policy, environment.sharedDataFile(), "read,readlink,write,delete");
+            addPath(policy, "path.shared_data", environment.sharedDataFile(), "read,readlink,write,delete");
         }
         for (Path path : environment.dataFiles()) {
-            addPath(policy, path, "read,readlink,write,delete");
+            addPath(policy, "path.data", path, "read,readlink,write,delete");
         }
         for (Path path : environment.dataWithClusterFiles()) {
-            addPath(policy, path, "read,readlink,write,delete");
+            addPath(policy, "path.data", path, "read,readlink,write,delete");
         }
         for (Path path : environment.repoFiles()) {
-            addPath(policy, path, "read,readlink,write,delete");
+            addPath(policy, "path.repo", path, "read,readlink,write,delete");
         }
         if (environment.pidFile() != null) {
             // we just need permission to remove the file if its elsewhere.
@@ -145,10 +189,20 @@ final class Security {
         return policy;
     }
     
-    /** Add access to path (and all files underneath it */
-    static void addPath(Permissions policy, Path path, String permissions) throws IOException {
-        // paths may not exist yet
-        ensureDirectoryExists(path);
+    /**
+     * Add access to path (and all files underneath it)
+     * @param policy current policy to add permissions to
+     * @param configurationName the configuration name associated with the path (for error messages only)
+     * @param path the path itself
+     * @param permissions set of filepermissions to grant to the path
+     */
+    static void addPath(Permissions policy, String configurationName, Path path, String permissions) {
+        // paths may not exist yet, this also checks accessibility
+        try {
+            ensureDirectoryExists(path);
+        } catch (IOException e) {
+            throw new IllegalStateException("Unable to access '" + configurationName + "' (" + path + ")", e);
+        }
 
         // add each path twice: once for itself, again for files underneath it
         policy.add(new FilePermission(path.toString(), permissions));
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/StartupError.java b/core/src/main/java/org/elasticsearch/bootstrap/StartupError.java
new file mode 100644
index 0000000..9a5df8e
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/bootstrap/StartupError.java
@@ -0,0 +1,115 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.bootstrap;
+
+import org.elasticsearch.common.inject.CreationException;
+import org.elasticsearch.common.inject.spi.Message;
+
+import java.io.PrintStream;
+
+/**
+ * Wraps an exception in a special way that it gets formatted
+ * "reasonably". This means limits on stacktrace frames and
+ * cleanup for guice, and some guidance about consulting full
+ * logs for the whole exception.
+ */
+//TODO: remove this when guice is removed, and exceptions are cleaned up
+//this is horrible, but its what we must do
+final class StartupError extends RuntimeException {
+    
+    /** maximum length of a stacktrace, before we truncate it */
+    static final int STACKTRACE_LIMIT = 30;
+    /** all lines from this package are RLE-compressed */
+    static final String GUICE_PACKAGE = "org.elasticsearch.common.inject";
+    
+    /** 
+     * Create a new StartupError that will format {@code cause}
+     * to the console on failure.
+     */
+    StartupError(Throwable cause) {
+        super(cause);
+    }
+
+    /*
+     * This logic actually prints the exception to the console, its
+     * what is invoked by the JVM when we throw the exception from main()
+     */
+    @Override
+    public void printStackTrace(PrintStream s) {
+        Throwable originalCause = getCause();
+        Throwable cause = originalCause;
+        if (cause instanceof CreationException) {
+            cause = getFirstGuiceCause((CreationException)cause);
+        }
+        
+        String message = cause.toString();
+        s.println(message);
+        
+        if (cause != null) {
+            // walk to the root cause
+            while (cause.getCause() != null) {
+                cause = cause.getCause();
+            }
+
+            // print the root cause message, only if it differs!
+            if (cause != originalCause && (message.equals(cause.toString()) == false)) {
+                s.println("Likely root cause: " + cause);
+            }
+
+            // print stacktrace of cause
+            StackTraceElement stack[] = cause.getStackTrace();
+            int linesWritten = 0;
+            for (int i = 0; i < stack.length; i++) {
+                if (linesWritten == STACKTRACE_LIMIT) {
+                    s.println("\t<<<truncated>>>");
+                    break;
+                }
+                String line = stack[i].toString();
+                
+                // skip past contiguous runs of this garbage:
+                if (line.startsWith(GUICE_PACKAGE)) {
+                    while (i + 1 < stack.length && stack[i + 1].toString().startsWith(GUICE_PACKAGE)) {
+                        i++;
+                    }
+                    s.println("\tat <<<guice>>>");
+                    linesWritten++;
+                    continue;
+                }
+
+                s.println("\tat " + line.toString());
+                linesWritten++;
+            }
+        }
+        s.println("Refer to the log for complete error details.");
+    }
+    
+    /** 
+     * Returns first cause from a guice error (it can have multiple).
+     */
+    static Throwable getFirstGuiceCause(CreationException guice) {
+        for (Message message : guice.getErrorMessages()) {
+            Throwable cause = message.getCause();
+            if (cause != null) {
+                return cause;
+            }
+        }
+        return guice; // we tried
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java b/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
index 5ae074a..a8e2bf5 100644
--- a/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
+++ b/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
@@ -35,6 +35,7 @@ import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.component.LifecycleComponent;
 import org.elasticsearch.common.inject.Injector;
+import org.elasticsearch.common.inject.Module;
 import org.elasticsearch.common.inject.ModulesBuilder;
 import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.settings.Settings;
@@ -132,7 +133,11 @@ public class TransportClient extends AbstractClient {
             try {
                 ModulesBuilder modules = new ModulesBuilder();
                 modules.add(new Version.Module(version));
-                modules.add(new PluginsModule(this.settings, pluginsService));
+                // plugin modules must be added here, before others or we can get crazy injection errors...
+                for (Module pluginModule : pluginsService.nodeModules()) {
+                    modules.add(pluginModule);
+                }
+                modules.add(new PluginsModule(pluginsService));
                 modules.add(new EnvironmentModule(environment));
                 modules.add(new SettingsModule(this.settings));
                 modules.add(new NetworkModule());
@@ -149,6 +154,8 @@ public class TransportClient extends AbstractClient {
                 modules.add(new ClientTransportModule());
                 modules.add(new CircuitBreakerModule(this.settings));
 
+                pluginsService.processModules(modules);
+
                 Injector injector = modules.createInjector();
                 injector.getInstance(TransportService.class).start();
                 TransportClient transportClient = new TransportClient(injector);
diff --git a/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java b/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java
index 118229d..4ffc2ab 100644
--- a/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java
+++ b/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.client.transport;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Sets;
 import org.elasticsearch.ExceptionsHelper;
@@ -72,12 +73,12 @@ public class TransportClientNodesService extends AbstractComponent {
     private final Headers headers;
 
     // nodes that are added to be discovered
-    private volatile List<DiscoveryNode> listedNodes = Collections.emptyList();
+    private volatile ImmutableList<DiscoveryNode> listedNodes = ImmutableList.of();
 
     private final Object mutex = new Object();
 
-    private volatile List<DiscoveryNode> nodes = Collections.emptyList();
-    private volatile List<DiscoveryNode> filteredNodes = Collections.emptyList();
+    private volatile List<DiscoveryNode> nodes = ImmutableList.of();
+    private volatile List<DiscoveryNode> filteredNodes = ImmutableList.of();
 
     private final AtomicInteger tempNodeIdGenerator = new AtomicInteger();
 
@@ -118,11 +119,11 @@ public class TransportClientNodesService extends AbstractComponent {
     }
 
     public List<TransportAddress> transportAddresses() {
-        List<TransportAddress> lstBuilder = new ArrayList<>();
+        ImmutableList.Builder<TransportAddress> lstBuilder = ImmutableList.builder();
         for (DiscoveryNode listedNode : listedNodes) {
             lstBuilder.add(listedNode.address());
         }
-        return Collections.unmodifiableList(lstBuilder);
+        return lstBuilder.build();
     }
 
     public List<DiscoveryNode> connectedNodes() {
@@ -159,14 +160,14 @@ public class TransportClientNodesService extends AbstractComponent {
             if (filtered.isEmpty()) {
                 return this;
             }
-            List<DiscoveryNode> builder = new ArrayList<>();
+            ImmutableList.Builder<DiscoveryNode> builder = ImmutableList.builder();
             builder.addAll(listedNodes());
             for (TransportAddress transportAddress : filtered) {
                 DiscoveryNode node = new DiscoveryNode("#transport#-" + tempNodeIdGenerator.incrementAndGet(), transportAddress, minCompatibilityVersion);
                 logger.debug("adding address [{}]", node);
                 builder.add(node);
             }
-            listedNodes = Collections.unmodifiableList(builder);
+            listedNodes = builder.build();
             nodesSampler.sample();
         }
         return this;
@@ -177,7 +178,7 @@ public class TransportClientNodesService extends AbstractComponent {
             if (closed) {
                 throw new IllegalStateException("transport client is closed, can't remove an address");
             }
-            List<DiscoveryNode> builder = new ArrayList<>();
+            ImmutableList.Builder<DiscoveryNode> builder = ImmutableList.builder();
             for (DiscoveryNode otherNode : listedNodes) {
                 if (!otherNode.address().equals(transportAddress)) {
                     builder.add(otherNode);
@@ -185,7 +186,7 @@ public class TransportClientNodesService extends AbstractComponent {
                     logger.debug("removing address [{}]", otherNode);
                 }
             }
-            listedNodes = Collections.unmodifiableList(builder);
+            listedNodes = builder.build();
             nodesSampler.sample();
         }
         return this;
@@ -260,7 +261,7 @@ public class TransportClientNodesService extends AbstractComponent {
             for (DiscoveryNode listedNode : listedNodes) {
                 transportService.disconnectFromNode(listedNode);
             }
-            nodes = Collections.emptyList();
+            nodes = ImmutableList.of();
         }
     }
 
@@ -310,7 +311,7 @@ public class TransportClientNodesService extends AbstractComponent {
                 }
             }
 
-            return Collections.unmodifiableList(new ArrayList<>(nodes));
+            return new ImmutableList.Builder<DiscoveryNode>().addAll(nodes).build();
         }
 
     }
@@ -375,7 +376,7 @@ public class TransportClientNodesService extends AbstractComponent {
             }
 
             nodes = validateNewNodes(newNodes);
-            filteredNodes = Collections.unmodifiableList(new ArrayList<>(newFilteredNodes));
+            filteredNodes = ImmutableList.copyOf(newFilteredNodes);
         }
     }
 
@@ -475,7 +476,7 @@ public class TransportClientNodesService extends AbstractComponent {
             }
 
             nodes = validateNewNodes(newNodes);
-            filteredNodes = Collections.unmodifiableList(new ArrayList<>(newFilteredNodes));
+            filteredNodes = ImmutableList.copyOf(newFilteredNodes);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java b/core/src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java
index b63b590..1ceb822 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java
@@ -20,13 +20,13 @@
 package org.elasticsearch.cluster;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.List;
 
 /**
@@ -86,7 +86,7 @@ public class ClusterChangedEvent {
             return Arrays.asList(state.metaData().indices().keys().toArray(String.class));
         }
         if (!metaDataChanged()) {
-            return Collections.emptyList();
+            return ImmutableList.of();
         }
         List<String> created = null;
         for (ObjectCursor<String> cursor : state.metaData().indices().keys()) {
@@ -98,7 +98,7 @@ public class ClusterChangedEvent {
                 created.add(index);
             }
         }
-        return created == null ? Collections.<String>emptyList() : created;
+        return created == null ? ImmutableList.<String>of() : created;
     }
 
     /**
@@ -116,10 +116,10 @@ public class ClusterChangedEvent {
         // See discussion on https://github.com/elastic/elasticsearch/pull/9952 and
         // https://github.com/elastic/elasticsearch/issues/11665
         if (hasNewMaster() || previousState == null) {
-            return Collections.emptyList();
+            return ImmutableList.of();
         }
         if (!metaDataChanged()) {
-            return Collections.emptyList();
+            return ImmutableList.of();
         }
         List<String> deleted = null;
         for (ObjectCursor<String> cursor : previousState.metaData().indices().keys()) {
@@ -131,7 +131,7 @@ public class ClusterChangedEvent {
                 deleted.add(index);
             }
         }
-        return deleted == null ? Collections.<String>emptyList() : deleted;
+        return deleted == null ? ImmutableList.<String>of() : deleted;
     }
 
     public boolean metaDataChanged() {
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterInfo.java b/core/src/main/java/org/elasticsearch/cluster/ClusterInfo.java
index 5e2d35a..f21bc9f 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterInfo.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterInfo.java
@@ -30,7 +30,7 @@ import java.util.Map;
  * <code>InternalClusterInfoService.shardIdentifierFromRouting(String)</code>
  * for the key used in the shardSizes map
  */
-public final class ClusterInfo {
+public class ClusterInfo {
 
     private final Map<String, DiskUsage> usages;
     final Map<String, Long> shardSizes;
@@ -54,6 +54,11 @@ public final class ClusterInfo {
         return shardSizes.get(shardIdentifierFromRouting(shardRouting));
     }
 
+    public long getShardSize(ShardRouting shardRouting, long defaultValue) {
+        Long shardSize = getShardSize(shardRouting);
+        return shardSize == null ? defaultValue : shardSize;
+    }
+
     /**
      * Method that incorporates the ShardId for the shard into a string that
      * includes a 'p' or 'r' depending on whether the shard is a primary.
diff --git a/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java b/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java
index 567812e..8be9465 100644
--- a/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster;
 
+import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.LatchedActionListener;
 import org.elasticsearch.action.admin.cluster.node.stats.NodeStats;
@@ -36,6 +37,7 @@ import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.util.concurrent.AbstractRunnable;
 import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;
 import org.elasticsearch.monitor.fs.FsInfo;
 import org.elasticsearch.node.settings.NodeSettingsService;
@@ -45,6 +47,7 @@ import org.elasticsearch.transport.ReceiveTimeoutTransportException;
 import java.util.*;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicReference;
 
 /**
  * InternalClusterInfoService provides the ClusterInfoService interface,
diff --git a/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java b/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java
index eabe615..857f1a3 100644
--- a/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java
+++ b/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.ClusterState.Custom;
 import org.elasticsearch.cluster.metadata.SnapshotId;
@@ -29,9 +30,6 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.shard.ShardId;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 
@@ -44,14 +42,14 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
 
     public static final RestoreInProgress PROTO = new RestoreInProgress();
 
-    private final List<Entry> entries;
+    private final ImmutableList<Entry> entries;
 
     /**
      * Constructs new restore metadata
      *
      * @param entries list of currently running restore processes
      */
-    public RestoreInProgress(List<Entry> entries) {
+    public RestoreInProgress(ImmutableList<Entry> entries) {
         this.entries = entries;
     }
 
@@ -61,7 +59,7 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
      * @param entries list of currently running restore processes
      */
     public RestoreInProgress(Entry... entries) {
-        this.entries = Arrays.asList(entries);
+        this.entries = ImmutableList.copyOf(entries);
     }
 
     /**
@@ -113,7 +111,7 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
         private final State state;
         private final SnapshotId snapshotId;
         private final ImmutableMap<ShardId, ShardRestoreStatus> shards;
-        private final List<String> indices;
+        private final ImmutableList<String> indices;
 
         /**
          * Creates new restore metadata
@@ -123,7 +121,7 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
          * @param indices    list of indices being restored
          * @param shards     list of shards being restored and thier current restore status
          */
-        public Entry(SnapshotId snapshotId, State state, List<String> indices, ImmutableMap<ShardId, ShardRestoreStatus> shards) {
+        public Entry(SnapshotId snapshotId, State state, ImmutableList<String> indices, ImmutableMap<ShardId, ShardRestoreStatus> shards) {
             this.snapshotId = snapshotId;
             this.state = state;
             this.indices = indices;
@@ -166,7 +164,7 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
          *
          * @return list of indices
          */
-        public List<String> indices() {
+        public ImmutableList<String> indices() {
             return indices;
         }
 
@@ -415,7 +413,7 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
             SnapshotId snapshotId = SnapshotId.readSnapshotId(in);
             State state = State.fromValue(in.readByte());
             int indices = in.readVInt();
-            List<String> indexBuilder = new ArrayList<>();
+            ImmutableList.Builder<String> indexBuilder = ImmutableList.builder();
             for (int j = 0; j < indices; j++) {
                 indexBuilder.add(in.readString());
             }
@@ -426,7 +424,7 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
                 ShardRestoreStatus shardState = ShardRestoreStatus.readShardRestoreStatus(in);
                 builder.put(shardId, shardState);
             }
-            entries[i] = new Entry(snapshotId, state, Collections.unmodifiableList(indexBuilder), builder.build());
+            entries[i] = new Entry(snapshotId, state, indexBuilder.build(), builder.build());
         }
         return new RestoreInProgress(entries);
     }
diff --git a/core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java b/core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java
index a6babbb..a315e68 100644
--- a/core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java
+++ b/core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.ClusterState.Custom;
 import org.elasticsearch.cluster.metadata.SnapshotId;
@@ -30,10 +31,7 @@ import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.index.shard.ShardId;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 
@@ -69,11 +67,11 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
         private final SnapshotId snapshotId;
         private final boolean includeGlobalState;
         private final ImmutableMap<ShardId, ShardSnapshotStatus> shards;
-        private final List<String> indices;
-        private final ImmutableMap<String, List<ShardId>> waitingIndices;
+        private final ImmutableList<String> indices;
+        private final ImmutableMap<String, ImmutableList<ShardId>> waitingIndices;
         private final long startTime;
 
-        public Entry(SnapshotId snapshotId, boolean includeGlobalState, State state, List<String> indices, long startTime, ImmutableMap<ShardId, ShardSnapshotStatus> shards) {
+        public Entry(SnapshotId snapshotId, boolean includeGlobalState, State state, ImmutableList<String> indices, long startTime, ImmutableMap<ShardId, ShardSnapshotStatus> shards) {
             this.state = state;
             this.snapshotId = snapshotId;
             this.includeGlobalState = includeGlobalState;
@@ -108,11 +106,11 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
             return state;
         }
 
-        public List<String> indices() {
+        public ImmutableList<String> indices() {
             return indices;
         }
 
-        public ImmutableMap<String, List<ShardId>> waitingIndices() {
+        public ImmutableMap<String, ImmutableList<ShardId>> waitingIndices() {
             return waitingIndices;
         }
 
@@ -154,22 +152,22 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
             return result;
         }
 
-        private ImmutableMap<String, List<ShardId>> findWaitingIndices(ImmutableMap<ShardId, ShardSnapshotStatus> shards) {
-            Map<String, List<ShardId>> waitingIndicesMap = newHashMap();
+        private ImmutableMap<String, ImmutableList<ShardId>> findWaitingIndices(ImmutableMap<ShardId, ShardSnapshotStatus> shards) {
+            Map<String, ImmutableList.Builder<ShardId>> waitingIndicesMap = newHashMap();
             for (ImmutableMap.Entry<ShardId, ShardSnapshotStatus> entry : shards.entrySet()) {
                 if (entry.getValue().state() == State.WAITING) {
-                    List<ShardId> waitingShards = waitingIndicesMap.get(entry.getKey().getIndex());
+                    ImmutableList.Builder<ShardId> waitingShards = waitingIndicesMap.get(entry.getKey().getIndex());
                     if (waitingShards == null) {
-                        waitingShards = new ArrayList<>();
+                        waitingShards = ImmutableList.builder();
                         waitingIndicesMap.put(entry.getKey().getIndex(), waitingShards);
                     }
                     waitingShards.add(entry.getKey());
                 }
             }
             if (!waitingIndicesMap.isEmpty()) {
-                ImmutableMap.Builder<String, List<ShardId>> waitingIndicesBuilder = ImmutableMap.builder();
-                for (Map.Entry<String, List<ShardId>> entry : waitingIndicesMap.entrySet()) {
-                    waitingIndicesBuilder.put(entry.getKey(), Collections.unmodifiableList(entry.getValue()));
+                ImmutableMap.Builder<String, ImmutableList<ShardId>> waitingIndicesBuilder = ImmutableMap.builder();
+                for (Map.Entry<String, ImmutableList.Builder<ShardId>> entry : waitingIndicesMap.entrySet()) {
+                    waitingIndicesBuilder.put(entry.getKey(), entry.getValue().build());
                 }
                 return waitingIndicesBuilder.build();
             } else {
@@ -326,15 +324,15 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
         }
     }
 
-    private final List<Entry> entries;
+    private final ImmutableList<Entry> entries;
 
 
-    public SnapshotsInProgress(List<Entry> entries) {
+    public SnapshotsInProgress(ImmutableList<Entry> entries) {
         this.entries = entries;
     }
 
     public SnapshotsInProgress(Entry... entries) {
-        this.entries = Arrays.asList(entries);
+        this.entries = ImmutableList.copyOf(entries);
     }
 
     public List<Entry> entries() {
@@ -363,7 +361,7 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
             boolean includeGlobalState = in.readBoolean();
             State state = State.fromValue(in.readByte());
             int indices = in.readVInt();
-            List<String> indexBuilder = new ArrayList<>();
+            ImmutableList.Builder<String> indexBuilder = ImmutableList.builder();
             for (int j = 0; j < indices; j++) {
                 indexBuilder.add(in.readString());
             }
@@ -376,7 +374,7 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
                 State shardState = State.fromValue(in.readByte());
                 builder.put(shardId, new ShardSnapshotStatus(nodeId, shardState));
             }
-            entries[i] = new Entry(snapshotId, includeGlobalState, state, Collections.unmodifiableList(indexBuilder), startTime, builder.build());
+            entries[i] = new Entry(snapshotId, includeGlobalState, state, indexBuilder.build(), startTime, builder.build());
         }
         return new SnapshotsInProgress(entries);
     }
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java
index c930f29..2f28dca 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.cluster.metadata;
 
 import com.google.common.base.Predicate;
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.action.IndicesRequest;
 import org.elasticsearch.action.support.IndicesOptions;
@@ -49,13 +50,13 @@ import static com.google.common.collect.Maps.newHashMap;
 
 public class IndexNameExpressionResolver extends AbstractComponent {
 
-    private final List<ExpressionResolver> expressionResolvers;
+    private final ImmutableList<ExpressionResolver> expressionResolvers;
     private final DateMathExpressionResolver dateMathExpressionResolver;
 
     @Inject
     public IndexNameExpressionResolver(Settings settings) {
         super(settings);
-        expressionResolvers = Arrays.asList(
+        expressionResolvers = ImmutableList.of(
                 dateMathExpressionResolver = new DateMathExpressionResolver(settings),
                 new WildcardExpressionResolver()
         );
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
index a06c71f..32098e5 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
@@ -234,7 +234,7 @@ public class MetaData implements Iterable<IndexMetaData>, Diffable<MetaData>, Fr
      * @param concreteIndices The concrete indexes the index aliases must point to order to be returned.
      * @return the found index aliases grouped by index
      */
-    public ImmutableOpenMap<String, List<AliasMetaData>> findAliases(final String[] aliases, String[] concreteIndices) {
+    public ImmutableOpenMap<String, ImmutableList<AliasMetaData>> findAliases(final String[] aliases, String[] concreteIndices) {
         assert aliases != null;
         assert concreteIndices != null;
         if (concreteIndices.length == 0) {
@@ -242,7 +242,7 @@ public class MetaData implements Iterable<IndexMetaData>, Diffable<MetaData>, Fr
         }
 
         boolean matchAllAliases = matchAllAliases(aliases);
-        ImmutableOpenMap.Builder<String, List<AliasMetaData>> mapBuilder = ImmutableOpenMap.builder();
+        ImmutableOpenMap.Builder<String, ImmutableList<AliasMetaData>> mapBuilder = ImmutableOpenMap.builder();
         Iterable<String> intersection = HppcMaps.intersection(ObjectHashSet.from(concreteIndices), indices.keys());
         for (String index : intersection) {
             IndexMetaData indexMetaData = indices.get(index);
@@ -262,7 +262,7 @@ public class MetaData implements Iterable<IndexMetaData>, Diffable<MetaData>, Fr
                         return o1.alias().compareTo(o2.alias());
                     }
                 });
-                mapBuilder.put(index, Collections.unmodifiableList(filteredValues));
+                mapBuilder.put(index, ImmutableList.copyOf(filteredValues));
             }
         }
         return mapBuilder.build();
@@ -345,7 +345,7 @@ public class MetaData implements Iterable<IndexMetaData>, Diffable<MetaData>, Fr
         return indexMapBuilder.build();
     }
 
-    public ImmutableOpenMap<String, List<IndexWarmersMetaData.Entry>> findWarmers(String[] concreteIndices, final String[] types, final String[] uncheckedWarmers) {
+    public ImmutableOpenMap<String, ImmutableList<IndexWarmersMetaData.Entry>> findWarmers(String[] concreteIndices, final String[] types, final String[] uncheckedWarmers) {
         assert uncheckedWarmers != null;
         assert concreteIndices != null;
         if (concreteIndices.length == 0) {
@@ -354,7 +354,7 @@ public class MetaData implements Iterable<IndexMetaData>, Diffable<MetaData>, Fr
         // special _all check to behave the same like not specifying anything for the warmers (not for the indices)
         final String[] warmers = Strings.isAllOrWildcard(uncheckedWarmers) ? Strings.EMPTY_ARRAY : uncheckedWarmers;
 
-        ImmutableOpenMap.Builder<String, List<IndexWarmersMetaData.Entry>> mapBuilder = ImmutableOpenMap.builder();
+        ImmutableOpenMap.Builder<String, ImmutableList<IndexWarmersMetaData.Entry>> mapBuilder = ImmutableOpenMap.builder();
         Iterable<String> intersection = HppcMaps.intersection(ObjectHashSet.from(concreteIndices), indices.keys());
         for (String index : intersection) {
             IndexMetaData indexMetaData = indices.get(index);
@@ -363,7 +363,6 @@ public class MetaData implements Iterable<IndexMetaData>, Diffable<MetaData>, Fr
                 continue;
             }
 
-            // TODO: make this a List so we don't have to copy below
             Collection<IndexWarmersMetaData.Entry> filteredWarmers = Collections2.filter(indexWarmersMetaData.entries(), new Predicate<IndexWarmersMetaData.Entry>() {
 
                 @Override
@@ -381,7 +380,7 @@ public class MetaData implements Iterable<IndexMetaData>, Diffable<MetaData>, Fr
 
             });
             if (!filteredWarmers.isEmpty()) {
-                mapBuilder.put(index, Collections.unmodifiableList(new ArrayList<>(filteredWarmers)));
+                mapBuilder.put(index, ImmutableList.copyOf(filteredWarmers));
             }
         }
         return mapBuilder.build();
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
index 2571e29..ff878a4 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
@@ -24,7 +24,6 @@ import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
 import com.google.common.base.Charsets;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
-
 import org.apache.lucene.util.CollectionUtil;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
@@ -46,6 +45,7 @@ import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.ValidationException;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.compress.CompressedXContent;
 import org.elasticsearch.common.inject.Inject;
@@ -60,12 +60,15 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.index.Index;
+import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.MapperParsingException;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.IndexService;
-import org.elasticsearch.indices.*;
+import org.elasticsearch.indices.IndexAlreadyExistsException;
+import org.elasticsearch.indices.IndexCreationException;
+import org.elasticsearch.indices.IndicesService;
+import org.elasticsearch.indices.InvalidIndexNameException;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.joda.time.DateTime;
@@ -514,6 +517,15 @@ public class MetaDataCreateIndexService extends AbstractComponent {
     }
 
     public void validateIndexSettings(String indexName, Settings settings) throws IndexCreationException {
+        List<String> validationErrors = getIndexSettingsValidationErrors(settings);
+        if (validationErrors.isEmpty() == false) {
+            ValidationException validationException = new ValidationException();
+            validationException.addValidationErrors(validationErrors);
+            throw new IndexCreationException(new Index(indexName), validationException);
+        }
+    }
+
+    List<String> getIndexSettingsValidationErrors(Settings settings) {
         String customPath = settings.get(IndexMetaData.SETTING_DATA_PATH, null);
         List<String> validationErrors = Lists.newArrayList();
         if (customPath != null && env.sharedDataFile() == null) {
@@ -530,22 +542,9 @@ public class MetaDataCreateIndexService extends AbstractComponent {
             validationErrors.add("index must have 1 or more primary shards");
         }
         if (number_of_replicas != null && number_of_replicas < 0) {
-           validationErrors.add("index must have 0 or more replica shards");
-        }
-        if (validationErrors.isEmpty() == false) {
-            throw new IndexCreationException(new Index(indexName),
-                new IllegalArgumentException(getMessage(validationErrors)));
-        }
-    }
-
-    private String getMessage(List<String> validationErrors) {
-        StringBuilder sb = new StringBuilder();
-        sb.append("Validation Failed: ");
-        int index = 0;
-        for (String error : validationErrors) {
-            sb.append(++index).append(": ").append(error).append(";");
+            validationErrors.add("index must have 0 or more replica shards");
         }
-        return sb.toString();
+        return validationErrors;
     }
 
     private static class DefaultIndexTemplateFilter implements IndexTemplateFilter {
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java
index 96f7915..66ad39f 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java
@@ -29,12 +29,12 @@ import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.TimeoutClusterStateUpdateTask;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.ValidationException;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.indices.IndexCreationException;
 import org.elasticsearch.indices.IndexTemplateAlreadyExistsException;
 import org.elasticsearch.indices.IndexTemplateMissingException;
 import org.elasticsearch.indices.InvalidIndexTemplateException;
@@ -179,41 +179,44 @@ public class MetaDataIndexTemplateService extends AbstractComponent {
     }
 
     private void validate(PutRequest request) {
+        List<String> validationErrors = Lists.newArrayList();
         if (request.name.contains(" ")) {
-            throw new InvalidIndexTemplateException(request.name, "name must not contain a space");
+            validationErrors.add("name must not contain a space");
         }
         if (request.name.contains(",")) {
-            throw new InvalidIndexTemplateException(request.name, "name must not contain a ','");
+            validationErrors.add("name must not contain a ','");
         }
         if (request.name.contains("#")) {
-            throw new InvalidIndexTemplateException(request.name, "name must not contain a '#'");
+            validationErrors.add("name must not contain a '#'");
         }
         if (request.name.startsWith("_")) {
-            throw new InvalidIndexTemplateException(request.name, "name must not start with '_'");
+            validationErrors.add("name must not start with '_'");
         }
         if (!request.name.toLowerCase(Locale.ROOT).equals(request.name)) {
-            throw new InvalidIndexTemplateException(request.name, "name must be lower cased");
+            validationErrors.add("name must be lower cased");
         }
         if (request.template.contains(" ")) {
-            throw new InvalidIndexTemplateException(request.name, "template must not contain a space");
+            validationErrors.add("template must not contain a space");
         }
         if (request.template.contains(",")) {
-            throw new InvalidIndexTemplateException(request.name, "template must not contain a ','");
+            validationErrors.add("template must not contain a ','");
         }
         if (request.template.contains("#")) {
-            throw new InvalidIndexTemplateException(request.name, "template must not contain a '#'");
+            validationErrors.add("template must not contain a '#'");
         }
         if (request.template.startsWith("_")) {
-            throw new InvalidIndexTemplateException(request.name, "template must not start with '_'");
+            validationErrors.add("template must not start with '_'");
         }
         if (!Strings.validFileNameExcludingAstrix(request.template)) {
-            throw new InvalidIndexTemplateException(request.name, "template must not container the following characters " + Strings.INVALID_FILENAME_CHARS);
+            validationErrors.add("template must not container the following characters " + Strings.INVALID_FILENAME_CHARS);
         }
 
-        try {
-            metaDataCreateIndexService.validateIndexSettings(request.name, request.settings);
-        } catch (IndexCreationException exception) {
-            throw new InvalidIndexTemplateException(request.name, exception.getDetailedMessage());
+        List<String> indexSettingsValidation = metaDataCreateIndexService.getIndexSettingsValidationErrors(request.settings);
+        validationErrors.addAll(indexSettingsValidation);
+        if (!validationErrors.isEmpty()) {
+            ValidationException validationException = new ValidationException();
+            validationException.addValidationErrors(validationErrors);
+            throw new InvalidIndexTemplateException(request.name, validationException.getMessage());
         }
 
         for (Alias alias : request.aliases) {
@@ -271,7 +274,7 @@ public class MetaDataIndexTemplateService extends AbstractComponent {
             this.mappings.putAll(mappings);
             return this;
         }
-        
+
         public PutRequest aliases(Set<Alias> aliases) {
             this.aliases.addAll(aliases);
             return this;
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/RepositoriesMetaData.java b/core/src/main/java/org/elasticsearch/cluster/metadata/RepositoriesMetaData.java
index 23a4c32..48e40d1 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/RepositoriesMetaData.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/RepositoriesMetaData.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster.metadata;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.cluster.AbstractDiffable;
 import org.elasticsearch.cluster.metadata.MetaData.Custom;
@@ -32,9 +33,9 @@ import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.EnumSet;
 import java.util.List;
+import java.util.Map;
 
 /**
  * Contains metadata about registered snapshot repositories
@@ -45,7 +46,7 @@ public class RepositoriesMetaData extends AbstractDiffable<Custom> implements Me
 
     public static final RepositoriesMetaData PROTO = new RepositoriesMetaData();
 
-    private final List<RepositoryMetaData> repositories;
+    private final ImmutableList<RepositoryMetaData> repositories;
 
     /**
      * Constructs new repository metadata
@@ -53,7 +54,7 @@ public class RepositoriesMetaData extends AbstractDiffable<Custom> implements Me
      * @param repositories list of repositories
      */
     public RepositoriesMetaData(RepositoryMetaData... repositories) {
-        this.repositories = Arrays.asList(repositories);
+        this.repositories = ImmutableList.copyOf(repositories);
     }
 
     /**
@@ -61,7 +62,7 @@ public class RepositoriesMetaData extends AbstractDiffable<Custom> implements Me
      *
      * @return list of repositories
      */
-    public List<RepositoryMetaData> repositories() {
+    public ImmutableList<RepositoryMetaData> repositories() {
         return this.repositories;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
index 84e6b57..4d8229e 100644
--- a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
+++ b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster.node;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 
 import org.elasticsearch.Version;
@@ -34,8 +35,6 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.net.InetAddress;
-import java.util.Collections;
-import java.util.List;
 import java.util.Map;
 
 import static org.elasticsearch.common.transport.TransportAddressSerializers.addressToStream;
@@ -93,7 +92,7 @@ public class DiscoveryNode implements Streamable, ToXContent {
         return Booleans.isExplicitTrue(data);
     }
 
-    public static final List<DiscoveryNode> EMPTY_LIST = Collections.emptyList();
+    public static final ImmutableList<DiscoveryNode> EMPTY_LIST = ImmutableList.of();
 
     private String nodeName = "";
     private String nodeId;
@@ -361,16 +360,16 @@ public class DiscoveryNode implements Streamable, ToXContent {
     public String toString() {
         StringBuilder sb = new StringBuilder();
         if (nodeName.length() > 0) {
-            sb.append('[').append(nodeName).append(']');
+            sb.append('{').append(nodeName).append('}');
         }
         if (nodeId != null) {
-            sb.append('[').append(nodeId).append(']');
+            sb.append('{').append(nodeId).append('}');
         }
         if (Strings.hasLength(hostName)) {
-            sb.append('[').append(hostName).append(']');
+            sb.append('{').append(hostName).append('}');
         }
         if (address != null) {
-            sb.append('[').append(address).append(']');
+            sb.append('{').append(address).append('}');
         }
         if (!attributes.isEmpty()) {
             sb.append(attributes);
diff --git a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java
index 5151d66..1b95aed 100644
--- a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java
+++ b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java
@@ -22,6 +22,7 @@ package org.elasticsearch.cluster.node;
 import com.carrotsearch.hppc.ObjectHashSet;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.UnmodifiableIterator;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.AbstractDiffable;
@@ -34,7 +35,6 @@ import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.transport.TransportAddress;
 
 import java.io.IOException;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -434,7 +434,7 @@ public class DiscoveryNodes extends AbstractDiffable<DiscoveryNodes> implements
                 newMasterNode = masterNode();
             }
         }
-        return new Delta(previousMasterNode, newMasterNode, localNodeId, Collections.unmodifiableList(removed), Collections.unmodifiableList(added));
+        return new Delta(previousMasterNode, newMasterNode, localNodeId, ImmutableList.copyOf(removed), ImmutableList.copyOf(added));
     }
 
     @Override
@@ -473,14 +473,14 @@ public class DiscoveryNodes extends AbstractDiffable<DiscoveryNodes> implements
         private final String localNodeId;
         private final DiscoveryNode previousMasterNode;
         private final DiscoveryNode newMasterNode;
-        private final List<DiscoveryNode> removed;
-        private final List<DiscoveryNode> added;
+        private final ImmutableList<DiscoveryNode> removed;
+        private final ImmutableList<DiscoveryNode> added;
 
-        public Delta(String localNodeId, List<DiscoveryNode> removed, List<DiscoveryNode> added) {
+        public Delta(String localNodeId, ImmutableList<DiscoveryNode> removed, ImmutableList<DiscoveryNode> added) {
             this(null, null, localNodeId, removed, added);
         }
 
-        public Delta(@Nullable DiscoveryNode previousMasterNode, @Nullable DiscoveryNode newMasterNode, String localNodeId, List<DiscoveryNode> removed, List<DiscoveryNode> added) {
+        public Delta(@Nullable DiscoveryNode previousMasterNode, @Nullable DiscoveryNode newMasterNode, String localNodeId, ImmutableList<DiscoveryNode> removed, ImmutableList<DiscoveryNode> added) {
             this.previousMasterNode = previousMasterNode;
             this.newMasterNode = newMasterNode;
             this.localNodeId = localNodeId;
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java b/core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java
index 5dfff2d..85a14e2 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java
@@ -22,6 +22,7 @@ package org.elasticsearch.cluster.routing;
 import com.carrotsearch.hppc.IntSet;
 import com.carrotsearch.hppc.cursors.IntCursor;
 import com.carrotsearch.hppc.cursors.IntObjectCursor;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Sets;
 import com.google.common.collect.UnmodifiableIterator;
 
@@ -36,7 +37,6 @@ import org.elasticsearch.index.shard.ShardId;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Comparator;
 import java.util.List;
 import java.util.Set;
@@ -76,7 +76,7 @@ public class IndexRoutingTable extends AbstractDiffable<IndexRoutingTable> imple
         this.index = index;
         this.shuffler = new RotationShardShuffler(ThreadLocalRandom.current().nextInt());
         this.shards = shards;
-        List<ShardRouting> allActiveShards = new ArrayList<>();
+        ImmutableList.Builder<ShardRouting> allActiveShards = ImmutableList.builder();
         for (IntObjectCursor<IndexShardRoutingTable> cursor : shards) {
             for (ShardRouting shardRouting : cursor.value) {
                 shardRouting.freeze();
@@ -85,7 +85,7 @@ public class IndexRoutingTable extends AbstractDiffable<IndexRoutingTable> imple
                 }
             }
         }
-        this.allActiveShards = Collections.unmodifiableList(allActiveShards);
+        this.allActiveShards = allActiveShards.build();
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java b/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java
index e1aa7a0..bc13e08 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster.routing;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Sets;
 import org.elasticsearch.cluster.node.DiscoveryNode;
@@ -47,30 +48,30 @@ public class IndexShardRoutingTable implements Iterable<ShardRouting> {
     final ShardId shardId;
 
     final ShardRouting primary;
-    final List<ShardRouting> primaryAsList;
-    final List<ShardRouting> replicas;
-    final List<ShardRouting> shards;
-    final List<ShardRouting> activeShards;
-    final List<ShardRouting> assignedShards;
-    final static List<ShardRouting> NO_SHARDS = Collections.emptyList();
+    final ImmutableList<ShardRouting> primaryAsList;
+    final ImmutableList<ShardRouting> replicas;
+    final ImmutableList<ShardRouting> shards;
+    final ImmutableList<ShardRouting> activeShards;
+    final ImmutableList<ShardRouting> assignedShards;
+    final static ImmutableList<ShardRouting> NO_SHARDS = ImmutableList.of();
     final boolean allShardsStarted;
 
     /**
      * The initializing list, including ones that are initializing on a target node because of relocation.
      * If we can come up with a better variable name, it would be nice...
      */
-    final List<ShardRouting> allInitializingShards;
+    final ImmutableList<ShardRouting> allInitializingShards;
 
     IndexShardRoutingTable(ShardId shardId, List<ShardRouting> shards) {
         this.shardId = shardId;
         this.shuffler = new RotationShardShuffler(ThreadLocalRandom.current().nextInt());
-        this.shards = Collections.unmodifiableList(shards);
+        this.shards = ImmutableList.copyOf(shards);
 
         ShardRouting primary = null;
-        List<ShardRouting> replicas = new ArrayList<>();
-        List<ShardRouting> activeShards = new ArrayList<>();
-        List<ShardRouting> assignedShards = new ArrayList<>();
-        List<ShardRouting> allInitializingShards = new ArrayList<>();
+        ImmutableList.Builder<ShardRouting> replicas = ImmutableList.builder();
+        ImmutableList.Builder<ShardRouting> activeShards = ImmutableList.builder();
+        ImmutableList.Builder<ShardRouting> assignedShards = ImmutableList.builder();
+        ImmutableList.Builder<ShardRouting> allInitializingShards = ImmutableList.builder();
         boolean allShardsStarted = true;
         for (ShardRouting shard : shards) {
             if (shard.primary()) {
@@ -99,14 +100,14 @@ public class IndexShardRoutingTable implements Iterable<ShardRouting> {
 
         this.primary = primary;
         if (primary != null) {
-            this.primaryAsList = Collections.singletonList(primary);
+            this.primaryAsList = ImmutableList.of(primary);
         } else {
-            this.primaryAsList = Collections.emptyList();
+            this.primaryAsList = ImmutableList.of();
         }
-        this.replicas = Collections.unmodifiableList(replicas);
-        this.activeShards = Collections.unmodifiableList(activeShards);
-        this.assignedShards = Collections.unmodifiableList(assignedShards);
-        this.allInitializingShards = Collections.unmodifiableList(allInitializingShards);
+        this.replicas = replicas.build();
+        this.activeShards = activeShards.build();
+        this.assignedShards = assignedShards.build();
+        this.allInitializingShards = allInitializingShards.build();
     }
 
     /**
@@ -140,7 +141,7 @@ public class IndexShardRoutingTable implements Iterable<ShardRouting> {
                 shardRoutings.add(new ShardRouting(shards.get(i), highestVersion));
             }
         }
-        return new IndexShardRoutingTable(shardId, Collections.unmodifiableList(shardRoutings));
+        return new IndexShardRoutingTable(shardId, ImmutableList.copyOf(shardRoutings));
     }
 
     /**
@@ -463,11 +464,11 @@ public class IndexShardRoutingTable implements Iterable<ShardRouting> {
 
     static class AttributesRoutings {
 
-        public final List<ShardRouting> withSameAttribute;
-        public final List<ShardRouting> withoutSameAttribute;
+        public final ImmutableList<ShardRouting> withSameAttribute;
+        public final ImmutableList<ShardRouting> withoutSameAttribute;
         public final int totalSize;
 
-        AttributesRoutings(List<ShardRouting> withSameAttribute, List<ShardRouting> withoutSameAttribute) {
+        AttributesRoutings(ImmutableList<ShardRouting> withSameAttribute, ImmutableList<ShardRouting> withoutSameAttribute) {
             this.withSameAttribute = withSameAttribute;
             this.withoutSameAttribute = withoutSameAttribute;
             this.totalSize = withoutSameAttribute.size() + withSameAttribute.size();
@@ -483,9 +484,9 @@ public class IndexShardRoutingTable implements Iterable<ShardRouting> {
         if (shardRoutings == null) {
             synchronized (shardsByAttributeMutex) {
                 ArrayList<ShardRouting> from = new ArrayList<>(activeShards);
-                List<ShardRouting> to = collectAttributeShards(key, nodes, from);
+                ImmutableList<ShardRouting> to = collectAttributeShards(key, nodes, from);
 
-                shardRoutings = new AttributesRoutings(to, Collections.unmodifiableList(from));
+                shardRoutings = new AttributesRoutings(to, ImmutableList.copyOf(from));
                 activeShardsByAttributes = MapBuilder.newMapBuilder(activeShardsByAttributes).put(key, shardRoutings).immutableMap();
             }
         }
@@ -497,15 +498,15 @@ public class IndexShardRoutingTable implements Iterable<ShardRouting> {
         if (shardRoutings == null) {
             synchronized (shardsByAttributeMutex) {
                 ArrayList<ShardRouting> from = new ArrayList<>(allInitializingShards);
-                List<ShardRouting> to = collectAttributeShards(key, nodes, from);
-                shardRoutings = new AttributesRoutings(to, Collections.unmodifiableList(from));
+                ImmutableList<ShardRouting> to = collectAttributeShards(key, nodes, from);
+                shardRoutings = new AttributesRoutings(to, ImmutableList.copyOf(from));
                 initializingShardsByAttributes = MapBuilder.newMapBuilder(initializingShardsByAttributes).put(key, shardRoutings).immutableMap();
             }
         }
         return shardRoutings;
     }
 
-    private static List<ShardRouting> collectAttributeShards(AttributesKey key, DiscoveryNodes nodes, ArrayList<ShardRouting> from) {
+    private static ImmutableList<ShardRouting> collectAttributeShards(AttributesKey key, DiscoveryNodes nodes, ArrayList<ShardRouting> from) {
         final ArrayList<ShardRouting> to = new ArrayList<>();
         for (final String attribute : key.attributes) {
             final String localAttributeValue = nodes.localNode().attributes().get(attribute);
@@ -522,7 +523,7 @@ public class IndexShardRoutingTable implements Iterable<ShardRouting> {
                 }
             }
         }
-        return Collections.unmodifiableList(to);
+        return ImmutableList.copyOf(to);
     }
 
     public ShardIterator preferAttributesActiveInitializingShardsIt(String[] attributes, DiscoveryNodes nodes) {
@@ -611,7 +612,7 @@ public class IndexShardRoutingTable implements Iterable<ShardRouting> {
         }
 
         public IndexShardRoutingTable build() {
-            return new IndexShardRoutingTable(shardId, Collections.unmodifiableList(shards));
+            return new IndexShardRoutingTable(shardId, ImmutableList.copyOf(shards));
         }
 
         public static IndexShardRoutingTable readFrom(StreamInput in) throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
index 7acedd2..8c07429 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
@@ -345,10 +345,10 @@ public class RoutingNodes implements Iterable<RoutingNode> {
     /**
      * Moves a shard from unassigned to initialize state
      */
-    public void initialize(ShardRouting shard, String nodeId) {
+    public void initialize(ShardRouting shard, String nodeId, long expectedSize) {
         ensureMutable();
         assert shard.unassigned() : shard;
-        shard.initialize(nodeId);
+        shard.initialize(nodeId, expectedSize);
         node(nodeId).add(shard);
         inactiveShardCount++;
         if (shard.primary()) {
@@ -362,10 +362,10 @@ public class RoutingNodes implements Iterable<RoutingNode> {
      * shard as well as assigning it. And returning the target initializing
      * shard.
      */
-    public ShardRouting relocate(ShardRouting shard, String nodeId) {
+    public ShardRouting relocate(ShardRouting shard, String nodeId, long expectedShardSize) {
         ensureMutable();
         relocatingShards++;
-        shard.relocate(nodeId);
+        shard.relocate(nodeId, expectedShardSize);
         ShardRouting target = shard.buildTargetRelocatingShard();
         node(target.currentNodeId()).add(target);
         assignedShardsAdd(target);
@@ -608,16 +608,9 @@ public class RoutingNodes implements Iterable<RoutingNode> {
             /**
              * Initializes the current unassigned shard and moves it from the unassigned list.
              */
-            public void initialize(String nodeId) {
-                initialize(nodeId, current.version());
-            }
-
-            /**
-             * Initializes the current unassigned shard and moves it from the unassigned list.
-             */
-            public void initialize(String nodeId, long version) {
+            public void initialize(String nodeId, long version, long expectedShardSize) {
                 innerRemove();
-                nodes.initialize(new ShardRouting(current, version), nodeId);
+                nodes.initialize(new ShardRouting(current, version), nodeId, expectedShardSize);
             }
 
             /**
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTableValidation.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTableValidation.java
index 32f5654..bd1e283 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTableValidation.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTableValidation.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster.routing;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -26,7 +27,6 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
 
 import java.io.IOException;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 
@@ -55,7 +55,7 @@ public class RoutingTableValidation implements Streamable {
 
     public List<String> allFailures() {
         if (failures().isEmpty() && indicesFailures().isEmpty()) {
-            return Collections.emptyList();
+            return ImmutableList.of();
         }
         List<String> allFailures = newArrayList(failures());
         for (Map.Entry<String, List<String>> entry : indicesFailures().entrySet()) {
@@ -68,7 +68,7 @@ public class RoutingTableValidation implements Streamable {
 
     public List<String> failures() {
         if (failures == null) {
-            return Collections.emptyList();
+            return ImmutableList.of();
         }
         return failures;
     }
@@ -82,11 +82,11 @@ public class RoutingTableValidation implements Streamable {
 
     public List<String> indexFailures(String index) {
         if (indicesFailures == null) {
-            return Collections.emptyList();
+            return ImmutableList.of();
         }
         List<String> indexFailures = indicesFailures.get(index);
         if (indexFailures == null) {
-            return Collections.emptyList();
+            return ImmutableList.of();
         }
         return indexFailures;
     }
@@ -122,7 +122,7 @@ public class RoutingTableValidation implements Streamable {
         valid = in.readBoolean();
         int size = in.readVInt();
         if (size == 0) {
-            failures = Collections.emptyList();
+            failures = ImmutableList.of();
         } else {
             failures = Lists.newArrayListWithCapacity(size);
             for (int i = 0; i < size; i++) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java b/core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java
index 9c0af60..60764ab 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java
@@ -37,6 +37,11 @@ import java.util.List;
  */
 public final class ShardRouting implements Streamable, ToXContent {
 
+    /**
+     * Used if shard size is not available
+     */
+    public static final long UNAVAILABLE_EXPECTED_SHARD_SIZE = -1;
+
     private String index;
     private int shardId;
     private String currentNodeId;
@@ -50,6 +55,7 @@ public final class ShardRouting implements Streamable, ToXContent {
     private final transient List<ShardRouting> asList;
     private transient ShardId shardIdentifier;
     private boolean frozen = false;
+    private long expectedShardSize = UNAVAILABLE_EXPECTED_SHARD_SIZE;
 
     private ShardRouting() {
         this.asList = Collections.singletonList(this);
@@ -60,7 +66,7 @@ public final class ShardRouting implements Streamable, ToXContent {
     }
 
     public ShardRouting(ShardRouting copy, long version) {
-        this(copy.index(), copy.id(), copy.currentNodeId(), copy.relocatingNodeId(), copy.restoreSource(), copy.primary(), copy.state(), version, copy.unassignedInfo(), copy.allocationId(), true);
+        this(copy.index(), copy.id(), copy.currentNodeId(), copy.relocatingNodeId(), copy.restoreSource(), copy.primary(), copy.state(), version, copy.unassignedInfo(), copy.allocationId(), true, copy.getExpectedShardSize());
     }
 
     /**
@@ -69,7 +75,7 @@ public final class ShardRouting implements Streamable, ToXContent {
      */
     ShardRouting(String index, int shardId, String currentNodeId,
                  String relocatingNodeId, RestoreSource restoreSource, boolean primary, ShardRoutingState state, long version,
-                 UnassignedInfo unassignedInfo, AllocationId allocationId, boolean internal) {
+                 UnassignedInfo unassignedInfo, AllocationId allocationId, boolean internal, long expectedShardSize) {
         this.index = index;
         this.shardId = shardId;
         this.currentNodeId = currentNodeId;
@@ -81,6 +87,9 @@ public final class ShardRouting implements Streamable, ToXContent {
         this.restoreSource = restoreSource;
         this.unassignedInfo = unassignedInfo;
         this.allocationId = allocationId;
+        this.expectedShardSize = expectedShardSize;
+        assert expectedShardSize == UNAVAILABLE_EXPECTED_SHARD_SIZE || state == ShardRoutingState.INITIALIZING || state == ShardRoutingState.RELOCATING : expectedShardSize + " state: " + state;
+        assert expectedShardSize >= 0 || state != ShardRoutingState.INITIALIZING || state != ShardRoutingState.RELOCATING : expectedShardSize + " state: " + state;
         assert !(state == ShardRoutingState.UNASSIGNED && unassignedInfo == null) : "unassigned shard must be created with meta";
         if (!internal) {
             assert state == ShardRoutingState.UNASSIGNED;
@@ -88,13 +97,14 @@ public final class ShardRouting implements Streamable, ToXContent {
             assert relocatingNodeId == null;
             assert allocationId == null;
         }
+
     }
 
     /**
      * Creates a new unassigned shard.
      */
     public static ShardRouting newUnassigned(String index, int shardId, RestoreSource restoreSource, boolean primary, UnassignedInfo unassignedInfo) {
-        return new ShardRouting(index, shardId, null, null, restoreSource, primary, ShardRoutingState.UNASSIGNED, 0, unassignedInfo, null, true);
+        return new ShardRouting(index, shardId, null, null, restoreSource, primary, ShardRoutingState.UNASSIGNED, 0, unassignedInfo, null, true, UNAVAILABLE_EXPECTED_SHARD_SIZE);
     }
 
     /**
@@ -205,7 +215,7 @@ public final class ShardRouting implements Streamable, ToXContent {
     public ShardRouting buildTargetRelocatingShard() {
         assert relocating();
         return new ShardRouting(index, shardId, relocatingNodeId, currentNodeId, restoreSource, primary, ShardRoutingState.INITIALIZING, version, unassignedInfo,
-                AllocationId.newTargetRelocation(allocationId), true);
+                AllocationId.newTargetRelocation(allocationId), true, expectedShardSize);
     }
 
     /**
@@ -317,6 +327,11 @@ public final class ShardRouting implements Streamable, ToXContent {
         if (in.readBoolean()) {
             allocationId = new AllocationId(in);
         }
+        if (relocating() || initializing()) {
+            expectedShardSize = in.readLong();
+        } else {
+            expectedShardSize = UNAVAILABLE_EXPECTED_SHARD_SIZE;
+        }
         freeze();
     }
 
@@ -368,6 +383,10 @@ public final class ShardRouting implements Streamable, ToXContent {
         } else {
             out.writeBoolean(false);
         }
+        if (relocating() || initializing()) {
+            out.writeLong(expectedShardSize);
+        }
+
     }
 
     @Override
@@ -397,12 +416,13 @@ public final class ShardRouting implements Streamable, ToXContent {
         relocatingNodeId = null;
         this.unassignedInfo = unassignedInfo;
         allocationId = null;
+        expectedShardSize = UNAVAILABLE_EXPECTED_SHARD_SIZE;
     }
 
     /**
      * Initializes an unassigned shard on a node.
      */
-    void initialize(String nodeId) {
+    void initialize(String nodeId, long expectedShardSize) {
         ensureNotFrozen();
         version++;
         assert state == ShardRoutingState.UNASSIGNED : this;
@@ -410,6 +430,7 @@ public final class ShardRouting implements Streamable, ToXContent {
         state = ShardRoutingState.INITIALIZING;
         currentNodeId = nodeId;
         allocationId = AllocationId.newInitializing();
+        this.expectedShardSize = expectedShardSize;
     }
 
     /**
@@ -417,13 +438,14 @@ public final class ShardRouting implements Streamable, ToXContent {
      *
      * @param relocatingNodeId id of the node to relocate the shard
      */
-    void relocate(String relocatingNodeId) {
+    void relocate(String relocatingNodeId, long expectedShardSize) {
         ensureNotFrozen();
         version++;
         assert state == ShardRoutingState.STARTED : "current shard has to be started in order to be relocated " + this;
         state = ShardRoutingState.RELOCATING;
         this.relocatingNodeId = relocatingNodeId;
         this.allocationId = AllocationId.newRelocation(allocationId);
+        this.expectedShardSize = expectedShardSize;
     }
 
     /**
@@ -436,7 +458,7 @@ public final class ShardRouting implements Streamable, ToXContent {
         assert state == ShardRoutingState.RELOCATING : this;
         assert assignedToNode() : this;
         assert relocatingNodeId != null : this;
-
+        expectedShardSize = UNAVAILABLE_EXPECTED_SHARD_SIZE;
         state = ShardRoutingState.STARTED;
         relocatingNodeId = null;
         allocationId = AllocationId.cancelRelocation(allocationId);
@@ -470,6 +492,7 @@ public final class ShardRouting implements Streamable, ToXContent {
             // relocation target
             allocationId = AllocationId.finishRelocation(allocationId);
         }
+        expectedShardSize = UNAVAILABLE_EXPECTED_SHARD_SIZE;
         state = ShardRoutingState.STARTED;
     }
 
@@ -669,6 +692,9 @@ public final class ShardRouting implements Streamable, ToXContent {
         if (this.unassignedInfo != null) {
             sb.append(", ").append(unassignedInfo.toString());
         }
+        if (expectedShardSize != UNAVAILABLE_EXPECTED_SHARD_SIZE) {
+            sb.append(", expected_shard_size[").append(expectedShardSize).append("]");
+        }
         return sb.toString();
     }
 
@@ -682,7 +708,9 @@ public final class ShardRouting implements Streamable, ToXContent {
                 .field("shard", shardId().id())
                 .field("index", shardId().index().name())
                 .field("version", version);
-
+        if (expectedShardSize != UNAVAILABLE_EXPECTED_SHARD_SIZE){
+            builder.field("expected_shard_size_in_bytes", expectedShardSize);
+        }
         if (restoreSource() != null) {
             builder.field("restore_source");
             restoreSource().toXContent(builder, params);
@@ -709,4 +737,12 @@ public final class ShardRouting implements Streamable, ToXContent {
     boolean isFrozen() {
         return frozen;
     }
+
+    /**
+     * Returns the expected shard size for {@link ShardRoutingState#RELOCATING} and {@link ShardRoutingState#INITIALIZING}
+     * shards. If it's size is not available {@value #UNAVAILABLE_EXPECTED_SHARD_SIZE} will be returned.
+     */
+    public long getExpectedShardSize() {
+        return expectedShardSize;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java
index e93b846..e5a4685 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.cluster.routing.allocation;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 import org.elasticsearch.cluster.ClusterInfoService;
 import org.elasticsearch.cluster.ClusterState;
@@ -35,7 +36,6 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 
@@ -86,7 +86,7 @@ public class AllocationService extends AbstractComponent {
     }
 
     public RoutingAllocation.Result applyFailedShard(ClusterState clusterState, ShardRouting failedShard) {
-        return applyFailedShards(clusterState, Collections.singletonList(new FailedRerouteAllocation.FailedShard(failedShard, null, null)));
+        return applyFailedShards(clusterState, ImmutableList.of(new FailedRerouteAllocation.FailedShard(failedShard, null, null)));
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
index 9c42256..4d56659 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
@@ -507,7 +507,7 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards
                     Decision decision = allocation.deciders().canAllocate(shard, target, allocation);
                     if (decision.type() == Type.YES) { // TODO maybe we can respect throttling here too?
                         sourceNode.removeShard(shard);
-                        ShardRouting targetRelocatingShard = routingNodes.relocate(shard, target.nodeId());
+                        ShardRouting targetRelocatingShard = routingNodes.relocate(shard, target.nodeId(), allocation.clusterInfo().getShardSize(shard, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));
                         currentNode.addShard(targetRelocatingShard, decision);
                         if (logger.isTraceEnabled()) {
                             logger.trace("Moved shard [{}] to node [{}]", shard, currentNode.getNodeId());
@@ -687,7 +687,7 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards
                             if (logger.isTraceEnabled()) {
                                 logger.trace("Assigned shard [{}] to [{}]", shard, minNode.getNodeId());
                             }
-                            routingNodes.initialize(shard, routingNodes.node(minNode.getNodeId()).nodeId());
+                            routingNodes.initialize(shard, routingNodes.node(minNode.getNodeId()).nodeId(), allocation.clusterInfo().getShardSize(shard, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));
                             changed = true;
                             continue; // don't add to ignoreUnassigned
                         } else {
@@ -779,10 +779,10 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards
                         /* now allocate on the cluster - if we are started we need to relocate the shard */
                         if (candidate.started()) {
                             RoutingNode lowRoutingNode = routingNodes.node(minNode.getNodeId());
-                            routingNodes.relocate(candidate, lowRoutingNode.nodeId());
+                            routingNodes.relocate(candidate, lowRoutingNode.nodeId(), allocation.clusterInfo().getShardSize(candidate, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));
 
                         } else {
-                            routingNodes.initialize(candidate, routingNodes.node(minNode.getNodeId()).nodeId());
+                            routingNodes.initialize(candidate, routingNodes.node(minNode.getNodeId()).nodeId(), allocation.clusterInfo().getShardSize(candidate, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));
                         }
                         return true;
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateAllocationCommand.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateAllocationCommand.java
index 5e9f44b..b210557 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateAllocationCommand.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateAllocationCommand.java
@@ -231,7 +231,7 @@ public class AllocateAllocationCommand implements AllocationCommand {
                 unassigned.updateUnassignedInfo(new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED,
                         "force allocation from previous reason " + unassigned.unassignedInfo().getReason() + ", " + unassigned.unassignedInfo().getMessage(), unassigned.unassignedInfo().getFailure()));
             }
-            it.initialize(routingNode.nodeId());
+            it.initialize(routingNode.nodeId(), unassigned.version(), allocation.clusterInfo().getShardSize(unassigned, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));
             break;
         }
         return new RerouteExplanation(this, decision);
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java
index f945da3..614397a 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java
@@ -178,7 +178,7 @@ public class MoveAllocationCommand implements AllocationCommand {
             if (decision.type() == Decision.Type.THROTTLE) {
                 // its being throttled, maybe have a flag to take it into account and fail? for now, just do it since the "user" wants it...
             }
-            allocation.routingNodes().relocate(shardRouting, toRoutingNode.nodeId());
+            allocation.routingNodes().relocate(shardRouting, toRoutingNode.nodeId(), allocation.clusterInfo().getShardSize(shardRouting, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));
         }
 
         if (!found) {
diff --git a/core/src/main/java/org/elasticsearch/common/ValidationException.java b/core/src/main/java/org/elasticsearch/common/ValidationException.java
new file mode 100644
index 0000000..1328876
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/ValidationException.java
@@ -0,0 +1,71 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common;
+
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * Encapsulates an accumulation of validation errors
+ */
+public class ValidationException extends IllegalArgumentException {
+    private final List<String> validationErrors = new ArrayList<>();
+
+    public ValidationException() {
+        super("validation failed");
+    }
+
+    /**
+     * Add a new validation error to the accumulating validation errors
+     * @param error the error to add
+     */
+    public void addValidationError(String error) {
+        validationErrors.add(error);
+    }
+
+    /**
+     * Add a sequence of validation errors to the accumulating validation errors
+     * @param errors the errors to add
+     */
+    public void addValidationErrors(Iterable<String> errors) {
+        for (String error : errors) {
+            validationErrors.add(error);
+        }
+    }
+
+    /**
+     * Returns the validation errors accumulated
+     * @return
+     */
+    public List<String> validationErrors() {
+        return validationErrors;
+    }
+
+    @Override
+    public String getMessage() {
+        StringBuilder sb = new StringBuilder();
+        sb.append("Validation Failed: ");
+        int index = 0;
+        for (String error : validationErrors) {
+            sb.append(++index).append(": ").append(error).append(";");
+        }
+        return sb.toString();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/blobstore/BlobPath.java b/core/src/main/java/org/elasticsearch/common/blobstore/BlobPath.java
index 7636097..5c1ff00 100644
--- a/core/src/main/java/org/elasticsearch/common/blobstore/BlobPath.java
+++ b/core/src/main/java/org/elasticsearch/common/blobstore/BlobPath.java
@@ -19,28 +19,26 @@
 
 package org.elasticsearch.common.blobstore;
 
+import com.google.common.collect.ImmutableList;
 
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Iterator;
-import java.util.List;
 
 /**
  *
  */
 public class BlobPath implements Iterable<String> {
 
-    private final List<String> paths;
+    private final ImmutableList<String> paths;
 
     public BlobPath() {
-        this.paths = Collections.emptyList();
+        this.paths = ImmutableList.of();
     }
 
     public static BlobPath cleanPath() {
         return new BlobPath();
     }
 
-    private BlobPath(List<String> paths) {
+    private BlobPath(ImmutableList<String> paths) {
         this.paths = paths;
     }
 
@@ -54,10 +52,8 @@ public class BlobPath implements Iterable<String> {
     }
 
     public BlobPath add(String path) {
-        List<String> paths = new ArrayList<>();
-        paths.addAll(this.paths);
-        paths.add(path);
-        return new BlobPath(Collections.unmodifiableList(paths));
+        ImmutableList.Builder<String> builder = ImmutableList.builder();
+        return new BlobPath(builder.addAll(paths).add(path).build());
     }
 
     public String buildAsString(String separator) {
diff --git a/core/src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java b/core/src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java
index 0ee003c..a4a1543 100644
--- a/core/src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java
+++ b/core/src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java
@@ -360,10 +360,11 @@ public class HttpDownloadHelper {
 
             if (connection instanceof HttpURLConnection) {
                 ((HttpURLConnection) connection).setInstanceFollowRedirects(false);
-                ((HttpURLConnection) connection).setUseCaches(true);
-                ((HttpURLConnection) connection).setConnectTimeout(5000);
+                connection.setUseCaches(true);
+                connection.setConnectTimeout(5000);
             }
             connection.setRequestProperty("ES-Version", Version.CURRENT.toString());
+            connection.setRequestProperty("ES-Build-Hash", Build.CURRENT.hashShort());
             connection.setRequestProperty("User-Agent", "elasticsearch-plugin-manager");
 
             // connect to the remote site (may take some time)
diff --git a/core/src/main/java/org/elasticsearch/common/inject/EncounterImpl.java b/core/src/main/java/org/elasticsearch/common/inject/EncounterImpl.java
index 3a60676..4a06a48 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/EncounterImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/EncounterImpl.java
@@ -16,13 +16,13 @@
 
 package org.elasticsearch.common.inject;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 import org.elasticsearch.common.inject.internal.Errors;
 import org.elasticsearch.common.inject.spi.InjectionListener;
 import org.elasticsearch.common.inject.spi.Message;
 import org.elasticsearch.common.inject.spi.TypeEncounter;
 
-import java.util.Collections;
 import java.util.List;
 
 import static com.google.common.base.Preconditions.checkState;
@@ -47,16 +47,16 @@ final class EncounterImpl<T> implements TypeEncounter<T> {
         valid = false;
     }
 
-    public List<MembersInjector<? super T>> getMembersInjectors() {
+    public ImmutableList<MembersInjector<? super T>> getMembersInjectors() {
         return membersInjectors == null
-                ? Collections.<MembersInjector<? super T>>emptyList()
-                : Collections.unmodifiableList(membersInjectors);
+                ? ImmutableList.<MembersInjector<? super T>>of()
+                : ImmutableList.copyOf(membersInjectors);
     }
 
-    public List<InjectionListener<? super T>> getInjectionListeners() {
+    public ImmutableList<InjectionListener<? super T>> getInjectionListeners() {
         return injectionListeners == null
-                ? Collections.<InjectionListener<? super T>>emptyList()
-                : Collections.unmodifiableList(injectionListeners);
+                ? ImmutableList.<InjectionListener<? super T>>of()
+                : ImmutableList.copyOf(injectionListeners);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/common/inject/InjectionRequestProcessor.java b/core/src/main/java/org/elasticsearch/common/inject/InjectionRequestProcessor.java
index 5af88fd..35e69d3 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/InjectionRequestProcessor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/InjectionRequestProcessor.java
@@ -16,6 +16,7 @@
 
 package org.elasticsearch.common.inject;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 import org.elasticsearch.common.inject.internal.Errors;
 import org.elasticsearch.common.inject.internal.ErrorsException;
@@ -84,7 +85,7 @@ class InjectionRequestProcessor extends AbstractProcessor {
         final InjectorImpl injector;
         final Object source;
         final StaticInjectionRequest request;
-        List<SingleMemberInjector> memberInjectors;
+        ImmutableList<SingleMemberInjector> memberInjectors;
 
         public StaticInjection(InjectorImpl injector, StaticInjectionRequest request) {
             this.injector = injector;
diff --git a/core/src/main/java/org/elasticsearch/common/inject/InjectorImpl.java b/core/src/main/java/org/elasticsearch/common/inject/InjectorImpl.java
index 5e576b3..6f2540c 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/InjectorImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/InjectorImpl.java
@@ -16,6 +16,7 @@
 
 package org.elasticsearch.common.inject;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
@@ -26,7 +27,6 @@ import org.elasticsearch.common.inject.util.Providers;
 
 import java.lang.annotation.Annotation;
 import java.lang.reflect.*;
-import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
@@ -142,7 +142,7 @@ class InjectorImpl implements Injector, Lookups {
 
     @Override
     public Injector createChildInjector(Module... modules) {
-        return createChildInjector(Arrays.asList(modules));
+        return createChildInjector(ImmutableList.copyOf(modules));
     }
 
     /**
@@ -694,7 +694,7 @@ class InjectorImpl implements Injector, Lookups {
             List<Binding<?>> bindings = multimap.get(type);
             return bindings != null
                     ? Collections.<Binding<T>>unmodifiableList((List) multimap.get(type))
-                    : Collections.<Binding<T>>emptyList();
+                    : ImmutableList.<Binding<T>>of();
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/common/inject/MembersInjectorImpl.java b/core/src/main/java/org/elasticsearch/common/inject/MembersInjectorImpl.java
index 399a231..0bac3d8 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/MembersInjectorImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/MembersInjectorImpl.java
@@ -16,6 +16,7 @@
 
 package org.elasticsearch.common.inject;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
 import org.elasticsearch.common.inject.internal.Errors;
 import org.elasticsearch.common.inject.internal.ErrorsException;
@@ -23,8 +24,6 @@ import org.elasticsearch.common.inject.internal.InternalContext;
 import org.elasticsearch.common.inject.spi.InjectionListener;
 import org.elasticsearch.common.inject.spi.InjectionPoint;
 
-import java.util.List;
-
 /**
  * Injects members of instances of a given type.
  *
@@ -33,12 +32,12 @@ import java.util.List;
 class MembersInjectorImpl<T> implements MembersInjector<T> {
     private final TypeLiteral<T> typeLiteral;
     private final InjectorImpl injector;
-    private final List<SingleMemberInjector> memberInjectors;
-    private final List<MembersInjector<? super T>> userMembersInjectors;
-    private final List<InjectionListener<? super T>> injectionListeners;
+    private final ImmutableList<SingleMemberInjector> memberInjectors;
+    private final ImmutableList<MembersInjector<? super T>> userMembersInjectors;
+    private final ImmutableList<InjectionListener<? super T>> injectionListeners;
 
     MembersInjectorImpl(InjectorImpl injector, TypeLiteral<T> typeLiteral,
-                        EncounterImpl<T> encounter, List<SingleMemberInjector> memberInjectors) {
+                        EncounterImpl<T> encounter, ImmutableList<SingleMemberInjector> memberInjectors) {
         this.injector = injector;
         this.typeLiteral = typeLiteral;
         this.memberInjectors = memberInjectors;
@@ -46,7 +45,7 @@ class MembersInjectorImpl<T> implements MembersInjector<T> {
         this.injectionListeners = encounter.getInjectionListeners();
     }
 
-    public List<SingleMemberInjector> getMemberInjectors() {
+    public ImmutableList<SingleMemberInjector> getMemberInjectors() {
         return memberInjectors;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/common/inject/MembersInjectorStore.java b/core/src/main/java/org/elasticsearch/common/inject/MembersInjectorStore.java
index fdb6653..99d0924 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/MembersInjectorStore.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/MembersInjectorStore.java
@@ -16,6 +16,7 @@
 
 package org.elasticsearch.common.inject;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 import org.elasticsearch.common.inject.internal.Errors;
 import org.elasticsearch.common.inject.internal.ErrorsException;
@@ -24,7 +25,6 @@ import org.elasticsearch.common.inject.spi.InjectionPoint;
 import org.elasticsearch.common.inject.spi.TypeListenerBinding;
 
 import java.lang.reflect.Field;
-import java.util.Collections;
 import java.util.List;
 import java.util.Set;
 
@@ -35,7 +35,7 @@ import java.util.Set;
  */
 class MembersInjectorStore {
     private final InjectorImpl injector;
-    private final List<TypeListenerBinding> typeListenerBindings;
+    private final ImmutableList<TypeListenerBinding> typeListenerBindings;
 
     private final FailableCache<TypeLiteral<?>, MembersInjectorImpl<?>> cache
             = new FailableCache<TypeLiteral<?>, MembersInjectorImpl<?>>() {
@@ -49,7 +49,7 @@ class MembersInjectorStore {
     MembersInjectorStore(InjectorImpl injector,
                          List<TypeListenerBinding> typeListenerBindings) {
         this.injector = injector;
-        this.typeListenerBindings = Collections.unmodifiableList(typeListenerBindings);
+        this.typeListenerBindings = ImmutableList.copyOf(typeListenerBindings);
     }
 
     /**
@@ -82,7 +82,7 @@ class MembersInjectorStore {
             errors.merge(e.getErrorMessages());
             injectionPoints = e.getPartialValue();
         }
-        List<SingleMemberInjector> injectors = getInjectors(injectionPoints, errors);
+        ImmutableList<SingleMemberInjector> injectors = getInjectors(injectionPoints, errors);
         errors.throwIfNewErrors(numErrorsBefore);
 
         EncounterImpl<T> encounter = new EncounterImpl<>(errors, injector.lookups);
@@ -104,7 +104,7 @@ class MembersInjectorStore {
     /**
      * Returns the injectors for the specified injection points.
      */
-    List<SingleMemberInjector> getInjectors(
+    ImmutableList<SingleMemberInjector> getInjectors(
             Set<InjectionPoint> injectionPoints, Errors errors) {
         List<SingleMemberInjector> injectors = Lists.newArrayList();
         for (InjectionPoint injectionPoint : injectionPoints) {
@@ -120,6 +120,6 @@ class MembersInjectorStore {
                 // ignored for now
             }
         }
-        return Collections.unmodifiableList(injectors);
+        return ImmutableList.copyOf(injectors);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Modules.java b/core/src/main/java/org/elasticsearch/common/inject/Modules.java
deleted file mode 100644
index edb08dd..0000000
--- a/core/src/main/java/org/elasticsearch/common/inject/Modules.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.inject;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.settings.Settings;
-
-import java.lang.reflect.Constructor;
-
-/**
- *
- */
-public class Modules {
-
-    public static Module createModule(Class<? extends Module> moduleClass, @Nullable Settings settings) {
-        Constructor<? extends Module> constructor;
-        try {
-            constructor = moduleClass.getConstructor(Settings.class);
-            try {
-                return constructor.newInstance(settings);
-            } catch (Exception e) {
-                throw new ElasticsearchException("Failed to create module [" + moduleClass + "]", e);
-            }
-        } catch (NoSuchMethodException e) {
-            try {
-                constructor = moduleClass.getConstructor();
-                try {
-                    return constructor.newInstance();
-                } catch (Exception e1) {
-                    throw new ElasticsearchException("Failed to create module [" + moduleClass + "]", e);
-                }
-            } catch (NoSuchMethodException e1) {
-                throw new ElasticsearchException("No constructor for [" + moduleClass + "]");
-            }
-        }
-    }
-
-    public static void processModules(Iterable<Module> modules) {
-        for (Module module : modules) {
-            if (module instanceof PreProcessModule) {
-                for (Module module1 : modules) {
-                    ((PreProcessModule) module).processModule(module1);
-                }
-            }
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/common/inject/ModulesBuilder.java b/core/src/main/java/org/elasticsearch/common/inject/ModulesBuilder.java
index 3443312..c65a07d 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/ModulesBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/ModulesBuilder.java
@@ -31,20 +31,9 @@ public class ModulesBuilder implements Iterable<Module> {
 
     private final List<Module> modules = Lists.newArrayList();
 
-    public ModulesBuilder add(Module... modules) {
-        for (Module module : modules) {
-            add(module);
-        }
-        return this;
-    }
-
-    public ModulesBuilder add(Module module) {
-        modules.add(module);
-        if (module instanceof SpawnModules) {
-            Iterable<? extends Module> spawned = ((SpawnModules) module).spawnModules();
-            for (Module spawn : spawned) {
-                add(spawn);
-            }
+    public ModulesBuilder add(Module... newModules) {
+        for (Module module : newModules) {
+            modules.add(module);
         }
         return this;
     }
@@ -55,7 +44,6 @@ public class ModulesBuilder implements Iterable<Module> {
     }
 
     public Injector createInjector() {
-        Modules.processModules(modules);
         Injector injector = Guice.createInjector(modules);
         Injectors.cleanCaches(injector);
         // in ES, we always create all instances as if they are eager singletons
@@ -65,7 +53,6 @@ public class ModulesBuilder implements Iterable<Module> {
     }
 
     public Injector createChildInjector(Injector injector) {
-        Modules.processModules(modules);
         Injector childInjector = injector.createChildInjector(modules);
         Injectors.cleanCaches(childInjector);
         // in ES, we always create all instances as if they are eager singletons
diff --git a/core/src/main/java/org/elasticsearch/common/inject/ProvisionException.java b/core/src/main/java/org/elasticsearch/common/inject/ProvisionException.java
index b124dfc..4c0c365 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/ProvisionException.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/ProvisionException.java
@@ -16,12 +16,12 @@
 
 package org.elasticsearch.common.inject;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
 import org.elasticsearch.common.inject.internal.Errors;
 import org.elasticsearch.common.inject.spi.Message;
 
 import java.util.Collection;
-import java.util.Collections;
 
 import static com.google.common.base.Preconditions.checkArgument;
 
@@ -47,7 +47,7 @@ public final class ProvisionException extends RuntimeException {
 
     public ProvisionException(String message, Throwable cause) {
         super(cause);
-        this.messages = ImmutableSet.of(new Message(Collections.emptyList(), message, cause));
+        this.messages = ImmutableSet.of(new Message(ImmutableList.of(), message, cause));
     }
 
     public ProvisionException(String message) {
diff --git a/core/src/main/java/org/elasticsearch/common/inject/SpawnModules.java b/core/src/main/java/org/elasticsearch/common/inject/SpawnModules.java
deleted file mode 100644
index e500535..0000000
--- a/core/src/main/java/org/elasticsearch/common/inject/SpawnModules.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.inject;
-
-/**
- * This interface can be added to a Module to spawn sub modules. DO NOT USE.
- *
- * This is fundamentally broken.
- * <ul>
- * <li>If you have a plugin with multiple modules, return all the modules at once.</li>
- * <li>If you are trying to make the implementation of a module "pluggable", don't do it.
- * This is not extendable because custom implementations (using onModule) cannot be
- * registered before spawnModules() is called.</li>
- * </ul>
- */
-public interface SpawnModules {
-
-    Iterable<? extends Module> spawnModules();
-}
diff --git a/core/src/main/java/org/elasticsearch/common/inject/State.java b/core/src/main/java/org/elasticsearch/common/inject/State.java
index 53d1bdd..b3f662c 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/State.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/State.java
@@ -16,6 +16,7 @@
 
 package org.elasticsearch.common.inject;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
 import org.elasticsearch.common.inject.internal.BindingImpl;
 import org.elasticsearch.common.inject.internal.Errors;
@@ -23,7 +24,6 @@ import org.elasticsearch.common.inject.internal.MatcherAndConverter;
 import org.elasticsearch.common.inject.spi.TypeListenerBinding;
 
 import java.lang.annotation.Annotation;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 
@@ -89,7 +89,7 @@ interface State {
 
         @Override
         public List<TypeListenerBinding> getTypeListenerBindings() {
-            return Collections.emptyList();
+            return ImmutableList.of();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/common/inject/TypeLiteral.java b/core/src/main/java/org/elasticsearch/common/inject/TypeLiteral.java
index b83df09..37dfebb 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/TypeLiteral.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/TypeLiteral.java
@@ -16,12 +16,11 @@
 
 package org.elasticsearch.common.inject;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.common.inject.internal.MoreTypes;
 import org.elasticsearch.common.inject.util.Types;
 
 import java.lang.reflect.*;
-import java.util.Arrays;
-import java.util.Collections;
 import java.util.List;
 
 import static com.google.common.base.Preconditions.checkArgument;
@@ -175,7 +174,7 @@ public class TypeLiteral<T> {
         for (int t = 0; t < types.length; t++) {
             result[t] = resolve(types[t]);
         }
-        return Arrays.asList(result);
+        return ImmutableList.copyOf(result);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java
index 21c094b..2bfbcef 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java
@@ -16,6 +16,7 @@
 
 package org.elasticsearch.common.inject.assistedinject;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Iterables;
 import com.google.common.collect.Lists;
@@ -30,7 +31,6 @@ import java.lang.reflect.InvocationHandler;
 import java.lang.reflect.Method;
 import java.lang.reflect.Proxy;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.List;
 
 import static com.google.common.base.Preconditions.checkState;
@@ -81,7 +81,7 @@ final class FactoryProvider2<F> implements InvocationHandler, Provider<F> {
      */
     private final Key<?> producedType;
     private final ImmutableMap<Method, Key<?>> returnTypesByMethod;
-    private final ImmutableMap<Method, List<Key<?>>> paramTypes;
+    private final ImmutableMap<Method, ImmutableList<Key<?>>> paramTypes;
 
     /**
      * the hosting injector, or null if we haven't been initialized yet
@@ -108,7 +108,7 @@ final class FactoryProvider2<F> implements InvocationHandler, Provider<F> {
 
         try {
             ImmutableMap.Builder<Method, Key<?>> returnTypesBuilder = ImmutableMap.builder();
-            ImmutableMap.Builder<Method, List<Key<?>>> paramTypesBuilder
+            ImmutableMap.Builder<Method, ImmutableList<Key<?>>> paramTypesBuilder
                     = ImmutableMap.builder();
             // TODO: also grab methods from superinterfaces
             for (Method method : factoryRawType.getMethods()) {
@@ -123,7 +123,7 @@ final class FactoryProvider2<F> implements InvocationHandler, Provider<F> {
                     Key<?> paramKey = getKey(param, method, paramAnnotations[p++], errors);
                     keys.add(assistKey(method, paramKey, errors));
                 }
-                paramTypesBuilder.put(method, Collections.unmodifiableList(keys));
+                paramTypesBuilder.put(method, ImmutableList.copyOf(keys));
             }
             returnTypesByMethod = returnTypesBuilder.build();
             paramTypes = paramTypesBuilder.build();
@@ -165,8 +165,8 @@ final class FactoryProvider2<F> implements InvocationHandler, Provider<F> {
     @Inject
     void initialize(Injector injector) {
         if (this.injector != null) {
-            throw new ConfigurationException(Collections.singletonList(new Message(FactoryProvider2.class,
-                "Factories.create() factories may only be used in one Injector!")));
+            throw new ConfigurationException(ImmutableList.of(new Message(FactoryProvider2.class,
+                    "Factories.create() factories may only be used in one Injector!")));
         }
 
         this.injector = injector;
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/Errors.java b/core/src/main/java/org/elasticsearch/common/inject/internal/Errors.java
index 5bd9fd0..e898007 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/Errors.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/Errors.java
@@ -16,6 +16,7 @@
 
 package org.elasticsearch.common.inject.internal;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Lists;
 import org.apache.lucene.util.CollectionUtil;
@@ -435,7 +436,7 @@ public final class Errors implements Serializable {
 
     public List<Message> getMessages() {
         if (root.errors == null) {
-            return Collections.emptyList();
+            return ImmutableList.of();
         }
 
         List<Message> result = Lists.newArrayList(root.errors);
@@ -552,7 +553,7 @@ public final class Errors implements Serializable {
         abstract String toString(T t);
     }
 
-    private static final Collection<Converter<?>> converters = Arrays.asList(
+    private static final Collection<Converter<?>> converters = ImmutableList.of(
             new Converter<Class>(Class.class) {
                 @Override
                 public String toString(Class c) {
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java b/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java
index 4d0a315..0e6a33f 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java
@@ -16,6 +16,7 @@
 
 package org.elasticsearch.common.inject.internal;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
@@ -27,7 +28,6 @@ import org.elasticsearch.common.inject.spi.Element;
 import org.elasticsearch.common.inject.spi.ElementVisitor;
 import org.elasticsearch.common.inject.spi.PrivateElements;
 
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -53,7 +53,7 @@ public final class PrivateElementsImpl implements PrivateElements {
     /**
      * lazily instantiated
      */
-    private List<Element> elements;
+    private ImmutableList<Element> elements;
 
     /**
      * lazily instantiated
@@ -73,7 +73,7 @@ public final class PrivateElementsImpl implements PrivateElements {
     @Override
     public List<Element> getElements() {
         if (elements == null) {
-            elements = Collections.unmodifiableList(elementsMutable);
+            elements = ImmutableList.copyOf(elementsMutable);
             elementsMutable = null;
         }
 
diff --git a/core/src/main/java/org/elasticsearch/common/inject/multibindings/Multibinder.java b/core/src/main/java/org/elasticsearch/common/inject/multibindings/Multibinder.java
index d887488..fac73d3 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/multibindings/Multibinder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/multibindings/Multibinder.java
@@ -16,6 +16,7 @@
 
 package org.elasticsearch.common.inject.multibindings;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Lists;
 import org.elasticsearch.common.inject.*;
@@ -319,6 +320,6 @@ public abstract class Multibinder<T> {
 
         NullPointerException npe = new NullPointerException(name);
         throw new ConfigurationException(ImmutableSet.of(
-                new Message(Collections.emptyList(), npe.toString(), npe)));
+                new Message(ImmutableList.of(), npe.toString(), npe)));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/Elements.java b/core/src/main/java/org/elasticsearch/common/inject/spi/Elements.java
index 42844f2..c6dc1e4 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/Elements.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/Elements.java
@@ -16,9 +16,9 @@
 
 package org.elasticsearch.common.inject.spi;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Sets;
-import org.elasticsearch.bootstrap.Bootstrap;
 import org.elasticsearch.common.inject.*;
 import org.elasticsearch.common.inject.binder.AnnotatedBindingBuilder;
 import org.elasticsearch.common.inject.binder.AnnotatedConstantBindingBuilder;
@@ -238,7 +238,7 @@ public final class Elements {
         @Override
         public void addError(Throwable t) {
             String message = "An exception was caught and reported. Message: " + t.getMessage();
-            elements.add(new Message(Collections.singletonList(getSource()), message, t));
+            elements.add(new Message(ImmutableList.of(getSource()), message, t));
         }
 
         @Override
@@ -342,7 +342,7 @@ public final class Elements {
             return builder;
         }
 
-        private static ESLogger logger = Loggers.getLogger(Bootstrap.class);
+        private static ESLogger logger = Loggers.getLogger(Elements.class);
 
         protected Object getSource() {
             Object ret;
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java b/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java
index daf4e5f..8aef647 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java
@@ -16,6 +16,7 @@
 
 package org.elasticsearch.common.inject.spi;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Lists;
 import org.elasticsearch.common.inject.ConfigurationException;
@@ -42,10 +43,10 @@ public final class InjectionPoint {
 
     private final boolean optional;
     private final Member member;
-    private final List<Dependency<?>> dependencies;
+    private final ImmutableList<Dependency<?>> dependencies;
 
     private InjectionPoint(Member member,
-                           List<Dependency<?>> dependencies, boolean optional) {
+                           ImmutableList<Dependency<?>> dependencies, boolean optional) {
         this.member = member;
         this.dependencies = dependencies;
         this.optional = optional;
@@ -83,11 +84,11 @@ public final class InjectionPoint {
         }
         errors.throwConfigurationExceptionIfErrorsExist();
 
-        this.dependencies = Collections.<Dependency<?>>singletonList(
-            newDependency(key, Nullability.allowsNull(annotations), -1));
+        this.dependencies = ImmutableList.<Dependency<?>>of(
+                newDependency(key, Nullability.allowsNull(annotations), -1));
     }
 
-    private List<Dependency<?>> forMember(Member member, TypeLiteral<?> type,
+    private ImmutableList<Dependency<?>> forMember(Member member, TypeLiteral<?> type,
                                                    Annotation[][] parameterAnnotations) {
         Errors errors = new Errors(member);
         Iterator<Annotation[]> annotationsIterator = Arrays.asList(parameterAnnotations).iterator();
@@ -107,7 +108,7 @@ public final class InjectionPoint {
         }
 
         errors.throwConfigurationExceptionIfErrorsExist();
-        return Collections.unmodifiableList(dependencies);
+        return ImmutableList.copyOf(dependencies);
     }
 
     // This metohd is necessary to create a Dependency<T> with proper generic type information
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/Message.java b/core/src/main/java/org/elasticsearch/common/inject/spi/Message.java
index 0723c0e..fb778e1 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/Message.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/Message.java
@@ -17,14 +17,13 @@
 package org.elasticsearch.common.inject.spi;
 
 import com.google.common.base.Objects;
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.common.inject.Binder;
 import org.elasticsearch.common.inject.internal.Errors;
 import org.elasticsearch.common.inject.internal.SourceProvider;
 
 import java.io.ObjectStreamException;
 import java.io.Serializable;
-import java.util.Arrays;
-import java.util.Collections;
 import java.util.List;
 
 import static com.google.common.base.Preconditions.checkNotNull;
@@ -51,17 +50,17 @@ public final class Message implements Serializable, Element {
      * @since 2.0
      */
     public Message(List<Object> sources, String message, Throwable cause) {
-        this.sources = Collections.unmodifiableList(sources);
+        this.sources = ImmutableList.copyOf(sources);
         this.message = checkNotNull(message, "message");
         this.cause = cause;
     }
 
     public Message(Object source, String message) {
-        this(Collections.singletonList(source), message, null);
+        this(ImmutableList.of(source), message, null);
     }
 
     public Message(String message) {
-        this(Collections.emptyList(), message, null);
+        this(ImmutableList.of(), message, null);
     }
 
     @Override
@@ -139,7 +138,7 @@ public final class Message implements Serializable, Element {
         for (int i = 0; i < sourcesAsStrings.length; i++) {
             sourcesAsStrings[i] = Errors.convert(sourcesAsStrings[i]).toString();
         }
-        return new Message(Arrays.asList(sourcesAsStrings), message, cause);
+        return new Message(ImmutableList.copyOf(sourcesAsStrings), message, cause);
     }
 
     private static final long serialVersionUID = 0;
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
index cb42a9f..1b22a69 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
@@ -542,6 +542,8 @@ public abstract class StreamInput extends InputStream {
                     return (T) readStackTrace(new IllegalStateException(readOptionalString(), readThrowable()), this);
                 case 17:
                     return (T) readStackTrace(new LockObtainFailedException(readOptionalString(), readThrowable()), this);
+                case 18:
+                    return (T) readStackTrace(new InterruptedException(readOptionalString()), this);
                 default:
                     assert false : "no such exception for id: " + key;
             }
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
index afd6073..8ce9e24 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
@@ -590,6 +590,9 @@ public abstract class StreamOutput extends OutputStream {
                 writeVInt(16);
             } else if (throwable instanceof LockObtainFailedException) {
                 writeVInt(17);
+            } else if (throwable instanceof InterruptedException) {
+                writeVInt(18);
+                writeCause = false;
             } else {
                 ElasticsearchException ex;
                 final String name = throwable.getClass().getName();
diff --git a/core/src/main/java/org/elasticsearch/common/joda/Joda.java b/core/src/main/java/org/elasticsearch/common/joda/Joda.java
index 5f1ffb4..174fe22 100644
--- a/core/src/main/java/org/elasticsearch/common/joda/Joda.java
+++ b/core/src/main/java/org/elasticsearch/common/joda/Joda.java
@@ -275,9 +275,9 @@ public class Joda {
                 .toFormatter()
                 .withZoneUTC();
 
-        DateTimeFormatterBuilder builder = new DateTimeFormatterBuilder().append(longFormatter.withZone(DateTimeZone.UTC).getPrinter(), new DateTimeParser[] {longFormatter.getParser(), shortFormatter.getParser()});
+        DateTimeFormatterBuilder builder = new DateTimeFormatterBuilder().append(longFormatter.withZone(DateTimeZone.UTC).getPrinter(), new DateTimeParser[]{longFormatter.getParser(), shortFormatter.getParser(), new EpochTimeParser(true)});
 
-        return new FormatDateTimeFormatter("yyyy/MM/dd HH:mm:ss||yyyy/MM/dd", builder.toFormatter().withZone(DateTimeZone.UTC), Locale.ROOT);
+        return new FormatDateTimeFormatter("yyyy/MM/dd HH:mm:ss||yyyy/MM/dd||epoch_millis", builder.toFormatter().withZone(DateTimeZone.UTC), Locale.ROOT);
     }
 
 
diff --git a/core/src/main/java/org/elasticsearch/common/logging/Loggers.java b/core/src/main/java/org/elasticsearch/common/logging/Loggers.java
index de657c0..93d4cc3 100644
--- a/core/src/main/java/org/elasticsearch/common/logging/Loggers.java
+++ b/core/src/main/java/org/elasticsearch/common/logging/Loggers.java
@@ -84,6 +84,7 @@ public class Loggers {
         }
     }
 
+    @SuppressForbidden(reason = "do not know what this method does")
     public static ESLogger getLogger(String loggerName, Settings settings, String... prefixes) {
         List<String> prefixesList = newArrayList();
         if (settings.getAsBoolean("logger.logHostAddress", false)) {
diff --git a/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java b/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java
index 2e86bef..0b4cdbd 100644
--- a/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java
+++ b/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.common.logging.log4j;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import org.apache.log4j.PropertyConfigurator;
 import org.elasticsearch.ElasticsearchException;
@@ -34,7 +35,6 @@ import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.SimpleFileVisitor;
 import java.nio.file.attribute.BasicFileAttributes;
-import java.util.Arrays;
 import java.util.EnumSet;
 import java.util.List;
 import java.util.Map;
@@ -47,7 +47,7 @@ import static org.elasticsearch.common.settings.Settings.settingsBuilder;
  */
 public class LogConfigurator {
 
-    static final List<String> ALLOWED_SUFFIXES = Arrays.asList(".yml", ".yaml", ".json", ".properties");
+    static final List<String> ALLOWED_SUFFIXES = ImmutableList.of(".yml", ".yaml", ".json", ".properties");
 
     private static boolean loaded;
 
diff --git a/core/src/main/java/org/elasticsearch/common/network/IfConfig.java b/core/src/main/java/org/elasticsearch/common/network/IfConfig.java
new file mode 100644
index 0000000..ea33274
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/network/IfConfig.java
@@ -0,0 +1,167 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.network;
+
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.Loggers;
+
+import java.io.IOException;
+import java.net.Inet6Address;
+import java.net.InetAddress;
+import java.net.InterfaceAddress;
+import java.net.NetworkInterface;
+import java.net.SocketException;
+import java.util.List;
+import java.util.Locale;
+
+/** 
+ * Simple class to log {@code ifconfig}-style output at DEBUG logging.
+ */
+final class IfConfig {
+
+    private static final ESLogger logger = Loggers.getLogger(IfConfig.class);    
+    private static final String INDENT = "        ";
+    
+    /** log interface configuration at debug level, if its enabled */
+    static void logIfNecessary() {
+        if (logger.isDebugEnabled()) {
+            try {
+                doLogging();
+            } catch (IOException | SecurityException e) {
+                logger.warn("unable to gather network information", e);
+            }
+        }
+    }
+    
+    /** perform actual logging: might throw exception if things go wrong */
+    private static void doLogging() throws IOException {
+        StringBuilder msg = new StringBuilder();
+        for (NetworkInterface nic : NetworkUtils.getInterfaces()) {
+            msg.append(System.lineSeparator());
+
+            // ordinary name
+            msg.append(nic.getName());
+            msg.append(System.lineSeparator());
+            
+            // display name (e.g. on windows)
+            if (!nic.getName().equals(nic.getDisplayName())) {
+                msg.append(INDENT);
+                msg.append(nic.getDisplayName());
+                msg.append(System.lineSeparator());
+            }
+            
+            // addresses: v4 first, then v6
+            List<InterfaceAddress> addresses = nic.getInterfaceAddresses();
+            for (InterfaceAddress address : addresses) {
+                if (address.getAddress() instanceof Inet6Address == false) {
+                    msg.append(INDENT);
+                    msg.append(formatAddress(address));
+                    msg.append(System.lineSeparator());
+                }
+            }
+            
+            for (InterfaceAddress address : addresses) {
+                if (address.getAddress() instanceof Inet6Address) {
+                    msg.append(INDENT);
+                    msg.append(formatAddress(address));
+                    msg.append(System.lineSeparator());
+                }
+            }
+            
+            // hardware address
+            byte hardware[] = nic.getHardwareAddress();
+            if (hardware != null) {
+                msg.append(INDENT);
+                msg.append("hardware ");
+                for (int i = 0; i < hardware.length; i++) {
+                    if (i > 0) {
+                        msg.append(":");
+                    }
+                    msg.append(String.format(Locale.ROOT, "%02X", hardware[i]));
+                }
+                msg.append(System.lineSeparator());
+            }
+             
+            // attributes
+            msg.append(INDENT);
+            msg.append(formatFlags(nic));
+            msg.append(System.lineSeparator());
+        }
+        logger.debug("configuration:" + System.lineSeparator() + "{}", msg.toString());
+    }
+    
+    /** format internet address: java's default doesn't include everything useful */
+    private static String formatAddress(InterfaceAddress interfaceAddress) throws IOException {
+        StringBuilder sb = new StringBuilder();
+        
+        InetAddress address = interfaceAddress.getAddress();
+        if (address instanceof Inet6Address) {
+            sb.append("inet6 ");
+            sb.append(NetworkAddress.formatAddress(address));
+            sb.append(" prefixlen:");
+            sb.append(interfaceAddress.getNetworkPrefixLength());
+        } else {
+            sb.append("inet ");
+            sb.append(NetworkAddress.formatAddress(address));
+            int netmask = 0xFFFFFFFF << (32 - interfaceAddress.getNetworkPrefixLength());
+            sb.append(" netmask:" + NetworkAddress.formatAddress(InetAddress.getByAddress(new byte[] {
+                    (byte)(netmask >>> 24), 
+                    (byte)(netmask >>> 16 & 0xFF), 
+                    (byte)(netmask >>> 8 & 0xFF), 
+                    (byte)(netmask & 0xFF) 
+            })));
+            InetAddress broadcast = interfaceAddress.getBroadcast();
+            if (broadcast != null) {
+                sb.append(" broadcast:" + NetworkAddress.formatAddress(broadcast));
+            }
+        }
+        if (address.isLoopbackAddress()) {
+            sb.append(" scope:host");
+        } else if (address.isLinkLocalAddress()) {
+            sb.append(" scope:link");
+        } else if (address.isSiteLocalAddress()) {
+            sb.append(" scope:site");
+        }
+        return sb.toString();
+    }
+    
+    /** format network interface flags */
+    private static String formatFlags(NetworkInterface nic) throws SocketException {
+        StringBuilder flags = new StringBuilder();
+        if (nic.isUp()) {
+            flags.append("UP ");
+        }
+        if (nic.supportsMulticast()) {
+            flags.append("MULTICAST ");
+        }
+        if (nic.isLoopback()) {
+            flags.append("LOOPBACK ");
+        }
+        if (nic.isPointToPoint()) {
+            flags.append("POINTOPOINT ");
+        }
+        if (nic.isVirtual()) {
+            flags.append("VIRTUAL ");
+        }
+        flags.append("mtu:" + nic.getMTU());
+        flags.append(" index:" + nic.getIndex());
+        return flags.toString();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/network/MulticastChannel.java b/core/src/main/java/org/elasticsearch/common/network/MulticastChannel.java
deleted file mode 100644
index 73d4e30..0000000
--- a/core/src/main/java/org/elasticsearch/common/network/MulticastChannel.java
+++ /dev/null
@@ -1,383 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.network;
-
-import com.google.common.collect.Maps;
-import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.ESLoggerFactory;
-import org.elasticsearch.common.settings.Settings;
-
-import java.io.Closeable;
-import java.net.*;
-import java.util.Map;
-import java.util.concurrent.CopyOnWriteArrayList;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import static org.elasticsearch.common.util.concurrent.EsExecutors.daemonThreadFactory;
-
-/**
- * A multicast channel that supports registering for receive events, and sending datagram packets. Allows
- * to easily share the same multicast socket if it holds the same config.
- */
-public abstract class MulticastChannel implements Closeable {
-
-    /**
-     * Builds a channel based on the provided config, allowing to control if sharing a channel that uses
-     * the same config is allowed or not.
-     */
-    public static MulticastChannel getChannel(String name, boolean shared, Config config, Listener listener) throws Exception {
-        if (!shared) {
-            return new Plain(listener, name, config);
-        }
-        return Shared.getSharedChannel(listener, config);
-    }
-
-    /**
-     * Config of multicast channel.
-     */
-    public static final class Config {
-        public final int port;
-        public final String group;
-        public final int bufferSize;
-        public final int ttl;
-        public final InetAddress multicastInterface;
-        public final boolean deferToInterface;
-
-        public Config(int port, String group, int bufferSize, int ttl,
-                      InetAddress multicastInterface, boolean deferToInterface) {
-            this.port = port;
-            this.group = group;
-            this.bufferSize = bufferSize;
-            this.ttl = ttl;
-            this.multicastInterface = multicastInterface;
-            this.deferToInterface = deferToInterface;
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
-
-            Config config = (Config) o;
-
-            if (bufferSize != config.bufferSize) return false;
-            if (port != config.port) return false;
-            if (ttl != config.ttl) return false;
-            if (group != null ? !group.equals(config.group) : config.group != null) return false;
-            if (multicastInterface != null ? !multicastInterface.equals(config.multicastInterface) : config.multicastInterface != null)
-                return false;
-
-            return true;
-        }
-
-        @Override
-        public int hashCode() {
-            int result = port;
-            result = 31 * result + (group != null ? group.hashCode() : 0);
-            result = 31 * result + bufferSize;
-            result = 31 * result + ttl;
-            result = 31 * result + (multicastInterface != null ? multicastInterface.hashCode() : 0);
-            return result;
-        }
-    }
-
-    /**
-     * Listener that gets called when data is received on the multicast channel.
-     */
-    public static interface Listener {
-        void onMessage(BytesReference data, SocketAddress address);
-    }
-
-    /**
-     * Simple listener that wraps multiple listeners into one.
-     */
-    public static class MultiListener implements Listener {
-
-        private final CopyOnWriteArrayList<Listener> listeners = new CopyOnWriteArrayList<>();
-
-        public void add(Listener listener) {
-            this.listeners.add(listener);
-        }
-
-        public boolean remove(Listener listener) {
-            return this.listeners.remove(listener);
-        }
-
-        @Override
-        public void onMessage(BytesReference data, SocketAddress address) {
-            for (Listener listener : listeners) {
-                listener.onMessage(data, address);
-            }
-        }
-    }
-
-    protected final Listener listener;
-    private AtomicBoolean closed = new AtomicBoolean();
-
-    protected MulticastChannel(Listener listener) {
-        this.listener = listener;
-    }
-
-    /**
-     * Send the data over the multicast channel.
-     */
-    public abstract void send(BytesReference data) throws Exception;
-
-    /**
-     * Close the channel.
-     */
-    @Override
-    public void close() {
-        if (closed.compareAndSet(false, true)) {
-            close(listener);
-        }
-    }
-
-    protected abstract void close(Listener listener);
-
-    public static final String SHARED_CHANNEL_NAME = "#shared#";
-    /**
-     * A shared channel that keeps a static map of Config -> Shared channels, and closes shared
-     * channel once their reference count has reached 0. It also handles de-registering relevant
-     * listener from the shared list of listeners.
-     */
-    private final static class Shared extends MulticastChannel {
-
-        private static final Map<Config, Shared> sharedChannels = Maps.newHashMap();
-        private static final Object mutex = new Object(); // global mutex so we don't sync on static methods (.class)
-
-        static MulticastChannel getSharedChannel(Listener listener, Config config) throws Exception {
-
-            synchronized (mutex) {
-                Shared shared = sharedChannels.get(config);
-                if (shared != null) {
-                    shared.incRef();
-                    ((MultiListener) shared.listener).add(listener);
-                } else {
-                    MultiListener multiListener = new MultiListener();
-                    multiListener.add(listener);
-                    shared = new Shared(multiListener, new Plain(multiListener, SHARED_CHANNEL_NAME, config));
-                    sharedChannels.put(config, shared);
-                }
-                return new Delegate(listener, shared);
-            }
-        }
-
-        static void close(Shared shared, Listener listener) {
-            synchronized (mutex) {
-                // remove this
-                boolean removed = ((MultiListener) shared.listener).remove(listener);
-                assert removed : "a listener should be removed";
-                if (shared.decRef() == 0) {
-                    assert ((MultiListener) shared.listener).listeners.isEmpty();
-                    sharedChannels.remove(shared.channel.getConfig());
-                    shared.channel.close();
-                }
-            }
-        }
-
-        final Plain channel;
-        private int refCount = 1;
-
-        Shared(MultiListener listener, Plain channel) {
-            super(listener);
-            this.channel = channel;
-        }
-
-        private void incRef() {
-            refCount++;
-        }
-
-        private int decRef() {
-            --refCount;
-            assert refCount >= 0 : "illegal ref counting, close called multiple times";
-            return refCount;
-        }
-
-        @Override
-        public void send(BytesReference data) throws Exception {
-            channel.send(data);
-        }
-
-        @Override
-        public void close() {
-            assert false : "Shared references should never be closed directly, only via Delegate";
-        }
-
-        @Override
-        protected void close(Listener listener) {
-            close(this, listener);
-        }
-    }
-
-    /**
-     * A light weight delegate that wraps another channel, mainly to support delegating
-     * the close method with the provided listener and not holding existing listener.
-     */
-    private final static class Delegate extends MulticastChannel {
-
-        private final MulticastChannel channel;
-
-        Delegate(Listener listener, MulticastChannel channel) {
-            super(listener);
-            this.channel = channel;
-        }
-
-        @Override
-        public void send(BytesReference data) throws Exception {
-            channel.send(data);
-        }
-
-        @Override
-        protected void close(Listener listener) {
-            channel.close(listener); // we delegate here to the close with our listener, not with the delegate listener
-        }
-    }
-
-    /**
-     * Simple implementation of a channel.
-     */
-    private static class Plain extends MulticastChannel {
-        private final ESLogger logger;
-        private final Config config;
-
-        private volatile MulticastSocket multicastSocket;
-        private final DatagramPacket datagramPacketSend;
-        private final DatagramPacket datagramPacketReceive;
-
-        private final Object sendMutex = new Object();
-        private final Object receiveMutex = new Object();
-
-        private final Receiver receiver;
-        private final Thread receiverThread;
-
-        Plain(Listener listener, String name, Config config) throws Exception {
-            super(listener);
-            this.logger = ESLoggerFactory.getLogger(name);
-            this.config = config;
-            this.datagramPacketReceive = new DatagramPacket(new byte[config.bufferSize], config.bufferSize);
-            this.datagramPacketSend = new DatagramPacket(new byte[config.bufferSize], config.bufferSize, InetAddress.getByName(config.group), config.port);
-            this.multicastSocket = buildMulticastSocket(config);
-            this.receiver = new Receiver();
-            this.receiverThread = daemonThreadFactory(Settings.builder().put("name", name).build(), "discovery#multicast#receiver").newThread(receiver);
-            this.receiverThread.start();
-        }
-
-        private MulticastSocket buildMulticastSocket(Config config) throws Exception {
-            SocketAddress addr = new InetSocketAddress(InetAddress.getByName(config.group), config.port);
-            MulticastSocket multicastSocket = new MulticastSocket(config.port);
-            try {
-                multicastSocket.setTimeToLive(config.ttl);
-                // OSX is not smart enough to tell that a socket bound to the
-                // 'lo0' interface needs to make sure to send the UDP packet
-                // out of the lo0 interface, so we need to do some special
-                // workarounds to fix it.
-                if (config.deferToInterface) {
-                    // 'null' here tells the socket to deter to the interface set
-                    // with .setInterface
-                    multicastSocket.joinGroup(addr, null);
-                    multicastSocket.setInterface(config.multicastInterface);
-                } else {
-                    multicastSocket.setInterface(config.multicastInterface);
-                    multicastSocket.joinGroup(InetAddress.getByName(config.group));
-                }
-                multicastSocket.setReceiveBufferSize(config.bufferSize);
-                multicastSocket.setSendBufferSize(config.bufferSize);
-                multicastSocket.setSoTimeout(60000);
-            } catch (Throwable e) {
-                IOUtils.closeWhileHandlingException(multicastSocket);
-                throw e;
-            }
-            return multicastSocket;
-        }
-
-        public Config getConfig() {
-            return this.config;
-        }
-
-        @Override
-        public void send(BytesReference data) throws Exception {
-            synchronized (sendMutex) {
-                datagramPacketSend.setData(data.toBytes());
-                multicastSocket.send(datagramPacketSend);
-            }
-        }
-
-        @Override
-        protected void close(Listener listener) {
-            receiver.stop();
-            receiverThread.interrupt();
-            if (multicastSocket != null) {
-                IOUtils.closeWhileHandlingException(multicastSocket);
-                multicastSocket = null;
-            }
-            try {
-                receiverThread.join(10000);
-            } catch (InterruptedException e) {
-                Thread.currentThread().interrupt();
-            }
-        }
-
-        private class Receiver implements Runnable {
-
-            private volatile boolean running = true;
-
-            public void stop() {
-                running = false;
-            }
-
-            @Override
-            public void run() {
-                while (running) {
-                    try {
-                        synchronized (receiveMutex) {
-                            try {
-                                multicastSocket.receive(datagramPacketReceive);
-                            } catch (SocketTimeoutException ignore) {
-                                continue;
-                            } catch (Exception e) {
-                                if (running) {
-                                    if (multicastSocket.isClosed()) {
-                                        logger.warn("multicast socket closed while running, restarting...");
-                                        multicastSocket = buildMulticastSocket(config);
-                                    } else {
-                                        logger.warn("failed to receive packet, throttling...", e);
-                                        Thread.sleep(500);
-                                    }
-                                }
-                                continue;
-                            }
-                        }
-                        if (datagramPacketReceive.getData().length > 0) {
-                            listener.onMessage(new BytesArray(datagramPacketReceive.getData()), datagramPacketReceive.getSocketAddress());
-                        }
-                    } catch (Throwable e) {
-                        if (running) {
-                            logger.warn("unexpected exception in multicast receiver", e);
-                        }
-                    }
-                }
-            }
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/common/network/NetworkAddress.java b/core/src/main/java/org/elasticsearch/common/network/NetworkAddress.java
new file mode 100644
index 0000000..91eda6b
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/network/NetworkAddress.java
@@ -0,0 +1,183 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.network;
+
+import com.google.common.net.InetAddresses;
+
+import org.elasticsearch.common.SuppressForbidden;
+
+import java.net.Inet6Address;
+import java.net.InetAddress;
+import java.net.InetSocketAddress;
+import java.util.Objects;
+
+/** 
+ * Utility functions for presentation of network addresses.
+ * <p>
+ * Java's address formatting is particularly bad, every address
+ * has an optional host if its resolved, so IPv4 addresses often
+ * look like this (note the confusing leading slash):
+ * <pre>
+ *    {@code /127.0.0.1}
+ * </pre>
+ * IPv6 addresses are even worse, with no IPv6 address compression,
+ * and often containing things like numeric scopeids, which are even
+ * more confusing (e.g. not going to work in any user's browser, refer
+ * to an interface on <b>another</b> machine, etc):
+ * <pre>
+ *    {@code /0:0:0:0:0:0:0:1%1}
+ * </pre>
+ * This class provides sane address formatting instead, e.g. 
+ * {@code 127.0.0.1} and {@code ::1} respectively. No methods do reverse
+ * lookups.
+ */
+public final class NetworkAddress {
+    /** No instantiation */
+    private NetworkAddress() {}
+
+    /**
+     * Formats a network address (with optional host) for display purposes.
+     * <p>
+     * If the host is already resolved (typically because, we looked up
+     * a name to do that), then we include it, otherwise it is
+     * omitted. See {@link #formatAddress(InetAddress)} if you only
+     * want the address.
+     * <p>
+     * IPv6 addresses are compressed and without scope
+     * identifiers.
+     * <p>
+     * Example output with already-resolved hostnames:
+     * <ul>
+     *   <li>IPv4: {@code localhost/127.0.0.1}</li>
+     *   <li>IPv6: {@code localhost/::1}</li>
+     * </ul>
+     * <p>
+     * Example output with just an address:
+     * <ul>
+     *   <li>IPv4: {@code 127.0.0.1}</li>
+     *   <li>IPv6: {@code ::1}</li>
+     * </ul>
+     * @param address IPv4 or IPv6 address
+     * @return formatted string
+     * @see #formatAddress(InetAddress)
+     */
+    public static String format(InetAddress address) {
+        return format(address, -1, true);
+    }
+
+    /**
+     * Formats a network address and port for display purposes.
+     * <p>
+     * If the host is already resolved (typically because, we looked up
+     * a name to do that), then we include it, otherwise it is
+     * omitted. See {@link #formatAddress(InetSocketAddress)} if you only
+     * want the address.
+     * <p>
+     * This formats the address with {@link #format(InetAddress)}
+     * and appends the port number. IPv6 addresses will be bracketed.
+     * <p>
+     * Example output with already-resolved hostnames:
+     * <ul>
+     *   <li>IPv4: {@code localhost/127.0.0.1:9300}</li>
+     *   <li>IPv6: {@code localhost/[::1]:9300}</li>
+     * </ul>
+     * <p>
+     * Example output with just an address:
+     * <ul>
+     *   <li>IPv4: {@code 127.0.0.1:9300}</li>
+     *   <li>IPv6: {@code [::1]:9300}</li>
+     * </ul>
+     * @param address IPv4 or IPv6 address with port
+     * @return formatted string
+     * @see #formatAddress(InetSocketAddress)
+     */
+    public static String format(InetSocketAddress address) {
+        return format(address.getAddress(), address.getPort(), true);
+    }
+    
+    /**
+     * Formats a network address for display purposes.
+     * <p>
+     * This formats only the address, any hostname information,
+     * if present, is ignored. IPv6 addresses are compressed 
+     * and without scope identifiers.
+     * <p>
+     * Example output with just an address:
+     * <ul>
+     *   <li>IPv4: {@code 127.0.0.1}</li>
+     *   <li>IPv6: {@code ::1}</li>
+     * </ul>
+     * @param address IPv4 or IPv6 address
+     * @return formatted string
+     */
+    public static String formatAddress(InetAddress address) {
+        return format(address, -1, false);
+    }
+    
+    /**
+     * Formats a network address and port for display purposes.
+     * <p>
+     * This formats the address with {@link #formatAddress(InetAddress)}
+     * and appends the port number. IPv6 addresses will be bracketed.
+     * Any host information, if present is ignored.
+     * <p>
+     * Example output:
+     * <ul>
+     *   <li>IPv4: {@code 127.0.0.1:9300}</li>
+     *   <li>IPv6: {@code [::1]:9300}</li>
+     * </ul>
+     * @param address IPv4 or IPv6 address with port
+     * @return formatted string
+     */
+    public static String formatAddress(InetSocketAddress address) {
+        return format(address.getAddress(), address.getPort(), false);
+    }
+    
+    // note, we don't validate port, because we only allow InetSocketAddress
+    @SuppressForbidden(reason = "we call toString to avoid a DNS lookup")
+    static String format(InetAddress address, int port, boolean includeHost) {
+        Objects.requireNonNull(address);
+        
+        StringBuilder builder = new StringBuilder();
+
+        if (includeHost) {
+            // must use toString, to avoid DNS lookup. but the format is specified in the spec
+            String toString = address.toString();
+            int separator = toString.indexOf('/');
+            if (separator > 0) {
+                // append hostname, with the slash too
+                builder.append(toString, 0, separator + 1);
+            }
+        }
+                
+        if (port != -1 && address instanceof Inet6Address) {
+            builder.append(InetAddresses.toUriString(address));
+        } else {
+            builder.append(InetAddresses.toAddrString(address));
+        }
+        
+        if (port != -1) {
+            builder.append(':');
+            builder.append(port);
+        }
+        
+        return builder.toString();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/network/NetworkService.java b/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
index 9f6b77a..1ce318c 100644
--- a/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
+++ b/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
@@ -82,7 +82,7 @@ public class NetworkService extends AbstractComponent {
     @Inject
     public NetworkService(Settings settings) {
         super(settings);
-        InetSocketTransportAddress.setResolveAddress(settings.getAsBoolean("network.address.serialization.resolve", false));
+        IfConfig.logIfNecessary();
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/common/network/NetworkUtils.java b/core/src/main/java/org/elasticsearch/common/network/NetworkUtils.java
index 39705e8..62bc91c 100644
--- a/core/src/main/java/org/elasticsearch/common/network/NetworkUtils.java
+++ b/core/src/main/java/org/elasticsearch/common/network/NetworkUtils.java
@@ -21,8 +21,6 @@ package org.elasticsearch.common.network;
 
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Constants;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
 
 import java.net.Inet4Address;
 import java.net.Inet6Address;
@@ -34,10 +32,12 @@ import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.Comparator;
+import java.util.HashSet;
 import java.util.List;
 
 /**
- * Utilities for network interfaces / addresses
+ * Utilities for network interfaces / addresses binding and publishing.
+ * Its only intended for that purpose, not general purpose usage!!!!
  */
 public abstract class NetworkUtils {
 
@@ -52,6 +52,31 @@ public abstract class NetworkUtils {
      */
     @Deprecated
     static final boolean PREFER_V6 = Boolean.parseBoolean(System.getProperty("java.net.preferIPv6Addresses", "false"));
+
+    /**
+     * True if we can bind to a v6 address. Its silly, but for *binding* we have a need to know
+     * if the stack works. this can prevent scary noise on IPv4-only hosts.
+     * @deprecated transition mechanism only, do not use
+     */
+    @Deprecated
+    public static final boolean SUPPORTS_V6;
+
+    static {
+        boolean v = false;
+        try {
+            for (NetworkInterface nic : getInterfaces()) {
+                for (InetAddress address : Collections.list(nic.getInetAddresses())) {
+                    if (address instanceof Inet6Address) {
+                        v = true;
+                        break;
+                    }
+                }
+            }
+        } catch (SecurityException | SocketException misconfiguration) {
+            v = true; // be optimistic, you misconfigure, then you get noise to your screen
+        }
+        SUPPORTS_V6 = v;
+    }
     
     /** Sorts an address by preference. This way code like publishing can just pick the first one */
     static int sortKey(InetAddress address, boolean prefer_v6) {
@@ -84,7 +109,7 @@ public abstract class NetworkUtils {
      * @deprecated remove this when multihoming is really correct
      */
     @Deprecated
-    private static void sortAddresses(List<InetAddress> list) {
+    static void sortAddresses(List<InetAddress> list) {
         Collections.sort(list, new Comparator<InetAddress>() {
             @Override
             public int compare(InetAddress left, InetAddress right) {
@@ -97,8 +122,6 @@ public abstract class NetworkUtils {
         });
     }
     
-    private final static ESLogger logger = Loggers.getLogger(NetworkUtils.class);
-
     /** Return all interfaces (and subinterfaces) on the system */
     static List<NetworkInterface> getInterfaces() throws SocketException {
         List<NetworkInterface> all = new ArrayList<>();
@@ -128,7 +151,7 @@ public abstract class NetworkUtils {
     }
     
     /** Returns addresses for all loopback interfaces that are up. */
-    public static InetAddress[] getLoopbackAddresses() throws SocketException {
+    static InetAddress[] getLoopbackAddresses() throws SocketException {
         List<InetAddress> list = new ArrayList<>();
         for (NetworkInterface intf : getInterfaces()) {
             if (intf.isLoopback() && intf.isUp()) {
@@ -143,7 +166,7 @@ public abstract class NetworkUtils {
     }
     
     /** Returns addresses for the first non-loopback interface that is up. */
-    public static InetAddress[] getFirstNonLoopbackAddresses() throws SocketException {
+    static InetAddress[] getFirstNonLoopbackAddresses() throws SocketException {
         List<InetAddress> list = new ArrayList<>();
         for (NetworkInterface intf : getInterfaces()) {
             if (intf.isLoopback() == false && intf.isUp()) {
@@ -159,7 +182,7 @@ public abstract class NetworkUtils {
     }
     
     /** Returns addresses for the given interface (it must be marked up) */
-    public static InetAddress[] getAddressesForInterface(String name) throws SocketException {
+    static InetAddress[] getAddressesForInterface(String name) throws SocketException {
         NetworkInterface intf = NetworkInterface.getByName(name);
         if (intf == null) {
             throw new IllegalArgumentException("No interface named '" + name + "' found, got " + getInterfaces());
@@ -176,14 +199,17 @@ public abstract class NetworkUtils {
     }
     
     /** Returns addresses for the given host, sorted by order of preference */
-    public static InetAddress[] getAllByName(String host) throws UnknownHostException {
+    static InetAddress[] getAllByName(String host) throws UnknownHostException {
         InetAddress addresses[] = InetAddress.getAllByName(host);
-        sortAddresses(Arrays.asList(addresses));
-        return addresses;
+        // deduplicate, in case of resolver misconfiguration
+        // stuff like https://bugzilla.redhat.com/show_bug.cgi?id=496300
+        List<InetAddress> unique = new ArrayList<>(new HashSet<>(Arrays.asList(addresses)));
+        sortAddresses(unique);
+        return unique.toArray(new InetAddress[unique.size()]);
     }
     
     /** Returns only the IPV4 addresses in {@code addresses} */
-    public static InetAddress[] filterIPV4(InetAddress addresses[]) {
+    static InetAddress[] filterIPV4(InetAddress addresses[]) {
         List<InetAddress> list = new ArrayList<>();
         for (InetAddress address : addresses) {
             if (address instanceof Inet4Address) {
@@ -197,7 +223,7 @@ public abstract class NetworkUtils {
     }
     
     /** Returns only the IPV6 addresses in {@code addresses} */
-    public static InetAddress[] filterIPV6(InetAddress addresses[]) {
+    static InetAddress[] filterIPV6(InetAddress addresses[]) {
         List<InetAddress> list = new ArrayList<>();
         for (InetAddress address : addresses) {
             if (address instanceof Inet6Address) {
diff --git a/core/src/main/java/org/elasticsearch/common/settings/Settings.java b/core/src/main/java/org/elasticsearch/common/settings/Settings.java
index 9309e1c..13383d6 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/Settings.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/Settings.java
@@ -28,7 +28,6 @@ import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Booleans;
-import org.elasticsearch.common.Classes;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.io.Streams;
 import org.elasticsearch.common.io.stream.StreamInput;
diff --git a/core/src/main/java/org/elasticsearch/common/transport/InetSocketTransportAddress.java b/core/src/main/java/org/elasticsearch/common/transport/InetSocketTransportAddress.java
index f4f686f..6778d74 100644
--- a/core/src/main/java/org/elasticsearch/common/transport/InetSocketTransportAddress.java
+++ b/core/src/main/java/org/elasticsearch/common/transport/InetSocketTransportAddress.java
@@ -21,9 +21,9 @@ package org.elasticsearch.common.transport;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.network.NetworkAddress;
 
 import java.io.IOException;
-import java.net.Inet6Address;
 import java.net.InetAddress;
 import java.net.InetSocketAddress;
 
@@ -32,52 +32,34 @@ import java.net.InetSocketAddress;
  */
 public final class InetSocketTransportAddress implements TransportAddress {
 
-    private static boolean resolveAddress = false;
-
-    public static void setResolveAddress(boolean resolveAddress) {
-        InetSocketTransportAddress.resolveAddress = resolveAddress;
-    }
-
-    public static boolean getResolveAddress() {
-        return resolveAddress;
-    }
-
     public static final InetSocketTransportAddress PROTO = new InetSocketTransportAddress();
 
     private final InetSocketAddress address;
 
     public InetSocketTransportAddress(StreamInput in) throws IOException {
-        if (in.readByte() == 0) {
-            int len = in.readByte();
-            byte[] a = new byte[len]; // 4 bytes (IPv4) or 16 bytes (IPv6)
-            in.readFully(a);
-            InetAddress inetAddress;
-            if (len == 16) {
-                int scope_id = in.readInt();
-                inetAddress = Inet6Address.getByAddress(null, a, scope_id);
-            } else {
-                inetAddress = InetAddress.getByAddress(a);
-            }
-            int port = in.readInt();
-            this.address = new InetSocketAddress(inetAddress, port);
-        } else {
-            this.address = new InetSocketAddress(in.readString(), in.readInt());
-        }
+        final int len = in.readByte();
+        final byte[] a = new byte[len]; // 4 bytes (IPv4) or 16 bytes (IPv6)
+        in.readFully(a);
+        InetAddress inetAddress = InetAddress.getByAddress(a);
+        int port = in.readInt();
+        this.address = new InetSocketAddress(inetAddress, port);
     }
 
     private InetSocketTransportAddress() {
         address = null;
     }
 
-    public InetSocketTransportAddress(String hostname, int port) {
-        this(new InetSocketAddress(hostname, port));
-    }
-
     public InetSocketTransportAddress(InetAddress address, int port) {
         this(new InetSocketAddress(address, port));
     }
 
     public InetSocketTransportAddress(InetSocketAddress address) {
+        if (address == null) {
+            throw new IllegalArgumentException("InetSocketAddress must not be null");
+        }
+        if (address.getAddress() == null) {
+            throw new IllegalArgumentException("Address must be resolved but wasn't - InetSocketAddress#getAddress() returned null");
+        }
         this.address = address;
     }
 
@@ -94,12 +76,12 @@ public final class InetSocketTransportAddress implements TransportAddress {
 
     @Override
     public String getHost() {
-        return address.getHostName();
+       return getAddress(); // just delegate no resolving
     }
 
     @Override
     public String getAddress() {
-        return address.getAddress().getHostAddress();
+        return NetworkAddress.formatAddress(address.getAddress());
     }
 
     @Override
@@ -118,20 +100,16 @@ public final class InetSocketTransportAddress implements TransportAddress {
 
     @Override
     public void writeTo(StreamOutput out) throws IOException {
-        if (!resolveAddress && address.getAddress() != null) {
-            out.writeByte((byte) 0);
-            byte[] bytes = address().getAddress().getAddress();  // 4 bytes (IPv4) or 16 bytes (IPv6)
-            out.writeByte((byte) bytes.length); // 1 byte
-            out.write(bytes, 0, bytes.length);
-            if (address().getAddress() instanceof Inet6Address)
-                out.writeInt(((Inet6Address) address.getAddress()).getScopeId());
-        } else {
-            out.writeByte((byte) 1);
-            out.writeString(address.getHostName());
-        }
+        byte[] bytes = address().getAddress().getAddress();  // 4 bytes (IPv4) or 16 bytes (IPv6)
+        out.writeByte((byte) bytes.length); // 1 byte
+        out.write(bytes, 0, bytes.length);
+        // don't serialize scope ids over the network!!!!
+        // these only make sense with respect to the local machine, and will only formulate
+        // the address incorrectly remotely.
         out.writeInt(address.getPort());
     }
 
+
     @Override
     public boolean equals(Object o) {
         if (this == o) return true;
@@ -147,6 +125,6 @@ public final class InetSocketTransportAddress implements TransportAddress {
 
     @Override
     public String toString() {
-        return "inet[" + address + "]";
+        return NetworkAddress.format(address);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java b/core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java
index 3ac810d..48150c7 100644
--- a/core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java
+++ b/core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java
@@ -24,6 +24,7 @@ import com.carrotsearch.hppc.FloatArrayList;
 import com.carrotsearch.hppc.LongArrayList;
 import com.carrotsearch.hppc.ObjectArrayList;
 import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Iterators;
 import com.google.common.collect.Lists;
 import org.apache.lucene.util.*;
diff --git a/core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java b/core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java
index 4a5b3fc..1ec8eb7 100644
--- a/core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java
+++ b/core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java
@@ -187,7 +187,7 @@ public abstract class ExtensionPoint {
         protected final void bindExtensions(Binder binder) {
             Multibinder<T> allocationMultibinder = Multibinder.newSetBinder(binder, extensionClass);
             for (Class<? extends T> clazz : extensions) {
-                allocationMultibinder.addBinding().to(clazz);
+                allocationMultibinder.addBinding().to(clazz).asEagerSingleton();
             }
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/common/util/concurrent/AtomicArray.java b/core/src/main/java/org/elasticsearch/common/util/concurrent/AtomicArray.java
index 2278220..38953c5 100644
--- a/core/src/main/java/org/elasticsearch/common/util/concurrent/AtomicArray.java
+++ b/core/src/main/java/org/elasticsearch/common/util/concurrent/AtomicArray.java
@@ -19,10 +19,10 @@
 
 package org.elasticsearch.common.util.concurrent;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.ElasticsearchGenerationException;
 
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.concurrent.atomic.AtomicReferenceArray;
 
@@ -93,7 +93,7 @@ public class AtomicArray<E> {
     public List<Entry<E>> asList() {
         if (nonNullList == null) {
             if (array == null || array.length() == 0) {
-                nonNullList = Collections.emptyList();
+                nonNullList = ImmutableList.of();
             } else {
                 List<Entry<E>> list = new ArrayList<>(array.length());
                 for (int i = 0; i < array.length(); i++) {
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/support/filtering/FilteringJsonGenerator.java b/core/src/main/java/org/elasticsearch/common/xcontent/support/filtering/FilteringJsonGenerator.java
index b70a1ae..2748b4b 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/support/filtering/FilteringJsonGenerator.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/support/filtering/FilteringJsonGenerator.java
@@ -23,6 +23,7 @@ import com.fasterxml.jackson.core.Base64Variant;
 import com.fasterxml.jackson.core.JsonGenerator;
 import com.fasterxml.jackson.core.JsonParser;
 import com.fasterxml.jackson.core.SerializableString;
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.json.BaseJsonGenerator;
@@ -33,8 +34,6 @@ import java.io.OutputStream;
 import java.math.BigDecimal;
 import java.math.BigInteger;
 import java.util.ArrayDeque;
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.Queue;
 
@@ -62,7 +61,7 @@ public class FilteringJsonGenerator extends BaseJsonGenerator {
     public FilteringJsonGenerator(JsonGenerator generator, String[] filters) {
         super(generator);
 
-        List<String[]> builder = new ArrayList<>();
+        ImmutableList.Builder<String[]> builder = ImmutableList.builder();
         if (filters != null) {
             for (String filter : filters) {
                 String[] matcher = Strings.delimitedListToStringArray(filter, ".");
@@ -73,7 +72,7 @@ public class FilteringJsonGenerator extends BaseJsonGenerator {
         }
 
         // Creates a root context that matches all filtering rules
-        this.context = get(null, null, Collections.unmodifiableList(builder));
+        this.context = get(null, null, builder.build());
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/discovery/DiscoveryModule.java b/core/src/main/java/org/elasticsearch/discovery/DiscoveryModule.java
index 19d6966..1ab6087 100644
--- a/core/src/main/java/org/elasticsearch/discovery/DiscoveryModule.java
+++ b/core/src/main/java/org/elasticsearch/discovery/DiscoveryModule.java
@@ -19,18 +19,20 @@
 
 package org.elasticsearch.discovery;
 
-import com.google.common.collect.Lists;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.util.ExtensionPoint;
 import org.elasticsearch.discovery.local.LocalDiscovery;
 import org.elasticsearch.discovery.zen.ZenDiscovery;
 import org.elasticsearch.discovery.zen.elect.ElectMasterService;
+import org.elasticsearch.discovery.zen.ping.ZenPing;
 import org.elasticsearch.discovery.zen.ping.ZenPingService;
 import org.elasticsearch.discovery.zen.ping.unicast.UnicastHostsProvider;
+import org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing;
 
+import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -44,7 +46,8 @@ public class DiscoveryModule extends AbstractModule {
     public static final String ZEN_MASTER_SERVICE_TYPE_KEY = "discovery.zen.masterservice.type";
 
     private final Settings settings;
-    private final List<Class<? extends UnicastHostsProvider>> unicastHostProviders = Lists.newArrayList();
+    private final List<Class<? extends UnicastHostsProvider>> unicastHostProviders = new ArrayList<>();
+    private final ExtensionPoint.ClassSet<ZenPing> zenPings = new ExtensionPoint.ClassSet<>("zen_ping", ZenPing.class);
     private final Map<String, Class<? extends Discovery>> discoveryTypes = new HashMap<>();
     private final Map<String, Class<? extends ElectMasterService>> masterServiceType = new HashMap<>();
 
@@ -53,6 +56,8 @@ public class DiscoveryModule extends AbstractModule {
         addDiscoveryType("local", LocalDiscovery.class);
         addDiscoveryType("zen", ZenDiscovery.class);
         addElectMasterService("zen", ElectMasterService.class);
+        // always add the unicast hosts, or things get angry!
+        addZenPing(UnicastZenPing.class);
     }
 
     /**
@@ -82,6 +87,10 @@ public class DiscoveryModule extends AbstractModule {
         this.masterServiceType.put(type, masterService);
     }
 
+    public void addZenPing(Class<? extends ZenPing> clazz) {
+        zenPings.registerExtension(clazz);
+    }
+
     @Override
     protected void configure() {
         String defaultType = DiscoveryNode.localNode(settings) ? "local" : "zen";
@@ -107,6 +116,7 @@ public class DiscoveryModule extends AbstractModule {
             for (Class<? extends UnicastHostsProvider> unicastHostProvider : unicastHostProviders) {
                 unicastHostsProviderMultibinder.addBinding().to(unicastHostProvider);
             }
+            zenPings.bind(binder());
         }
         bind(Discovery.class).to(discoveryClass).asEagerSingleton();
         bind(DiscoveryService.class).asEagerSingleton();
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/ping/ZenPingService.java b/core/src/main/java/org/elasticsearch/discovery/zen/ping/ZenPingService.java
index 9d3fa1c..6933d8c 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/ping/ZenPingService.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/ping/ZenPingService.java
@@ -19,21 +19,11 @@
 
 package org.elasticsearch.discovery.zen.ping;
 
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.network.NetworkService;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;
-import org.elasticsearch.discovery.zen.elect.ElectMasterService;
-import org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing;
-import org.elasticsearch.discovery.zen.ping.unicast.UnicastHostsProvider;
-import org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
 
 import java.util.ArrayList;
 import java.util.Collections;
@@ -43,28 +33,17 @@ import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReference;
 
-/**
- *
- */
 public class ZenPingService extends AbstractLifecycleComponent<ZenPing> implements ZenPing {
 
-    private volatile List<? extends ZenPing> zenPings = Collections.emptyList();
+    private List<ZenPing> zenPings = Collections.emptyList();
 
     @Inject
-    public ZenPingService(Settings settings, ThreadPool threadPool, TransportService transportService, ClusterName clusterName, NetworkService networkService,
-                          Version version, ElectMasterService electMasterService, @Nullable Set<UnicastHostsProvider> unicastHostsProviders) {
+    public ZenPingService(Settings settings, Set<ZenPing> zenPings) {
         super(settings);
-        List<ZenPing> zenPingsBuilder = new ArrayList<>();
-        if (this.settings.getAsBoolean("discovery.zen.ping.multicast.enabled", true)) {
-            zenPingsBuilder.add(new MulticastZenPing(settings, threadPool, transportService, clusterName, networkService, version));
-        }
-        // always add the unicast hosts, so it will be able to receive unicast requests even when working in multicast
-        zenPingsBuilder.add(new UnicastZenPing(settings, threadPool, transportService, clusterName, version, electMasterService, unicastHostsProviders));
-
-        this.zenPings = Collections.unmodifiableList(zenPingsBuilder);
+        this.zenPings = Collections.unmodifiableList(new ArrayList<>(zenPings));
     }
 
-    public List<? extends ZenPing> zenPings() {
+    public List<ZenPing> zenPings() {
         return this.zenPings;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java b/core/src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java
deleted file mode 100644
index 26e3a6d..0000000
--- a/core/src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java
+++ /dev/null
@@ -1,565 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.zen.ping.multicast;
-
-import org.apache.lucene.util.Constants;
-import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.component.AbstractLifecycleComponent;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.network.MulticastChannel;
-import org.elasticsearch.common.network.NetworkService;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.util.concurrent.AbstractRunnable;
-import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.discovery.zen.ping.PingContextProvider;
-import org.elasticsearch.discovery.zen.ping.ZenPing;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.*;
-
-import java.io.IOException;
-import java.net.SocketAddress;
-import java.util.Map;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.atomic.AtomicReference;
-
-import static org.elasticsearch.cluster.node.DiscoveryNode.readNode;
-import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
-import static org.elasticsearch.common.util.concurrent.ConcurrentCollections.newConcurrentMap;
-
-/**
- *
- */
-public class MulticastZenPing extends AbstractLifecycleComponent<ZenPing> implements ZenPing {
-
-    public static final String ACTION_NAME = "internal:discovery/zen/multicast";
-
-    private static final byte[] INTERNAL_HEADER = new byte[]{1, 9, 8, 4};
-
-    private static final int PING_SIZE_ESTIMATE = 150;
-
-    private final String address;
-    private final int port;
-    private final String group;
-    private final int bufferSize;
-    private final int ttl;
-
-    private final ThreadPool threadPool;
-    private final TransportService transportService;
-    private final ClusterName clusterName;
-    private final NetworkService networkService;
-    private final Version version;
-    private volatile PingContextProvider contextProvider;
-
-    private final boolean pingEnabled;
-
-    private volatile MulticastChannel multicastChannel;
-
-    private final AtomicInteger pingIdGenerator = new AtomicInteger();
-    private final Map<Integer, PingCollection> receivedResponses = newConcurrentMap();
-
-    public MulticastZenPing(ThreadPool threadPool, TransportService transportService, ClusterName clusterName, Version version) {
-        this(EMPTY_SETTINGS, threadPool, transportService, clusterName, new NetworkService(EMPTY_SETTINGS), version);
-    }
-
-    public MulticastZenPing(Settings settings, ThreadPool threadPool, TransportService transportService, ClusterName clusterName, NetworkService networkService, Version version) {
-        super(settings);
-        this.threadPool = threadPool;
-        this.transportService = transportService;
-        this.clusterName = clusterName;
-        this.networkService = networkService;
-        this.version = version;
-
-        this.address = this.settings.get("discovery.zen.ping.multicast.address");
-        this.port = this.settings.getAsInt("discovery.zen.ping.multicast.port", 54328);
-        this.group = this.settings.get("discovery.zen.ping.multicast.group", "224.2.2.4");
-        this.bufferSize = this.settings.getAsInt("discovery.zen.ping.multicast.buffer_size", 2048);
-        this.ttl = this.settings.getAsInt("discovery.zen.ping.multicast.ttl", 3);
-
-        this.pingEnabled = this.settings.getAsBoolean("discovery.zen.ping.multicast.ping.enabled", true);
-
-        logger.debug("using group [{}], with port [{}], ttl [{}], and address [{}]", group, port, ttl, address);
-
-        this.transportService.registerRequestHandler(ACTION_NAME, MulticastPingResponse.class, ThreadPool.Names.SAME, new MulticastPingResponseRequestHandler());
-    }
-
-    @Override
-    public void setPingContextProvider(PingContextProvider nodesProvider) {
-        if (lifecycle.started()) {
-            throw new IllegalStateException("Can't set nodes provider when started");
-        }
-        this.contextProvider = nodesProvider;
-    }
-
-    @Override
-    protected void doStart() {
-        try {
-            // we know OSX has bugs in the JVM when creating multiple instances of multicast sockets
-            // causing for "socket close" exceptions when receive and/or crashes
-            boolean shared = settings.getAsBoolean("discovery.zen.ping.multicast.shared", Constants.MAC_OS_X);
-            // OSX does not correctly send multicasts FROM the right interface
-            boolean deferToInterface = settings.getAsBoolean("discovery.zen.ping.multicast.defer_group_to_set_interface", Constants.MAC_OS_X);
-            multicastChannel = MulticastChannel.getChannel(nodeName(), shared,
-                    new MulticastChannel.Config(port, group, bufferSize, ttl,
-                            // don't use publish address, the use case for that is e.g. a firewall or proxy and
-                            // may not even be bound to an interface on this machine! use the first bound address.
-                            networkService.resolveBindHostAddress(address)[0],
-                            deferToInterface),
-                    new Receiver());
-        } catch (Throwable t) {
-            String msg = "multicast failed to start [{}], disabling. Consider using IPv4 only (by defining env. variable `ES_USE_IPV4`)";
-            if (logger.isDebugEnabled()) {
-                logger.debug(msg, t, ExceptionsHelper.detailedMessage(t));
-            } else {
-                logger.info(msg, ExceptionsHelper.detailedMessage(t));
-            }
-        }
-    }
-
-    @Override
-    protected void doStop() {
-        if (multicastChannel != null) {
-            multicastChannel.close();
-            multicastChannel = null;
-        }
-    }
-
-    @Override
-    protected void doClose() {
-    }
-
-    public PingResponse[] pingAndWait(TimeValue timeout) {
-        final AtomicReference<PingResponse[]> response = new AtomicReference<>();
-        final CountDownLatch latch = new CountDownLatch(1);
-        try {
-            ping(new PingListener() {
-                @Override
-                public void onPing(PingResponse[] pings) {
-                    response.set(pings);
-                    latch.countDown();
-                }
-            }, timeout);
-        } catch (EsRejectedExecutionException ex) {
-            logger.debug("Ping execution rejected", ex);
-            return PingResponse.EMPTY;
-        }
-        try {
-            latch.await();
-            return response.get();
-        } catch (InterruptedException e) {
-            Thread.currentThread().interrupt();
-            return PingResponse.EMPTY;
-        }
-    }
-
-    @Override
-    public void ping(final PingListener listener, final TimeValue timeout) {
-        if (!pingEnabled || multicastChannel == null) {
-            threadPool.generic().execute(new Runnable() {
-                @Override
-                public void run() {
-                    listener.onPing(PingResponse.EMPTY);
-                }
-            });
-            return;
-        }
-        final int id = pingIdGenerator.incrementAndGet();
-        try {
-            receivedResponses.put(id, new PingCollection());
-            sendPingRequest(id);
-            // try and send another ping request halfway through (just in case someone woke up during it...)
-            // this can be a good trade-off to nailing the initial lookup or un-delivered messages
-            threadPool.schedule(TimeValue.timeValueMillis(timeout.millis() / 2), ThreadPool.Names.GENERIC, new AbstractRunnable() {
-                @Override
-                public void onFailure(Throwable t) {
-                    logger.warn("[{}] failed to send second ping request", t, id);
-                    finalizePingCycle(id, listener);
-                }
-
-                @Override
-                public void doRun() {
-                    sendPingRequest(id);
-                    threadPool.schedule(TimeValue.timeValueMillis(timeout.millis() / 2), ThreadPool.Names.GENERIC, new AbstractRunnable() {
-                        @Override
-                        public void onFailure(Throwable t) {
-                            logger.warn("[{}] failed to send third ping request", t, id);
-                            finalizePingCycle(id, listener);
-                        }
-
-                        @Override
-                        public void doRun() {
-                            // make one last ping, but finalize as soon as all nodes have responded or a timeout has past
-                            PingCollection collection = receivedResponses.get(id);
-                            FinalizingPingCollection finalizingPingCollection = new FinalizingPingCollection(id, collection, collection.size(), listener);
-                            receivedResponses.put(id, finalizingPingCollection);
-                            logger.trace("[{}] sending last pings", id);
-                            sendPingRequest(id);
-                            threadPool.schedule(TimeValue.timeValueMillis(timeout.millis() / 4), ThreadPool.Names.GENERIC, new AbstractRunnable() {
-                                @Override
-                                public void onFailure(Throwable t) {
-                                    logger.warn("[{}] failed to finalize ping", t, id);
-                                }
-
-                                @Override
-                                protected void doRun() throws Exception {
-                                    finalizePingCycle(id, listener);
-                                }
-                            });
-                        }
-                    });
-                }
-            });
-        } catch (Exception e) {
-            logger.warn("failed to ping", e);
-            finalizePingCycle(id, listener);
-        }
-    }
-
-    /**
-     * takes all pings collected for a given id and pass them to the given listener.
-     * this method is safe to call multiple times as is guaranteed to only finalize once.
-     */
-    private void finalizePingCycle(int id, final PingListener listener) {
-        PingCollection responses = receivedResponses.remove(id);
-        if (responses != null) {
-            listener.onPing(responses.toArray());
-        }
-    }
-
-    private void sendPingRequest(int id) {
-        try {
-            BytesStreamOutput out = new BytesStreamOutput(PING_SIZE_ESTIMATE);
-            out.writeBytes(INTERNAL_HEADER);
-            // TODO: change to min_required version!
-            Version.writeVersion(version, out);
-            out.writeInt(id);
-            clusterName.writeTo(out);
-            contextProvider.nodes().localNode().writeTo(out);
-            out.close();
-            multicastChannel.send(out.bytes());
-            if (logger.isTraceEnabled()) {
-                logger.trace("[{}] sending ping request", id);
-            }
-        } catch (Exception e) {
-            if (lifecycle.stoppedOrClosed()) {
-                return;
-            }
-            if (logger.isDebugEnabled()) {
-                logger.debug("failed to send multicast ping request", e);
-            } else {
-                logger.warn("failed to send multicast ping request: {}", ExceptionsHelper.detailedMessage(e));
-            }
-        }
-    }
-
-    class FinalizingPingCollection extends PingCollection {
-        final private PingCollection internalCollection;
-        final private int expectedResponses;
-        final private AtomicInteger responseCount;
-        final private PingListener listener;
-        final private int id;
-
-        public FinalizingPingCollection(int id, PingCollection internalCollection, int expectedResponses, PingListener listener) {
-            this.id = id;
-            this.internalCollection = internalCollection;
-            this.expectedResponses = expectedResponses;
-            this.responseCount = new AtomicInteger();
-            this.listener = listener;
-        }
-
-        @Override
-        public synchronized boolean addPing(PingResponse ping) {
-            if (internalCollection.addPing(ping)) {
-                if (responseCount.incrementAndGet() >= expectedResponses) {
-                    logger.trace("[{}] all nodes responded", id);
-                    finish();
-                }
-                return true;
-            }
-            return false;
-        }
-
-        @Override
-        public synchronized void addPings(PingResponse[] pings) {
-            internalCollection.addPings(pings);
-        }
-
-        @Override
-        public synchronized PingResponse[] toArray() {
-            return internalCollection.toArray();
-        }
-
-        void finish() {
-            // spawn another thread as we may be running on a network thread
-            threadPool.generic().execute(new AbstractRunnable() {
-                @Override
-                public void onFailure(Throwable t) {
-                    logger.error("failed to call ping listener", t);
-                }
-
-                @Override
-                protected void doRun() throws Exception {
-                    finalizePingCycle(id, listener);
-                }
-            });
-        }
-    }
-
-    class MulticastPingResponseRequestHandler implements TransportRequestHandler<MulticastPingResponse> {
-        @Override
-        public void messageReceived(MulticastPingResponse request, TransportChannel channel) throws Exception {
-            if (logger.isTraceEnabled()) {
-                logger.trace("[{}] received {}", request.id, request.pingResponse);
-            }
-            PingCollection responses = receivedResponses.get(request.id);
-            if (responses == null) {
-                logger.warn("received ping response {} with no matching id [{}]", request.pingResponse, request.id);
-            } else {
-                responses.addPing(request.pingResponse);
-            }
-            channel.sendResponse(TransportResponse.Empty.INSTANCE);
-        }
-    }
-
-    static class MulticastPingResponse extends TransportRequest {
-
-        int id;
-
-        PingResponse pingResponse;
-
-        MulticastPingResponse() {
-        }
-
-        @Override
-        public void readFrom(StreamInput in) throws IOException {
-            super.readFrom(in);
-            id = in.readInt();
-            pingResponse = PingResponse.readPingResponse(in);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            super.writeTo(out);
-            out.writeInt(id);
-            pingResponse.writeTo(out);
-        }
-    }
-
-
-    private class Receiver implements MulticastChannel.Listener {
-
-        @Override
-        public void onMessage(BytesReference data, SocketAddress address) {
-            int id = -1;
-            DiscoveryNode requestingNodeX = null;
-            ClusterName clusterName = null;
-
-            Map<String, Object> externalPingData = null;
-            XContentType xContentType = null;
-
-            try {
-                boolean internal = false;
-                if (data.length() > 4) {
-                    int counter = 0;
-                    for (; counter < INTERNAL_HEADER.length; counter++) {
-                        if (data.get(counter) != INTERNAL_HEADER[counter]) {
-                            break;
-                        }
-                    }
-                    if (counter == INTERNAL_HEADER.length) {
-                        internal = true;
-                    }
-                }
-                if (internal) {
-                    StreamInput input = StreamInput.wrap(new BytesArray(data.toBytes(), INTERNAL_HEADER.length, data.length() - INTERNAL_HEADER.length));
-                    Version version = Version.readVersion(input);
-                    input.setVersion(version);
-                    id = input.readInt();
-                    clusterName = ClusterName.readClusterName(input);
-                    requestingNodeX = readNode(input);
-                } else {
-                    xContentType = XContentFactory.xContentType(data);
-                    if (xContentType != null) {
-                        // an external ping
-                        try (XContentParser parser = XContentFactory.xContent(xContentType).createParser(data)) {
-                            externalPingData = parser.map();
-                        }
-                    } else {
-                        throw new IllegalStateException("failed multicast message, probably message from previous version");
-                    }
-                }
-                if (externalPingData != null) {
-                    handleExternalPingRequest(externalPingData, xContentType, address);
-                } else {
-                    handleNodePingRequest(id, requestingNodeX, clusterName);
-                }
-            } catch (Exception e) {
-                if (!lifecycle.started() || (e instanceof EsRejectedExecutionException)) {
-                    logger.debug("failed to read requesting data from {}", e, address);
-                } else {
-                    logger.warn("failed to read requesting data from {}", e, address);
-                }
-            }
-        }
-
-        @SuppressWarnings("unchecked")
-        private void handleExternalPingRequest(Map<String, Object> externalPingData, XContentType contentType, SocketAddress remoteAddress) {
-            if (externalPingData.containsKey("response")) {
-                // ignoring responses sent over the multicast channel
-                logger.trace("got an external ping response (ignoring) from {}, content {}", remoteAddress, externalPingData);
-                return;
-            }
-
-            if (multicastChannel == null) {
-                logger.debug("can't send ping response, no socket, from {}, content {}", remoteAddress, externalPingData);
-                return;
-            }
-
-            Map<String, Object> request = (Map<String, Object>) externalPingData.get("request");
-            if (request == null) {
-                logger.warn("malformed external ping request, no 'request' element from {}, content {}", remoteAddress, externalPingData);
-                return;
-            }
-
-            final String requestClusterName = request.containsKey("cluster_name") ? request.get("cluster_name").toString() : request.containsKey("clusterName") ? request.get("clusterName").toString() : null;
-            if (requestClusterName == null) {
-                logger.warn("malformed external ping request, missing 'cluster_name' element within request, from {}, content {}", remoteAddress, externalPingData);
-                return;
-            }
-
-            if (!requestClusterName.equals(clusterName.value())) {
-                logger.trace("got request for cluster_name {}, but our cluster_name is {}, from {}, content {}",
-                        requestClusterName, clusterName.value(), remoteAddress, externalPingData);
-                return;
-            }
-            if (logger.isTraceEnabled()) {
-                logger.trace("got external ping request from {}, content {}", remoteAddress, externalPingData);
-            }
-
-            try {
-                DiscoveryNode localNode = contextProvider.nodes().localNode();
-
-                XContentBuilder builder = XContentFactory.contentBuilder(contentType);
-                builder.startObject().startObject("response");
-                builder.field("cluster_name", clusterName.value());
-                builder.startObject("version").field("number", version.number()).field("snapshot_build", version.snapshot).endObject();
-                builder.field("transport_address", localNode.address().toString());
-
-                if (contextProvider.nodeService() != null) {
-                    for (Map.Entry<String, String> attr : contextProvider.nodeService().attributes().entrySet()) {
-                        builder.field(attr.getKey(), attr.getValue());
-                    }
-                }
-
-                builder.startObject("attributes");
-                for (Map.Entry<String, String> attr : localNode.attributes().entrySet()) {
-                    builder.field(attr.getKey(), attr.getValue());
-                }
-                builder.endObject();
-
-                builder.endObject().endObject();
-                multicastChannel.send(builder.bytes());
-                if (logger.isTraceEnabled()) {
-                    logger.trace("sending external ping response {}", builder.string());
-                }
-            } catch (Exception e) {
-                logger.warn("failed to send external multicast response", e);
-            }
-        }
-
-        private void handleNodePingRequest(int id, DiscoveryNode requestingNodeX, ClusterName requestClusterName) {
-            if (!pingEnabled || multicastChannel == null) {
-                return;
-            }
-            final DiscoveryNodes discoveryNodes = contextProvider.nodes();
-            final DiscoveryNode requestingNode = requestingNodeX;
-            if (requestingNode.id().equals(discoveryNodes.localNodeId())) {
-                // that's me, ignore
-                return;
-            }
-            if (!requestClusterName.equals(clusterName)) {
-                if (logger.isTraceEnabled()) {
-                    logger.trace("[{}] received ping_request from [{}], but wrong cluster_name [{}], expected [{}], ignoring",
-                            id, requestingNode, requestClusterName.value(), clusterName.value());
-                }
-                return;
-            }
-            // don't connect between two client nodes, no need for that...
-            if (!discoveryNodes.localNode().shouldConnectTo(requestingNode)) {
-                if (logger.isTraceEnabled()) {
-                    logger.trace("[{}] received ping_request from [{}], both are client nodes, ignoring", id, requestingNode, requestClusterName);
-                }
-                return;
-            }
-            final MulticastPingResponse multicastPingResponse = new MulticastPingResponse();
-            multicastPingResponse.id = id;
-            multicastPingResponse.pingResponse = new PingResponse(discoveryNodes.localNode(), discoveryNodes.masterNode(), clusterName, contextProvider.nodeHasJoinedClusterOnce());
-
-            if (logger.isTraceEnabled()) {
-                logger.trace("[{}] received ping_request from [{}], sending {}", id, requestingNode, multicastPingResponse.pingResponse);
-            }
-
-            if (!transportService.nodeConnected(requestingNode)) {
-                // do the connect and send on a thread pool
-                threadPool.generic().execute(new Runnable() {
-                    @Override
-                    public void run() {
-                        // connect to the node if possible
-                        try {
-                            transportService.connectToNode(requestingNode);
-                            transportService.sendRequest(requestingNode, ACTION_NAME, multicastPingResponse, new EmptyTransportResponseHandler(ThreadPool.Names.SAME) {
-                                @Override
-                                public void handleException(TransportException exp) {
-                                    logger.warn("failed to receive confirmation on sent ping response to [{}]", exp, requestingNode);
-                                }
-                            });
-                        } catch (Exception e) {
-                            if (lifecycle.started()) {
-                                logger.warn("failed to connect to requesting node {}", e, requestingNode);
-                            }
-                        }
-                    }
-                });
-            } else {
-                transportService.sendRequest(requestingNode, ACTION_NAME, multicastPingResponse, new EmptyTransportResponseHandler(ThreadPool.Names.SAME) {
-                    @Override
-                    public void handleException(TransportException exp) {
-                        if (lifecycle.started()) {
-                            logger.warn("failed to receive confirmation on sent ping response to [{}]", exp, requestingNode);
-                        }
-                    }
-                });
-            }
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java b/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java
index b16b616..06820a9 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java
@@ -29,6 +29,7 @@ import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
+import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.settings.Settings;
@@ -63,8 +64,12 @@ import static org.elasticsearch.discovery.zen.ping.ZenPing.PingResponse.readPing
 public class UnicastZenPing extends AbstractLifecycleComponent<ZenPing> implements ZenPing {
 
     public static final String ACTION_NAME = "internal:discovery/zen/unicast";
+    public static final String DISCOVERY_ZEN_PING_UNICAST_HOSTS = "discovery.zen.ping.unicast.hosts";
+
+    // these limits are per-address
+    public static final int LIMIT_FOREIGN_PORTS_COUNT = 1;
+    public static final int LIMIT_LOCAL_PORTS_COUNT = 5;
 
-    public static final int LIMIT_PORTS_COUNT = 1;
 
     private final ThreadPool threadPool;
     private final TransportService transportService;
@@ -96,6 +101,7 @@ public class UnicastZenPing extends AbstractLifecycleComponent<ZenPing> implemen
 
     private volatile boolean closed = false;
 
+    @Inject
     public UnicastZenPing(Settings settings, ThreadPool threadPool, TransportService transportService, ClusterName clusterName,
                           Version version, ElectMasterService electMasterService, @Nullable Set<UnicastHostsProvider> unicastHostsProviders) {
         super(settings);
@@ -111,21 +117,30 @@ public class UnicastZenPing extends AbstractLifecycleComponent<ZenPing> implemen
         }
 
         this.concurrentConnects = this.settings.getAsInt("discovery.zen.ping.unicast.concurrent_connects", 10);
-        String[] hostArr = this.settings.getAsArray("discovery.zen.ping.unicast.hosts");
+        String[] hostArr = this.settings.getAsArray(DISCOVERY_ZEN_PING_UNICAST_HOSTS);
         // trim the hosts
         for (int i = 0; i < hostArr.length; i++) {
             hostArr[i] = hostArr[i].trim();
         }
         List<String> hosts = Lists.newArrayList(hostArr);
+        final int limitPortCounts;
+        if (hosts.isEmpty()) {
+            // if unicast hosts are not specified, fill with simple defaults on the local machine
+            limitPortCounts = LIMIT_LOCAL_PORTS_COUNT;
+            hosts.addAll(transportService.getLocalAddresses());
+        } else {
+            // we only limit to 1 addresses, makes no sense to ping 100 ports
+            limitPortCounts = LIMIT_FOREIGN_PORTS_COUNT;
+        }
+
         logger.debug("using initial hosts {}, with concurrent_connects [{}]", hosts, concurrentConnects);
 
         List<DiscoveryNode> configuredTargetNodes = Lists.newArrayList();
         for (String host : hosts) {
             try {
-                TransportAddress[] addresses = transportService.addressesFromString(host);
-                // we only limit to 1 addresses, makes no sense to ping 100 ports
-                for (int i = 0; (i < addresses.length && i < LIMIT_PORTS_COUNT); i++) {
-                    configuredTargetNodes.add(new DiscoveryNode(UNICAST_NODE_PREFIX + unicastNodeIdGenerator.incrementAndGet() + "#", addresses[i], version.minimumCompatibilityVersion()));
+                TransportAddress[] addresses = transportService.addressesFromString(host, limitPortCounts);
+                for (TransportAddress address : addresses) {
+                    configuredTargetNodes.add(new DiscoveryNode(UNICAST_NODE_PREFIX + unicastNodeIdGenerator.incrementAndGet() + "#", address, version.minimumCompatibilityVersion()));
                 }
             } catch (Exception e) {
                 throw new IllegalArgumentException("Failed to resolve address for [" + host + "]", e);
diff --git a/core/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java b/core/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java
index 983fcf7..869f2dc 100644
--- a/core/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java
+++ b/core/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java
@@ -28,8 +28,6 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.env.NodeEnvironment;
 
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Map;
 import java.util.Set;
 
@@ -141,7 +139,7 @@ public class DanglingIndicesState extends AbstractComponent {
             return;
         }
         try {
-            allocateDangledIndices.allocateDangled(Collections.unmodifiableCollection(danglingIndices.values()), new LocalAllocateDangledIndices.Listener() {
+            allocateDangledIndices.allocateDangled(ImmutableList.copyOf(danglingIndices.values()), new LocalAllocateDangledIndices.Listener() {
                 @Override
                 public void onResponse(LocalAllocateDangledIndices.AllocateDangledResponse response) {
                     logger.trace("allocated dangled");
diff --git a/core/src/main/java/org/elasticsearch/gateway/MetaDataStateFormat.java b/core/src/main/java/org/elasticsearch/gateway/MetaDataStateFormat.java
index 523e9bc..23546f1 100644
--- a/core/src/main/java/org/elasticsearch/gateway/MetaDataStateFormat.java
+++ b/core/src/main/java/org/elasticsearch/gateway/MetaDataStateFormat.java
@@ -43,12 +43,10 @@ import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
 
+import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.OutputStream;
-import java.nio.file.DirectoryStream;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.nio.file.StandardCopyOption;
+import java.nio.file.*;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.regex.Matcher;
@@ -253,10 +251,9 @@ public abstract class MetaDataStateFormat<T> {
         if (dataLocations != null) { // select all eligable files first
             for (Path dataLocation : dataLocations) {
                 final Path stateDir = dataLocation.resolve(STATE_DIR_NAME);
-                if (!Files.isDirectory(stateDir)) {
-                    continue;
-                }
                 // now, iterate over the current versions, and find latest one
+                // we don't check if the stateDir is present since it could be deleted
+                // after the check. Also if there is a _state file and it's not a dir something is really wrong
                 try (DirectoryStream<Path> paths = Files.newDirectoryStream(stateDir)) { // we don't pass a glob since we need the group part for parsing
                     for (Path stateFile : paths) {
                         final Matcher matcher = stateFilePattern.matcher(stateFile.getFileName().toString());
@@ -270,6 +267,8 @@ public abstract class MetaDataStateFormat<T> {
                             files.add(pav);
                         }
                     }
+                } catch (NoSuchFileException | FileNotFoundException ex) {
+                    // no _state directory -- move on
                 }
             }
         }
diff --git a/core/src/main/java/org/elasticsearch/gateway/PrimaryShardAllocator.java b/core/src/main/java/org/elasticsearch/gateway/PrimaryShardAllocator.java
index 0dc7de4..b4bf636 100644
--- a/core/src/main/java/org/elasticsearch/gateway/PrimaryShardAllocator.java
+++ b/core/src/main/java/org/elasticsearch/gateway/PrimaryShardAllocator.java
@@ -94,12 +94,12 @@ public abstract class PrimaryShardAllocator extends AbstractComponent {
                 DiscoveryNode node = nodesToAllocate.yesNodes.get(0);
                 logger.debug("[{}][{}]: allocating [{}] to [{}] on primary allocation", shard.index(), shard.id(), shard, node);
                 changed = true;
-                unassignedIterator.initialize(node.id(), nodesAndVersions.highestVersion);
+                unassignedIterator.initialize(node.id(), nodesAndVersions.highestVersion, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE);
             } else if (nodesToAllocate.throttleNodes.isEmpty() == true && nodesToAllocate.noNodes.isEmpty() == false) {
                 DiscoveryNode node = nodesToAllocate.noNodes.get(0);
                 logger.debug("[{}][{}]: forcing allocating [{}] to [{}] on primary allocation", shard.index(), shard.id(), shard, node);
                 changed = true;
-                unassignedIterator.initialize(node.id(), nodesAndVersions.highestVersion);
+                unassignedIterator.initialize(node.id(), nodesAndVersions.highestVersion, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE);
             } else {
                 // we are throttling this, but we have enough to allocate to this node, ignore it for now
                 logger.debug("[{}][{}]: throttling allocation [{}] to [{}] on primary allocation", shard.index(), shard.id(), shard, nodesToAllocate.throttleNodes);
diff --git a/core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java b/core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java
index 9af9a4e..03772f7 100644
--- a/core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java
+++ b/core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java
@@ -169,7 +169,7 @@ public abstract class ReplicaShardAllocator extends AbstractComponent {
                     logger.debug("[{}][{}]: allocating [{}] to [{}] in order to reuse its unallocated persistent store", shard.index(), shard.id(), shard, nodeWithHighestMatch.node());
                     // we found a match
                     changed = true;
-                    unassignedIterator.initialize(nodeWithHighestMatch.nodeId());
+                    unassignedIterator.initialize(nodeWithHighestMatch.nodeId(), shard.version(), allocation.clusterInfo().getShardSize(shard, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));
                 }
             } else if (matchingNodes.hasAnyData() == false) {
                 // if we didn't manage to find *any* data (regardless of matching sizes), check if the allocation
diff --git a/core/src/main/java/org/elasticsearch/http/HttpServer.java b/core/src/main/java/org/elasticsearch/http/HttpServer.java
index 40067e2..0fab142 100644
--- a/core/src/main/java/org/elasticsearch/http/HttpServer.java
+++ b/core/src/main/java/org/elasticsearch/http/HttpServer.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.http;
 
 import com.google.common.collect.ImmutableMap;
+import com.google.common.io.ByteStreams;
 
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
@@ -30,6 +31,7 @@ import org.elasticsearch.node.service.NodeService;
 import org.elasticsearch.rest.*;
 
 import java.io.IOException;
+import java.io.InputStream;
 import java.nio.file.*;
 import java.nio.file.attribute.BasicFileAttributes;
 import java.util.HashMap;
@@ -114,10 +116,14 @@ public class HttpServer extends AbstractLifecycleComponent<HttpServer> {
     }
 
     public void internalDispatchRequest(final HttpRequest request, final HttpChannel channel) {
-        if (request.rawPath().startsWith("/_plugin/")) {
+        String rawPath = request.rawPath();
+        if (rawPath.startsWith("/_plugin/")) {
             RestFilterChain filterChain = restController.filterChain(pluginSiteFilter);
             filterChain.continueProcessing(request, channel);
             return;
+        } else if (rawPath.equals("/favicon.ico")) {
+            handleFavicon(request, channel);
+            return;
         }
         restController.dispatchRequest(request, channel);
     }
@@ -131,6 +137,22 @@ public class HttpServer extends AbstractLifecycleComponent<HttpServer> {
         }
     }
 
+    void handleFavicon(HttpRequest request, HttpChannel channel) {
+        if (request.method() == RestRequest.Method.GET) {
+            try {
+                try (InputStream stream = getClass().getResourceAsStream("/config/favicon.ico")) {
+                    byte[] content = ByteStreams.toByteArray(stream);
+                    BytesRestResponse restResponse = new BytesRestResponse(RestStatus.OK, "image/x-icon", content);
+                    channel.sendResponse(restResponse);
+                }
+            } catch (IOException e) {
+                channel.sendResponse(new BytesRestResponse(INTERNAL_SERVER_ERROR));
+            }
+        } else {
+            channel.sendResponse(new BytesRestResponse(FORBIDDEN));
+        }
+    }
+
     void handlePluginSite(HttpRequest request, HttpChannel channel) throws IOException {
         if (disableSites) {
             channel.sendResponse(new BytesRestResponse(FORBIDDEN));
diff --git a/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java b/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java
index 664f7a8..fa0c1a9 100644
--- a/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java
+++ b/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java
@@ -24,6 +24,7 @@ import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.netty.NettyUtils;
 import org.elasticsearch.common.netty.OpenChannelsHandler;
+import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.common.network.NetworkService;
 import org.elasticsearch.common.network.NetworkUtils;
 import org.elasticsearch.common.settings.Settings;
@@ -274,7 +275,7 @@ public class NettyHttpServerTransport extends AbstractLifecycleComponent<HttpSer
     private void bindAddress(final InetAddress hostAddress) {
         PortsRange portsRange = new PortsRange(port);
         final AtomicReference<Exception> lastException = new AtomicReference<>();
-        final AtomicReference<SocketAddress> boundSocket = new AtomicReference<>();
+        final AtomicReference<InetSocketAddress> boundSocket = new AtomicReference<>();
         boolean success = portsRange.iterate(new PortsRange.PortCallback() {
             @Override
             public boolean onPortNumber(int portNumber) {
@@ -282,7 +283,7 @@ public class NettyHttpServerTransport extends AbstractLifecycleComponent<HttpSer
                     synchronized (serverChannels) {
                         Channel channel = serverBootstrap.bind(new InetSocketAddress(hostAddress, portNumber));
                         serverChannels.add(channel);
-                        boundSocket.set(channel.getLocalAddress());
+                        boundSocket.set((InetSocketAddress) channel.getLocalAddress());
                     }
                 } catch (Exception e) {
                     lastException.set(e);
@@ -294,7 +295,7 @@ public class NettyHttpServerTransport extends AbstractLifecycleComponent<HttpSer
         if (!success) {
             throw new BindHttpException("Failed to bind to [" + port + "]", lastException.get());
         }
-        logger.info("Bound http to address [{}]", boundSocket.get());
+        logger.info("Bound http to address {{}}", NetworkAddress.format(boundSocket.get()));
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/index/IndexService.java b/core/src/main/java/org/elasticsearch/index/IndexService.java
index be7b8e5..7be71c8 100644
--- a/core/src/main/java/org/elasticsearch/index/IndexService.java
+++ b/core/src/main/java/org/elasticsearch/index/IndexService.java
@@ -26,6 +26,7 @@ import com.google.common.collect.Iterators;
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.collect.Tuple;
@@ -55,7 +56,6 @@ import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.indices.InternalIndicesLifecycle;
 import org.elasticsearch.indices.cache.query.IndicesQueryCache;
 import org.elasticsearch.plugins.PluginsService;
-import org.elasticsearch.plugins.ShardsPluginsModule;
 
 import java.io.Closeable;
 import java.io.IOException;
@@ -270,7 +270,8 @@ public class IndexService extends AbstractIndexComponent implements IndexCompone
         }
     }
 
-    public synchronized IndexShard createShard(int sShardId, boolean primary) {
+    public synchronized IndexShard createShard(int sShardId, ShardRouting routing) {
+        final boolean primary = routing.primary();
         /*
          * TODO: we execute this in parallel but it's a synced method. Yet, we might
          * be able to serialize the execution via the cluster state in the future. for now we just
@@ -299,7 +300,7 @@ public class IndexService extends AbstractIndexComponent implements IndexCompone
                 }
             }
             if (path == null) {
-                path = ShardPath.selectNewPathForShard(nodeEnv, shardId, indexSettings, getAvgShardSizeInBytes(), this);
+                path = ShardPath.selectNewPathForShard(nodeEnv, shardId, indexSettings, routing.getExpectedShardSize() == ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE ? getAvgShardSizeInBytes() : routing.getExpectedShardSize(), this);
                 logger.debug("{} creating using a new path [{}]", shardId, path);
             } else {
                 logger.debug("{} creating using an existing path [{}]", shardId, path);
@@ -315,7 +316,10 @@ public class IndexService extends AbstractIndexComponent implements IndexCompone
             final boolean canDeleteShardContent = IndexMetaData.isOnSharedFilesystem(indexSettings) == false ||
                     (primary && IndexMetaData.isOnSharedFilesystem(indexSettings));
             ModulesBuilder modules = new ModulesBuilder();
-            modules.add(new ShardsPluginsModule(indexSettings, pluginsService));
+            // plugin modules must be added here, before others or we can get crazy injection errors...
+            for (Module pluginModule : pluginsService.shardModules(indexSettings)) {
+                modules.add(pluginModule);
+            }
             modules.add(new IndexShardModule(shardId, primary, indexSettings));
             modules.add(new StoreModule(injector.getInstance(IndexStore.class).shardDirectory(), lock,
                     new StoreCloseListener(shardId, canDeleteShardContent,  new Closeable() {
@@ -325,6 +329,9 @@ public class IndexService extends AbstractIndexComponent implements IndexCompone
                         }
                     }), path));
             modules.add(new DeletionPolicyModule());
+
+            pluginsService.processModules(modules);
+
             try {
                 shardInjector = modules.createChildInjector(injector);
             } catch (CreationException e) {
diff --git a/core/src/main/java/org/elasticsearch/index/fielddata/ScriptDocValues.java b/core/src/main/java/org/elasticsearch/index/fielddata/ScriptDocValues.java
index da12065..254b8b3 100644
--- a/core/src/main/java/org/elasticsearch/index/fielddata/ScriptDocValues.java
+++ b/core/src/main/java/org/elasticsearch/index/fielddata/ScriptDocValues.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.fielddata;
 
+import com.google.common.collect.ImmutableList;
 
 import org.apache.lucene.index.SortedNumericDocValues;
 import org.apache.lucene.util.BytesRef;
@@ -29,7 +30,6 @@ import org.joda.time.DateTimeZone;
 import org.joda.time.MutableDateTime;
 
 import java.util.AbstractList;
-import java.util.Collections;
 import java.util.List;
 
 
@@ -85,7 +85,7 @@ public interface ScriptDocValues<T> extends List<T> {
 
         @Override
         public List<String> getValues() {
-            return Collections.unmodifiableList(this);
+            return ImmutableList.copyOf(this);
         }
 
         @Override
@@ -128,7 +128,7 @@ public interface ScriptDocValues<T> extends List<T> {
 
         @Override
         public List<Long> getValues() {
-            return Collections.unmodifiableList(this);
+            return ImmutableList.copyOf(this);
         }
 
         public MutableDateTime getDate() {
@@ -175,7 +175,7 @@ public interface ScriptDocValues<T> extends List<T> {
 
         @Override
         public List<Double> getValues() {
-            return Collections.unmodifiableList(this);
+            return ImmutableList.copyOf(this);
         }
 
         @Override
@@ -238,7 +238,7 @@ public interface ScriptDocValues<T> extends List<T> {
 
         @Override
         public List<GeoPoint> getValues() {
-            return Collections.unmodifiableList(this);
+            return ImmutableList.copyOf(this);
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/index/get/ShardGetService.java b/core/src/main/java/org/elasticsearch/index/get/ShardGetService.java
index 8331e0a..632284d 100644
--- a/core/src/main/java/org/elasticsearch/index/get/ShardGetService.java
+++ b/core/src/main/java/org/elasticsearch/index/get/ShardGetService.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.get;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Sets;
 
 import org.apache.lucene.index.Term;
@@ -48,7 +49,6 @@ import org.elasticsearch.search.lookup.SearchLookup;
 
 import java.io.IOException;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -258,7 +258,7 @@ public final class ShardGetService extends AbstractIndexShardComponent {
                         if (value instanceof List) {
                             fields.put(field, new GetField(field, (List) value));
                         } else {
-                            fields.put(field, new GetField(field, Collections.singletonList(value)));
+                            fields.put(field, new GetField(field, ImmutableList.of(value)));
                         }
                     }
                 }
@@ -383,7 +383,7 @@ public final class ShardGetService extends AbstractIndexShardComponent {
                     if (value instanceof List) {
                         fields.put(field, new GetField(field, (List) value));
                     } else {
-                        fields.put(field, new GetField(field, Collections.singletonList(value)));
+                        fields.put(field, new GetField(field, ImmutableList.of(value)));
                     }
                 }
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java
index 2d2ac6c..5f6893e 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java
@@ -22,6 +22,7 @@ package org.elasticsearch.index.mapper;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
 import com.google.common.base.Function;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Iterators;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
@@ -694,9 +695,9 @@ public abstract class FieldMapper extends Mapper {
      */
     public static class CopyTo {
 
-        private final List<String> copyToFields;
+        private final ImmutableList<String> copyToFields;
 
-        private CopyTo(List<String> copyToFields) {
+        private CopyTo(ImmutableList<String> copyToFields) {
             this.copyToFields = copyToFields;
         }
 
@@ -712,7 +713,7 @@ public abstract class FieldMapper extends Mapper {
         }
 
         public static class Builder {
-            private final List<String> copyToBuilders = new ArrayList<>();
+            private final ImmutableList.Builder<String> copyToBuilders = ImmutableList.builder();
 
             public Builder add(String field) {
                 copyToBuilders.add(field);
@@ -720,7 +721,7 @@ public abstract class FieldMapper extends Mapper {
             }
 
             public CopyTo build() {
-                return new CopyTo(Collections.unmodifiableList(copyToBuilders));
+                return new CopyTo(copyToBuilders.build());
             }
         }
 
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java b/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java
index 73694d4..db2d0b2 100755
--- a/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java
@@ -22,6 +22,7 @@ package org.elasticsearch.index.mapper;
 import com.carrotsearch.hppc.ObjectHashSet;
 import com.google.common.base.Function;
 import com.google.common.base.Predicate;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Iterators;
@@ -67,7 +68,6 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
@@ -261,6 +261,9 @@ public class MapperService extends AbstractIndexComponent implements Closeable {
             if (mapper.type().length() == 0) {
                 throw new InvalidTypeNameException("mapping type name is empty");
             }
+            if (Version.indexCreated(indexSettings).onOrAfter(Version.V_2_0_0_beta1) && mapper.type().length() > 255) {
+                throw new InvalidTypeNameException("mapping type name [" + mapper.type() + "] is too long; limit is length 255 but was [" + mapper.type().length() + "]");
+            }
             if (mapper.type().charAt(0) == '_') {
                 throw new InvalidTypeNameException("mapping type name [" + mapper.type() + "] can't start with '_'");
             }
@@ -527,7 +530,7 @@ public class MapperService extends AbstractIndexComponent implements Closeable {
     public Collection<String> simpleMatchToIndexNames(String pattern) {
         if (Regex.isSimpleMatchPattern(pattern) == false) {
             // no wildcards
-            return Collections.singletonList(pattern);
+            return ImmutableList.of(pattern);
         }
         return fieldTypes.simpleMatchToIndexNames(pattern);
     }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/ObjectMappers.java b/core/src/main/java/org/elasticsearch/index/mapper/ObjectMappers.java
new file mode 100644
index 0000000..52d4ea3
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/index/mapper/ObjectMappers.java
@@ -0,0 +1,124 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.mapper;
+
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.UnmodifiableIterator;
+import org.elasticsearch.index.mapper.object.ObjectMapper;
+
+/**
+ * A holder for several {@link org.elasticsearch.index.mapper.object.ObjectMapper}.
+ */
+public class ObjectMappers implements Iterable<ObjectMapper> {
+
+    private final ImmutableList<ObjectMapper> objectMappers;
+    private final boolean hasNested;
+
+    public ObjectMappers() {
+        this(ImmutableList.<ObjectMapper>of());
+    }
+
+    public ObjectMappers(ObjectMapper objectMapper) {
+        this(new ObjectMapper[]{objectMapper});
+    }
+
+    public ObjectMappers(ObjectMapper[] objectMappers) {
+        this(ImmutableList.copyOf(objectMappers));
+    }
+
+    public ObjectMappers(ImmutableList<ObjectMapper> objectMappers) {
+        this.objectMappers = objectMappers;
+        boolean hasNested = false;
+        for (ObjectMapper objectMapper : objectMappers) {
+            if (objectMapper.nested().isNested()) {
+                hasNested = true;
+                break;
+            }
+        }
+        this.hasNested = hasNested;
+    }
+
+    /**
+     * Is one of the object mappers has a nested mapping set?
+     */
+    public boolean hasNested() {
+        return this.hasNested;
+    }
+
+    public ObjectMapper mapper() {
+        if (objectMappers.isEmpty()) {
+            return null;
+        }
+        return objectMappers.get(0);
+    }
+
+    public boolean isEmpty() {
+        return objectMappers.isEmpty();
+    }
+
+    public ImmutableList<ObjectMapper> mappers() {
+        return this.objectMappers;
+    }
+
+    @Override
+    public UnmodifiableIterator<ObjectMapper> iterator() {
+        return objectMappers.iterator();
+    }
+
+    /**
+     * Concats and returns a new {@link org.elasticsearch.index.mapper.ObjectMappers}.
+     */
+    public ObjectMappers concat(ObjectMapper mapper) {
+        return new ObjectMappers(new ImmutableList.Builder<ObjectMapper>().addAll(objectMappers).add(mapper).build());
+    }
+
+    /**
+     * Concats and returns a new {@link org.elasticsearch.index.mapper.ObjectMappers}.
+     */
+    public ObjectMappers concat(ObjectMappers mappers) {
+        return new ObjectMappers(new ImmutableList.Builder<ObjectMapper>().addAll(objectMappers).addAll(mappers).build());
+    }
+
+    public ObjectMappers remove(Iterable<ObjectMapper> mappers) {
+        ImmutableList.Builder<ObjectMapper> builder = new ImmutableList.Builder<>();
+        for (ObjectMapper objectMapper : objectMappers) {
+            boolean found = false;
+            for (ObjectMapper mapper : mappers) {
+                if (objectMapper == mapper) { // identify equality
+                    found = true;
+                }
+            }
+            if (!found) {
+                builder.add(objectMapper);
+            }
+        }
+        return new ObjectMappers(builder.build());
+    }
+
+    public ObjectMappers remove(ObjectMapper mapper) {
+        ImmutableList.Builder<ObjectMapper> builder = new ImmutableList.Builder<>();
+        for (ObjectMapper objectMapper : objectMappers) {
+            if (objectMapper != mapper) { // identify equality
+                builder.add(objectMapper);
+            }
+        }
+        return new ObjectMappers(builder.build());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java
index dcbb19f..340eb81 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.query;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Iterables;
 
 import org.apache.lucene.queries.TermsQuery;
@@ -33,7 +34,6 @@ import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.List;
 
 /**
@@ -96,7 +96,7 @@ public class IdsQueryParser implements QueryParser {
                 }
             } else if (token.isValue()) {
                 if ("type".equals(currentFieldName) || "_type".equals(currentFieldName)) {
-                    types = Collections.singletonList(parser.text());
+                    types = ImmutableList.of(parser.text());
                 } else if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else if ("_name".equals(currentFieldName)) {
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
index e4a643f..6f73f08 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
@@ -78,8 +78,6 @@ public class MatchQueryBuilder extends QueryBuilder implements BoostableQueryBui
 
     private String minimumShouldMatch;
 
-    private String rewrite = null;
-
     private String fuzzyRewrite = null;
 
     private Boolean lenient;
@@ -179,11 +177,6 @@ public class MatchQueryBuilder extends QueryBuilder implements BoostableQueryBui
         return this;
     }
 
-    public MatchQueryBuilder rewrite(String rewrite) {
-        this.rewrite = rewrite;
-        return this;
-    }
-
     public MatchQueryBuilder fuzzyRewrite(String fuzzyRewrite) {
         this.fuzzyRewrite = fuzzyRewrite;
         return this;
@@ -249,9 +242,6 @@ public class MatchQueryBuilder extends QueryBuilder implements BoostableQueryBui
         if (minimumShouldMatch != null) {
             builder.field("minimum_should_match", minimumShouldMatch);
         }
-        if (rewrite != null) {
-            builder.field("rewrite", rewrite);
-        }
         if (fuzzyRewrite != null) {
             builder.field("fuzzy_rewrite", fuzzyRewrite);
         }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
index 1f7f960..d9a0751 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
@@ -61,8 +61,6 @@ public class MultiMatchQueryBuilder extends QueryBuilder implements BoostableQue
 
     private String minimumShouldMatch;
 
-    private String rewrite = null;
-
     private String fuzzyRewrite = null;
 
     private Boolean useDisMax;
@@ -255,11 +253,6 @@ public class MultiMatchQueryBuilder extends QueryBuilder implements BoostableQue
         return this;
     }
 
-    public MultiMatchQueryBuilder rewrite(String rewrite) {
-        this.rewrite = rewrite;
-        return this;
-    }
-
     public MultiMatchQueryBuilder fuzzyRewrite(String fuzzyRewrite) {
         this.fuzzyRewrite = fuzzyRewrite;
         return this;
@@ -367,9 +360,6 @@ public class MultiMatchQueryBuilder extends QueryBuilder implements BoostableQue
         if (minimumShouldMatch != null) {
             builder.field("minimum_should_match", minimumShouldMatch);
         }
-        if (rewrite != null) {
-            builder.field("rewrite", rewrite);
-        }
         if (fuzzyRewrite != null) {
             builder.field("fuzzy_rewrite", fuzzyRewrite);
         }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
index 7965eec..7959dc1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
@@ -68,7 +68,6 @@ public class QueryStringQueryBuilder extends QueryBuilder implements BoostableQu
 
     private Locale locale;
 
-
     private float boost = -1;
 
     private Fuzziness fuzziness;
@@ -99,6 +98,8 @@ public class QueryStringQueryBuilder extends QueryBuilder implements BoostableQu
     /** To limit effort spent determinizing regexp queries. */
     private Integer maxDeterminizedStates;
 
+    private Boolean escape;
+
     public QueryStringQueryBuilder(String queryString) {
         this.queryString = queryString;
     }
@@ -159,11 +160,11 @@ public class QueryStringQueryBuilder extends QueryBuilder implements BoostableQu
     /**
      * Sets the boolean operator of the query parser used to parse the query string.
      * <p/>
-     * <p>In default mode ({@link FieldQueryBuilder.Operator#OR}) terms without any modifiers
+     * <p>In default mode ({@link Operator#OR}) terms without any modifiers
      * are considered optional: for example <code>capital of Hungary</code> is equal to
      * <code>capital OR of OR Hungary</code>.
      * <p/>
-     * <p>In {@link FieldQueryBuilder.Operator#AND} mode terms are considered to be in conjunction: the
+     * <p>In {@link Operator#AND} mode terms are considered to be in conjunction: the
      * above mentioned query is parsed as <code>capital AND of AND Hungary</code>
      */
     public QueryStringQueryBuilder defaultOperator(Operator defaultOperator) {
@@ -342,6 +343,14 @@ public class QueryStringQueryBuilder extends QueryBuilder implements BoostableQu
         return this;
     }
 
+    /**
+     * Set to <tt>true</tt> to enable escaping of the query string
+     */
+    public QueryStringQueryBuilder escape(boolean escape) {
+        this.escape = escape;
+        return this;
+    }
+
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject(QueryStringQueryParser.NAME);
@@ -431,6 +440,9 @@ public class QueryStringQueryBuilder extends QueryBuilder implements BoostableQu
         if (timeZone != null) {
             builder.field("time_zone", timeZone);
         }
+        if (escape != null) {
+            builder.field("escape", escape);
+        }
         builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/search/stats/StatsGroupsParseElement.java b/core/src/main/java/org/elasticsearch/index/search/stats/StatsGroupsParseElement.java
index f4dcebd..e35607a 100644
--- a/core/src/main/java/org/elasticsearch/index/search/stats/StatsGroupsParseElement.java
+++ b/core/src/main/java/org/elasticsearch/index/search/stats/StatsGroupsParseElement.java
@@ -19,12 +19,12 @@
 
 package org.elasticsearch.index.search.stats;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 
 /**
@@ -35,7 +35,7 @@ public class StatsGroupsParseElement implements SearchParseElement {
     public void parse(XContentParser parser, SearchContext context) throws Exception {
         XContentParser.Token token = parser.currentToken();
         if (token.isValue()) {
-            context.groupStats(Collections.singletonList(parser.text()));
+            context.groupStats(ImmutableList.of(parser.text()));
         } else if (token == XContentParser.Token.START_ARRAY) {
             List<String> groupStats = new ArrayList<>(4);
             while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
diff --git a/core/src/main/java/org/elasticsearch/index/shard/CommitPoint.java b/core/src/main/java/org/elasticsearch/index/shard/CommitPoint.java
index 6afa0c9..bae4ae8 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/CommitPoint.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/CommitPoint.java
@@ -19,10 +19,10 @@
 
 package org.elasticsearch.index.shard;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.index.store.StoreFileMetaData;
 
-import java.util.Collections;
 import java.util.List;
 
 /**
@@ -30,7 +30,7 @@ import java.util.List;
  */
 public class CommitPoint {
 
-    public static final CommitPoint NULL = new CommitPoint(-1, "_null_", Type.GENERATED, Collections.<CommitPoint.FileInfo>emptyList(), Collections.<CommitPoint.FileInfo>emptyList());
+    public static final CommitPoint NULL = new CommitPoint(-1, "_null_", Type.GENERATED, ImmutableList.<CommitPoint.FileInfo>of(), ImmutableList.<CommitPoint.FileInfo>of());
 
     public static class FileInfo {
         private final String name;
@@ -81,16 +81,16 @@ public class CommitPoint {
 
     private final Type type;
 
-    private final List<FileInfo> indexFiles;
+    private final ImmutableList<FileInfo> indexFiles;
 
-    private final List<FileInfo> translogFiles;
+    private final ImmutableList<FileInfo> translogFiles;
 
     public CommitPoint(long version, String name, Type type, List<FileInfo> indexFiles, List<FileInfo> translogFiles) {
         this.version = version;
         this.name = name;
         this.type = type;
-        this.indexFiles = Collections.unmodifiableList(indexFiles);
-        this.translogFiles = Collections.unmodifiableList(translogFiles);
+        this.indexFiles = ImmutableList.copyOf(indexFiles);
+        this.translogFiles = ImmutableList.copyOf(translogFiles);
     }
 
     public long version() {
diff --git a/core/src/main/java/org/elasticsearch/index/shard/CommitPoints.java b/core/src/main/java/org/elasticsearch/index/shard/CommitPoints.java
index 9f4a2aa..a56a0f6 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/CommitPoints.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/CommitPoints.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.shard;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 import org.apache.lucene.util.CollectionUtil;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -27,7 +28,6 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
 
 import java.io.IOException;
-import java.util.Collections;
 import java.util.Comparator;
 import java.util.Iterator;
 import java.util.List;
@@ -37,7 +37,7 @@ import java.util.List;
  */
 public class CommitPoints implements Iterable<CommitPoint> {
 
-    private final List<CommitPoint> commitPoints;
+    private final ImmutableList<CommitPoint> commitPoints;
 
     public CommitPoints(List<CommitPoint> commitPoints) {
         CollectionUtil.introSort(commitPoints, new Comparator<CommitPoint>() {
@@ -46,7 +46,7 @@ public class CommitPoints implements Iterable<CommitPoint> {
                 return (o2.version() < o1.version() ? -1 : (o2.version() == o1.version() ? 0 : 1));
             }
         });
-        this.commitPoints = Collections.unmodifiableList(commitPoints);
+        this.commitPoints = ImmutableList.copyOf(commitPoints);
     }
 
     public List<CommitPoint> commits() {
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
index dc1207a..e60b046 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -21,8 +21,15 @@ package org.elasticsearch.index.shard;
 
 import com.google.common.base.Charsets;
 import com.google.common.base.Preconditions;
+
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.index.CheckIndex;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.DisjunctionMaxQuery;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.MatchNoDocsQuery;
+import org.apache.lucene.search.Query;
 import org.apache.lucene.search.QueryCachingPolicy;
 import org.apache.lucene.search.UsageTrackingQueryCachingPolicy;
 import org.apache.lucene.store.AlreadyClosedException;
@@ -162,8 +169,8 @@ public class IndexShard extends AbstractIndexShardComponent {
 
     private TimeValue refreshInterval;
 
-    private volatile ScheduledFuture refreshScheduledFuture;
-    private volatile ScheduledFuture mergeScheduleFuture;
+    private volatile ScheduledFuture<?> refreshScheduledFuture;
+    private volatile ScheduledFuture<?> mergeScheduleFuture;
     protected volatile ShardRouting shardRouting;
     protected volatile IndexShardState state;
     protected final AtomicReference<Engine> currentEngineReference = new AtomicReference<>();
@@ -252,7 +259,42 @@ public class IndexShard extends AbstractIndexShardComponent {
         if (indexSettings.getAsBoolean(IndexCacheModule.QUERY_CACHE_EVERYTHING, false)) {
             cachingPolicy = QueryCachingPolicy.ALWAYS_CACHE;
         } else {
-            cachingPolicy = new UsageTrackingQueryCachingPolicy();
+            assert Version.CURRENT.luceneVersion == org.apache.lucene.util.Version.LUCENE_5_2_1;
+            // TODO: remove this hack in Lucene 5.4, use UsageTrackingQueryCachingPolicy directly
+            // See https://issues.apache.org/jira/browse/LUCENE-6748
+            // cachingPolicy = new UsageTrackingQueryCachingPolicy();
+
+            final QueryCachingPolicy wrapped = new UsageTrackingQueryCachingPolicy();
+            cachingPolicy = new QueryCachingPolicy() {
+
+                @Override
+                public boolean shouldCache(Query query, LeafReaderContext context) throws IOException {
+                    if (query instanceof MatchAllDocsQuery
+                            // MatchNoDocsQuery currently rewrites to a BooleanQuery,
+                            // but who knows, it might get its own Weight one day
+                            || query instanceof MatchNoDocsQuery) {
+                        return false;
+                    }
+                    if (query instanceof BooleanQuery) {
+                        BooleanQuery bq = (BooleanQuery) query;
+                        if (bq.clauses().isEmpty()) {
+                            return false;
+                        }
+                    }
+                    if (query instanceof DisjunctionMaxQuery) {
+                        DisjunctionMaxQuery dmq = (DisjunctionMaxQuery) query;
+                        if (dmq.getDisjuncts().isEmpty()) {
+                            return false;
+                        }
+                    }
+                    return wrapped.shouldCache(query, context);
+                }
+
+                @Override
+                public void onUse(Query query) {
+                    wrapped.onUse(query);
+                }
+            };
         }
         this.engineConfig = newEngineConfig(translogConfig, cachingPolicy);
         this.indexShardOperationCounter = new IndexShardOperationCounter(logger, shardId);
diff --git a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java
index 282e869..0e997c1 100644
--- a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java
+++ b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.snapshots.blobstore;
 
+import com.google.common.collect.ImmutableList;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Version;
 import org.elasticsearch.ElasticsearchParseException;
@@ -32,7 +33,6 @@ import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.index.store.StoreFileMetaData;
 
 import java.io.IOException;
-import java.util.Collections;
 import java.util.List;
 
 import static com.google.common.collect.Lists.newArrayList;
@@ -323,7 +323,7 @@ public class BlobStoreIndexShardSnapshot implements ToXContent, FromXContentBuil
 
     private final long totalSize;
 
-    private final List<FileInfo> indexFiles;
+    private final ImmutableList<FileInfo> indexFiles;
 
     /**
      * Constructs new shard snapshot metadata from snapshot metadata
@@ -342,7 +342,7 @@ public class BlobStoreIndexShardSnapshot implements ToXContent, FromXContentBuil
         assert indexVersion >= 0;
         this.snapshot = snapshot;
         this.indexVersion = indexVersion;
-        this.indexFiles = Collections.unmodifiableList(indexFiles);
+        this.indexFiles = ImmutableList.copyOf(indexFiles);
         this.startTime = startTime;
         this.time = time;
         this.numberOfFiles = numberOfFiles;
@@ -355,7 +355,7 @@ public class BlobStoreIndexShardSnapshot implements ToXContent, FromXContentBuil
     private BlobStoreIndexShardSnapshot() {
         this.snapshot = "";
         this.indexVersion = 0;
-        this.indexFiles = Collections.emptyList();
+        this.indexFiles = ImmutableList.of();
         this.startTime = 0;
         this.time = 0;
         this.numberOfFiles = 0;
@@ -520,7 +520,7 @@ public class BlobStoreIndexShardSnapshot implements ToXContent, FromXContentBuil
                 }
             }
         }
-        return new BlobStoreIndexShardSnapshot(snapshot, indexVersion, Collections.unmodifiableList(indexFiles),
+        return new BlobStoreIndexShardSnapshot(snapshot, indexVersion, ImmutableList.copyOf(indexFiles),
                 startTime, time, numberOfFiles, totalSize);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
index fa06dff..19bf4ee 100644
--- a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
+++ b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.snapshots.blobstore;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.ParseField;
@@ -27,8 +28,6 @@ import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot.FileInfo;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
@@ -46,12 +45,12 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
 
     public static final BlobStoreIndexShardSnapshots PROTO = new BlobStoreIndexShardSnapshots();
 
-    private final List<SnapshotFiles> shardSnapshots;
+    private final ImmutableList<SnapshotFiles> shardSnapshots;
     private final ImmutableMap<String, FileInfo> files;
-    private final ImmutableMap<String, List<FileInfo>> physicalFiles;
+    private final ImmutableMap<String, ImmutableList<FileInfo>> physicalFiles;
 
     public BlobStoreIndexShardSnapshots(List<SnapshotFiles> shardSnapshots) {
-        this.shardSnapshots = Collections.unmodifiableList(shardSnapshots);
+        this.shardSnapshots = ImmutableList.copyOf(shardSnapshots);
         // Map between blob names and file info
         Map<String, FileInfo> newFiles = newHashMap();
         // Map between original physical names and file info
@@ -75,15 +74,15 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
                 physicalFileList.add(newFiles.get(fileInfo.name()));
             }
         }
-        ImmutableMap.Builder<String, List<FileInfo>> mapBuilder = ImmutableMap.builder();
+        ImmutableMap.Builder<String, ImmutableList<FileInfo>> mapBuilder = ImmutableMap.builder();
         for (Map.Entry<String, List<FileInfo>> entry : physicalFiles.entrySet()) {
-            mapBuilder.put(entry.getKey(), Collections.unmodifiableList(entry.getValue()));
+            mapBuilder.put(entry.getKey(), ImmutableList.copyOf(entry.getValue()));
         }
         this.physicalFiles = mapBuilder.build();
         this.files = ImmutableMap.copyOf(newFiles);
     }
 
-    private BlobStoreIndexShardSnapshots(ImmutableMap<String, FileInfo> files, List<SnapshotFiles> shardSnapshots) {
+    private BlobStoreIndexShardSnapshots(ImmutableMap<String, FileInfo> files, ImmutableList<SnapshotFiles> shardSnapshots) {
         this.shardSnapshots = shardSnapshots;
         this.files = files;
         Map<String, List<FileInfo>> physicalFiles = newHashMap();
@@ -97,15 +96,15 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
                 physicalFileList.add(files.get(fileInfo.name()));
             }
         }
-        ImmutableMap.Builder<String, List<FileInfo>> mapBuilder = ImmutableMap.builder();
+        ImmutableMap.Builder<String, ImmutableList<FileInfo>> mapBuilder = ImmutableMap.builder();
         for (Map.Entry<String, List<FileInfo>> entry : physicalFiles.entrySet()) {
-            mapBuilder.put(entry.getKey(), Collections.unmodifiableList(entry.getValue()));
+            mapBuilder.put(entry.getKey(), ImmutableList.copyOf(entry.getValue()));
         }
         this.physicalFiles = mapBuilder.build();
     }
 
     private BlobStoreIndexShardSnapshots() {
-        shardSnapshots = Collections.emptyList();
+        shardSnapshots = ImmutableList.of();
         files = ImmutableMap.of();
         physicalFiles = ImmutableMap.of();
     }
@@ -286,17 +285,17 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
         }
 
         ImmutableMap<String, FileInfo> files = filesBuilder.build();
-        List<SnapshotFiles> snapshots = new ArrayList<>();
+        ImmutableList.Builder<SnapshotFiles> snapshots = ImmutableList.builder();
         for (Map.Entry<String, List<String>> entry : snapshotsMap.entrySet()) {
-            List<FileInfo> fileInfosBuilder = new ArrayList<>();
+            ImmutableList.Builder<FileInfo> fileInfosBuilder = ImmutableList.builder();
             for (String file : entry.getValue()) {
                 FileInfo fileInfo = files.get(file);
                 assert fileInfo != null;
                 fileInfosBuilder.add(fileInfo);
             }
-            snapshots.add(new SnapshotFiles(entry.getKey(), Collections.unmodifiableList(fileInfosBuilder)));
+            snapshots.add(new SnapshotFiles(entry.getKey(), fileInfosBuilder.build()));
         }
-        return new BlobStoreIndexShardSnapshots(files, Collections.unmodifiableList(snapshots));
+        return new BlobStoreIndexShardSnapshots(files, snapshots.build());
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/SnapshotFiles.java b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/SnapshotFiles.java
index 44d40c9..aa265f1 100644
--- a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/SnapshotFiles.java
+++ b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/SnapshotFiles.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.index.snapshots.blobstore;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot.FileInfo;
 
 import java.util.List;
diff --git a/core/src/main/java/org/elasticsearch/index/store/Store.java b/core/src/main/java/org/elasticsearch/index/store/Store.java
index 847f3e3..2736717 100644
--- a/core/src/main/java/org/elasticsearch/index/store/Store.java
+++ b/core/src/main/java/org/elasticsearch/index/store/Store.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.store;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Iterables;
 import org.apache.lucene.codecs.CodecUtil;
@@ -1004,9 +1005,9 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
          * NOTE: this diff will not contain the <tt>segments.gen</tt> file. This file is omitted on recovery.
          */
         public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {
-            final List<StoreFileMetaData> identical = new ArrayList<>();
-            final List<StoreFileMetaData> different = new ArrayList<>();
-            final List<StoreFileMetaData> missing = new ArrayList<>();
+            final ImmutableList.Builder<StoreFileMetaData> identical = ImmutableList.builder();
+            final ImmutableList.Builder<StoreFileMetaData> different = ImmutableList.builder();
+            final ImmutableList.Builder<StoreFileMetaData> missing = ImmutableList.builder();
             final Map<String, List<StoreFileMetaData>> perSegment = new HashMap<>();
             final List<StoreFileMetaData> perCommitStoreFiles = new ArrayList<>();
 
@@ -1052,7 +1053,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
                     different.addAll(identicalFiles);
                 }
             }
-            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical), Collections.unmodifiableList(different), Collections.unmodifiableList(missing));
+            RecoveryDiff recoveryDiff = new RecoveryDiff(identical.build(), different.build(), missing.build());
             assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)
                     : "some files are missing recoveryDiff size: [" + recoveryDiff.size() + "] metadata size: [" + this.metadata.size() + "] contains  segments.gen: [" + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + "]";
             return recoveryDiff;
diff --git a/core/src/main/java/org/elasticsearch/indices/IndicesService.java b/core/src/main/java/org/elasticsearch/indices/IndicesService.java
index 022fbd9..43fdb3d 100644
--- a/core/src/main/java/org/elasticsearch/indices/IndicesService.java
+++ b/core/src/main/java/org/elasticsearch/indices/IndicesService.java
@@ -20,7 +20,12 @@
 package org.elasticsearch.indices;
 
 import com.google.common.base.Function;
-import com.google.common.collect.*;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.ImmutableSet;
+import com.google.common.collect.Iterables;
+import com.google.common.collect.Iterators;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.util.CollectionUtil;
 import org.apache.lucene.util.IOUtils;
@@ -35,7 +40,12 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
-import org.elasticsearch.common.inject.*;
+import org.elasticsearch.common.inject.CreationException;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.inject.Injector;
+import org.elasticsearch.common.inject.Injectors;
+import org.elasticsearch.common.inject.Module;
+import org.elasticsearch.common.inject.ModulesBuilder;
 import org.elasticsearch.common.io.FileSystemUtils;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
@@ -43,7 +53,12 @@ import org.elasticsearch.common.util.concurrent.EsExecutors;
 import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.env.ShardLock;
 import org.elasticsearch.gateway.MetaDataStateFormat;
-import org.elasticsearch.index.*;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.index.IndexModule;
+import org.elasticsearch.index.IndexNameModule;
+import org.elasticsearch.index.IndexNotFoundException;
+import org.elasticsearch.index.IndexService;
+import org.elasticsearch.index.LocalNodeIdModule;
 import org.elasticsearch.index.aliases.IndexAliasesServiceModule;
 import org.elasticsearch.index.analysis.AnalysisModule;
 import org.elasticsearch.index.analysis.AnalysisService;
@@ -71,13 +86,16 @@ import org.elasticsearch.index.store.IndexStore;
 import org.elasticsearch.index.store.IndexStoreModule;
 import org.elasticsearch.indices.analysis.IndicesAnalysisService;
 import org.elasticsearch.indices.recovery.RecoverySettings;
-import org.elasticsearch.plugins.IndexPluginsModule;
 import org.elasticsearch.plugins.PluginsService;
 
 import java.io.Closeable;
 import java.io.IOException;
 import java.nio.file.Files;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
@@ -306,7 +324,10 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
         modules.add(new IndexNameModule(index));
         modules.add(new LocalNodeIdModule(localNodeId));
         modules.add(new IndexSettingsModule(index, indexSettings));
-        modules.add(new IndexPluginsModule(indexSettings, pluginsService));
+        // plugin modules must be added here, before others or we can get crazy injection errors...
+        for (Module pluginModule : pluginsService.indexModules(indexSettings)) {
+            modules.add(pluginModule);
+        }
         modules.add(new IndexStoreModule(indexSettings));
         modules.add(new AnalysisModule(indexSettings, indicesAnalysisService));
         modules.add(new SimilarityModule(indexSettings));
@@ -315,6 +336,8 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
         modules.add(new MapperServiceModule());
         modules.add(new IndexAliasesServiceModule());
         modules.add(new IndexModule(indexSettings));
+        
+        pluginsService.processModules(modules);
 
         Injector indexInjector;
         try {
diff --git a/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java b/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
index d8e8139..c669a9d 100644
--- a/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
+++ b/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
@@ -638,7 +638,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
                 if (logger.isDebugEnabled()) {
                     logger.debug("[{}][{}] creating shard", shardRouting.index(), shardId);
                 }
-                IndexShard indexShard = indexService.createShard(shardId, shardRouting.primary());
+                IndexShard indexShard = indexService.createShard(shardId, shardRouting);
                 indexShard.updateRoutingEntry(shardRouting, state.blocks().disableStatePersistence() == false);
                 indexShard.addFailedEngineListener(failedEngineHandler);
             } catch (IndexShardAlreadyExistsException e) {
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java
index cc58305..a0ef2ef 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.indices.recovery;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.RestoreSource;
 import org.elasticsearch.common.Nullable;
@@ -33,8 +34,6 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.shard.ShardId;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Locale;
@@ -713,7 +712,7 @@ public class RecoveryState implements ToXContent, Streamable {
         private long targetThrottleTimeInNanos = UNKNOWN;
 
         public synchronized List<File> fileDetails() {
-            return Collections.unmodifiableList(new ArrayList<>(fileDetails.values()));
+            return ImmutableList.copyOf(fileDetails.values());
         }
 
         public synchronized void reset() {
diff --git a/core/src/main/java/org/elasticsearch/monitor/process/ProcessProbe.java b/core/src/main/java/org/elasticsearch/monitor/process/ProcessProbe.java
index b1d1c7f..822f4f5 100644
--- a/core/src/main/java/org/elasticsearch/monitor/process/ProcessProbe.java
+++ b/core/src/main/java/org/elasticsearch/monitor/process/ProcessProbe.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.monitor.process;
 
-import org.elasticsearch.bootstrap.Bootstrap;
+import org.elasticsearch.bootstrap.BootstrapInfo;
 
 import java.lang.management.ManagementFactory;
 import java.lang.management.OperatingSystemMXBean;
@@ -136,7 +136,7 @@ public class ProcessProbe {
     }
 
     public ProcessInfo processInfo() {
-        return new ProcessInfo(jvmInfo().pid(), Bootstrap.isMemoryLocked());
+        return new ProcessInfo(jvmInfo().pid(), BootstrapInfo.isMemoryLocked());
     }
 
     public ProcessStats processStats() {
diff --git a/core/src/main/java/org/elasticsearch/node/Node.java b/core/src/main/java/org/elasticsearch/node/Node.java
index 45e5522..6da2270 100644
--- a/core/src/main/java/org/elasticsearch/node/Node.java
+++ b/core/src/main/java/org/elasticsearch/node/Node.java
@@ -35,6 +35,7 @@ import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.component.Lifecycle;
 import org.elasticsearch.common.component.LifecycleComponent;
 import org.elasticsearch.common.inject.Injector;
+import org.elasticsearch.common.inject.Module;
 import org.elasticsearch.common.inject.ModulesBuilder;
 import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.lease.Releasables;
@@ -159,7 +160,11 @@ public class Node implements Releasable {
             ModulesBuilder modules = new ModulesBuilder();
             modules.add(new Version.Module(version));
             modules.add(new CircuitBreakerModule(settings));
-            modules.add(new PluginsModule(settings, pluginsService));
+            // plugin modules must be added here, before others or we can get crazy injection errors...
+            for (Module pluginModule : pluginsService.nodeModules()) {
+                modules.add(pluginModule);
+            }
+            modules.add(new PluginsModule(pluginsService));
             modules.add(new SettingsModule(settings));
             modules.add(new NodeModule(this));
             modules.add(new NetworkModule());
@@ -187,6 +192,9 @@ public class Node implements Releasable {
             modules.add(new RepositoriesModule());
             modules.add(new TribeModule());
 
+
+            pluginsService.processModules(modules);
+
             injector = modules.createInjector();
 
             client = injector.getInstance(Client.class);
diff --git a/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java b/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
index 4355c9f..3eb2ed6 100644
--- a/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
+++ b/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
@@ -20,6 +20,8 @@
 package org.elasticsearch.node.internal;
 
 import com.google.common.base.Charsets;
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.Sets;
 import com.google.common.collect.UnmodifiableIterator;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.common.Booleans;
@@ -37,9 +39,9 @@ import java.io.InputStreamReader;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 import java.util.concurrent.ThreadLocalRandom;
 
 import static org.elasticsearch.common.Strings.cleanPath;
@@ -50,7 +52,7 @@ import static org.elasticsearch.common.settings.Settings.settingsBuilder;
  */
 public class InternalSettingsPreparer {
 
-    static final List<String> ALLOWED_SUFFIXES = Arrays.asList(".yml", ".yaml", ".json", ".properties");
+    static final List<String> ALLOWED_SUFFIXES = ImmutableList.of(".yml", ".yaml", ".json", ".properties");
 
     public static final String SECRET_PROMPT_VALUE = "${prompt.secret}";
     public static final String TEXT_PROMPT_VALUE = "${prompt.text}";
@@ -113,13 +115,21 @@ public class InternalSettingsPreparer {
                 }
             }
             if (loadFromEnv) {
+                boolean settingsFileFound = false;
+                Set<String> foundSuffixes = Sets.newHashSet();
                 for (String allowedSuffix : ALLOWED_SUFFIXES) {
-                    try {
-                        settingsBuilder.loadFromPath(environment.configFile().resolve("elasticsearch" + allowedSuffix));
-                    } catch (SettingsException e) {
-                        // ignore
+                    Path path = environment.configFile().resolve("elasticsearch" + allowedSuffix);
+                    if (Files.exists(path)) {
+                        if (!settingsFileFound) {
+                            settingsBuilder.loadFromPath(path);
+                        }
+                        settingsFileFound = true;
+                        foundSuffixes.add(allowedSuffix);
                     }
                 }
+                if (foundSuffixes.size() > 1) {
+                    throw new SettingsException("multiple settings files found with suffixes: " + Strings.collectionToDelimitedString(foundSuffixes, ","));
+                }
             }
         }
 
diff --git a/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java b/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
index dd2c5e8..d42b858 100644
--- a/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
+++ b/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
@@ -19,6 +19,7 @@
 package org.elasticsearch.percolator;
 
 import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
+import com.google.common.collect.ImmutableList;
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexableField;
@@ -26,7 +27,6 @@ import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Counter;
@@ -52,7 +52,6 @@ import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.search.Scroll;
 import org.elasticsearch.search.SearchHitField;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
@@ -67,6 +66,7 @@ import org.elasticsearch.search.highlight.SearchContextHighlight;
 import org.elasticsearch.search.internal.ContextIndexSearcher;
 import org.elasticsearch.search.internal.InternalSearchHit;
 import org.elasticsearch.search.internal.InternalSearchHitField;
+import org.elasticsearch.search.internal.ScrollContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.internal.ShardSearchRequest;
 import org.elasticsearch.search.lookup.LeafSearchLookup;
@@ -155,7 +155,7 @@ public class PercolateContext extends SearchContext {
 
         Map<String, SearchHitField> fields = new HashMap<>();
         for (IndexableField field : parsedDocument.rootDoc().getFields()) {
-            fields.put(field.name(), new InternalSearchHitField(field.name(), Collections.emptyList()));
+            fields.put(field.name(), new InternalSearchHitField(field.name(), ImmutableList.of()));
         }
         hitContext().reset(
                 new InternalSearchHit(0, "unknown", new StringText(parsedDocument.type()), fields),
@@ -347,12 +347,12 @@ public class PercolateContext extends SearchContext {
     }
 
     @Override
-    public Scroll scroll() {
+    public ScrollContext scrollContext() {
         throw new UnsupportedOperationException();
     }
 
     @Override
-    public SearchContext scroll(Scroll scroll) {
+    public SearchContext scrollContext(ScrollContext scroll) {
         throw new UnsupportedOperationException();
     }
 
@@ -621,16 +621,6 @@ public class PercolateContext extends SearchContext {
     }
 
     @Override
-    public void lastEmittedDoc(ScoreDoc doc) {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public ScoreDoc lastEmittedDoc() {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
     public DfsSearchResult dfsResult() {
         throw new UnsupportedOperationException();
     }
diff --git a/core/src/main/java/org/elasticsearch/plugins/IndexPluginsModule.java b/core/src/main/java/org/elasticsearch/plugins/IndexPluginsModule.java
deleted file mode 100644
index a45f7d7..0000000
--- a/core/src/main/java/org/elasticsearch/plugins/IndexPluginsModule.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugins;
-
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Module;
-import org.elasticsearch.common.inject.PreProcessModule;
-import org.elasticsearch.common.inject.SpawnModules;
-import org.elasticsearch.common.settings.Settings;
-
-/**
- *
- */
-public class IndexPluginsModule extends AbstractModule implements SpawnModules, PreProcessModule {
-
-    private final Settings settings;
-
-    private final PluginsService pluginsService;
-
-    public IndexPluginsModule(Settings settings, PluginsService pluginsService) {
-        this.settings = settings;
-        this.pluginsService = pluginsService;
-    }
-
-    @Override
-    public Iterable<? extends Module> spawnModules() {
-        return pluginsService.indexModules(settings);
-    }
-
-    @Override
-    public void processModule(Module module) {
-        pluginsService.processModule(module);
-    }
-
-    @Override
-    protected void configure() {
-    }
-}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginInfo.java b/core/src/main/java/org/elasticsearch/plugins/PluginInfo.java
index 66f24e3..c305893 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginInfo.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginInfo.java
@@ -52,7 +52,7 @@ public class PluginInfo implements Streamable, ToXContent {
     private String description;
     private boolean site;
     private String version;
-    
+
     private boolean jvm;
     private String classname;
     private boolean isolated;
@@ -86,7 +86,11 @@ public class PluginInfo implements Streamable, ToXContent {
         try (InputStream stream = Files.newInputStream(descriptor)) {
             props.load(stream);
         }
-        String name = dir.getFileName().toString();
+        String name = props.getProperty("name");
+        if (name == null || name.isEmpty()) {
+            throw new IllegalArgumentException("Property [name] is missing in [" + descriptor + "]");
+        }
+        PluginManager.checkForForbiddenName(name);
         String description = props.getProperty("description");
         if (description == null) {
             throw new IllegalArgumentException("Property [description] is missing for plugin [" + name + "]");
@@ -95,6 +99,7 @@ public class PluginInfo implements Streamable, ToXContent {
         if (version == null) {
             throw new IllegalArgumentException("Property [version] is missing for plugin [" + name + "]");
         }
+
         boolean jvm = Boolean.parseBoolean(props.getProperty("jvm"));
         boolean site = Boolean.parseBoolean(props.getProperty("site"));
         if (jvm == false && site == false) {
@@ -115,6 +120,7 @@ public class PluginInfo implements Streamable, ToXContent {
             if (javaVersionString == null) {
                 throw new IllegalArgumentException("Property [java.version] is missing for jvm plugin [" + name + "]");
             }
+            JarHell.checkVersionFormat(javaVersionString);
             JarHell.checkJavaVersion(name, javaVersionString);
             isolated = Boolean.parseBoolean(props.getProperty("isolated", "true"));
             classname = props.getProperty("classname");
@@ -122,7 +128,7 @@ public class PluginInfo implements Streamable, ToXContent {
                 throw new IllegalArgumentException("Property [classname] is missing for jvm plugin [" + name + "]");
             }
         }
-        
+
         if (site) {
             if (!Files.exists(dir.resolve("_site"))) {
                 throw new IllegalArgumentException("Plugin [" + name + "] is a site plugin but has no '_site/' directory");
@@ -159,14 +165,14 @@ public class PluginInfo implements Streamable, ToXContent {
     public boolean isJvm() {
         return jvm;
     }
-    
+
     /**
      * @return true if jvm plugin has isolated classloader
      */
     public boolean isIsolated() {
         return isolated;
     }
-    
+
     /**
      * @return jvm plugin's classname
      */
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
index 234d719..e03b950 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
@@ -84,6 +84,7 @@ public class PluginManager {
                     "cloud-azure",
                     "cloud-gce",
                     "delete-by-query",
+                    "discovery-multicast",
                     "lang-javascript",
                     "lang-python",
                     "mapper-murmur3",
@@ -91,11 +92,11 @@ public class PluginManager {
             ).build();
 
     private final Environment environment;
-    private String url;
+    private URL url;
     private OutputMode outputMode;
     private TimeValue timeout;
 
-    public PluginManager(Environment environment, String url, OutputMode outputMode, TimeValue timeout) {
+    public PluginManager(Environment environment, URL url, OutputMode outputMode, TimeValue timeout) {
         this.environment = environment;
         this.url = url;
         this.outputMode = outputMode;
@@ -103,8 +104,8 @@ public class PluginManager {
     }
 
     public void downloadAndExtract(String name, Terminal terminal) throws IOException {
-        if (name == null) {
-            throw new IllegalArgumentException("plugin name must be supplied with install [name].");
+        if (name == null && url == null) {
+            throw new IllegalArgumentException("plugin name or url must be supplied with install.");
         }
 
         if (!Files.exists(environment.pluginsFile())) {
@@ -116,8 +117,14 @@ public class PluginManager {
             throw new IOException("plugin directory " + environment.pluginsFile() + " is read only");
         }
 
-        PluginHandle pluginHandle = PluginHandle.parse(name);
-        checkForForbiddenName(pluginHandle.name);
+        PluginHandle pluginHandle;
+        if (name != null) {
+            pluginHandle = PluginHandle.parse(name);
+            checkForForbiddenName(pluginHandle.name);
+        } else {
+            // if we have no name but url, use temporary name that will be overwritten later
+            pluginHandle = new PluginHandle("temp_name" + new Random().nextInt(), null, null);
+        }
 
         Path pluginFile = download(pluginHandle, terminal);
         extract(pluginHandle, terminal, pluginFile);
@@ -138,7 +145,7 @@ public class PluginManager {
 
         // first, try directly from the URL provided
         if (url != null) {
-            URL pluginUrl = new URL(url);
+            URL pluginUrl = url;
             boolean isSecureProcotol = "https".equalsIgnoreCase(pluginUrl.getProtocol());
             boolean isAuthInfoSet = !Strings.isNullOrEmpty(pluginUrl.getUserInfo());
             if (isAuthInfoSet && !isSecureProcotol) {
@@ -204,14 +211,10 @@ public class PluginManager {
     }
 
     private void extract(PluginHandle pluginHandle, Terminal terminal, Path pluginFile) throws IOException {
-        final Path extractLocation = pluginHandle.extractedDir(environment);
-        if (Files.exists(extractLocation)) {
-            throw new IOException("plugin directory " + extractLocation.toAbsolutePath() + " already exists. To update the plugin, uninstall it first using 'remove " + pluginHandle.name + "' command");
-        }
 
         // unzip plugin to a staging temp dir, named for the plugin
         Path tmp = Files.createTempDirectory(environment.tmpFile(), null);
-        Path root = tmp.resolve(pluginHandle.name); 
+        Path root = tmp.resolve(pluginHandle.name);
         unzipPlugin(pluginFile, root);
 
         // find the actual root (in case its unzipped with extra directory wrapping)
@@ -226,6 +229,13 @@ public class PluginManager {
             jarHellCheck(root, info.isIsolated());
         }
 
+        // update name in handle based on 'name' property found in descriptor file
+        pluginHandle = new PluginHandle(info.getName(), pluginHandle.version, pluginHandle.user);
+        final Path extractLocation = pluginHandle.extractedDir(environment);
+        if (Files.exists(extractLocation)) {
+            throw new IOException("plugin directory " + extractLocation.toAbsolutePath() + " already exists. To update the plugin, uninstall it first using 'remove " + pluginHandle.name + "' command");
+        }
+
         // install plugin
         FileSystemUtils.copyDirectoryRecursively(root, extractLocation);
         terminal.println("Installed %s into %s", pluginHandle.name, extractLocation.toAbsolutePath());
@@ -334,7 +344,7 @@ public class PluginManager {
 
     private void unzipPlugin(Path zip, Path target) throws IOException {
         Files.createDirectories(target);
-        
+
         try (ZipInputStream zipInput = new ZipInputStream(Files.newInputStream(zip))) {
             ZipEntry entry;
             byte[] buffer = new byte[8192];
@@ -395,7 +405,7 @@ public class PluginManager {
         }
     }
 
-    private static void checkForForbiddenName(String name) {
+    static void checkForForbiddenName(String name) {
         if (!hasLength(name) || BLACKLIST.contains(name.toLowerCase(Locale.ROOT))) {
             throw new IllegalArgumentException("Illegal plugin name: " + name);
         }
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginManagerCliParser.java b/core/src/main/java/org/elasticsearch/plugins/PluginManagerCliParser.java
index 3732e8b..7f521fd 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginManagerCliParser.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginManagerCliParser.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.plugins;
 
 import com.google.common.base.Strings;
+
 import org.apache.commons.cli.CommandLine;
 import org.elasticsearch.common.cli.CliTool;
 import org.elasticsearch.common.cli.CliToolConfig;
@@ -32,7 +33,8 @@ import org.elasticsearch.env.Environment;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.elasticsearch.plugins.PluginManager.OutputMode;
 
-import java.io.IOException;
+import java.net.MalformedURLException;
+import java.net.URL;
 import java.util.Locale;
 
 import static org.elasticsearch.common.cli.CliToolConfig.Builder.cmd;
@@ -166,19 +168,29 @@ public class PluginManagerCliParser extends CliTool {
         private static final String NAME = "install";
 
         private static final CliToolConfig.Cmd CMD = cmd(NAME, Install.class)
-                .options(option("u", "url").required(false).hasArg(true))
                 .options(option("t", "timeout").required(false).hasArg(false))
                 .build();
 
         static Command parse(Terminal terminal, CommandLine cli) {
             String[] args = cli.getArgs();
+
+            // install [plugin-name/url]
             if ((args == null) || (args.length == 0)) {
-                return exitCmd(ExitStatus.USAGE, terminal, "plugin name is missing (type -h for help)");
+                return exitCmd(ExitStatus.USAGE, terminal, "plugin name or url is missing (type -h for help)");
             }
-
             String name = args[0];
+
+            URL optionalPluginUrl = null;
+            // try parsing cli argument as URL
+            try {
+                optionalPluginUrl = new URL(name);
+                name = null;
+            } catch (MalformedURLException e) {
+                // we tried to parse the cli argument as url and failed
+                // continue treating it as a symbolic plugin name like `analysis-icu` etc.
+            }
+
             TimeValue timeout = TimeValue.parseTimeValue(cli.getOptionValue("t"), DEFAULT_TIMEOUT, "cli");
-            String url = cli.getOptionValue("u");
 
             OutputMode outputMode = OutputMode.DEFAULT;
             if (cli.hasOption("s")) {
@@ -188,15 +200,15 @@ public class PluginManagerCliParser extends CliTool {
                 outputMode = OutputMode.VERBOSE;
             }
 
-            return new Install(terminal, name, outputMode, url, timeout);
+            return new Install(terminal, name, outputMode, optionalPluginUrl, timeout);
         }
 
         final String name;
         private OutputMode outputMode;
-        final String url;
+        final URL url;
         final TimeValue timeout;
 
-        Install(Terminal terminal, String name, OutputMode outputMode, String url, TimeValue timeout) {
+        Install(Terminal terminal, String name, OutputMode outputMode, URL url, TimeValue timeout) {
             super(terminal);
             this.name = name;
             this.outputMode = outputMode;
@@ -207,7 +219,11 @@ public class PluginManagerCliParser extends CliTool {
         @Override
         public ExitStatus execute(Settings settings, Environment env) throws Exception {
             PluginManager pluginManager = new PluginManager(env, url, outputMode, timeout);
-            terminal.println("-> Installing " + Strings.nullToEmpty(name) + "...");
+            if (name != null) {
+                terminal.println("-> Installing " + Strings.nullToEmpty(name) + "...");
+            } else {
+                terminal.println("-> Installing from " + url + "...");
+            }
             pluginManager.downloadAndExtract(name, terminal);
             return ExitStatus.OK;
         }
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginsModule.java b/core/src/main/java/org/elasticsearch/plugins/PluginsModule.java
index 050a901..04e468c 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginsModule.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginsModule.java
@@ -20,36 +20,16 @@
 package org.elasticsearch.plugins;
 
 import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Module;
-import org.elasticsearch.common.inject.PreProcessModule;
-import org.elasticsearch.common.inject.SpawnModules;
-import org.elasticsearch.common.settings.Settings;
 
-/**
- *
- */
-public class PluginsModule extends AbstractModule implements SpawnModules, PreProcessModule {
-
-    private final Settings settings;
+public class PluginsModule extends AbstractModule {
 
     private final PluginsService pluginsService;
 
-    public PluginsModule(Settings settings, PluginsService pluginsService) {
-        this.settings = settings;
+    public PluginsModule(PluginsService pluginsService) {
         this.pluginsService = pluginsService;
     }
 
     @Override
-    public Iterable<? extends Module> spawnModules() {
-        return pluginsService.nodeModules();
-    }
-
-    @Override
-    public void processModule(Module module) {
-        pluginsService.processModule(module);
-    }
-
-    @Override
     protected void configure() {
         bind(PluginsService.class).toInstance(pluginsService);
     }
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginsService.java b/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
index bbe238b..8b3488e 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
@@ -19,11 +19,17 @@
 
 package org.elasticsearch.plugins;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 
+import org.apache.lucene.analysis.util.CharFilterFactory;
+import org.apache.lucene.analysis.util.TokenFilterFactory;
+import org.apache.lucene.analysis.util.TokenizerFactory;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.PostingsFormat;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.admin.cluster.node.info.PluginsInfo;
-import org.elasticsearch.bootstrap.Bootstrap;
 import org.elasticsearch.bootstrap.JarHell;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.collect.MapBuilder;
@@ -64,7 +70,7 @@ public class PluginsService extends AbstractComponent {
     /**
      * We keep around a list of plugins
      */
-    private final List<Tuple<PluginInfo, Plugin>> plugins;
+    private final ImmutableList<Tuple<PluginInfo, Plugin>> plugins;
     private final PluginsInfo info;
 
     private final ImmutableMap<Plugin, List<OnModuleReference>> onModuleReferences;
@@ -87,7 +93,7 @@ public class PluginsService extends AbstractComponent {
     public PluginsService(Settings settings, Environment environment) {
         super(settings);
 
-        List<Tuple<PluginInfo, Plugin>> tupleBuilder = new ArrayList<>();
+        ImmutableList.Builder<Tuple<PluginInfo, Plugin>> tupleBuilder = ImmutableList.builder();
 
         // first we load specified plugins via 'plugin.types' settings parameter.
         // this is a hack for what is between unit and integration tests...
@@ -106,10 +112,10 @@ public class PluginsService extends AbstractComponent {
           List<Bundle> bundles = getPluginBundles(environment);
           tupleBuilder.addAll(loadBundles(bundles));
         } catch (IOException ex) {
-          throw new IllegalStateException(ex);
+          throw new IllegalStateException("Unable to initialize plugins", ex);
         }
 
-        plugins = Collections.unmodifiableList(tupleBuilder);
+        plugins = tupleBuilder.build();
         info = new PluginsInfo();
         for (Tuple<PluginInfo, Plugin> tuple : plugins) {
             info.add(tuple.v1());
@@ -170,7 +176,7 @@ public class PluginsService extends AbstractComponent {
         this.onModuleReferences = onModuleReferences.immutableMap();
     }
 
-    public List<Tuple<PluginInfo, Plugin>> plugins() {
+    public ImmutableList<Tuple<PluginInfo, Plugin>> plugins() {
         return plugins;
     }
 
@@ -278,9 +284,10 @@ public class PluginsService extends AbstractComponent {
     }
 
     static List<Bundle> getPluginBundles(Environment environment) throws IOException {
-        ESLogger logger = Loggers.getLogger(Bootstrap.class);
+        ESLogger logger = Loggers.getLogger(PluginsService.class);
 
         Path pluginsDirectory = environment.pluginsFile();
+        // TODO: remove this leniency, but tests bogusly rely on it
         if (!isAccessibleDirectory(pluginsDirectory, logger)) {
             return Collections.emptyList();
         }
@@ -322,7 +329,7 @@ public class PluginsService extends AbstractComponent {
     }
 
     private List<Tuple<PluginInfo,Plugin>> loadBundles(List<Bundle> bundles) {
-        List<Tuple<PluginInfo, Plugin>> plugins = new ArrayList<>();
+        ImmutableList.Builder<Tuple<PluginInfo, Plugin>> plugins = ImmutableList.builder();
 
         for (Bundle bundle : bundles) {
             // jar-hell check the bundle against the parent classloader
@@ -346,6 +353,8 @@ public class PluginsService extends AbstractComponent {
             for (PluginInfo pluginInfo : bundle.plugins) {
                 final Plugin plugin;
                 if (pluginInfo.isJvm()) {
+                    // reload lucene SPI with any new services from the plugin
+                    reloadLuceneSPI(loader);
                     plugin = loadPlugin(pluginInfo.getClassname(), settings, loader);
                 } else {
                     plugin = new SitePlugin(pluginInfo.getName(), pluginInfo.getDescription());
@@ -354,7 +363,25 @@ public class PluginsService extends AbstractComponent {
             }
         }
 
-        return Collections.unmodifiableList(plugins);
+        return plugins.build();
+    }
+
+    /**
+     * Reloads all Lucene SPI implementations using the new classloader.
+     * This method must be called after the new classloader has been created to
+     * register the services for use.
+     */
+    static void reloadLuceneSPI(ClassLoader loader) {
+        // do NOT change the order of these method calls!
+
+        // Codecs:
+        PostingsFormat.reloadPostingsFormats(loader);
+        DocValuesFormat.reloadDocValuesFormats(loader);
+        Codec.reloadCodecs(loader);
+        // Analysis:
+        CharFilterFactory.reloadCharFilters(loader);
+        TokenFilterFactory.reloadTokenFilters(loader);
+        TokenizerFactory.reloadTokenizers(loader);
     }
 
     private Plugin loadPlugin(String className, Settings settings, ClassLoader loader) {
diff --git a/core/src/main/java/org/elasticsearch/plugins/ShardsPluginsModule.java b/core/src/main/java/org/elasticsearch/plugins/ShardsPluginsModule.java
deleted file mode 100644
index 5797b2d..0000000
--- a/core/src/main/java/org/elasticsearch/plugins/ShardsPluginsModule.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugins;
-
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Module;
-import org.elasticsearch.common.inject.PreProcessModule;
-import org.elasticsearch.common.inject.SpawnModules;
-import org.elasticsearch.common.settings.Settings;
-
-/**
- *
- */
-public class ShardsPluginsModule extends AbstractModule implements SpawnModules, PreProcessModule {
-
-    private final Settings settings;
-
-    private final PluginsService pluginsService;
-
-    public ShardsPluginsModule(Settings settings, PluginsService pluginsService) {
-        this.settings = settings;
-        this.pluginsService = pluginsService;
-    }
-
-    @Override
-    public Iterable<? extends Module> spawnModules() {
-        return pluginsService.shardModules(settings);
-    }
-
-    @Override
-    public void processModule(Module module) {
-        pluginsService.processModule(module);
-    }
-
-    @Override
-    protected void configure() {
-    }
-}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/repositories/RepositoryModule.java b/core/src/main/java/org/elasticsearch/repositories/RepositoryModule.java
index eca82cc..3ed3a78 100644
--- a/core/src/main/java/org/elasticsearch/repositories/RepositoryModule.java
+++ b/core/src/main/java/org/elasticsearch/repositories/RepositoryModule.java
@@ -20,14 +20,8 @@
 package org.elasticsearch.repositories;
 
 import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Module;
-import org.elasticsearch.common.inject.Modules;
-import org.elasticsearch.common.inject.SpawnModules;
 import org.elasticsearch.common.settings.Settings;
 
-import java.util.Arrays;
-import java.util.Collections;
-
 /**
  * Binds repository classes for the specific repository type.
  */
diff --git a/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java b/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java
index ff67456..3e9fbaa 100644
--- a/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java
+++ b/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.repositories.blobstore;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.io.ByteStreams;
 import org.apache.lucene.store.RateLimiter;
 import org.elasticsearch.ElasticsearchParseException;
@@ -317,13 +318,13 @@ public abstract class BlobStoreRepository extends AbstractLifecycleComponent<Rep
             // Delete snapshot from the snapshot list
             List<SnapshotId> snapshotIds = snapshots();
             if (snapshotIds.contains(snapshotId)) {
-                List<SnapshotId> builder = new ArrayList<>();
+                ImmutableList.Builder<SnapshotId> builder = ImmutableList.builder();
                 for (SnapshotId id : snapshotIds) {
                     if (!snapshotId.equals(id)) {
                         builder.add(id);
                     }
                 }
-                snapshotIds = Collections.unmodifiableList(builder);
+                snapshotIds = builder.build();
             }
             writeSnapshotList(snapshotIds);
             // Now delete all indices
@@ -364,9 +365,7 @@ public abstract class BlobStoreRepository extends AbstractLifecycleComponent<Rep
             snapshotFormat.write(blobStoreSnapshot, snapshotsBlobContainer, snapshotId.getSnapshot());
             List<SnapshotId> snapshotIds = snapshots();
             if (!snapshotIds.contains(snapshotId)) {
-                snapshotIds = new ArrayList<>(snapshotIds);
-                snapshotIds.add(snapshotId);
-                snapshotIds = Collections.unmodifiableList(snapshotIds);
+                snapshotIds = ImmutableList.<SnapshotId>builder().addAll(snapshotIds).add(snapshotId).build();
             }
             writeSnapshotList(snapshotIds);
             return blobStoreSnapshot;
@@ -405,7 +404,7 @@ public abstract class BlobStoreRepository extends AbstractLifecycleComponent<Rep
                 }
                 snapshots.add(new SnapshotId(repositoryName, name));
             }
-            return Collections.unmodifiableList(snapshots);
+            return ImmutableList.copyOf(snapshots);
         } catch (IOException ex) {
             throw new RepositoryException(repositoryName, "failed to list snapshots in repository", ex);
         }
@@ -596,7 +595,7 @@ public abstract class BlobStoreRepository extends AbstractLifecycleComponent<Rep
                     }
                 }
             }
-            return Collections.unmodifiableList(snapshots);
+            return ImmutableList.copyOf(snapshots);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/repositories/uri/URLRepository.java b/core/src/main/java/org/elasticsearch/repositories/uri/URLRepository.java
index f3c439b..42a27c1 100644
--- a/core/src/main/java/org/elasticsearch/repositories/uri/URLRepository.java
+++ b/core/src/main/java/org/elasticsearch/repositories/uri/URLRepository.java
@@ -156,7 +156,7 @@ public class URLRepository extends BlobStoreRepository {
                     logger.warn("cannot parse the specified url [{}]", url);
                     throw new RepositoryException(repositoryName, "cannot parse the specified url [" + url + "]");
                 }
-                // We didn't match white list - try to resolve against repo.path
+                // We didn't match white list - try to resolve against path.repo
                 URL normalizedUrl = environment.resolveRepoURL(url);
                 if (normalizedUrl == null) {
                     logger.warn("The specified url [{}] doesn't start with any repository paths specified by the path.repo setting: [{}] or by repositories.url.allowed_urls setting: [{}] ", url, environment.repoFiles());
diff --git a/core/src/main/java/org/elasticsearch/rest/BytesRestResponse.java b/core/src/main/java/org/elasticsearch/rest/BytesRestResponse.java
index b64c9c5..fc944f4 100644
--- a/core/src/main/java/org/elasticsearch/rest/BytesRestResponse.java
+++ b/core/src/main/java/org/elasticsearch/rest/BytesRestResponse.java
@@ -25,6 +25,8 @@ import org.elasticsearch.bootstrap.Elasticsearch;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.collect.Tuple;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.ESLoggerFactory;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
@@ -115,11 +117,20 @@ public class BytesRestResponse extends RestResponse {
         return this.status;
     }
 
+    private static final ESLogger SUPPRESSED_ERROR_LOGGER = ESLoggerFactory.getLogger("rest.suppressed");
+
     private static XContentBuilder convert(RestChannel channel, RestStatus status, Throwable t) throws IOException {
         XContentBuilder builder = channel.newErrorBuilder().startObject();
         if (t == null) {
             builder.field("error", "unknown");
         } else if (channel.detailedErrorsEnabled()) {
+            final ToXContent.Params params;
+            if (channel.request().paramAsBoolean("error_trace", !ElasticsearchException.REST_EXCEPTION_SKIP_STACK_TRACE_DEFAULT)) {
+                params =  new ToXContent.DelegatingMapParams(Collections.singletonMap(ElasticsearchException.REST_EXCEPTION_SKIP_STACK_TRACE, "false"), channel.request());
+            } else {
+                SUPPRESSED_ERROR_LOGGER.info("{} Params: {}", t, channel.request().path(), channel.request().params());
+                params = channel.request();
+            }
             builder.field("error");
             builder.startObject();
             final ElasticsearchException[] rootCauses = ElasticsearchException.guessRootCauses(t);
@@ -127,16 +138,13 @@ public class BytesRestResponse extends RestResponse {
             builder.startArray();
             for (ElasticsearchException rootCause : rootCauses){
                 builder.startObject();
-                rootCause.toXContent(builder, new ToXContent.DelegatingMapParams(Collections.singletonMap(ElasticsearchException.REST_EXCEPTION_SKIP_CAUSE, "true"), channel.request()));
+                rootCause.toXContent(builder, new ToXContent.DelegatingMapParams(Collections.singletonMap(ElasticsearchException.REST_EXCEPTION_SKIP_CAUSE, "true"), params));
                 builder.endObject();
             }
             builder.endArray();
 
-            ElasticsearchException.toXContent(builder, channel.request(), t);
+            ElasticsearchException.toXContent(builder, params, t);
             builder.endObject();
-            if (channel.request().paramAsBoolean("error_trace", false)) {
-                buildErrorTrace(t, builder);
-            }
         } else {
             builder.field("error", simpleMessage(t));
         }
@@ -145,45 +153,6 @@ public class BytesRestResponse extends RestResponse {
         return builder;
     }
 
-
-    private static void buildErrorTrace(Throwable t, XContentBuilder builder) throws IOException {
-        builder.startObject("error_trace");
-        boolean first = true;
-        int counter = 0;
-        while (t != null) {
-            // bail if there are more than 10 levels, becomes useless really...
-            if (counter++ > 10) {
-                break;
-            }
-            if (!first) {
-                builder.startObject("cause");
-            }
-            buildThrowable(t, builder);
-            if (!first) {
-                builder.endObject();
-            }
-            t = t.getCause();
-            first = false;
-        }
-        builder.endObject();
-    }
-
-    private static void buildThrowable(Throwable t, XContentBuilder builder) throws IOException {
-        builder.field("message", t.getMessage());
-        for (StackTraceElement stElement : t.getStackTrace()) {
-            builder.startObject("at")
-                    .field("class", stElement.getClassName())
-                    .field("method", stElement.getMethodName());
-            if (stElement.getFileName() != null) {
-                builder.field("file", stElement.getFileName());
-            }
-            if (stElement.getLineNumber() >= 0) {
-                builder.field("line", stElement.getLineNumber());
-            }
-            builder.endObject();
-        }
-    }
-
     /*
      * Builds a simple error string from the message of the first ElasticsearchException
      */
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/get/RestGetIndicesAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/get/RestGetIndicesAction.java
index da2130a..e4ff660 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/get/RestGetIndicesAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/get/RestGetIndicesAction.java
@@ -19,6 +19,7 @@
 package org.elasticsearch.rest.action.admin.indices.get;
 
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+import com.google.common.collect.ImmutableList;
 
 import org.elasticsearch.action.admin.indices.get.GetIndexRequest;
 import org.elasticsearch.action.admin.indices.get.GetIndexRequest.Feature;
@@ -45,7 +46,6 @@ import org.elasticsearch.rest.action.support.RestBuilderListener;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 
 import java.io.IOException;
-import java.util.List;
 
 import static org.elasticsearch.rest.RestRequest.Method.GET;
 import static org.elasticsearch.rest.RestStatus.OK;
@@ -117,7 +117,7 @@ public class RestGetIndicesAction extends BaseRestHandler {
                 return new BytesRestResponse(OK, builder);
             }
 
-            private void writeAliases(List<AliasMetaData> aliases, XContentBuilder builder, Params params) throws IOException {
+            private void writeAliases(ImmutableList<AliasMetaData> aliases, XContentBuilder builder, Params params) throws IOException {
                 builder.startObject(Fields.ALIASES);
                 if (aliases != null) {
                     for (AliasMetaData alias : aliases) {
@@ -144,7 +144,7 @@ public class RestGetIndicesAction extends BaseRestHandler {
                 builder.endObject();
             }
 
-            private void writeWarmers(List<IndexWarmersMetaData.Entry> warmers, XContentBuilder builder, Params params) throws IOException {
+            private void writeWarmers(ImmutableList<IndexWarmersMetaData.Entry> warmers, XContentBuilder builder, Params params) throws IOException {
                 builder.startObject(Fields.WARMERS);
                 if (warmers != null) {
                     for (IndexWarmersMetaData.Entry warmer : warmers) {
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/get/RestGetWarmerAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/get/RestGetWarmerAction.java
index 67e0101..be83ccb 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/get/RestGetWarmerAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/get/RestGetWarmerAction.java
@@ -19,6 +19,7 @@
 package org.elasticsearch.rest.action.admin.indices.warmer.get;
 
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.action.admin.indices.warmer.get.GetWarmersRequest;
 import org.elasticsearch.action.admin.indices.warmer.get.GetWarmersResponse;
 import org.elasticsearch.action.support.IndicesOptions;
@@ -31,8 +32,6 @@ import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestBuilderListener;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 
-import java.util.List;
-
 import static org.elasticsearch.rest.RestRequest.Method.GET;
 import static org.elasticsearch.rest.RestStatus.OK;
 
@@ -69,7 +68,7 @@ public class RestGetWarmerAction extends BaseRestHandler {
                 }
 
                 builder.startObject();
-                for (ObjectObjectCursor<String, List<IndexWarmersMetaData.Entry>> entry : response.warmers()) {
+                for (ObjectObjectCursor<String, ImmutableList<IndexWarmersMetaData.Entry>> entry : response.warmers()) {
                     builder.startObject(entry.key, XContentBuilder.FieldCaseConversion.NONE);
                     builder.startObject(IndexWarmersMetaData.TYPE, XContentBuilder.FieldCaseConversion.NONE);
                     for (IndexWarmersMetaData.Entry warmerEntry : entry.value) {
diff --git a/core/src/main/java/org/elasticsearch/search/SearchService.java b/core/src/main/java/org/elasticsearch/search/SearchService.java
index 4beacda..15eb3c0 100644
--- a/core/src/main/java/org/elasticsearch/search/SearchService.java
+++ b/core/src/main/java/org/elasticsearch/search/SearchService.java
@@ -26,7 +26,6 @@ import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.IndexOptions;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.search.QueryCachingPolicy;
 import org.apache.lucene.search.TopDocs;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ElasticsearchParseException;
@@ -54,7 +53,6 @@ import org.elasticsearch.common.xcontent.XContentLocation;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.Index;
 import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.cache.IndexCache;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.fielddata.FieldDataType;
 import org.elasticsearch.index.fielddata.IndexFieldData;
@@ -82,7 +80,6 @@ import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.search.dfs.CachedDfSource;
 import org.elasticsearch.search.dfs.DfsPhase;
 import org.elasticsearch.search.dfs.DfsSearchResult;
 import org.elasticsearch.search.fetch.*;
@@ -266,6 +263,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
         }
     }
 
+    @Deprecated // remove in 3.0
     public QuerySearchResult executeScan(ShardSearchRequest request) {
         final SearchContext context = createAndPutContext(request);
         final int originalSize = context.size();
@@ -274,7 +272,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
                 throw new IllegalArgumentException("aggregations are not supported with search_type=scan");
             }
 
-            if (context.scroll() == null) {
+            if (context.scrollContext() == null || context.scrollContext().scroll == null) {
                 throw new ElasticsearchException("Scroll must be provided when scanning...");
             }
 
@@ -322,7 +320,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
             try {
                 shortcutDocIdsToLoadForScanning(context);
                 fetchPhase.execute(context);
-                if (context.scroll() == null || context.fetchResult().hits().hits().length < context.size()) {
+                if (context.scrollContext() == null || context.fetchResult().hits().hits().length < context.size()) {
                     freeContext(request.id());
                 } else {
                     contextProcessedSuccessfully(context);
@@ -365,7 +363,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
 
             loadOrExecuteQueryPhase(request, context, queryPhase);
 
-            if (context.queryResult().topDocs().scoreDocs.length == 0 && context.scroll() == null) {
+            if (context.queryResult().topDocs().scoreDocs.length == 0 && context.scrollContext() == null) {
                 freeContext(context.id());
             } else {
                 contextProcessedSuccessfully(context);
@@ -412,23 +410,14 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
     public QuerySearchResult executeQueryPhase(QuerySearchRequest request) {
         final SearchContext context = findContext(request.id());
         contextProcessing(context);
+        context.searcher().setAggregatedDfs(request.dfs());
         IndexShard indexShard = context.indexShard();
-        try {
-            final IndexCache indexCache = indexShard.indexService().cache();
-            final QueryCachingPolicy cachingPolicy = indexShard.getQueryCachingPolicy();
-            context.searcher().dfSource(new CachedDfSource(context.searcher().getIndexReader(), request.dfs(), context.similarityService().similarity(),
-                    indexCache.query(), cachingPolicy));
-        } catch (Throwable e) {
-            processFailure(context, e);
-            cleanContext(context);
-            throw new QueryPhaseExecutionException(context, "Failed to set aggregated df", e);
-        }
         ShardSearchStats shardSearchStats = indexShard.searchService();
         try {
             shardSearchStats.onPreQueryPhase(context);
             long time = System.nanoTime();
             queryPhase.execute(context);
-            if (context.queryResult().topDocs().scoreDocs.length == 0 && context.scroll() == null) {
+            if (context.queryResult().topDocs().scoreDocs.length == 0 && context.scrollContext() == null) {
                 // no hits, we can release the context since there will be no fetch phase
                 freeContext(context.id());
             } else {
@@ -446,6 +435,16 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
         }
     }
 
+    private boolean fetchPhaseShouldFreeContext(SearchContext context) {
+        if (context.scrollContext() == null) {
+            // simple search, no scroll
+            return true;
+        } else {
+            // scroll request, but the scroll was not extended
+            return context.scrollContext().scroll == null;
+        }
+    }
+
     public QueryFetchSearchResult executeFetchPhase(ShardSearchRequest request) {
         final SearchContext context = createAndPutContext(request);
         contextProcessing(context);
@@ -465,7 +464,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
             try {
                 shortcutDocIdsToLoad(context);
                 fetchPhase.execute(context);
-                if (context.scroll() == null) {
+                if (fetchPhaseShouldFreeContext(context)) {
                     freeContext(context.id());
                 } else {
                     contextProcessedSuccessfully(context);
@@ -488,17 +487,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
     public QueryFetchSearchResult executeFetchPhase(QuerySearchRequest request) {
         final SearchContext context = findContext(request.id());
         contextProcessing(context);
-        try {
-            final IndexShard indexShard = context.indexShard();
-            final IndexCache indexCache = indexShard.indexService().cache();
-            final QueryCachingPolicy cachingPolicy = indexShard.getQueryCachingPolicy();
-            context.searcher().dfSource(new CachedDfSource(context.searcher().getIndexReader(), request.dfs(), context.similarityService().similarity(),
-                    indexCache.query(), cachingPolicy));
-        } catch (Throwable e) {
-            freeContext(context.id());
-            cleanContext(context);
-            throw new QueryPhaseExecutionException(context, "Failed to set aggregated df", e);
-        }
+        context.searcher().setAggregatedDfs(request.dfs());
         try {
             ShardSearchStats shardSearchStats = context.indexShard().searchService();
             shardSearchStats.onPreQueryPhase(context);
@@ -515,7 +504,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
             try {
                 shortcutDocIdsToLoad(context);
                 fetchPhase.execute(context);
-                if (context.scroll() == null) {
+                if (fetchPhaseShouldFreeContext(context)) {
                     freeContext(request.id());
                 } else {
                     contextProcessedSuccessfully(context);
@@ -555,7 +544,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
             try {
                 shortcutDocIdsToLoad(context);
                 fetchPhase.execute(context);
-                if (context.scroll() == null) {
+                if (fetchPhaseShouldFreeContext(context)) {
                     freeContext(request.id());
                 } else {
                     contextProcessedSuccessfully(context);
@@ -581,13 +570,13 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
         final ShardSearchStats shardSearchStats = context.indexShard().searchService();
         try {
             if (request.lastEmittedDoc() != null) {
-                context.lastEmittedDoc(request.lastEmittedDoc());
+                context.scrollContext().lastEmittedDoc = request.lastEmittedDoc();
             }
             context.docIdsToLoad(request.docIds(), 0, request.docIdsSize());
             shardSearchStats.onPreFetchPhase(context);
             long time = System.nanoTime();
             fetchPhase.execute(context);
-            if (context.scroll() == null) {
+            if (fetchPhaseShouldFreeContext(context)) {
                 freeContext(request.id());
             } else {
                 contextProcessedSuccessfully(context);
@@ -642,7 +631,10 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
         SearchContext context = new DefaultSearchContext(idGenerator.incrementAndGet(), request, shardTarget, engineSearcher, indexService, indexShard, scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher, defaultSearchTimeout);
         SearchContext.setCurrent(context);
         try {
-            context.scroll(request.scroll());
+            if (request.scroll() != null) {
+                context.scrollContext(new ScrollContext());
+                context.scrollContext().scroll = request.scroll();
+            }
 
             parseTemplate(request, context);
             parseSource(context, request.source());
@@ -695,7 +687,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
         if (context != null) {
             try {
                 context.indexShard().searchService().onFreeContext(context);
-                if (context.scroll() != null) {
+                if (context.scrollContext() != null) {
                     context.indexShard().searchService().onFreeScrollContext(context);
                 }
             } finally {
@@ -708,7 +700,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
 
     public void freeAllScrollContexts() {
         for (SearchContext searchContext : activeContexts.values()) {
-            if (searchContext.scroll() != null) {
+            if (searchContext.scrollContext() != null) {
                 freeContext(searchContext.id());
             }
         }
@@ -902,7 +894,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
     private void processScroll(InternalScrollSearchRequest request, SearchContext context) {
         // process scroll
         context.from(context.from() + context.size());
-        context.scroll(request.scroll());
+        context.scrollContext().scroll = request.scroll();
         // update the context keep alive based on the new scroll value
         if (request.scroll() != null && request.scroll().keepAlive() != null) {
             context.keepAlive(request.scroll().keepAlive().millis());
diff --git a/core/src/main/java/org/elasticsearch/search/action/SearchServiceTransportAction.java b/core/src/main/java/org/elasticsearch/search/action/SearchServiceTransportAction.java
index 5730a02..4205fd9 100644
--- a/core/src/main/java/org/elasticsearch/search/action/SearchServiceTransportAction.java
+++ b/core/src/main/java/org/elasticsearch/search/action/SearchServiceTransportAction.java
@@ -418,6 +418,7 @@ public class SearchServiceTransportAction extends AbstractComponent {
         }
     }
 
+    @Deprecated // remove in 3.0
     class SearchScanTransportHandler implements TransportRequestHandler<ShardSearchTransportRequest> {
         @Override
         public void messageReceived(ShardSearchTransportRequest request, TransportChannel channel) throws Exception {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/InternalAggregation.java b/core/src/main/java/org/elasticsearch/search/aggregations/InternalAggregation.java
index d92e959..1f32c0a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/InternalAggregation.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/InternalAggregation.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.search.aggregations;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 
 import org.elasticsearch.common.bytes.BytesArray;
@@ -35,7 +36,6 @@ import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
 import org.elasticsearch.search.aggregations.support.AggregationPath;
 
 import java.io.IOException;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 
@@ -226,7 +226,7 @@ public abstract class InternalAggregation implements Aggregation, ToXContent, St
         metaData = in.readMap();
         int size = in.readVInt();
         if (size == 0) {
-            pipelineAggregators = Collections.emptyList();
+            pipelineAggregators = ImmutableList.of();
         } else {
             pipelineAggregators = Lists.newArrayListWithCapacity(size);
             for (int i = 0; i < size; i++) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java b/core/src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java
index 960c382..ceefcae 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java
@@ -19,6 +19,7 @@
 package org.elasticsearch.search.aggregations;
 
 import com.google.common.base.Function;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Iterators;
 import com.google.common.collect.Lists;
@@ -36,7 +37,6 @@ import org.elasticsearch.search.aggregations.support.AggregationPath;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
@@ -57,7 +57,7 @@ public class InternalAggregations implements Aggregations, ToXContent, Streamabl
         }
     };
 
-    private List<InternalAggregation> aggregations = Collections.emptyList();
+    private List<InternalAggregation> aggregations = ImmutableList.of();
 
     private Map<String, InternalAggregation> aggregationsAsMap;
 
@@ -212,7 +212,7 @@ public class InternalAggregations implements Aggregations, ToXContent, Streamabl
     public void readFrom(StreamInput in) throws IOException {
         int size = in.readVInt();
         if (size == 0) {
-            aggregations = Collections.emptyList();
+            aggregations = ImmutableList.of();
             aggregationsAsMap = ImmutableMap.of();
         } else {
             aggregations = Lists.newArrayListWithCapacity(size);
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
index 1e660f0..1b2d5fc 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
@@ -31,7 +31,7 @@ import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
-final class CardinalityAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource> {
+final class CardinalityAggregatorFactory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource> {
 
     private final long precisionThreshold;
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationPath.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationPath.java
index 14d6c5e..85af60a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationPath.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationPath.java
@@ -186,9 +186,8 @@ public class AggregationPath {
     }
 
     public AggregationPath subPath(int offset, int length) {
-        PathElement[] subTokens = new PathElement[length];
-        System.arraycopy(pathElements, offset, subTokens, 0, length);
-        return new AggregationPath(pathElements);
+        List<PathElement> subTokens = new ArrayList<>(pathElements.subList(offset, offset + length));
+        return new AggregationPath(subTokens);
     }
 
     /**
@@ -266,12 +265,12 @@ public class AggregationPath {
         }
         return aggregator;
     }
-    
+
     /**
      * Resolves the topmost aggregator pointed by this path using the given root as a point of reference.
      *
      * @param root      The point of reference of this path
-     * @return          The first child aggregator of the root pointed by this path 
+     * @return          The first child aggregator of the root pointed by this path
      */
     public Aggregator resolveTopmostAggregator(Aggregator root) {
         AggregationPath.PathElement token = pathElements.get(0);
@@ -279,7 +278,7 @@ public class AggregationPath {
         assert (aggregator instanceof SingleBucketAggregator )
                 || (aggregator instanceof NumericMetricsAggregator) : "this should be picked up before aggregation execution - on validate";
         return aggregator;
-    }    
+    }
 
     /**
      * Validates this path over the given aggregator as a point of reference.
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormat.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormat.java
index 33cf9cc..9a3be56 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormat.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormat.java
@@ -69,14 +69,14 @@ public class ValueFormat {
         public static final DateTime DEFAULT = new DateTime(DateFieldMapper.Defaults.DATE_TIME_FORMATTER.format(), ValueFormatter.DateTime.DEFAULT, ValueParser.DateMath.DEFAULT);
 
         public static DateTime format(String format, DateTimeZone timezone) {
-            return new DateTime(format, new ValueFormatter.DateTime(format, timezone), new ValueParser.DateMath(format));
+            return new DateTime(format, new ValueFormatter.DateTime(format, timezone), new ValueParser.DateMath(format, timezone));
         }
 
         public static DateTime mapper(DateFieldMapper.DateFieldType fieldType, DateTimeZone timezone) {
-            return new DateTime(fieldType.dateTimeFormatter().format(), ValueFormatter.DateTime.mapper(fieldType, timezone), ValueParser.DateMath.mapper(fieldType));
+            return new DateTime(fieldType.dateTimeFormatter().format(), ValueFormatter.DateTime.mapper(fieldType, timezone), ValueParser.DateMath.mapper(fieldType, timezone));
         }
 
-        public DateTime(String pattern, ValueFormatter formatter, ValueParser parser) {
+        private DateTime(String pattern, ValueFormatter formatter, ValueParser parser) {
             super(pattern, formatter, parser);
         }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueParser.java
index c650244..acd88f7 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueParser.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.search.aggregations.support.format;
 
+import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.joda.DateMathParser;
 import org.elasticsearch.common.joda.FormatDateTimeFormatter;
 import org.elasticsearch.common.joda.Joda;
@@ -25,6 +26,7 @@ import org.elasticsearch.index.mapper.core.DateFieldMapper;
 import org.elasticsearch.index.mapper.ip.IpFieldMapper;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.internal.SearchContext;
+import org.joda.time.DateTimeZone;
 
 import java.text.DecimalFormat;
 import java.text.DecimalFormatSymbols;
@@ -80,16 +82,21 @@ public interface ValueParser {
      */
     static class DateMath implements ValueParser {
 
-        public static final DateMath DEFAULT = new ValueParser.DateMath(new DateMathParser(DateFieldMapper.Defaults.DATE_TIME_FORMATTER));
+        public static final DateMath DEFAULT = new ValueParser.DateMath(new DateMathParser(DateFieldMapper.Defaults.DATE_TIME_FORMATTER), DateTimeZone.UTC);
 
         private DateMathParser parser;
 
-        public DateMath(String format) {
-            this(new DateMathParser(Joda.forPattern(format)));
+        private DateTimeZone timezone = DateTimeZone.UTC;
+
+        public DateMath(String format, DateTimeZone timezone) {
+            this(new DateMathParser(Joda.forPattern(format)), timezone);
         }
 
-        public DateMath(DateMathParser parser) {
+        public DateMath(DateMathParser parser, @Nullable DateTimeZone timeZone) {
             this.parser = parser;
+            if (timeZone != null) {
+                this.timezone = timeZone;
+            }
         }
 
         @Override
@@ -100,7 +107,7 @@ public interface ValueParser {
                     return searchContext.nowInMillis();
                 }
             };
-            return parser.parse(value, now);
+            return parser.parse(value, now, false, timezone);
         }
 
         @Override
@@ -108,8 +115,8 @@ public interface ValueParser {
             return parseLong(value, searchContext);
         }
 
-        public static DateMath mapper(DateFieldMapper.DateFieldType fieldType) {
-            return new DateMath(new DateMathParser(fieldType.dateTimeFormatter()));
+        public static DateMath mapper(DateFieldMapper.DateFieldType fieldType, @Nullable DateTimeZone timezone) {
+            return new DateMath(new DateMathParser(fieldType.dateTimeFormatter()), timezone);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
index a70dd8d..0eb4c5c 100644
--- a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
@@ -21,6 +21,7 @@ package org.elasticsearch.search.builder;
 
 import com.carrotsearch.hppc.ObjectFloatHashMap;
 import com.google.common.base.Charsets;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 
 import org.elasticsearch.ElasticsearchGenerationException;
@@ -538,7 +539,7 @@ public class SearchSourceBuilder extends ToXContentToBytes {
      * per field.
      */
     public SearchSourceBuilder noFields() {
-        this.fieldNames = Collections.emptyList();
+        this.fieldNames = ImmutableList.of();
         return this;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/dfs/CachedDfSource.java b/core/src/main/java/org/elasticsearch/search/dfs/CachedDfSource.java
deleted file mode 100644
index dbd66ab..0000000
--- a/core/src/main/java/org/elasticsearch/search/dfs/CachedDfSource.java
+++ /dev/null
@@ -1,97 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.dfs;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.*;
-import org.apache.lucene.search.*;
-import org.apache.lucene.search.similarities.Similarity;
-
-import java.io.IOException;
-import java.util.List;
-
-/**
- *
- */
-public class CachedDfSource extends IndexSearcher {
-
-    private final AggregatedDfs aggregatedDfs;
-
-    private final int maxDoc;
-
-    public CachedDfSource(IndexReader reader, AggregatedDfs aggregatedDfs, Similarity similarity,
-            QueryCache queryCache, QueryCachingPolicy queryCachingPolicy) throws IOException {
-        super(reader);
-        this.aggregatedDfs = aggregatedDfs;
-        setSimilarity(similarity);
-        setQueryCache(queryCache);
-        setQueryCachingPolicy(queryCachingPolicy);
-        if (aggregatedDfs.maxDoc() > Integer.MAX_VALUE) {
-            maxDoc = Integer.MAX_VALUE;
-        } else {
-            maxDoc = (int) aggregatedDfs.maxDoc();
-        }
-    }
-
-
-    @Override
-    public TermStatistics termStatistics(Term term, TermContext context) throws IOException {
-        TermStatistics termStatistics = aggregatedDfs.termStatistics().get(term);
-        if (termStatistics == null) {
-            // we don't have stats for this - this might be a must_not clauses etc. that doesn't allow extract terms on the query
-           return super.termStatistics(term, context);
-        }
-        return termStatistics;
-    }
-
-    @Override
-    public CollectionStatistics collectionStatistics(String field) throws IOException {
-        CollectionStatistics collectionStatistics = aggregatedDfs.fieldStatistics().get(field);
-        if (collectionStatistics == null) {
-            // we don't have stats for this - this might be a must_not clauses etc. that doesn't allow extract terms on the query
-           return super.collectionStatistics(field);
-        }
-        return collectionStatistics;
-    }
-    
-    public int maxDoc() {
-        return this.maxDoc;
-    }
-
-    @Override
-    public Document doc(int i) {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public void doc(int docID, StoredFieldVisitor fieldVisitor) throws IOException {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public Explanation explain(Weight weight, int doc) {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    protected void search(List<LeafReaderContext> leaves, Weight weight, Collector collector) throws IOException {
-        throw new UnsupportedOperationException();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java
index ee391ad..81f13c3 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.search.fetch;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.ReaderUtil;
@@ -66,7 +67,6 @@ import org.elasticsearch.search.lookup.SourceLookup;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -294,7 +294,7 @@ public class FetchPhase implements SearchPhase {
                     nestedParsedSource = (List<Map<String, Object>>) extractedValue;
                 } else if (extractedValue instanceof Map) {
                     // nested field has an object value in the _source. This just means the nested field has just one inner object, which is valid, but uncommon.
-                    nestedParsedSource = Collections.singletonList((Map<String, Object>) extractedValue);
+                    nestedParsedSource = ImmutableList.of((Map < String, Object >) extractedValue);
                 } else {
                     throw new IllegalStateException("extracted source isn't an object or an array");
                 }
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java b/core/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java
index 07e931c..5b9ab72 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.search.highlight;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.component.AbstractComponent;
@@ -33,10 +34,7 @@ import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.internal.InternalSearchHit;
 import org.elasticsearch.search.internal.SearchContext;
 
-import java.util.Arrays;
 import java.util.Collection;
-import java.util.Collections;
-import java.util.List;
 import java.util.Map;
 
 import static com.google.common.collect.Maps.newHashMap;
@@ -46,7 +44,7 @@ import static com.google.common.collect.Maps.newHashMap;
  */
 public class HighlightPhase extends AbstractComponent implements FetchSubPhase {
 
-    private static final List<String> STANDARD_HIGHLIGHTERS_BY_PRECEDENCE = Arrays.asList("fvh", "postings", "plain");
+    private static final ImmutableList<String> STANDARD_HIGHLIGHTERS_BY_PRECEDENCE = ImmutableList.of("fvh", "postings", "plain");
 
     private final Highlighters highlighters;
 
@@ -84,7 +82,7 @@ public class HighlightPhase extends AbstractComponent implements FetchSubPhase {
                 DocumentMapper documentMapper = context.mapperService().documentMapper(hitContext.hit().type());
                 fieldNamesToHighlight = documentMapper.mappers().simpleMatchToFullName(field.field());
             } else {
-                fieldNamesToHighlight = Collections.singletonList(field.field());
+                fieldNamesToHighlight = ImmutableList.of(field.field());
             }
 
             if (context.highlight().forceSource(field)) {
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/HighlightUtils.java b/core/src/main/java/org/elasticsearch/search/highlight/HighlightUtils.java
index b26be72..3358aec 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/HighlightUtils.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/HighlightUtils.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.search.highlight;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
 import org.apache.lucene.search.highlight.DefaultEncoder;
 import org.apache.lucene.search.highlight.Encoder;
@@ -29,7 +30,6 @@ import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.lookup.SourceLookup;
 
 import java.io.IOException;
-import java.util.Collections;
 import java.util.List;
 
 public final class HighlightUtils {
@@ -52,7 +52,7 @@ public final class HighlightUtils {
             textsToHighlight = fieldVisitor.fields().get(mapper.fieldType().names().indexName());
             if (textsToHighlight == null) {
                 // Can happen if the document doesn't have the field to highlight
-                textsToHighlight = Collections.emptyList();
+                textsToHighlight = ImmutableList.of();
             }
         } else {
             SourceLookup sourceLookup = searchContext.lookup().source();
diff --git a/core/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java b/core/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java
index 11ce914..2f55bf8 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java
@@ -20,15 +20,13 @@
 package org.elasticsearch.search.internal;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.Explanation;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.Weight;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermContext;
+import org.apache.lucene.search.*;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.index.engine.Engine;
-import org.elasticsearch.search.dfs.CachedDfSource;
+import org.elasticsearch.search.dfs.AggregatedDfs;
 import org.elasticsearch.search.internal.SearchContext.Lifetime;
 
 import java.io.IOException;
@@ -46,21 +44,23 @@ public class ContextIndexSearcher extends IndexSearcher implements Releasable {
 
     private final SearchContext searchContext;
 
-    private CachedDfSource dfSource;
+    private AggregatedDfs aggregatedDfs;
 
     public ContextIndexSearcher(SearchContext searchContext, Engine.Searcher searcher) {
         super(searcher.reader());
         in = searcher.searcher();
         this.searchContext = searchContext;
         setSimilarity(searcher.searcher().getSimilarity(true));
+        setQueryCache(searchContext.indexShard().indexService().cache().query());
+        setQueryCachingPolicy(searchContext.indexShard().getQueryCachingPolicy());
     }
 
     @Override
     public void close() {
     }
 
-    public void dfSource(CachedDfSource dfSource) {
-        this.dfSource = dfSource;
+    public void setAggregatedDfs(AggregatedDfs aggregatedDfs) {
+        this.aggregatedDfs = aggregatedDfs;
     }
 
     @Override
@@ -75,10 +75,12 @@ public class ContextIndexSearcher extends IndexSearcher implements Releasable {
 
     @Override
     public Weight createNormalizedWeight(Query query, boolean needsScores) throws IOException {
+        // During tests we prefer to use the wrapped IndexSearcher, because then we use the AssertingIndexSearcher
+        // it is hacky, because if we perform a dfs search, we don't use the wrapped IndexSearcher...
         try {
             // if scores are needed and we have dfs data then use it
-            if (dfSource != null && needsScores) {
-                return dfSource.createNormalizedWeight(query, needsScores);
+            if (aggregatedDfs != null && needsScores) {
+                return super.createNormalizedWeight(query, needsScores);
             }
             return in.createNormalizedWeight(query, needsScores);
         } catch (Throwable t) {
@@ -104,4 +106,32 @@ public class ContextIndexSearcher extends IndexSearcher implements Releasable {
             searchContext.clearReleasables(Lifetime.COLLECTION);
         }
     }
+
+    @Override
+    public TermStatistics termStatistics(Term term, TermContext context) throws IOException {
+        if (aggregatedDfs == null) {
+            // we are either executing the dfs phase or the search_type doesn't include the dfs phase.
+            return super.termStatistics(term, context);
+        }
+        TermStatistics termStatistics = aggregatedDfs.termStatistics().get(term);
+        if (termStatistics == null) {
+            // we don't have stats for this - this might be a must_not clauses etc. that doesn't allow extract terms on the query
+            return super.termStatistics(term, context);
+        }
+        return termStatistics;
+    }
+
+    @Override
+    public CollectionStatistics collectionStatistics(String field) throws IOException {
+        if (aggregatedDfs == null) {
+            // we are either executing the dfs phase or the search_type doesn't include the dfs phase.
+            return super.collectionStatistics(field);
+        }
+        CollectionStatistics collectionStatistics = aggregatedDfs.fieldStatistics().get(field);
+        if (collectionStatistics == null) {
+            // we don't have stats for this - this might be a must_not clauses etc. that doesn't allow extract terms on the query
+            return super.collectionStatistics(field);
+        }
+        return collectionStatistics;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
index aa855ab..a3015b9 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.search.internal;
 
 import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 
 import org.apache.lucene.search.BooleanClause.Occur;
@@ -48,7 +49,6 @@ import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.search.Scroll;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
@@ -97,7 +97,7 @@ public class DefaultSearchContext extends SearchContext {
     // terminate after count
     private int terminateAfter = DEFAULT_TERMINATE_AFTER;
     private List<String> groupStats;
-    private Scroll scroll;
+    private ScrollContext scrollContext;
     private boolean explain;
     private boolean version = false; // by default, we don't return versions
     private List<String> fieldNames;
@@ -289,13 +289,13 @@ public class DefaultSearchContext extends SearchContext {
     }
 
     @Override
-    public Scroll scroll() {
-        return this.scroll;
+    public ScrollContext scrollContext() {
+        return this.scrollContext;
     }
 
     @Override
-    public SearchContext scroll(Scroll scroll) {
-        this.scroll = scroll;
+    public SearchContext scrollContext(ScrollContext scrollContext) {
+        this.scrollContext = scrollContext;
         return this;
     }
 
@@ -574,7 +574,7 @@ public class DefaultSearchContext extends SearchContext {
 
     @Override
     public void emptyFieldNames() {
-        this.fieldNames = Collections.emptyList();
+        this.fieldNames = ImmutableList.of();
     }
 
     @Override
@@ -652,16 +652,6 @@ public class DefaultSearchContext extends SearchContext {
     }
 
     @Override
-    public void lastEmittedDoc(ScoreDoc doc) {
-        this.lastEmittedDoc = doc;
-    }
-
-    @Override
-    public ScoreDoc lastEmittedDoc() {
-        return lastEmittedDoc;
-    }
-
-    @Override
     public SearchLookup lookup() {
         // TODO: The types should take into account the parsing context in QueryParserContext...
         if (searchLookup == null) {
diff --git a/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
index e2f6b48..2f79d03 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
@@ -23,7 +23,6 @@ import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
 
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.util.Counter;
 import org.elasticsearch.action.search.SearchType;
@@ -42,7 +41,6 @@ import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.search.Scroll;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
@@ -154,13 +152,13 @@ public abstract class FilteredSearchContext extends SearchContext {
     }
 
     @Override
-    public Scroll scroll() {
-        return in.scroll();
+    public ScrollContext scrollContext() {
+        return in.scrollContext();
     }
 
     @Override
-    public SearchContext scroll(Scroll scroll) {
-        return in.scroll(scroll);
+    public SearchContext scrollContext(ScrollContext scroll) {
+        return in.scrollContext(scroll);
     }
 
     @Override
@@ -484,16 +482,6 @@ public abstract class FilteredSearchContext extends SearchContext {
     }
 
     @Override
-    public void lastEmittedDoc(ScoreDoc doc) {
-        in.lastEmittedDoc(doc);
-    }
-
-    @Override
-    public ScoreDoc lastEmittedDoc() {
-        return in.lastEmittedDoc();
-    }
-
-    @Override
     public SearchLookup lookup() {
         return in.lookup();
     }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/ScrollContext.java b/core/src/main/java/org/elasticsearch/search/internal/ScrollContext.java
new file mode 100644
index 0000000..1744b6f
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/internal/ScrollContext.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.internal;
+
+import org.apache.lucene.search.ScoreDoc;
+import org.elasticsearch.search.Scroll;
+
+/** Wrapper around information that needs to stay around when scrolling. */
+public class ScrollContext {
+
+    public int totalHits = -1;
+    public float maxScore;
+    public ScoreDoc lastEmittedDoc;
+    public Scroll scroll;
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
index 72feec7..781d13a 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
@@ -24,7 +24,6 @@ import com.google.common.collect.MultimapBuilder;
 
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.util.Counter;
 import org.elasticsearch.action.search.SearchType;
@@ -159,9 +158,9 @@ public abstract class SearchContext implements Releasable, HasContextAndHeaders
 
     protected abstract long nowInMillisImpl();
 
-    public abstract Scroll scroll();
+    public abstract ScrollContext scrollContext();
 
-    public abstract SearchContext scroll(Scroll scroll);
+    public abstract SearchContext scrollContext(ScrollContext scroll);
 
     public abstract SearchContextAggregations aggregations();
 
@@ -303,10 +302,6 @@ public abstract class SearchContext implements Releasable, HasContextAndHeaders
 
     public abstract void keepAlive(long keepAlive);
 
-    public abstract void lastEmittedDoc(ScoreDoc doc);
-
-    public abstract ScoreDoc lastEmittedDoc();
-
     public abstract SearchLookup lookup();
 
     public abstract DfsSearchResult dfsResult();
diff --git a/core/src/main/java/org/elasticsearch/search/internal/SubSearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/SubSearchContext.java
index f6a6faa..a1a6fd0 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/SubSearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/SubSearchContext.java
@@ -18,15 +18,13 @@
  */
 package org.elasticsearch.search.internal;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.util.Counter;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.index.query.ParsedQuery;
-import org.elasticsearch.search.Scroll;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
@@ -38,7 +36,6 @@ import org.elasticsearch.search.query.QuerySearchResult;
 import org.elasticsearch.search.rescore.RescoreSearchContext;
 import org.elasticsearch.search.suggest.SuggestionSearchContext;
 
-import java.util.Collections;
 import java.util.List;
 
 /**
@@ -101,7 +98,7 @@ public class SubSearchContext extends FilteredSearchContext {
     }
 
     @Override
-    public SearchContext scroll(Scroll scroll) {
+    public SearchContext scrollContext(ScrollContext scrollContext) {
         throw new UnsupportedOperationException("Not supported");
     }
 
@@ -243,7 +240,7 @@ public class SubSearchContext extends FilteredSearchContext {
 
     @Override
     public void emptyFieldNames() {
-        this.fieldNames = Collections.emptyList();
+        this.fieldNames = ImmutableList.of();
     }
 
     @Override
@@ -305,11 +302,6 @@ public class SubSearchContext extends FilteredSearchContext {
     }
 
     @Override
-    public void lastEmittedDoc(ScoreDoc doc) {
-        throw new UnsupportedOperationException("Not supported");
-    }
-
-    @Override
     public QuerySearchResult queryResult() {
         return querySearchResult;
     }
diff --git a/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java b/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java
index a9105d5..8f21f46 100644
--- a/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java
@@ -21,12 +21,16 @@ package org.elasticsearch.search.query;
 
 import com.google.common.collect.ImmutableMap;
 
+import org.apache.lucene.queries.MinDocQuery;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.FieldDoc;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MultiCollector;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.TimeLimitingCollector;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.search.TopDocsCollector;
@@ -43,8 +47,8 @@ import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.SearchPhase;
 import org.elasticsearch.search.SearchService;
 import org.elasticsearch.search.aggregations.AggregationPhase;
+import org.elasticsearch.search.internal.ScrollContext;
 import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.search.internal.SearchContext.Lifetime;
 import org.elasticsearch.search.rescore.RescorePhase;
 import org.elasticsearch.search.rescore.RescoreSearchContext;
 import org.elasticsearch.search.scan.ScanContext.ScanCollector;
@@ -52,7 +56,6 @@ import org.elasticsearch.search.sort.SortParseElement;
 import org.elasticsearch.search.sort.TrackScoresParseElement;
 import org.elasticsearch.search.suggest.SuggestPhase;
 
-import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
@@ -115,6 +118,7 @@ public class QueryPhase implements SearchPhase {
 
         searchContext.queryResult().searchTimedOut(false);
 
+        final SearchType searchType = searchContext.searchType();
         boolean rescore = false;
         try {
             searchContext.queryResult().from(searchContext.from());
@@ -138,7 +142,7 @@ public class QueryPhase implements SearchPhase {
                         return new TopDocs(totalHitCountCollector.getTotalHits(), Lucene.EMPTY_SCORE_DOCS, 0);
                     }
                 };
-            } else if (searchContext.searchType() == SearchType.SCAN) {
+            } else if (searchType == SearchType.SCAN) {
                 query = searchContext.scanContext().wrapQuery(query);
                 final ScanCollector scanCollector = searchContext.scanContext().collector(searchContext);
                 collector = scanCollector;
@@ -150,11 +154,32 @@ public class QueryPhase implements SearchPhase {
                 };
             } else {
                 // Perhaps have a dedicated scroll phase?
+                final ScrollContext scrollContext = searchContext.scrollContext();
+                assert (scrollContext != null) == (searchContext.request().scroll() != null);
                 final TopDocsCollector<?> topDocsCollector;
                 ScoreDoc lastEmittedDoc;
                 if (searchContext.request().scroll() != null) {
                     numDocs = Math.min(searchContext.size(), totalNumDocs);
-                    lastEmittedDoc = searchContext.lastEmittedDoc();
+                    lastEmittedDoc = scrollContext.lastEmittedDoc;
+
+                    if (Sort.INDEXORDER.equals(searchContext.sort())) {
+                        if (scrollContext.totalHits == -1) {
+                            // first round
+                            assert scrollContext.lastEmittedDoc == null;
+                            // there is not much that we can optimize here since we want to collect all
+                            // documents in order to get the total number of hits
+                        } else {
+                            // now this gets interesting: since we sort in index-order, we can directly
+                            // skip to the desired doc and stop collecting after ${size} matches
+                            if (scrollContext.lastEmittedDoc != null) {
+                                BooleanQuery bq = new BooleanQuery();
+                                bq.add(query, Occur.MUST);
+                                bq.add(new MinDocQuery(lastEmittedDoc.doc + 1), Occur.FILTER);
+                                query = bq;
+                            }
+                            searchContext.terminateAfter(numDocs);
+                        }
+                    }
                 } else {
                     lastEmittedDoc = null;
                 }
@@ -177,7 +202,31 @@ public class QueryPhase implements SearchPhase {
                 topDocsCallable = new Callable<TopDocs>() {
                     @Override
                     public TopDocs call() throws Exception {
-                        return topDocsCollector.topDocs();
+                        TopDocs topDocs = topDocsCollector.topDocs();
+                        if (scrollContext != null) {
+                            if (scrollContext.totalHits == -1) {
+                                // first round
+                                scrollContext.totalHits = topDocs.totalHits;
+                                scrollContext.maxScore = topDocs.getMaxScore();
+                            } else {
+                                // subsequent round: the total number of hits and
+                                // the maximum score were computed on the first round
+                                topDocs.totalHits = scrollContext.totalHits;
+                                topDocs.setMaxScore(scrollContext.maxScore);
+                            }
+                            switch (searchType) {
+                            case QUERY_AND_FETCH:
+                            case DFS_QUERY_AND_FETCH:
+                                // for (DFS_)QUERY_AND_FETCH, we already know the last emitted doc
+                                if (topDocs.scoreDocs.length > 0) {
+                                    // set the last emitted doc
+                                    scrollContext.lastEmittedDoc = topDocs.scoreDocs[topDocs.scoreDocs.length - 1];
+                                }
+                            default:
+                                break;
+                            }
+                        }
+                        return topDocs;
                     }
                 };
             }
@@ -227,19 +276,7 @@ public class QueryPhase implements SearchPhase {
                 searchContext.queryResult().terminatedEarly(false);
             }
 
-            final TopDocs topDocs = topDocsCallable.call();
-            if (searchContext.request().scroll() != null) {
-                int size = topDocs.scoreDocs.length;
-                if (size > 0) {
-                    // In the case of *QUERY_AND_FETCH we don't get back to shards telling them which least
-                    // relevant docs got emitted as hit, we can simply mark the last doc as last emitted
-                    if (searchContext.searchType() == SearchType.QUERY_AND_FETCH ||
-                            searchContext.searchType() == SearchType.DFS_QUERY_AND_FETCH) {
-                        searchContext.lastEmittedDoc(topDocs.scoreDocs[size - 1]);
-                    }
-                }
-            }
-            searchContext.queryResult().topDocs(topDocs);
+            searchContext.queryResult().topDocs(topDocsCallable.call());
         } catch (Throwable e) {
             throw new QueryPhaseExecutionException(searchContext, "Failed to execute main query", e);
         }
diff --git a/core/src/main/java/org/elasticsearch/search/scan/ScanContext.java b/core/src/main/java/org/elasticsearch/search/scan/ScanContext.java
index c0018d4..b09a81b 100644
--- a/core/src/main/java/org/elasticsearch/search/scan/ScanContext.java
+++ b/core/src/main/java/org/elasticsearch/search/scan/ScanContext.java
@@ -20,18 +20,13 @@
 package org.elasticsearch.search.scan;
 
 import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.queries.MinDocQuery;
 import org.apache.lucene.search.CollectionTerminatedException;
-import org.apache.lucene.search.ConstantScoreScorer;
-import org.apache.lucene.search.ConstantScoreWeight;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.SimpleCollector;
 import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.search.Weight;
-import org.apache.lucene.util.Bits;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.search.internal.SearchContext;
 
@@ -118,93 +113,4 @@ public class ScanContext {
         }
     }
 
-    /**
-     * A filtering query that matches all doc IDs that are not deleted and
-     * greater than or equal to the configured doc ID.
-     */
-    // pkg-private for testing
-    static class MinDocQuery extends Query {
-
-        private final int minDoc;
-
-        MinDocQuery(int minDoc) {
-            this.minDoc = minDoc;
-        }
-
-        @Override
-        public int hashCode() {
-            return 31 * super.hashCode() + minDoc;
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (super.equals(obj) == false) {
-                return false;
-            }
-            MinDocQuery that = (MinDocQuery) obj;
-            return minDoc == that.minDoc;
-        }
-
-        @Override
-        public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
-            return new ConstantScoreWeight(this) {
-                @Override
-                public Scorer scorer(LeafReaderContext context, final Bits acceptDocs) throws IOException {
-                    final int maxDoc = context.reader().maxDoc();
-                    if (context.docBase + maxDoc <= minDoc) {
-                        return null;
-                    }
-                    final int segmentMinDoc = Math.max(0, minDoc - context.docBase);
-                    final DocIdSetIterator disi = new DocIdSetIterator() {
-
-                        int doc = -1;
-
-                        @Override
-                        public int docID() {
-                            return doc;
-                        }
-
-                        @Override
-                        public int nextDoc() throws IOException {
-                            return advance(doc + 1);
-                        }
-
-                        @Override
-                        public int advance(int target) throws IOException {
-                            assert target > doc;
-                            if (doc == -1) {
-                                // skip directly to minDoc
-                                doc = Math.max(target, segmentMinDoc);
-                            } else {
-                                doc = target;
-                            }
-                            while (doc < maxDoc) {
-                                if (acceptDocs == null || acceptDocs.get(doc)) {
-                                    break;
-                                }
-                                doc += 1;
-                            }
-                            if (doc >= maxDoc) {
-                                doc = NO_MORE_DOCS;
-                            }
-                            return doc;
-                        }
-
-                        @Override
-                        public long cost() {
-                            return maxDoc - minDoc;
-                        }
-
-                    };
-                    return new ConstantScoreScorer(this, score(), disi);
-                }
-            };
-        }
-
-        @Override
-        public String toString(String field) {
-            return "MinDocQuery(minDoc=" + minDoc  + ")";
-        }
-
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java b/core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java
index b976f0a..f2a996d 100644
--- a/core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java
+++ b/core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.search.warmer;
 
 import com.google.common.base.Objects;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 
 import org.elasticsearch.cluster.AbstractDiffable;
@@ -117,14 +118,14 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
         }
     }
 
-    private final List<Entry> entries;
+    private final ImmutableList<Entry> entries;
 
 
     public IndexWarmersMetaData(Entry... entries) {
-        this.entries = Arrays.asList(entries);
+        this.entries = ImmutableList.copyOf(entries);
     }
 
-    public List<Entry> entries() {
+    public ImmutableList<Entry> entries() {
         return this.entries;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/snapshots/RestoreInfo.java b/core/src/main/java/org/elasticsearch/snapshots/RestoreInfo.java
index cc9eeb0..548a8d2 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/RestoreInfo.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/RestoreInfo.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.snapshots;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
@@ -27,8 +28,6 @@ import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.rest.RestStatus;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 
 /**
@@ -40,7 +39,7 @@ public class RestoreInfo implements ToXContent, Streamable {
 
     private String name;
 
-    private List<String> indices;
+    private ImmutableList<String> indices;
 
     private int totalShards;
 
@@ -50,7 +49,7 @@ public class RestoreInfo implements ToXContent, Streamable {
 
     }
 
-    public RestoreInfo(String name, List<String> indices, int totalShards, int successfulShards) {
+    public RestoreInfo(String name, ImmutableList<String> indices, int totalShards, int successfulShards) {
         this.name = name;
         this.indices = indices;
         this.totalShards = totalShards;
@@ -148,11 +147,11 @@ public class RestoreInfo implements ToXContent, Streamable {
     public void readFrom(StreamInput in) throws IOException {
         name = in.readString();
         int size = in.readVInt();
-        List<String> indicesListBuilder = new ArrayList<>();
+        ImmutableList.Builder<String> indicesListBuilder = ImmutableList.builder();
         for (int i = 0; i < size; i++) {
             indicesListBuilder.add(in.readString());
         }
-        indices = Collections.unmodifiableList(indicesListBuilder);
+        indices = indicesListBuilder.build();
         totalShards = in.readVInt();
         successfulShards = in.readVInt();
     }
diff --git a/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java b/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
index 4f57940..d8c6fdc 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
@@ -22,6 +22,7 @@ import com.carrotsearch.hppc.IntHashSet;
 import com.carrotsearch.hppc.IntSet;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 
@@ -270,7 +271,7 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
                         }
 
                         shards = shardsBuilder.build();
-                        RestoreInProgress.Entry restoreEntry = new RestoreInProgress.Entry(snapshotId, RestoreInProgress.State.INIT, Collections.unmodifiableList(new ArrayList<>(renamedIndices.keySet())), shards);
+                        RestoreInProgress.Entry restoreEntry = new RestoreInProgress.Entry(snapshotId, RestoreInProgress.State.INIT, ImmutableList.copyOf(renamedIndices.keySet()), shards);
                         builder.putCustom(RestoreInProgress.TYPE, new RestoreInProgress(restoreEntry));
                     } else {
                         shards = ImmutableMap.of();
@@ -283,7 +284,7 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
 
                     if (completed(shards)) {
                         // We don't have any indices to restore - we are done
-                        restoreInfo = new RestoreInfo(request.name(), Collections.unmodifiableList(new ArrayList<>(renamedIndices.keySet())),
+                        restoreInfo = new RestoreInfo(request.name(), ImmutableList.copyOf(renamedIndices.keySet()),
                                 shards.size(), shards.size() - failedShards(shards));
                     }
 
diff --git a/core/src/main/java/org/elasticsearch/snapshots/Snapshot.java b/core/src/main/java/org/elasticsearch/snapshots/Snapshot.java
index 75abc40..05429ea 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/Snapshot.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/Snapshot.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.snapshots;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.ParseFieldMatcher;
@@ -56,7 +57,7 @@ public class Snapshot implements Comparable<Snapshot>, ToXContent, FromXContentB
 
     private final List<SnapshotShardFailure> shardFailures;
 
-    private final static List<SnapshotShardFailure> NO_FAILURES = Collections.emptyList();
+    private final static List<SnapshotShardFailure> NO_FAILURES = ImmutableList.of();
 
     public final static Snapshot PROTO = new Snapshot();
 
@@ -286,7 +287,7 @@ public class Snapshot implements Comparable<Snapshot>, ToXContent, FromXContentB
         Version version = Version.CURRENT;
         SnapshotState state = SnapshotState.IN_PROGRESS;
         String reason = null;
-        List<String> indices = Collections.emptyList();
+        ImmutableList<String> indices = ImmutableList.of();
         long startTime = 0;
         long endTime = 0;
         int totalShard = 0;
@@ -330,13 +331,13 @@ public class Snapshot implements Comparable<Snapshot>, ToXContent, FromXContentB
                                 while (parser.nextToken() != XContentParser.Token.END_ARRAY) {
                                     indicesArray.add(parser.text());
                                 }
-                                indices = Collections.unmodifiableList(indicesArray);
+                                indices = ImmutableList.copyOf(indicesArray);
                             } else if ("failures".equals(currentFieldName)) {
                                 ArrayList<SnapshotShardFailure> shardFailureArrayList = new ArrayList<>();
                                 while (parser.nextToken() != XContentParser.Token.END_ARRAY) {
                                     shardFailureArrayList.add(SnapshotShardFailure.fromXContent(parser));
                                 }
-                                shardFailures = Collections.unmodifiableList(shardFailureArrayList);
+                                shardFailures = ImmutableList.copyOf(shardFailureArrayList);
                             } else {
                                 // It was probably created by newer version - ignoring
                                 parser.skipChildren();
diff --git a/core/src/main/java/org/elasticsearch/snapshots/SnapshotInfo.java b/core/src/main/java/org/elasticsearch/snapshots/SnapshotInfo.java
index e7b6ce1..a54b1b3 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/SnapshotInfo.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/SnapshotInfo.java
@@ -19,8 +19,6 @@
 package org.elasticsearch.snapshots;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 
 import org.elasticsearch.Version;
@@ -34,6 +32,7 @@ import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.rest.RestStatus;
+import com.google.common.collect.ImmutableList;
 
 /**
  * Information about snapshot
@@ -261,11 +260,11 @@ public class SnapshotInfo implements ToXContent, Streamable {
     public void readFrom(StreamInput in) throws IOException {
         name = in.readString();
         int size = in.readVInt();
-        List<String> indicesListBuilder = new ArrayList<>();
+        ImmutableList.Builder<String> indicesListBuilder = ImmutableList.builder();
         for (int i = 0; i < size; i++) {
             indicesListBuilder.add(in.readString());
         }
-        indices = Collections.unmodifiableList(indicesListBuilder);
+        indices = indicesListBuilder.build();
         state = SnapshotState.fromValue(in.readByte());
         reason = in.readOptionalString();
         startTime = in.readVLong();
@@ -274,13 +273,13 @@ public class SnapshotInfo implements ToXContent, Streamable {
         successfulShards = in.readVInt();
         size = in.readVInt();
         if (size > 0) {
-            List<SnapshotShardFailure> failureBuilder = new ArrayList<>();
+            ImmutableList.Builder<SnapshotShardFailure> failureBuilder = ImmutableList.builder();
             for (int i = 0; i < size; i++) {
                 failureBuilder.add(SnapshotShardFailure.readSnapshotShardFailure(in));
             }
-            shardFailures = Collections.unmodifiableList(failureBuilder);
+            shardFailures = failureBuilder.build();
         } else {
-            shardFailures = Collections.emptyList();
+            shardFailures = ImmutableList.of();
         }
         version = Version.readVersion(in);
     }
diff --git a/core/src/main/java/org/elasticsearch/snapshots/SnapshotUtils.java b/core/src/main/java/org/elasticsearch/snapshots/SnapshotUtils.java
index feda027..84e2906 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/SnapshotUtils.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/SnapshotUtils.java
@@ -18,13 +18,11 @@
  */
 package org.elasticsearch.snapshots;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.index.IndexNotFoundException;
 
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
@@ -116,8 +114,8 @@ public class SnapshotUtils {
             }
         }
         if (result == null) {
-            return Arrays.asList(selectedIndices);
+            return ImmutableList.copyOf(selectedIndices);
         }
-        return Collections.unmodifiableList(new ArrayList<>(result));
+        return ImmutableList.copyOf(result);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java b/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java
index ab24130..da796d1 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.snapshots;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.CollectionUtil;
 import org.elasticsearch.ExceptionsHelper;
@@ -135,7 +136,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
         }
         ArrayList<Snapshot> snapshotList = newArrayList(snapshotSet);
         CollectionUtil.timSort(snapshotList);
-        return Collections.unmodifiableList(snapshotList);
+        return ImmutableList.copyOf(snapshotList);
     }
 
     /**
@@ -151,7 +152,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
             snapshotList.add(inProgressSnapshot(entry));
         }
         CollectionUtil.timSort(snapshotList);
-        return Collections.unmodifiableList(snapshotList);
+        return ImmutableList.copyOf(snapshotList);
     }
 
     /**
@@ -177,7 +178,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                 SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);
                 if (snapshots == null || snapshots.entries().isEmpty()) {
                     // Store newSnapshot here to be processed in clusterStateProcessed
-                    List<String> indices = Arrays.asList(indexNameExpressionResolver.concreteIndices(currentState, request.indicesOptions(), request.indices()));
+                    ImmutableList<String> indices = ImmutableList.copyOf(indexNameExpressionResolver.concreteIndices(currentState, request.indicesOptions(), request.indices()));
                     logger.trace("[{}][{}] creating snapshot for indices [{}]", request.repository(), request.name(), indices);
                     newSnapshot = new SnapshotsInProgress.Entry(snapshotId, request.includeGlobalState(), State.INIT, indices, System.currentTimeMillis(), null);
                     snapshots = new SnapshotsInProgress(newSnapshot);
@@ -296,7 +297,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                 @Override
                 public ClusterState execute(ClusterState currentState) {
                     SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);
-                    List<SnapshotsInProgress.Entry> entries = new ArrayList<>();
+                    ImmutableList.Builder<SnapshotsInProgress.Entry> entries = ImmutableList.builder();
                     for (SnapshotsInProgress.Entry entry : snapshots.entries()) {
                         if (entry.snapshotId().equals(snapshot.snapshotId())) {
                             // Replace the snapshot that was just created
@@ -333,7 +334,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                             entries.add(entry);
                         }
                     }
-                    return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, new SnapshotsInProgress(Collections.unmodifiableList(entries))).build();
+                    return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, new SnapshotsInProgress(entries.build())).build();
                 }
 
                 @Override
@@ -342,7 +343,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                     removeSnapshotFromClusterState(snapshot.snapshotId(), null, t);
                     try {
                         repositoriesService.repository(snapshot.snapshotId().getRepository()).finalizeSnapshot(
-                                snapshot.snapshotId(), snapshot.indices(), snapshot.startTime(), ExceptionsHelper.detailedMessage(t), 0, Collections.<SnapshotShardFailure>emptyList());
+                                snapshot.snapshotId(), snapshot.indices(), snapshot.startTime(), ExceptionsHelper.detailedMessage(t), 0, ImmutableList.<SnapshotShardFailure>of());
                     } catch (Throwable t2) {
                         logger.warn("[{}] failed to close snapshot in repository", snapshot.snapshotId());
                     }
@@ -372,7 +373,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
             if (snapshotCreated) {
                 try {
                     repositoriesService.repository(snapshot.snapshotId().getRepository()).finalizeSnapshot(snapshot.snapshotId(), snapshot.indices(), snapshot.startTime(),
-                            ExceptionsHelper.detailedMessage(t), 0, Collections.<SnapshotShardFailure>emptyList());
+                            ExceptionsHelper.detailedMessage(t), 0, ImmutableList.<SnapshotShardFailure>of());
                 } catch (Throwable t2) {
                     logger.warn("[{}] failed to close snapshot in repository", snapshot.snapshotId());
                 }
@@ -398,7 +399,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
     public List<SnapshotsInProgress.Entry> currentSnapshots(String repository, String[] snapshots) {
         SnapshotsInProgress snapshotsInProgress = clusterService.state().custom(SnapshotsInProgress.TYPE);
         if (snapshotsInProgress == null || snapshotsInProgress.entries().isEmpty()) {
-            return Collections.emptyList();
+            return ImmutableList.of();
         }
         if ("_all".equals(repository)) {
             return snapshotsInProgress.entries();
@@ -408,7 +409,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
             // Check this snapshot against the query
             SnapshotsInProgress.Entry entry = snapshotsInProgress.entries().get(0);
             if (!entry.snapshotId().getRepository().equals(repository)) {
-                return Collections.emptyList();
+                return ImmutableList.of();
             }
             if (snapshots != null && snapshots.length > 0) {
                 for (String snapshot : snapshots) {
@@ -416,12 +417,12 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                         return snapshotsInProgress.entries();
                     }
                 }
-                return Collections.emptyList();
+                return ImmutableList.of();
             } else {
                 return snapshotsInProgress.entries();
             }
         }
-        List<SnapshotsInProgress.Entry> builder = new ArrayList<>();
+        ImmutableList.Builder<SnapshotsInProgress.Entry> builder = ImmutableList.builder();
         for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {
             if (!entry.snapshotId().getRepository().equals(repository)) {
                 continue;
@@ -437,7 +438,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                 builder.add(entry);
             }
         }
-        return Collections.unmodifiableList(builder);
+        return builder.build();
     }
 
     /**
@@ -772,7 +773,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                             shardFailures.add(new SnapshotShardFailure(status.nodeId(), shardId.getIndex(), shardId.id(), status.reason()));
                         }
                     }
-                    Snapshot snapshot = repository.finalizeSnapshot(snapshotId, entry.indices(), entry.startTime(), failure, entry.shards().size(), Collections.unmodifiableList(shardFailures));
+                    Snapshot snapshot = repository.finalizeSnapshot(snapshotId, entry.indices(), entry.startTime(), failure, entry.shards().size(), ImmutableList.copyOf(shardFailures));
                     removeSnapshotFromClusterState(snapshotId, new SnapshotInfo(snapshot), null);
                 } catch (Throwable t) {
                     logger.warn("[{}] failed to finalize snapshot", t, snapshotId);
@@ -994,7 +995,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
      * @param indices      list of indices to be snapshotted
      * @return list of shard to be included into current snapshot
      */
-    private ImmutableMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards(ClusterState clusterState, List<String> indices) {
+    private ImmutableMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards(ClusterState clusterState, ImmutableList<String> indices) {
         ImmutableMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus> builder = ImmutableMap.builder();
         MetaData metaData = clusterState.metaData();
         for (String index : indices) {
diff --git a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
index 7c01367..0da74bb 100644
--- a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
+++ b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
@@ -727,7 +727,7 @@ public class ThreadPool extends AbstractComponent {
             if (queueSize == null) {
                 builder.field(Fields.QUEUE_SIZE, -1);
             } else {
-                builder.field(Fields.QUEUE_SIZE, queueSize.toString());
+                builder.field(Fields.QUEUE_SIZE, queueSize.singles());
             }
             builder.endObject();
             return builder;
diff --git a/core/src/main/java/org/elasticsearch/transport/Transport.java b/core/src/main/java/org/elasticsearch/transport/Transport.java
index 6264f3c..10fa9b2 100644
--- a/core/src/main/java/org/elasticsearch/transport/Transport.java
+++ b/core/src/main/java/org/elasticsearch/transport/Transport.java
@@ -25,6 +25,7 @@ import org.elasticsearch.common.transport.BoundTransportAddress;
 import org.elasticsearch.common.transport.TransportAddress;
 
 import java.io.IOException;
+import java.util.List;
 import java.util.Map;
 
 /**
@@ -32,6 +33,7 @@ import java.util.Map;
  */
 public interface Transport extends LifecycleComponent<Transport> {
 
+
     public static class TransportSettings {
         public static final String TRANSPORT_TCP_COMPRESS = "transport.tcp.compress";
     }
@@ -52,7 +54,7 @@ public interface Transport extends LifecycleComponent<Transport> {
     /**
      * Returns an address from its string representation.
      */
-    TransportAddress[] addressesFromString(String address) throws Exception;
+    TransportAddress[] addressesFromString(String address, int perAddressLimit) throws Exception;
 
     /**
      * Is the address type supported.
@@ -89,4 +91,6 @@ public interface Transport extends LifecycleComponent<Transport> {
      * Returns count of currently open connections
      */
     long serverOpen();
+
+    List<String> getLocalAddresses();
 }
diff --git a/core/src/main/java/org/elasticsearch/transport/TransportModule.java b/core/src/main/java/org/elasticsearch/transport/TransportModule.java
index fd8932b..0be8403 100644
--- a/core/src/main/java/org/elasticsearch/transport/TransportModule.java
+++ b/core/src/main/java/org/elasticsearch/transport/TransportModule.java
@@ -92,7 +92,6 @@ public class TransportModule extends AbstractModule {
         }
 
         bind(NamedWriteableRegistry.class).asEagerSingleton();
-
         if (configuredTransport != null) {
             logger.info("Using [{}] as transport, overridden by [{}]", configuredTransport.getName(), configuredTransportSource);
             bind(Transport.class).to(configuredTransport).asEagerSingleton();
diff --git a/core/src/main/java/org/elasticsearch/transport/TransportService.java b/core/src/main/java/org/elasticsearch/transport/TransportService.java
index 8159999..b70589c 100644
--- a/core/src/main/java/org/elasticsearch/transport/TransportService.java
+++ b/core/src/main/java/org/elasticsearch/transport/TransportService.java
@@ -40,10 +40,7 @@ import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.LinkedHashMap;
-import java.util.Map;
+import java.util.*;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.ScheduledFuture;
 import java.util.concurrent.atomic.AtomicBoolean;
@@ -221,6 +218,10 @@ public class TransportService extends AbstractLifecycleComponent<TransportServic
         return transport.boundAddress();
     }
 
+    public List<String> getLocalAddresses() {
+        return transport.getLocalAddresses();
+    }
+
     public boolean nodeConnected(DiscoveryNode node) {
         return node.equals(localNode) || transport.nodeConnected(node);
     }
@@ -383,8 +384,8 @@ public class TransportService extends AbstractLifecycleComponent<TransportServic
         return requestIds.getAndIncrement();
     }
 
-    public TransportAddress[] addressesFromString(String address) throws Exception {
-        return transport.addressesFromString(address);
+    public TransportAddress[] addressesFromString(String address, int perAddressLimit) throws Exception {
+        return transport.addressesFromString(address, perAddressLimit);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java b/core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java
index 3584a8b..a500fb3 100644
--- a/core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java
+++ b/core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java
@@ -41,8 +41,7 @@ import org.elasticsearch.transport.*;
 import org.elasticsearch.transport.support.TransportStatus;
 
 import java.io.IOException;
-import java.util.Collections;
-import java.util.Map;
+import java.util.*;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.ThreadFactory;
 import java.util.concurrent.ThreadPoolExecutor;
@@ -57,14 +56,13 @@ import static org.elasticsearch.common.util.concurrent.ConcurrentCollections.new
 public class LocalTransport extends AbstractLifecycleComponent<Transport> implements Transport {
 
     public static final String LOCAL_TRANSPORT_THREAD_NAME_PREFIX = "local_transport";
-
     private final ThreadPool threadPool;
     private final ThreadPoolExecutor workers;
     private final Version version;
     private volatile TransportServiceAdapter transportServiceAdapter;
     private volatile BoundTransportAddress boundAddress;
     private volatile LocalTransportAddress localAddress;
-    private final static ConcurrentMap<TransportAddress, LocalTransport> transports = newConcurrentMap();
+    private final static ConcurrentMap<LocalTransportAddress, LocalTransport> transports = newConcurrentMap();
     private static final AtomicLong transportAddressIdGenerator = new AtomicLong();
     private final ConcurrentMap<DiscoveryNode, LocalTransport> connectedNodes = newConcurrentMap();
     private final NamedWriteableRegistry namedWriteableRegistry;
@@ -78,7 +76,6 @@ public class LocalTransport extends AbstractLifecycleComponent<Transport> implem
         super(settings);
         this.threadPool = threadPool;
         this.version = version;
-
         int workerCount = this.settings.getAsInt(TRANSPORT_LOCAL_WORKERS, EsExecutors.boundedNumberOfProcessors(settings));
         int queueSize = this.settings.getAsInt(TRANSPORT_LOCAL_QUEUE, -1);
         logger.debug("creating [{}] workers, queue_size [{}]", workerCount, queueSize);
@@ -88,7 +85,7 @@ public class LocalTransport extends AbstractLifecycleComponent<Transport> implem
     }
 
     @Override
-    public TransportAddress[] addressesFromString(String address) {
+    public TransportAddress[] addressesFromString(String address, int perAddressLimit) {
         return new TransportAddress[]{new LocalTransportAddress(address)};
     }
 
@@ -359,4 +356,9 @@ public class LocalTransport extends AbstractLifecycleComponent<Transport> implem
             logger.error("failed to handle exception response [{}]", t, handler);
         }
     }
+
+    @Override
+    public List<String> getLocalAddresses() {
+        return Collections.singletonList("0.0.0.0");
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
index fdf2552..6b13ddd 100644
--- a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
+++ b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
@@ -20,9 +20,10 @@
 package org.elasticsearch.transport.netty;
 
 import com.google.common.base.Charsets;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
-import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
+
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.node.DiscoveryNode;
@@ -41,6 +42,7 @@ import org.elasticsearch.common.metrics.CounterMetric;
 import org.elasticsearch.common.netty.NettyUtils;
 import org.elasticsearch.common.netty.OpenChannelsHandler;
 import org.elasticsearch.common.netty.ReleaseChannelFutureListener;
+import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.common.network.NetworkService;
 import org.elasticsearch.common.network.NetworkUtils;
 import org.elasticsearch.common.settings.Settings;
@@ -74,6 +76,7 @@ import java.io.IOException;
 import java.net.InetAddress;
 import java.net.InetSocketAddress;
 import java.net.SocketAddress;
+import java.net.UnknownHostException;
 import java.nio.channels.CancelledKeyException;
 import java.util.*;
 import java.util.concurrent.*;
@@ -81,6 +84,8 @@ import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReference;
 import java.util.concurrent.locks.ReadWriteLock;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
 
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.*;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
@@ -403,6 +408,13 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
         } catch (IOException e) {
             throw new BindTransportException("Failed to resolve host [" + bindHost + "]", e);
         }
+        if (logger.isDebugEnabled()) {
+            String[] addresses = new String[hostAddresses.length];
+            for (int i = 0; i < hostAddresses.length; i++) {
+                addresses[i] = NetworkAddress.format(hostAddresses[i]);
+            }
+            logger.debug("binding server bootstrap to: {}", addresses);
+        }
         for (InetAddress hostAddress : hostAddresses) {
             bindServerBootstrap(name, hostAddress, settings);
         }
@@ -413,7 +425,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
         String port = settings.get("port");
         PortsRange portsRange = new PortsRange(port);
         final AtomicReference<Exception> lastException = new AtomicReference<>();
-        final AtomicReference<SocketAddress> boundSocket = new AtomicReference<>();
+        final AtomicReference<InetSocketAddress> boundSocket = new AtomicReference<>();
         boolean success = portsRange.iterate(new PortsRange.PortCallback() {
             @Override
             public boolean onPortNumber(int portNumber) {
@@ -426,7 +438,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
                             serverChannels.put(name, list);
                         }
                         list.add(channel);
-                        boundSocket.set(channel.getLocalAddress());
+                        boundSocket.set((InetSocketAddress)channel.getLocalAddress());
                     }
                 } catch (Exception e) {
                     lastException.set(e);
@@ -440,7 +452,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
         }
 
         if (!DEFAULT_PROFILE.equals(name)) {
-            InetSocketAddress boundAddress = (InetSocketAddress) boundSocket.get();
+            InetSocketAddress boundAddress = boundSocket.get();
             int publishPort = settings.getAsInt("publish_port", boundAddress.getPort());
             String publishHost = settings.get("publish_host", boundAddress.getHostString());
             InetSocketAddress publishAddress = createPublishAddress(publishHost, publishPort);
@@ -448,7 +460,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
             profileBoundAddresses.putIfAbsent(name, new BoundTransportAddress(new InetSocketTransportAddress(boundAddress), new InetSocketTransportAddress(publishAddress)));
         }
 
-        logger.info("Bound profile [{}] to address [{}]", name, boundSocket.get());
+        logger.info("Bound profile [{}] to address {{}}", name, NetworkAddress.format(boundSocket.get()));
     }
 
     private void createServerBootstrap(String name, Settings settings) {
@@ -496,7 +508,6 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
         serverBootstrap.setOption("child.receiveBufferSizePredictorFactory", receiveBufferSizePredictorFactory);
         serverBootstrap.setOption("reuseAddress", reuseAddress);
         serverBootstrap.setOption("child.reuseAddress", reuseAddress);
-
         serverBootstraps.put(name, serverBootstrap);
     }
 
@@ -579,35 +590,65 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
     }
 
     @Override
-    public TransportAddress[] addressesFromString(String address) throws Exception {
-        int index = address.indexOf('[');
-        if (index != -1) {
-            String host = address.substring(0, index);
-            Set<String> ports = Strings.commaDelimitedListToSet(address.substring(index + 1, address.indexOf(']')));
-            List<TransportAddress> addresses = Lists.newArrayList();
-            for (String port : ports) {
-                int[] iPorts = new PortsRange(port).ports();
-                for (int iPort : iPorts) {
-                    addresses.add(new InetSocketTransportAddress(host, iPort));
-                }
-            }
-            return addresses.toArray(new TransportAddress[addresses.size()]);
+    public TransportAddress[] addressesFromString(String address, int perAddressLimit) throws Exception {
+        return parse(address, settings.get("transport.profiles.default.port", 
+                              settings.get("transport.netty.port", 
+                              settings.get("transport.tcp.port", 
+                              DEFAULT_PORT_RANGE))), perAddressLimit);
+    }
+    
+    // this code is a take on guava's HostAndPort, like a HostAndPortRange
+    
+    // pattern for validating ipv6 bracked addresses. 
+    // not perfect, but PortsRange should take care of any port range validation, not a regex
+    private static final Pattern BRACKET_PATTERN = Pattern.compile("^\\[(.*:.*)\\](?::([\\d\\-]*))?$");
+
+    /** parse a hostname+port range spec into its equivalent addresses */
+    static TransportAddress[] parse(String hostPortString, String defaultPortRange, int perAddressLimit) throws UnknownHostException {
+        Objects.requireNonNull(hostPortString);
+        String host;
+        String portString = null;
+
+        if (hostPortString.startsWith("[")) {
+          // Parse a bracketed host, typically an IPv6 literal.
+          Matcher matcher = BRACKET_PATTERN.matcher(hostPortString);
+          if (!matcher.matches()) {
+              throw new IllegalArgumentException("Invalid bracketed host/port range: " + hostPortString);
+          }
+          host = matcher.group(1);
+          portString = matcher.group(2);  // could be null
         } else {
-            index = address.lastIndexOf(':');
-            if (index == -1) {
-                List<TransportAddress> addresses = Lists.newArrayList();
-                String defaultPort = settings.get("transport.profiles.default.port", settings.get("transport.netty.port", this.settings.get("transport.tcp.port", DEFAULT_PORT_RANGE)));
-                int[] iPorts = new PortsRange(defaultPort).ports();
-                for (int iPort : iPorts) {
-                    addresses.add(new InetSocketTransportAddress(address, iPort));
-                }
-                return addresses.toArray(new TransportAddress[addresses.size()]);
-            } else {
-                String host = address.substring(0, index);
-                int port = Integer.parseInt(address.substring(index + 1));
-                return new TransportAddress[]{new InetSocketTransportAddress(host, port)};
+          int colonPos = hostPortString.indexOf(':');
+          if (colonPos >= 0 && hostPortString.indexOf(':', colonPos + 1) == -1) {
+            // Exactly 1 colon.  Split into host:port.
+            host = hostPortString.substring(0, colonPos);
+            portString = hostPortString.substring(colonPos + 1);
+          } else {
+            // 0 or 2+ colons.  Bare hostname or IPv6 literal.
+            host = hostPortString;
+            // 2+ colons and not bracketed: exception
+            if (colonPos >= 0) {
+                throw new IllegalArgumentException("IPv6 addresses must be bracketed: " + hostPortString);
+            }
+          }
+        }
+        
+        // if port isn't specified, fill with the default
+        if (portString == null || portString.isEmpty()) {
+            portString = defaultPortRange;
+        }
+        
+        // generate address for each port in the range
+        Set<InetAddress> addresses = new HashSet<>(Arrays.asList(InetAddress.getAllByName(host)));
+        List<TransportAddress> transportAddresses = new ArrayList<>();
+        int[] ports = new PortsRange(portString).ports();
+        int limit = Math.min(ports.length, perAddressLimit);
+        for (int i = 0; i < limit; i++) {
+            for (InetAddress address : addresses) {
+                transportAddresses.add(new InetSocketTransportAddress(address, ports[i]));
             }
         }
+        return transportAddresses.toArray(new TransportAddress[transportAddresses.size()]);
     }
 
     @Override
@@ -671,6 +712,17 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
     }
 
     @Override
+    public List<String> getLocalAddresses() {
+        List<String> local = new ArrayList<>();
+        local.add("127.0.0.1");
+        // check if v6 is supported, if so, v4 will also work via mapped addresses.
+        if (NetworkUtils.SUPPORTS_V6) {
+            local.add("[::1]"); // may get ports appended!
+        }
+        return local;
+    }
+
+    @Override
     public void sendRequest(final DiscoveryNode node, final long requestId, final String action, final TransportRequest request, TransportRequestOptions options) throws IOException, TransportException {
 
         Channel targetChannel = nodeChannel(node, options);
@@ -895,15 +947,13 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
             }
         } catch (RuntimeException e) {
             // clean the futures
-            for (ChannelFuture[] futures : Arrays.asList(connectRecovery, connectBulk, connectReg, connectState, connectPing)) {
-                for (ChannelFuture future : futures) {
-                    future.cancel();
-                    if (future.getChannel() != null && future.getChannel().isOpen()) {
-                        try {
-                            future.getChannel().close();
-                        } catch (Exception e1) {
-                            // ignore
-                        }
+            for (ChannelFuture future : ImmutableList.<ChannelFuture>builder().add(connectRecovery).add(connectBulk).add(connectReg).add(connectState).add(connectPing).build()) {
+                future.cancel();
+                if (future.getChannel() != null && future.getChannel().isOpen()) {
+                    try {
+                        future.getChannel().close();
+                    } catch (Exception e1) {
+                        // ignore
                     }
                 }
             }
@@ -1080,7 +1130,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
 
     public static class NodeChannels {
 
-        List<Channel> allChannels = Collections.emptyList();
+        ImmutableList<Channel> allChannels = ImmutableList.of();
         private Channel[] recovery;
         private final AtomicInteger recoveryCounter = new AtomicInteger();
         private Channel[] bulk;
@@ -1101,13 +1151,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
         }
 
         public void start() {
-            List<Channel> newAllChannels = new ArrayList<>();
-            newAllChannels.addAll(Arrays.asList(recovery));
-            newAllChannels.addAll(Arrays.asList(bulk));
-            newAllChannels.addAll(Arrays.asList(reg));
-            newAllChannels.addAll(Arrays.asList(state));
-            newAllChannels.addAll(Arrays.asList(ping));
-            this.allChannels = Collections.unmodifiableList(allChannels );
+            this.allChannels = ImmutableList.<Channel>builder().add(recovery).add(bulk).add(reg).add(state).add(ping).build();
         }
 
         public boolean hasChannel(Channel channel) {
diff --git a/core/src/main/resources/config/favicon.ico b/core/src/main/resources/config/favicon.ico
new file mode 100644
index 0000000..0eabd4c
Binary files /dev/null and b/core/src/main/resources/config/favicon.ico differ
diff --git a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
index 9aa943d..6bce1dd 100644
--- a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
+++ b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
@@ -4,29 +4,30 @@ NAME
 
 SYNOPSIS
 
-    plugin install <name>
+    plugin install <name or url>
 
 DESCRIPTION
 
-    This command installs an elasticsearch plugin
+    This command installs an elasticsearch plugin. It can be used as follows:
 
-    <name> can be one of the official plugins, or refer to a github repository, or to one of the official plugins
+    Officially supported or commercial plugins require just the plugin name:
 
-    The notation of just specifying a plugin name, downloads an officially supported plugin.
+        plugin install analysis-icu
+        plugin install shield
 
-    The notation of 'elasticsearch/plugin/version' allows to easily download a commercial elastic plugin.
+    Plugins from GitHub require 'username/repository' or 'username/repository/version':
 
-    The notation of 'groupId/artifactId/version' refers to community plugins using maven central or sonatype
+        plugin install lmenezes/elasticsearch-kopf
+        plugin install lmenezes/elasticsearch-kopf/1.5.7
 
-    The notation of 'username/repository' refers to a github repository.
+    Plugins from Maven Central or Sonatype require 'groupId/artifactId/version':
 
-EXAMPLES
+        plugin install org.elasticsearch/elasticsearch-mapper-attachments/2.6.0
 
-    plugin install analysis-kuromoji
+    Plugins can be installed from a custom URL or file location as follows:
 
-    plugin install elasticsearch/shield/latest
-
-    plugin install lmenezes/elasticsearch-kopf
+        plugin install http://some.domain.name//my-plugin-1.0.0.zip
+        plugin install file:/path/to/my-plugin-1.0.0.zip
 
 OFFICIAL PLUGINS
 
@@ -41,6 +42,7 @@ OFFICIAL PLUGINS
     - cloud-azure
     - cloud-gce
     - delete-by-query
+    - discovery-multicast
     - lang-javascript
     - lang-python
     - mapper-murmur3
@@ -49,8 +51,6 @@ OFFICIAL PLUGINS
 
 OPTIONS
 
-    -u,--url                     URL to retrieve the plugin from
-
     -t,--timeout                 Timeout until the plugin download is abort
 
     -v,--verbose                 Verbose output
diff --git a/core/src/test/java/org/apache/lucene/queries/MinDocQueryTests.java b/core/src/test/java/org/apache/lucene/queries/MinDocQueryTests.java
new file mode 100644
index 0000000..725bafb
--- /dev/null
+++ b/core/src/test/java/org/apache/lucene/queries/MinDocQueryTests.java
@@ -0,0 +1,61 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.lucene.queries;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.QueryUtils;
+import org.apache.lucene.store.Directory;
+import org.elasticsearch.test.ESTestCase;
+
+import java.io.IOException;
+
+public class MinDocQueryTests extends ESTestCase {
+
+    public void testBasics() {
+        MinDocQuery query1 = new MinDocQuery(42);
+        MinDocQuery query2 = new MinDocQuery(42);
+        MinDocQuery query3 = new MinDocQuery(43);
+        QueryUtils.check(query1);
+        QueryUtils.checkEqual(query1, query2);
+        QueryUtils.checkUnequal(query1, query3);
+    }
+
+    public void testRandom() throws IOException {
+        final int numDocs = randomIntBetween(10, 200);
+        final Document doc = new Document();
+        final Directory dir = newDirectory();
+        final RandomIndexWriter w = new RandomIndexWriter(getRandom(), dir);
+        for (int i = 0; i < numDocs; ++i) {
+            w.addDocument(doc);
+        }
+        final IndexReader reader = w.getReader();
+        final IndexSearcher searcher = newSearcher(reader);
+        for (int i = 0; i <= numDocs; ++i) {
+            assertEquals(numDocs - i, searcher.count(new MinDocQuery(i)));
+        }
+        w.close();
+        reader.close();
+        dir.close();
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/ESExceptionTests.java b/core/src/test/java/org/elasticsearch/ESExceptionTests.java
index 6be1b7c..dea127a 100644
--- a/core/src/test/java/org/elasticsearch/ESExceptionTests.java
+++ b/core/src/test/java/org/elasticsearch/ESExceptionTests.java
@@ -57,7 +57,7 @@ import java.util.Collections;
 import static org.hamcrest.Matchers.equalTo;
 
 public class ESExceptionTests extends ESTestCase {
-    private static final ToXContent.Params PARAMS = new ToXContent.MapParams(Collections.singletonMap(ElasticsearchException.REST_EXCEPTION_SKIP_STACK_TRACE, "true"));
+    private static final ToXContent.Params PARAMS = ToXContent.EMPTY_PARAMS;
 
     @Test
     public void testStatus() {
diff --git a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
index 034d31c..c8a042f 100644
--- a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
+++ b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
@@ -526,7 +526,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         try {
             XContentBuilder builder = XContentFactory.jsonBuilder();
             builder.startObject();
-            x.toXContent(builder, new ToXContent.MapParams(Collections.singletonMap(ElasticsearchException.REST_EXCEPTION_SKIP_STACK_TRACE, "true")));
+            x.toXContent(builder, ToXContent.EMPTY_PARAMS);
             builder.endObject();
             return builder.string();
         } catch (IOException e) {
@@ -607,4 +607,20 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals(ex.status(), e.status());
         assertEquals(RestStatus.UNAUTHORIZED, e.status());
     }
+
+    public void testInterruptedException() throws IOException {
+        InterruptedException orig = randomBoolean() ? new InterruptedException("boom") : new InterruptedException();
+        InterruptedException ex = serialize(orig);
+        assertEquals(orig.getMessage(), ex.getMessage());
+    }
+
+    public static class UnknownException extends Exception {
+        public UnknownException(String message) {
+            super(message);
+        }
+
+        public UnknownException(String message, Throwable cause) {
+            super(message, cause);
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/action/admin/indices/get/GetIndexIT.java b/core/src/test/java/org/elasticsearch/action/admin/indices/get/GetIndexIT.java
index 9484c5e..ce214dd 100644
--- a/core/src/test/java/org/elasticsearch/action/admin/indices/get/GetIndexIT.java
+++ b/core/src/test/java/org/elasticsearch/action/admin/indices/get/GetIndexIT.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.indices.get;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.action.admin.indices.alias.Alias;
 import org.elasticsearch.action.admin.indices.get.GetIndexRequest.Feature;
 import org.elasticsearch.cluster.metadata.AliasMetaData;
@@ -238,10 +239,10 @@ public class GetIndexIT extends ESIntegTestCase {
     }
 
     private void assertWarmers(GetIndexResponse response, String indexName) {
-        ImmutableOpenMap<String, List<Entry>> warmers = response.warmers();
+        ImmutableOpenMap<String, ImmutableList<Entry>> warmers = response.warmers();
         assertThat(warmers, notNullValue());
         assertThat(warmers.size(), equalTo(1));
-        List<Entry> indexWarmers = warmers.get(indexName);
+        ImmutableList<Entry> indexWarmers = warmers.get(indexName);
         assertThat(indexWarmers, notNullValue());
         assertThat(indexWarmers.size(), equalTo(1));
         Entry warmer = indexWarmers.get(0);
@@ -296,10 +297,10 @@ public class GetIndexIT extends ESIntegTestCase {
     }
 
     private void assertAliases(GetIndexResponse response, String indexName) {
-        ImmutableOpenMap<String, List<AliasMetaData>> aliases = response.aliases();
+        ImmutableOpenMap<String, ImmutableList<AliasMetaData>> aliases = response.aliases();
         assertThat(aliases, notNullValue());
         assertThat(aliases.size(), equalTo(1));
-        List<AliasMetaData> indexAliases = aliases.get(indexName);
+        ImmutableList<AliasMetaData> indexAliases = aliases.get(indexName);
         assertThat(indexAliases, notNullValue());
         assertThat(indexAliases.size(), equalTo(1));
         AliasMetaData alias = indexAliases.get(0);
diff --git a/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreResponseTest.java b/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreResponseTest.java
index 925e01e..12b61e2 100644
--- a/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreResponseTest.java
+++ b/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreResponseTest.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.indices.shards;
 
+import com.google.common.collect.ImmutableList;
 import org.apache.lucene.util.CollectionUtil;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.node.DiscoveryNode;
@@ -41,7 +42,7 @@ public class IndicesShardStoreResponseTest extends ESTestCase {
     @Test
     public void testBasicSerialization() throws Exception {
         ImmutableOpenMap.Builder<String, ImmutableOpenIntMap<List<IndicesShardStoresResponse.StoreStatus>>> indexStoreStatuses = ImmutableOpenMap.builder();
-        List<IndicesShardStoresResponse.Failure> failures = new ArrayList<>();
+        ImmutableList.Builder<IndicesShardStoresResponse.Failure> failures = ImmutableList.builder();
         ImmutableOpenIntMap.Builder<List<IndicesShardStoresResponse.StoreStatus>> storeStatuses = ImmutableOpenIntMap.builder();
 
         DiscoveryNode node1 = new DiscoveryNode("node1", DummyTransportAddress.INSTANCE, Version.CURRENT);
@@ -58,7 +59,7 @@ public class IndicesShardStoreResponseTest extends ESTestCase {
 
         failures.add(new IndicesShardStoresResponse.Failure("node1", "test", 3, new NodeDisconnectedException(node1, "")));
 
-        IndicesShardStoresResponse storesResponse = new IndicesShardStoresResponse(indexStoreStatuses.build(), Collections.unmodifiableList(failures));
+        IndicesShardStoresResponse storesResponse = new IndicesShardStoresResponse(indexStoreStatuses.build(), failures.build());
         XContentBuilder contentBuilder = XContentFactory.jsonBuilder();
         contentBuilder.startObject();
         storesResponse.toXContent(contentBuilder, ToXContent.EMPTY_PARAMS);
diff --git a/core/src/test/java/org/elasticsearch/action/admin/indices/template/put/MetaDataIndexTemplateServiceTests.java b/core/src/test/java/org/elasticsearch/action/admin/indices/template/put/MetaDataIndexTemplateServiceTests.java
index 197a6b3..0966212 100644
--- a/core/src/test/java/org/elasticsearch/action/admin/indices/template/put/MetaDataIndexTemplateServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/action/admin/indices/template/put/MetaDataIndexTemplateServiceTests.java
@@ -33,13 +33,47 @@ import org.elasticsearch.indices.InvalidIndexTemplateException;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
-import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
+import static org.hamcrest.CoreMatchers.containsString;
+import static org.hamcrest.CoreMatchers.instanceOf;
+import static org.hamcrest.Matchers.contains;
+
 public class MetaDataIndexTemplateServiceTests extends ESTestCase {
     @Test
-    public void testIndexTemplateInvalidNumberOfShards() throws IOException {
+    public void testIndexTemplateInvalidNumberOfShards() {
+        PutRequest request = new PutRequest("test", "test_shards");
+        request.template("test_shards*");
+
+        Map<String, Object> map = Maps.newHashMap();
+        map.put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, "0");
+        request.settings(Settings.settingsBuilder().put(map).build());
+
+        List<Throwable> throwables = putTemplate(request);
+        assertEquals(throwables.size(), 1);
+        assertThat(throwables.get(0), instanceOf(InvalidIndexTemplateException.class));
+        assertThat(throwables.get(0).getMessage(), containsString("index must have 1 or more primary shards"));
+    }
+
+    @Test
+    public void testIndexTemplateValidationAccumulatesValidationErrors() {
+        PutRequest request = new PutRequest("test", "putTemplate shards");
+        request.template("_test_shards*");
+
+        Map<String, Object> map = Maps.newHashMap();
+        map.put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, "0");
+        request.settings(Settings.settingsBuilder().put(map).build());
+
+        List<Throwable> throwables = putTemplate(request);
+        assertEquals(throwables.size(), 1);
+        assertThat(throwables.get(0), instanceOf(InvalidIndexTemplateException.class));
+        assertThat(throwables.get(0).getMessage(), containsString("name must not contain a space"));
+        assertThat(throwables.get(0).getMessage(), containsString("template must not start with '_'"));
+        assertThat(throwables.get(0).getMessage(), containsString("index must have 1 or more primary shards"));
+    }
+
+    private static List<Throwable> putTemplate(PutRequest request) {
         MetaDataCreateIndexService createIndexService = new MetaDataCreateIndexService(
                 Settings.EMPTY,
                 null,
@@ -55,13 +89,6 @@ public class MetaDataIndexTemplateServiceTests extends ESTestCase {
         );
         MetaDataIndexTemplateService service = new MetaDataIndexTemplateService(Settings.EMPTY, null, createIndexService, null);
 
-        PutRequest request = new PutRequest("test", "test_shards");
-        request.template("test_shards*");
-
-        Map<String, Object> map = Maps.newHashMap();
-        map.put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, "0");
-        request.settings(Settings.settingsBuilder().put(map).build());
-
         final List<Throwable> throwables = Lists.newArrayList();
         service.putTemplate(request, new MetaDataIndexTemplateService.PutListener() {
             @Override
@@ -74,8 +101,7 @@ public class MetaDataIndexTemplateServiceTests extends ESTestCase {
                 throwables.add(t);
             }
         });
-        assertEquals(throwables.size(), 1);
-        assertTrue(throwables.get(0) instanceof InvalidIndexTemplateException);
-        assertTrue(throwables.get(0).getMessage().contains("index must have 1 or more primary shards"));
+
+        return throwables;
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java b/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java
index cad5679..d2533ea 100644
--- a/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java
+++ b/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java
@@ -125,7 +125,7 @@ public class MultiSearchRequestTests extends ESTestCase {
     public void testResponseErrorToXContent() throws IOException {
         MultiSearchResponse response = new MultiSearchResponse(new MultiSearchResponse.Item[]{new MultiSearchResponse.Item(null, new IllegalStateException("foobar")), new MultiSearchResponse.Item(null, new IllegalStateException("baaaaaazzzz"))});
         XContentBuilder builder = XContentFactory.jsonBuilder();
-        response.toXContent(builder, new ToXContent.MapParams(Collections.singletonMap(ElasticsearchException.REST_EXCEPTION_SKIP_STACK_TRACE, "true")));
+        response.toXContent(builder, ToXContent.EMPTY_PARAMS);
         assertEquals("\"responses\"[{\"error\":{\"root_cause\":[{\"type\":\"illegal_state_exception\",\"reason\":\"foobar\"}],\"type\":\"illegal_state_exception\",\"reason\":\"foobar\"}},{\"error\":{\"root_cause\":[{\"type\":\"illegal_state_exception\",\"reason\":\"baaaaaazzzz\"}],\"type\":\"illegal_state_exception\",\"reason\":\"baaaaaazzzz\"}}]",
                 builder.string());
     }
diff --git a/core/src/test/java/org/elasticsearch/benchmark/mapping/ManyMappingsBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/mapping/ManyMappingsBenchmark.java
index 5a347ef..840c3c1 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/mapping/ManyMappingsBenchmark.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/mapping/ManyMappingsBenchmark.java
@@ -21,7 +21,7 @@ package org.elasticsearch.benchmark.mapping;
 
 import org.elasticsearch.action.bulk.BulkRequestBuilder;
 import org.elasticsearch.action.support.IndicesOptions;
-import org.elasticsearch.bootstrap.Bootstrap;
+import org.elasticsearch.bootstrap.BootstrapForTesting;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
@@ -85,7 +85,7 @@ public class ManyMappingsBenchmark {
 
     public static void main(String[] args) throws Exception {
         System.setProperty("es.logger.prefix", "");
-        Bootstrap.initializeNatives(true, false);
+        BootstrapForTesting.ensureInitialized();
         Settings settings = settingsBuilder()
                 .put("")
                 .put(SETTING_NUMBER_OF_SHARDS, 5)
diff --git a/core/src/test/java/org/elasticsearch/benchmark/recovery/ReplicaRecoveryBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/recovery/ReplicaRecoveryBenchmark.java
index 9648947..ad2d1a7 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/recovery/ReplicaRecoveryBenchmark.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/recovery/ReplicaRecoveryBenchmark.java
@@ -20,7 +20,7 @@ package org.elasticsearch.benchmark.recovery;
 
 import org.elasticsearch.action.admin.indices.recovery.RecoveryResponse;
 import org.elasticsearch.action.admin.indices.recovery.ShardRecoveryResponse;
-import org.elasticsearch.bootstrap.Bootstrap;
+import org.elasticsearch.bootstrap.BootstrapForTesting;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider;
@@ -57,7 +57,7 @@ public class ReplicaRecoveryBenchmark {
 
     public static void main(String[] args) throws Exception {
         System.setProperty("es.logger.prefix", "");
-        Bootstrap.initializeNatives(true, false);
+        BootstrapForTesting.ensureInitialized();
 
         Settings settings = settingsBuilder()
                 .put("gateway.type", "local")
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/GlobalOrdinalsBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/GlobalOrdinalsBenchmark.java
index 808d64d..c986dc4 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/GlobalOrdinalsBenchmark.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/GlobalOrdinalsBenchmark.java
@@ -21,12 +21,13 @@ package org.elasticsearch.benchmark.search.aggregations;
 import com.carrotsearch.hppc.IntIntHashMap;
 import com.carrotsearch.hppc.ObjectHashSet;
 import com.carrotsearch.randomizedtesting.generators.RandomStrings;
+
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
 import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;
 import org.elasticsearch.action.bulk.BulkRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.benchmark.search.aggregations.TermsAggregationSearchBenchmark.StatsResult;
-import org.elasticsearch.bootstrap.Bootstrap;
+import org.elasticsearch.bootstrap.BootstrapForTesting;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeValue;
@@ -66,7 +67,7 @@ public class GlobalOrdinalsBenchmark {
 
     public static void main(String[] args) throws Exception {
         System.setProperty("es.logger.prefix", "");
-        Bootstrap.initializeNatives(true, false);
+        BootstrapForTesting.ensureInitialized();
         Random random = new Random();
 
         Settings settings = settingsBuilder()
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/SubAggregationSearchCollectModeBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/SubAggregationSearchCollectModeBenchmark.java
index 91fc72b..464c67a 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/SubAggregationSearchCollectModeBenchmark.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/SubAggregationSearchCollectModeBenchmark.java
@@ -27,7 +27,7 @@ import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;
 import org.elasticsearch.action.bulk.BulkRequestBuilder;
 import org.elasticsearch.action.bulk.BulkResponse;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.bootstrap.Bootstrap;
+import org.elasticsearch.bootstrap.BootstrapForTesting;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.StopWatch;
@@ -71,7 +71,7 @@ public class SubAggregationSearchCollectModeBenchmark {
     static Node[] nodes;
 
     public static void main(String[] args) throws Exception {
-        Bootstrap.initializeNatives(true, false);
+        BootstrapForTesting.ensureInitialized();
         Random random = new Random();
 
         Settings settings = settingsBuilder()
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchAndIndexingBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchAndIndexingBenchmark.java
index 517f6ba..0d11da0 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchAndIndexingBenchmark.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchAndIndexingBenchmark.java
@@ -20,13 +20,14 @@ package org.elasticsearch.benchmark.search.aggregations;
 
 import com.carrotsearch.hppc.ObjectScatterSet;
 import com.carrotsearch.randomizedtesting.generators.RandomStrings;
+
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
 import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;
 import org.elasticsearch.action.bulk.BulkRequestBuilder;
 import org.elasticsearch.action.bulk.BulkResponse;
 import org.elasticsearch.action.get.GetResponse;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.bootstrap.Bootstrap;
+import org.elasticsearch.bootstrap.BootstrapForTesting;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.settings.Settings;
@@ -71,7 +72,7 @@ public class TermsAggregationSearchAndIndexingBenchmark {
     static Node[] nodes;
 
     public static void main(String[] args) throws Exception {
-        Bootstrap.initializeNatives(true, false);
+        BootstrapForTesting.ensureInitialized();
         Settings settings = settingsBuilder()
                 .put("refresh_interval", "-1")
                 .put(SETTING_NUMBER_OF_SHARDS, 1)
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java
index e669b0d..e278feb 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java
@@ -28,7 +28,7 @@ import org.elasticsearch.action.bulk.BulkRequestBuilder;
 import org.elasticsearch.action.bulk.BulkResponse;
 import org.elasticsearch.action.search.SearchRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.bootstrap.Bootstrap;
+import org.elasticsearch.bootstrap.BootstrapForTesting;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.StopWatch;
@@ -99,7 +99,7 @@ public class TermsAggregationSearchBenchmark {
     }
 
     public static void main(String[] args) throws Exception {
-        Bootstrap.initializeNatives(true, false);
+        BootstrapForTesting.ensureInitialized();
         Random random = new Random();
 
         Settings settings = settingsBuilder()
diff --git a/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java b/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java
index 7f3ce81..84f53d1 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java
@@ -35,6 +35,7 @@ import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.*;
 import org.elasticsearch.transport.netty.NettyTransport;
 
+import java.net.InetAddress;
 import java.util.concurrent.CountDownLatch;
 
 import static org.elasticsearch.transport.TransportRequestOptions.options;
@@ -44,7 +45,7 @@ import static org.elasticsearch.transport.TransportRequestOptions.options;
  */
 public class BenchmarkNettyLargeMessages {
 
-    public static void main(String[] args) throws InterruptedException {
+    public static void main(String[] args) throws Exception {
         final ByteSizeValue payloadSize = new ByteSizeValue(10, ByteSizeUnit.MB);
         final int NUMBER_OF_ITERATIONS = 100000;
         final int NUMBER_OF_CLIENTS = 5;
@@ -63,7 +64,7 @@ public class BenchmarkNettyLargeMessages {
                 new NettyTransport(settings, threadPool, networkService, BigArrays.NON_RECYCLING_INSTANCE, Version.CURRENT, new NamedWriteableRegistry()), threadPool
         ).start();
 
-        final DiscoveryNode bigNode = new DiscoveryNode("big", new InetSocketTransportAddress("localhost", 9300), Version.CURRENT);
+        final DiscoveryNode bigNode = new DiscoveryNode("big", new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9300), Version.CURRENT);
 //        final DiscoveryNode smallNode = new DiscoveryNode("small", new InetSocketTransportAddress("localhost", 9300));
         final DiscoveryNode smallNode = bigNode;
 
diff --git a/core/src/test/java/org/elasticsearch/benchmark/transport/netty/NettyEchoBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/transport/netty/NettyEchoBenchmark.java
index 61686eb..fd76504 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/transport/netty/NettyEchoBenchmark.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/transport/netty/NettyEchoBenchmark.java
@@ -27,13 +27,14 @@ import org.jboss.netty.channel.*;
 import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;
 import org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory;
 
+import java.net.InetAddress;
 import java.net.InetSocketAddress;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.Executors;
 
 public class NettyEchoBenchmark {
 
-    public static void main(String[] args) {
+    public static void main(String[] args) throws Exception {
         final int payloadSize = 100;
         int CYCLE_SIZE = 50000;
         final long NUMBER_OF_ITERATIONS = 500000;
@@ -58,7 +59,7 @@ public class NettyEchoBenchmark {
         });
 
         // Bind and start to accept incoming connections.
-        serverBootstrap.bind(new InetSocketAddress(9000));
+        serverBootstrap.bind(new InetSocketAddress(InetAddress.getLoopbackAddress(), 9000));
 
         ClientBootstrap clientBootstrap = new ClientBootstrap(
                 new NioClientSocketChannelFactory(
@@ -78,7 +79,7 @@ public class NettyEchoBenchmark {
         });
 
         // Start the connection attempt.
-        ChannelFuture future = clientBootstrap.connect(new InetSocketAddress("localhost", 9000));
+        ChannelFuture future = clientBootstrap.connect(new InetSocketAddress(InetAddress.getLoopbackAddress(), 9000));
         future.awaitUninterruptibly();
         Channel clientChannel = future.getChannel();
 
diff --git a/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java b/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java
index 0106f2a..83ae875 100644
--- a/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java
+++ b/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java
@@ -101,7 +101,7 @@ public class BootstrapForTesting {
                     }
                 }
                 // java.io.tmpdir
-                Security.addPath(perms, javaTmpDir, "read,readlink,write,delete");
+                Security.addPath(perms, "java.io.tmpdir", javaTmpDir, "read,readlink,write,delete");
                 // custom test config file
                 if (Strings.hasLength(System.getProperty("tests.config"))) {
                     perms.add(new FilePermission(System.getProperty("tests.config"), "read,readlink"));
diff --git a/core/src/test/java/org/elasticsearch/bootstrap/JarHellTests.java b/core/src/test/java/org/elasticsearch/bootstrap/JarHellTests.java
index 50c68eb..2d82567 100644
--- a/core/src/test/java/org/elasticsearch/bootstrap/JarHellTests.java
+++ b/core/src/test/java/org/elasticsearch/bootstrap/JarHellTests.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.bootstrap;
 
 import org.elasticsearch.Version;
+import org.elasticsearch.common.Strings;
 import org.elasticsearch.test.ESTestCase;
 
 import java.io.IOException;
@@ -27,6 +28,8 @@ import java.net.URL;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.StandardOpenOption;
+import java.util.ArrayList;
+import java.util.List;
 import java.util.jar.Attributes;
 import java.util.jar.JarOutputStream;
 import java.util.jar.Manifest;
@@ -153,22 +156,25 @@ public class JarHellTests extends ESTestCase {
 
     public void testRequiredJDKVersionTooOld() throws Exception {
         Path dir = createTempDir();
-        String previousJavaVersion = System.getProperty("java.specification.version");
-        System.setProperty("java.specification.version", "1.7");
+        List<Integer> current = JavaVersion.current().getVersion();
+        List<Integer> target = new ArrayList<>(current.size());
+        for (int i = 0; i < current.size(); i++) {
+            target.add(current.get(i) + 1);
+        }
+        JavaVersion targetVersion = JavaVersion.parse(Strings.collectionToDelimitedString(target, "."));
+
 
         Manifest manifest = new Manifest();
         Attributes attributes = manifest.getMainAttributes();
         attributes.put(Attributes.Name.MANIFEST_VERSION, "1.0.0");
-        attributes.put(new Attributes.Name("X-Compile-Target-JDK"), "1.8");
+        attributes.put(new Attributes.Name("X-Compile-Target-JDK"), targetVersion.toString());
         URL[] jars = {makeJar(dir, "foo.jar", manifest, "Foo.class")};
         try {
             JarHell.checkJarHell(jars);
             fail("did not get expected exception");
         } catch (IllegalStateException e) {
-            assertTrue(e.getMessage().contains("requires Java 1.8"));
-            assertTrue(e.getMessage().contains("your system: 1.7"));
-        } finally {
-            System.setProperty("java.specification.version", previousJavaVersion);
+            assertTrue(e.getMessage().contains("requires Java " + targetVersion.toString()));
+            assertTrue(e.getMessage().contains("your system: " + JavaVersion.current().toString()));
         }
     }
 
@@ -213,7 +219,12 @@ public class JarHellTests extends ESTestCase {
         attributes.put(Attributes.Name.MANIFEST_VERSION, "1.0.0");
         attributes.put(new Attributes.Name("X-Compile-Target-JDK"), "bogus");
         URL[] jars = {makeJar(dir, "foo.jar", manifest, "Foo.class")};
-        JarHell.checkJarHell(jars);
+        try {
+            JarHell.checkJarHell(jars);
+            fail("did not get expected exception");
+        } catch (IllegalStateException e) {
+            assertTrue(e.getMessage().equals("version string must be a sequence of nonnegative decimal integers separated by \".\"'s and may have leading zeros but was bogus"));
+        }
     }
 
     /** make sure if a plugin is compiled against the same ES version, it works */
@@ -242,4 +253,26 @@ public class JarHellTests extends ESTestCase {
             assertTrue(e.getMessage().contains("requires Elasticsearch 1.0-bogus"));
         }
     }
+
+    public void testValidVersions() {
+        String[] versions = new String[]{"1.7", "1.7.0", "0.1.7", "1.7.0.80"};
+        for (String version : versions) {
+            try {
+                JarHell.checkVersionFormat(version);
+            } catch (IllegalStateException e) {
+                fail(version + " should be accepted as a valid version format");
+            }
+        }
+    }
+
+    public void testInvalidVersions() {
+        String[] versions = new String[]{"", "1.7.0_80", "1.7."};
+        for (String version : versions) {
+            try {
+                JarHell.checkVersionFormat(version);
+                fail("\"" + version + "\"" + " should be rejected as an invalid version format");
+            } catch (IllegalStateException e) {
+            }
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/bootstrap/JavaVersionTests.java b/core/src/test/java/org/elasticsearch/bootstrap/JavaVersionTests.java
new file mode 100644
index 0000000..851e0fd
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/bootstrap/JavaVersionTests.java
@@ -0,0 +1,79 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.bootstrap;
+
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.util.List;
+
+import static org.hamcrest.CoreMatchers.is;
+
+public class JavaVersionTests extends ESTestCase {
+    @Test
+    public void testParse() {
+        JavaVersion javaVersion = JavaVersion.parse("1.7.0");
+        List<Integer> version = javaVersion.getVersion();
+        assertThat(3, is(version.size()));
+        assertThat(1, is(version.get(0)));
+        assertThat(7, is(version.get(1)));
+        assertThat(0, is(version.get(2)));
+    }
+
+    @Test
+    public void testToString() {
+        JavaVersion javaVersion = JavaVersion.parse("1.7.0");
+        assertThat("1.7.0", is(javaVersion.toString()));
+    }
+
+    @Test
+    public void testCompare() {
+        JavaVersion onePointSix = JavaVersion.parse("1.6");
+        JavaVersion onePointSeven = JavaVersion.parse("1.7");
+        JavaVersion onePointSevenPointZero = JavaVersion.parse("1.7.0");
+        JavaVersion onePointSevenPointOne = JavaVersion.parse("1.7.1");
+        JavaVersion onePointSevenPointTwo = JavaVersion.parse("1.7.2");
+        JavaVersion onePointSevenPointOnePointOne = JavaVersion.parse("1.7.1.1");
+        JavaVersion onePointSevenPointTwoPointOne = JavaVersion.parse("1.7.2.1");
+
+        assertTrue(onePointSix.compareTo(onePointSeven) < 0);
+        assertTrue(onePointSeven.compareTo(onePointSix) > 0);
+        assertTrue(onePointSix.compareTo(onePointSix) == 0);
+        assertTrue(onePointSeven.compareTo(onePointSevenPointZero) == 0);
+        assertTrue(onePointSevenPointOnePointOne.compareTo(onePointSevenPointOne) > 0);
+        assertTrue(onePointSevenPointTwo.compareTo(onePointSevenPointTwoPointOne) < 0);
+    }
+
+    @Test
+    public void testValidVersions() {
+        String[] versions = new String[]{"1.7", "1.7.0", "0.1.7", "1.7.0.80"};
+        for (String version : versions) {
+            assertTrue(JavaVersion.isValid(version));
+        }
+    }
+
+    @Test
+    public void testInvalidVersions() {
+        String[] versions = new String[]{"", "1.7.0_80", "1.7."};
+        for (String version : versions) {
+            assertFalse(JavaVersion.isValid(version));
+        }
+    }
+}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/bootstrap/SecurityTests.java b/core/src/test/java/org/elasticsearch/bootstrap/SecurityTests.java
index 66abc4a..eaa2592 100644
--- a/core/src/test/java/org/elasticsearch/bootstrap/SecurityTests.java
+++ b/core/src/test/java/org/elasticsearch/bootstrap/SecurityTests.java
@@ -244,7 +244,7 @@ public class SecurityTests extends ESTestCase {
             assumeNoException("test cannot create symbolic links with security manager enabled", e);
         }
         Permissions permissions = new Permissions();
-        Security.addPath(permissions, link, "read");
+        Security.addPath(permissions, "testing", link, "read");
         assertExactPermissions(new FilePermission(link.toString(), "read"), permissions);
         assertExactPermissions(new FilePermission(link.resolve("foo").toString(), "read"), permissions);
         assertExactPermissions(new FilePermission(target.toString(), "read"), permissions);
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/GetIndexBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/bwcompat/GetIndexBackwardsCompatibilityIT.java
index a7e9380..d26d049 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/GetIndexBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/GetIndexBackwardsCompatibilityIT.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.bwcompat;
 
+import com.google.common.collect.ImmutableList;
 
 import org.elasticsearch.action.admin.indices.alias.Alias;
 import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;
@@ -32,8 +33,6 @@ import org.elasticsearch.search.warmer.IndexWarmersMetaData.Entry;
 import org.elasticsearch.test.ESBackcompatTestCase;
 import org.junit.Test;
 
-import java.util.List;
-
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.hamcrest.Matchers.anyOf;
 import static org.hamcrest.Matchers.equalTo;
@@ -47,10 +46,10 @@ public class GetIndexBackwardsCompatibilityIT extends ESBackcompatTestCase {
         assertAcked(createIndexResponse);
         GetIndexResponse getIndexResponse = client().admin().indices().prepareGetIndex().addIndices("test").addFeatures(Feature.ALIASES)
                 .execute().actionGet();
-        ImmutableOpenMap<String, List<AliasMetaData>> aliasesMap = getIndexResponse.aliases();
+        ImmutableOpenMap<String, ImmutableList<AliasMetaData>> aliasesMap = getIndexResponse.aliases();
         assertThat(aliasesMap, notNullValue());
         assertThat(aliasesMap.size(), equalTo(1));
-        List<AliasMetaData> aliasesList = aliasesMap.get("test");
+        ImmutableList<AliasMetaData> aliasesList = aliasesMap.get("test");
         assertThat(aliasesList, notNullValue());
         assertThat(aliasesList.size(), equalTo(1));
         AliasMetaData alias = aliasesList.get(0);
@@ -101,10 +100,10 @@ public class GetIndexBackwardsCompatibilityIT extends ESBackcompatTestCase {
         ensureSearchable("test");
         GetIndexResponse getIndexResponse = client().admin().indices().prepareGetIndex().addIndices("test").addFeatures(Feature.WARMERS)
                 .execute().actionGet();
-        ImmutableOpenMap<String, List<Entry>> warmersMap = getIndexResponse.warmers();
+        ImmutableOpenMap<String, ImmutableList<Entry>> warmersMap = getIndexResponse.warmers();
         assertThat(warmersMap, notNullValue());
         assertThat(warmersMap.size(), equalTo(1));
-        List<Entry> warmersList = warmersMap.get("test");
+        ImmutableList<Entry> warmersList = warmersMap.get("test");
         assertThat(warmersList, notNullValue());
         assertThat(warmersList.size(), equalTo(1));
         Entry warmer = warmersList.get(0);
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/UnicastBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/bwcompat/UnicastBackwardsCompatibilityIT.java
index f90eae4..59dd669 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/UnicastBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/UnicastBackwardsCompatibilityIT.java
@@ -33,7 +33,6 @@ public class UnicastBackwardsCompatibilityIT extends ESBackcompatTestCase {
         return Settings.builder()
                 .put(super.nodeSettings(nodeOrdinal))
                 .put("transport.tcp.port", 9380 + nodeOrdinal)
-                .put("discovery.zen.ping.multicast.enabled", false)
                 .put("discovery.zen.ping.unicast.hosts", "localhost:9380,localhost:9381,localhost:9390,localhost:9391")
                 .build();
     }
@@ -43,7 +42,6 @@ public class UnicastBackwardsCompatibilityIT extends ESBackcompatTestCase {
         return Settings.settingsBuilder()
                 .put(super.externalNodeSettings(nodeOrdinal))
                 .put("transport.tcp.port", 9390 + nodeOrdinal)
-                .put("discovery.zen.ping.multicast.enabled", false)
                 .put("discovery.zen.ping.unicast.hosts", "localhost:9380,localhost:9381,localhost:9390,localhost:9391")
                 .build();
     }
diff --git a/core/src/test/java/org/elasticsearch/client/transport/FailAndRetryMockTransport.java b/core/src/test/java/org/elasticsearch/client/transport/FailAndRetryMockTransport.java
index fcf64e0..95456fd 100644
--- a/core/src/test/java/org/elasticsearch/client/transport/FailAndRetryMockTransport.java
+++ b/core/src/test/java/org/elasticsearch/client/transport/FailAndRetryMockTransport.java
@@ -123,7 +123,7 @@ abstract class FailAndRetryMockTransport<Response extends TransportResponse> imp
     }
 
     @Override
-    public TransportAddress[] addressesFromString(String address) throws Exception {
+    public TransportAddress[] addressesFromString(String address, int perAddressLimit) throws Exception {
         throw new UnsupportedOperationException();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/client/transport/TransportClientNodesServiceTests.java b/core/src/test/java/org/elasticsearch/client/transport/TransportClientNodesServiceTests.java
index ae107b9..a1e10b0 100644
--- a/core/src/test/java/org/elasticsearch/client/transport/TransportClientNodesServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/client/transport/TransportClientNodesServiceTests.java
@@ -32,6 +32,8 @@ import org.elasticsearch.transport.*;
 import org.junit.Test;
 
 import java.io.Closeable;
+import java.util.Collections;
+import java.util.List;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicInteger;
@@ -54,6 +56,11 @@ public class TransportClientNodesServiceTests extends ESTestCase {
             threadPool = new ThreadPool("transport-client-nodes-service-tests");
             transport = new FailAndRetryMockTransport<TestResponse>(getRandom()) {
                 @Override
+                public List<String> getLocalAddresses() {
+                    return Collections.EMPTY_LIST;
+                }
+
+                @Override
                 protected TestResponse newResponse() {
                     return  new TestResponse();
                 }
diff --git a/core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryIT.java b/core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryIT.java
index 307cc0d..42fa3fc 100644
--- a/core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryIT.java
+++ b/core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryIT.java
@@ -60,7 +60,7 @@ public class TransportClientRetryIT extends ESIntegTestCase {
 
         Settings.Builder builder = settingsBuilder().put("client.transport.nodes_sampler_interval", "1s")
                 .put("name", "transport_client_retry_test")
-                .put("node.mode", InternalTestCluster.nodeMode())
+                .put("node.mode", internalCluster().getNodeMode())
                 .put(ClusterName.SETTING, internalCluster().getClusterName())
                 .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true)
                 .put("path.home", createTempDir());
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java b/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java
index 71c2c29..9776f66 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java
@@ -66,6 +66,7 @@ import static org.hamcrest.Matchers.notNullValue;
  *
  */
 @ClusterScope(scope = Scope.TEST, numDataNodes = 0)
+@ESIntegTestCase.SuppressLocalMode
 public class ClusterServiceIT extends ESIntegTestCase {
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java b/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java
index 1aa1602..edd6254 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.cluster;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.block.ClusterBlock;
@@ -43,7 +44,6 @@ import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
-import java.util.Collections;
 import java.util.List;
 
 import static org.elasticsearch.cluster.metadata.AliasMetaData.newAliasMetaDataBuilder;
@@ -659,14 +659,14 @@ public class ClusterStateDiffIT extends ESIntegTestCase {
                                 new SnapshotId(randomName("repo"), randomName("snap")),
                                 randomBoolean(),
                                 SnapshotsInProgress.State.fromValue((byte) randomIntBetween(0, 6)),
-                                Collections.<String>emptyList(),
+                                ImmutableList.<String>of(),
                                 Math.abs(randomLong()),
                                 ImmutableMap.<ShardId, SnapshotsInProgress.ShardSnapshotStatus>of()));
                     case 1:
                         return new RestoreInProgress(new RestoreInProgress.Entry(
                                 new SnapshotId(randomName("repo"), randomName("snap")),
                                 RestoreInProgress.State.fromValue((byte) randomIntBetween(0, 3)),
-                                Collections.<String>emptyList(),
+                                ImmutableList.<String>of(),
                                 ImmutableMap.<ShardId, RestoreInProgress.ShardRestoreStatus>of()));
                     default:
                         throw new IllegalArgumentException("Shouldn't be here");
diff --git a/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java b/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java
index 6939df2..a00679d 100644
--- a/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java
@@ -44,6 +44,7 @@ import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitC
 import static org.hamcrest.Matchers.*;
 
 @ClusterScope(scope = Scope.TEST, numDataNodes = 0)
+@ESIntegTestCase.SuppressLocalMode
 public class MinimumMasterNodesIT extends ESIntegTestCase {
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java b/core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java
index de0952e..28ef5e1 100644
--- a/core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java
@@ -57,6 +57,7 @@ import static org.hamcrest.Matchers.lessThan;
 /**
  */
 @ClusterScope(scope = Scope.TEST, numDataNodes = 0)
+@ESIntegTestCase.SuppressLocalMode
 public class NoMasterNodeIT extends ESIntegTestCase {
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/cluster/SpecificMasterNodesIT.java b/core/src/test/java/org/elasticsearch/cluster/SpecificMasterNodesIT.java
index 2876f79..43b403d 100644
--- a/core/src/test/java/org/elasticsearch/cluster/SpecificMasterNodesIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/SpecificMasterNodesIT.java
@@ -34,6 +34,7 @@ import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcke
 import static org.hamcrest.Matchers.*;
 
 @ClusterScope(scope = Scope.TEST, numDataNodes = 0)
+@ESIntegTestCase.SuppressLocalMode
 public class SpecificMasterNodesIT extends ESIntegTestCase {
 
     protected final Settings.Builder settingsBuilder() {
diff --git a/core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java b/core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java
index 1ec4c23..84b31e3 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java
@@ -46,8 +46,8 @@ import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
 import com.google.common.base.Predicate;
+import com.google.common.collect.ImmutableList;
 
-import java.util.List;
 import java.util.concurrent.TimeUnit;
 
 import static org.elasticsearch.cluster.metadata.IndexMetaData.*;
@@ -100,7 +100,7 @@ public class AckIT extends ESIntegTestCase {
         for (Client client : clients()) {
             GetWarmersResponse getWarmersResponse = client.admin().indices().prepareGetWarmers().setLocal(true).get();
             assertThat(getWarmersResponse.warmers().size(), equalTo(1));
-            ObjectObjectCursor<String, List<IndexWarmersMetaData.Entry>> entry = getWarmersResponse.warmers().iterator().next();
+            ObjectObjectCursor<String, ImmutableList<IndexWarmersMetaData.Entry>> entry = getWarmersResponse.warmers().iterator().next();
             assertThat(entry.key, equalTo("test"));
             assertThat(entry.value.size(), equalTo(1));
             assertThat(entry.value.get(0).name(), equalTo("custom_warmer"));
diff --git a/core/src/test/java/org/elasticsearch/cluster/allocation/SimpleAllocationIT.java b/core/src/test/java/org/elasticsearch/cluster/allocation/SimpleAllocationIT.java
index 27b221d..1360937 100644
--- a/core/src/test/java/org/elasticsearch/cluster/allocation/SimpleAllocationIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/allocation/SimpleAllocationIT.java
@@ -18,12 +18,16 @@
  */
 package org.elasticsearch.cluster.allocation;
 
+import org.elasticsearch.cluster.ClusterInfoService;
+import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.InternalClusterInfoService;
 import org.elasticsearch.cluster.routing.RoutingNode;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.hamcrest.Matchers.equalTo;
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/AllocationIdTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/AllocationIdTests.java
index 9394dc8..262b3db 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/AllocationIdTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/AllocationIdTests.java
@@ -35,7 +35,7 @@ public class AllocationIdTests extends ESTestCase {
         assertThat(shard.allocationId(), nullValue());
 
         logger.info("-- initialize the shard");
-        shard.initialize("node1");
+        shard.initialize("node1", -1);
         AllocationId allocationId = shard.allocationId();
         assertThat(allocationId, notNullValue());
         assertThat(allocationId.getId(), notNullValue());
@@ -53,12 +53,12 @@ public class AllocationIdTests extends ESTestCase {
     public void testSuccessfulRelocation() {
         logger.info("-- build started shard");
         ShardRouting shard = ShardRouting.newUnassigned("test", 0, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, null));
-        shard.initialize("node1");
+        shard.initialize("node1", -1);
         shard.moveToStarted();
 
         AllocationId allocationId = shard.allocationId();
         logger.info("-- relocate the shard");
-        shard.relocate("node2");
+        shard.relocate("node2", -1);
         assertThat(shard.allocationId(), not(equalTo(allocationId)));
         assertThat(shard.allocationId().getId(), equalTo(allocationId.getId()));
         assertThat(shard.allocationId().getRelocationId(), notNullValue());
@@ -77,12 +77,12 @@ public class AllocationIdTests extends ESTestCase {
     public void testCancelRelocation() {
         logger.info("-- build started shard");
         ShardRouting shard = ShardRouting.newUnassigned("test", 0, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, null));
-        shard.initialize("node1");
+        shard.initialize("node1", -1);
         shard.moveToStarted();
 
         AllocationId allocationId = shard.allocationId();
         logger.info("-- relocate the shard");
-        shard.relocate("node2");
+        shard.relocate("node2", -1);
         assertThat(shard.allocationId(), not(equalTo(allocationId)));
         assertThat(shard.allocationId().getId(), equalTo(allocationId.getId()));
         assertThat(shard.allocationId().getRelocationId(), notNullValue());
@@ -98,7 +98,7 @@ public class AllocationIdTests extends ESTestCase {
     public void testMoveToUnassigned() {
         logger.info("-- build started shard");
         ShardRouting shard = ShardRouting.newUnassigned("test", 0, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, null));
-        shard.initialize("node1");
+        shard.initialize("node1", -1);
         shard.moveToStarted();
 
         logger.info("-- move to unassigned");
@@ -110,7 +110,7 @@ public class AllocationIdTests extends ESTestCase {
     public void testReinitializing() {
         logger.info("-- build started shard");
         ShardRouting shard = ShardRouting.newUnassigned("test", 0, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, null));
-        shard.initialize("node1");
+        shard.initialize("node1", -1);
         shard.moveToStarted();
         AllocationId allocationId = shard.allocationId();
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RandomShardRoutingMutator.java b/core/src/test/java/org/elasticsearch/cluster/routing/RandomShardRoutingMutator.java
index 4ca849d..b451183 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/RandomShardRoutingMutator.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/RandomShardRoutingMutator.java
@@ -42,7 +42,7 @@ public final class RandomShardRoutingMutator {
                 break;
             case 1:
                 if (shardRouting.unassigned()) {
-                    shardRouting.initialize(randomFrom(nodes));
+                    shardRouting.initialize(randomFrom(nodes), -1);
                 }
                 break;
             case 2:
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingHelper.java b/core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingHelper.java
index f2eb15a..d7ef26c 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingHelper.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingHelper.java
@@ -25,7 +25,11 @@ package org.elasticsearch.cluster.routing;
 public class ShardRoutingHelper {
 
     public static void relocate(ShardRouting routing, String nodeId) {
-        routing.relocate(nodeId);
+        relocate(routing, nodeId, -1);
+    }
+
+    public static void relocate(ShardRouting routing, String nodeId, long expectedByteSize) {
+        routing.relocate(nodeId, expectedByteSize);
     }
 
     public static void moveToStarted(ShardRouting routing) {
@@ -33,6 +37,10 @@ public class ShardRoutingHelper {
     }
 
     public static void initialize(ShardRouting routing, String nodeId) {
-        routing.initialize(nodeId);
+        initialize(routing, nodeId, -1);
+    }
+
+    public static void initialize(ShardRouting routing, String nodeId, long expectedSize) {
+        routing.initialize(nodeId, expectedSize);
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java
index 79b7ec5..146e80c 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java
@@ -103,12 +103,12 @@ public class ShardRoutingTests extends ESTestCase {
         ShardRouting startedShard1 = new ShardRouting(initializingShard1);
         startedShard1.moveToStarted();
         ShardRouting sourceShard0a = new ShardRouting(startedShard0);
-        sourceShard0a.relocate("node2");
+        sourceShard0a.relocate("node2", -1);
         ShardRouting targetShard0a = sourceShard0a.buildTargetRelocatingShard();
         ShardRouting sourceShard0b = new ShardRouting(startedShard0);
-        sourceShard0b.relocate("node2");
+        sourceShard0b.relocate("node2", -1);
         ShardRouting sourceShard1 = new ShardRouting(startedShard1);
-        sourceShard1.relocate("node2");
+        sourceShard1.relocate("node2", -1);
 
         // test true scenarios
         assertTrue(targetShard0a.isRelocationTargetOf(sourceShard0a));
@@ -254,7 +254,7 @@ public class ShardRoutingTests extends ESTestCase {
             }
 
             try {
-                routing.initialize("boom");
+                routing.initialize("boom", -1);
                 fail("must be frozen");
             } catch (IllegalStateException ex) {
                 // expected
@@ -273,7 +273,7 @@ public class ShardRoutingTests extends ESTestCase {
             }
 
             try {
-                routing.relocate("foobar");
+                routing.relocate("foobar", -1);
                 fail("must be frozen");
             } catch (IllegalStateException ex) {
                 // expected
@@ -287,4 +287,39 @@ public class ShardRoutingTests extends ESTestCase {
             assertEquals(version, routing.version());
         }
     }
+
+    public void testExpectedSize() throws IOException {
+        final int iters = randomIntBetween(10, 100);
+        for (int i = 0; i < iters; i++) {
+            ShardRouting routing = randomShardRouting("test", 0);
+            long byteSize = randomIntBetween(0, Integer.MAX_VALUE);
+            if (routing.unassigned()) {
+                ShardRoutingHelper.initialize(routing, "foo", byteSize);
+            } else if (routing.started()) {
+                ShardRoutingHelper.relocate(routing, "foo", byteSize);
+            } else {
+                byteSize = -1;
+            }
+            if (randomBoolean()) {
+                BytesStreamOutput out = new BytesStreamOutput();
+                routing.writeTo(out);
+                routing = ShardRouting.readShardRoutingEntry(StreamInput.wrap(out.bytes()));
+            }
+            if (routing.initializing() || routing.relocating()) {
+                assertEquals(routing.toString(), byteSize, routing.getExpectedShardSize());
+                if (byteSize >= 0) {
+                    assertTrue(routing.toString(), routing.toString().contains("expected_shard_size[" + byteSize + "]"));
+                }
+                if (routing.initializing()) {
+                    routing = new ShardRouting(routing);
+                    routing.moveToStarted();
+                    assertEquals(-1, routing.getExpectedShardSize());
+                    assertFalse(routing.toString(), routing.toString().contains("expected_shard_size[" + byteSize + "]"));
+                }
+            } else {
+                assertFalse(routing.toString(), routing.toString().contains("expected_shard_size [" + byteSize + "]"));
+                assertEquals(byteSize, routing.getExpectedShardSize());
+            }
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/TestShardRouting.java b/core/src/test/java/org/elasticsearch/cluster/routing/TestShardRouting.java
index 0d60b6b..df9e1f8 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/TestShardRouting.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/TestShardRouting.java
@@ -28,25 +28,25 @@ import org.elasticsearch.test.ESTestCase;
 public class TestShardRouting {
 
     public static ShardRouting newShardRouting(String index, int shardId, String currentNodeId, boolean primary, ShardRoutingState state, long version) {
-        return new ShardRouting(index, shardId, currentNodeId, null, null, primary, state, version, buildUnassignedInfo(state), buildAllocationId(state), true);
+        return new ShardRouting(index, shardId, currentNodeId, null, null, primary, state, version, buildUnassignedInfo(state), buildAllocationId(state), true, -1);
     }
 
     public static ShardRouting newShardRouting(String index, int shardId, String currentNodeId, String relocatingNodeId, boolean primary, ShardRoutingState state, long version) {
-        return new ShardRouting(index, shardId, currentNodeId, relocatingNodeId, null, primary, state, version, buildUnassignedInfo(state), buildAllocationId(state), true);
+        return new ShardRouting(index, shardId, currentNodeId, relocatingNodeId, null, primary, state, version, buildUnassignedInfo(state), buildAllocationId(state), true, -1);
     }
 
     public static ShardRouting newShardRouting(String index, int shardId, String currentNodeId, String relocatingNodeId, boolean primary, ShardRoutingState state, AllocationId allocationId, long version) {
-        return new ShardRouting(index, shardId, currentNodeId, relocatingNodeId, null, primary, state, version, buildUnassignedInfo(state), allocationId, true);
+        return new ShardRouting(index, shardId, currentNodeId, relocatingNodeId, null, primary, state, version, buildUnassignedInfo(state), allocationId, true, -1);
     }
 
     public static ShardRouting newShardRouting(String index, int shardId, String currentNodeId, String relocatingNodeId, RestoreSource restoreSource, boolean primary, ShardRoutingState state, long version) {
-        return new ShardRouting(index, shardId, currentNodeId, relocatingNodeId, restoreSource, primary, state, version, buildUnassignedInfo(state), buildAllocationId(state), true);
+        return new ShardRouting(index, shardId, currentNodeId, relocatingNodeId, restoreSource, primary, state, version, buildUnassignedInfo(state), buildAllocationId(state), true, -1);
     }
 
     public static ShardRouting newShardRouting(String index, int shardId, String currentNodeId,
                                                String relocatingNodeId, RestoreSource restoreSource, boolean primary, ShardRoutingState state, long version,
                                                UnassignedInfo unassignedInfo) {
-        return new ShardRouting(index, shardId, currentNodeId, relocatingNodeId, restoreSource, primary, state, version, unassignedInfo, buildAllocationId(state), true);
+        return new ShardRouting(index, shardId, currentNodeId, relocatingNodeId, restoreSource, primary, state, version, unassignedInfo, buildAllocationId(state), true, -1);
     }
 
     private static AllocationId buildAllocationId(ShardRoutingState state) {
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java
index c6929d8..a0b610c 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java
@@ -21,6 +21,7 @@ package org.elasticsearch.cluster.routing;
 
 import com.carrotsearch.hppc.IntHashSet;
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterState;
@@ -37,7 +38,6 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
-import java.util.Collections;
 import java.util.EnumSet;
 
 import static org.elasticsearch.cluster.routing.ShardRoutingState.*;
@@ -192,7 +192,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
         ShardRouting shard = TestShardRouting.newShardRouting("test", 1, null, null, null, true, ShardRoutingState.UNASSIGNED, 1, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, null));
         ShardRouting mutable = new ShardRouting(shard);
         assertThat(mutable.unassignedInfo(), notNullValue());
-        mutable.initialize("test_node");
+        mutable.initialize("test_node", -1);
         assertThat(mutable.state(), equalTo(ShardRoutingState.INITIALIZING));
         assertThat(mutable.unassignedInfo(), notNullValue());
         mutable.moveToStarted();
@@ -251,7 +251,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
         assertThat(clusterState.getRoutingNodes().hasUnassigned(), equalTo(false));
         // fail shard
         ShardRouting shardToFail = clusterState.getRoutingNodes().shardsWithState(STARTED).get(0);
-        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyFailedShards(clusterState, Collections.singletonList(new FailedRerouteAllocation.FailedShard(shardToFail, "test fail", null)))).build();
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyFailedShards(clusterState, ImmutableList.of(new FailedRerouteAllocation.FailedShard(shardToFail, "test fail", null)))).build();
         // verify the reason and details
         assertThat(clusterState.getRoutingNodes().hasUnassigned(), equalTo(true));
         assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
index f9306ad..e17fe47 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
@@ -369,37 +369,37 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
                     switch (sr.id()) {
                         case 0:
                             if (sr.primary()) {
-                                allocation.routingNodes().initialize(sr, "node1");
+                                allocation.routingNodes().initialize(sr, "node1", -1);
                             } else {
-                                allocation.routingNodes().initialize(sr, "node0");
+                                allocation.routingNodes().initialize(sr, "node0", -1);
                             }
                             break;
                         case 1:
                             if (sr.primary()) {
-                                allocation.routingNodes().initialize(sr, "node1");
+                                allocation.routingNodes().initialize(sr, "node1", -1);
                             } else {
-                                allocation.routingNodes().initialize(sr, "node2");
+                                allocation.routingNodes().initialize(sr, "node2", -1);
                             }
                             break;
                         case 2:
                             if (sr.primary()) {
-                                allocation.routingNodes().initialize(sr, "node3");
+                                allocation.routingNodes().initialize(sr, "node3", -1);
                             } else {
-                                allocation.routingNodes().initialize(sr, "node2");
+                                allocation.routingNodes().initialize(sr, "node2", -1);
                             }
                             break;
                         case 3:
                             if (sr.primary()) {
-                                allocation.routingNodes().initialize(sr, "node3");
+                                allocation.routingNodes().initialize(sr, "node3", -1);
                             } else {
-                                allocation.routingNodes().initialize(sr, "node1");
+                                allocation.routingNodes().initialize(sr, "node1", -1);
                             }
                             break;
                         case 4:
                             if (sr.primary()) {
-                                allocation.routingNodes().initialize(sr, "node2");
+                                allocation.routingNodes().initialize(sr, "node2", -1);
                             } else {
-                                allocation.routingNodes().initialize(sr, "node0");
+                                allocation.routingNodes().initialize(sr, "node0", -1);
                             }
                             break;
                     }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ExpectedShardSizeAllocationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ExpectedShardSizeAllocationTests.java
new file mode 100644
index 0000000..e512fcd
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ExpectedShardSizeAllocationTests.java
@@ -0,0 +1,179 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cluster.routing.allocation;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.ClusterInfo;
+import org.elasticsearch.cluster.ClusterInfoService;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.cluster.node.DiscoveryNodes;
+import org.elasticsearch.cluster.routing.RoutingNodes;
+import org.elasticsearch.cluster.routing.RoutingTable;
+import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.routing.ShardRoutingState;
+import org.elasticsearch.cluster.routing.allocation.command.AllocationCommands;
+import org.elasticsearch.cluster.routing.allocation.command.MoveAllocationCommand;
+import org.elasticsearch.cluster.routing.allocation.decider.ShardsLimitAllocationDecider;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.test.ESAllocationTestCase;
+import org.junit.Test;
+
+import java.util.Collections;
+
+import static org.elasticsearch.cluster.routing.ShardRoutingState.*;
+import static org.elasticsearch.cluster.routing.allocation.RoutingNodesUtils.numberOfShardsOfType;
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.hamcrest.Matchers.equalTo;
+
+/**
+ */
+public class ExpectedShardSizeAllocationTests extends ESAllocationTestCase {
+
+    private final ESLogger logger = Loggers.getLogger(ExpectedShardSizeAllocationTests.class);
+
+    @Test
+    public void testInitializingHasExpectedSize() {
+        final long byteSize = randomIntBetween(0, Integer.MAX_VALUE);
+        AllocationService strategy = createAllocationService(Settings.EMPTY, new ClusterInfoService() {
+            @Override
+            public ClusterInfo getClusterInfo() {
+                return new ClusterInfo(Collections.EMPTY_MAP, Collections.EMPTY_MAP) {
+                    @Override
+                    public Long getShardSize(ShardRouting shardRouting) {
+                        if (shardRouting.index().equals("test") && shardRouting.shardId().getId() == 0) {
+                            return byteSize;
+                        }
+                        return null;
+                    }
+                };
+            }
+
+            @Override
+            public void addListener(Listener listener) {
+            }
+        });
+
+        logger.info("Building initial routing table");
+
+        MetaData metaData = MetaData.builder()
+                .put(IndexMetaData.builder("test").settings(settings(Version.CURRENT)
+                        .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
+                        .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1)))
+                .build();
+
+        RoutingTable routingTable = RoutingTable.builder()
+                .addAsNew(metaData.index("test"))
+                .build();
+
+        ClusterState clusterState = ClusterState.builder(org.elasticsearch.cluster.ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
+        logger.info("Adding one node and performing rerouting");
+        clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1"))).build();
+        routingTable = strategy.reroute(clusterState).routingTable();
+        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
+
+        assertEquals(1, clusterState.getRoutingNodes().node("node1").numberOfShardsWithState(ShardRoutingState.INITIALIZING));
+        assertEquals(byteSize, clusterState.getRoutingNodes().getRoutingTable().shardsWithState(ShardRoutingState.INITIALIZING).get(0).getExpectedShardSize());
+        logger.info("Start the primary shard");
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
+        routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
+        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
+
+        assertEquals(1, clusterState.getRoutingNodes().node("node1").numberOfShardsWithState(ShardRoutingState.STARTED));
+        assertEquals(1, clusterState.getRoutingNodes().unassigned().size());
+
+        logger.info("Add another one node and reroute");
+        clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).put(newNode("node2"))).build();
+        routingTable = strategy.reroute(clusterState).routingTable();
+        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
+
+        assertEquals(1, clusterState.getRoutingNodes().node("node2").numberOfShardsWithState(ShardRoutingState.INITIALIZING));
+        assertEquals(byteSize, clusterState.getRoutingNodes().getRoutingTable().shardsWithState(ShardRoutingState.INITIALIZING).get(0).getExpectedShardSize());
+    }
+
+    @Test
+    public void testExpectedSizeOnMove() {
+        final long byteSize = randomIntBetween(0, Integer.MAX_VALUE);
+        final AllocationService allocation = createAllocationService(Settings.EMPTY, new ClusterInfoService() {
+            @Override
+            public ClusterInfo getClusterInfo() {
+                return new ClusterInfo(Collections.EMPTY_MAP, Collections.EMPTY_MAP) {
+                    @Override
+                    public Long getShardSize(ShardRouting shardRouting) {
+                        if (shardRouting.index().equals("test") && shardRouting.shardId().getId() == 0) {
+                            return byteSize;
+                        }
+                        return null;
+                    }
+                };
+            }
+
+            @Override
+            public void addListener(Listener listener) {
+            }
+        });
+        logger.info("creating an index with 1 shard, no replica");
+        MetaData metaData = MetaData.builder()
+                .put(IndexMetaData.builder("test").settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(0))
+                .build();
+        RoutingTable routingTable = RoutingTable.builder()
+                .addAsNew(metaData.index("test"))
+                .build();
+        ClusterState clusterState = ClusterState.builder(org.elasticsearch.cluster.ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
+
+        logger.info("adding two nodes and performing rerouting");
+        clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2"))).build();
+        RoutingAllocation.Result rerouteResult = allocation.reroute(clusterState);
+        clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
+
+        logger.info("start primary shard");
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
+        clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
+
+        logger.info("move the shard");
+        String existingNodeId = clusterState.routingTable().index("test").shard(0).primaryShard().currentNodeId();
+        String toNodeId;
+        if ("node1".equals(existingNodeId)) {
+            toNodeId = "node2";
+        } else {
+            toNodeId = "node1";
+        }
+        rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new MoveAllocationCommand(new ShardId("test", 0), existingNodeId, toNodeId)));
+        assertThat(rerouteResult.changed(), equalTo(true));
+        clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
+        assertEquals(clusterState.getRoutingNodes().node(existingNodeId).get(0).state(), ShardRoutingState.RELOCATING);
+        assertEquals(clusterState.getRoutingNodes().node(toNodeId).get(0).state(),ShardRoutingState.INITIALIZING);
+
+        assertEquals(clusterState.getRoutingNodes().node(existingNodeId).get(0).getExpectedShardSize(), byteSize);
+        assertEquals(clusterState.getRoutingNodes().node(toNodeId).get(0).getExpectedShardSize(), byteSize);
+
+        logger.info("finish moving the shard");
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
+        clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
+
+        assertThat(clusterState.getRoutingNodes().node(existingNodeId).isEmpty(), equalTo(true));
+        assertThat(clusterState.getRoutingNodes().node(toNodeId).get(0).state(), equalTo(ShardRoutingState.STARTED));
+        assertEquals(clusterState.getRoutingNodes().node(toNodeId).get(0).getExpectedShardSize(), -1);
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java
index ff2ae10..78083a6 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster.routing.allocation;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -34,7 +35,6 @@ import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
 import java.util.ArrayList;
-import java.util.Collections;
 
 import static org.elasticsearch.cluster.routing.ShardRoutingState.*;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
@@ -562,7 +562,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
         assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
 
         // start another replica shard, while keep one initializing
-        clusterState = ClusterState.builder(clusterState).routingTable(allocation.applyStartedShards(clusterState, Collections.singletonList(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).get(0))).routingTable()).build();
+        clusterState = ClusterState.builder(clusterState).routingTable(allocation.applyStartedShards(clusterState, ImmutableList.of(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).get(0))).routingTable()).build();
         assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(2));
         assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java
index 4b1b08e..5651038 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java
@@ -20,6 +20,8 @@
 package org.elasticsearch.cluster.routing.allocation;
 
 import org.elasticsearch.Version;
+import org.elasticsearch.cluster.ClusterInfo;
+import org.elasticsearch.cluster.ClusterInfoService;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
@@ -27,12 +29,16 @@ import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.RoutingNode;
 import org.elasticsearch.cluster.routing.RoutingNodes;
 import org.elasticsearch.cluster.routing.RoutingTable;
+import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
+import java.util.Collections;
+
 import static org.elasticsearch.cluster.routing.ShardRoutingState.*;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.hamcrest.Matchers.equalTo;
@@ -47,12 +53,33 @@ public class RebalanceAfterActiveTests extends ESAllocationTestCase {
 
     @Test
     public void testRebalanceOnlyAfterAllShardsAreActive() {
-        AllocationService strategy = createAllocationService(settingsBuilder()
-                .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
-                .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
-                .build());
+        final long[] sizes = new long[5];
+        for (int i =0; i < sizes.length; i++) {
+            sizes[i] = randomIntBetween(0, Integer.MAX_VALUE);
+        }
 
+        AllocationService strategy = createAllocationService(settingsBuilder()
+                        .put("cluster.routing.allocation.concurrent_recoveries", 10)
+                        .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
+                        .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
+                        .build(),
+                new ClusterInfoService() {
+                    @Override
+                    public ClusterInfo getClusterInfo() {
+                        return new ClusterInfo(Collections.EMPTY_MAP, Collections.EMPTY_MAP) {
+                            @Override
+                            public Long getShardSize(ShardRouting shardRouting) {
+                                if (shardRouting.index().equals("test")) {
+                                    return sizes[shardRouting.getId()];
+                                }
+                                return null;                    }
+                        };
+                    }
+
+                    @Override
+                    public void addListener(Listener listener) {
+                    }
+                });
         logger.info("Building initial routing table");
 
         MetaData metaData = MetaData.builder()
@@ -97,6 +124,7 @@ public class RebalanceAfterActiveTests extends ESAllocationTestCase {
             assertThat(routingTable.index("test").shard(i).shards().size(), equalTo(2));
             assertThat(routingTable.index("test").shard(i).primaryShard().state(), equalTo(STARTED));
             assertThat(routingTable.index("test").shard(i).replicaShards().get(0).state(), equalTo(INITIALIZING));
+            assertEquals(routingTable.index("test").shard(i).replicaShards().get(0).getExpectedShardSize(), sizes[i]);
         }
 
         logger.info("now, start 8 more nodes, and check that no rebalancing/relocation have happened");
@@ -112,6 +140,8 @@ public class RebalanceAfterActiveTests extends ESAllocationTestCase {
             assertThat(routingTable.index("test").shard(i).shards().size(), equalTo(2));
             assertThat(routingTable.index("test").shard(i).primaryShard().state(), equalTo(STARTED));
             assertThat(routingTable.index("test").shard(i).replicaShards().get(0).state(), equalTo(INITIALIZING));
+            assertEquals(routingTable.index("test").shard(i).replicaShards().get(0).getExpectedShardSize(), sizes[i]);
+
         }
 
         logger.info("start the replica shards, rebalancing should start");
@@ -124,6 +154,16 @@ public class RebalanceAfterActiveTests extends ESAllocationTestCase {
         // we only allow one relocation at a time
         assertThat(routingTable.shardsWithState(STARTED).size(), equalTo(5));
         assertThat(routingTable.shardsWithState(RELOCATING).size(), equalTo(5));
+        for (int i = 0; i < routingTable.index("test").shards().size(); i++) {
+            int num = 0;
+            for (ShardRouting routing : routingTable.index("test").shard(i).shards()) {
+                if (routing.state() == RELOCATING || routing.state() == INITIALIZING) {
+                    assertEquals(routing.getExpectedShardSize(), sizes[i]);
+                    num++;
+                }
+            }
+            assertTrue(num > 0);
+        }
 
         logger.info("complete relocation, other half of relocation should happen");
         routingNodes = clusterState.getRoutingNodes();
@@ -135,6 +175,14 @@ public class RebalanceAfterActiveTests extends ESAllocationTestCase {
         // we now only relocate 3, since 2 remain where they are!
         assertThat(routingTable.shardsWithState(STARTED).size(), equalTo(7));
         assertThat(routingTable.shardsWithState(RELOCATING).size(), equalTo(3));
+        for (int i = 0; i < routingTable.index("test").shards().size(); i++) {
+            for (ShardRouting routing : routingTable.index("test").shard(i).shards()) {
+                if (routing.state() == RELOCATING || routing.state() == INITIALIZING) {
+                    assertEquals(routing.getExpectedShardSize(), sizes[i]);
+                }
+            }
+        }
+
 
         logger.info("complete relocation, thats it!");
         routingNodes = clusterState.getRoutingNodes();
diff --git a/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java b/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java
index d405fb1..4e6eabc 100644
--- a/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster.structure;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterName;
@@ -36,8 +37,6 @@ import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
-import java.util.Collections;
-
 import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.hamcrest.Matchers.*;
@@ -47,28 +46,28 @@ public class RoutingIteratorTests extends ESAllocationTestCase {
     @Test
     public void testEmptyIterator() {
         ShardShuffler shuffler = new RotationShardShuffler(0);
-        ShardIterator shardIterator = new PlainShardIterator(new ShardId("test1", 0), shuffler.shuffle(Collections.<ShardRouting>emptyList()));
+        ShardIterator shardIterator = new PlainShardIterator(new ShardId("test1", 0), shuffler.shuffle(ImmutableList.<ShardRouting>of()));
         assertThat(shardIterator.remaining(), equalTo(0));
         assertThat(shardIterator.nextOrNull(), nullValue());
         assertThat(shardIterator.remaining(), equalTo(0));
         assertThat(shardIterator.nextOrNull(), nullValue());
         assertThat(shardIterator.remaining(), equalTo(0));
 
-        shardIterator = new PlainShardIterator(new ShardId("test1", 0), shuffler.shuffle(Collections.<ShardRouting>emptyList()));
+        shardIterator = new PlainShardIterator(new ShardId("test1", 0), shuffler.shuffle(ImmutableList.<ShardRouting>of()));
         assertThat(shardIterator.remaining(), equalTo(0));
         assertThat(shardIterator.nextOrNull(), nullValue());
         assertThat(shardIterator.remaining(), equalTo(0));
         assertThat(shardIterator.nextOrNull(), nullValue());
         assertThat(shardIterator.remaining(), equalTo(0));
 
-        shardIterator = new PlainShardIterator(new ShardId("test1", 0), shuffler.shuffle(Collections.<ShardRouting>emptyList()));
+        shardIterator = new PlainShardIterator(new ShardId("test1", 0), shuffler.shuffle(ImmutableList.<ShardRouting>of()));
         assertThat(shardIterator.remaining(), equalTo(0));
         assertThat(shardIterator.nextOrNull(), nullValue());
         assertThat(shardIterator.remaining(), equalTo(0));
         assertThat(shardIterator.nextOrNull(), nullValue());
         assertThat(shardIterator.remaining(), equalTo(0));
 
-        shardIterator = new PlainShardIterator(new ShardId("test1", 0), shuffler.shuffle(Collections.<ShardRouting>emptyList()));
+        shardIterator = new PlainShardIterator(new ShardId("test1", 0), shuffler.shuffle(ImmutableList.<ShardRouting>of()));
         assertThat(shardIterator.remaining(), equalTo(0));
         assertThat(shardIterator.nextOrNull(), nullValue());
         assertThat(shardIterator.remaining(), equalTo(0));
diff --git a/core/src/test/java/org/elasticsearch/common/network/NetworkAddressTests.java b/core/src/test/java/org/elasticsearch/common/network/NetworkAddressTests.java
new file mode 100644
index 0000000..5847bb7
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/network/NetworkAddressTests.java
@@ -0,0 +1,99 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.network;
+
+import org.elasticsearch.test.ESTestCase;
+
+import java.io.IOException;
+import java.net.Inet6Address;
+import java.net.InetAddress;
+import java.net.InetSocketAddress;
+
+/**
+ * Tests for network address formatting. Please avoid using any methods that cause DNS lookups!
+ */
+public class NetworkAddressTests extends ESTestCase {
+    
+    public void testFormatV4() throws Exception {
+        assertEquals("localhost/127.0.0.1", NetworkAddress.format(forge("localhost", "127.0.0.1")));
+        assertEquals("127.0.0.1", NetworkAddress.format(forge(null, "127.0.0.1")));
+    }
+    
+    public void testFormatV6() throws Exception {
+        assertEquals("localhost/::1", NetworkAddress.format(forge("localhost", "::1")));
+        assertEquals("::1", NetworkAddress.format(forge(null, "::1")));
+    }
+    
+    public void testFormatAddressV4() throws Exception {
+        assertEquals("127.0.0.1", NetworkAddress.formatAddress(forge("localhost", "127.0.0.1")));
+        assertEquals("127.0.0.1", NetworkAddress.formatAddress(forge(null, "127.0.0.1")));
+    }
+    
+    public void testFormatAddressV6() throws Exception {
+        assertEquals("::1", NetworkAddress.formatAddress(forge("localhost", "::1")));
+        assertEquals("::1", NetworkAddress.formatAddress(forge(null, "::1")));
+    }
+    
+    public void testFormatPortV4() throws Exception {
+        assertEquals("localhost/127.0.0.1:1234", NetworkAddress.format(new InetSocketAddress(forge("localhost", "127.0.0.1"), 1234)));
+        assertEquals("127.0.0.1:1234", NetworkAddress.format(new InetSocketAddress(forge(null, "127.0.0.1"), 1234)));
+    }
+    
+    public void testFormatPortV6() throws Exception {
+        assertEquals("localhost/[::1]:1234", NetworkAddress.format(new InetSocketAddress(forge("localhost", "::1"), 1234)));
+        assertEquals("[::1]:1234",NetworkAddress.format(new InetSocketAddress(forge(null, "::1"), 1234)));
+    }
+    
+    public void testFormatAddressPortV4() throws Exception {
+        assertEquals("127.0.0.1:1234", NetworkAddress.formatAddress(new InetSocketAddress(forge("localhost", "127.0.0.1"), 1234)));
+        assertEquals("127.0.0.1:1234", NetworkAddress.formatAddress(new InetSocketAddress(forge(null, "127.0.0.1"), 1234)));
+    }
+    
+    public void testFormatAddressPortV6() throws Exception {
+        assertEquals("[::1]:1234", NetworkAddress.formatAddress(new InetSocketAddress(forge("localhost", "::1"), 1234)));
+        assertEquals("[::1]:1234", NetworkAddress.formatAddress(new InetSocketAddress(forge(null, "::1"), 1234)));
+    }
+    
+    public void testNoScopeID() throws Exception {
+        assertEquals("::1", NetworkAddress.format(forgeScoped(null, "::1", 5)));
+        assertEquals("localhost/::1", NetworkAddress.format(forgeScoped("localhost", "::1", 5)));
+        
+        assertEquals("::1", NetworkAddress.formatAddress(forgeScoped(null, "::1", 5)));
+        assertEquals("::1", NetworkAddress.formatAddress(forgeScoped("localhost", "::1", 5)));
+        
+        assertEquals("[::1]:1234", NetworkAddress.format(new InetSocketAddress(forgeScoped(null, "::1", 5), 1234)));
+        assertEquals("localhost/[::1]:1234", NetworkAddress.format(new InetSocketAddress(forgeScoped("localhost", "::1", 5), 1234)));
+        
+        assertEquals("[::1]:1234", NetworkAddress.formatAddress(new InetSocketAddress(forgeScoped(null, "::1", 5), 1234)));
+        assertEquals("[::1]:1234", NetworkAddress.formatAddress(new InetSocketAddress(forgeScoped("localhost", "::1", 5), 1234)));
+    }
+    
+    /** creates address without any lookups. hostname can be null, for missing */
+    private InetAddress forge(String hostname, String address) throws IOException {
+        byte bytes[] = InetAddress.getByName(address).getAddress();
+        return InetAddress.getByAddress(hostname, bytes);
+    }
+    
+    /** creates scoped ipv6 address without any lookups. hostname can be null, for missing */
+    private InetAddress forgeScoped(String hostname, String address, int scopeid) throws IOException {
+        byte bytes[] = InetAddress.getByName(address).getAddress();
+        return Inet6Address.getByAddress(hostname, bytes, scopeid);
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/util/CollectionUtilsTests.java b/core/src/test/java/org/elasticsearch/common/util/CollectionUtilsTests.java
index da2f918..09c4b80 100644
--- a/core/src/test/java/org/elasticsearch/common/util/CollectionUtilsTests.java
+++ b/core/src/test/java/org/elasticsearch/common/util/CollectionUtilsTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.common.util;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Iterables;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefArray;
@@ -36,7 +37,7 @@ public class CollectionUtilsTests extends ESTestCase {
 
     @Test
     public void rotateEmpty() {
-        assertTrue(CollectionUtils.rotate(Collections.emptyList(), randomInt()).isEmpty());
+        assertTrue(CollectionUtils.rotate(ImmutableList.of(), randomInt()).isEmpty());
     }
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
index fecb3d9..c18abdc 100644
--- a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
+++ b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
@@ -72,6 +72,7 @@ import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcke
 import static org.hamcrest.Matchers.*;
 
 @ClusterScope(scope = Scope.TEST, numDataNodes = 0, transportClientRatio = 0)
+@ESIntegTestCase.SuppressLocalMode
 public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
 
     private static final TimeValue DISRUPTION_HEALING_OVERHEAD = TimeValue.timeValueSeconds(40); // we use 30s as timeout in many places.
@@ -104,23 +105,10 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
     }
 
     private List<String> startCluster(int numberOfNodes, int minimumMasterNode) throws ExecutionException, InterruptedException {
-        configureCluster(numberOfNodes, minimumMasterNode);
-        List<String> nodes = internalCluster().startNodesAsync(numberOfNodes).get();
-        ensureStableCluster(numberOfNodes);
-
-        // TODO: this is a temporary solution so that nodes will not base their reaction to a partition based on previous successful results
-        for (ZenPingService pingService : internalCluster().getInstances(ZenPingService.class)) {
-            for (ZenPing zenPing : pingService.zenPings()) {
-                if (zenPing instanceof UnicastZenPing) {
-                    ((UnicastZenPing) zenPing).clearTemporalResponses();
-                }
-            }
-        }
-        return nodes;
+        return startCluster(numberOfNodes, minimumMasterNode, null);
     }
 
-
-    private List<String> startUnicastCluster(int numberOfNodes, @Nullable int[] unicastHostsOrdinals, int minimumMasterNode) throws ExecutionException, InterruptedException {
+    private List<String> startCluster(int numberOfNodes, int minimumMasterNode, @Nullable int[] unicastHostsOrdinals) throws ExecutionException, InterruptedException {
         configureUnicastCluster(numberOfNodes, unicastHostsOrdinals, minimumMasterNode);
         List<String> nodes = internalCluster().startNodesAsync(numberOfNodes).get();
         ensureStableCluster(numberOfNodes);
@@ -142,38 +130,18 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
             .put("discovery.zen.join_timeout", "10s")  // still long to induce failures but to long so test won't time out
             .put(DiscoverySettings.PUBLISH_TIMEOUT, "1s") // <-- for hitting simulated network failures quickly
             .put("http.enabled", false) // just to make test quicker
+            .put("transport.host", "127.0.0.1") // only bind on one IF we use v4 here by default
+            .put("transport.bind_host", "127.0.0.1")
+            .put("transport.publish_host", "127.0.0.1")
             .put("gateway.local.list_timeout", "10s") // still long to induce failures but to long so test won't time out
             .put("plugin.types", MockTransportService.TestPlugin.class.getName())
             .build();
 
-    private void configureCluster(int numberOfNodes, int minimumMasterNode) throws ExecutionException, InterruptedException {
-        if (randomBoolean()) {
-            configureMulticastCluster(numberOfNodes, minimumMasterNode);
-        } else {
-            configureUnicastCluster(numberOfNodes, null, minimumMasterNode);
-        }
-
-    }
-
-    private void configureMulticastCluster(int numberOfNodes, int minimumMasterNode) throws ExecutionException, InterruptedException {
-        if (minimumMasterNode < 0) {
-            minimumMasterNode = numberOfNodes / 2 + 1;
-        }
-        // TODO: Rarely use default settings form some of these
-        Settings settings = Settings.builder()
-                .put(DEFAULT_SETTINGS)
-                .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, minimumMasterNode)
-                .build();
-
-        if (discoveryConfig == null) {
-            discoveryConfig = new ClusterDiscoveryConfiguration(numberOfNodes, settings);
-        }
-    }
-
     private void configureUnicastCluster(int numberOfNodes, @Nullable int[] unicastHostsOrdinals, int minimumMasterNode) throws ExecutionException, InterruptedException {
         if (minimumMasterNode < 0) {
             minimumMasterNode = numberOfNodes / 2 + 1;
         }
+        logger.info("---> configured unicast");
         // TODO: Rarely use default settings form some of these
         Settings nodeSettings = Settings.builder()
                 .put(DEFAULT_SETTINGS)
@@ -286,10 +254,8 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
 
         NetworkPartition networkPartition = addRandomPartition();
 
-        assertEquals(1, networkPartition.getMinoritySide().size());
-        final String isolatedNode = networkPartition.getMinoritySide().iterator().next();
-        assertEquals(2, networkPartition.getMajoritySide().size());
-        final String nonIsolatedNode = networkPartition.getMajoritySide().iterator().next();
+        final String isolatedNode = networkPartition.getMinoritySide().get(0);
+        final String nonIsolatedNode = networkPartition.getMajoritySide().get(0);
 
         // Simulate a network issue between the unlucky node and the rest of the cluster.
         networkPartition.startDisrupting();
@@ -366,7 +332,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
         NetworkPartition networkPartition = addRandomIsolation(isolatedNode);
         networkPartition.startDisrupting();
 
-        String nonIsolatedNode = networkPartition.getMajoritySide().iterator().next();
+        String nonIsolatedNode = networkPartition.getMajoritySide().get(0);
 
         // make sure cluster reforms
         ensureStableCluster(2, nonIsolatedNode);
@@ -558,9 +524,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
      */
     @Test
     public void testMasterNodeGCs() throws Exception {
-        // TODO: on mac OS multicast threads are shared between nodes and we therefore we can't simulate GC and stop pinging for just one node
-        // find a way to block thread creation in the generic thread pool to avoid this.
-        List<String> nodes = startUnicastCluster(3, null, -1);
+        List<String> nodes = startCluster(3, -1);
 
         String oldMasterNode = internalCluster().getMasterName();
         // a very long GC, but it's OK as we remove the disruption when it has had an effect
@@ -602,10 +566,8 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
      */
     @Test
     public void testStaleMasterNotHijackingMajority() throws Exception {
-        // TODO: on mac OS multicast threads are shared between nodes and we therefore we can't simulate GC and stop pinging for just one node
-        // find a way to block thread creation in the generic thread pool to avoid this.
         // 3 node cluster with unicast discovery and minimum_master_nodes set to 2:
-        final List<String> nodes = startUnicastCluster(3, null, 2);
+        final List<String> nodes = startCluster(3, 2);
 
         // Save the current master node as old master node, because that node will get frozen
         final String oldMasterNode = internalCluster().getMasterName();
@@ -772,7 +734,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
      */
     @Test
     public void unicastSinglePingResponseContainsMaster() throws Exception {
-        List<String> nodes = startUnicastCluster(4, new int[]{0}, -1);
+        List<String> nodes = startCluster(4, -1, new int[] {0});
         // Figure out what is the elected master node
         final String masterNode = internalCluster().getMasterName();
         logger.info("---> legit elected master node=" + masterNode);
@@ -809,7 +771,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
     @Test
     @TestLogging("discovery.zen:TRACE,cluster.service:TRACE")
     public void isolatedUnicastNodes() throws Exception {
-        List<String> nodes = startUnicastCluster(4, new int[]{0}, -1);
+        List<String> nodes = startCluster(4, -1, new int[]{0});
         // Figure out what is the elected master node
         final String unicastTarget = nodes.get(0);
 
@@ -892,7 +854,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
 
     @Test
     public void testClusterFormingWithASlowNode() throws Exception {
-        configureCluster(3, 2);
+        configureUnicastCluster(3, null, 2);
 
         SlowClusterStateProcessing disruption = new SlowClusterStateProcessing(getRandom(), 0, 0, 1000, 2000);
 
@@ -953,7 +915,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
     @Test
     public void testIndexImportedFromDataOnlyNodesIfMasterLostDataFolder() throws Exception {
         // test for https://github.com/elastic/elasticsearch/issues/8823
-        configureCluster(2, 1);
+        configureUnicastCluster(2, null, 1);
         String masterNode = internalCluster().startMasterOnlyNode(Settings.EMPTY);
         internalCluster().startDataOnlyNode(Settings.EMPTY);
 
@@ -976,7 +938,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
     @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/11665")
     @Test
     public void testIndicesDeleted() throws Exception {
-        configureCluster(3, 2);
+        configureUnicastCluster(3, null, 2);
         Future<List<String>> masterNodes= internalCluster().startMasterOnlyNodesAsync(2);
         Future<String> dataNode = internalCluster().startDataOnlyNodeAsync();
         dataNode.get();
@@ -1039,38 +1001,6 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
         return list.get(0);
     }
 
-    private void ensureStableCluster(int nodeCount) {
-        ensureStableCluster(nodeCount, TimeValue.timeValueSeconds(30));
-    }
-
-    private void ensureStableCluster(int nodeCount, TimeValue timeValue) {
-        ensureStableCluster(nodeCount, timeValue, false, null);
-    }
-
-    private void ensureStableCluster(int nodeCount, @Nullable String viaNode) {
-        ensureStableCluster(nodeCount, TimeValue.timeValueSeconds(30), false, viaNode);
-    }
-
-    private void ensureStableCluster(int nodeCount, TimeValue timeValue, boolean local, @Nullable String viaNode) {
-        if (viaNode == null) {
-            viaNode = randomFrom(internalCluster().getNodeNames());
-        }
-        logger.debug("ensuring cluster is stable with [{}] nodes. access node: [{}]. timeout: [{}]", nodeCount, viaNode, timeValue);
-        ClusterHealthResponse clusterHealthResponse = client(viaNode).admin().cluster().prepareHealth()
-                .setWaitForEvents(Priority.LANGUID)
-                .setWaitForNodes(Integer.toString(nodeCount))
-                .setTimeout(timeValue)
-                .setLocal(local)
-                .setWaitForRelocatingShards(0)
-                .get();
-        if (clusterHealthResponse.isTimedOut()) {
-            ClusterStateResponse stateResponse = client(viaNode).admin().cluster().prepareState().get();
-            fail("failed to reach a stable cluster of [" + nodeCount + "] nodes. Tried via [" + viaNode + "]. last cluster state:\n"
-                    + stateResponse.getState().prettyPrint());
-        }
-        assertThat(clusterHealthResponse.isTimedOut(), is(false));
-    }
-
     private ClusterState getNodeClusterState(String node) {
         return client(node).admin().cluster().prepareState().setLocal(true).get().getState();
     }
diff --git a/core/src/test/java/org/elasticsearch/discovery/ZenUnicastDiscoveryIT.java b/core/src/test/java/org/elasticsearch/discovery/ZenUnicastDiscoveryIT.java
index 891d538..f11f2ea 100644
--- a/core/src/test/java/org/elasticsearch/discovery/ZenUnicastDiscoveryIT.java
+++ b/core/src/test/java/org/elasticsearch/discovery/ZenUnicastDiscoveryIT.java
@@ -22,6 +22,7 @@ package org.elasticsearch.discovery;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
 import org.elasticsearch.test.ESIntegTestCase.Scope;
@@ -35,6 +36,7 @@ import java.util.concurrent.ExecutionException;
 import static org.hamcrest.Matchers.equalTo;
 
 @ClusterScope(scope = Scope.TEST, numDataNodes = 0)
+@ESIntegTestCase.SuppressLocalMode
 public class ZenUnicastDiscoveryIT extends ESIntegTestCase {
 
     private ClusterDiscoveryConfiguration discoveryConfig;
@@ -80,12 +82,15 @@ public class ZenUnicastDiscoveryIT extends ESIntegTestCase {
         int currentNumNodes = randomIntBetween(3, 5);
         final int min_master_nodes = currentNumNodes / 2 + 1;
         int currentNumOfUnicastHosts = randomIntBetween(min_master_nodes, currentNumNodes);
-        final Settings settings = Settings.settingsBuilder().put("discovery.zen.minimum_master_nodes", min_master_nodes).build();
+        final Settings settings = Settings.settingsBuilder()
+                .put("discovery.zen.join_timeout", TimeValue.timeValueSeconds(10))
+                .put("discovery.zen.minimum_master_nodes", min_master_nodes)
+                .build();
         discoveryConfig = new ClusterDiscoveryConfiguration.UnicastZen(currentNumNodes, currentNumOfUnicastHosts, settings);
 
         List<String> nodes = internalCluster().startNodesAsync(currentNumNodes).get();
 
-        ensureGreen();
+        ensureStableCluster(currentNumNodes);
 
         DiscoveryNode masterDiscoNode = null;
         for (String node : nodes) {
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java b/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java
index 943b054..cc29337 100644
--- a/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java
+++ b/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java
@@ -32,6 +32,7 @@ import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.transport.InetSocketTransportAddress;
 import org.elasticsearch.common.transport.LocalTransportAddress;
 import org.elasticsearch.discovery.Discovery;
 import org.elasticsearch.discovery.zen.elect.ElectMasterService;
@@ -41,15 +42,13 @@ import org.elasticsearch.discovery.zen.publish.PublishClusterStateAction;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.junit.annotations.TestLogging;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.BytesTransportRequest;
-import org.elasticsearch.transport.EmptyTransportResponseHandler;
-import org.elasticsearch.transport.TransportException;
-import org.elasticsearch.transport.TransportResponse;
-import org.elasticsearch.transport.TransportService;
+import org.elasticsearch.transport.*;
 import org.hamcrest.Matchers;
 import org.junit.Test;
 
 import java.io.IOException;
+import java.net.InetAddress;
+import java.net.UnknownHostException;
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.List;
@@ -66,6 +65,7 @@ import static org.hamcrest.Matchers.nullValue;
 import static org.hamcrest.Matchers.sameInstance;
 
 @ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0, numClientNodes = 0)
+@ESIntegTestCase.SuppressLocalMode
 public class ZenDiscoveryIT extends ESIntegTestCase {
 
     @Test
@@ -226,15 +226,14 @@ public class ZenDiscoveryIT extends ESIntegTestCase {
     }
 
     @Test
-    public void testHandleNodeJoin_incompatibleMinVersion() {
+    public void testHandleNodeJoin_incompatibleMinVersion() throws UnknownHostException {
         Settings nodeSettings = Settings.settingsBuilder()
                 .put("discovery.type", "zen") // <-- To override the local setting if set externally
-                .put("node.mode", "local")  // <-- force local transport so we can fake a network address
                 .build();
         String nodeName = internalCluster().startNode(nodeSettings, Version.V_2_0_0_beta1);
         ZenDiscovery zenDiscovery = (ZenDiscovery) internalCluster().getInstance(Discovery.class, nodeName);
 
-        DiscoveryNode node = new DiscoveryNode("_node_id", new LocalTransportAddress("_id"), Version.V_1_6_0);
+        DiscoveryNode node = new DiscoveryNode("_node_id", new InetSocketTransportAddress(InetAddress.getByName("0.0.0.0"), 0), Version.V_1_6_0);
         final AtomicReference<IllegalStateException> holder = new AtomicReference<>();
         zenDiscovery.handleJoinRequest(node, new MembershipAction.JoinCallback() {
             @Override
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPingIT.java b/core/src/test/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPingIT.java
deleted file mode 100644
index 00da8b1..0000000
--- a/core/src/test/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPingIT.java
+++ /dev/null
@@ -1,181 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.zen.ping.multicast;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.discovery.zen.ping.PingContextProvider;
-import org.elasticsearch.discovery.zen.ping.ZenPing;
-import org.elasticsearch.node.service.NodeService;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-import org.elasticsearch.transport.local.LocalTransport;
-import org.junit.Test;
-
-import java.net.DatagramPacket;
-import java.net.InetAddress;
-import java.net.MulticastSocket;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class MulticastZenPingIT extends ESTestCase {
-
-    private Settings buildRandomMulticast(Settings settings) {
-        Settings.Builder builder = Settings.builder().put(settings);
-        builder.put("discovery.zen.ping.multicast.group", "224.2.3." + randomIntBetween(0, 255));
-        builder.put("discovery.zen.ping.multicast.port", randomIntBetween(55000, 56000));
-        if (randomBoolean()) {
-            builder.put("discovery.zen.ping.multicast.shared", randomBoolean());
-        }
-        return builder.build();
-    }
-
-    @Test
-    public void testSimplePings() throws InterruptedException {
-        Settings settings = Settings.EMPTY;
-        settings = buildRandomMulticast(settings);
-
-        ThreadPool threadPool = new ThreadPool("testSimplePings");
-        final ClusterName clusterName = new ClusterName("test");
-        final TransportService transportServiceA = new TransportService(new LocalTransport(settings, threadPool, Version.CURRENT, new NamedWriteableRegistry()), threadPool).start();
-        final DiscoveryNode nodeA = new DiscoveryNode("A", transportServiceA.boundAddress().publishAddress(), Version.CURRENT);
-
-        final TransportService transportServiceB = new TransportService(new LocalTransport(settings, threadPool, Version.CURRENT, new NamedWriteableRegistry()), threadPool).start();
-        final DiscoveryNode nodeB = new DiscoveryNode("B", transportServiceB.boundAddress().publishAddress(), Version.CURRENT);
-
-        MulticastZenPing zenPingA = new MulticastZenPing(threadPool, transportServiceA, clusterName, Version.CURRENT);
-        zenPingA.setPingContextProvider(new PingContextProvider() {
-            @Override
-            public DiscoveryNodes nodes() {
-                return DiscoveryNodes.builder().put(nodeA).localNodeId("A").build();
-            }
-
-            @Override
-            public NodeService nodeService() {
-                return null;
-            }
-
-            @Override
-            public boolean nodeHasJoinedClusterOnce() {
-                return false;
-            }
-        });
-        zenPingA.start();
-
-        MulticastZenPing zenPingB = new MulticastZenPing(threadPool, transportServiceB, clusterName, Version.CURRENT);
-        zenPingB.setPingContextProvider(new PingContextProvider() {
-            @Override
-            public DiscoveryNodes nodes() {
-                return DiscoveryNodes.builder().put(nodeB).localNodeId("B").build();
-            }
-
-            @Override
-            public NodeService nodeService() {
-                return null;
-            }
-
-            @Override
-            public boolean nodeHasJoinedClusterOnce() {
-                return true;
-            }
-        });
-        zenPingB.start();
-
-        try {
-            logger.info("ping from A");
-            ZenPing.PingResponse[] pingResponses = zenPingA.pingAndWait(TimeValue.timeValueSeconds(1));
-            assertThat(pingResponses.length, equalTo(1));
-            assertThat(pingResponses[0].node().id(), equalTo("B"));
-            assertTrue(pingResponses[0].hasJoinedOnce());
-
-            logger.info("ping from B");
-            pingResponses = zenPingB.pingAndWait(TimeValue.timeValueSeconds(1));
-            assertThat(pingResponses.length, equalTo(1));
-            assertThat(pingResponses[0].node().id(), equalTo("A"));
-            assertFalse(pingResponses[0].hasJoinedOnce());
-
-        } finally {
-            zenPingA.close();
-            zenPingB.close();
-            transportServiceA.close();
-            transportServiceB.close();
-            terminate(threadPool);
-        }
-    }
-
-    @Test
-    public void testExternalPing() throws Exception {
-        Settings settings = Settings.EMPTY;
-        settings = buildRandomMulticast(settings);
-
-        final ThreadPool threadPool = new ThreadPool("testExternalPing");
-        final ClusterName clusterName = new ClusterName("test");
-        final TransportService transportServiceA = new TransportService(new LocalTransport(settings, threadPool, Version.CURRENT, new NamedWriteableRegistry()), threadPool).start();
-        final DiscoveryNode nodeA = new DiscoveryNode("A", transportServiceA.boundAddress().publishAddress(), Version.CURRENT);
-
-        MulticastZenPing zenPingA = new MulticastZenPing(threadPool, transportServiceA, clusterName, Version.CURRENT);
-        zenPingA.setPingContextProvider(new PingContextProvider() {
-            @Override
-            public DiscoveryNodes nodes() {
-                return DiscoveryNodes.builder().put(nodeA).localNodeId("A").build();
-            }
-
-            @Override
-            public NodeService nodeService() {
-                return null;
-            }
-
-            @Override
-            public boolean nodeHasJoinedClusterOnce() {
-                return false;
-            }
-        });
-        zenPingA.start();
-
-        MulticastSocket multicastSocket = null;
-        try {
-            Loggers.getLogger(MulticastZenPing.class).setLevel("TRACE");
-            multicastSocket = new MulticastSocket(54328);
-            multicastSocket.setReceiveBufferSize(2048);
-            multicastSocket.setSendBufferSize(2048);
-            multicastSocket.setSoTimeout(60000);
-
-            DatagramPacket datagramPacket = new DatagramPacket(new byte[2048], 2048, InetAddress.getByName("224.2.2.4"), 54328);
-            XContentBuilder builder = XContentFactory.jsonBuilder().startObject().startObject("request").field("cluster_name", "test").endObject().endObject();
-            datagramPacket.setData(builder.bytes().toBytes());
-            multicastSocket.send(datagramPacket);
-            Thread.sleep(100);
-        } finally {
-            Loggers.getLogger(MulticastZenPing.class).setLevel("INFO");
-            if (multicastSocket != null) multicastSocket.close();
-            zenPingA.close();
-            terminate(threadPool);
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPingIT.java b/core/src/test/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPingIT.java
index 83d3145..509740e 100644
--- a/core/src/test/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPingIT.java
+++ b/core/src/test/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPingIT.java
@@ -24,6 +24,7 @@ import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
+import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.common.network.NetworkService;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.InetSocketTransportAddress;
@@ -39,6 +40,8 @@ import org.elasticsearch.transport.TransportService;
 import org.elasticsearch.transport.netty.NettyTransport;
 import org.junit.Test;
 
+import java.net.InetSocketAddress;
+
 import static org.hamcrest.Matchers.equalTo;
 
 public class UnicastZenPingIT extends ESTestCase {
@@ -68,8 +71,8 @@ public class UnicastZenPingIT extends ESTestCase {
         InetSocketTransportAddress addressB = (InetSocketTransportAddress) transportB.boundAddress().publishAddress();
 
         Settings hostsSettings = Settings.settingsBuilder().putArray("discovery.zen.ping.unicast.hosts",
-                addressA.address().getAddress().getHostAddress() + ":" + addressA.address().getPort(),
-                addressB.address().getAddress().getHostAddress() + ":" + addressB.address().getPort())
+                NetworkAddress.formatAddress(new InetSocketAddress(addressA.address().getAddress(), addressA.address().getPort())),
+                NetworkAddress.formatAddress(new InetSocketAddress(addressB.address().getAddress(), addressB.address().getPort())))
                 .build();
 
         UnicastZenPing zenPingA = new UnicastZenPing(hostsSettings, threadPool, transportServiceA, clusterName, Version.CURRENT, electMasterService, null);
diff --git a/core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java b/core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java
index b19a552..2b4ccf7 100644
--- a/core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java
+++ b/core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java
@@ -21,6 +21,7 @@ package org.elasticsearch.gateway;
 
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
 import org.elasticsearch.Version;
+import org.elasticsearch.cluster.ClusterInfo;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
@@ -302,7 +303,7 @@ public class ReplicaShardAllocatorTests extends ESAllocationTestCase {
                 .metaData(metaData)
                 .routingTable(routingTable)
                 .nodes(DiscoveryNodes.builder().put(node1).put(node2).put(node3)).build();
-        return new RoutingAllocation(deciders, new RoutingNodes(state, false), state.nodes(), null);
+        return new RoutingAllocation(deciders, new RoutingNodes(state, false), state.nodes(), ClusterInfo.EMPTY);
     }
 
     private RoutingAllocation onePrimaryOnNode1And1ReplicaRecovering(AllocationDeciders deciders) {
@@ -321,7 +322,7 @@ public class ReplicaShardAllocatorTests extends ESAllocationTestCase {
                 .metaData(metaData)
                 .routingTable(routingTable)
                 .nodes(DiscoveryNodes.builder().put(node1).put(node2).put(node3)).build();
-        return new RoutingAllocation(deciders, new RoutingNodes(state, false), state.nodes(), null);
+        return new RoutingAllocation(deciders, new RoutingNodes(state, false), state.nodes(), ClusterInfo.EMPTY);
     }
 
     class TestAllocator extends ReplicaShardAllocator {
diff --git a/core/src/test/java/org/elasticsearch/http/netty/pipelining/HttpPipeliningHandlerTest.java b/core/src/test/java/org/elasticsearch/http/netty/pipelining/HttpPipeliningHandlerTest.java
index b6a8df5..f0368fb 100644
--- a/core/src/test/java/org/elasticsearch/http/netty/pipelining/HttpPipeliningHandlerTest.java
+++ b/core/src/test/java/org/elasticsearch/http/netty/pipelining/HttpPipeliningHandlerTest.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.http.netty.pipelining;
 
+import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.test.ESTestCase;
 import org.jboss.netty.bootstrap.ClientBootstrap;
 import org.jboss.netty.bootstrap.ServerBootstrap;
@@ -32,6 +33,7 @@ import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
 
+import java.net.InetAddress;
 import java.net.InetSocketAddress;
 import java.util.ArrayList;
 import java.util.List;
@@ -57,7 +59,7 @@ public class HttpPipeliningHandlerTest extends ESTestCase {
     private static final long CONNECTION_TIMEOUT = 10000L;
     private static final String CONTENT_TYPE_TEXT = "text/plain; charset=UTF-8";
     // TODO make me random
-    private static final InetSocketAddress HOST_ADDR = new InetSocketAddress("127.0.0.1", 9080);
+    private static final InetSocketAddress HOST_ADDR = new InetSocketAddress(InetAddress.getLoopbackAddress(), 9080);
     private static final String PATH1 = "/1";
     private static final String PATH2 = "/2";
     private static final String SOME_RESPONSE_TEXT = "some response for ";
@@ -123,13 +125,14 @@ public class HttpPipeliningHandlerTest extends ESTestCase {
         assertTrue(connectionFuture.await(CONNECTION_TIMEOUT));
         final Channel clientChannel = connectionFuture.getChannel();
 
+        // NetworkAddress.formatAddress makes a proper HOST header.
         final HttpRequest request1 = new DefaultHttpRequest(
                 HTTP_1_1, HttpMethod.GET, PATH1);
-        request1.headers().add(HOST, HOST_ADDR.toString());
+        request1.headers().add(HOST, NetworkAddress.formatAddress(HOST_ADDR));
 
         final HttpRequest request2 = new DefaultHttpRequest(
                 HTTP_1_1, HttpMethod.GET, PATH2);
-        request2.headers().add(HOST, HOST_ADDR.toString());
+        request2.headers().add(HOST, NetworkAddress.formatAddress(HOST_ADDR));
 
         clientChannel.write(request1);
         clientChannel.write(request2);
diff --git a/core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java b/core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java
index 7725047..bdbcd45 100644
--- a/core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java
+++ b/core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java
@@ -48,6 +48,7 @@ import static org.hamcrest.Matchers.equalTo;
  * Test failure when index replication actions fail mid-flight
  */
 @ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0, transportClientRatio = 0)
+@ESIntegTestCase.SuppressLocalMode
 public class TransportIndexFailuresIT extends ESIntegTestCase {
 
     private static final Settings nodeSettings = Settings.settingsBuilder()
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTest.java b/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTest.java
index 21f6fc8..d1c78c6 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTest.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTest.java
@@ -19,12 +19,18 @@
 
 package org.elasticsearch.index.mapper;
 
+import org.elasticsearch.Version;
+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.ExpectedException;
 
+import static org.elasticsearch.test.VersionUtils.getFirstVersion;
+import static org.elasticsearch.test.VersionUtils.getPreviousVersion;
+import static org.elasticsearch.test.VersionUtils.randomVersionBetween;
 import static org.hamcrest.CoreMatchers.containsString;
+import static org.hamcrest.CoreMatchers.is;
 import static org.hamcrest.Matchers.hasToString;
 
 public class MapperServiceTest extends ESSingleNodeTestCase {
@@ -46,4 +52,39 @@ public class MapperServiceTest extends ESSingleNodeTestCase {
                 .execute()
                 .actionGet();
     }
+
+    @Test
+    public void testThatLongTypeNameIsNotRejectedOnPreElasticsearchVersionTwo() {
+        String index = "text-index";
+        String field = "field";
+        String type = new String(new char[256]).replace("\0", "a");
+
+        CreateIndexResponse response =
+                client()
+                        .admin()
+                        .indices()
+                        .prepareCreate(index)
+                        .setSettings(settings(randomVersionBetween(random(), getFirstVersion(), getPreviousVersion(Version.V_2_0_0_beta1))))
+                        .addMapping(type, field, "type=string")
+                        .execute()
+                        .actionGet();
+        assertNotNull(response);
+    }
+
+    @Test
+    public void testTypeNameTooLong() {
+        String index = "text-index";
+        String field = "field";
+        String type = new String(new char[256]).replace("\0", "a");
+
+        expectedException.expect(MapperParsingException.class);
+        expectedException.expect(hasToString(containsString("mapping type name [" + type + "] is too long; limit is length 255 but was [256]")));
+        client()
+                .admin()
+                .indices()
+                .prepareCreate(index)
+                .addMapping(type, field, "type=string")
+                .execute()
+                .actionGet();
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperIntegrationIT.java b/core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperIntegrationIT.java
index 613cdde..837a7a0 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperIntegrationIT.java
@@ -21,6 +21,7 @@ package org.elasticsearch.index.mapper.core;
 
 import com.carrotsearch.randomizedtesting.annotations.Name;
 import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.bulk.BulkResponse;
 import org.elasticsearch.action.index.IndexRequestBuilder;
@@ -35,7 +36,6 @@ import org.junit.Test;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.List;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
@@ -98,8 +98,8 @@ public class TokenCountFieldMapperIntegrationIT extends ESIntegTestCase {
     public void facetByTokenCount() throws IOException {
         init();
 
-        String facetField = randomFrom(Arrays.asList(
-            "foo.token_count", "foo.token_count_unstored", "foo.token_count_with_doc_values"));
+        String facetField = randomFrom(ImmutableList.of(
+                "foo.token_count", "foo.token_count_unstored", "foo.token_count_with_doc_values"));
         SearchResponse result = searchByNumericRange(1, 10)
                 .addAggregation(AggregationBuilders.terms("facet").field(facetField)).get();
         assertSearchReturns(result, "single", "bulk1", "bulk2", "multi", "multibulk1", "multibulk2");
@@ -166,7 +166,7 @@ public class TokenCountFieldMapperIntegrationIT extends ESIntegTestCase {
 
     private SearchRequestBuilder searchByNumericRange(int low, int high) {
         return prepareSearch().setQuery(QueryBuilders.rangeQuery(randomFrom(
-                Arrays.asList("foo.token_count", "foo.token_count_unstored", "foo.token_count_with_doc_values")
+                ImmutableList.of("foo.token_count", "foo.token_count_unstored", "foo.token_count_with_doc_values")
         )).gte(low).lte(high));
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java
index aacf34b..fb67401 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java
@@ -83,6 +83,9 @@ public class SimpleDateMappingTests extends ESSingleNodeTestCase {
 
         FieldMapper fieldMapper = defaultMapper.mappers().smartNameFieldMapper("date_field1");
         assertThat(fieldMapper, instanceOf(DateFieldMapper.class));
+        DateFieldMapper dateFieldMapper = (DateFieldMapper)fieldMapper;
+        assertEquals("yyyy/MM/dd HH:mm:ss||yyyy/MM/dd||epoch_millis", dateFieldMapper.fieldType().dateTimeFormatter().format());
+        assertEquals(1265587200000L, dateFieldMapper.fieldType().dateTimeFormatter().parser().parseMillis("1265587200000"));
         fieldMapper = defaultMapper.mappers().smartNameFieldMapper("date_field2");
         assertThat(fieldMapper, instanceOf(DateFieldMapper.class));
 
diff --git a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
index 3aadd0d..1efe8d7 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
@@ -25,6 +25,7 @@ import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.stats.IndexStats;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.cluster.*;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingState;
@@ -454,6 +455,22 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         assertPathHasBeenCleared(idxPath);
     }
 
+    public void testExpectedShardSizeIsPresent() throws InterruptedException {
+        assertAcked(client().admin().indices().prepareCreate("test")
+                .setSettings(SETTING_NUMBER_OF_SHARDS, 1, SETTING_NUMBER_OF_REPLICAS, 0));
+        for (int i = 0; i < 50; i++) {
+            client().prepareIndex("test", "test").setSource("{}").get();
+        }
+        ensureGreen("test");
+        InternalClusterInfoService clusterInfoService = (InternalClusterInfoService) getInstanceFromNode(ClusterInfoService.class);
+        InternalClusterInfoService.ClusterInfoUpdateJob job = clusterInfoService.new ClusterInfoUpdateJob(false);
+        job.run();
+        ClusterState state = getInstanceFromNode(ClusterService.class).state();
+        Long test = clusterInfoService.getClusterInfo().getShardSize(state.getRoutingTable().index("test").getShards().get(0).primaryShard());
+        assertNotNull(test);
+        assertTrue(test > 0);
+    }
+
     public void testIndexCanChangeCustomDataPath() throws Exception {
         Environment env = getInstanceFromNode(Environment.class);
         Path idxPath = env.sharedDataFile().resolve(randomAsciiOfLength(10));
diff --git a/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java b/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
index c75f2fa..a1b2508 100644
--- a/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
@@ -64,6 +64,7 @@ import static org.hamcrest.Matchers.*;
 /**
  */
 @ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0, numClientNodes = 0, transportClientRatio = 0)
+@ESIntegTestCase.SuppressLocalMode
 public class RareClusterStateIT extends ESIntegTestCase {
 
     @Override
diff --git a/core/src/test/java/org/elasticsearch/indices/warmer/IndicesWarmerBlocksIT.java b/core/src/test/java/org/elasticsearch/indices/warmer/IndicesWarmerBlocksIT.java
index 0ee4ab6..e6afa4c 100644
--- a/core/src/test/java/org/elasticsearch/indices/warmer/IndicesWarmerBlocksIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/warmer/IndicesWarmerBlocksIT.java
@@ -21,6 +21,7 @@ package org.elasticsearch.indices.warmer;
 
 
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.action.admin.indices.warmer.get.GetWarmersResponse;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
@@ -29,7 +30,6 @@ import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
 import org.junit.Test;
 
 import java.util.Arrays;
-import java.util.List;
 
 import static org.elasticsearch.cluster.metadata.IndexMetaData.*;
 import static org.elasticsearch.cluster.metadata.MetaData.CLUSTER_READ_ONLY_BLOCK;
@@ -106,7 +106,7 @@ public class IndicesWarmerBlocksIT extends ESIntegTestCase {
                 GetWarmersResponse response = client().admin().indices().prepareGetWarmers("test-blocks").get();
                 assertThat(response.warmers().size(), equalTo(1));
 
-                ObjectObjectCursor<String, List<IndexWarmersMetaData.Entry>> entry = response.warmers().iterator().next();
+                ObjectObjectCursor<String, ImmutableList<IndexWarmersMetaData.Entry>> entry = response.warmers().iterator().next();
                 assertThat(entry.key, equalTo("test-blocks"));
                 assertThat(entry.value.size(), equalTo(1));
                 assertThat(entry.value.iterator().next().name(), equalTo("warmer_block"));
diff --git a/core/src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerIT.java b/core/src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerIT.java
index 687da40..6179af7 100644
--- a/core/src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerIT.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.indices.warmer;
 
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+import com.google.common.collect.ImmutableList;
 
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.admin.indices.segments.IndexSegments;
@@ -45,7 +46,6 @@ import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
 import org.junit.Test;
 
-import java.util.List;
 import java.util.Locale;
 
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
@@ -185,7 +185,7 @@ public class SimpleIndicesWarmerIT extends ESIntegTestCase {
 
         GetWarmersResponse getWarmersResponse = client().admin().indices().prepareGetWarmers("test").get();
         assertThat(getWarmersResponse.warmers().size(), equalTo(1));
-        ObjectObjectCursor<String, List<IndexWarmersMetaData.Entry>> entry = getWarmersResponse.warmers().iterator().next();
+        ObjectObjectCursor<String, ImmutableList<IndexWarmersMetaData.Entry>> entry = getWarmersResponse.warmers().iterator().next();
         assertThat(entry.key, equalTo("test"));
         assertThat(entry.value.size(), equalTo(1));
         assertThat(entry.value.iterator().next().name(), equalTo("custom_warmer"));
diff --git a/core/src/test/java/org/elasticsearch/monitor/process/ProcessProbeTests.java b/core/src/test/java/org/elasticsearch/monitor/process/ProcessProbeTests.java
index 449ea12..18b5f7a 100644
--- a/core/src/test/java/org/elasticsearch/monitor/process/ProcessProbeTests.java
+++ b/core/src/test/java/org/elasticsearch/monitor/process/ProcessProbeTests.java
@@ -20,7 +20,7 @@
 package org.elasticsearch.monitor.process;
 
 import org.apache.lucene.util.Constants;
-import org.elasticsearch.bootstrap.Bootstrap;
+import org.elasticsearch.bootstrap.BootstrapInfo;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
@@ -37,7 +37,7 @@ public class ProcessProbeTests extends ESTestCase {
         assertNotNull(info);
         assertThat(info.getRefreshInterval(), greaterThanOrEqualTo(0L));
         assertThat(info.getId(), equalTo(jvmInfo().pid()));
-        assertThat(info.isMlockall(), equalTo(Bootstrap.isMemoryLocked()));
+        assertThat(info.isMlockall(), equalTo(BootstrapInfo.isMemoryLocked()));
     }
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java b/core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java
index f868c04..b5bd800 100644
--- a/core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java
+++ b/core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java
@@ -23,15 +23,17 @@ import org.elasticsearch.common.cli.CliToolTestCase;
 import org.elasticsearch.common.cli.Terminal;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsException;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.After;
 import org.junit.Before;
+import org.junit.Rule;
 import org.junit.Test;
+import org.junit.rules.ExpectedException;
 
+import java.io.IOException;
 import java.io.InputStream;
-import java.net.URL;
-import java.net.URLClassLoader;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.util.ArrayList;
@@ -42,6 +44,8 @@ import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.hamcrest.Matchers.*;
 
 public class InternalSettingsPreparerTests extends ESTestCase {
+    @Rule
+    public ExpectedException expectedException = ExpectedException.none();
 
     @Before
     public void setupSystemProperties() {
@@ -76,29 +80,6 @@ public class InternalSettingsPreparerTests extends ESTestCase {
     }
 
     @Test
-    public void testAlternateConfigFileSuffixes() throws Exception {
-        InputStream yaml = getClass().getResourceAsStream("/config/elasticsearch.yaml");
-        InputStream json = getClass().getResourceAsStream("/config/elasticsearch.json");
-        InputStream properties = getClass().getResourceAsStream("/config/elasticsearch.properties");
-        Path home = createTempDir();
-        Path config = home.resolve("config");
-        Files.createDirectory(config);
-        Files.copy(yaml, config.resolve("elasticsearch.yaml"));
-        Files.copy(json, config.resolve("elasticsearch.json"));
-        Files.copy(properties, config.resolve("elasticsearch.properties"));
-
-        // test that we can read config files with .yaml, .json, and .properties suffixes
-        Tuple<Settings, Environment> tuple = InternalSettingsPreparer.prepareSettings(settingsBuilder()
-                .put("config.ignore_system_properties", true)
-                .put("path.home", home)
-                .build(), true);
-
-        assertThat(tuple.v1().get("yaml.config.exists"), equalTo("true"));
-        assertThat(tuple.v1().get("json.config.exists"), equalTo("true"));
-        assertThat(tuple.v1().get("properties.config.exists"), equalTo("true"));
-    }
-
-    @Test
     public void testReplacePromptPlaceholders() {
         final List<String> replacedSecretProperties = new ArrayList<>();
         final List<String> replacedTextProperties = new ArrayList<>();
@@ -235,4 +216,37 @@ public class InternalSettingsPreparerTests extends ESTestCase {
         assertThat(settings.get("name"), is("prompted name 0"));
         assertThat(settings.get("node.name"), is("prompted name 0"));
     }
+
+    @Test(expected = SettingsException.class)
+    public void testGarbageIsNotSwallowed() throws IOException {
+        InputStream garbage = getClass().getResourceAsStream("/config/garbage/garbage.yml");
+        Path home = createTempDir();
+        Path config = home.resolve("config");
+        Files.createDirectory(config);
+        Files.copy(garbage, config.resolve("elasticsearch.yml"));
+        InternalSettingsPreparer.prepareSettings(settingsBuilder()
+                .put("config.ignore_system_properties", true)
+                .put("path.home", home)
+                .build(), true);
+    }
+
+    public void testMultipleSettingsFileNotAllowed() throws IOException {
+        InputStream yaml = getClass().getResourceAsStream("/config/elasticsearch.yaml");
+        InputStream properties = getClass().getResourceAsStream("/config/elasticsearch.properties");
+        Path home = createTempDir();
+        Path config = home.resolve("config");
+        Files.createDirectory(config);
+        Files.copy(yaml, config.resolve("elasticsearch.yaml"));
+        Files.copy(properties, config.resolve("elasticsearch.properties"));
+
+        expectedException.expect(SettingsException.class);
+        expectedException.expectMessage("multiple settings files found with suffixes: ");
+        expectedException.expectMessage("yaml");
+        expectedException.expectMessage("properties");
+
+        InternalSettingsPreparer.prepareSettings(settingsBuilder()
+                .put("config.ignore_system_properties", true)
+                .put("path.home", home)
+                .build(), true);
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/options/detailederrors/DetailedErrorsEnabledIT.java b/core/src/test/java/org/elasticsearch/options/detailederrors/DetailedErrorsEnabledIT.java
index af3e570..050d88c 100644
--- a/core/src/test/java/org/elasticsearch/options/detailederrors/DetailedErrorsEnabledIT.java
+++ b/core/src/test/java/org/elasticsearch/options/detailederrors/DetailedErrorsEnabledIT.java
@@ -32,6 +32,7 @@ import org.elasticsearch.test.rest.client.http.HttpResponse;
 import org.junit.Test;
 
 import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.not;
 
 /**
  * Tests that by default the error_trace parameter can be used to show stacktraces
@@ -59,6 +60,16 @@ public class DetailedErrorsEnabledIT extends ESIntegTestCase {
                 .execute();
 
         assertThat(response.getHeaders().get("Content-Type"), containsString("application/json"));
-        assertThat(response.getBody(), containsString("\"error_trace\":{\"message\":\"Validation Failed"));
+        assertThat(response.getBody(), containsString("\"stack_trace\":\"[Validation Failed: 1: index / indices is missing;]; nested: ActionRequestValidationException[Validation Failed: 1:"));
+
+        // Make the HTTP request
+        response = new HttpRequestBuilder(HttpClients.createDefault())
+                .httpTransport(internalCluster().getDataNodeInstance(HttpServerTransport.class))
+                .path("/")
+                .method(HttpDeleteWithEntity.METHOD_NAME)
+                .execute();
+
+        assertThat(response.getHeaders().get("Content-Type"), containsString("application/json"));
+        assertThat(response.getBody(), not(containsString("\"stack_trace\":\"[Validation Failed: 1: index / indices is missing;]; nested: ActionRequestValidationException[Validation Failed: 1:")));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/plugins/PluggableTransportModuleIT.java b/core/src/test/java/org/elasticsearch/plugins/PluggableTransportModuleIT.java
index 0790438..bbeeac1 100644
--- a/core/src/test/java/org/elasticsearch/plugins/PluggableTransportModuleIT.java
+++ b/core/src/test/java/org/elasticsearch/plugins/PluggableTransportModuleIT.java
@@ -23,6 +23,7 @@ import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.DiscoveryModule;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.transport.AssertingLocalTransport;
 import org.elasticsearch.threadpool.ThreadPool;
@@ -49,6 +50,7 @@ public class PluggableTransportModuleIT extends ESIntegTestCase {
     protected Settings nodeSettings(int nodeOrdinal) {
         return settingsBuilder()
                 .put(super.nodeSettings(nodeOrdinal))
+                .put(DiscoveryModule.DISCOVERY_TYPE_KEY, "local")
                 .put("plugin.types", CountingSentRequestsPlugin.class.getName())
                 .build();
     }
diff --git a/core/src/test/java/org/elasticsearch/plugins/PluginInfoTests.java b/core/src/test/java/org/elasticsearch/plugins/PluginInfoTests.java
index 2cdf823..58180b2 100644
--- a/core/src/test/java/org/elasticsearch/plugins/PluginInfoTests.java
+++ b/core/src/test/java/org/elasticsearch/plugins/PluginInfoTests.java
@@ -21,6 +21,7 @@ package org.elasticsearch.plugins;
 
 import com.google.common.base.Function;
 import com.google.common.collect.Lists;
+
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.cluster.node.info.PluginsInfo;
 import org.elasticsearch.test.ESTestCase;
@@ -53,13 +54,14 @@ public class PluginInfoTests extends ESTestCase {
         Path pluginDir = createTempDir().resolve("fake-plugin");
         writeProperties(pluginDir,
             "description", "fake desc",
+            "name", "my_plugin",
             "version", "1.0",
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", System.getProperty("java.specification.version"),
             "jvm", "true",
             "classname", "FakePlugin");
         PluginInfo info = PluginInfo.readFromProperties(pluginDir);
-        assertEquals("fake-plugin", info.getName());
+        assertEquals("my_plugin", info.getName());
         assertEquals("fake desc", info.getDescription());
         assertEquals("1.0", info.getVersion());
         assertEquals("FakePlugin", info.getClassname());
@@ -69,11 +71,30 @@ public class PluginInfoTests extends ESTestCase {
         assertNull(info.getUrl());
     }
 
-    public void testReadFromPropertiesDescriptionMissing() throws Exception {
+    public void testReadFromPropertiesNameMissing() throws Exception {
         Path pluginDir = createTempDir().resolve("fake-plugin");
         writeProperties(pluginDir);
         try {
             PluginInfo.readFromProperties(pluginDir);
+            fail("expected missing name exception");
+        } catch (IllegalArgumentException e) {
+            assertTrue(e.getMessage().contains("Property [name] is missing in"));
+        }
+
+        writeProperties(pluginDir, "name", "");
+        try {
+            PluginInfo.readFromProperties(pluginDir);
+            fail("expected missing name exception");
+        } catch (IllegalArgumentException e) {
+            assertTrue(e.getMessage().contains("Property [name] is missing in"));
+        }
+    }
+
+    public void testReadFromPropertiesDescriptionMissing() throws Exception {
+        Path pluginDir = createTempDir().resolve("fake-plugin");
+        writeProperties(pluginDir, "name", "fake-plugin");
+        try {
+            PluginInfo.readFromProperties(pluginDir);
             fail("expected missing description exception");
         } catch (IllegalArgumentException e) {
             assertTrue(e.getMessage().contains("[description] is missing"));
@@ -82,7 +103,7 @@ public class PluginInfoTests extends ESTestCase {
 
     public void testReadFromPropertiesVersionMissing() throws Exception {
         Path pluginDir = createTempDir().resolve("fake-plugin");
-        writeProperties(pluginDir, "description", "fake desc");
+        writeProperties(pluginDir, "description", "fake desc", "name", "fake-plugin");
         try {
             PluginInfo.readFromProperties(pluginDir);
             fail("expected missing version exception");
@@ -95,7 +116,8 @@ public class PluginInfoTests extends ESTestCase {
         Path pluginDir = createTempDir().resolve("fake-plugin");
         writeProperties(pluginDir,
             "description", "fake desc",
-            "version", "1.0");
+            "version", "1.0",
+            "name", "my_plugin");
         try {
             PluginInfo.readFromProperties(pluginDir);
             fail("expected jvm or site exception");
@@ -108,6 +130,7 @@ public class PluginInfoTests extends ESTestCase {
         Path pluginDir = createTempDir().resolve("fake-plugin");
         writeProperties(pluginDir,
             "description", "fake desc",
+            "name", "my_plugin",
             "version", "1.0",
             "jvm", "true");
         try {
@@ -122,6 +145,7 @@ public class PluginInfoTests extends ESTestCase {
         Path pluginDir = createTempDir().resolve("fake-plugin");
         writeProperties(pluginDir,
             "description", "fake desc",
+            "name", "my_plugin",
             "elasticsearch.version", Version.CURRENT.toString(),
             "version", "1.0",
             "jvm", "true");
@@ -134,9 +158,11 @@ public class PluginInfoTests extends ESTestCase {
     }
 
     public void testReadFromPropertiesJavaVersionIncompatible() throws Exception {
-        Path pluginDir = createTempDir().resolve("fake-plugin");
+        String pluginName = "fake-plugin";
+        Path pluginDir = createTempDir().resolve(pluginName);
         writeProperties(pluginDir,
             "description", "fake desc",
+            "name", pluginName,
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", "1000000.0",
             "classname", "FakePlugin",
@@ -146,7 +172,26 @@ public class PluginInfoTests extends ESTestCase {
             PluginInfo.readFromProperties(pluginDir);
             fail("expected incompatible java version exception");
         } catch (IllegalStateException e) {
-            assertTrue(e.getMessage(), e.getMessage().contains("fake-plugin requires Java"));
+            assertTrue(e.getMessage(), e.getMessage().contains(pluginName + " requires Java"));
+        }
+    }
+
+    public void testReadFromPropertiesBadJavaVersionFormat() throws Exception {
+        String pluginName = "fake-plugin";
+        Path pluginDir = createTempDir().resolve(pluginName);
+        writeProperties(pluginDir,
+                "description", "fake desc",
+                "name", pluginName,
+                "elasticsearch.version", Version.CURRENT.toString(),
+                "java.version", "1.7.0_80",
+                "classname", "FakePlugin",
+                "version", "1.0",
+                "jvm", "true");
+        try {
+            PluginInfo.readFromProperties(pluginDir);
+            fail("expected bad java version format exception");
+        } catch (IllegalStateException e) {
+            assertTrue(e.getMessage(), e.getMessage().equals("version string must be a sequence of nonnegative decimal integers separated by \".\"'s and may have leading zeros but was 1.7.0_80"));
         }
     }
 
@@ -156,6 +201,7 @@ public class PluginInfoTests extends ESTestCase {
             "description", "fake desc",
             "version", "1.0",
             "jvm", "true",
+            "name", "my_plugin",
             "elasticsearch.version", "bogus");
         try {
             PluginInfo.readFromProperties(pluginDir);
@@ -169,6 +215,7 @@ public class PluginInfoTests extends ESTestCase {
         Path pluginDir = createTempDir().resolve("fake-plugin");
         writeProperties(pluginDir,
             "description", "fake desc",
+            "name", "my_plugin",
             "version", "1.0",
             "jvm", "true",
             "elasticsearch.version", Version.V_1_7_0.toString());
@@ -184,6 +231,7 @@ public class PluginInfoTests extends ESTestCase {
         Path pluginDir = createTempDir().resolve("fake-plugin");
         writeProperties(pluginDir,
             "description", "fake desc",
+            "name", "my_plugin",
             "version", "1.0",
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", System.getProperty("java.specification.version"),
@@ -201,6 +249,7 @@ public class PluginInfoTests extends ESTestCase {
         Files.createDirectories(pluginDir.resolve("_site"));
         writeProperties(pluginDir,
             "description", "fake desc",
+            "name", "my_plugin",
             "version", "1.0",
             "site", "true");
         PluginInfo info = PluginInfo.readFromProperties(pluginDir);
@@ -208,11 +257,12 @@ public class PluginInfoTests extends ESTestCase {
         assertFalse(info.isJvm());
         assertEquals("NA", info.getClassname());
     }
-    
+
     public void testReadFromPropertiesSitePluginWithoutSite() throws Exception {
         Path pluginDir = createTempDir().resolve("fake-plugin");
         writeProperties(pluginDir,
             "description", "fake desc",
+            "name", "my_plugin",
             "version", "1.0",
             "site", "true");
         try {
diff --git a/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java b/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
index 4acb818..8e56b36 100644
--- a/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
+++ b/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
@@ -20,6 +20,7 @@ package org.elasticsearch.plugins;
 
 import com.google.common.base.Charsets;
 import com.google.common.hash.Hashing;
+
 import org.apache.http.impl.client.HttpClients;
 import org.apache.lucene.util.LuceneTestCase;
 import org.elasticsearch.Version;
@@ -51,8 +52,10 @@ import org.junit.Test;
 import javax.net.ssl.HttpsURLConnection;
 import javax.net.ssl.SSLContext;
 import javax.net.ssl.SSLSocketFactory;
+
 import java.io.BufferedWriter;
 import java.io.IOException;
+import java.net.InetAddress;
 import java.net.InetSocketAddress;
 import java.nio.charset.StandardCharsets;
 import java.nio.file.FileVisitResult;
@@ -175,11 +178,13 @@ public class PluginManagerIT extends ESIntegTestCase {
         Path pluginDir = createTempDir().resolve("fake-plugin");
         String pluginUrl = createPlugin(pluginDir,
             "description", "fake desc",
+            "name", "fake-plugin",
             "version", "1.0",
             "elasticsearch.version", Version.CURRENT.toString(),
+            "java.version", System.getProperty("java.specification.version"),
             "jvm", "true",
             "classname", "FakePlugin");
-        assertStatus("install --url " + pluginUrl, USAGE);
+        assertStatus("install", USAGE);
     }
 
     @Test
@@ -191,21 +196,22 @@ public class PluginManagerIT extends ESIntegTestCase {
         Files.createFile(pluginDir.resolve("bin").resolve("tool"));
         Files.createDirectories(pluginDir.resolve("config"));
         Files.createFile(pluginDir.resolve("config").resolve("file"));
-        
+
         String pluginUrl = createPlugin(pluginDir,
             "description", "fake desc",
+            "name", pluginName,
             "version", "1.0",
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", System.getProperty("java.specification.version"),
             "jvm", "true",
             "classname", "FakePlugin");
-        
+
         Environment env = initialSettings.v2();
         Path binDir = env.binFile();
         Path pluginBinDir = binDir.resolve(pluginName);
 
         Path pluginConfigDir = env.configFile().resolve(pluginName);
-        assertStatusOk("install " + pluginName + " --url " + pluginUrl + " --verbose");
+        assertStatusOk("install " + pluginUrl + " --verbose");
 
         terminal.getTerminalOutput().clear();
         assertStatusOk("list");
@@ -236,19 +242,20 @@ public class PluginManagerIT extends ESIntegTestCase {
         // create config/test.txt with contents 'version1'
         Files.createDirectories(pluginDir.resolve("config"));
         Files.write(pluginDir.resolve("config").resolve("test.txt"), "version1".getBytes(StandardCharsets.UTF_8));
-        
+
         String pluginUrl = createPlugin(pluginDir,
             "description", "fake desc",
+            "name", pluginName,
             "version", "1.0",
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", System.getProperty("java.specification.version"),
             "jvm", "true",
             "classname", "FakePlugin");
-        
+
         Environment env = initialSettings.v2();
         Path pluginConfigDir = env.configFile().resolve(pluginName);
 
-        assertStatusOk(String.format(Locale.ROOT, "install %s --url %s --verbose", pluginName, pluginUrl));
+        assertStatusOk(String.format(Locale.ROOT, "install %s --verbose", pluginUrl));
 
         /*
         First time, our plugin contains:
@@ -275,13 +282,14 @@ public class PluginManagerIT extends ESIntegTestCase {
         Files.write(pluginDir.resolve("config").resolve("dir").resolve("subdir").resolve("testsubdir.txt"), "version1".getBytes(StandardCharsets.UTF_8));
         pluginUrl = createPlugin(pluginDir,
                 "description", "fake desc",
+                "name", pluginName,
                 "version", "2.0",
                 "elasticsearch.version", Version.CURRENT.toString(),
                 "java.version", System.getProperty("java.specification.version"),
                 "jvm", "true",
                 "classname", "FakePlugin");
- 
-        assertStatusOk(String.format(Locale.ROOT, "install %s --url %s --verbose", pluginName, pluginUrl));
+
+        assertStatusOk(String.format(Locale.ROOT, "install %s --verbose", pluginUrl));
 
         assertFileContent(pluginConfigDir, "test.txt", "version1");
         assertFileContent(pluginConfigDir, "test.txt.new", "version2");
@@ -311,13 +319,14 @@ public class PluginManagerIT extends ESIntegTestCase {
         Files.write(pluginDir.resolve("config").resolve("dir").resolve("subdir").resolve("testsubdir.txt"), "version2".getBytes(StandardCharsets.UTF_8));
         pluginUrl = createPlugin(pluginDir,
                 "description", "fake desc",
+                "name", pluginName,
                 "version", "3.0",
                 "elasticsearch.version", Version.CURRENT.toString(),
                 "java.version", System.getProperty("java.specification.version"),
                 "jvm", "true",
                 "classname", "FakePlugin");
 
-        assertStatusOk(String.format(Locale.ROOT, "install %s --url %s --verbose", pluginName, pluginUrl));
+        assertStatusOk(String.format(Locale.ROOT, "install %s --verbose", pluginUrl));
 
         assertFileContent(pluginConfigDir, "test.txt", "version1");
         assertFileContent(pluginConfigDir, "test2.txt", "version1");
@@ -339,17 +348,18 @@ public class PluginManagerIT extends ESIntegTestCase {
         Files.createFile(pluginDir.resolve("bin").resolve("tool"));;
         String pluginUrl = createPlugin(pluginDir,
             "description", "fake desc",
+            "name", "fake-plugin",
             "version", "1.0",
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", System.getProperty("java.specification.version"),
             "jvm", "true",
             "classname", "FakePlugin");
-        
+
         Environment env = initialSettings.v2();
         Path binDir = env.binFile();
         Path pluginBinDir = binDir.resolve(pluginName);
 
-        assertStatusOk(String.format(Locale.ROOT, "install %s --url %s --verbose", pluginName, pluginUrl));
+        assertStatusOk(String.format(Locale.ROOT, "install %s --verbose", pluginUrl));
         assertThatPluginIsListed(pluginName);
         assertDirectoryExists(pluginBinDir);
     }
@@ -373,12 +383,13 @@ public class PluginManagerIT extends ESIntegTestCase {
         Path pluginDir = createTempDir().resolve(pluginName);
         String pluginUrl = createPlugin(pluginDir,
             "description", "fake desc",
+            "name", pluginName,
             "version", "1.0",
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", System.getProperty("java.specification.version"),
             "jvm", "true",
             "classname", "FakePlugin");
-        assertStatusOk(String.format(Locale.ROOT, "install %s --url %s --verbose", pluginName, pluginUrl));
+        assertStatusOk(String.format(Locale.ROOT, "install %s --verbose", pluginUrl));
         assertThatPluginIsListed(pluginName);
     }
 
@@ -389,10 +400,11 @@ public class PluginManagerIT extends ESIntegTestCase {
         Files.createDirectories(pluginDir.resolve("_site"));
         Files.createFile(pluginDir.resolve("_site").resolve("somefile"));
         String pluginUrl = createPlugin(pluginDir,
-                "description", "fake desc",
-                "version", "1.0",
-                "site", "true");
-        assertStatusOk(String.format(Locale.ROOT, "install %s --url %s --verbose", pluginName, pluginUrl));
+            "description", "fake desc",
+            "name", pluginName,
+            "version", "1.0",
+            "site", "true");
+        assertStatusOk(String.format(Locale.ROOT, "install %s --verbose", pluginUrl));
         assertThatPluginIsListed(pluginName);
         // We want to check that Plugin Manager moves content to _site
         assertFileExists(initialSettings.v2().pluginsFile().resolve(pluginName).resolve("_site"));
@@ -408,7 +420,7 @@ public class PluginManagerIT extends ESIntegTestCase {
                 "description", "fake desc",
                 "version", "1.0",
                 "site", "true");
-        assertStatus(String.format(Locale.ROOT, "install %s --url %s --verbose", pluginName, pluginUrl),
+        assertStatus(String.format(Locale.ROOT, "install %s --verbose", pluginUrl),
                 ExitStatus.IO_ERROR);
         assertThatPluginIsNotListed(pluginName);
         assertFileNotExists(initialSettings.v2().pluginsFile().resolve(pluginName).resolve("_site"));
@@ -419,7 +431,7 @@ public class PluginManagerIT extends ESIntegTestCase {
         if (pluginCoordinates == null) {
             assertStatusOk(String.format(Locale.ROOT, "install %s --verbose", pluginDescriptor));
         } else {
-            assertStatusOk(String.format(Locale.ROOT, "install %s --url %s --verbose", pluginDescriptor, pluginCoordinates));
+            assertStatusOk(String.format(Locale.ROOT, "install %s --verbose", pluginCoordinates));
         }
         assertThatPluginIsListed(pluginName);
 
@@ -493,15 +505,16 @@ public class PluginManagerIT extends ESIntegTestCase {
     @Test
     public void testRemovePlugin() throws Exception {
         String pluginName = "plugintest";
-        Path pluginDir = createTempDir().resolve(pluginName);        
+        Path pluginDir = createTempDir().resolve(pluginName);
         String pluginUrl = createPlugin(pluginDir,
             "description", "fake desc",
+            "name", pluginName,
             "version", "1.0.0",
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", System.getProperty("java.specification.version"),
             "jvm", "true",
             "classname", "FakePlugin");
-        
+
         // We want to remove plugin with plugin short name
         singlePluginInstallAndRemove("plugintest", "plugintest", pluginUrl);
 
@@ -550,6 +563,7 @@ public class PluginManagerIT extends ESIntegTestCase {
         PluginManager.checkForOfficialPlugins("lang-python");
         PluginManager.checkForOfficialPlugins("mapper-murmur3");
         PluginManager.checkForOfficialPlugins("mapper-size");
+        PluginManager.checkForOfficialPlugins("discovery-multicast");
 
         try {
             PluginManager.checkForOfficialPlugins("elasticsearch-mapper-attachment");
@@ -561,7 +575,7 @@ public class PluginManagerIT extends ESIntegTestCase {
 
     @Test
     public void testThatBasicAuthIsRejectedOnHttp() throws Exception {
-        assertStatus(String.format(Locale.ROOT, "install foo --url http://user:pass@localhost:12345/foo.zip --verbose"), CliTool.ExitStatus.IO_ERROR);
+        assertStatus(String.format(Locale.ROOT, "install http://user:pass@localhost:12345/foo.zip --verbose"), CliTool.ExitStatus.IO_ERROR);
         assertThat(terminal.getTerminalOutput(), hasItem(containsString("Basic auth is only supported for HTTPS!")));
     }
 
@@ -595,10 +609,10 @@ public class PluginManagerIT extends ESIntegTestCase {
                 }
             });
 
-            Channel channel = serverBootstrap.bind(new InetSocketAddress("localhost", 0));
+            Channel channel = serverBootstrap.bind(new InetSocketAddress(InetAddress.getByName("localhost"), 0));
             int port = ((InetSocketAddress) channel.getLocalAddress()).getPort();
             // IO_ERROR because there is no real file delivered...
-            assertStatus(String.format(Locale.ROOT, "install foo --url https://user:pass@localhost:%s/foo.zip --verbose --timeout 1s", port), ExitStatus.IO_ERROR);
+            assertStatus(String.format(Locale.ROOT, "install https://user:pass@localhost:%s/foo.zip --verbose --timeout 1s", port), ExitStatus.IO_ERROR);
 
             // ensure that we did not try any other data source like download.elastic.co, in case we specified our own local URL
             assertThat(terminal.getTerminalOutput(), not(hasItem(containsString("download.elastic.co"))));
@@ -635,7 +649,6 @@ public class PluginManagerIT extends ESIntegTestCase {
 
     private Tuple<Settings, Environment> buildInitialSettings() throws IOException {
         Settings settings = settingsBuilder()
-                .put("discovery.zen.ping.multicast.enabled", false)
                 .put("http.enabled", true)
                 .put("path.home", createTempDir()).build();
         return InternalSettingsPreparer.prepareSettings(settings, false);
diff --git a/core/src/test/java/org/elasticsearch/rest/BytesRestResponseTests.java b/core/src/test/java/org/elasticsearch/rest/BytesRestResponseTests.java
index 20a2058..e379ebb 100644
--- a/core/src/test/java/org/elasticsearch/rest/BytesRestResponseTests.java
+++ b/core/src/test/java/org/elasticsearch/rest/BytesRestResponseTests.java
@@ -105,8 +105,8 @@ public class BytesRestResponseTests extends ESTestCase {
         BytesRestResponse response = new BytesRestResponse(channel, t);
         String text = response.content().toUtf8();
         assertThat(text, containsString("\"type\":\"throwable\",\"reason\":\"an error occurred reading data\""));
-        assertThat(text, containsString("{\"type\":\"file_not_found_exception\",\"reason\":\"/foo/bar\"}"));
-        assertThat(text, containsString("\"error_trace\":{\"message\":\"an error occurred reading data\""));
+        assertThat(text, containsString("{\"type\":\"file_not_found_exception\""));
+        assertThat(text, containsString("\"stack_trace\":\"[an error occurred reading data]"));
     }
 
     public void testGuessRootCause() throws IOException {
@@ -176,7 +176,6 @@ public class BytesRestResponseTests extends ESTestCase {
 
         DetailedExceptionRestChannel(RestRequest request) {
             super(request, true);
-            request.params().put(ElasticsearchException.REST_EXCEPTION_SKIP_STACK_TRACE, "true");
         }
 
         @Override
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
index 4e81448..5b815a4 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
@@ -21,9 +21,11 @@ package org.elasticsearch.search.aggregations.bucket;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.joda.DateMathParser;
 import org.elasticsearch.common.joda.Joda;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.mapper.core.DateFieldMapper;
+import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
@@ -42,6 +44,7 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
+import java.util.concurrent.Callable;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.TimeUnit;
 
@@ -558,7 +561,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         assertThat(bucket.getDocCount(), equalTo(3l));
     }
 
-    
+
 
     /*
     [ Jan 2, Feb 3]
@@ -904,7 +907,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         assertThat(bucket.getDocCount(), equalTo(3l));
     }
 
-    
+
 
       /*
     [ Jan 2, Feb 3]
@@ -1195,6 +1198,70 @@ public class DateHistogramIT extends ESIntegTestCase {
         }
     }
 
+    /**
+     * Test date histogram aggregation with hour interval, timezone shift and
+     * extended bounds (see https://github.com/elastic/elasticsearch/issues/12278)
+     */
+    @Test
+    public void singleValueField_WithExtendedBoundsTimezone() throws Exception {
+
+        String index = "test12278";
+        prepareCreate(index)
+                .setSettings(Settings.builder().put(indexSettings()).put("index.number_of_shards", 1).put("index.number_of_replicas", 0))
+                .execute().actionGet();
+
+        DateMathParser parser = new DateMathParser(Joda.getStrictStandardDateFormatter());
+
+        final Callable<Long> callable = new Callable<Long>() {
+            @Override
+            public Long call() throws Exception {
+                return System.currentTimeMillis();
+            }
+        };
+
+        // we pick a random timezone offset of +12/-12 hours and insert two documents
+        // one at 00:00 in that time zone and one at 12:00
+        List<IndexRequestBuilder> builders = new ArrayList<>();
+        int timeZoneHourOffset = randomIntBetween(-12, 12);
+        DateTimeZone timezone = DateTimeZone.forOffsetHours(timeZoneHourOffset);
+        DateTime timeZoneStartToday = new DateTime(parser.parse("now/d", callable, false, timezone), DateTimeZone.UTC);
+        DateTime timeZoneNoonToday = new DateTime(parser.parse("now/d+12h", callable, false, timezone), DateTimeZone.UTC);
+        builders.add(indexDoc(index, timeZoneStartToday, 1));
+        builders.add(indexDoc(index, timeZoneNoonToday, 2));
+        indexRandom(true, builders);
+        ensureSearchable(index);
+
+        SearchResponse response = null;
+        // retrieve those docs with the same time zone and extended bounds
+        response = client()
+                .prepareSearch(index)
+                .setQuery(QueryBuilders.rangeQuery("date").from("now/d").to("now/d").includeLower(true).includeUpper(true).timeZone(timezone.getID()))
+                .addAggregation(
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.hours(1)).timeZone(timezone.getID()).minDocCount(0)
+                                .extendedBounds("now/d", "now/d+23h")
+                                ).execute().actionGet();
+        assertSearchResponse(response);
+
+        assertThat("Expected 24 buckets for one day aggregation with hourly interval", response.getHits().totalHits(), equalTo(2l));
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        List<? extends Bucket> buckets = histo.getBuckets();
+        assertThat(buckets.size(), equalTo(24));
+
+        for (int i = 0; i < buckets.size(); i++) {
+            Histogram.Bucket bucket = buckets.get(i);
+            assertThat(bucket, notNullValue());
+            assertThat("Bucket " + i +" had wrong key", (DateTime) bucket.getKey(), equalTo(new DateTime(timeZoneStartToday.getMillis() + (i * 60 * 60 * 1000), DateTimeZone.UTC)));
+            if (i == 0 || i == 12) {
+                assertThat(bucket.getDocCount(), equalTo(1l));
+            } else {
+                assertThat(bucket.getDocCount(), equalTo(0l));
+            }
+        }
+    }
+
     @Test
     public void singleValue_WithMultipleDateFormatsFromMapping() throws Exception {
 
@@ -1233,7 +1300,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .execute().actionGet();
 
         assertSearchResponse(response);
-        
+
         DateTimeZone tz = DateTimeZone.forID("+01:00");
 
         Histogram histo = response.getAggregations().get("histo");
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsIT.java
index 38872f2..8564929 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsIT.java
@@ -22,11 +22,13 @@ import com.google.common.base.Strings;
 
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
 import org.elasticsearch.index.mapper.internal.IndexFieldMapper;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.script.Script;
+import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
@@ -388,7 +390,7 @@ public class StringTermsIT extends AbstractTermsTestCase {
             assertThat(bucket.getDocCount(), equalTo(1l));
         }
 
-        // Check case with only exact term exclude clauses 
+        // Check case with only exact term exclude clauses
         response = client()
                 .prepareSearch("idx")
                 .setTypes("high_card_type")
@@ -690,11 +692,11 @@ public class StringTermsIT extends AbstractTermsTestCase {
     }
 
     /*
-     * 
+     *
      * [foo_val0, foo_val1] [foo_val1, foo_val2] [foo_val2, foo_val3] [foo_val3,
      * foo_val4] [foo_val4, foo_val5]
-     * 
-     * 
+     *
+     *
      * foo_val0 - doc_count: 1 - val_count: 2 foo_val1 - doc_count: 2 -
      * val_count: 4 foo_val2 - doc_count: 2 - val_count: 4 foo_val3 - doc_count:
      * 2 - val_count: 4 foo_val4 - doc_count: 2 - val_count: 4 foo_val5 -
@@ -996,6 +998,36 @@ public class StringTermsIT extends AbstractTermsTestCase {
     }
 
     @Test
+    public void singleValuedField_OrderedByIllegalAgg() throws Exception {
+        boolean asc = true;
+        try {
+            client()
+                .prepareSearch("idx")
+                .setTypes("type")
+                .addAggregation(
+                        terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
+                                .collectMode(randomFrom(SubAggCollectionMode.values()))
+                                .order(Terms.Order.aggregation("inner_terms>avg", asc))
+                                .subAggregation(terms("inner_terms").field(MULTI_VALUED_FIELD_NAME).subAggregation(avg("avg").field("i"))))
+                .execute().actionGet();
+            fail("Expected an exception");
+        } catch (SearchPhaseExecutionException e) {
+            ElasticsearchException[] rootCauses = e.guessRootCauses();
+            if (rootCauses.length == 1) {
+                ElasticsearchException rootCause = rootCauses[0];
+                if (rootCause instanceof AggregationExecutionException) {
+                    AggregationExecutionException aggException = (AggregationExecutionException) rootCause;
+                    assertThat(aggException.getMessage(), Matchers.startsWith("Invalid terms aggregation order path"));
+                } else {
+                    throw e;
+                }
+            } else {
+                throw e;
+            }
+        }
+    }
+
+    @Test
     public void singleValuedField_OrderedBySingleBucketSubAggregationAsc() throws Exception {
         boolean asc = randomBoolean();
         SearchResponse response = client()
diff --git a/core/src/test/java/org/elasticsearch/search/scan/ScanContextTests.java b/core/src/test/java/org/elasticsearch/search/scan/ScanContextTests.java
index e804393..38c01cb 100644
--- a/core/src/test/java/org/elasticsearch/search/scan/ScanContextTests.java
+++ b/core/src/test/java/org/elasticsearch/search/scan/ScanContextTests.java
@@ -27,13 +27,11 @@ import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
-import org.apache.lucene.search.QueryUtils;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.Directory;
-import org.elasticsearch.search.scan.ScanContext.MinDocQuery;
 import org.elasticsearch.search.scan.ScanContext.ScanCollector;
 import org.elasticsearch.test.ESTestCase;
 
@@ -44,33 +42,6 @@ import java.util.List;
 
 public class ScanContextTests extends ESTestCase {
 
-    public void testMinDocQueryBasics() {
-        MinDocQuery query1 = new MinDocQuery(42);
-        MinDocQuery query2 = new MinDocQuery(42);
-        MinDocQuery query3 = new MinDocQuery(43);
-        QueryUtils.check(query1);
-        QueryUtils.checkEqual(query1, query2);
-        QueryUtils.checkUnequal(query1, query3);
-    }
-
-    public void testMinDocQueryRandom() throws IOException {
-        final int numDocs = randomIntBetween(10, 200);
-        final Document doc = new Document();
-        final Directory dir = newDirectory();
-        final RandomIndexWriter w = new RandomIndexWriter(getRandom(), dir);
-        for (int i = 0; i < numDocs; ++i) {
-            w.addDocument(doc);
-        }
-        final IndexReader reader = w.getReader();
-        final IndexSearcher searcher = newSearcher(reader);
-        for (int i = 0; i <= numDocs; ++i) {
-            assertEquals(numDocs - i, searcher.count(new MinDocQuery(i)));
-        }
-        w.close();
-        reader.close();
-        dir.close();
-    }
-
     private static TopDocs execute(IndexSearcher searcher, ScanContext ctx, Query query, int pageSize, boolean trackScores) throws IOException {
         query = ctx.wrapQuery(query);
         ScanCollector collector = ctx.collector(pageSize, trackScores);
diff --git a/core/src/test/java/org/elasticsearch/search/scroll/DuelScrollIT.java b/core/src/test/java/org/elasticsearch/search/scroll/DuelScrollIT.java
index d4e354a..efa26f2 100644
--- a/core/src/test/java/org/elasticsearch/search/scroll/DuelScrollIT.java
+++ b/core/src/test/java/org/elasticsearch/search/scroll/DuelScrollIT.java
@@ -21,9 +21,13 @@ package org.elasticsearch.search.scroll;
 
 import com.carrotsearch.hppc.IntHashSet;
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
+
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHits;
 import org.elasticsearch.search.sort.SortBuilder;
@@ -226,4 +230,83 @@ public class DuelScrollIT extends ESIntegTestCase {
         }
     }
 
+    private int createIndex(boolean singleShard) throws Exception {
+        Settings.Builder settings = Settings.builder();
+        if (singleShard) {
+            settings.put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1);
+        }
+        // no replicas, as they might be ordered differently
+        settings.put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0);
+
+        assertAcked(prepareCreate("test").setSettings(settings.build()).get());
+        final int numDocs = randomIntBetween(10, 200);
+
+        IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocs];
+        for (int i = 0; i < numDocs; ++i) {
+            builders[i] = client().prepareIndex("test", "type", Integer.toString(i)).setSource("foo", random().nextBoolean());
+        }
+        indexRandom(true, builders);
+        return numDocs;
+    }
+
+    private void testDuelIndexOrder(SearchType searchType, boolean trackScores, int numDocs) throws Exception {
+        final int size = scaledRandomIntBetween(5, numDocs + 5);
+        final SearchResponse control = client().prepareSearch("test")
+                .setSearchType(searchType)
+                .setSize(numDocs)
+                .setQuery(QueryBuilders.matchQuery("foo", "true"))
+                .addSort(SortBuilders.fieldSort("_doc"))
+                .setTrackScores(trackScores)
+                .get();
+        assertNoFailures(control);
+
+        SearchResponse scroll = client().prepareSearch("test")
+                .setSearchType(searchType)
+                .setSize(size)
+                .setQuery(QueryBuilders.matchQuery("foo", "true"))
+                .addSort(SortBuilders.fieldSort("_doc"))
+                .setTrackScores(trackScores)
+                .setScroll("10m").get();
+
+        int scrollDocs = 0;
+        try {
+            while (true) {
+                assertNoFailures(scroll);
+                assertEquals(control.getHits().getTotalHits(), scroll.getHits().getTotalHits());
+                assertEquals(control.getHits().getMaxScore(), scroll.getHits().getMaxScore(), 0.01f);
+                if (scroll.getHits().hits().length == 0) {
+                    break;
+                }
+                for (int i = 0; i < scroll.getHits().hits().length; ++i) {
+                    SearchHit controlHit = control.getHits().getAt(scrollDocs + i);
+                    SearchHit scrollHit = scroll.getHits().getAt(i);
+                    assertEquals(controlHit.getId(), scrollHit.getId());
+                }
+                scrollDocs += scroll.getHits().hits().length;
+                scroll = client().prepareSearchScroll(scroll.getScrollId()).setScroll("10m").get();
+            }
+            assertEquals(control.getHits().getTotalHits(), scrollDocs);
+        } catch (AssertionError e) {
+            logger.info("Control:\n" + control);
+            logger.info("Scroll size=" + size + ", from=" + scrollDocs + ":\n" + scroll);
+            throw e;
+        } finally {
+            clearScroll(scroll.getScrollId());
+        }
+    }
+
+    public void testDuelIndexOrderQueryAndFetch() throws Exception {
+        final SearchType searchType = RandomPicks.randomFrom(random(), Arrays.asList(SearchType.QUERY_AND_FETCH, SearchType.DFS_QUERY_AND_FETCH));
+        // QUERY_AND_FETCH only works with a single shard
+        final int numDocs = createIndex(true);
+        testDuelIndexOrder(searchType, false, numDocs);
+        testDuelIndexOrder(searchType, true, numDocs);
+    }
+
+    public void testDuelIndexOrderQueryThenFetch() throws Exception {
+        final SearchType searchType = RandomPicks.randomFrom(random(), Arrays.asList(SearchType.QUERY_THEN_FETCH, SearchType.DFS_QUERY_THEN_FETCH));
+        final int numDocs = createIndex(false);
+        testDuelIndexOrder(searchType, false, numDocs);
+        testDuelIndexOrder(searchType, true, numDocs);
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java b/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
index aae7800..d006587 100644
--- a/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.search.suggest;
 
 import com.google.common.base.Charsets;
+import com.google.common.collect.ImmutableList;
 import com.google.common.io.Resources;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
@@ -966,7 +967,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
         assertAcked(builder.addMapping("type1", mapping));
         ensureGreen();
 
-        List<String> titles = new ArrayList<>();
+        ImmutableList.Builder<String> titles = ImmutableList.<String>builder();
 
         // We're going to be searching for:
         //   united states house of representatives elections in washington 2006
@@ -1057,7 +1058,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
         }
 
         List<IndexRequestBuilder> builders = new ArrayList<>();
-        for (String title: titles) {
+        for (String title: titles.build()) {
             builders.add(client().prepareIndex("test", "type1").setSource("title", title));
         }
         indexRandom(true, builders);
@@ -1112,7 +1113,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
         assertAcked(builder.addMapping("type1", mapping));
         ensureGreen();
 
-        List<String> titles = new ArrayList<>();
+        ImmutableList.Builder<String> titles = ImmutableList.<String>builder();
 
         titles.add("United States House of Representatives Elections in Washington 2006");
         titles.add("United States House of Representatives Elections in Washington 2005");
@@ -1122,7 +1123,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
         titles.add("Election");
 
         List<IndexRequestBuilder> builders = new ArrayList<>();
-        for (String title: titles) {
+        for (String title: titles.build()) {
             builders.add(client().prepareIndex("test", "type1").setSource("title", title));
         }
         indexRandom(true, builders);
diff --git a/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java b/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java
index 1cfecf3..75c7216 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java
@@ -21,6 +21,7 @@ package org.elasticsearch.snapshots;
 
 import com.carrotsearch.hppc.IntHashSet;
 import com.carrotsearch.hppc.IntSet;
+import com.google.common.collect.ImmutableList;
 import com.google.common.util.concurrent.ListenableFuture;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.action.ListenableActionFuture;
@@ -62,8 +63,10 @@ import org.elasticsearch.rest.RestResponse;
 import org.elasticsearch.rest.action.admin.cluster.repositories.get.RestGetRepositoriesAction;
 import org.elasticsearch.rest.action.admin.cluster.state.RestClusterStateAction;
 import org.elasticsearch.snapshots.mockstore.MockRepository;
+import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.InternalTestCluster;
 import org.elasticsearch.test.rest.FakeRestRequest;
+import org.elasticsearch.transport.TransportModule;
 import org.junit.Test;
 
 import java.io.IOException;
@@ -95,7 +98,8 @@ import static org.hamcrest.Matchers.nullValue;
 
 /**
  */
-@ClusterScope(scope = Scope.TEST, numDataNodes = 0)
+@ClusterScope(scope = Scope.TEST, numDataNodes = 0, transportClientRatio = 0)
+@ESIntegTestCase.SuppressLocalMode // TODO only restorePersistentSettingsTest needs this maybe factor out?
 public class DedicatedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTestCase {
 
     @Test
@@ -473,14 +477,14 @@ public class DedicatedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTest
                 @Override
                 public void run() {
                     SnapshotsStatusResponse snapshotsStatusResponse = client().admin().cluster().prepareSnapshotStatus("test-repo").setSnapshots("test-snap-2").get();
-                    List<SnapshotStatus> snapshotStatuses = snapshotsStatusResponse.getSnapshots();
+                    ImmutableList<SnapshotStatus> snapshotStatuses = snapshotsStatusResponse.getSnapshots();
                     assertEquals(snapshotStatuses.size(), 1);
                     logger.trace("current snapshot status [{}]", snapshotStatuses.get(0));
                     assertTrue(snapshotStatuses.get(0).getState().completed());
                 }
             }, 1, TimeUnit.MINUTES);
             SnapshotsStatusResponse snapshotsStatusResponse = client().admin().cluster().prepareSnapshotStatus("test-repo").setSnapshots("test-snap-2").get();
-            List<SnapshotStatus> snapshotStatuses = snapshotsStatusResponse.getSnapshots();
+            ImmutableList<SnapshotStatus> snapshotStatuses = snapshotsStatusResponse.getSnapshots();
             assertThat(snapshotStatuses.size(), equalTo(1));
             SnapshotStatus snapshotStatus = snapshotStatuses.get(0);
             logger.info("State: [{}], Reason: [{}]", createSnapshotResponse.getSnapshotInfo().state(), createSnapshotResponse.getSnapshotInfo().reason());
diff --git a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
index eb38838..7e28230 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.snapshots;
 
 import com.google.common.base.Predicate;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.ExceptionsHelper;
@@ -72,7 +73,6 @@ import java.nio.file.Path;
 import java.nio.file.StandardOpenOption;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.List;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.ExecutionException;
@@ -1791,9 +1791,9 @@ public class SharedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTestCas
                 shards.put(new ShardId("test-idx", 0), new ShardSnapshotStatus("unknown-node", State.ABORTED));
                 shards.put(new ShardId("test-idx", 1), new ShardSnapshotStatus("unknown-node", State.ABORTED));
                 shards.put(new ShardId("test-idx", 2), new ShardSnapshotStatus("unknown-node", State.ABORTED));
-                List<Entry> entries = new ArrayList<>();
-                entries.add(new Entry(new SnapshotId("test-repo", "test-snap"), true, State.ABORTED, Collections.singletonList("test-idx"), System.currentTimeMillis(), shards.build()));
-                return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, new SnapshotsInProgress(Collections.unmodifiableList(entries))).build();
+                ImmutableList.Builder<Entry> entries = ImmutableList.builder();
+                entries.add(new Entry(new SnapshotId("test-repo", "test-snap"), true, State.ABORTED, ImmutableList.of("test-idx"), System.currentTimeMillis(), shards.build()));
+                return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, new SnapshotsInProgress(entries.build())).build();
             }
 
             @Override
diff --git a/core/src/test/java/org/elasticsearch/snapshots/SnapshotUtilsTests.java b/core/src/test/java/org/elasticsearch/snapshots/SnapshotUtilsTests.java
index 8e9d7cb..76f057a 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/SnapshotUtilsTests.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/SnapshotUtilsTests.java
@@ -18,13 +18,14 @@
  */
 package org.elasticsearch.snapshots;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
-import java.util.Arrays;
 import java.util.List;
 
+import static org.hamcrest.MatcherAssert.assertThat;
 import static org.hamcrest.Matchers.containsInAnyOrder;
 
 /**
@@ -51,7 +52,7 @@ public class SnapshotUtilsTests extends ESTestCase {
     }
 
     private void assertIndexNameFiltering(String[] indices, String[] filter, IndicesOptions indicesOptions, String[] expected) {
-        List<String> indicesList = Arrays.asList(indices);
+        List<String> indicesList = ImmutableList.copyOf(indices);
         List<String> actual = SnapshotUtils.filterIndices(indicesList, filter, indicesOptions);
         assertThat(actual, containsInAnyOrder(expected));
     }
diff --git a/core/src/test/java/org/elasticsearch/stresstest/client/ClientFailover.java b/core/src/test/java/org/elasticsearch/stresstest/client/ClientFailover.java
index fc56f12..9466851 100644
--- a/core/src/test/java/org/elasticsearch/stresstest/client/ClientFailover.java
+++ b/core/src/test/java/org/elasticsearch/stresstest/client/ClientFailover.java
@@ -25,6 +25,7 @@ import org.elasticsearch.common.transport.InetSocketTransportAddress;
 import org.elasticsearch.node.Node;
 import org.elasticsearch.node.NodeBuilder;
 
+import java.net.InetAddress;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicLong;
@@ -42,9 +43,9 @@ public class ClientFailover {
         // TODO: what is this? a public static void main test?!?!
 
         final TransportClient client = TransportClient.builder().build()
-                .addTransportAddress(new InetSocketTransportAddress("localhost", 9300))
-                .addTransportAddress(new InetSocketTransportAddress("localhost", 9301))
-                .addTransportAddress(new InetSocketTransportAddress("localhost", 9302));
+                .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9300))
+                .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9301))
+                .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9302));
 
         final AtomicBoolean done = new AtomicBoolean();
         final AtomicLong indexed = new AtomicLong();
diff --git a/core/src/test/java/org/elasticsearch/stresstest/manyindices/ManyIndicesRemoteStressTest.java b/core/src/test/java/org/elasticsearch/stresstest/manyindices/ManyIndicesRemoteStressTest.java
index 591f5ce..1917fd6 100644
--- a/core/src/test/java/org/elasticsearch/stresstest/manyindices/ManyIndicesRemoteStressTest.java
+++ b/core/src/test/java/org/elasticsearch/stresstest/manyindices/ManyIndicesRemoteStressTest.java
@@ -28,6 +28,7 @@ import org.elasticsearch.common.transport.InetSocketTransportAddress;
 import org.elasticsearch.node.Node;
 import org.elasticsearch.node.NodeBuilder;
 
+import java.net.InetAddress;
 import java.util.Date;
 
 /**
@@ -49,7 +50,7 @@ public class ManyIndicesRemoteStressTest {
         Node node = null;
         // TODO: what is this? a public static void main test?!?!?!
         if (true) {
-            client = TransportClient.builder().settings(Settings.EMPTY).build().addTransportAddress(new InetSocketTransportAddress("localhost", 9300));
+            client = TransportClient.builder().settings(Settings.EMPTY).build().addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9300));
         } else {
             node = NodeBuilder.nodeBuilder().client(true).node();
             client = node.client();
diff --git a/core/src/test/java/org/elasticsearch/test/ESAllocationTestCase.java b/core/src/test/java/org/elasticsearch/test/ESAllocationTestCase.java
index a4f8f3b..991aea7 100644
--- a/core/src/test/java/org/elasticsearch/test/ESAllocationTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESAllocationTestCase.java
@@ -19,6 +19,7 @@
 package org.elasticsearch.test;
 
 import org.elasticsearch.Version;
+import org.elasticsearch.cluster.ClusterInfoService;
 import org.elasticsearch.cluster.ClusterModule;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.EmptyClusterInfoService;
@@ -63,7 +64,7 @@ public abstract class ESAllocationTestCase extends ESTestCase {
     }
 
     public static AllocationService createAllocationService(Settings settings, Random random) {
-        return createAllocationService(settings,  new NodeSettingsService(Settings.Builder.EMPTY_SETTINGS), random);
+        return createAllocationService(settings, new NodeSettingsService(Settings.Builder.EMPTY_SETTINGS), random);
     }
 
     public static AllocationService createAllocationService(Settings settings, NodeSettingsService nodeSettingsService, Random random) {
@@ -72,6 +73,13 @@ public abstract class ESAllocationTestCase extends ESTestCase {
                 new ShardsAllocators(settings, NoopGatewayAllocator.INSTANCE), EmptyClusterInfoService.INSTANCE);
     }
 
+    public static AllocationService createAllocationService(Settings settings, ClusterInfoService clusterInfoService) {
+        return new AllocationService(settings,
+                randomAllocationDeciders(settings, new NodeSettingsService(Settings.Builder.EMPTY_SETTINGS), getRandom()),
+                new ShardsAllocators(settings, NoopGatewayAllocator.INSTANCE), clusterInfoService);
+    }
+
+
 
     public static AllocationDeciders randomAllocationDeciders(Settings settings, NodeSettingsService nodeSettingsService, Random random) {
         final List<Class<? extends AllocationDecider>> defaultAllocationDeciders = ClusterModule.DEFAULT_ALLOCATION_DECIDERS;
diff --git a/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java b/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
index 80a60ff..798c66c 100644
--- a/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
@@ -29,9 +29,11 @@ import com.google.common.base.Predicate;
 import com.google.common.collect.Lists;
 
 import org.apache.http.impl.client.HttpClients;
+import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.routing.UnassignedInfo;
 import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
+import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.index.shard.MergeSchedulerConfig;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
@@ -117,6 +119,7 @@ import org.elasticsearch.search.SearchService;
 import org.elasticsearch.test.client.RandomizingClient;
 import org.elasticsearch.test.disruption.ServiceDisruptionScheme;
 import org.elasticsearch.test.rest.client.http.HttpRequestBuilder;
+import org.elasticsearch.transport.TransportModule;
 import org.hamcrest.Matchers;
 import org.joda.time.DateTimeZone;
 import org.junit.*;
@@ -124,7 +127,9 @@ import org.junit.*;
 import java.io.IOException;
 import java.io.InputStream;
 import java.lang.annotation.*;
+import java.net.InetAddress;
 import java.net.InetSocketAddress;
+import java.net.UnknownHostException;
 import java.nio.file.DirectoryStream;
 import java.nio.file.Files;
 import java.nio.file.Path;
@@ -1042,40 +1047,34 @@ public abstract class ESIntegTestCase extends ESTestCase {
      */
     protected void ensureClusterStateConsistency() throws IOException {
         if (cluster() != null) {
-            boolean getResolvedAddress = InetSocketTransportAddress.getResolveAddress();
-            try {
-                InetSocketTransportAddress.setResolveAddress(false);
-                ClusterState masterClusterState = client().admin().cluster().prepareState().all().get().getState();
-                byte[] masterClusterStateBytes = ClusterState.Builder.toBytes(masterClusterState);
+            ClusterState masterClusterState = client().admin().cluster().prepareState().all().get().getState();
+            byte[] masterClusterStateBytes = ClusterState.Builder.toBytes(masterClusterState);
+            // remove local node reference
+            masterClusterState = ClusterState.Builder.fromBytes(masterClusterStateBytes, null);
+            Map<String, Object> masterStateMap = convertToMap(masterClusterState);
+            int masterClusterStateSize = masterClusterState.toString().length();
+            String masterId = masterClusterState.nodes().masterNodeId();
+            for (Client client : cluster()) {
+                ClusterState localClusterState = client.admin().cluster().prepareState().all().setLocal(true).get().getState();
+                byte[] localClusterStateBytes = ClusterState.Builder.toBytes(localClusterState);
                 // remove local node reference
-                masterClusterState = ClusterState.Builder.fromBytes(masterClusterStateBytes, null);
-                Map<String, Object> masterStateMap = convertToMap(masterClusterState);
-                int masterClusterStateSize = masterClusterState.toString().length();
-                String masterId = masterClusterState.nodes().masterNodeId();
-                for (Client client : cluster()) {
-                    ClusterState localClusterState = client.admin().cluster().prepareState().all().setLocal(true).get().getState();
-                    byte[] localClusterStateBytes = ClusterState.Builder.toBytes(localClusterState);
-                    // remove local node reference
-                    localClusterState = ClusterState.Builder.fromBytes(localClusterStateBytes, null);
-                    final Map<String, Object> localStateMap = convertToMap(localClusterState);
-                    final int localClusterStateSize = localClusterState.toString().length();
-                    // Check that the non-master node has the same version of the cluster state as the master and that this node didn't disconnect from the master
-                    if (masterClusterState.version() == localClusterState.version() && localClusterState.nodes().nodes().containsKey(masterId)) {
-                        try {
-                            assertEquals("clusterstate UUID does not match", masterClusterState.stateUUID(), localClusterState.stateUUID());
-                            // We cannot compare serialization bytes since serialization order of maps is not guaranteed
-                            // but we can compare serialization sizes - they should be the same
-                            assertEquals("clusterstate size does not match", masterClusterStateSize, localClusterStateSize);
-                            // Compare JSON serialization
-                            assertNull("clusterstate JSON serialization does not match", differenceBetweenMapsIgnoringArrayOrder(masterStateMap, localStateMap));
-                        } catch (AssertionError error) {
-                            logger.error("Cluster state from master:\n{}\nLocal cluster state:\n{}", masterClusterState.toString(), localClusterState.toString());
-                            throw error;
-                        }
+                localClusterState = ClusterState.Builder.fromBytes(localClusterStateBytes, null);
+                final Map<String, Object> localStateMap = convertToMap(localClusterState);
+                final int localClusterStateSize = localClusterState.toString().length();
+                // Check that the non-master node has the same version of the cluster state as the master and that this node didn't disconnect from the master
+                if (masterClusterState.version() == localClusterState.version() && localClusterState.nodes().nodes().containsKey(masterId)) {
+                    try {
+                        assertEquals("clusterstate UUID does not match", masterClusterState.stateUUID(), localClusterState.stateUUID());
+                        // We cannot compare serialization bytes since serialization order of maps is not guaranteed
+                        // but we can compare serialization sizes - they should be the same
+                        assertEquals("clusterstate size does not match", masterClusterStateSize, localClusterStateSize);
+                        // Compare JSON serialization
+                        assertNull("clusterstate JSON serialization does not match", differenceBetweenMapsIgnoringArrayOrder(masterStateMap, localStateMap));
+                    } catch (AssertionError error) {
+                        logger.error("Cluster state from master:\n{}\nLocal cluster state:\n{}", masterClusterState.toString(), localClusterState.toString());
+                        throw error;
                     }
                 }
-            } finally {
-                InetSocketTransportAddress.setResolveAddress(getResolvedAddress);
             }
         }
 
@@ -1090,6 +1089,38 @@ public abstract class ESIntegTestCase extends ESTestCase {
         return ensureGreen(indices);
     }
 
+    protected void ensureStableCluster(int nodeCount) {
+        ensureStableCluster(nodeCount, TimeValue.timeValueSeconds(30));
+    }
+
+    protected void ensureStableCluster(int nodeCount, TimeValue timeValue) {
+        ensureStableCluster(nodeCount, timeValue, false, null);
+    }
+
+    protected void ensureStableCluster(int nodeCount, @Nullable String viaNode) {
+        ensureStableCluster(nodeCount, TimeValue.timeValueSeconds(30), false, viaNode);
+    }
+
+    protected void ensureStableCluster(int nodeCount, TimeValue timeValue, boolean local, @Nullable String viaNode) {
+        if (viaNode == null) {
+            viaNode = randomFrom(internalCluster().getNodeNames());
+        }
+        logger.debug("ensuring cluster is stable with [{}] nodes. access node: [{}]. timeout: [{}]", nodeCount, viaNode, timeValue);
+        ClusterHealthResponse clusterHealthResponse = client(viaNode).admin().cluster().prepareHealth()
+                .setWaitForEvents(Priority.LANGUID)
+                .setWaitForNodes(Integer.toString(nodeCount))
+                .setTimeout(timeValue)
+                .setLocal(local)
+                .setWaitForRelocatingShards(0)
+                .get();
+        if (clusterHealthResponse.isTimedOut()) {
+            ClusterStateResponse stateResponse = client(viaNode).admin().cluster().prepareState().get();
+            fail("failed to reach a stable cluster of [" + nodeCount + "] nodes. Tried via [" + viaNode + "]. last cluster state:\n"
+                    + stateResponse.getState().prettyPrint());
+        }
+        assertThat(clusterHealthResponse.isTimedOut(), is(false));
+    }
+
     /**
      * Syntactic sugar for:
      * <pre>
@@ -1553,49 +1584,51 @@ public abstract class ESIntegTestCase extends ESTestCase {
         assertThat(clearResponse.isSucceeded(), equalTo(true));
     }
 
-    private static ClusterScope getAnnotation(Class<?> clazz) {
+    private static <A extends Annotation> A getAnnotation(Class<?> clazz, Class<A> annotationClass) {
         if (clazz == Object.class || clazz == ESIntegTestCase.class) {
             return null;
         }
-        ClusterScope annotation = clazz.getAnnotation(ClusterScope.class);
+        A annotation = clazz.getAnnotation(annotationClass);
         if (annotation != null) {
             return annotation;
         }
-        return getAnnotation(clazz.getSuperclass());
+        return getAnnotation(clazz.getSuperclass(), annotationClass);
     }
 
+
+
     private Scope getCurrentClusterScope() {
         return getCurrentClusterScope(this.getClass());
     }
 
     private static Scope getCurrentClusterScope(Class<?> clazz) {
-        ClusterScope annotation = getAnnotation(clazz);
+        ClusterScope annotation = getAnnotation(clazz, ClusterScope.class);
         // if we are not annotated assume suite!
         return annotation == null ? Scope.SUITE : annotation.scope();
     }
 
     private int getNumDataNodes() {
-        ClusterScope annotation = getAnnotation(this.getClass());
+        ClusterScope annotation = getAnnotation(this.getClass(), ClusterScope.class);
         return annotation == null ? -1 : annotation.numDataNodes();
     }
 
     private int getMinNumDataNodes() {
-        ClusterScope annotation = getAnnotation(this.getClass());
+        ClusterScope annotation = getAnnotation(this.getClass(), ClusterScope.class);
         return annotation == null || annotation.minNumDataNodes() == -1 ? InternalTestCluster.DEFAULT_MIN_NUM_DATA_NODES : annotation.minNumDataNodes();
     }
 
     private int getMaxNumDataNodes() {
-        ClusterScope annotation = getAnnotation(this.getClass());
+        ClusterScope annotation = getAnnotation(this.getClass(), ClusterScope.class);
         return annotation == null || annotation.maxNumDataNodes() == -1 ? InternalTestCluster.DEFAULT_MAX_NUM_DATA_NODES : annotation.maxNumDataNodes();
     }
 
     private int getNumClientNodes() {
-        ClusterScope annotation = getAnnotation(this.getClass());
+        ClusterScope annotation = getAnnotation(this.getClass(), ClusterScope.class);
         return annotation == null ? InternalTestCluster.DEFAULT_NUM_CLIENT_NODES : annotation.numClientNodes();
     }
 
     private boolean randomDynamicTemplates() {
-        ClusterScope annotation = getAnnotation(this.getClass());
+        ClusterScope annotation = getAnnotation(this.getClass(), ClusterScope.class);
         return annotation == null || annotation.randomDynamicTemplates();
     }
 
@@ -1607,7 +1640,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
      * In other words subclasses must ensure this method is idempotent.
      */
     protected Settings nodeSettings(int nodeOrdinal) {
-        return settingsBuilder()
+        Settings.Builder builder = settingsBuilder()
                 // Default the watermarks to absurdly low to prevent the tests
                 // from failing on nodes without enough disk space
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, "1b")
@@ -1615,8 +1648,8 @@ public abstract class ESIntegTestCase extends ESTestCase {
                 .put("script.indexed", "on")
                 .put("script.inline", "on")
                         // wait short time for other active shards before actually deleting, default 30s not needed in tests
-                .put(IndicesStore.INDICES_STORE_DELETE_SHARD_TIMEOUT, new TimeValue(1, TimeUnit.SECONDS))
-                .build();
+                .put(IndicesStore.INDICES_STORE_DELETE_SHARD_TIMEOUT, new TimeValue(1, TimeUnit.SECONDS));
+        return builder.build();
     }
 
     /**
@@ -1629,7 +1662,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
         return Settings.EMPTY;
     }
 
-    private ExternalTestCluster buildExternalCluster(String clusterAddresses) {
+    private ExternalTestCluster buildExternalCluster(String clusterAddresses) throws UnknownHostException {
         String[] stringAddresses = clusterAddresses.split(",");
         TransportAddress[] transportAddresses = new TransportAddress[stringAddresses.length];
         int i = 0;
@@ -1639,7 +1672,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
                 throw new IllegalArgumentException("address [" + clusterAddresses + "] not valid");
             }
             try {
-                transportAddresses[i++] = new InetSocketTransportAddress(split[0], Integer.valueOf(split[1]));
+                transportAddresses[i++] = new InetSocketTransportAddress(InetAddress.getByName(split[0]), Integer.valueOf(split[1]));
             } catch (NumberFormatException e) {
                 throw new IllegalArgumentException("port is not valid, expected number but was [" + split[1] + "]");
             }
@@ -1693,7 +1726,18 @@ public abstract class ESIntegTestCase extends ESTestCase {
             minNumDataNodes = getMinNumDataNodes();
             maxNumDataNodes = getMaxNumDataNodes();
         }
-        return new InternalTestCluster(seed, createTempDir(), minNumDataNodes, maxNumDataNodes,
+        SuppressLocalMode noLocal = getAnnotation(this.getClass(), SuppressLocalMode.class);
+        SuppressNetworkMode noNetwork = getAnnotation(this.getClass(), SuppressNetworkMode.class);
+        String nodeMode = InternalTestCluster.configuredNodeMode();
+        if (noLocal != null && noNetwork != null) {
+            throw new IllegalStateException("Can't suppress both network and local mode");
+        } else if (noLocal != null){
+            nodeMode = "network";
+        } else if (noNetwork != null) {
+            nodeMode = "local";
+        }
+
+        return new InternalTestCluster(nodeMode, seed, createTempDir(), minNumDataNodes, maxNumDataNodes,
                 InternalTestCluster.clusterName(scope.name(), seed) + "-cluster", settingsSource, getNumClientNodes(),
                 InternalTestCluster.DEFAULT_ENABLE_HTTP_PIPELINING, nodePrefix);
     }
@@ -1715,7 +1759,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
      * return a random ratio in the interval <tt>[0..1]</tt>
      */
     protected double getPerTestTransportClientRatio() {
-        final ClusterScope annotation = getAnnotation(this.getClass());
+        final ClusterScope annotation = getAnnotation(this.getClass(), ClusterScope.class);
         double perTestRatio = -1;
         if (annotation != null) {
             perTestRatio = annotation.transportClientRatio();
@@ -1947,7 +1991,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
         TransportAddress publishAddress = randomFrom(nodes).getHttp().address().publishAddress();
         assertEquals(1, publishAddress.uniqueAddressTypeId());
         InetSocketAddress address = ((InetSocketTransportAddress) publishAddress).address();
-        return new HttpRequestBuilder(HttpClients.createDefault()).host(address.getHostName()).port(address.getPort());
+        return new HttpRequestBuilder(HttpClients.createDefault()).host(NetworkAddress.formatAddress(address.getAddress())).port(address.getPort());
     }
 
     /**
@@ -1973,4 +2017,19 @@ public abstract class ESIntegTestCase extends ESTestCase {
     @Inherited
     public @interface SuiteScopeTestCase {
     }
+
+    /**
+     * If used the test will never run in local mode.
+     */
+    @Retention(RetentionPolicy.RUNTIME)
+    @Inherited
+    public @interface SuppressLocalMode {}
+
+    /**
+     * If used the test will never run in network mode
+     */
+    @Retention(RetentionPolicy.RUNTIME)
+    @Inherited
+    public @interface SuppressNetworkMode {}
+
 }
diff --git a/core/src/test/java/org/elasticsearch/test/ESSingleNodeTestCase.java b/core/src/test/java/org/elasticsearch/test/ESSingleNodeTestCase.java
index 4b452d3..eb1581b 100644
--- a/core/src/test/java/org/elasticsearch/test/ESSingleNodeTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESSingleNodeTestCase.java
@@ -35,7 +35,6 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.concurrent.EsExecutors;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.node.Node;
@@ -46,12 +45,9 @@ import org.elasticsearch.threadpool.ThreadPool;
 import org.junit.After;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
-import org.junit.Ignore;
 
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.lessThanOrEqualTo;
+import static org.hamcrest.Matchers.*;
 
 /**
  * A test that keep a singleton node started for all tests that can be used to get
@@ -225,7 +221,7 @@ public abstract class ESSingleNodeTestCase extends ESTestCase {
         BigArrays bigArrays = indexService.injector().getInstance(BigArrays.class);
         ThreadPool threadPool = indexService.injector().getInstance(ThreadPool.class);
         PageCacheRecycler pageCacheRecycler = indexService.injector().getInstance(PageCacheRecycler.class);
-        return new TestSearchContext(threadPool, pageCacheRecycler, bigArrays, indexService, indexService.cache().query(), indexService.fieldData());
+        return new TestSearchContext(threadPool, pageCacheRecycler, bigArrays, indexService);
     }
 
     /**
diff --git a/core/src/test/java/org/elasticsearch/test/InternalTestCluster.java b/core/src/test/java/org/elasticsearch/test/InternalTestCluster.java
index 31ddd96..f24ac1e 100644
--- a/core/src/test/java/org/elasticsearch/test/InternalTestCluster.java
+++ b/core/src/test/java/org/elasticsearch/test/InternalTestCluster.java
@@ -192,8 +192,6 @@ public final class InternalTestCluster extends TestCluster {
 
     static final boolean DEFAULT_ENABLE_HTTP_PIPELINING = true;
 
-    public static final String NODE_MODE = nodeMode();
-
     /* sorted map to make traverse order reproducible, concurrent since we do checks on it not within a sync block */
     private final NavigableMap<String, NodeAndClient> nodes = new TreeMap<>();
 
@@ -227,16 +225,16 @@ public final class InternalTestCluster extends TestCluster {
     private final Path baseDir;
 
     private ServiceDisruptionScheme activeDisruptionScheme;
+    private String nodeMode;
 
-    public InternalTestCluster(long clusterSeed, Path baseDir, int minNumDataNodes, int maxNumDataNodes, String clusterName, int numClientNodes,
-                               boolean enableHttpPipelining, String nodePrefix) {
-        this(clusterSeed, baseDir, minNumDataNodes, maxNumDataNodes, clusterName, DEFAULT_SETTINGS_SOURCE, numClientNodes, enableHttpPipelining, nodePrefix);
-    }
-
-    public InternalTestCluster(long clusterSeed, Path baseDir,
+    public InternalTestCluster(String nodeMode, long clusterSeed, Path baseDir,
                                int minNumDataNodes, int maxNumDataNodes, String clusterName, SettingsSource settingsSource, int numClientNodes,
                                boolean enableHttpPipelining, String nodePrefix) {
         super(clusterSeed);
+        if ("network".equals(nodeMode) == false && "local".equals(nodeMode) == false) {
+            throw new IllegalArgumentException("Unknown nodeMode: " + nodeMode);
+        }
+        this.nodeMode = nodeMode;
         this.baseDir = baseDir;
         this.clusterName = clusterName;
         if (minNumDataNodes < 0 || maxNumDataNodes < 0) {
@@ -300,7 +298,7 @@ public final class InternalTestCluster extends TestCluster {
         builder.put("transport.tcp.port", BASE_PORT + "-" + (BASE_PORT + 100));
         builder.put("http.port", BASE_PORT + 101 + "-" + (BASE_PORT + 200));
         builder.put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true);
-        builder.put("node.mode", NODE_MODE);
+        builder.put("node.mode", nodeMode);
         builder.put("http.pipelining", enableHttpPipelining);
         if (Strings.hasLength(System.getProperty("es.logger.level"))) {
             builder.put("logger.level", System.getProperty("es.logger.level"));
@@ -327,7 +325,7 @@ public final class InternalTestCluster extends TestCluster {
         executor = EsExecutors.newCached("test runner", 0, TimeUnit.SECONDS, EsExecutors.daemonThreadFactory("test_" + clusterName));
     }
 
-    public static String nodeMode() {
+    public static String configuredNodeMode() {
         Builder builder = Settings.builder();
         if (Strings.isEmpty(System.getProperty("es.node.mode")) && Strings.isEmpty(System.getProperty("es.node.local"))) {
             return "local"; // default if nothing is specified
@@ -354,11 +352,8 @@ public final class InternalTestCluster extends TestCluster {
         return nodes.keySet().toArray(Strings.EMPTY_ARRAY);
     }
 
-    private static boolean isLocalTransportConfigured() {
-        if ("local".equals(System.getProperty("es.node.mode", "network"))) {
-            return true;
-        }
-        return Boolean.parseBoolean(System.getProperty("es.node.local", "false"));
+    private boolean isLocalTransportConfigured() {
+        return "local".equals(nodeMode);
     }
 
     private Settings getSettings(int nodeOrdinal, long nodeSeed, Settings others) {
@@ -378,7 +373,7 @@ public final class InternalTestCluster extends TestCluster {
         return builder.build();
     }
 
-    private static Settings getRandomNodeSettings(long seed) {
+    private Settings getRandomNodeSettings(long seed) {
         Random random = new Random(seed);
         Builder builder = Settings.settingsBuilder()
                 .put(SETTING_CLUSTER_NODE_SEED, seed);
@@ -782,6 +777,10 @@ public final class InternalTestCluster extends TestCluster {
         }
     }
 
+    public String getNodeMode() {
+        return nodeMode;
+    }
+
     private final class NodeAndClient implements Closeable {
         private Node node;
         private Client nodeClient;
@@ -844,7 +843,7 @@ public final class InternalTestCluster extends TestCluster {
             /* no sniff client for now - doesn't work will all tests since it might throw NoNodeAvailableException if nodes are shut down.
              * we first need support of transportClientRatio as annotations or so
              */
-            return transportClient = new TransportClientFactory(false, settingsSource.transportClient(), baseDir).client(node, clusterName);
+            return transportClient = new TransportClientFactory(false, settingsSource.transportClient(), baseDir, nodeMode).client(node, clusterName);
         }
 
         void resetClient() throws IOException {
@@ -901,11 +900,13 @@ public final class InternalTestCluster extends TestCluster {
         private final boolean sniff;
         private final Settings settings;
         private final Path baseDir;
+        private final String nodeMode;
 
-        TransportClientFactory(boolean sniff, Settings settings, Path baseDir) {
+        TransportClientFactory(boolean sniff, Settings settings, Path baseDir, String nodeMode) {
             this.sniff = sniff;
             this.settings = settings != null ? settings : Settings.EMPTY;
             this.baseDir = baseDir;
+            this.nodeMode = nodeMode;
         }
 
         public Client client(Node node, String clusterName) {
@@ -916,7 +917,7 @@ public final class InternalTestCluster extends TestCluster {
                     .put("path.home", baseDir)
                     .put("name", TRANSPORT_CLIENT_PREFIX + node.settings().get("name"))
                     .put(ClusterName.SETTING, clusterName).put("client.transport.sniff", sniff)
-                    .put("node.mode", nodeSettings.get("node.mode", NODE_MODE))
+                    .put("node.mode", nodeSettings.get("node.mode", nodeMode))
                     .put("node.local", nodeSettings.get("node.local", ""))
                     .put("logger.prefix", nodeSettings.get("logger.prefix", ""))
                     .put("logger.level", nodeSettings.get("logger.level", "INFO"))
diff --git a/core/src/test/java/org/elasticsearch/test/TestCluster.java b/core/src/test/java/org/elasticsearch/test/TestCluster.java
index 593ceee..4f95fc9 100644
--- a/core/src/test/java/org/elasticsearch/test/TestCluster.java
+++ b/core/src/test/java/org/elasticsearch/test/TestCluster.java
@@ -211,4 +211,6 @@ public abstract class TestCluster implements Iterable<Client>, Closeable {
      * Returns the cluster name
      */
     public abstract String getClusterName();
+
+
 }
diff --git a/core/src/test/java/org/elasticsearch/test/TestSearchContext.java b/core/src/test/java/org/elasticsearch/test/TestSearchContext.java
index 4527fb5..fa8aeed 100644
--- a/core/src/test/java/org/elasticsearch/test/TestSearchContext.java
+++ b/core/src/test/java/org/elasticsearch/test/TestSearchContext.java
@@ -23,18 +23,19 @@ import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.util.Counter;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
-import org.elasticsearch.common.*;
+import org.elasticsearch.common.HasContext;
+import org.elasticsearch.common.HasContextAndHeaders;
+import org.elasticsearch.common.HasHeaders;
+import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.analysis.AnalysisService;
 import org.elasticsearch.index.cache.bitset.BitsetFilterCache;
-import org.elasticsearch.index.cache.query.QueryCache;
 import org.elasticsearch.index.fielddata.IndexFieldDataService;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MapperService;
@@ -44,7 +45,6 @@ import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.search.Scroll;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
@@ -56,6 +56,7 @@ import org.elasticsearch.search.fetch.script.ScriptFieldsContext;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 import org.elasticsearch.search.highlight.SearchContextHighlight;
 import org.elasticsearch.search.internal.ContextIndexSearcher;
+import org.elasticsearch.search.internal.ScrollContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.internal.ShardSearchRequest;
 import org.elasticsearch.search.lookup.SearchLookup;
@@ -65,7 +66,11 @@ import org.elasticsearch.search.scan.ScanContext;
 import org.elasticsearch.search.suggest.SuggestionSearchContext;
 import org.elasticsearch.threadpool.ThreadPool;
 
-import java.util.*;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
 
 public class TestSearchContext extends SearchContext {
 
@@ -76,6 +81,7 @@ public class TestSearchContext extends SearchContext {
     final BitsetFilterCache fixedBitSetFilterCache;
     final ThreadPool threadPool;
     final Map<Class<?>, Collector> queryCollectors = new HashMap<>();
+    final IndexShard indexShard;
 
     ContextIndexSearcher searcher;
     int size;
@@ -86,7 +92,7 @@ public class TestSearchContext extends SearchContext {
     private final long originNanoTime = System.nanoTime();
     private final Map<String, FetchSubPhaseContext> subPhaseContexts = new HashMap<>();
 
-    public TestSearchContext(ThreadPool threadPool,PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, IndexService indexService, QueryCache filterCache, IndexFieldDataService indexFieldDataService) {
+    public TestSearchContext(ThreadPool threadPool,PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, IndexService indexService) {
         super(ParseFieldMatcher.STRICT);
         this.pageCacheRecycler = pageCacheRecycler;
         this.bigArrays = bigArrays.withCircuitBreaking();
@@ -94,6 +100,7 @@ public class TestSearchContext extends SearchContext {
         this.indexFieldDataService = indexService.fieldData();
         this.fixedBitSetFilterCache = indexService.bitsetFilterCache();
         this.threadPool = threadPool;
+        this.indexShard = indexService.shard(0);
     }
 
     public TestSearchContext() {
@@ -104,6 +111,7 @@ public class TestSearchContext extends SearchContext {
         this.indexFieldDataService = null;
         this.threadPool = null;
         this.fixedBitSetFilterCache = null;
+        this.indexShard = null;
     }
 
     public void setTypes(String... types) {
@@ -185,13 +193,13 @@ public class TestSearchContext extends SearchContext {
     }
 
     @Override
-    public Scroll scroll() {
+    public ScrollContext scrollContext() {
         return null;
     }
 
     @Override
-    public SearchContext scroll(Scroll scroll) {
-        return null;
+    public SearchContext scrollContext(ScrollContext scrollContext) {
+        throw new UnsupportedOperationException();
     }
 
     @Override
@@ -282,7 +290,7 @@ public class TestSearchContext extends SearchContext {
 
     @Override
     public IndexShard indexShard() {
-        return null;
+        return indexShard;
     }
 
     @Override
@@ -517,15 +525,6 @@ public class TestSearchContext extends SearchContext {
     }
 
     @Override
-    public void lastEmittedDoc(ScoreDoc doc) {
-    }
-
-    @Override
-    public ScoreDoc lastEmittedDoc() {
-        return null;
-    }
-
-    @Override
     public SearchLookup lookup() {
         return new SearchLookup(mapperService(), fieldData(), null);
     }
diff --git a/core/src/test/java/org/elasticsearch/test/VersionUtils.java b/core/src/test/java/org/elasticsearch/test/VersionUtils.java
index a3efe63..ebdad00 100644
--- a/core/src/test/java/org/elasticsearch/test/VersionUtils.java
+++ b/core/src/test/java/org/elasticsearch/test/VersionUtils.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.test;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.Version;
 
 import java.lang.reflect.Field;
@@ -52,21 +53,27 @@ public class VersionUtils {
         }
         List<Integer> idList = new ArrayList<>(ids);
         Collections.sort(idList);
-        List<Version> version = new ArrayList<>();
+        ImmutableList.Builder<Version> version = ImmutableList.builder();
         for (Integer integer : idList) {
             version.add(Version.fromId(integer));
         }
-        SORTED_VERSIONS = Collections.unmodifiableList(version);
+        SORTED_VERSIONS = version.build();
     }
 
     /** Returns immutable list of all known versions. */
     public static List<Version> allVersions() {
         return Collections.unmodifiableList(SORTED_VERSIONS);
     }
-    
+
+    public static Version getPreviousVersion(Version version) {
+        int index = SORTED_VERSIONS.indexOf(version);
+        assert index > 0;
+        return SORTED_VERSIONS.get(index - 1);
+    }
+
     /** Returns the {@link Version} before the {@link Version#CURRENT} */
     public static Version getPreviousVersion() {
-        Version version = SORTED_VERSIONS.get(SORTED_VERSIONS.size() - 2);
+        Version version = getPreviousVersion(Version.CURRENT);
         assert version.before(Version.CURRENT);
         return version;
     }
diff --git a/core/src/test/java/org/elasticsearch/test/discovery/ClusterDiscoveryConfiguration.java b/core/src/test/java/org/elasticsearch/test/discovery/ClusterDiscoveryConfiguration.java
index a170cd1..b7d0c2d 100644
--- a/core/src/test/java/org/elasticsearch/test/discovery/ClusterDiscoveryConfiguration.java
+++ b/core/src/test/java/org/elasticsearch/test/discovery/ClusterDiscoveryConfiguration.java
@@ -21,6 +21,7 @@ package org.elasticsearch.test.discovery;
 import com.carrotsearch.randomizedtesting.RandomizedTest;
 import com.google.common.primitives.Ints;
 import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.common.SuppressForbidden;
 import org.elasticsearch.common.network.NetworkUtils;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.test.InternalTestCluster;
@@ -28,6 +29,8 @@ import org.elasticsearch.test.SettingsSource;
 import org.elasticsearch.transport.local.LocalTransport;
 
 import java.io.IOException;
+import java.net.Inet4Address;
+import java.net.InetAddress;
 import java.net.InetSocketAddress;
 import java.net.ServerSocket;
 import java.util.HashSet;
@@ -36,6 +39,7 @@ import java.util.Set;
 public class ClusterDiscoveryConfiguration extends SettingsSource {
 
     static Settings DEFAULT_NODE_SETTINGS = Settings.settingsBuilder().put("discovery.type", "zen").build();
+    private static final String IP_ADDR = "127.0.0.1";
 
     final int numOfNodes;
     final Settings nodeSettings;
@@ -64,20 +68,11 @@ public class ClusterDiscoveryConfiguration extends SettingsSource {
 
         private final int[] unicastHostOrdinals;
         private final int[] unicastHostPorts;
-        private final boolean localMode;
-
-        public UnicastZen(int numOfNodes) {
-            this(numOfNodes, numOfNodes);
-        }
 
         public UnicastZen(int numOfNodes, Settings extraSettings) {
             this(numOfNodes, numOfNodes, extraSettings);
         }
 
-        public UnicastZen(int numOfNodes, int numOfUnicastHosts) {
-            this(numOfNodes, numOfUnicastHosts, Settings.EMPTY);
-        }
-
         public UnicastZen(int numOfNodes, int numOfUnicastHosts, Settings extraSettings) {
             super(numOfNodes, extraSettings);
             if (numOfUnicastHosts == numOfNodes) {
@@ -92,9 +87,8 @@ public class ClusterDiscoveryConfiguration extends SettingsSource {
                 }
                 unicastHostOrdinals = Ints.toArray(ordinals);
             }
-            this.localMode = nodeSettings.get("node.mode", InternalTestCluster.NODE_MODE).equals("local");
-            this.unicastHostPorts = localMode ? new int[0] : unicastHostPorts(numOfNodes);
-            assert localMode || unicastHostOrdinals.length <= unicastHostPorts.length;
+            this.unicastHostPorts = unicastHostPorts(numOfNodes);
+            assert unicastHostOrdinals.length <= unicastHostPorts.length;
         }
 
         public UnicastZen(int numOfNodes, int[] unicastHostOrdinals) {
@@ -104,9 +98,8 @@ public class ClusterDiscoveryConfiguration extends SettingsSource {
         public UnicastZen(int numOfNodes, Settings extraSettings, int[] unicastHostOrdinals) {
             super(numOfNodes, extraSettings);
             this.unicastHostOrdinals = unicastHostOrdinals;
-            this.localMode = nodeSettings.get("node.mode", InternalTestCluster.NODE_MODE).equals("local");
-            this.unicastHostPorts = localMode ? new int[0] : unicastHostPorts(numOfNodes);
-            assert localMode || unicastHostOrdinals.length <= unicastHostPorts.length;
+            this.unicastHostPorts = unicastHostPorts(numOfNodes);
+            assert unicastHostOrdinals.length <= unicastHostPorts.length;
         }
 
         private static int calcBasePort() {
@@ -115,29 +108,27 @@ public class ClusterDiscoveryConfiguration extends SettingsSource {
 
         @Override
         public Settings node(int nodeOrdinal) {
-            Settings.Builder builder = Settings.builder()
-                    .put("discovery.zen.ping.multicast.enabled", false);
+            Settings.Builder builder = Settings.builder();
 
             String[] unicastHosts = new String[unicastHostOrdinals.length];
-            if (localMode) {
-                builder.put(LocalTransport.TRANSPORT_LOCAL_ADDRESS, "node_" + nodeOrdinal);
-                for (int i = 0; i < unicastHosts.length; i++) {
-                    unicastHosts[i] = "node_" + unicastHostOrdinals[i];
-                }
-            } else if (nodeOrdinal >= unicastHostPorts.length) {
+            if (nodeOrdinal >= unicastHostPorts.length) {
                 throw new ElasticsearchException("nodeOrdinal [" + nodeOrdinal + "] is greater than the number unicast ports [" + unicastHostPorts.length + "]");
             } else {
                 // we need to pin the node port & host so we'd know where to point things
                 builder.put("transport.tcp.port", unicastHostPorts[nodeOrdinal]);
-                builder.put("transport.host", "localhost");
+                builder.put("transport.host", IP_ADDR); // only bind on one IF we use v4 here by default
+                builder.put("transport.bind_host", IP_ADDR);
+                builder.put("transport.publish_host", IP_ADDR);
+                builder.put("http.enabled", false);
                 for (int i = 0; i < unicastHostOrdinals.length; i++) {
-                    unicastHosts[i] = "localhost:" + (unicastHostPorts[unicastHostOrdinals[i]]);
+                    unicastHosts[i] = IP_ADDR + ":" + (unicastHostPorts[unicastHostOrdinals[i]]);
                 }
             }
             builder.putArray("discovery.zen.ping.unicast.hosts", unicastHosts);
             return builder.put(super.node(nodeOrdinal)).build();
         }
 
+        @SuppressForbidden(reason = "we know we pass a IP address")
         protected synchronized static int[] unicastHostPorts(int numHosts) {
             int[] unicastHostPorts = new int[numHosts];
 
@@ -150,7 +141,7 @@ public class ClusterDiscoveryConfiguration extends SettingsSource {
                     try (ServerSocket serverSocket = new ServerSocket()) {
                         // Set SO_REUSEADDR as we may bind here and not be able to reuse the address immediately without it.
                         serverSocket.setReuseAddress(NetworkUtils.defaultReuseAddress());
-                        serverSocket.bind(new InetSocketAddress("127.0.0.1", nextPort));
+                        serverSocket.bind(new InetSocketAddress(IP_ADDR, nextPort));
                         // bind was a success
                         foundPortInRange = true;
                         unicastHostPorts[i] = nextPort;
diff --git a/core/src/test/java/org/elasticsearch/test/disruption/NetworkPartition.java b/core/src/test/java/org/elasticsearch/test/disruption/NetworkPartition.java
index 88bcb90..174e83e 100644
--- a/core/src/test/java/org/elasticsearch/test/disruption/NetworkPartition.java
+++ b/core/src/test/java/org/elasticsearch/test/disruption/NetworkPartition.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.test.disruption;
 
+import com.google.common.collect.ImmutableList;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
@@ -26,8 +27,6 @@ import org.elasticsearch.test.InternalTestCluster;
 import org.elasticsearch.test.transport.MockTransportService;
 import org.elasticsearch.transport.TransportService;
 
-import java.util.Collection;
-import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Random;
@@ -69,15 +68,15 @@ public abstract class NetworkPartition implements ServiceDisruptionScheme {
     }
 
 
-    public Collection<String> getNodesSideOne() {
-        return Collections.unmodifiableCollection(nodesSideOne);
+    public List<String> getNodesSideOne() {
+        return ImmutableList.copyOf(nodesSideOne);
     }
 
-    public Collection<String> getNodesSideTwo() {
-        return Collections.unmodifiableCollection(nodesSideTwo);
+    public List<String> getNodesSideTwo() {
+        return ImmutableList.copyOf(nodesSideTwo);
     }
 
-    public Collection<String> getMajoritySide() {
+    public List<String> getMajoritySide() {
         if (nodesSideOne.size() >= nodesSideTwo.size()) {
             return getNodesSideOne();
         } else {
@@ -85,7 +84,7 @@ public abstract class NetworkPartition implements ServiceDisruptionScheme {
         }
     }
 
-    public Collection<String> getMinoritySide() {
+    public List<String> getMinoritySide() {
         if (nodesSideOne.size() >= nodesSideTwo.size()) {
             return getNodesSideTwo();
         } else {
diff --git a/core/src/test/java/org/elasticsearch/test/rest/client/RestClient.java b/core/src/test/java/org/elasticsearch/test/rest/client/RestClient.java
index 97854ff..f18f920 100644
--- a/core/src/test/java/org/elasticsearch/test/rest/client/RestClient.java
+++ b/core/src/test/java/org/elasticsearch/test/rest/client/RestClient.java
@@ -39,6 +39,7 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.io.PathUtils;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.test.rest.client.http.HttpRequestBuilder;
 import org.elasticsearch.test.rest.client.http.HttpResponse;
@@ -247,7 +248,7 @@ public class RestClient implements Closeable {
         return new HttpRequestBuilder(httpClient)
                 .addHeaders(headers)
                 .protocol(protocol)
-                .host(address.getHostName()).port(address.getPort());
+                .host(NetworkAddress.formatAddress(address.getAddress())).port(address.getPort());
     }
 
     protected HttpRequestBuilder httpRequestBuilder() {
diff --git a/core/src/test/java/org/elasticsearch/test/rest/client/http/HttpRequestBuilder.java b/core/src/test/java/org/elasticsearch/test/rest/client/http/HttpRequestBuilder.java
index 09f79a0..7791cda 100644
--- a/core/src/test/java/org/elasticsearch/test/rest/client/http/HttpRequestBuilder.java
+++ b/core/src/test/java/org/elasticsearch/test/rest/client/http/HttpRequestBuilder.java
@@ -27,6 +27,7 @@ import org.elasticsearch.client.support.Headers;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.common.transport.InetSocketTransportAddress;
 import org.elasticsearch.http.HttpServerTransport;
 
@@ -77,7 +78,7 @@ public class HttpRequestBuilder {
 
     public HttpRequestBuilder httpTransport(HttpServerTransport httpServerTransport) {
         InetSocketTransportAddress transportAddress = (InetSocketTransportAddress) httpServerTransport.boundAddress().publishAddress();
-        return host(transportAddress.address().getHostName()).port(transportAddress.address().getPort());
+        return host(NetworkAddress.formatAddress(transportAddress.address().getAddress())).port(transportAddress.address().getPort());
     }
 
     public HttpRequestBuilder port(int port) {
diff --git a/core/src/test/java/org/elasticsearch/test/rest/section/ApiCallSection.java b/core/src/test/java/org/elasticsearch/test/rest/section/ApiCallSection.java
index ae53366..2a49cd4 100644
--- a/core/src/test/java/org/elasticsearch/test/rest/section/ApiCallSection.java
+++ b/core/src/test/java/org/elasticsearch/test/rest/section/ApiCallSection.java
@@ -19,11 +19,11 @@
 package org.elasticsearch.test.rest.section;
 
 import com.google.common.base.Joiner;
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 
@@ -58,7 +58,7 @@ public class ApiCallSection {
     }
 
     public List<Map<String, Object>> getBodies() {
-        return Collections.unmodifiableList(bodies);
+        return ImmutableList.copyOf(bodies);
     }
 
     public void addBody(Map<String, Object> body) {
diff --git a/core/src/test/java/org/elasticsearch/test/test/InternalTestClusterTests.java b/core/src/test/java/org/elasticsearch/test/test/InternalTestClusterTests.java
index 97854f4..cf5c00e 100644
--- a/core/src/test/java/org/elasticsearch/test/test/InternalTestClusterTests.java
+++ b/core/src/test/java/org/elasticsearch/test/test/InternalTestClusterTests.java
@@ -55,8 +55,8 @@ public class InternalTestClusterTests extends ESTestCase {
         String nodePrefix = randomRealisticUnicodeOfCodepointLengthBetween(1, 10);
 
         Path baseDir = createTempDir();
-        InternalTestCluster cluster0 = new InternalTestCluster(clusterSeed, baseDir, minNumDataNodes, maxNumDataNodes, clusterName, settingsSource, numClientNodes, enableHttpPipelining, nodePrefix);
-        InternalTestCluster cluster1 = new InternalTestCluster(clusterSeed, baseDir, minNumDataNodes, maxNumDataNodes, clusterName, settingsSource, numClientNodes, enableHttpPipelining, nodePrefix);
+        InternalTestCluster cluster0 = new InternalTestCluster("local", clusterSeed, baseDir, minNumDataNodes, maxNumDataNodes, clusterName, settingsSource, numClientNodes, enableHttpPipelining, nodePrefix);
+        InternalTestCluster cluster1 = new InternalTestCluster("local", clusterSeed, baseDir, minNumDataNodes, maxNumDataNodes, clusterName, settingsSource, numClientNodes, enableHttpPipelining, nodePrefix);
         assertClusters(cluster0, cluster1, true);
 
     }
@@ -99,8 +99,8 @@ public class InternalTestClusterTests extends ESTestCase {
         String nodePrefix = "foobar";
 
         Path baseDir = createTempDir();
-        InternalTestCluster cluster0 = new InternalTestCluster(clusterSeed, baseDir, minNumDataNodes, maxNumDataNodes, clusterName1, settingsSource, numClientNodes, enableHttpPipelining, nodePrefix);
-        InternalTestCluster cluster1 = new InternalTestCluster(clusterSeed, baseDir, minNumDataNodes, maxNumDataNodes, clusterName2, settingsSource, numClientNodes, enableHttpPipelining, nodePrefix);
+        InternalTestCluster cluster0 = new InternalTestCluster("local", clusterSeed, baseDir, minNumDataNodes, maxNumDataNodes, clusterName1, settingsSource, numClientNodes, enableHttpPipelining, nodePrefix);
+        InternalTestCluster cluster1 = new InternalTestCluster("local", clusterSeed, baseDir, minNumDataNodes, maxNumDataNodes, clusterName2, settingsSource, numClientNodes, enableHttpPipelining, nodePrefix);
 
         assertClusters(cluster0, cluster1, false);
         long seed = randomLong();
@@ -124,7 +124,6 @@ public class InternalTestClusterTests extends ESTestCase {
             cluster0.afterTest();
             cluster1.afterTest();
         } finally {
-
             IOUtils.close(cluster0, cluster1);
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/test/transport/CapturingTransport.java b/core/src/test/java/org/elasticsearch/test/transport/CapturingTransport.java
index 8cb1f62..476b89a 100644
--- a/core/src/test/java/org/elasticsearch/test/transport/CapturingTransport.java
+++ b/core/src/test/java/org/elasticsearch/test/transport/CapturingTransport.java
@@ -28,10 +28,7 @@ import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.transport.*;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 import java.util.concurrent.BlockingQueue;
 
 /** A transport class that doesn't send anything but rather captures all requests for inspection from tests */
@@ -114,7 +111,8 @@ public class CapturingTransport implements Transport {
     }
 
     @Override
-    public TransportAddress[] addressesFromString(String address) throws Exception {
+    public TransportAddress[] addressesFromString(String address, int perAddressLimit) throws Exception {
+        // WTF
         return new TransportAddress[0];
     }
 
@@ -177,4 +175,9 @@ public class CapturingTransport implements Transport {
     public void close() {
 
     }
+
+    @Override
+    public List<String> getLocalAddresses() {
+        return Collections.EMPTY_LIST;
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/test/transport/MockTransportService.java b/core/src/test/java/org/elasticsearch/test/transport/MockTransportService.java
index 0aa2e34..f5da216 100644
--- a/core/src/test/java/org/elasticsearch/test/transport/MockTransportService.java
+++ b/core/src/test/java/org/elasticsearch/test/transport/MockTransportService.java
@@ -362,8 +362,8 @@ public class MockTransportService extends TransportService {
         }
 
         @Override
-        public TransportAddress[] addressesFromString(String address) throws Exception {
-            return transport.addressesFromString(address);
+        public TransportAddress[] addressesFromString(String address, int perAddressLimit) throws Exception {
+            return transport.addressesFromString(address, perAddressLimit);
         }
 
         @Override
@@ -402,6 +402,11 @@ public class MockTransportService extends TransportService {
         }
 
         @Override
+        public List<String> getLocalAddresses() {
+            return transport.getLocalAddresses();
+        }
+
+        @Override
         public Lifecycle.State lifecycleState() {
             return transport.lifecycleState();
         }
diff --git a/core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java b/core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java
index d0f1fa2..bf79185 100644
--- a/core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java
+++ b/core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java
@@ -23,7 +23,6 @@ import com.google.common.collect.Sets;
 import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;
 import org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse;
 import org.elasticsearch.action.index.IndexRequestBuilder;
-import org.elasticsearch.common.network.MulticastChannel;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -112,8 +111,7 @@ public class SimpleThreadPoolIT extends ESIntegTestCase {
         for (String threadName : threadNames) {
             // ignore some shared threads we know that are created within the same VM, like the shared discovery one
             // or the ones that are occasionally come up from ESSingleNodeTestCase
-            if (threadName.contains("[" + MulticastChannel.SHARED_CHANNEL_NAME + "]")
-                    || threadName.contains("[" + ESSingleNodeTestCase.nodeName() + "]")
+            if (threadName.contains("[" + ESSingleNodeTestCase.nodeName() + "]")
                     || threadName.contains("Keep-Alive-Timer")) {
                 continue;
             }
diff --git a/core/src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java b/core/src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java
index 3226662..be33df3 100644
--- a/core/src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java
+++ b/core/src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java
@@ -99,4 +99,23 @@ public class ThreadPoolSerializationTests extends ESTestCase {
         assertThat(threadPool.info("index").getQueueSize(), is(nullValue()));
         terminate(threadPool);
     }
+
+    @Test
+    public void testThatToXContentWritesInteger() throws Exception {
+        ThreadPool.Info info = new ThreadPool.Info("foo", "search", 1, 10, TimeValue.timeValueMillis(3000), SizeValue.parseSizeValue("1k"));
+        XContentBuilder builder = jsonBuilder();
+        builder.startObject();
+        info.toXContent(builder, ToXContent.EMPTY_PARAMS);
+        builder.endObject();
+
+        BytesReference bytesReference = builder.bytes();
+        Map<String, Object> map;
+        try (XContentParser parser = XContentFactory.xContent(bytesReference).createParser(bytesReference)) {
+            map = parser.map();
+        }
+        assertThat(map, hasKey("foo"));
+        map = (Map<String, Object>) map.get("foo");
+        assertThat(map, hasKey("queue_size"));
+        assertThat(map.get("queue_size").toString(), is("1000"));
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java b/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
index 811f6e5..566d1d8 100644
--- a/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
+++ b/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
@@ -42,7 +42,6 @@ import org.elasticsearch.client.FilterClient;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.Module;
-import org.elasticsearch.common.inject.PreProcessModule;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.http.HttpServerTransport;
@@ -401,22 +400,18 @@ public class ContextAndHeaderTransportIT extends ESIntegTestCase {
         public Collection<Module> nodeModules() {
             return Collections.<Module>singletonList(new ActionLoggingModule());
         }
-    }
-
-    public static class ActionLoggingModule extends AbstractModule implements PreProcessModule {
 
+        public void onModule(ActionModule module) {
+            module.registerFilter(LoggingFilter.class);
+        }
+    }
 
+    public static class ActionLoggingModule extends AbstractModule {
         @Override
         protected void configure() {
             bind(LoggingFilter.class).asEagerSingleton();
         }
 
-        @Override
-        public void processModule(Module module) {
-            if (module instanceof ActionModule) {
-                ((ActionModule)module).registerFilter(LoggingFilter.class);
-            }
-        }
     }
 
     public static class LoggingFilter extends ActionFilter.Simple {
diff --git a/core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java b/core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java
index 3ffc945..5f80aa3 100644
--- a/core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java
+++ b/core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.transport;
 
 import com.google.common.base.Charsets;
+
 import org.elasticsearch.Version;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.network.NetworkService;
@@ -39,6 +40,7 @@ import org.junit.Test;
 
 import java.io.BufferedReader;
 import java.io.InputStreamReader;
+import java.net.InetAddress;
 import java.net.Socket;
 
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
@@ -55,7 +57,7 @@ public class NettySizeHeaderFrameDecoderTests extends ESTestCase {
     private ThreadPool threadPool;
     private NettyTransport nettyTransport;
     private int port;
-    private String host;
+    private InetAddress host;
 
     @Before
     public void startThreadPool() {
@@ -70,7 +72,7 @@ public class NettySizeHeaderFrameDecoderTests extends ESTestCase {
 
         InetSocketTransportAddress transportAddress = (InetSocketTransportAddress) nettyTransport.boundAddress().boundAddress();
         port = transportAddress.address().getPort();
-        host = transportAddress.address().getHostString();
+        host = transportAddress.address().getAddress();
 
     }
 
diff --git a/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortIntegrationIT.java b/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortIntegrationIT.java
index 78ba16a..e945f16 100644
--- a/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortIntegrationIT.java
@@ -23,6 +23,7 @@ import org.elasticsearch.action.admin.cluster.health.ClusterHealthStatus;
 import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;
 import org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse;
 import org.elasticsearch.client.transport.TransportClient;
+import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.InetSocketTransportAddress;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -32,6 +33,7 @@ import org.elasticsearch.test.transport.MockTransportService;
 import org.elasticsearch.transport.TransportModule;
 import org.junit.Test;
 
+import java.net.InetAddress;
 import java.util.Locale;
 
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
@@ -71,7 +73,7 @@ public class NettyTransportMultiPortIntegrationIT extends ESIntegTestCase {
                 .put("path.home", createTempDir().toString())
                 .build();
         try (TransportClient transportClient = TransportClient.builder().settings(settings).loadConfigSettings(false).build()) {
-            transportClient.addTransportAddress(new InetSocketTransportAddress("127.0.0.1", randomPort));
+            transportClient.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("127.0.0.1"), randomPort));
             ClusterHealthResponse response = transportClient.admin().cluster().prepareHealth().get();
             assertThat(response.getStatus(), is(ClusterHealthStatus.GREEN));
         }
@@ -93,7 +95,7 @@ public class NettyTransportMultiPortIntegrationIT extends ESIntegTestCase {
             // publish address
             assertThat(nodeInfo.getTransport().getProfileAddresses().get("client1").publishAddress(), instanceOf(InetSocketTransportAddress.class));
             InetSocketTransportAddress publishAddress = (InetSocketTransportAddress) nodeInfo.getTransport().getProfileAddresses().get("client1").publishAddress();
-            assertThat(publishAddress.address().getHostName(), is("127.0.0.7"));
+            assertThat(NetworkAddress.formatAddress(publishAddress.address().getAddress()), is("127.0.0.7"));
             assertThat(publishAddress.address().getPort(), is(4321));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortTests.java b/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortTests.java
index 1a494de..11e5fee 100644
--- a/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortTests.java
+++ b/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortTests.java
@@ -170,7 +170,7 @@ public class NettyTransportMultiPortTests extends ESTestCase {
                         // Set SO_REUSEADDR as we may bind here and not be able
                         // to reuse the address immediately without it.
                         serverSocket.setReuseAddress(NetworkUtils.defaultReuseAddress());
-                        serverSocket.bind(new InetSocketAddress(nextPort));
+                        serverSocket.bind(new InetSocketAddress(InetAddress.getLoopbackAddress(), nextPort));
 
                         // bind was a success
                         logger.debug("port [{}] available.", nextPort);
@@ -199,7 +199,7 @@ public class NettyTransportMultiPortTests extends ESTestCase {
 
     private void assertConnectionRefused(int port) throws Exception {
         try {
-            trySocketConnection(new InetSocketTransportAddress("localhost", port).address());
+            trySocketConnection(new InetSocketTransportAddress(InetAddress.getByName("localhost"), port).address());
             fail("Expected to get exception when connecting to port " + port);
         } catch (IOException e) {
             // expected
@@ -213,7 +213,7 @@ public class NettyTransportMultiPortTests extends ESTestCase {
 
     private void assertPortIsBound(String host, int port) throws Exception {
         logger.info("Trying to connect to [{}]:[{}]", host, port);
-        trySocketConnection(new InetSocketTransportAddress(host, port).address());
+        trySocketConnection(new InetSocketTransportAddress(InetAddress.getByName(host), port).address());
     }
 
     private void trySocketConnection(InetSocketAddress address) throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportTests.java b/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportTests.java
new file mode 100644
index 0000000..a5bd661
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportTests.java
@@ -0,0 +1,130 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.transport.netty;
+
+import org.elasticsearch.common.transport.TransportAddress;
+import org.elasticsearch.test.ESTestCase;
+
+/** Unit tests for NettyTransport */
+public class NettyTransportTests extends ESTestCase {
+    
+    /** Test ipv4 host with a default port works */
+    public void testParseV4DefaultPort() throws Exception {
+        TransportAddress[] addresses = NettyTransport.parse("127.0.0.1", "1234", Integer.MAX_VALUE);
+        assertEquals(1, addresses.length);
+
+        assertEquals("127.0.0.1", addresses[0].getAddress());
+        assertEquals(1234, addresses[0].getPort());
+    }
+
+    /** Test ipv4 host with a default port range works */
+    public void testParseV4DefaultRange() throws Exception {
+        TransportAddress[] addresses = NettyTransport.parse("127.0.0.1", "1234-1235", Integer.MAX_VALUE);
+        assertEquals(2, addresses.length);
+
+        assertEquals("127.0.0.1", addresses[0].getAddress());
+        assertEquals(1234, addresses[0].getPort());
+        
+        assertEquals("127.0.0.1", addresses[1].getAddress());
+        assertEquals(1235, addresses[1].getPort());
+    }
+
+    /** Test ipv4 host with port works */
+    public void testParseV4WithPort() throws Exception {
+        TransportAddress[] addresses = NettyTransport.parse("127.0.0.1:2345", "1234", Integer.MAX_VALUE);
+        assertEquals(1, addresses.length);
+
+        assertEquals("127.0.0.1", addresses[0].getAddress());
+        assertEquals(2345, addresses[0].getPort());
+    }
+
+    /** Test ipv4 host with port range works */
+    public void testParseV4WithPortRange() throws Exception {
+        TransportAddress[] addresses = NettyTransport.parse("127.0.0.1:2345-2346", "1234", Integer.MAX_VALUE);
+        assertEquals(2, addresses.length);
+
+        assertEquals("127.0.0.1", addresses[0].getAddress());
+        assertEquals(2345, addresses[0].getPort());
+
+        assertEquals("127.0.0.1", addresses[1].getAddress());
+        assertEquals(2346, addresses[1].getPort());
+    }
+
+    /** Test unbracketed ipv6 hosts in configuration fail. Leave no ambiguity */
+    public void testParseV6UnBracketed() throws Exception {
+        try {
+            NettyTransport.parse("::1", "1234", Integer.MAX_VALUE);
+            fail("should have gotten exception");
+        } catch (IllegalArgumentException expected) {
+            assertTrue(expected.getMessage().contains("must be bracketed"));
+        }
+    }
+
+    /** Test ipv6 host with a default port works */
+    public void testParseV6DefaultPort() throws Exception {
+        TransportAddress[] addresses = NettyTransport.parse("[::1]", "1234", Integer.MAX_VALUE);
+        assertEquals(1, addresses.length);
+
+        assertEquals("::1", addresses[0].getAddress());
+        assertEquals(1234, addresses[0].getPort());
+    }
+
+    /** Test ipv6 host with a default port range works */
+    public void testParseV6DefaultRange() throws Exception {
+        TransportAddress[] addresses = NettyTransport.parse("[::1]", "1234-1235", Integer.MAX_VALUE);
+        assertEquals(2, addresses.length);
+
+        assertEquals("::1", addresses[0].getAddress());
+        assertEquals(1234, addresses[0].getPort());
+        
+        assertEquals("::1", addresses[1].getAddress());
+        assertEquals(1235, addresses[1].getPort());
+    }
+
+    /** Test ipv6 host with port works */
+    public void testParseV6WithPort() throws Exception {
+        TransportAddress[] addresses = NettyTransport.parse("[::1]:2345", "1234", Integer.MAX_VALUE);
+        assertEquals(1, addresses.length);
+
+        assertEquals("::1", addresses[0].getAddress());
+        assertEquals(2345, addresses[0].getPort());
+    }
+
+    /** Test ipv6 host with port range works */
+    public void testParseV6WithPortRange() throws Exception {
+        TransportAddress[] addresses = NettyTransport.parse("[::1]:2345-2346", "1234", Integer.MAX_VALUE);
+        assertEquals(2, addresses.length);
+
+        assertEquals("::1", addresses[0].getAddress());
+        assertEquals(2345, addresses[0].getPort());
+
+        assertEquals("::1", addresses[1].getAddress());
+        assertEquals(2346, addresses[1].getPort());
+    }
+    
+    /** Test per-address limit */
+    public void testAddressLimit() throws Exception {
+        TransportAddress[] addresses = NettyTransport.parse("[::1]:100-200", "1000", 3);
+        assertEquals(3, addresses.length);
+        assertEquals(100, addresses[0].getPort());
+        assertEquals(101, addresses[1].getPort());
+        assertEquals(102, addresses[2].getPort());
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/transport/netty/SimpleNettyTransportTests.java b/core/src/test/java/org/elasticsearch/transport/netty/SimpleNettyTransportTests.java
index a0b6ddb..5b85571 100644
--- a/core/src/test/java/org/elasticsearch/transport/netty/SimpleNettyTransportTests.java
+++ b/core/src/test/java/org/elasticsearch/transport/netty/SimpleNettyTransportTests.java
@@ -31,6 +31,9 @@ import org.elasticsearch.transport.AbstractSimpleTransportTests;
 import org.elasticsearch.transport.ConnectTransportException;
 import org.junit.Test;
 
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+
 public class SimpleNettyTransportTests extends AbstractSimpleTransportTests {
 
     @Override
@@ -44,7 +47,7 @@ public class SimpleNettyTransportTests extends AbstractSimpleTransportTests {
     }
 
     @Test(expected = ConnectTransportException.class)
-    public void testConnectException() {
-        serviceA.connectToNode(new DiscoveryNode("C", new InetSocketTransportAddress("localhost", 9876), Version.CURRENT));
+    public void testConnectException() throws UnknownHostException {
+        serviceA.connectToNode(new DiscoveryNode("C", new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9876), Version.CURRENT));
     }
 }
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/tribe/TribeIT.java b/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
index 5b8d8f3..4847d2b 100644
--- a/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
+++ b/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
@@ -23,6 +23,8 @@ import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.LuceneTestCase;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthStatus;
+import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;
+import org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.Requests;
 import org.elasticsearch.cluster.ClusterState;
@@ -32,11 +34,14 @@ import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.discovery.MasterNotDiscoveredException;
+import org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing;
 import org.elasticsearch.node.Node;
 import org.elasticsearch.node.NodeBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.InternalTestCluster;
+import org.elasticsearch.test.SettingsSource;
 import org.elasticsearch.test.TestCluster;
 import org.junit.After;
 import org.junit.AfterClass;
@@ -44,6 +49,8 @@ import org.junit.BeforeClass;
 import org.junit.Test;
 
 import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
 import java.util.Map;
 
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
@@ -69,8 +76,21 @@ public class TribeIT extends ESIntegTestCase {
     @BeforeClass
     public static void setupSecondCluster() throws Exception {
         ESIntegTestCase.beforeClass();
+        SettingsSource source = new SettingsSource() {
+            @Override
+            public Settings node(int nodeOrdinal) {
+                final int base = InternalTestCluster.BASE_PORT + 1000;
+                return Settings.builder().put("transport.tcp.port", base + "-" + (base + 100)).build();
+            }
+
+            @Override
+            public Settings transportClient() {
+                return node(0);
+            }
+        };
         // create another cluster
-        cluster2 = new InternalTestCluster(randomLong(), createTempDir(), 2, 2, Strings.randomBase64UUID(getRandom()), 0, false, SECOND_CLUSTER_NODE_PREFIX);
+        cluster2 = new InternalTestCluster(InternalTestCluster.configuredNodeMode(), randomLong(), createTempDir(), 2, 2, Strings.randomBase64UUID(getRandom()), source, 0, false, SECOND_CLUSTER_NODE_PREFIX);
+
         cluster2.beforeTest(getRandom(), 0.1);
         cluster2.ensureAtLeastNumDataNodes(2);
     }
@@ -109,6 +129,10 @@ public class TribeIT extends ESIntegTestCase {
             tribe1Defaults.put("tribe.t1." + entry.getKey(), entry.getValue());
             tribe2Defaults.put("tribe.t2." + entry.getKey(), entry.getValue());
         }
+        // give each tribe it's unicast hosts to connect to
+        tribe1Defaults.putArray("tribe.t1." + UnicastZenPing.DISCOVERY_ZEN_PING_UNICAST_HOSTS, getUnicastHosts(internalCluster().client()));
+        tribe1Defaults.putArray("tribe.t2." + UnicastZenPing.DISCOVERY_ZEN_PING_UNICAST_HOSTS, getUnicastHosts(cluster2.client()));
+
         Settings merged = Settings.builder()
                 .put("tribe.t1.cluster.name", internalCluster().getClusterName())
                 .put("tribe.t2.cluster.name", cluster2.getClusterName())
@@ -421,4 +445,14 @@ public class TribeIT extends ESIntegTestCase {
         }
         return count;
     }
+
+    public String[] getUnicastHosts(Client client) {
+        ArrayList<String> unicastHosts = new ArrayList<>();
+        NodesInfoResponse nodeInfos = client.admin().cluster().prepareNodesInfo().clear().setTransport(true).get();
+        for (NodeInfo info : nodeInfos.getNodes()) {
+            TransportAddress address = info.getTransport().getAddress().publishAddress();
+            unicastHosts.add(address.getAddress() + ":" + address.getPort());
+        }
+        return unicastHosts.toArray(new String[unicastHosts.size()]);
+    }
 }
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java b/core/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java
index 7bcbd99..10b4501 100644
--- a/core/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java
+++ b/core/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java
@@ -48,7 +48,7 @@ public class TribeUnitTests extends ESTestCase {
     private static Node tribe1;
     private static Node tribe2;
 
-    private static final String NODE_MODE = InternalTestCluster.nodeMode();
+    private static final String NODE_MODE = InternalTestCluster.configuredNodeMode();
 
     @BeforeClass
     public static void createTribes() {
diff --git a/core/src/test/resources/config/garbage/garbage.yml b/core/src/test/resources/config/garbage/garbage.yml
new file mode 100644
index 0000000..36c5fdb
--- /dev/null
+++ b/core/src/test/resources/config/garbage/garbage.yml
@@ -0,0 +1,7 @@
+SKDFLK@$#L%@KL#%L#@$#@L$ #L$@$ #L@K$#L $L $K#L#@L $#L
+!!@!@$(#%#)(@)% #(%)
+#(%#@)%@#)% (@#%()
+()#%@#% (@ )%@%(@#)% @( %)@ %(@)
+)(%)@()(%)()(#%)@#
+
+node.name: "Hiro Takachiho"
diff --git a/core/src/test/resources/org/elasticsearch/test_plugins/anotherplugin/plugin-descriptor.properties b/core/src/test/resources/org/elasticsearch/test_plugins/anotherplugin/plugin-descriptor.properties
index 8a08b72..66741ad 100644
--- a/core/src/test/resources/org/elasticsearch/test_plugins/anotherplugin/plugin-descriptor.properties
+++ b/core/src/test/resources/org/elasticsearch/test_plugins/anotherplugin/plugin-descriptor.properties
@@ -1,3 +1,4 @@
 site=true
 description=anotherplugin
 version=1.0
+name=anotherplugin
diff --git a/core/src/test/resources/org/elasticsearch/test_plugins/dummy/plugin-descriptor.properties b/core/src/test/resources/org/elasticsearch/test_plugins/dummy/plugin-descriptor.properties
index 71f5a59..91ae24c 100644
--- a/core/src/test/resources/org/elasticsearch/test_plugins/dummy/plugin-descriptor.properties
+++ b/core/src/test/resources/org/elasticsearch/test_plugins/dummy/plugin-descriptor.properties
@@ -1,3 +1,4 @@
 site=true
 description=dummy
 version=1.0
+name=dummy
diff --git a/core/src/test/resources/org/elasticsearch/test_plugins/subdir/plugin-descriptor.properties b/core/src/test/resources/org/elasticsearch/test_plugins/subdir/plugin-descriptor.properties
index f6a05a4..fa8950e 100644
--- a/core/src/test/resources/org/elasticsearch/test_plugins/subdir/plugin-descriptor.properties
+++ b/core/src/test/resources/org/elasticsearch/test_plugins/subdir/plugin-descriptor.properties
@@ -1,3 +1,4 @@
 site=true
 description=subdir
 version=1.0
+name=subdir
diff --git a/dev-tools/build_release.py b/dev-tools/build_release.py
deleted file mode 100644
index 5d41aef..0000000
--- a/dev-tools/build_release.py
+++ /dev/null
@@ -1,764 +0,0 @@
-# Licensed to Elasticsearch under one or more contributor
-# license agreements. See the NOTICE file distributed with
-# this work for additional information regarding copyright
-# ownership. Elasticsearch licenses this file to you under
-# the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance  with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on 
-# an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
-# either express or implied. See the License for the specific
-# language governing permissions and limitations under the License.
-
-import re
-import tempfile
-import shutil
-import os
-import datetime
-import json
-import time
-import sys
-import argparse
-import hmac
-import urllib
-import fnmatch
-import socket
-import urllib.request
-import subprocess
-
-from functools import partial
-from http.client import HTTPConnection
-from http.client import HTTPSConnection
-
-
-""" 
- This tool builds a release from the a given elasticsearch branch.
- In order to execute it go in the top level directory and run:
-   $ python3 dev_tools/build_release.py --branch 0.90 --publish --remote origin
-
- By default this script runs in 'dry' mode which essentially simulates a release. If the
- '--publish' option is set the actual release is done. The script takes over almost all
- steps necessary for a release from a high level point of view it does the following things:
-
-  - run prerequisit checks ie. check for Java 1.7 being presend or S3 credentials available as env variables
-  - detect the version to release from the specified branch (--branch) or the current branch
-  - creates a release branch & updates pom.xml and Version.java to point to a release version rather than a snapshot
-  - builds the artifacts and runs smoke-tests on the build zip & tar.gz files
-  - commits the new version and merges the release branch into the source branch
-  - creates a tag and pushes the commit to the specified origin (--remote)
-  - publishes the releases to Sonatype and S3
-
-Once it's done it will print all the remaining steps.
-
- Prerequisites:
-    - Python 3k for script execution
-    - Boto for S3 Upload ($ apt-get install python-boto)
-    - RPM for RPM building ($ apt-get install rpm)
-    - S3 keys exported via ENV variables (AWS_ACCESS_KEY_ID,  AWS_SECRET_ACCESS_KEY)
-    - GPG data exported via ENV variables (GPG_KEY_ID, GPG_PASSPHRASE, optionally GPG_KEYRING)
-    - S3 target repository via ENV variables (S3_BUCKET_SYNC_TO, optionally S3_BUCKET_SYNC_FROM)
-"""
-env = os.environ
-
-PLUGINS = [('license', 'elasticsearch/license/latest'),
-           ('bigdesk', 'lukas-vlcek/bigdesk'),
-           ('paramedic', 'karmi/elasticsearch-paramedic'),
-           ('segmentspy', 'polyfractal/elasticsearch-segmentspy'),
-           ('inquisitor', 'polyfractal/elasticsearch-inquisitor'),
-           ('head', 'mobz/elasticsearch-head')]
-
-LOG = env.get('ES_RELEASE_LOG', '/tmp/elasticsearch_release.log')
-
-# console colors
-COLOR_OK = '\033[92m'
-COLOR_END = '\033[0m'
-COLOR_FAIL = '\033[91m'
-
-def log(msg):
-  log_plain('\n%s' % msg)
-
-def log_plain(msg):
-  f = open(LOG, mode='ab')
-  f.write(msg.encode('utf-8'))
-  f.close()
-
-def run(command, quiet=False):
-  log('%s: RUN: %s\n' % (datetime.datetime.now(), command))
-  if os.system('%s >> %s 2>&1' % (command, LOG)):
-    msg = '    FAILED: %s [see log %s]' % (command, LOG)
-    if not quiet:
-      print(msg)
-    raise RuntimeError(msg)
-
-try:
-  JAVA_HOME = env['JAVA_HOME']
-except KeyError:
-  raise RuntimeError("""
-  Please set JAVA_HOME in the env before running release tool
-  On OSX use: export JAVA_HOME=`/usr/libexec/java_home -v '1.7*'`""")
-
-try:
-  JAVA_HOME = env['JAVA7_HOME']
-except KeyError:
-  pass #no JAVA7_HOME - we rely on JAVA_HOME
-
-
-try:
-  # make sure mvn3 is used if mvn3 is available
-  # some systems use maven 2 as default
-  subprocess.check_output('mvn3 --version', shell=True, stderr=subprocess.STDOUT)
-  MVN = 'mvn3'
-except subprocess.CalledProcessError:
-  MVN = 'mvn'
-
-def java_exe():
-  path = JAVA_HOME
-  return 'export JAVA_HOME="%s" PATH="%s/bin:$PATH" JAVACMD="%s/bin/java"' % (path, path, path)
-
-def verify_java_version(version):
-  s = os.popen('%s; java -version 2>&1' % java_exe()).read()
-  if ' version "%s.' % version not in s:
-    raise RuntimeError('got wrong version for java %s:\n%s' % (version, s))
-
-# Verifies the java version. We guarantee that we run with Java 1.7
-# If 1.7 is not available fail the build!
-def verify_mvn_java_version(version, mvn):
-  s = os.popen('%s; %s --version 2>&1' % (java_exe(), mvn)).read()
-  if 'Java version: %s' % version not in s:
-    raise RuntimeError('got wrong java version for %s %s:\n%s' % (mvn, version, s))
-
-# Returns the hash of the current git HEAD revision
-def get_head_hash():
-  return os.popen(' git rev-parse --verify HEAD 2>&1').read().strip()
-
-# Returns the hash of the given tag revision
-def get_tag_hash(tag):
-  return os.popen('git show-ref --tags %s --hash 2>&1' % (tag)).read().strip()
-
-# Returns the name of the current branch
-def get_current_branch():
-  return os.popen('git rev-parse --abbrev-ref HEAD  2>&1').read().strip()
-
-# Utility that returns the name of the release branch for a given version
-def release_branch(version):
-  return 'release_branch_%s' % version
-
-# runs get fetch on the given remote
-def fetch(remote):
-  run('git fetch %s' % remote)
-
-# Creates a new release branch from the given source branch
-# and rebases the source branch from the remote before creating
-# the release branch. Note: This fails if the source branch
-# doesn't exist on the provided remote.
-def create_release_branch(remote, src_branch, release):
-  run('git checkout %s' % src_branch)
-  run('git pull --rebase %s %s' % (remote, src_branch))
-  run('git checkout -b %s' % (release_branch(release)))
-
-
-# Reads the given file and applies the
-# callback to it. If the callback changed
-# a line the given file is replaced with
-# the modified input.
-def process_file(file_path, line_callback):
-  fh, abs_path = tempfile.mkstemp()
-  modified = False
-  with open(abs_path,'w', encoding='utf-8') as new_file:
-    with open(file_path, encoding='utf-8') as old_file:
-      for line in old_file:
-        new_line = line_callback(line)
-        modified = modified or (new_line != line)
-        new_file.write(new_line)
-  os.close(fh)
-  if modified:
-    #Remove original file
-    os.remove(file_path)
-    #Move new file
-    shutil.move(abs_path, file_path)
-    return True
-  else:
-    # nothing to do - just remove the tmp file
-    os.remove(abs_path)
-    return False
-
-# Walks the given directory path (defaults to 'docs')
-# and replaces all 'coming[$version]' tags with
-# 'added[$version]'. This method only accesses asciidoc files.
-def update_reference_docs(release_version, path='docs'):
-  pattern = 'coming[%s' % (release_version)
-  replacement = 'added[%s' % (release_version)
-  pending_files = []
-  def callback(line):
-    return line.replace(pattern, replacement)
-  for root, _, file_names in os.walk(path):
-    for file_name in fnmatch.filter(file_names, '*.asciidoc'):
-      full_path = os.path.join(root, file_name)
-      if process_file(full_path, callback):
-        pending_files.append(os.path.join(root, file_name))
-  return pending_files
-
-# Moves the pom.xml file from a snapshot to a release
-def remove_maven_snapshot(pom, release):
-  pattern = '<version>%s-SNAPSHOT</version>' % (release)
-  replacement = '<version>%s</version>' % (release)
-  def callback(line):
-    return line.replace(pattern, replacement)
-  process_file(pom, callback)
-
-# Moves the Version.java file from a snapshot to a release
-def remove_version_snapshot(version_file, release):
-  # 1.0.0.Beta1 -> 1_0_0_Beta1
-  release = release.replace('.', '_')
-  pattern = 'new Version(V_%s_ID, true' % (release)
-  replacement = 'new Version(V_%s_ID, false' % (release)
-  def callback(line):
-    return line.replace(pattern, replacement)
-  process_file(version_file, callback)
-
-# Stages the given files for the next git commit
-def add_pending_files(*files):
-  for file in files:
-    run('git add %s' % (file))
-
-# Executes a git commit with 'release [version]' as the commit message
-def commit_release(release):
-  run('git commit -m "release [%s]"' % release)
-
-def commit_feature_flags(release):
-    run('git commit -m "Update Documentation Feature Flags [%s]"' % release)
-
-def tag_release(release):
-  run('git tag -a v%s -m "Tag release version %s"' % (release, release))
-
-def run_mvn(*cmd):
-  for c in cmd:
-    run('%s; %s %s' % (java_exe(), MVN, c))
-
-def build_release(release_version, run_tests=False, dry_run=True, cpus=1, bwc_version=None):
-  target = 'deploy'
-  if dry_run:
-    target = 'package'
-  if run_tests:
-    run_mvn('clean',
-            'test -Dtests.jvms=%s -Des.node.mode=local' % (cpus),
-            'test -Dtests.jvms=%s -Des.node.mode=network' % (cpus))
-  if bwc_version:
-      print('Running Backwards compatibility tests against version [%s]' % (bwc_version))
-      run_mvn('clean', 'test -Dtests.filter=@backwards -Dtests.bwc.version=%s -Dtests.bwc=true -Dtests.jvms=1' % bwc_version)
-  run_mvn('clean test-compile -Dforbidden.test.signatures="org.apache.lucene.util.LuceneTestCase\$AwaitsFix @ Please fix all bugs before release"')
-  # dont sign the RPM, so older distros will be able to use the uploaded RPM package
-  gpg_args = '-Dgpg.key="%s" -Dgpg.passphrase="%s" -Ddeb.sign=true -Drpm.sign=false' % (env.get('GPG_KEY_ID'), env.get('GPG_PASSPHRASE'))
-  if env.get('GPG_KEYRING'):
-    gpg_args += ' -Dgpg.keyring="%s"' % env.get('GPG_KEYRING')
-  run_mvn('clean %s -DskipTests %s' % (target, gpg_args))
-  success = False
-  try:
-    # create additional signed RPM for the repositories
-    run_mvn('-f distribution/rpm/pom.xml package -DskipTests -Dsign.rpm=true -Drpm.outputDirectory=target/releases/signed/ %s' % (gpg_args))
-    rpm = os.path.join('target/releases/signed', 'elasticsearch-%s.rpm' % release_version)
-    if os.path.isfile(rpm):
-      log('Signed RPM [%s] contains: ' % rpm)
-      run('rpm -pqli %s' % rpm)
-      success = True
-  finally:
-    if not success:
-      print("""
-  RPM Bulding failed make sure "rpm" tools are installed.
-  Use on of the following commands to install:
-    $ brew install rpm # on OSX
-    $ apt-get install rpm # on Ubuntu et.al
-  """)
-
-# Uses the github API to fetch open tickets for the given release version
-# if it finds any tickets open for that version it will throw an exception
-def ensure_no_open_tickets(version):
-  version = "v%s" % version
-  conn = HTTPSConnection('api.github.com')
-  try:
-    log('Checking for open tickets on Github for version %s' % version)
-    log('Check if node is available')
-    conn.request('GET', '/repos/elastic/elasticsearch/issues?state=open&labels=%s' % version, headers= {'User-Agent' : 'Elasticsearch version checker'})
-    res = conn.getresponse()
-    if res.status == 200:
-      issues = json.loads(res.read().decode("utf-8"))
-      if issues:
-        urls = []
-        for issue in issues:
-          urls.append(issue['html_url'])
-        raise RuntimeError('Found open issues for release version %s:\n%s' % (version, '\n'.join(urls)))
-      else:
-        log("No open issues found for version %s" % version)
-    else:
-      raise RuntimeError('Failed to fetch issue list from Github for release version %s' % version)
-  except socket.error as e:
-    log("Failed to fetch issue list from Github for release version %s' % version - Exception: [%s]" % (version, e))
-    #that is ok it might not be there yet
-  finally:
-    conn.close()
-
-def wait_for_node_startup(host='127.0.0.1', port=9200,timeout=15):
-  for _ in range(timeout):
-    conn = HTTPConnection(host, port, timeout)
-    try:
-      log('Waiting until node becomes available for 1 second')
-      time.sleep(1)
-      log('Check if node is available')
-      conn.request('GET', '')
-      res = conn.getresponse()
-      if res.status == 200:
-        return True
-    except socket.error as e:
-      log("Failed while waiting for node - Exception: [%s]" % e)
-      #that is ok it might not be there yet
-    finally:
-      conn.close()
-
-  return False
-
-# Ensures we are using a true Lucene release, not a snapshot build:
-def verify_lucene_version():
-  s = open('pom.xml', encoding='utf-8').read()
-  if 'download.elastic.co/lucenesnapshots' in s:
-    raise RuntimeError('pom.xml contains download.elastic.co/lucenesnapshots repository: remove that before releasing')
-
-  m = re.search(r'<lucene.version>(.*?)</lucene.version>', s)
-  if m is None:
-    raise RuntimeError('unable to locate lucene.version in pom.xml')
-  lucene_version = m.group(1)
-
-  m = re.search(r'<lucene.maven.version>(.*?)</lucene.maven.version>', s)
-  if m is None:
-    raise RuntimeError('unable to locate lucene.maven.version in pom.xml')
-  lucene_maven_version = m.group(1)
-  if lucene_version != lucene_maven_version:
-    raise RuntimeError('pom.xml is still using a snapshot release of lucene (%s): cutover to a real lucene release before releasing' % lucene_maven_version)
-    
-# Checks the pom.xml for the release version.
-# This method fails if the pom file has no SNAPSHOT version set ie.
-# if the version is already on a release version we fail.
-# Returns the next version string ie. 0.90.7
-def find_release_version(src_branch):
-  run('git checkout %s' % src_branch)
-  with open('pom.xml', encoding='utf-8') as file:
-    for line in file:
-      match = re.search(r'<version>(.+)-SNAPSHOT</version>', line)
-      if match:
-        return match.group(1)
-    raise RuntimeError('Could not find release version in branch %s' % src_branch)
-
-def artifact_names(release):
-  artifacts = []
-  artifacts.append(os.path.join('distribution/zip/target/releases', 'elasticsearch-%s.zip' % (release)))
-  artifacts.append(os.path.join('distribution/tar/target/releases', 'elasticsearch-%s.tar.gz' % (release)))
-  artifacts.append(os.path.join('distribution/deb/target/releases', 'elasticsearch-%s.deb' % (release)))
-  artifacts.append(os.path.join('distribution/rpm/target/releases', 'elasticsearch-%s.rpm' % (release)))
-  return artifacts
-
-def get_artifacts(release):
-  common_artifacts = artifact_names(release)
-  for f in common_artifacts:
-    if not os.path.isfile(f):
-      raise RuntimeError('Could not find required artifact at %s' % f)
-  return common_artifacts
-
-# Sample URL:
-# http://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/elasticsearch-rpm/2.0.0-beta1-SNAPSHOT/elasticsearch-rpm-2.0.0-beta1-SNAPSHOT.rpm
-def download_and_verify(release, files, plugins=None, base_url='https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution'):
-  print('Downloading and verifying release %s from %s' % (release, base_url))
-  tmp_dir = tempfile.mkdtemp()
-  try:
-    downloaded_files = []
-    for file in files:
-      name = os.path.basename(file)
-      if name.endswith('tar.gz'):
-        url = '%s/tar/elasticsearch/%s/%s' % (base_url, release, name)
-      elif name.endswith('zip'):
-        url = '%s/zip/elasticsearch/%s/%s' % (base_url, release, name)
-      elif name.endswith('rpm'):
-        url = '%s/rpm/elasticsearch/%s/%s' % (base_url, release, name)
-      elif name.endswith('deb'):
-        url = '%s/deb/elasticsearch/%s/%s' % (base_url, release, name)
-      abs_file_path = os.path.join(tmp_dir, name)
-      print('  Downloading %s' % (url))
-      downloaded_files.append(abs_file_path)
-      urllib.request.urlretrieve(url, abs_file_path)
-      url = ''.join([url, '.sha1'])
-      checksum_file = os.path.join(tmp_dir, ''.join([abs_file_path, '.sha1']))
-      urllib.request.urlretrieve(url, checksum_file)
-      print('  Verifying checksum %s' % (checksum_file))
-      run('cd %s && sha1sum -c %s' % (tmp_dir, os.path.basename(checksum_file)))
-    smoke_test_release(release, downloaded_files, get_tag_hash('v%s' % release), plugins)
-    print('  SUCCESS')
-  finally:
-    shutil.rmtree(tmp_dir)
-
-def smoke_test_release(release, files, expected_hash, plugins):
-  for release_file in files:
-    if not os.path.isfile(release_file):
-      raise RuntimeError('Smoketest failed missing file %s' % (release_file))
-    tmp_dir = tempfile.mkdtemp()
-    if release_file.endswith('tar.gz'):
-      run('tar -xzf %s -C %s' % (release_file, tmp_dir))
-    elif release_file.endswith('zip'):
-      run('unzip %s -d %s' % (release_file, tmp_dir))
-    else:
-      log('Skip SmokeTest for [%s]' % release_file)
-      continue # nothing to do here
-    es_run_path = os.path.join(tmp_dir, 'elasticsearch-%s' % (release), 'bin/elasticsearch')
-    print('  Smoke testing package [%s]' % release_file)
-    es_plugin_path = os.path.join(tmp_dir, 'elasticsearch-%s' % (release),'bin/plugin')
-    plugin_names = {}
-    for name, plugin  in plugins:
-      print('  Install plugin [%s] from [%s]' % (name, plugin))
-      run('%s; %s install %s' % (java_exe(), es_plugin_path, plugin))
-      plugin_names[name] = True
-
-    background = '-d'
-    print('  Starting elasticsearch deamon from [%s]' % os.path.join(tmp_dir, 'elasticsearch-%s' % release))
-    run('%s; %s -Des.node.name=smoke_tester -Des.cluster.name=prepare_release -Des.discovery.zen.ping.multicast.enabled=false -Des.script.inline=on -Des.script.indexed=on %s'
-         % (java_exe(), es_run_path, background))
-    conn = HTTPConnection('127.0.0.1', 9200, 20);
-    wait_for_node_startup()
-    try:
-      try:
-        conn.request('GET', '')
-        res = conn.getresponse()
-        if res.status == 200:
-          version = json.loads(res.read().decode("utf-8"))['version']
-          if release != version['number']:
-            raise RuntimeError('Expected version [%s] but was [%s]' % (release, version['number']))
-          if version['build_snapshot']:
-            raise RuntimeError('Expected non snapshot version')
-          if version['build_hash'].strip() !=  expected_hash:
-            raise RuntimeError('HEAD hash does not match expected [%s] but got [%s]' % (expected_hash, version['build_hash']))
-          print('  Running REST Spec tests against package [%s]' % release_file)
-          run_mvn('test -Dtests.cluster=%s -Dtests.jvms=1 -Dtests.class=*.*RestTests' % ("127.0.0.1:9300"))
-          print('  Verify if plugins are listed in _nodes')
-          conn.request('GET', '/_nodes?plugin=true&pretty=true')
-          res = conn.getresponse()
-          if res.status == 200:
-            nodes = json.loads(res.read().decode("utf-8"))['nodes']
-            for _, node in nodes.items():
-              node_plugins = node['plugins']
-              for node_plugin in node_plugins:
-                if not plugin_names.get(node_plugin['name'], False):
-                  raise RuntimeError('Unexpeced plugin %s' % node_plugin['name'])
-                del plugin_names[node_plugin['name']]
-            if plugin_names:
-              raise RuntimeError('Plugins not loaded %s' % list(plugin_names.keys()))
-
-          else:
-           raise RuntimeError('Expected HTTP 200 but got %s' % res.status)
-        else:
-          raise RuntimeError('Expected HTTP 200 but got %s' % res.status)
-      finally:
-        conn.request('POST', '/_cluster/nodes/_local/_shutdown')
-        time.sleep(1) # give the node some time to shut down
-        if conn.getresponse().status != 200:
-          raise RuntimeError('Expected HTTP 200 but got %s on node shutdown' % res.status)
-
-    finally:
-      conn.close()
-    shutil.rmtree(tmp_dir)
-
-def merge_tag_push(remote, src_branch, release_version, dry_run):
-  run('git checkout %s' %  src_branch)
-  run('git merge %s' %  release_branch(release_version))
-  run('git tag v%s' % release_version)
-  if not dry_run:
-    run('git push %s %s' % (remote, src_branch)) # push the commit
-    run('git push %s v%s' % (remote, release_version)) # push the tag
-  else:
-    print('  dryrun [True] -- skipping push to remote %s' % remote)
-
-def publish_repositories(version, dry_run=True):
-  if dry_run:
-    print('Skipping package repository update')
-  else:
-    print('Triggering repository update for version %s - calling dev-tools/build_repositories.sh %s' % (version, src_branch))
-    # src_branch is a version like 1.5/1.6/2.0/etc.. so we can use this
-    run('dev-tools/build_repositories.sh %s' % src_branch)
-
-def print_sonatype_notice():
-  settings = os.path.join(os.path.expanduser('~'), '.m2/settings.xml')
-  if os.path.isfile(settings):
-     with open(settings, encoding='utf-8') as settings_file:
-       for line in settings_file:
-         if line.strip() == '<id>sonatype-nexus-snapshots</id>':
-           # moving out - we found the indicator no need to print the warning
-           return
-  print("""
-    NOTE: No sonatype settings detected, make sure you have configured
-    your sonatype credentials in '~/.m2/settings.xml':
-
-    <settings>
-    ...
-    <servers>
-      <server>
-        <id>sonatype-nexus-snapshots</id>
-        <username>your-jira-id</username>
-        <password>your-jira-pwd</password>
-      </server>
-      <server>
-        <id>sonatype-nexus-staging</id>
-        <username>your-jira-id</username>
-        <password>your-jira-pwd</password>
-      </server>
-    </servers>
-    ...
-  </settings>
-  """)
-
-def check_command_exists(name, cmd):
-  try:
-    subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
-  except subprocess.CalledProcessError:
-    raise RuntimeError('Could not run command %s - please make sure it is installed' % (name))
-
-VERSION_FILE = 'src/main/java/org/elasticsearch/Version.java'
-POM_FILE = 'pom.xml'
-
-# finds the highest available bwc version to test against
-def find_bwc_version(release_version, bwc_dir='backwards'):
-  log('  Lookup bwc version in directory [%s]' % bwc_dir)
-  bwc_version = None
-  if os.path.exists(bwc_dir) and os.path.isdir(bwc_dir):
-    max_version = [int(x) for x in release_version.split('.')]
-    for dir in os.listdir(bwc_dir):
-      if os.path.isdir(os.path.join(bwc_dir, dir)) and dir.startswith('elasticsearch-'):
-        version = [int(x) for x in dir[len('elasticsearch-'):].split('.')]
-        if version < max_version: # bwc tests only against smaller versions
-          if (not bwc_version) or version > [int(x) for x in bwc_version.split('.')]:
-            bwc_version = dir[len('elasticsearch-'):]
-    log('  Using bwc version [%s]' % bwc_version)
-  else:
-    log('  bwc directory [%s] does not exists or is not a directory - skipping' % bwc_dir)
-  return bwc_version
-
-def ensure_checkout_is_clean(branchName):
-  # Make sure no local mods:
-  s = subprocess.check_output('git diff --shortstat', shell=True)
-  if len(s) > 0:
-    raise RuntimeError('git diff --shortstat is non-empty: got:\n%s' % s)
-
-  # Make sure no untracked files:
-  s = subprocess.check_output('git status', shell=True).decode('utf-8', errors='replace')
-  if 'Untracked files:' in s:
-    raise RuntimeError('git status shows untracked files: got:\n%s' % s)
-
-  # Make sure we are on the right branch (NOTE: a bit weak, since we default to current branch):
-  if 'On branch %s' % branchName not in s:
-    raise RuntimeError('git status does not show branch %s: got:\n%s' % (branchName, s))
-
-  # Make sure we have all changes from origin:
-  if 'is behind' in s:
-    raise RuntimeError('git status shows not all changes pulled from origin; try running "git pull origin %s": got:\n%s' % (branchName, s))
-
-  # Make sure we no local unpushed changes (this is supposed to be a clean area):
-  if 'is ahead' in s:
-    raise RuntimeError('git status shows local commits; try running "git fetch origin", "git checkout %s", "git reset --hard origin/%s": got:\n%s' % (branchName, branchName, s))
-
-# Checks all source files for //NORELEASE comments
-def check_norelease(path='src'):
-  pattern = re.compile(r'\bnorelease\b', re.IGNORECASE)
-  for root, _, file_names in os.walk(path):
-    for file_name in fnmatch.filter(file_names, '*.java'):
-      full_path = os.path.join(root, file_name)
-      line_number = 0
-      with open(full_path, 'r', encoding='utf-8') as current_file:
-        for line in current_file:
-          line_number = line_number + 1
-          if pattern.search(line):
-            raise RuntimeError('Found //norelease comment in %s line %s' % (full_path, line_number))
-
-def run_and_print(text, run_function):
-  try:
-    print(text, end='')
-    run_function()
-    print(COLOR_OK + 'OK' + COLOR_END)
-    return True
-  except RuntimeError:
-    print(COLOR_FAIL + 'NOT OK' + COLOR_END)
-    return False
-
-def check_env_var(text, env_var):
-  try:
-    print(text, end='')
-    env[env_var]
-    print(COLOR_OK + 'OK' + COLOR_END)
-    return True
-  except KeyError:
-    print(COLOR_FAIL + 'NOT OK' + COLOR_END)
-    return False
-
-def check_environment_and_commandline_tools(check_only):
-  checks = list()
-  checks.append(check_env_var('Checking for AWS env configuration AWS_SECRET_ACCESS_KEY_ID...     ', 'AWS_SECRET_ACCESS_KEY'))
-  checks.append(check_env_var('Checking for AWS env configuration AWS_ACCESS_KEY_ID...            ', 'AWS_ACCESS_KEY_ID'))
-  checks.append(check_env_var('Checking for SONATYPE env configuration SONATYPE_USERNAME...       ', 'SONATYPE_USERNAME'))
-  checks.append(check_env_var('Checking for SONATYPE env configuration SONATYPE_PASSWORD...       ', 'SONATYPE_PASSWORD'))
-  checks.append(check_env_var('Checking for GPG env configuration GPG_KEY_ID...                   ', 'GPG_KEY_ID'))
-  checks.append(check_env_var('Checking for GPG env configuration GPG_PASSPHRASE...               ', 'GPG_PASSPHRASE'))
-  checks.append(check_env_var('Checking for S3 repo upload env configuration S3_BUCKET_SYNC_TO... ', 'S3_BUCKET_SYNC_TO'))
-  checks.append(check_env_var('Checking for git env configuration GIT_AUTHOR_NAME...              ', 'GIT_AUTHOR_NAME'))
-  checks.append(check_env_var('Checking for git env configuration GIT_AUTHOR_EMAIL...             ', 'GIT_AUTHOR_EMAIL'))
-
-  checks.append(run_and_print('Checking command: rpm...            ', partial(check_command_exists, 'rpm', 'rpm --version')))
-  checks.append(run_and_print('Checking command: dpkg...           ', partial(check_command_exists, 'dpkg', 'dpkg --version')))
-  checks.append(run_and_print('Checking command: gpg...            ', partial(check_command_exists, 'gpg', 'gpg --version')))
-  checks.append(run_and_print('Checking command: expect...         ', partial(check_command_exists, 'expect', 'expect -v')))
-  checks.append(run_and_print('Checking command: createrepo...     ', partial(check_command_exists, 'createrepo', 'createrepo --version')))
-  checks.append(run_and_print('Checking command: s3cmd...          ', partial(check_command_exists, 's3cmd', 's3cmd --version')))
-  checks.append(run_and_print('Checking command: apt-ftparchive... ', partial(check_command_exists, 'apt-ftparchive', 'apt-ftparchive --version')))
-
-  # boto, check error code being returned
-  location = os.path.dirname(os.path.realpath(__file__))
-  command = 'python %s/upload-s3.py -h' % (location)
-  checks.append(run_and_print('Testing boto python dependency...   ', partial(check_command_exists, 'python-boto', command)))
-
-  checks.append(run_and_print('Checking java version...            ', partial(verify_java_version, '1.7')))
-  checks.append(run_and_print('Checking java mvn version...        ', partial(verify_mvn_java_version, '1.7', MVN)))
-
-  if check_only:
-    sys.exit(0)
-
-  if False in checks:
-    print("Exiting due to failing checks")
-    sys.exit(0)
-
-if __name__ == '__main__':
-  parser = argparse.ArgumentParser(description='Builds and publishes a Elasticsearch Release')
-  parser.add_argument('--branch', '-b', metavar='RELEASE_BRANCH', default=get_current_branch(),
-                       help='The branch to release from. Defaults to the current branch.')
-  parser.add_argument('--cpus', '-c', metavar='1', default=1,
-                       help='The number of cpus to use for running the test. Default is [1]')
-  parser.add_argument('--skiptests', '-t', dest='tests', action='store_false',
-                      help='Skips tests before release. Tests are run by default.')
-  parser.set_defaults(tests=True)
-  parser.add_argument('--remote', '-r', metavar='origin', default='origin',
-                      help='The remote to push the release commit and tag to. Default is [origin]')
-  parser.add_argument('--publish', '-d', dest='dryrun', action='store_false',
-                      help='Publishes the release. Disable by default.')
-  parser.add_argument('--smoke', '-s', dest='smoke', default='',
-                      help='Smoke tests the given release')
-  parser.add_argument('--bwc', '-w', dest='bwc', metavar='backwards', default='backwards',
-                      help='Backwards compatibility version path to use to run compatibility tests against')
-  parser.add_argument('--check-only', dest='check_only', action='store_true',
-                      help='Checks and reports for all requirements and then exits')
-
-  parser.set_defaults(dryrun=True)
-  parser.set_defaults(smoke=None)
-  parser.set_defaults(check_only=False)
-  args = parser.parse_args()
-  bwc_path = args.bwc
-  src_branch = args.branch
-  remote = args.remote
-  run_tests = args.tests
-  dry_run = args.dryrun
-  cpus = args.cpus
-  build = not args.smoke
-  smoke_test_version = args.smoke
-
-  check_environment_and_commandline_tools(args.check_only)
-
-  # we print a notice if we can not find the relevant infos in the ~/.m2/settings.xml
-  print_sonatype_notice()
-
-  # we require to build with 1.7
-  verify_java_version('1.7')
-  verify_mvn_java_version('1.7', MVN)
-
-  if os.path.exists(LOG):
-    raise RuntimeError('please remove old release log %s first' % LOG)
-
-  if not dry_run:
-    print('WARNING: dryrun is set to "false" - this will push and publish the release')
-    input('Press Enter to continue...')
-
-  print(''.join(['-' for _ in range(80)]))
-  print('Preparing Release from branch [%s] running tests: [%s] dryrun: [%s]' % (src_branch, run_tests, dry_run))
-  print('  JAVA_HOME is [%s]' % JAVA_HOME)
-  print('  Running with maven command: [%s] ' % (MVN))
-  if build:
-    check_norelease(path='src')
-    ensure_checkout_is_clean(src_branch)
-    verify_lucene_version()
-    release_version = find_release_version(src_branch)
-    ensure_no_open_tickets(release_version)
-    if not dry_run:
-      smoke_test_version = release_version
-    head_hash = get_head_hash()
-    run_mvn('clean') # clean the env!
-    print('  Release version: [%s]' % release_version)
-    create_release_branch(remote, src_branch, release_version)
-    print('  Created release branch [%s]' % (release_branch(release_version)))
-    success = False
-    try:
-      pending_files = [POM_FILE, VERSION_FILE]
-      remove_maven_snapshot(POM_FILE, release_version)
-      remove_version_snapshot(VERSION_FILE, release_version)
-      print('  Done removing snapshot version')
-      add_pending_files(*pending_files) # expects var args use * to expand
-      commit_release(release_version)
-      pending_files = update_reference_docs(release_version)
-      version_head_hash = None
-      # split commits for docs and version to enable easy cherry-picking
-      if pending_files:
-        add_pending_files(*pending_files) # expects var args use * to expand
-        commit_feature_flags(release_version)
-        version_head_hash = get_head_hash()
-      print('  Committed release version [%s]' % release_version)
-      print(''.join(['-' for _ in range(80)]))
-      print('Building Release candidate')
-      input('Press Enter to continue...')
-      if not dry_run:
-        print('  Running maven builds now and publish to Sonatype and S3 - run-tests [%s]' % run_tests)
-      else:
-        print('  Running maven builds now run-tests [%s]' % run_tests)
-      build_release(release_version, run_tests=run_tests, dry_run=dry_run, cpus=cpus, bwc_version=find_bwc_version(release_version, bwc_path))
-      artifacts = get_artifacts(release_version)
-      smoke_test_release(release_version, artifacts, get_head_hash(), PLUGINS)
-      print(''.join(['-' for _ in range(80)]))
-      print('Finish Release -- dry_run: %s' % dry_run)
-      input('Press Enter to continue...')
-      print('  merge release branch, tag and push to %s %s -- dry_run: %s' % (remote, src_branch, dry_run))
-      merge_tag_push(remote, src_branch, release_version, dry_run)
-      print('  Updating package repositories -- dry_run: %s' % dry_run)
-      publish_repositories(src_branch, dry_run=dry_run)
-      cherry_pick_command = '.'
-      if version_head_hash:
-        cherry_pick_command = ' and cherry-pick the documentation changes: \'git cherry-pick %s\' to the development branch' % (version_head_hash)
-      pending_msg = """
-      Release successful pending steps:
-        * create a new vX.Y.Z label on github for the next release, with label color #dddddd (https://github.com/elastic/elasticsearch/labels)
-        * publish the maven artifacts on Sonatype: https://oss.sonatype.org/index.html
-           - here is a guide: http://central.sonatype.org/pages/releasing-the-deployment.html
-        * check if the release is there https://oss.sonatype.org/content/repositories/releases/org/elasticsearch/elasticsearch/%(version)s
-        * announce the release on the website / blog post
-        * tweet about the release
-        * announce the release in the google group/mailinglist
-        * Move to a Snapshot version to the current branch for the next point release%(cherry_pick)s
-      """
-      print(pending_msg % { 'version' : release_version, 'cherry_pick' : cherry_pick_command} )
-      success = True
-    finally:
-      if not success:
-        run('git reset --hard HEAD')
-        run('git checkout %s' %  src_branch)
-      elif dry_run:
-        run('git reset --hard %s' % head_hash)
-        run('git tag -d v%s' % release_version)
-      # we delete this one anyways
-      run('git branch -D %s' %  (release_branch(release_version)))
-  else:
-    print("Skipping build - smoketest only against version %s" % smoke_test_version)
-    run_mvn('clean') # clean the env!
-    
-  if smoke_test_version:
-    fetch(remote)
-    download_and_verify(smoke_test_version, artifact_names(smoke_test_version), plugins=PLUGINS)
diff --git a/dev-tools/create_bwc_index.py b/dev-tools/create_bwc_index.py
index 393b016..1a2bfbd 100644
--- a/dev-tools/create_bwc_index.py
+++ b/dev-tools/create_bwc_index.py
@@ -141,7 +141,6 @@ def start_node(version, release_dir, data_dir, repo_dir, tcp_port=DEFAULT_TRANSP
     '-Des.path.logs=logs',
     '-Des.cluster.name=%s' % cluster_name,
     '-Des.network.host=localhost',
-    '-Des.discovery.zen.ping.multicast.enabled=false',
     '-Des.transport.tcp.port=%s' % tcp_port,
     '-Des.http.port=%s' % http_port,
     '-Des.path.repo=%s' % repo_dir
diff --git a/dev-tools/prepare_release_candidate.py b/dev-tools/prepare_release_candidate.py
new file mode 100644
index 0000000..baaac88
--- /dev/null
+++ b/dev-tools/prepare_release_candidate.py
@@ -0,0 +1,273 @@
+# Licensed to Elasticsearch under one or more contributor
+# license agreements. See the NOTICE file distributed with
+# this work for additional information regarding copyright
+# ownership. Elasticsearch licenses this file to you under
+# the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance  with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on
+# an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
+# either express or implied. See the License for the specific
+# language governing permissions and limitations under the License.
+
+# Prepare a release
+#
+# 1. Update the Version.java to remove the snapshot bit
+# 2. Remove the -SNAPSHOT suffix in all pom.xml files
+#
+# USAGE:
+#
+# python3 ./dev-tools/prepare-release.py
+#
+# Note: Ensure the script is run from the root directory
+#
+
+import fnmatch
+import argparse
+from prepare_release_update_documentation import update_reference_docs
+import subprocess
+import tempfile
+import re
+import os
+import shutil
+
+VERSION_FILE = 'core/src/main/java/org/elasticsearch/Version.java'
+POM_FILE = 'pom.xml'
+MAIL_TEMPLATE = """
+Hi all
+
+The new release candidate for %(version)s based on this commit[1]  is now available, including the x-plugins, and RPM/deb repos:
+
+   - ZIP [2]
+   - tar.gz [3]
+   - RPM [4]
+   - deb [5]
+
+Plugins can be installed as follows,
+
+    bin/plugin -Des.plugins.staging=true install cloud-aws
+
+The same goes for the x-plugins:
+
+    bin/plugin -Des.plugins.staging=true install license
+    bin/plugin -Des.plugins.staging=true install shield
+    bin/plugin -Des.plugins.staging=true install watcher
+
+To install the deb from an APT repo:
+
+APT line sources.list line:
+
+deb http://download.elasticsearch.org/elasticsearch/staging/%(version)s-%(hash)s/repos/elasticsearch/%(major_minor_version)s/debian/ stable main
+
+To install the RPM, create a YUM file like:
+
+    /etc/yum.repos.d/elasticsearch.repo
+
+containing:
+
+[elasticsearch-2.0]
+name=Elasticsearch repository for packages
+baseurl=http://download.elasticsearch.org/elasticsearch/staging/%(version)s-%(hash)s/repos/elasticsearch/%(major_minor_version)s/centos
+gpgcheck=1
+gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
+enabled=1
+
+
+[1] https://github.com/elastic/elasticsearch/commit/%(hash)s
+[2] http://download.elasticsearch.org/elasticsearch/staging/%(version)s-%(hash)s/org/elasticsearch/distribution/zip/elasticsearch/%(version)s/elasticsearch-%(version)s.zip
+[3] http://download.elasticsearch.org/elasticsearch/staging/%(version)s-%(hash)s/org/elasticsearch/distribution/tar/elasticsearch/%(version)s/elasticsearch-%(version)s.tar.gz
+[4] http://download.elasticsearch.org/elasticsearch/staging/%(version)s-%(hash)s/org/elasticsearch/distribution/rpm/elasticsearch/%(version)s/elasticsearch-%(version)s.rpm
+[5] http://download.elasticsearch.org/elasticsearch/staging/%(version)s-%(hash)s/org/elasticsearch/distribution/deb/elasticsearch/%(version)s/elasticsearch-%(version)s.deb
+"""
+
+def run(command, env_vars=None):
+  if env_vars:
+    for key, value in env_vars.items():
+      os.putenv(key, value)
+  if os.system('%s' % (command)):
+    raise RuntimeError('    FAILED: %s' % (command))
+
+def ensure_checkout_is_clean():
+  # Make sure no local mods:
+  s = subprocess.check_output('git diff --shortstat', shell=True).decode('utf-8')
+  if len(s) > 0:
+    raise RuntimeError('git diff --shortstat is non-empty got:\n%s' % s)
+
+  # Make sure no untracked files:
+  s = subprocess.check_output('git status', shell=True).decode('utf-8', errors='replace')
+  if 'Untracked files:' in s:
+    if 'dev-tools/__pycache__/' in s:
+      print('*** NOTE: invoke python with -B to prevent __pycache__ directories ***')
+    raise RuntimeError('git status shows untracked files got:\n%s' % s)
+
+  # Make sure we have all changes from origin:
+  if 'is behind' in s:
+    raise RuntimeError('git status shows not all changes pulled from origin; try running "git pull origin" in this branch got:\n%s' % (s))
+
+  # Make sure we no local unpushed changes (this is supposed to be a clean area):
+  if 'is ahead' in s:
+    raise RuntimeError('git status shows local commits; try running "git fetch origin", "git checkout ", "git reset --hard origin/" in this branch got:\n%s' % (s))
+
+# Reads the given file and applies the
+# callback to it. If the callback changed
+# a line the given file is replaced with
+# the modified input.
+def process_file(file_path, line_callback):
+  fh, abs_path = tempfile.mkstemp()
+  modified = False
+  with open(abs_path,'w', encoding='utf-8') as new_file:
+    with open(file_path, encoding='utf-8') as old_file:
+      for line in old_file:
+        new_line = line_callback(line)
+        modified = modified or (new_line != line)
+        new_file.write(new_line)
+  os.close(fh)
+  if modified:
+    #Remove original file
+    os.remove(file_path)
+    #Move new file
+    shutil.move(abs_path, file_path)
+    return True
+  else:
+    # nothing to do - just remove the tmp file
+    os.remove(abs_path)
+    return False
+
+# Moves the Version.java file from a snapshot to a release
+def remove_version_snapshot(version_file, release):
+  # 1.0.0.Beta1 -> 1_0_0_Beta1
+  release = release.replace('.', '_')
+  release = release.replace('-', '_')
+  pattern = 'new Version(V_%s_ID, true' % (release)
+  replacement = 'new Version(V_%s_ID, false' % (release)
+  def callback(line):
+    return line.replace(pattern, replacement)
+  processed = process_file(version_file, callback)
+  if not processed:
+    raise RuntimeError('failed to remove snapshot version for %s' % (release))
+
+def rename_local_meta_files(path):
+  for root, _, file_names in os.walk(path):
+    for file_name in fnmatch.filter(file_names, 'maven-metadata-local.xml*'):
+      full_path = os.path.join(root, file_name)
+      os.rename(full_path, os.path.join(root, file_name.replace('-local', '')))
+
+# Checks the pom.xml for the release version.
+# This method fails if the pom file has no SNAPSHOT version set ie.
+# if the version is already on a release version we fail.
+# Returns the next version string ie. 0.90.7
+def find_release_version():
+  with open('pom.xml', encoding='utf-8') as file:
+    for line in file:
+      match = re.search(r'<version>(.+)-SNAPSHOT</version>', line)
+      if match:
+        return match.group(1)
+    raise RuntimeError('Could not find release version in branch')
+
+
+if __name__ == "__main__":
+  parser = argparse.ArgumentParser(description='Builds and publishes a Elasticsearch Release')
+  parser.add_argument('--deploy', '-d', dest='deploy', action='store_true',
+                      help='Installs and Deploys the release on a sonartype staging repository.')
+  parser.add_argument('--skipDocCheck', '-c', dest='skip_doc_check', action='store_false',
+                      help='Skips any checks for pending documentation changes')
+  parser.add_argument('--push-s3', '-p', dest='push', action='store_true',
+                      help='Pushes artifacts to the S3 staging area')
+  parser.add_argument('--install_only', '-i', dest='install_only', action='store_true',
+                      help='Only runs a maven install to skip the remove deployment step')
+  parser.add_argument('--gpg-key', '-k', dest='gpg_key', default="D88E42B4",
+                      help='Allows you to specify a different gpg_key to be used instead of the default release key')
+  parser.set_defaults(deploy=False)
+  parser.set_defaults(skip_doc_check=False)
+  parser.set_defaults(push=False)
+  parser.set_defaults(install_only=False)
+  args = parser.parse_args()
+  install_and_deploy = args.deploy
+  skip_doc_check = args.skip_doc_check
+  push = args.push
+  gpg_key = args.gpg_key
+  install_only = args.install_only
+
+  ensure_checkout_is_clean()
+  release_version = find_release_version()
+  if not re.match('(\d+\.\d+)\.*',release_version):
+    raise RuntimeError('illegal release version format: %s' % (release_version))
+  major_minor_version = re.match('(\d+\.\d+)\.*',release_version).group(1)
+
+  print('*** Preparing release version: [%s]' % release_version)
+
+  if not skip_doc_check:
+    print('*** Check for pending documentation changes')
+    pending_files = update_reference_docs(release_version)
+    if pending_files:
+      raise RuntimeError('pending coming[%s] documentation changes found in %s' % (release_version, pending_files))
+
+
+  run('cd dev-tools && mvn versions:set -DnewVersion=%s -DgenerateBackupPoms=false' % (release_version))
+  run('cd rest-api-spec && mvn versions:set -DnewVersion=%s -DgenerateBackupPoms=false' % (release_version))
+  run('mvn versions:set -DnewVersion=%s -DgenerateBackupPoms=false' % (release_version))
+
+  remove_version_snapshot(VERSION_FILE, release_version)
+
+  print('*** Done removing snapshot version. DO NOT COMMIT THIS, WHEN CREATING A RELEASE CANDIDATE.')
+
+  shortHash = subprocess.check_output('git log --pretty=format:"%h" -n 1', shell=True).decode('utf-8')
+  localRepo = '/tmp/elasticsearch-%s-%s' % (release_version, shortHash)
+  localRepoElasticsearch = localRepo + '/org/elasticsearch'
+  if os.path.exists(localRepoElasticsearch):
+    print('clean local repository %s' % localRepoElasticsearch)
+    shutil.rmtree(localRepoElasticsearch)
+
+  if install_only:
+    mvn_target = 'install'
+  else:
+    mvn_target = 'deploy'
+  install_command = 'mvn clean %s -Prelease -Dskip.integ.tests=true -Dgpg.keyname="%s" -Dpackaging.rpm.rpmbuild=/usr/bin/rpmbuild -Drpm.sign=true -Dmaven.repo.local=%s -Dno.commit.pattern="\\bno(n|)commit\\b" -Dforbidden.test.signatures=""' % (mvn_target, gpg_key, localRepo)
+  clean_repo_command = 'find %s -name _remote.repositories -exec rm {} \;' % (localRepoElasticsearch)
+  rename_metadata_files_command = 'for i in $(find %s -name "maven-metadata-local.xml*") ; do mv "$i" "${i/-local/}" ; done' % (localRepoElasticsearch)
+  s3_sync_command = 's3cmd sync %s s3://download.elasticsearch.org/elasticsearch/staging/%s-%s/org/' % (localRepoElasticsearch, release_version, shortHash)
+  s3_bucket_sync_to = 'download.elasticsearch.org/elasticsearch/staging/%s-%s/repos' % (release_version, shortHash)
+  build_repo_command = 'dev-tools/build_repositories.sh %s' % (major_minor_version)
+  if install_and_deploy:
+    for cmd in [install_command, clean_repo_command]:
+      run(cmd)
+    rename_local_meta_files(localRepoElasticsearch)
+  else:
+    print('')
+    print('*** To create a release candidate run: ')
+    print('  %s' % (install_command))
+    print('  1. Remove all _remote.repositories: %s' % (clean_repo_command))
+    print('  2. Rename all maven metadata files: %s' % (rename_metadata_files_command))
+  if push:
+    run(s3_sync_command)
+    env_vars = {'S3_BUCKET_SYNC_TO': s3_bucket_sync_to}
+    run(build_repo_command, env_vars)
+  else:
+    print('')
+    print('*** To push a release candidate to s3 run: ')
+    print('  1. Sync %s into S3 bucket' % (localRepoElasticsearch))
+    print ('    %s' % (s3_sync_command))
+    print('  2. Create repositories: ')
+    print ('    export S3_BUCKET_SYNC_TO="%s"' % (s3_bucket_sync_to))
+    print('     %s' % (build_repo_command))
+    print('')
+    print('NOTE: the above mvn command will promt you several times for the GPG passphrase of the key you specified you can alternatively pass it via -Dgpg.passphrase=yourPassPhrase')
+    print(' since RPM signing doesn\'t support gpg-agents the recommended way to set the password is to add a release profile to your settings.xml:')
+    print("""
+  <profiles>
+    <profile>
+      <id>release</id>
+      <properties>
+        <gpg.passphrase>YourPasswordGoesHere</gpg.passphrase>
+      </properties>
+    </profile>
+  </profiles>
+    """)
+    print('NOTE: Running s3cmd might require you to create a config file with your credentials, if the s3cmd does not support suppliying them via the command line!')
+  print('*** Once the release is deployed and published send out the following mail to dev@elastic.co:')
+  print(MAIL_TEMPLATE % ({'version' : release_version, 'hash': shortHash, 'major_minor_version' : major_minor_version}))
+
diff --git a/dev-tools/prepare_release_create_release_version.py b/dev-tools/prepare_release_create_release_version.py
deleted file mode 100644
index 53da1ae..0000000
--- a/dev-tools/prepare_release_create_release_version.py
+++ /dev/null
@@ -1,144 +0,0 @@
-# Licensed to Elasticsearch under one or more contributor
-# license agreements. See the NOTICE file distributed with
-# this work for additional information regarding copyright
-# ownership. Elasticsearch licenses this file to you under
-# the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance  with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on
-# an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
-# either express or implied. See the License for the specific
-# language governing permissions and limitations under the License.
-
-# Prepare a release
-#
-# 1. Update the Version.java to remove the snapshot bit
-# 2. Remove the -SNAPSHOT suffix in all pom.xml files
-#
-# USAGE:
-#
-# python3 ./dev-tools/prepare-release.py
-#
-# Note: Ensure the script is run from the root directory
-#
-
-import fnmatch
-import subprocess
-import tempfile
-import re
-import os
-import shutil
-
-VERSION_FILE = 'core/src/main/java/org/elasticsearch/Version.java'
-POM_FILE = 'pom.xml'
-
-def run(command):
-  if os.system('%s' % (command)):
-    raise RuntimeError('    FAILED: %s' % (command))
-
-def ensure_checkout_is_clean():
-  # Make sure no local mods:
-  s = subprocess.check_output('git diff --shortstat', shell=True)
-  if len(s) > 0:
-    raise RuntimeError('git diff --shortstat is non-empty: got:\n%s' % s)
-
-  # Make sure no untracked files:
-  s = subprocess.check_output('git status', shell=True).decode('utf-8', errors='replace')
-  if 'Untracked files:' in s:
-    raise RuntimeError('git status shows untracked files: got:\n%s' % s)
-
-  # Make sure we have all changes from origin:
-  if 'is behind' in s:
-    raise RuntimeError('git status shows not all changes pulled from origin; try running "git pull origin" in this branch: got:\n%s' % (s))
-
-  # Make sure we no local unpushed changes (this is supposed to be a clean area):
-  if 'is ahead' in s:
-    raise RuntimeError('git status shows local commits; try running "git fetch origin", "git checkout ", "git reset --hard origin/" in this branch: got:\n%s' % (s))
-
-# Reads the given file and applies the
-# callback to it. If the callback changed
-# a line the given file is replaced with
-# the modified input.
-def process_file(file_path, line_callback):
-  fh, abs_path = tempfile.mkstemp()
-  modified = False
-  with open(abs_path,'w', encoding='utf-8') as new_file:
-    with open(file_path, encoding='utf-8') as old_file:
-      for line in old_file:
-        new_line = line_callback(line)
-        modified = modified or (new_line != line)
-        new_file.write(new_line)
-  os.close(fh)
-  if modified:
-    #Remove original file
-    os.remove(file_path)
-    #Move new file
-    shutil.move(abs_path, file_path)
-    return True
-  else:
-    # nothing to do - just remove the tmp file
-    os.remove(abs_path)
-    return False
-
-# Moves the Version.java file from a snapshot to a release
-def remove_version_snapshot(version_file, release):
-  # 1.0.0.Beta1 -> 1_0_0_Beta1
-  release = release.replace('.', '_')
-  release = release.replace('-', '_')
-  pattern = 'new Version(V_%s_ID, true' % (release)
-  replacement = 'new Version(V_%s_ID, false' % (release)
-  def callback(line):
-    return line.replace(pattern, replacement)
-  processed = process_file(version_file, callback)
-  if not processed:
-    raise RuntimeError('failed to remove snapshot version for %s' % (release))
-
-# Checks the pom.xml for the release version.
-# This method fails if the pom file has no SNAPSHOT version set ie.
-# if the version is already on a release version we fail.
-# Returns the next version string ie. 0.90.7
-def find_release_version():
-  with open('pom.xml', encoding='utf-8') as file:
-    for line in file:
-      match = re.search(r'<version>(.+)-SNAPSHOT</version>', line)
-      if match:
-        return match.group(1)
-    raise RuntimeError('Could not find release version in branch')
-
-
-if __name__ == "__main__":
-  release_version = find_release_version()
-
-  print('*** Preparing release version: [%s]' % release_version)
-
-  ensure_checkout_is_clean()
-
-  run('cd dev-tools && mvn versions:set -DnewVersion=%s -DgenerateBackupPoms=false' % (release_version))
-  run('cd rest-api-spec && mvn versions:set -DnewVersion=%s -DgenerateBackupPoms=false' % (release_version))
-  run('mvn versions:set -DnewVersion=%s -DgenerateBackupPoms=false' % (release_version))
-
-  remove_version_snapshot(VERSION_FILE, release_version)
-
-  print('*** Done removing snapshot version. DO NOT COMMIT THIS, WHEN CREATING A RELEASE CANDIDATE.')
-
-  shortHash = subprocess.check_output('git log --pretty=format:"%h" -n 1', shell=True).decode('utf-8')
-  localRepo = '/tmp/elasticsearch-%s-%s' % (release_version, shortHash)
-  localRepoElasticsearch = localRepo + '/org/elasticsearch'
-  print('')
-  print('*** To create a release candidate run: ')
-  print('  mvn clean install deploy -Prelease -DskipTests -Dgpg.keyname="D88E42B4" -Dpackaging.rpm.rpmbuild=/usr/bin/rpmbuild -Drpm.sign=true -Dmaven.repo.local=%s -Dno.commit.pattern="\\bno(n|)commit\\b" -Dforbidden.test.signatures=""' % (localRepo))
-  print('  1. Remove all _remote.repositories: find %s -name _remote.repositories -exec rm {} \;' % (localRepoElasticsearch))
-  print('  2. Rename all maven metadata files: for i in $(find %s -name "maven-metadata-local.xml*") ; do mv "$i" "${i/-local/}" ; done' % (localRepoElasticsearch))
-  print('  3. Sync %s into S3 bucket' % (localRepoElasticsearch))
-  print ('    s3cmd sync %s s3://download.elasticsearch.org/elasticsearch/staging/elasticsearch-%s-%s/maven/org/' % (localRepoElasticsearch, release_version, shortHash))
-  print('  4. Create repositories: ')
-  print ('    export S3_BUCKET_SYNC_TO="download.elasticsearch.org/elasticsearch/staging/elasticsearch-%s-%s/repos"' % (release_version, shortHash))
-  print ('    export S3_BUCKET_SYNC_FROM="$S3_BUCKET_SYNC_TO"')
-  print('     dev-tools/build_repositories.sh %s' % (release_version))
-  print('')
-  print('NOTE: the above mvn command will promt you several times for the GPG passphrase of the key you specified you can alternatively pass it via -Dgpg.passphrase=yourPassPhrase')
-  print('NOTE: Running s3cmd might require you to create a config file with your credentials, if the s3cmd does not support suppliying them via the command line!')
diff --git a/dev-tools/src/main/resources/ant/integration-tests.xml b/dev-tools/src/main/resources/ant/integration-tests.xml
index f5e87c0..b5439cb 100644
--- a/dev-tools/src/main/resources/ant/integration-tests.xml
+++ b/dev-tools/src/main/resources/ant/integration-tests.xml
@@ -87,8 +87,6 @@
       <run-script script="@{home}/bin/plugin">
         <nested>
           <arg value="install"/>
-          <arg value="@{name}"/>
-          <arg value="-u"/>
           <arg value="${url}"/>
         </nested>
       </run-script>
@@ -138,8 +136,7 @@
       <attribute name="home" default="${integ.scratch}/elasticsearch-${elasticsearch.version}"/>
       <attribute name="spawn" default="true"/>
       <attribute name="args" default="${integ.args}"/>
-      <attribute name="es.unicast.enabled" default="false"/>
-      <attribute name="es.unicast.hosts" default=""/>
+      <attribute name="es.unicast.hosts" default="localhost:${integ.transport.port}"/>
       <attribute name="es.cluster.name" default="${integ.cluster.name}"/>
       <attribute name="es.http.port" default="${integ.http.port}"/>
       <attribute name="es.transport.tcp.port" default="${integ.transport.port}"/>
@@ -147,6 +144,14 @@
       <attribute name="jvm.args" default="${tests.jvm.argline}"/>
       <element name="additional-args" optional="true"/>
     <sequential>
+      <!-- make sure no elasticsearch instance is currently running and listening on the port we need -->
+      <fail message="This test expects port @{es.http.port} to be free but an elasticsearch instance is already running and listening on that port.
+      Maybe the last test run did not manage to shut down the node correctly?
+      You must kill it before tests can run.">
+        <condition>
+          <socket server="localhost" port="@{es.http.port}"></socket>
+        </condition>
+      </fail>
       <!-- run bin/elasticsearch with args -->
       <echo>Starting up external cluster...</echo>
 
@@ -160,11 +165,9 @@
           <arg value="-Des.http.port=@{es.http.port}"/>
           <arg value="-Des.transport.tcp.port=@{es.transport.tcp.port}"/>
           <arg value="-Des.pidfile=@{es.pidfile}"/>
-          <arg value="-Des.discovery.zen.ping.unicast.enabled=@{es.unicast.enabled}"/>
           <arg value="-Des.discovery.zen.ping.unicast.hosts=@{es.unicast.hosts}"/>
           <arg value="-Des.path.repo=@{home}/repo"/>
           <arg value="-Des.path.shared_data=@{home}/../"/>
-          <arg value="-Des.discovery.zen.ping.multicast.enabled=false"/>
           <arg value="-Des.script.inline=on"/>
           <arg value="-Des.script.indexed=on"/>
           <arg value="-Des.repositories.url.allowed_urls=http://snapshot.test*"/>
@@ -237,7 +240,7 @@
     <attribute name="es.pidfile" default="${integ.pidfile}"/>
     <attribute name="es.peer.list" />
     <sequential>
-    <startup-elasticsearch es.pidfile="@{es.pidfile}" es.unicast.enabled="true"
+    <startup-elasticsearch es.pidfile="@{es.pidfile}"
                            es.transport.tcp.port="@{es.transport.port}" es.http.port="@{es.http.port}"
                            es.unicast.hosts="@{es.peer.list}"/>
     </sequential>
diff --git a/dev-tools/src/main/resources/forbidden/all-signatures.txt b/dev-tools/src/main/resources/forbidden/all-signatures.txt
index e61d58d..00d4871 100644
--- a/dev-tools/src/main/resources/forbidden/all-signatures.txt
+++ b/dev-tools/src/main/resources/forbidden/all-signatures.txt
@@ -59,3 +59,30 @@ java.nio.file.Files#isHidden(java.nio.file.Path) @ Dependent on the operating sy
 
 java.nio.file.Files#getFileStore(java.nio.file.Path) @ Use Environment.getFileStore() instead, impacted by JDK-8034057
 java.nio.file.Files#isWritable(java.nio.file.Path) @ Use Environment.isWritable() instead, impacted by JDK-8034057
+
+@defaultMessage Resolve hosts explicitly to the address(es) you want with InetAddress.
+java.net.InetSocketAddress#<init>(java.lang.String,int)
+java.net.Socket#<init>(java.lang.String,int)
+java.net.Socket#<init>(java.lang.String,int,java.net.InetAddress,int)
+
+@defaultMessage Don't bind to wildcard addresses. Be specific.
+java.net.DatagramSocket#<init>()
+java.net.DatagramSocket#<init>(int)
+java.net.InetSocketAddress#<init>(int)
+java.net.MulticastSocket#<init>()
+java.net.MulticastSocket#<init>(int)
+java.net.ServerSocket#<init>(int)
+java.net.ServerSocket#<init>(int,int)
+
+@defaultMessage use NetworkAddress format/formatAddress to print IP or IP+ports
+java.net.InetAddress#toString()
+java.net.InetAddress#getHostAddress()
+java.net.Inet4Address#getHostAddress()
+java.net.Inet6Address#getHostAddress()
+java.net.InetSocketAddress#toString()
+
+@defaultMessage avoid DNS lookups by accident: if you have a valid reason, then @SuppressWarnings with that reason so its completely clear
+java.net.InetAddress#getHostName()
+java.net.InetAddress#getCanonicalHostName()
+
+java.net.InetSocketAddress#getHostName() @ Use getHostString() instead, which avoids a DNS lookup
diff --git a/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties b/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties
index 26bee6d..67d139e 100644
--- a/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties
+++ b/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties
@@ -36,6 +36,9 @@ description=${project.description}
 # 'version': plugin's version
 version=${project.version}
 #
+# 'name': the plugin name
+name=${elasticsearch.plugin.name}
+
 ### mandatory elements for site plugins:
 #
 # 'site': set to true to indicate contents of the _site/
@@ -55,6 +58,9 @@ jvm=${elasticsearch.plugin.jvm}
 classname=${elasticsearch.plugin.classname}
 #
 # 'java.version' version of java the code is built against
+# use the system property java.specification.version
+# version string must be a sequence of nonnegative decimal integers
+# separated by "."'s and may have leading zeros
 java.version=${maven.compiler.target}
 #
 # 'elasticsearch.version' version of elasticsearch compiled against
diff --git a/dev-tools/upgrade-tests.py b/dev-tools/upgrade-tests.py
index 0c18d4d..69bf1cf 100644
--- a/dev-tools/upgrade-tests.py
+++ b/dev-tools/upgrade-tests.py
@@ -105,8 +105,7 @@ def start_node(version, data_dir, node_dir, unicast_host_list, tcp_port, http_po
     foreground = ''
   return subprocess.Popen([es_run_path,
     '-Des.path.data=%s' % data_dir, '-Des.cluster.name=upgrade_test',  
-    '-Des.discovery.zen.ping.unicast.hosts=%s' % unicast_host_list, 
-    '-Des.discovery.zen.ping.multicast.enabled=false',
+    '-Des.discovery.zen.ping.unicast.hosts=%s' % unicast_host_list,
     '-Des.transport.tcp.port=%s' % tcp_port,
     '-Des.http.port=%s' % http_port,
     foreground], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
diff --git a/distribution/pom.xml b/distribution/pom.xml
index 52969ee..41a1729 100644
--- a/distribution/pom.xml
+++ b/distribution/pom.xml
@@ -33,9 +33,6 @@
         <project.licenses.dir>${project.basedir}/../licenses</project.licenses.dir>
         <project.licenses.check_target>${integ.scratch}</project.licenses.check_target>
 
-        <!-- rpmbuild location : default to /usr/bin/rpmbuild -->
-        <packaging.rpm.rpmbuild>/usr/bin/rpmbuild</packaging.rpm.rpmbuild>
-
         <!-- we expect packaging formats to have integration tests, but not unit tests -->
         <skip.unit.tests>true</skip.unit.tests>
     </properties>
@@ -172,6 +169,11 @@
     </modules>
 
     <profiles>
+        <!--
+            We include automatically RPM module when it's available in common locations.
+            If your rpmbuild is in another location (but in path), run maven with rpm profile:
+             mvn deploy -Prpm
+        -->
         <profile>
             <id>macos_brew</id>
             <activation>
@@ -190,7 +192,7 @@
             <activation>
                 <file>
                     <!-- Folks having /usr/bin/rpmbuild available will be able to build the rpm module -->
-                    <exists>${packaging.rpm.rpmbuild}</exists>
+                    <exists>/usr/bin/rpmbuild</exists>
                 </file>
             </activation>
             <modules>
diff --git a/distribution/src/main/resources/config/elasticsearch.yml b/distribution/src/main/resources/config/elasticsearch.yml
index b3baf76..b1b1122 100644
--- a/distribution/src/main/resources/config/elasticsearch.yml
+++ b/distribution/src/main/resources/config/elasticsearch.yml
@@ -71,13 +71,10 @@
 #
 # --------------------------------- Discovery ----------------------------------
 #
-# Elasticsearch nodes will find each other via multicast, by default.
-#
-# To use the unicast discovery, disable the multicast discovery:
-#
-# discovery.zen.ping.multicast.enabled: false
+# Elasticsearch nodes will find each other via unicast, by default.
 #
 # Pass an initial list of hosts to perform discovery when new node is started:
+# The default list of hosts is ["127.0.0.1", "[::1]"]
 #
 # discovery.zen.ping.unicast.hosts: ["host1", "host2"]
 #
diff --git a/docs/java-api/client.asciidoc b/docs/java-api/client.asciidoc
index 4a747b7..cfc45b7 100644
--- a/docs/java-api/client.asciidoc
+++ b/docs/java-api/client.asciidoc
@@ -152,8 +152,8 @@ be "two hop" operations).
 // on startup
 
 Client client = TransportClient.builder().build()
-        .addTransportAddress(new InetSocketTransportAddress("host1", 9300))
-        .addTransportAddress(new InetSocketTransportAddress("host2", 9300));
+        .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("host1"), 9300))
+        .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("host2"), 9300));
 
 // on shutdown
 
diff --git a/docs/plugins/delete-by-query.asciidoc b/docs/plugins/delete-by-query.asciidoc
index a422cc3..a207ae5 100644
--- a/docs/plugins/delete-by-query.asciidoc
+++ b/docs/plugins/delete-by-query.asciidoc
@@ -6,7 +6,7 @@ The delete-by-query plugin adds support for deleting all of the documents
 replacement for the problematic _delete-by-query_ functionality which has been
 removed from Elasticsearch core.
 
-Internally, it uses the {ref}/search-request-scroll.html#scroll-scan[Scan/Scroll]
+Internally, it uses {ref}/search-request-scroll.html[Scroll]
 and {ref}/docs-bulk.html[Bulk] APIs to delete documents in an efficient and
 safe manner. It is slower than the old _delete-by-query_ functionality, but
 fixes the problems with the previous implementation.
@@ -101,7 +101,7 @@ See {ref}/search-uri-request.html[URI search request] for details.
 
 `size`::
 
-The number of hits returned *per shard* by the {ref}/search-request-scroll.html#scroll-scan[scan]
+The number of hits returned by the {ref}/search-request-scroll.html[scroll]
 request.  Defaults to 10.  May also be specified in the request body.
 
 `timeout`::
@@ -148,7 +148,7 @@ The JSON response looks like this:
 --------------------------------------------------
 
 Internally, the query is used to execute an initial
-{ref}/search-request-scroll.html#scroll-scan[scroll/scan] request. As hits are
+{ref}/search-request-scroll.html[scroll] request. As hits are
 pulled from the scroll API, they are passed to the {ref}/docs-bulk.html[Bulk
 API] for deletion.
 
@@ -157,7 +157,7 @@ was visible to search at the time the request was executed.  Any documents
 that have been reindexed or updated during execution will not be deleted.
 
 Since documents can be updated or deleted by external operations during the
-_scan-scroll-bulk_ process, the plugin keeps track of different counters for
+_scroll-bulk_ process, the plugin keeps track of different counters for
 each index, with the totals displayed under the `_all` index.  The counters
 are as follows:
 
@@ -212,7 +212,7 @@ Resiliency::
 === New delete-by-query implementation
 
 The new implementation, provided by this plugin, is built internally
-using  {ref}/search-request-scroll.html#scroll-scan[scan and scroll] to return
+using  {ref}/search-request-scroll.html[scroll] to return
 the document IDs and versions of all the documents that need to be deleted.
 It then uses  the {ref}/docs-bulk.html[`bulk` API] to do the actual deletion.
 
@@ -231,8 +231,8 @@ try-once::
 
 syntactic sugar::
 
-    A delete-by-query is equivalent to a scan/scroll search and corresponding
-    bulk-deletes by ID.
+    A delete-by-query is equivalent to a scroll search ordered by `_doc` and
+    corresponding bulk-deletes by ID.
 
 point-in-time::
 
@@ -267,4 +267,4 @@ move the functionality to a plugin instead of replacing the feautre in core:
 * There is currently no way to monitor or cancel a running delete-by-query
   request, except for the `timeout` parameter.
 
-We have plans to solve both of these issues in a later version of Elasticsearch.
\ No newline at end of file
+We have plans to solve both of these issues in a later version of Elasticsearch.
diff --git a/docs/plugins/discovery-multicast.asciidoc b/docs/plugins/discovery-multicast.asciidoc
new file mode 100644
index 0000000..75acbd8
--- /dev/null
+++ b/docs/plugins/discovery-multicast.asciidoc
@@ -0,0 +1,55 @@
+[[discovery-multicast]]
+=== Multicast Discovery Plugin
+
+The Multicast Discovery plugin provides the ability to form a cluster using
+TCP/IP multicast messages.
+
+[[discovery-multicast-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install discovery-multicast
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[discovery-multicast-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove discovery-multicast
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[discovery-multicast-usage]]
+==== Configuring multicast discovery
+
+Multicast ping discovery of other nodes is done by sending one or more
+multicast requests which existing nodes will receive and
+respond to. It provides the following settings with the
+`discovery.zen.ping.multicast` prefix:
+
+[cols="<,<",options="header",]
+|=======================================================================
+|Setting |Description
+|`group` |The group address to use. Defaults to `224.2.2.4`.
+
+|`port` |The port to use. Defaults to `54328`.
+
+|`ttl` |The ttl of the multicast message. Defaults to `3`.
+
+|`address` |The address to bind to, defaults to `null` which means it
+will bind `network.bind_host`
+
+|`enabled` |Whether multicast ping discovery is enabled. Defaults to `false`.
+|=======================================================================
diff --git a/docs/plugins/discovery.asciidoc b/docs/plugins/discovery.asciidoc
index 723d900..3b80ecc 100644
--- a/docs/plugins/discovery.asciidoc
+++ b/docs/plugins/discovery.asciidoc
@@ -26,6 +26,10 @@ support for using Azure as a repository for
 
 The Google Compute Engine Cloud plugin uses the GCE API for unicast discovery.
 
+<<discovery-multicast,Multicast>>::
+
+The multicast plugin sends multicast messages to discover other nodes in the cluster.
+
 [float]
 ==== Community contributed discovery plugins
 
@@ -41,5 +45,7 @@ include::cloud-azure.asciidoc[]
 
 include::cloud-gce.asciidoc[]
 
+include::discovery-multicast.asciidoc[]
+
 
 
diff --git a/docs/plugins/plugin-script.asciidoc b/docs/plugins/plugin-script.asciidoc
index 06263d7..fc1c91c 100644
--- a/docs/plugins/plugin-script.asciidoc
+++ b/docs/plugins/plugin-script.asciidoc
@@ -84,15 +84,15 @@ A plugin can also be downloaded directly from a custom location by specifying th
 
 [source,shell]
 -----------------------------------
-sudo bin/plugin install [plugin-name] --url [url] <1>
+sudo bin/plugin install [url] <1>
 -----------------------------------
-<1> Both the URL and the plugin name must be specified.
+<1> must be a valid URL, the plugin name is determined from its descriptor.
 
 For instance, to install a plugin from your local file system, you could run:
 
 [source,shell]
 -----------------------------------
-sudo bin/plugin install my_plugin --url file:/path/to/plugin.zip
+sudo bin/plugin install file:/path/to/plugin.zip
 -----------------------------------
 
 [[listing-removing]]
diff --git a/docs/reference/aggregations/pipeline/avg-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/avg-bucket-aggregation.asciidoc
index 8311f58..bbd540d 100644
--- a/docs/reference/aggregations/pipeline/avg-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/avg-bucket-aggregation.asciidoc
@@ -52,13 +52,13 @@ The following snippet calculates the average of the total monthly `sales`:
         },
         "avg_monthly_sales": {
             "avg_bucket": {
-                "buckets_paths": "sales_per_month>sales" <1>
+                "buckets_path": "sales_per_month>sales" <1>
             }
         }
     }
 }
 --------------------------------------------------
-<1> `bucket_paths` instructs this avg_bucket aggregation that we want the (mean) average value of the `sales` aggregation in the 
+<1> `buckets_path` instructs this avg_bucket aggregation that we want the (mean) average value of the `sales` aggregation in the 
 `sales_per_month` date histogram.
 
 And the following may be the response:
diff --git a/docs/reference/aggregations/pipeline/derivative-aggregation.asciidoc b/docs/reference/aggregations/pipeline/derivative-aggregation.asciidoc
index 9358040..ec63600 100644
--- a/docs/reference/aggregations/pipeline/derivative-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/derivative-aggregation.asciidoc
@@ -54,7 +54,7 @@ The following snippet calculates the derivative of the total monthly `sales`:
                 },
                 "sales_deriv": {
                     "derivative": {
-                        "buckets_paths": "sales" <1>
+                        "buckets_path": "sales" <1>
                     }
                 }
             }
@@ -63,7 +63,7 @@ The following snippet calculates the derivative of the total monthly `sales`:
 }
 --------------------------------------------------
 
-<1> `bucket_paths` instructs this derivative aggregation to use the output of the `sales` aggregation for the derivative
+<1> `buckets_path` instructs this derivative aggregation to use the output of the `sales` aggregation for the derivative
 
 And the following may be the response:
 
@@ -137,12 +137,12 @@ monthly sales:
                 },
                 "sales_deriv": {
                     "derivative": {
-                        "buckets_paths": "sales"
+                        "buckets_path": "sales"
                     }
                 },
                 "sales_2nd_deriv": {
                     "derivative": {
-                        "buckets_paths": "sales_deriv" <1>
+                        "buckets_path": "sales_deriv" <1>
                     }
                 }
             }
@@ -151,7 +151,7 @@ monthly sales:
 }
 --------------------------------------------------
 
-<1> `bucket_paths` for the second derivative points to the name of the first derivative
+<1> `buckets_path` for the second derivative points to the name of the first derivative
 
 And the following may be the response:
 
@@ -225,7 +225,7 @@ of the total sales per month but ask for the derivative of the sales as in the u
                 },
                 "sales_deriv": {
                     "derivative": {
-                        "buckets_paths": "sales",
+                        "buckets_path": "sales",
                         "unit": "day" <1>
                     }
                 }
diff --git a/docs/reference/getting-started.asciidoc b/docs/reference/getting-started.asciidoc
index b1d40ea..4d95786 100755
--- a/docs/reference/getting-started.asciidoc
+++ b/docs/reference/getting-started.asciidoc
@@ -203,7 +203,7 @@ We can see that our cluster named "elasticsearch" is up with a green status.
 
 Whenever we ask for the cluster health, we either get green, yellow, or red. Green means everything is good (cluster is fully functional), yellow means all data is available but some replicas are not yet allocated (cluster is fully functional), and red means some data is not available for whatever reason. Note that even if a cluster is red, it still is partially functional (i.e. it will continue to serve search requests from the available shards) but you will likely need to fix it ASAP since you have missing data.
 
-Also from the above response, we can see and total of 1 node and that we have 0 shards since we have no data in it yet. Note that since we are using the default cluster name (elasticsearch) and since Elasticsearch uses multicast network discovery by default to find other nodes, it is possible that you could accidentally start up more than one node in your network and have them all join a single cluster. In this scenario, you may see more than 1 node in the above response.
+Also from the above response, we can see and total of 1 node and that we have 0 shards since we have no data in it yet. Note that since we are using the default cluster name (elasticsearch) and since Elasticsearch uses unicast network discovery by default to find other nodes on the same machine, it is possible that you could accidentally start up more than one node on your computer and have them all join a single cluster. In this scenario, you may see more than 1 node in the above response.
 
 We can also get a list of nodes in our cluster as follows:
 
diff --git a/docs/reference/glossary.asciidoc b/docs/reference/glossary.asciidoc
index 6f7061f..09fcd5b 100644
--- a/docs/reference/glossary.asciidoc
+++ b/docs/reference/glossary.asciidoc
@@ -86,9 +86,8 @@
   server for testing purposes, but usually you should have one node per
   server.
   +
-  At startup, a node will use unicast (or multicast, if specified) to
-  discover an existing cluster with the same cluster name and will try
-  to join that cluster.
+  At startup, a node will use unicast to discover an existing cluster with
+  the same cluster name and will try to join that cluster.
 
  [[glossary-primary-shard]] primary shard ::
 
diff --git a/docs/reference/indices/templates.asciidoc b/docs/reference/indices/templates.asciidoc
index 3a5a3c5..ca09a87 100644
--- a/docs/reference/indices/templates.asciidoc
+++ b/docs/reference/indices/templates.asciidoc
@@ -1,7 +1,7 @@
 [[indices-templates]]
 == Index Templates
 
-Index templates allow to define templates that will automatically be
+Index templates allow you to define templates that will automatically be
 applied to new indices created. The templates include both settings and
 mappings, and a simple pattern template that controls if the template
 will be applied to the index created. For example:
diff --git a/docs/reference/migration/index.asciidoc b/docs/reference/migration/index.asciidoc
index ebfad2a..19a3e1f 100644
--- a/docs/reference/migration/index.asciidoc
+++ b/docs/reference/migration/index.asciidoc
@@ -16,6 +16,7 @@ As a general rule:
 
 See <<setup-upgrade>> for more info.
 --
+include::migrate_2_1.asciidoc[]
 
 include::migrate_2_0.asciidoc[]
 
diff --git a/docs/reference/migration/migrate_2_0/removals.asciidoc b/docs/reference/migration/migrate_2_0/removals.asciidoc
index afdc109..f02bf3a 100644
--- a/docs/reference/migration/migrate_2_0/removals.asciidoc
+++ b/docs/reference/migration/migrate_2_0/removals.asciidoc
@@ -27,8 +27,8 @@ The old delete-by-query functionality was fast but unsafe.  It could lead to
 document differences between the primary and replica shards, and could even
 produce out of memory exceptions and cause the cluster to crash.
 
-This feature has been reimplemented using the <<scroll-scan,scroll/scan>> and
-the <<docs-bulk,`bulk`>> API, which may be slower for queries which match
+This feature has been reimplemented using the <<search-request-scroll,scroll>> and
+<<docs-bulk,`bulk`>> APIs, which may be slower for queries which match
 large numbers of documents, but is safe.
 
 Currently, a long running delete-by-query job cannot be cancelled, which is
diff --git a/docs/reference/migration/migrate_2_0/stats.asciidoc b/docs/reference/migration/migrate_2_0/stats.asciidoc
index 46f3c68..84635f5 100644
--- a/docs/reference/migration/migrate_2_0/stats.asciidoc
+++ b/docs/reference/migration/migrate_2_0/stats.asciidoc
@@ -55,3 +55,7 @@ headers by default. Verbosity can be turned off with the `v` parameter:
 GET _cat/shards?v=0
 -----------------
 
+==== Nodes Stats API
+
+Queue lengths are now reported as basic numeric so they can easily processed by code. Before we used a human
+readable format. For example, a queue with 1,000 items is now reported as `1000` instead of `1k`.
diff --git a/docs/reference/migration/migrate_2_1.asciidoc b/docs/reference/migration/migrate_2_1.asciidoc
new file mode 100644
index 0000000..7542fb3
--- /dev/null
+++ b/docs/reference/migration/migrate_2_1.asciidoc
@@ -0,0 +1,27 @@
+[[breaking-changes-2.1]]
+== Breaking changes in 2.1
+
+This section discusses the changes that you need to be aware of when migrating
+your application to Elasticsearch 2.1.
+
+=== Search changes
+
+==== `search_type=scan` deprecated
+
+The `scan` search type has been deprecated. All benefits from this search
+type can now be achieved by doing a scroll request that sorts documents in
+`_doc` order, for instance:
+
+[source,sh]
+---------------
+GET /my_index/_search?scroll=2m
+{
+  "sort": [
+    "_doc"
+  ]
+}
+---------------
+
+Scroll requests sorted by `_doc` have been optimized to more efficiently resume
+from where the previous request stopped, so this will have the same performance
+characteristics as the former `scan` search type.
diff --git a/docs/reference/modules/discovery/zen.asciidoc b/docs/reference/modules/discovery/zen.asciidoc
index 8f0bd1f..fa5ad6a 100644
--- a/docs/reference/modules/discovery/zen.asciidoc
+++ b/docs/reference/modules/discovery/zen.asciidoc
@@ -2,8 +2,8 @@
 === Zen Discovery
 
 The zen discovery is the built in discovery module for elasticsearch and
-the default. It provides both multicast and unicast discovery as well
-being easily extended to support cloud environments.
+the default. It provides unicast discovery, but can be extended to
+support cloud environments and other forms of discovery.
 
 The zen discovery is integrated with other modules, for example, all
 communication between nodes is done using the
@@ -16,39 +16,13 @@ It is separated into several sub modules, which are explained below:
 ==== Ping
 
 This is the process where a node uses the discovery mechanisms to find
-other nodes. There is support for both multicast and unicast based
-discovery (these mechanisms can be used in conjunction as well).
-
-[float]
-[[multicast]]
-===== Multicast
-
-Multicast ping discovery of other nodes is done by sending one or more
-multicast requests which existing nodes will receive and
-respond to. It provides the following settings with the
-`discovery.zen.ping.multicast` prefix:
-
-[cols="<,<",options="header",]
-|=======================================================================
-|Setting |Description
-|`group` |The group address to use. Defaults to `224.2.2.4`.
-
-|`port` |The port to use. Defaults to `54328`.
-
-|`ttl` |The ttl of the multicast message. Defaults to `3`.
-
-|`address` |The address to bind to, defaults to `null` which means it
-will bind `network.bind_host`
-
-|`enabled` |Whether multicast ping discovery is enabled. Defaults to `true`.
-|=======================================================================
+other nodes.
 
 [float]
 [[unicast]]
 ===== Unicast
 
-The unicast discovery allows for discovery when multicast is
-not enabled. It basically requires a list of hosts to use that will act
+The unicast discovery requires a list of hosts to use that will act
 as gossip routers. It provides the following settings with the
 `discovery.zen.ping.unicast` prefix:
 
@@ -57,7 +31,8 @@ as gossip routers. It provides the following settings with the
 |Setting |Description
 |`hosts` |Either an array setting or a comma delimited setting. Each
 value is either in the form of `host:port`, or in the form of
-`host[port1-port2]`.
+`host:port1-port2`. Note that IPv6 hosts must be bracketed. Defaults to
+`127.0.0.1, [::1]`
 |=======================================================================
 
 The unicast discovery uses the
@@ -128,45 +103,6 @@ considered failed. Defaults to `3`.
 |=======================================================================
 
 [float]
-==== External Multicast
-
-The multicast discovery also supports external multicast requests to
-discover nodes. The external client can send a request to the multicast
-IP/group and port, in the form of:
-
-[source,js]
---------------------------------------------------
-{
-    "request" : {
-        "cluster_name": "test_cluster"
-    }
-}
---------------------------------------------------
-
-And the response will be similar to node info response (with node level
-information only, including transport/http addresses, and node
-attributes):
-
-[source,js]
---------------------------------------------------
-{
-    "response" : {
-        "cluster_name" : "test_cluster",
-        "transport_address" : "...",
-        "http_address" : "...",
-        "attributes" : {
-            "..."
-        }
-    }
-}
---------------------------------------------------
-
-Note, it can still be enabled, with disabled internal multicast
-discovery, but still have external discovery working by keeping
-`discovery.zen.ping.multicast.enabled` set to `true` (the default), but,
-setting `discovery.zen.ping.multicast.ping.enabled` to `false`.
-
-[float]
 ==== Cluster state updates
 
 The master node is the only node in a cluster that can make changes to the
diff --git a/docs/reference/modules/snapshots.asciidoc b/docs/reference/modules/snapshots.asciidoc
index 86fd952..32f412e 100644
--- a/docs/reference/modules/snapshots.asciidoc
+++ b/docs/reference/modules/snapshots.asciidoc
@@ -81,7 +81,7 @@ a prefix and back slashes are properly escaped:
 
 [source,yaml]
 --------------
-repo.path: ["\\\\MY_SERVER\\Snapshots"]
+path.repo: ["\\\\MY_SERVER\\Snapshots"]
 --------------
 
 After all nodes are restarted, the following command can be used to register the shared file system repository with
@@ -141,7 +141,7 @@ This setting supports wildcards in the place of host, path, query, and fragment.
 repositories.url.allowed_urls: ["http://www.example.org/root/*", "https://*.mydomain.com/*?*#*"]
 -----------------------------------
 
-URL repositories with `file:` URLs can only point to locations registered in the `repo.path` setting similiar to
+URL repositories with `file:` URLs can only point to locations registered in the `path.repo` setting similiar to
 shared file system repository.
 
 [float]
diff --git a/docs/reference/modules/tribe.asciidoc b/docs/reference/modules/tribe.asciidoc
index c329674..6164666 100644
--- a/docs/reference/modules/tribe.asciidoc
+++ b/docs/reference/modules/tribe.asciidoc
@@ -25,7 +25,7 @@ tribe:
 
 The example above configures connections to two clusters, name `t1` and `t2`
 respectively.  The tribe node will create a <<modules-node,node client>> to
-connect each cluster using <<multicast,multicast discovery>> by default. Any
+connect each cluster using <<unicast,unicast discovery>> by default. Any
 other settings for the connection can be configured under `tribe.{name}`, just
 like the `cluster.name` in the example.
 
diff --git a/docs/reference/search/request-body.asciidoc b/docs/reference/search/request-body.asciidoc
index 6c7d127..1469073 100644
--- a/docs/reference/search/request-body.asciidoc
+++ b/docs/reference/search/request-body.asciidoc
@@ -64,7 +64,7 @@ And here is a sample response:
 `search_type`::
 
     The type of the search operation to perform. Can be
-    `dfs_query_then_fetch`, `query_then_fetch`, or 'scan'.
+    `dfs_query_then_fetch` or `query_then_fetch`.
     Defaults to `query_then_fetch`.
     See <<search-request-search-type,_Search Type_>> for more.
 
diff --git a/docs/reference/search/request/scroll.asciidoc b/docs/reference/search/request/scroll.asciidoc
index 338d5d0..2ad1f57 100644
--- a/docs/reference/search/request/scroll.asciidoc
+++ b/docs/reference/search/request/scroll.asciidoc
@@ -90,59 +90,20 @@ used.
 NOTE: If the request specifies aggregations, only the initial search response
 will contain the aggregations results.
 
-[[scroll-scan]]
-==== Efficient scrolling with Scroll-Scan
-
-Deep pagination with <<search-request-from-size,`from` and `size`>> -- e.g.
-`?size=10&from=10000` -- is very inefficient as (in this example) 100,000
-sorted results have to be retrieved from each shard and resorted in order to
-return just 10 results.  This process has to be repeated for every page
-requested.
-
-The `scroll` API keeps track of which results have already been returned and
-so is able to return sorted results more efficiently than with deep
-pagination.  However, sorting results (which happens by default) still has a
-cost.
-
-Normally, you just want to retrieve all results and the order doesn't matter.
-Scrolling can be combined with the <<scan,`scan`>> search type to disable
-any scoring or sorting and to return results in the most efficient way
-possible.  All that is needed is to add `search_type=scan` to the query string
-of the initial search request:
+NOTE: Scroll requests have optimizations that make them faster when the sort
+order is `_doc`. If you want to iterate over all documents regardless of the
+order, this is the most efficient option:
 
 [source,js]
 --------------------------------------------------
-curl 'localhost:9200/twitter/tweet/_search?scroll=1m&search_type=scan' <1> -d '
+curl -XGET 'localhost:9200/_search?scroll=1m' -d '
 {
-    "query": {
-        "match" : {
-            "title" : "elasticsearch"
-        }
-    }
+  "sort": [
+    "_doc"
+  }
 }
 '
 --------------------------------------------------
-<1> Setting `search_type` to `scan` disables sorting and makes scrolling
-    very efficient.
-
-A scanning scroll request differs from a standard scroll request in four
-ways:
-
-* No score is calculated and sorting is disabled. Results are returned in
-  the order they appear in the index.
-
-* Aggregations are not supported.
-
-* The response of the initial `search` request will not contain any results in
-  the `hits` array. The first results will be returned by the first `scroll`
-  request.
-
-* The <<search-request-from-size,`size` parameter>> controls the number of
-  results *per shard*, not per request, so a `size` of `10` which hits 5
-  shards will return a maximum of 50 results per `scroll` request.
-
-If you want the scoring to happen, even without sorting on it, set the
-`track_scores` parameter to `true`.
 
 [[scroll-search-context]]
 ==== Keeping the search context alive
@@ -176,9 +137,11 @@ curl -XGET localhost:9200/_nodes/stats/indices/search?pretty
 
 ==== Clear scroll API
 
-Search contexts are removed automatically either when all results have been
-retrieved or when the `scroll` timeout has been exceeded.  However, you can
-clear a search context manually with the `clear-scroll` API:
+Search context are automatically removed when the `scroll` timeout has been
+exceeded. However keeping scrolls open has a cost, as discussed in the
+<<scroll-search-context,previous section>> so scrolls should be explicitly
+cleared as soon as the scroll is not being used anymore using the
+`clear-scroll` API:
 
 [source,js]
 ---------------------------------------
diff --git a/docs/reference/search/request/search-type.asciidoc b/docs/reference/search/request/search-type.asciidoc
index 6ad75dc..ab2d916 100644
--- a/docs/reference/search/request/search-type.asciidoc
+++ b/docs/reference/search/request/search-type.asciidoc
@@ -26,8 +26,8 @@ each shard using these global frequencies.
 Also, because of the need to sort the results, getting back a large
 document set, or even scrolling it, while maintaining the correct sorting
 behavior can be a very expensive operation. For large result set
-scrolling without sorting, the `scan` search type (explained below) is
-also available.
+scrolling, it is best to sort by `_doc` if the order in which documents
+are returned is not important.
 
 Elasticsearch is very flexible and allows to control the type of search
 to execute on a *per search request* basis. The type can be configured
@@ -77,9 +77,11 @@ API as it provides more options.
 [[scan]]
 ==== Scan
 
+deprecated[2.1.0, `scan` does not provide any benefits over a regular `scroll` request sorted by `_doc`]
+
 Parameter value: *scan*.
 
 The `scan` search type disables sorting in order to allow very efficient
-scrolling through large result sets.  See <<scroll-scan>> for more.
+scrolling through large result sets.
 
 
diff --git a/docs/reference/search/request/sort.asciidoc b/docs/reference/search/request/sort.asciidoc
index 0a8f368..1cc07e2 100644
--- a/docs/reference/search/request/sort.asciidoc
+++ b/docs/reference/search/request/sort.asciidoc
@@ -3,7 +3,7 @@
 
 Allows to add one or more sort on specific fields. Each sort can be
 reversed as well. The sort is defined on a per field level, with special
-field name for `_score` to sort by score.
+field name for `_score` to sort by score, and `_doc` to sort by index order.
 
 [source,js]
 --------------------------------------------------
@@ -21,6 +21,10 @@ field name for `_score` to sort by score.
 }
 --------------------------------------------------
 
+NOTE: `_doc` has no real use-case besides being the most efficient sort order.
+So if you don't care about the order in which documents are returned, then you
+should sort by `_doc`. This especially helps when <<search-request-scroll,scrolling>>.
+
 ==== Sort Values
 
 The sort values for each document returned are also returned as part of
diff --git a/docs/reference/search/suggesters/completion-suggest.asciidoc b/docs/reference/search/suggesters/completion-suggest.asciidoc
index 456dc8b..3b4f70f 100644
--- a/docs/reference/search/suggesters/completion-suggest.asciidoc
+++ b/docs/reference/search/suggesters/completion-suggest.asciidoc
@@ -165,7 +165,7 @@ curl -X POST 'localhost:9200/music/_suggest?pretty' -d '{
   "song-suggest" : [ {
     "text" : "n",
     "offset" : 0,
-    "length" : 4,
+    "length" : 1,
     "options" : [ {
       "text" : "Nirvana - Nevermind",
       "score" : 34.0, "payload" : {"artistId":2321}
diff --git a/docs/reference/search/uri-request.asciidoc b/docs/reference/search/uri-request.asciidoc
index 646dfa5..e01f9e9 100644
--- a/docs/reference/search/uri-request.asciidoc
+++ b/docs/reference/search/uri-request.asciidoc
@@ -103,7 +103,9 @@ Defaults to no terminate_after.
 |`size` |The number of hits to return. Defaults to `10`.
 
 |`search_type` |The type of the search operation to perform. Can be
-`dfs_query_then_fetch`, `query_then_fetch`, `scan` or `count`
+`dfs_query_then_fetch`, `query_then_fetch`, `scan`
+deprecated[2.1.0,Replaced by a regular `scroll` sorted by `_doc`]
+or `count`
 deprecated[2.0.0-beta1,Replaced by `size: 0`]. Defaults to `query_then_fetch`. See
 <<search-request-search-type,_Search Type_>> for
 more details on the different types of search that can be performed.
diff --git a/plugins/cloud-aws/rest-api-spec/test/cloud_aws/20_repository.yaml b/plugins/cloud-aws/rest-api-spec/test/cloud_aws/20_repository.yaml
new file mode 100644
index 0000000..df26e51
--- /dev/null
+++ b/plugins/cloud-aws/rest-api-spec/test/cloud_aws/20_repository.yaml
@@ -0,0 +1,23 @@
+# Integration tests for Cloud AWS components
+#
+"S3 repository can be registereed":
+    - do:
+        snapshot.create_repository:
+          repository: test_repo_s3_1
+          verify: false
+          body:
+            type: s3
+            settings:
+              bucket: "my_bucket_name"
+              access_key: "AKVAIQBF2RECL7FJWGJQ"
+              secret_key: "vExyMThREXeRMm/b/LRzEB8jWwvzQeXgjqMX+6br"
+
+    # Get repositry
+    - do:
+        snapshot.get_repository:
+          repository: test_repo_s3_1
+
+    - is_true: test_repo_s3_1
+    - is_true: test_repo_s3_1.settings.bucket
+    - is_false: test_repo_s3_1.settings.access_key
+    - is_false: test_repo_s3_1.settings.secret_key
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java
index 8cc8259..f8ecc3c 100644
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java
+++ b/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java
@@ -50,10 +50,11 @@ public class AwsEc2Service extends AbstractLifecycleComponent<AwsEc2Service> {
     @Inject
     public AwsEc2Service(Settings settings, SettingsFilter settingsFilter, NetworkService networkService, DiscoveryNodeService discoveryNodeService) {
         super(settings);
-        settingsFilter.addFilter("cloud.key");
-        settingsFilter.addFilter("cloud.account");
         settingsFilter.addFilter("cloud.aws.access_key");
         settingsFilter.addFilter("cloud.aws.secret_key");
+        // Filter repository-specific settings
+        settingsFilter.addFilter("access_key");
+        settingsFilter.addFilter("secret_key");
         // add specific ec2 name resolver
         networkService.addCustomNameResolver(new Ec2NameResolver(settings));
         discoveryNodeService.addCustomAttributeProvider(new Ec2CustomNodeAttributes(settings));
@@ -77,8 +78,8 @@ public class AwsEc2Service extends AbstractLifecycleComponent<AwsEc2Service> {
         } else {
             throw new IllegalArgumentException("No protocol supported [" + protocol + "], can either be [http] or [https]");
         }
-        String account = settings.get("cloud.aws.access_key", settings.get("cloud.account"));
-        String key = settings.get("cloud.aws.secret_key", settings.get("cloud.key"));
+        String account = settings.get("cloud.aws.access_key");
+        String key = settings.get("cloud.aws.secret_key");
 
         String proxyHost = settings.get("cloud.aws.proxy_host");
         proxyHost = settings.get("cloud.aws.ec2.proxy_host", proxyHost);
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
index d9bd5b2..b7e029b 100644
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
+++ b/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
@@ -55,8 +55,8 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
     @Override
     public synchronized AmazonS3 client() {
         String endpoint = getDefaultEndpoint();
-        String account = settings.get("cloud.aws.access_key", settings.get("cloud.account"));
-        String key = settings.get("cloud.aws.secret_key", settings.get("cloud.key"));
+        String account = settings.get("cloud.aws.access_key");
+        String key = settings.get("cloud.aws.secret_key");
 
         return getClient(endpoint, null, account, key, null);
     }
@@ -75,8 +75,8 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
             endpoint = getDefaultEndpoint();
         }
         if (account == null || key == null) {
-            account = settings.get("cloud.aws.access_key", settings.get("cloud.account"));
-            key = settings.get("cloud.aws.secret_key", settings.get("cloud.key"));
+            account = settings.get("cloud.aws.access_key");
+            key = settings.get("cloud.aws.secret_key");
         }
 
         return getClient(endpoint, protocol, account, key, maxRetries);
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/discovery/ec2/AwsEc2UnicastHostsProvider.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/discovery/ec2/AwsEc2UnicastHostsProvider.java
index 4ffd73c..ab11541 100644
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/discovery/ec2/AwsEc2UnicastHostsProvider.java
+++ b/plugins/cloud-aws/src/main/java/org/elasticsearch/discovery/ec2/AwsEc2UnicastHostsProvider.java
@@ -156,9 +156,9 @@ public class AwsEc2UnicastHostsProvider extends AbstractComponent implements Uni
                 }
                 if (address != null) {
                     try {
-                        TransportAddress[] addresses = transportService.addressesFromString(address);
-                        // we only limit to 1 addresses, makes no sense to ping 100 ports
-                        for (int i = 0; (i < addresses.length && i < UnicastZenPing.LIMIT_PORTS_COUNT); i++) {
+                        // we only limit to 1 port per address, makes no sense to ping 100 ports
+                        TransportAddress[] addresses = transportService.addressesFromString(address, 1);
+                        for (int i = 0; i < addresses.length; i++) {
                             logger.trace("adding {}, address {}, transport_address {}", instance.getInstanceId(), address, addresses[i]);
                             discoNodes.add(new DiscoveryNode("#cloud-" + instance.getInstanceId() + "-" + i, addresses[i], version.minimumCompatibilityVersion()));
                         }
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/discovery/azure/AzureUnicastHostsProvider.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/discovery/azure/AzureUnicastHostsProvider.java
index bfd6aac..4b20dfe 100644
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/discovery/azure/AzureUnicastHostsProvider.java
+++ b/plugins/cloud-azure/src/main/java/org/elasticsearch/discovery/azure/AzureUnicastHostsProvider.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.discovery.azure;
 
 import com.microsoft.windowsazure.management.compute.models.*;
+
 import org.elasticsearch.Version;
 import org.elasticsearch.cloud.azure.AzureServiceDisableException;
 import org.elasticsearch.cloud.azure.AzureServiceRemoteException;
@@ -28,6 +29,7 @@ import org.elasticsearch.cloud.azure.management.AzureComputeService.Discovery;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.common.network.NetworkService;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.TransportAddress;
@@ -37,6 +39,7 @@ import org.elasticsearch.transport.TransportService;
 
 import java.io.IOException;
 import java.net.InetAddress;
+import java.net.InetSocketAddress;
 import java.util.ArrayList;
 import java.util.Locale;
 import java.util.List;
@@ -216,9 +219,9 @@ public class AzureUnicastHostsProvider extends AbstractComponent implements Unic
 
                         if (privateIp != null) {
                             if (privateIp.equals(ipAddress)) {
-                                logger.trace("adding ourselves {}", ipAddress);
+                                logger.trace("adding ourselves {}", NetworkAddress.format(ipAddress));
                             }
-                            networkAddress = privateIp.getHostAddress();
+                            networkAddress = NetworkAddress.formatAddress(privateIp);
                         } else {
                             logger.trace("no private ip provided. ignoring [{}]...", instance.getInstanceName());
                         }
@@ -231,7 +234,7 @@ public class AzureUnicastHostsProvider extends AbstractComponent implements Unic
                                 continue;
                             }
 
-                            networkAddress = endpoint.getVirtualIPAddress().getHostAddress() + ":" + endpoint.getPort();
+                            networkAddress = NetworkAddress.formatAddress(new InetSocketAddress(endpoint.getVirtualIPAddress(), endpoint.getPort()));
                         }
 
                         if (networkAddress == null) {
@@ -251,11 +254,13 @@ public class AzureUnicastHostsProvider extends AbstractComponent implements Unic
                 }
 
                 try {
-                    TransportAddress[] addresses = transportService.addressesFromString(networkAddress);
-                    // we only limit to 1 addresses, makes no sense to ping 100 ports
-                    logger.trace("adding {}, transport_address {}", networkAddress, addresses[0]);
-                    cachedDiscoNodes.add(new DiscoveryNode("#cloud-" + instance.getInstanceName(), addresses[0],
+                    // we only limit to 1 port per address, makes no sense to ping 100 ports
+                    TransportAddress[] addresses = transportService.addressesFromString(networkAddress, 1);
+                    for (TransportAddress address : addresses) {
+                        logger.trace("adding {}, transport_address {}", networkAddress, address);
+                        cachedDiscoNodes.add(new DiscoveryNode("#cloud-" + instance.getInstanceName(), address,
                             version.minimumCompatibilityVersion()));
+                    }
                 } catch (Exception e) {
                     logger.warn("can not convert [{}] to transport address. skipping. [{}]", networkAddress, e.getMessage());
                 }
diff --git a/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java b/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
index afa6437..bad3b20 100755
--- a/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
+++ b/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
@@ -45,8 +45,5 @@ public class GceDiscovery extends ZenDiscovery {
                         ElectMasterService electMasterService) {
         super(settings, clusterName, threadPool, transportService, clusterService, nodeSettingsService,
                 pingService, electMasterService, discoverySettings);
-
-        // TODO Add again force disable multicast
-        // See related issue in AWS plugin https://github.com/elastic/elasticsearch-cloud-aws/issues/179
     }
 }
diff --git a/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java b/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java
index cf02dff..5415f58 100644
--- a/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java
+++ b/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java
@@ -22,12 +22,14 @@ package org.elasticsearch.discovery.gce;
 import com.google.api.services.compute.model.AccessConfig;
 import com.google.api.services.compute.model.Instance;
 import com.google.api.services.compute.model.NetworkInterface;
+
 import org.elasticsearch.Version;
 import org.elasticsearch.cloud.gce.GceComputeService;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.common.network.NetworkService;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.TransportAddress;
@@ -114,7 +116,7 @@ public class GceUnicastHostsProvider extends AbstractComponent implements Unicas
         try {
             InetAddress inetAddress = networkService.resolvePublishHostAddress(null);
             if (inetAddress != null) {
-                ipAddress = inetAddress.getHostAddress();
+                ipAddress = NetworkAddress.formatAddress(inetAddress);
             }
         } catch (IOException e) {
             // We can't find the publish host address... Hmmm. Too bad :-(
@@ -224,13 +226,15 @@ public class GceUnicastHostsProvider extends AbstractComponent implements Unicas
                         }
 
                         // ip_private is a single IP Address. We need to build a TransportAddress from it
-                        TransportAddress[] addresses = transportService.addressesFromString(address);
-
                         // If user has set `es_port` metadata, we don't need to ping all ports
                         // we only limit to 1 addresses, makes no sense to ping 100 ports
-                        logger.trace("adding {}, type {}, address {}, transport_address {}, status {}", name, type,
-                                ip_private, addresses[0], status);
-                        cachedDiscoNodes.add(new DiscoveryNode("#cloud-" + name + "-" + 0, addresses[0], version.minimumCompatibilityVersion()));
+                        TransportAddress[] addresses = transportService.addressesFromString(address, 1);
+
+                        for (TransportAddress transportAddress : addresses) {
+                            logger.trace("adding {}, type {}, address {}, transport_address {}, status {}", name, type,
+                                    ip_private, transportAddress, status);
+                            cachedDiscoNodes.add(new DiscoveryNode("#cloud-" + name + "-" + 0, transportAddress, version.minimumCompatibilityVersion()));
+                        }
                     }
                 } catch (Exception e) {
                     logger.warn("failed to add {}, address {}", e, name, ip_private);
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java
index 602b0a4..7f06ce6 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java
@@ -102,12 +102,17 @@ public class TransportDeleteByQueryAction extends HandledTransportAction<DeleteB
         void executeScan() {
             try {
                 final SearchRequest scanRequest = new SearchRequest(request.indices()).types(request.types()).indicesOptions(request.indicesOptions());
-                scanRequest.searchType(SearchType.SCAN).scroll(request.scroll());
+                scanRequest.scroll(request.scroll());
                 if (request.routing() != null) {
                     scanRequest.routing(request.routing());
                 }
 
-                SearchSourceBuilder source = new SearchSourceBuilder().query(request.source()).fields("_routing", "_parent").fetchSource(false).version(true);
+                SearchSourceBuilder source = new SearchSourceBuilder()
+                        .query(request.source())
+                        .fields("_routing", "_parent")
+                        .sort("_doc") // important for performance
+                        .fetchSource(false)
+                        .version(true);
                 if (request.size() > 0) {
                     source.size(request.size());
                 }
@@ -121,17 +126,9 @@ public class TransportDeleteByQueryAction extends HandledTransportAction<DeleteB
                     @Override
                     public void onResponse(SearchResponse searchResponse) {
                         long hits = searchResponse.getHits().getTotalHits();
-                        logger.trace("scan request executed: found [{}] document(s) to delete", hits);
-                        addShardFailures(searchResponse.getShardFailures());
-
-                        if (hits == 0) {
-                            finishHim(searchResponse.getScrollId(), false, null);
-                            return;
-                        }
+                        logger.trace("first request executed: found [{}] document(s) to delete", hits);
                         total.set(hits);
-
-                        logger.trace("start scrolling [{}] document(s)", hits);
-                        executeScroll(searchResponse.getScrollId());
+                        deleteHits(null, searchResponse);
                     }
 
                     @Override
@@ -151,53 +148,7 @@ public class TransportDeleteByQueryAction extends HandledTransportAction<DeleteB
                 scrollAction.execute(new SearchScrollRequest(scrollId).scroll(request.scroll()), new ActionListener<SearchResponse>() {
                     @Override
                     public void onResponse(SearchResponse scrollResponse) {
-                        final SearchHit[] docs = scrollResponse.getHits().getHits();
-                        final String nextScrollId = scrollResponse.getScrollId();
-                        addShardFailures(scrollResponse.getShardFailures());
-
-                        if (logger.isTraceEnabled()) {
-                            logger.trace("scroll request [{}] executed: [{}] document(s) returned", scrollId, docs.length);
-                        }
-
-                        if ((docs.length == 0) || (nextScrollId == null)) {
-                            logger.trace("scrolling documents terminated");
-                            finishHim(scrollId, false, null);
-                            return;
-                        }
-
-                        if (hasTimedOut()) {
-                            logger.trace("scrolling documents timed out");
-                            finishHim(scrollId, true, null);
-                            return;
-                        }
-
-                        // Delete the scrolled documents using the Bulk API
-                        BulkRequest bulkRequest = new BulkRequest();
-                        for (SearchHit doc : docs) {
-                            DeleteRequest delete = new DeleteRequest(doc.index(), doc.type(), doc.id()).version(doc.version());
-                            SearchHitField routing = doc.field("_routing");
-                            if (routing != null) {
-                                delete.routing((String) routing.value());
-                            }
-                            SearchHitField parent = doc.field("_parent");
-                            if (parent != null) {
-                                delete.parent((String) parent.value());
-                            }
-                            bulkRequest.add(delete);
-                        }
-
-                        logger.trace("executing bulk request with [{}] deletions", bulkRequest.numberOfActions());
-                        client.bulk(bulkRequest, new ActionListener<BulkResponse>() {
-                            @Override
-                            public void onResponse(BulkResponse bulkResponse) {
-                                onBulkResponse(nextScrollId, bulkResponse);
-                            }
-
-                            @Override
-                            public void onFailure(Throwable e) {
-                                onBulkFailure(nextScrollId, docs, e);
-                            }
-                        });
+                        deleteHits(scrollId, scrollResponse);
                     }
 
                     @Override
@@ -212,6 +163,58 @@ public class TransportDeleteByQueryAction extends HandledTransportAction<DeleteB
             }
         }
 
+        void deleteHits(String scrollId, SearchResponse scrollResponse) {
+            final SearchHit[] docs = scrollResponse.getHits().getHits();
+            final String nextScrollId = scrollResponse.getScrollId();
+            addShardFailures(scrollResponse.getShardFailures());
+
+            if (logger.isTraceEnabled()) {
+                logger.trace("scroll request [{}] executed: [{}] document(s) returned", scrollId, docs.length);
+            }
+
+            if ((docs.length == 0) || (nextScrollId == null)) {
+                logger.trace("scrolling documents terminated");
+                // if scrollId is null we are on the first request - just pass the nextScrollId which sill be non-null if the query matched no docs
+                finishHim(scrollId == null ? nextScrollId : scrollId, false, null);
+                return;
+            }
+
+            if (hasTimedOut()) {
+                logger.trace("scrolling documents timed out");
+                // if scrollId is null we are on the first request - just pass the nextScrollId which sill be non-null if the query matched no docs
+                finishHim(scrollId == null ? nextScrollId : scrollId, true, null);
+                return;
+            }
+
+            // Delete the scrolled documents using the Bulk API
+            BulkRequest bulkRequest = new BulkRequest();
+            for (SearchHit doc : docs) {
+                DeleteRequest delete = new DeleteRequest(doc.index(), doc.type(), doc.id()).version(doc.version());
+                SearchHitField routing = doc.field("_routing");
+                if (routing != null) {
+                    delete.routing((String) routing.value());
+                }
+                SearchHitField parent = doc.field("_parent");
+                if (parent != null) {
+                    delete.parent((String) parent.value());
+                }
+                bulkRequest.add(delete);
+            }
+
+            logger.trace("executing bulk request with [{}] deletions", bulkRequest.numberOfActions());
+            client.bulk(bulkRequest, new ActionListener<BulkResponse>() {
+                @Override
+                public void onResponse(BulkResponse bulkResponse) {
+                    onBulkResponse(nextScrollId, bulkResponse);
+                }
+
+                @Override
+                public void onFailure(Throwable e) {
+                    onBulkFailure(nextScrollId, docs, e);
+                }
+            });
+        }
+
         void onBulkResponse(String scrollId, BulkResponse bulkResponse) {
             try {
                 for (BulkItemResponse item : bulkResponse.getItems()) {
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/plugin/deletebyquery/DeleteByQueryModule.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/plugin/deletebyquery/DeleteByQueryModule.java
deleted file mode 100644
index 338a714..0000000
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/plugin/deletebyquery/DeleteByQueryModule.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.deletebyquery;
-
-import org.elasticsearch.action.ActionModule;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Module;
-import org.elasticsearch.common.inject.PreProcessModule;
-import org.elasticsearch.action.deletebyquery.DeleteByQueryAction;
-import org.elasticsearch.action.deletebyquery.TransportDeleteByQueryAction;
-import org.elasticsearch.rest.action.deletebyquery.RestDeleteByQueryAction;
-import org.elasticsearch.rest.RestModule;
-
-public class DeleteByQueryModule extends AbstractModule implements PreProcessModule {
-
-    @Override
-    public void processModule(Module module) {
-        if (module instanceof RestModule) {
-            RestModule restModule = (RestModule) module;
-            restModule.addRestAction(RestDeleteByQueryAction.class);
-        }
-        if (module instanceof ActionModule) {
-            ActionModule actionModule = (ActionModule) module;
-            actionModule.registerAction(DeleteByQueryAction.INSTANCE, TransportDeleteByQueryAction.class);
-        }
-    }
-
-    @Override
-    protected void configure() {
-    }
-}
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/plugin/deletebyquery/DeleteByQueryPlugin.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/plugin/deletebyquery/DeleteByQueryPlugin.java
index 7e50e93..b189745 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/plugin/deletebyquery/DeleteByQueryPlugin.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/plugin/deletebyquery/DeleteByQueryPlugin.java
@@ -19,8 +19,13 @@
 
 package org.elasticsearch.plugin.deletebyquery;
 
+import org.elasticsearch.action.ActionModule;
+import org.elasticsearch.action.deletebyquery.DeleteByQueryAction;
+import org.elasticsearch.action.deletebyquery.TransportDeleteByQueryAction;
 import org.elasticsearch.common.inject.Module;
 import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.rest.RestModule;
+import org.elasticsearch.rest.action.deletebyquery.RestDeleteByQueryAction;
 
 import java.util.Collection;
 import java.util.Collections;
@@ -39,8 +44,12 @@ public class DeleteByQueryPlugin extends Plugin {
         return "Elasticsearch Delete-By-Query Plugin";
     }
 
-    @Override
-    public Collection<Module> nodeModules() {
-        return Collections.<Module>singletonList(new DeleteByQueryModule());
+    public void onModule(ActionModule actionModule) {
+        actionModule.registerAction(DeleteByQueryAction.INSTANCE, TransportDeleteByQueryAction.class);
+    }
+
+    public void onModule(RestModule restModule) {
+        restModule.addRestAction(RestDeleteByQueryAction.class);
     }
+
 }
diff --git a/plugins/discovery-multicast/LICENSE.txt b/plugins/discovery-multicast/LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/discovery-multicast/LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/discovery-multicast/NOTICE.txt b/plugins/discovery-multicast/NOTICE.txt
new file mode 100644
index 0000000..4880904
--- /dev/null
+++ b/plugins/discovery-multicast/NOTICE.txt
@@ -0,0 +1,8 @@
+Elasticsearch
+Copyright 2009-2015 Elasticsearch
+
+This product includes software developed by The Apache Software
+Foundation (http://www.apache.org/).
+
+The LICENSE and NOTICE files for all dependencies may be found in the licenses/
+directory.
diff --git a/plugins/discovery-multicast/licenses/no_deps.txt b/plugins/discovery-multicast/licenses/no_deps.txt
new file mode 100644
index 0000000..e69de29
diff --git a/plugins/discovery-multicast/pom.xml b/plugins/discovery-multicast/pom.xml
new file mode 100644
index 0000000..937034a
--- /dev/null
+++ b/plugins/discovery-multicast/pom.xml
@@ -0,0 +1,33 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.elasticsearch.plugin</groupId>
+        <artifactId>plugins</artifactId>
+        <version>2.1.0-SNAPSHOT</version>
+    </parent>
+
+    <artifactId>discovery-multicast</artifactId>
+    <name>Plugin: Discovery: Multicast</name>
+    <description>The Multicast Discovery plugin allows discovery other nodes using multicast requests</description>
+
+    <properties>
+        <elasticsearch.plugin.classname>org.elasticsearch.plugin.discovery.multicast.MulticastDiscoveryPlugin</elasticsearch.plugin.classname>
+        <tests.jvms>1</tests.jvms>
+        <tests.rest.suite>discovery_multicast</tests.rest.suite>
+        <tests.rest.load_packaged>false</tests.rest.load_packaged>
+    </properties>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
diff --git a/plugins/discovery-multicast/rest-api-spec/test/discovery_multicast/10_basic.yaml b/plugins/discovery-multicast/rest-api-spec/test/discovery_multicast/10_basic.yaml
new file mode 100644
index 0000000..4c11023
--- /dev/null
+++ b/plugins/discovery-multicast/rest-api-spec/test/discovery_multicast/10_basic.yaml
@@ -0,0 +1,14 @@
+# Integration tests for multicast discovery
+#
+"Multicast discovery loaded":
+    - do:
+        cluster.state: {}
+
+    # Get master node id
+    - set: { master_node: master }
+
+    - do:
+        nodes.info: {}
+
+    - match:  { nodes.$master.plugins.0.name: discovery-multicast  }
+    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastChannel.java b/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastChannel.java
new file mode 100644
index 0000000..7fdeabe
--- /dev/null
+++ b/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastChannel.java
@@ -0,0 +1,386 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plugin.discovery.multicast;
+
+import com.google.common.collect.Maps;
+
+import org.apache.lucene.util.IOUtils;
+import org.elasticsearch.common.SuppressForbidden;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.ESLoggerFactory;
+import org.elasticsearch.common.settings.Settings;
+
+import java.io.Closeable;
+import java.net.*;
+import java.util.Map;
+import java.util.concurrent.CopyOnWriteArrayList;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import static org.elasticsearch.common.util.concurrent.EsExecutors.daemonThreadFactory;
+
+/**
+ * A multicast channel that supports registering for receive events, and sending datagram packets. Allows
+ * to easily share the same multicast socket if it holds the same config.
+ */
+public abstract class MulticastChannel implements Closeable {
+
+    /**
+     * Builds a channel based on the provided config, allowing to control if sharing a channel that uses
+     * the same config is allowed or not.
+     */
+    public static MulticastChannel getChannel(String name, boolean shared, Config config, Listener listener) throws Exception {
+        if (!shared) {
+            return new Plain(listener, name, config);
+        }
+        return Shared.getSharedChannel(listener, config);
+    }
+
+    /**
+     * Config of multicast channel.
+     */
+    public static final class Config {
+        public final int port;
+        public final String group;
+        public final int bufferSize;
+        public final int ttl;
+        public final InetAddress multicastInterface;
+        public final boolean deferToInterface;
+
+        public Config(int port, String group, int bufferSize, int ttl,
+                      InetAddress multicastInterface, boolean deferToInterface) {
+            this.port = port;
+            this.group = group;
+            this.bufferSize = bufferSize;
+            this.ttl = ttl;
+            this.multicastInterface = multicastInterface;
+            this.deferToInterface = deferToInterface;
+        }
+
+        @Override
+        public boolean equals(Object o) {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+
+            Config config = (Config) o;
+
+            if (bufferSize != config.bufferSize) return false;
+            if (port != config.port) return false;
+            if (ttl != config.ttl) return false;
+            if (group != null ? !group.equals(config.group) : config.group != null) return false;
+            if (multicastInterface != null ? !multicastInterface.equals(config.multicastInterface) : config.multicastInterface != null)
+                return false;
+
+            return true;
+        }
+
+        @Override
+        public int hashCode() {
+            int result = port;
+            result = 31 * result + (group != null ? group.hashCode() : 0);
+            result = 31 * result + bufferSize;
+            result = 31 * result + ttl;
+            result = 31 * result + (multicastInterface != null ? multicastInterface.hashCode() : 0);
+            return result;
+        }
+    }
+
+    /**
+     * Listener that gets called when data is received on the multicast channel.
+     */
+    public static interface Listener {
+        void onMessage(BytesReference data, SocketAddress address);
+    }
+
+    /**
+     * Simple listener that wraps multiple listeners into one.
+     */
+    public static class MultiListener implements Listener {
+
+        private final CopyOnWriteArrayList<Listener> listeners = new CopyOnWriteArrayList<>();
+
+        public void add(Listener listener) {
+            this.listeners.add(listener);
+        }
+
+        public boolean remove(Listener listener) {
+            return this.listeners.remove(listener);
+        }
+
+        @Override
+        public void onMessage(BytesReference data, SocketAddress address) {
+            for (Listener listener : listeners) {
+                listener.onMessage(data, address);
+            }
+        }
+    }
+
+    protected final Listener listener;
+    private AtomicBoolean closed = new AtomicBoolean();
+
+    protected MulticastChannel(Listener listener) {
+        this.listener = listener;
+    }
+
+    /**
+     * Send the data over the multicast channel.
+     */
+    public abstract void send(BytesReference data) throws Exception;
+
+    /**
+     * Close the channel.
+     */
+    @Override
+    public void close() {
+        if (closed.compareAndSet(false, true)) {
+            close(listener);
+        }
+    }
+
+    protected abstract void close(Listener listener);
+
+    public static final String SHARED_CHANNEL_NAME = "#shared#";
+    /**
+     * A shared channel that keeps a static map of Config -> Shared channels, and closes shared
+     * channel once their reference count has reached 0. It also handles de-registering relevant
+     * listener from the shared list of listeners.
+     */
+    private final static class Shared extends MulticastChannel {
+
+        private static final Map<Config, Shared> sharedChannels = Maps.newHashMap();
+        private static final Object mutex = new Object(); // global mutex so we don't sync on static methods (.class)
+
+        static MulticastChannel getSharedChannel(Listener listener, Config config) throws Exception {
+
+            synchronized (mutex) {
+                Shared shared = sharedChannels.get(config);
+                if (shared != null) {
+                    shared.incRef();
+                    ((MultiListener) shared.listener).add(listener);
+                } else {
+                    MultiListener multiListener = new MultiListener();
+                    multiListener.add(listener);
+                    shared = new Shared(multiListener, new Plain(multiListener, SHARED_CHANNEL_NAME, config));
+                    sharedChannels.put(config, shared);
+                }
+                return new Delegate(listener, shared);
+            }
+        }
+
+        static void close(Shared shared, Listener listener) {
+            synchronized (mutex) {
+                // remove this
+                boolean removed = ((MultiListener) shared.listener).remove(listener);
+                assert removed : "a listener should be removed";
+                if (shared.decRef() == 0) {
+                    assert ((MultiListener) shared.listener).listeners.isEmpty();
+                    sharedChannels.remove(shared.channel.getConfig());
+                    shared.channel.close();
+                }
+            }
+        }
+
+        final Plain channel;
+        private int refCount = 1;
+
+        Shared(MultiListener listener, Plain channel) {
+            super(listener);
+            this.channel = channel;
+        }
+
+        private void incRef() {
+            refCount++;
+        }
+
+        private int decRef() {
+            --refCount;
+            assert refCount >= 0 : "illegal ref counting, close called multiple times";
+            return refCount;
+        }
+
+        @Override
+        public void send(BytesReference data) throws Exception {
+            channel.send(data);
+        }
+
+        @Override
+        public void close() {
+            assert false : "Shared references should never be closed directly, only via Delegate";
+        }
+
+        @Override
+        protected void close(Listener listener) {
+            close(this, listener);
+        }
+    }
+
+    /**
+     * A light weight delegate that wraps another channel, mainly to support delegating
+     * the close method with the provided listener and not holding existing listener.
+     */
+    private final static class Delegate extends MulticastChannel {
+
+        private final MulticastChannel channel;
+
+        Delegate(Listener listener, MulticastChannel channel) {
+            super(listener);
+            this.channel = channel;
+        }
+
+        @Override
+        public void send(BytesReference data) throws Exception {
+            channel.send(data);
+        }
+
+        @Override
+        protected void close(Listener listener) {
+            channel.close(listener); // we delegate here to the close with our listener, not with the delegate listener
+        }
+    }
+
+    /**
+     * Simple implementation of a channel.
+     */
+    @SuppressForbidden(reason = "I bind to wildcard addresses. I am a total nightmare")
+    private static class Plain extends MulticastChannel {
+        private final ESLogger logger;
+        private final Config config;
+
+        private volatile MulticastSocket multicastSocket;
+        private final DatagramPacket datagramPacketSend;
+        private final DatagramPacket datagramPacketReceive;
+
+        private final Object sendMutex = new Object();
+        private final Object receiveMutex = new Object();
+
+        private final Receiver receiver;
+        private final Thread receiverThread;
+
+        Plain(Listener listener, String name, Config config) throws Exception {
+            super(listener);
+            this.logger = ESLoggerFactory.getLogger(name);
+            this.config = config;
+            this.datagramPacketReceive = new DatagramPacket(new byte[config.bufferSize], config.bufferSize);
+            this.datagramPacketSend = new DatagramPacket(new byte[config.bufferSize], config.bufferSize, InetAddress.getByName(config.group), config.port);
+            this.multicastSocket = buildMulticastSocket(config);
+            this.receiver = new Receiver();
+            this.receiverThread = daemonThreadFactory(Settings.builder().put("name", name).build(), "discovery#multicast#receiver").newThread(receiver);
+            this.receiverThread.start();
+        }
+
+        private MulticastSocket buildMulticastSocket(Config config) throws Exception {
+            SocketAddress addr = new InetSocketAddress(InetAddress.getByName(config.group), config.port);
+            MulticastSocket multicastSocket = new MulticastSocket(config.port);
+            try {
+                multicastSocket.setTimeToLive(config.ttl);
+                // OSX is not smart enough to tell that a socket bound to the
+                // 'lo0' interface needs to make sure to send the UDP packet
+                // out of the lo0 interface, so we need to do some special
+                // workarounds to fix it.
+                if (config.deferToInterface) {
+                    // 'null' here tells the socket to deter to the interface set
+                    // with .setInterface
+                    multicastSocket.joinGroup(addr, null);
+                    multicastSocket.setInterface(config.multicastInterface);
+                } else {
+                    multicastSocket.setInterface(config.multicastInterface);
+                    multicastSocket.joinGroup(InetAddress.getByName(config.group));
+                }
+                multicastSocket.setReceiveBufferSize(config.bufferSize);
+                multicastSocket.setSendBufferSize(config.bufferSize);
+                multicastSocket.setSoTimeout(60000);
+            } catch (Throwable e) {
+                IOUtils.closeWhileHandlingException(multicastSocket);
+                throw e;
+            }
+            return multicastSocket;
+        }
+
+        public Config getConfig() {
+            return this.config;
+        }
+
+        @Override
+        public void send(BytesReference data) throws Exception {
+            synchronized (sendMutex) {
+                datagramPacketSend.setData(data.toBytes());
+                multicastSocket.send(datagramPacketSend);
+            }
+        }
+
+        @Override
+        protected void close(Listener listener) {
+            receiver.stop();
+            receiverThread.interrupt();
+            if (multicastSocket != null) {
+                IOUtils.closeWhileHandlingException(multicastSocket);
+                multicastSocket = null;
+            }
+            try {
+                receiverThread.join(10000);
+            } catch (InterruptedException e) {
+                Thread.currentThread().interrupt();
+            }
+        }
+
+        private class Receiver implements Runnable {
+
+            private volatile boolean running = true;
+
+            public void stop() {
+                running = false;
+            }
+
+            @Override
+            public void run() {
+                while (running) {
+                    try {
+                        synchronized (receiveMutex) {
+                            try {
+                                multicastSocket.receive(datagramPacketReceive);
+                            } catch (SocketTimeoutException ignore) {
+                                continue;
+                            } catch (Exception e) {
+                                if (running) {
+                                    if (multicastSocket.isClosed()) {
+                                        logger.warn("multicast socket closed while running, restarting...");
+                                        multicastSocket = buildMulticastSocket(config);
+                                    } else {
+                                        logger.warn("failed to receive packet, throttling...", e);
+                                        Thread.sleep(500);
+                                    }
+                                }
+                                continue;
+                            }
+                        }
+                        if (datagramPacketReceive.getData().length > 0) {
+                            listener.onMessage(new BytesArray(datagramPacketReceive.getData()), datagramPacketReceive.getSocketAddress());
+                        }
+                    } catch (Throwable e) {
+                        if (running) {
+                            logger.warn("unexpected exception in multicast receiver", e);
+                        }
+                    }
+                }
+            }
+        }
+    }
+}
diff --git a/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastDiscoveryPlugin.java b/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastDiscoveryPlugin.java
new file mode 100644
index 0000000..f0a7343
--- /dev/null
+++ b/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastDiscoveryPlugin.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plugin.discovery.multicast;
+
+import org.elasticsearch.common.inject.Module;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.DiscoveryModule;
+import org.elasticsearch.plugin.discovery.multicast.MulticastZenPing;
+import org.elasticsearch.plugins.Plugin;
+
+import java.util.Collection;
+
+public class MulticastDiscoveryPlugin extends Plugin {
+
+    private final Settings settings;
+
+    public MulticastDiscoveryPlugin(Settings settings) {
+        this.settings = settings;
+    }
+
+    @Override
+    public String name() {
+        return "discovery-multicast";
+    }
+
+    @Override
+    public String description() {
+        return "Multicast Discovery Plugin";
+    }
+    
+    public void onModule(DiscoveryModule module) {
+        if (settings.getAsBoolean("discovery.zen.ping.multicast.enabled", false)) {
+            module.addZenPing(MulticastZenPing.class);
+        }
+    }
+}
diff --git a/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java b/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java
new file mode 100644
index 0000000..1abac08
--- /dev/null
+++ b/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java
@@ -0,0 +1,566 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plugin.discovery.multicast;
+
+import org.apache.lucene.util.Constants;
+import org.elasticsearch.ExceptionsHelper;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.ClusterName;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.cluster.node.DiscoveryNodes;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.component.AbstractLifecycleComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.io.stream.BytesStreamOutput;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.util.concurrent.AbstractRunnable;
+import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.discovery.zen.ping.PingContextProvider;
+import org.elasticsearch.discovery.zen.ping.ZenPing;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.*;
+
+import java.io.IOException;
+import java.net.SocketAddress;
+import java.util.Map;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.atomic.AtomicReference;
+
+import static org.elasticsearch.cluster.node.DiscoveryNode.readNode;
+import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
+import static org.elasticsearch.common.util.concurrent.ConcurrentCollections.newConcurrentMap;
+
+/**
+ *
+ */
+public class MulticastZenPing extends AbstractLifecycleComponent<ZenPing> implements ZenPing {
+
+    public static final String ACTION_NAME = "internal:discovery/zen/multicast";
+
+    private static final byte[] INTERNAL_HEADER = new byte[]{1, 9, 8, 4};
+
+    private static final int PING_SIZE_ESTIMATE = 150;
+
+    private final String address;
+    private final int port;
+    private final String group;
+    private final int bufferSize;
+    private final int ttl;
+
+    private final ThreadPool threadPool;
+    private final TransportService transportService;
+    private final ClusterName clusterName;
+    private final NetworkService networkService;
+    private final Version version;
+    private volatile PingContextProvider contextProvider;
+
+    private final boolean pingEnabled;
+
+    private volatile MulticastChannel multicastChannel;
+
+    private final AtomicInteger pingIdGenerator = new AtomicInteger();
+    private final Map<Integer, PingCollection> receivedResponses = newConcurrentMap();
+
+    public MulticastZenPing(ThreadPool threadPool, TransportService transportService, ClusterName clusterName, Version version) {
+        this(EMPTY_SETTINGS, threadPool, transportService, clusterName, new NetworkService(EMPTY_SETTINGS), version);
+    }
+
+    @Inject
+    public MulticastZenPing(Settings settings, ThreadPool threadPool, TransportService transportService, ClusterName clusterName, NetworkService networkService, Version version) {
+        super(settings);
+        this.threadPool = threadPool;
+        this.transportService = transportService;
+        this.clusterName = clusterName;
+        this.networkService = networkService;
+        this.version = version;
+
+        this.address = this.settings.get("discovery.zen.ping.multicast.address");
+        this.port = this.settings.getAsInt("discovery.zen.ping.multicast.port", 54328);
+        this.group = this.settings.get("discovery.zen.ping.multicast.group", "224.2.2.4");
+        this.bufferSize = this.settings.getAsInt("discovery.zen.ping.multicast.buffer_size", 2048);
+        this.ttl = this.settings.getAsInt("discovery.zen.ping.multicast.ttl", 3);
+
+        this.pingEnabled = this.settings.getAsBoolean("discovery.zen.ping.multicast.ping.enabled", true);
+
+        logger.debug("using group [{}], with port [{}], ttl [{}], and address [{}]", group, port, ttl, address);
+
+        this.transportService.registerRequestHandler(ACTION_NAME, MulticastPingResponse.class, ThreadPool.Names.SAME, new MulticastPingResponseRequestHandler());
+    }
+
+    @Override
+    public void setPingContextProvider(PingContextProvider nodesProvider) {
+        if (lifecycle.started()) {
+            throw new IllegalStateException("Can't set nodes provider when started");
+        }
+        this.contextProvider = nodesProvider;
+    }
+
+    @Override
+    protected void doStart() {
+        try {
+            // we know OSX has bugs in the JVM when creating multiple instances of multicast sockets
+            // causing for "socket close" exceptions when receive and/or crashes
+            boolean shared = settings.getAsBoolean("discovery.zen.ping.multicast.shared", Constants.MAC_OS_X);
+            // OSX does not correctly send multicasts FROM the right interface
+            boolean deferToInterface = settings.getAsBoolean("discovery.zen.ping.multicast.defer_group_to_set_interface", Constants.MAC_OS_X);
+            multicastChannel = MulticastChannel.getChannel(nodeName(), shared,
+                    new MulticastChannel.Config(port, group, bufferSize, ttl,
+                            // don't use publish address, the use case for that is e.g. a firewall or proxy and
+                            // may not even be bound to an interface on this machine! use the first bound address.
+                            networkService.resolveBindHostAddress(address)[0],
+                            deferToInterface),
+                    new Receiver());
+        } catch (Throwable t) {
+            String msg = "multicast failed to start [{}], disabling. Consider using IPv4 only (by defining env. variable `ES_USE_IPV4`)";
+            if (logger.isDebugEnabled()) {
+                logger.debug(msg, t, ExceptionsHelper.detailedMessage(t));
+            } else {
+                logger.info(msg, ExceptionsHelper.detailedMessage(t));
+            }
+        }
+    }
+
+    @Override
+    protected void doStop() {
+        if (multicastChannel != null) {
+            multicastChannel.close();
+            multicastChannel = null;
+        }
+    }
+
+    @Override
+    protected void doClose() {
+    }
+
+    public PingResponse[] pingAndWait(TimeValue timeout) {
+        final AtomicReference<PingResponse[]> response = new AtomicReference<>();
+        final CountDownLatch latch = new CountDownLatch(1);
+        try {
+            ping(new PingListener() {
+                @Override
+                public void onPing(PingResponse[] pings) {
+                    response.set(pings);
+                    latch.countDown();
+                }
+            }, timeout);
+        } catch (EsRejectedExecutionException ex) {
+            logger.debug("Ping execution rejected", ex);
+            return PingResponse.EMPTY;
+        }
+        try {
+            latch.await();
+            return response.get();
+        } catch (InterruptedException e) {
+            Thread.currentThread().interrupt();
+            return PingResponse.EMPTY;
+        }
+    }
+
+    @Override
+    public void ping(final PingListener listener, final TimeValue timeout) {
+        if (!pingEnabled || multicastChannel == null) {
+            threadPool.generic().execute(new Runnable() {
+                @Override
+                public void run() {
+                    listener.onPing(PingResponse.EMPTY);
+                }
+            });
+            return;
+        }
+        final int id = pingIdGenerator.incrementAndGet();
+        try {
+            receivedResponses.put(id, new PingCollection());
+            sendPingRequest(id);
+            // try and send another ping request halfway through (just in case someone woke up during it...)
+            // this can be a good trade-off to nailing the initial lookup or un-delivered messages
+            threadPool.schedule(TimeValue.timeValueMillis(timeout.millis() / 2), ThreadPool.Names.GENERIC, new AbstractRunnable() {
+                @Override
+                public void onFailure(Throwable t) {
+                    logger.warn("[{}] failed to send second ping request", t, id);
+                    finalizePingCycle(id, listener);
+                }
+
+                @Override
+                public void doRun() {
+                    sendPingRequest(id);
+                    threadPool.schedule(TimeValue.timeValueMillis(timeout.millis() / 2), ThreadPool.Names.GENERIC, new AbstractRunnable() {
+                        @Override
+                        public void onFailure(Throwable t) {
+                            logger.warn("[{}] failed to send third ping request", t, id);
+                            finalizePingCycle(id, listener);
+                        }
+
+                        @Override
+                        public void doRun() {
+                            // make one last ping, but finalize as soon as all nodes have responded or a timeout has past
+                            PingCollection collection = receivedResponses.get(id);
+                            FinalizingPingCollection finalizingPingCollection = new FinalizingPingCollection(id, collection, collection.size(), listener);
+                            receivedResponses.put(id, finalizingPingCollection);
+                            logger.trace("[{}] sending last pings", id);
+                            sendPingRequest(id);
+                            threadPool.schedule(TimeValue.timeValueMillis(timeout.millis() / 4), ThreadPool.Names.GENERIC, new AbstractRunnable() {
+                                @Override
+                                public void onFailure(Throwable t) {
+                                    logger.warn("[{}] failed to finalize ping", t, id);
+                                }
+
+                                @Override
+                                protected void doRun() throws Exception {
+                                    finalizePingCycle(id, listener);
+                                }
+                            });
+                        }
+                    });
+                }
+            });
+        } catch (Exception e) {
+            logger.warn("failed to ping", e);
+            finalizePingCycle(id, listener);
+        }
+    }
+
+    /**
+     * takes all pings collected for a given id and pass them to the given listener.
+     * this method is safe to call multiple times as is guaranteed to only finalize once.
+     */
+    private void finalizePingCycle(int id, final PingListener listener) {
+        PingCollection responses = receivedResponses.remove(id);
+        if (responses != null) {
+            listener.onPing(responses.toArray());
+        }
+    }
+
+    private void sendPingRequest(int id) {
+        try {
+            BytesStreamOutput out = new BytesStreamOutput(PING_SIZE_ESTIMATE);
+            out.writeBytes(INTERNAL_HEADER);
+            // TODO: change to min_required version!
+            Version.writeVersion(version, out);
+            out.writeInt(id);
+            clusterName.writeTo(out);
+            contextProvider.nodes().localNode().writeTo(out);
+            out.close();
+            multicastChannel.send(out.bytes());
+            if (logger.isTraceEnabled()) {
+                logger.trace("[{}] sending ping request", id);
+            }
+        } catch (Exception e) {
+            if (lifecycle.stoppedOrClosed()) {
+                return;
+            }
+            if (logger.isDebugEnabled()) {
+                logger.debug("failed to send multicast ping request", e);
+            } else {
+                logger.warn("failed to send multicast ping request: {}", ExceptionsHelper.detailedMessage(e));
+            }
+        }
+    }
+
+    class FinalizingPingCollection extends PingCollection {
+        final private PingCollection internalCollection;
+        final private int expectedResponses;
+        final private AtomicInteger responseCount;
+        final private PingListener listener;
+        final private int id;
+
+        public FinalizingPingCollection(int id, PingCollection internalCollection, int expectedResponses, PingListener listener) {
+            this.id = id;
+            this.internalCollection = internalCollection;
+            this.expectedResponses = expectedResponses;
+            this.responseCount = new AtomicInteger();
+            this.listener = listener;
+        }
+
+        @Override
+        public synchronized boolean addPing(PingResponse ping) {
+            if (internalCollection.addPing(ping)) {
+                if (responseCount.incrementAndGet() >= expectedResponses) {
+                    logger.trace("[{}] all nodes responded", id);
+                    finish();
+                }
+                return true;
+            }
+            return false;
+        }
+
+        @Override
+        public synchronized void addPings(PingResponse[] pings) {
+            internalCollection.addPings(pings);
+        }
+
+        @Override
+        public synchronized PingResponse[] toArray() {
+            return internalCollection.toArray();
+        }
+
+        void finish() {
+            // spawn another thread as we may be running on a network thread
+            threadPool.generic().execute(new AbstractRunnable() {
+                @Override
+                public void onFailure(Throwable t) {
+                    logger.error("failed to call ping listener", t);
+                }
+
+                @Override
+                protected void doRun() throws Exception {
+                    finalizePingCycle(id, listener);
+                }
+            });
+        }
+    }
+
+    class MulticastPingResponseRequestHandler implements TransportRequestHandler<MulticastPingResponse> {
+        @Override
+        public void messageReceived(MulticastPingResponse request, TransportChannel channel) throws Exception {
+            if (logger.isTraceEnabled()) {
+                logger.trace("[{}] received {}", request.id, request.pingResponse);
+            }
+            PingCollection responses = receivedResponses.get(request.id);
+            if (responses == null) {
+                logger.warn("received ping response {} with no matching id [{}]", request.pingResponse, request.id);
+            } else {
+                responses.addPing(request.pingResponse);
+            }
+            channel.sendResponse(TransportResponse.Empty.INSTANCE);
+        }
+    }
+
+    static class MulticastPingResponse extends TransportRequest {
+
+        int id;
+
+        PingResponse pingResponse;
+
+        MulticastPingResponse() {
+        }
+
+        @Override
+        public void readFrom(StreamInput in) throws IOException {
+            super.readFrom(in);
+            id = in.readInt();
+            pingResponse = PingResponse.readPingResponse(in);
+        }
+
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            super.writeTo(out);
+            out.writeInt(id);
+            pingResponse.writeTo(out);
+        }
+    }
+
+
+    private class Receiver implements MulticastChannel.Listener {
+
+        @Override
+        public void onMessage(BytesReference data, SocketAddress address) {
+            int id = -1;
+            DiscoveryNode requestingNodeX = null;
+            ClusterName clusterName = null;
+
+            Map<String, Object> externalPingData = null;
+            XContentType xContentType = null;
+
+            try {
+                boolean internal = false;
+                if (data.length() > 4) {
+                    int counter = 0;
+                    for (; counter < INTERNAL_HEADER.length; counter++) {
+                        if (data.get(counter) != INTERNAL_HEADER[counter]) {
+                            break;
+                        }
+                    }
+                    if (counter == INTERNAL_HEADER.length) {
+                        internal = true;
+                    }
+                }
+                if (internal) {
+                    StreamInput input = StreamInput.wrap(new BytesArray(data.toBytes(), INTERNAL_HEADER.length, data.length() - INTERNAL_HEADER.length));
+                    Version version = Version.readVersion(input);
+                    input.setVersion(version);
+                    id = input.readInt();
+                    clusterName = ClusterName.readClusterName(input);
+                    requestingNodeX = readNode(input);
+                } else {
+                    xContentType = XContentFactory.xContentType(data);
+                    if (xContentType != null) {
+                        // an external ping
+                        try (XContentParser parser = XContentFactory.xContent(xContentType).createParser(data)) {
+                            externalPingData = parser.map();
+                        }
+                    } else {
+                        throw new IllegalStateException("failed multicast message, probably message from previous version");
+                    }
+                }
+                if (externalPingData != null) {
+                    handleExternalPingRequest(externalPingData, xContentType, address);
+                } else {
+                    handleNodePingRequest(id, requestingNodeX, clusterName);
+                }
+            } catch (Exception e) {
+                if (!lifecycle.started() || (e instanceof EsRejectedExecutionException)) {
+                    logger.debug("failed to read requesting data from {}", e, address);
+                } else {
+                    logger.warn("failed to read requesting data from {}", e, address);
+                }
+            }
+        }
+
+        @SuppressWarnings("unchecked")
+        private void handleExternalPingRequest(Map<String, Object> externalPingData, XContentType contentType, SocketAddress remoteAddress) {
+            if (externalPingData.containsKey("response")) {
+                // ignoring responses sent over the multicast channel
+                logger.trace("got an external ping response (ignoring) from {}, content {}", remoteAddress, externalPingData);
+                return;
+            }
+
+            if (multicastChannel == null) {
+                logger.debug("can't send ping response, no socket, from {}, content {}", remoteAddress, externalPingData);
+                return;
+            }
+
+            Map<String, Object> request = (Map<String, Object>) externalPingData.get("request");
+            if (request == null) {
+                logger.warn("malformed external ping request, no 'request' element from {}, content {}", remoteAddress, externalPingData);
+                return;
+            }
+
+            final String requestClusterName = request.containsKey("cluster_name") ? request.get("cluster_name").toString() : request.containsKey("clusterName") ? request.get("clusterName").toString() : null;
+            if (requestClusterName == null) {
+                logger.warn("malformed external ping request, missing 'cluster_name' element within request, from {}, content {}", remoteAddress, externalPingData);
+                return;
+            }
+
+            if (!requestClusterName.equals(clusterName.value())) {
+                logger.trace("got request for cluster_name {}, but our cluster_name is {}, from {}, content {}",
+                        requestClusterName, clusterName.value(), remoteAddress, externalPingData);
+                return;
+            }
+            if (logger.isTraceEnabled()) {
+                logger.trace("got external ping request from {}, content {}", remoteAddress, externalPingData);
+            }
+
+            try {
+                DiscoveryNode localNode = contextProvider.nodes().localNode();
+
+                XContentBuilder builder = XContentFactory.contentBuilder(contentType);
+                builder.startObject().startObject("response");
+                builder.field("cluster_name", clusterName.value());
+                builder.startObject("version").field("number", version.number()).field("snapshot_build", version.snapshot).endObject();
+                builder.field("transport_address", localNode.address().toString());
+
+                if (contextProvider.nodeService() != null) {
+                    for (Map.Entry<String, String> attr : contextProvider.nodeService().attributes().entrySet()) {
+                        builder.field(attr.getKey(), attr.getValue());
+                    }
+                }
+
+                builder.startObject("attributes");
+                for (Map.Entry<String, String> attr : localNode.attributes().entrySet()) {
+                    builder.field(attr.getKey(), attr.getValue());
+                }
+                builder.endObject();
+
+                builder.endObject().endObject();
+                multicastChannel.send(builder.bytes());
+                if (logger.isTraceEnabled()) {
+                    logger.trace("sending external ping response {}", builder.string());
+                }
+            } catch (Exception e) {
+                logger.warn("failed to send external multicast response", e);
+            }
+        }
+
+        private void handleNodePingRequest(int id, DiscoveryNode requestingNodeX, ClusterName requestClusterName) {
+            if (!pingEnabled || multicastChannel == null) {
+                return;
+            }
+            final DiscoveryNodes discoveryNodes = contextProvider.nodes();
+            final DiscoveryNode requestingNode = requestingNodeX;
+            if (requestingNode.id().equals(discoveryNodes.localNodeId())) {
+                // that's me, ignore
+                return;
+            }
+            if (!requestClusterName.equals(clusterName)) {
+                if (logger.isTraceEnabled()) {
+                    logger.trace("[{}] received ping_request from [{}], but wrong cluster_name [{}], expected [{}], ignoring",
+                            id, requestingNode, requestClusterName.value(), clusterName.value());
+                }
+                return;
+            }
+            // don't connect between two client nodes, no need for that...
+            if (!discoveryNodes.localNode().shouldConnectTo(requestingNode)) {
+                if (logger.isTraceEnabled()) {
+                    logger.trace("[{}] received ping_request from [{}], both are client nodes, ignoring", id, requestingNode, requestClusterName);
+                }
+                return;
+            }
+            final MulticastPingResponse multicastPingResponse = new MulticastPingResponse();
+            multicastPingResponse.id = id;
+            multicastPingResponse.pingResponse = new PingResponse(discoveryNodes.localNode(), discoveryNodes.masterNode(), clusterName, contextProvider.nodeHasJoinedClusterOnce());
+
+            if (logger.isTraceEnabled()) {
+                logger.trace("[{}] received ping_request from [{}], sending {}", id, requestingNode, multicastPingResponse.pingResponse);
+            }
+
+            if (!transportService.nodeConnected(requestingNode)) {
+                // do the connect and send on a thread pool
+                threadPool.generic().execute(new Runnable() {
+                    @Override
+                    public void run() {
+                        // connect to the node if possible
+                        try {
+                            transportService.connectToNode(requestingNode);
+                            transportService.sendRequest(requestingNode, ACTION_NAME, multicastPingResponse, new EmptyTransportResponseHandler(ThreadPool.Names.SAME) {
+                                @Override
+                                public void handleException(TransportException exp) {
+                                    logger.warn("failed to receive confirmation on sent ping response to [{}]", exp, requestingNode);
+                                }
+                            });
+                        } catch (Exception e) {
+                            if (lifecycle.started()) {
+                                logger.warn("failed to connect to requesting node {}", e, requestingNode);
+                            }
+                        }
+                    }
+                });
+            } else {
+                transportService.sendRequest(requestingNode, ACTION_NAME, multicastPingResponse, new EmptyTransportResponseHandler(ThreadPool.Names.SAME) {
+                    @Override
+                    public void handleException(TransportException exp) {
+                        if (lifecycle.started()) {
+                            logger.warn("failed to receive confirmation on sent ping response to [{}]", exp, requestingNode);
+                        }
+                    }
+                });
+            }
+        }
+    }
+}
diff --git a/plugins/discovery-multicast/src/test/java/org/elasticsearch/plugin/discovery/multicast/MulticastDiscoveryRestIT.java b/plugins/discovery-multicast/src/test/java/org/elasticsearch/plugin/discovery/multicast/MulticastDiscoveryRestIT.java
new file mode 100644
index 0000000..c6af20c
--- /dev/null
+++ b/plugins/discovery-multicast/src/test/java/org/elasticsearch/plugin/discovery/multicast/MulticastDiscoveryRestIT.java
@@ -0,0 +1,41 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plugin.discovery.multicast;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+
+public class MulticastDiscoveryRestIT extends ESRestTestCase {
+
+    public MulticastDiscoveryRestIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+}
+
diff --git a/plugins/discovery-multicast/src/test/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPingTests.java b/plugins/discovery-multicast/src/test/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPingTests.java
new file mode 100644
index 0000000..7ab0aa7
--- /dev/null
+++ b/plugins/discovery-multicast/src/test/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPingTests.java
@@ -0,0 +1,184 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plugin.discovery.multicast;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.ClusterName;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.cluster.node.DiscoveryNodes;
+import org.elasticsearch.common.SuppressForbidden;
+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
+import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.discovery.zen.ping.PingContextProvider;
+import org.elasticsearch.discovery.zen.ping.ZenPing;
+import org.elasticsearch.node.service.NodeService;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.TransportService;
+import org.elasticsearch.transport.local.LocalTransport;
+import org.hamcrest.Matchers;
+import org.junit.Assert;
+import org.junit.Test;
+
+import java.net.DatagramPacket;
+import java.net.InetAddress;
+import java.net.MulticastSocket;
+
+public class MulticastZenPingTests extends ESTestCase {
+
+    private Settings buildRandomMulticast(Settings settings) {
+        Settings.Builder builder = Settings.builder().put(settings);
+        builder.put("discovery.zen.ping.multicast.group", "224.2.3." + randomIntBetween(0, 255));
+        builder.put("discovery.zen.ping.multicast.port", randomIntBetween(55000, 56000));
+        builder.put("discovery.zen.ping.multicast.enabled", true);
+        if (randomBoolean()) {
+            builder.put("discovery.zen.ping.multicast.shared", randomBoolean());
+        }
+        return builder.build();
+    }
+
+    @Test
+    public void testSimplePings() throws InterruptedException {
+        Settings settings = Settings.EMPTY;
+        settings = buildRandomMulticast(settings);
+        Thread.sleep(30000);
+
+        ThreadPool threadPool = new ThreadPool("testSimplePings");
+        final ClusterName clusterName = new ClusterName("test");
+        final TransportService transportServiceA = new TransportService(new LocalTransport(settings, threadPool, Version.CURRENT, new NamedWriteableRegistry()), threadPool).start();
+        final DiscoveryNode nodeA = new DiscoveryNode("A", transportServiceA.boundAddress().publishAddress(), Version.CURRENT);
+
+        final TransportService transportServiceB = new TransportService(new LocalTransport(settings, threadPool, Version.CURRENT, new NamedWriteableRegistry()), threadPool).start();
+        final DiscoveryNode nodeB = new DiscoveryNode("B", transportServiceB.boundAddress().publishAddress(), Version.CURRENT);
+
+        MulticastZenPing zenPingA = new MulticastZenPing(threadPool, transportServiceA, clusterName, Version.CURRENT);
+        zenPingA.setPingContextProvider(new PingContextProvider() {
+            @Override
+            public DiscoveryNodes nodes() {
+                return DiscoveryNodes.builder().put(nodeA).localNodeId("A").build();
+            }
+
+            @Override
+            public NodeService nodeService() {
+                return null;
+            }
+
+            @Override
+            public boolean nodeHasJoinedClusterOnce() {
+                return false;
+            }
+        });
+        zenPingA.start();
+
+        MulticastZenPing zenPingB = new MulticastZenPing(threadPool, transportServiceB, clusterName, Version.CURRENT);
+        zenPingB.setPingContextProvider(new PingContextProvider() {
+            @Override
+            public DiscoveryNodes nodes() {
+                return DiscoveryNodes.builder().put(nodeB).localNodeId("B").build();
+            }
+
+            @Override
+            public NodeService nodeService() {
+                return null;
+            }
+
+            @Override
+            public boolean nodeHasJoinedClusterOnce() {
+                return true;
+            }
+        });
+        zenPingB.start();
+
+        try {
+            logger.info("ping from A");
+            ZenPing.PingResponse[] pingResponses = zenPingA.pingAndWait(TimeValue.timeValueSeconds(1));
+            Assert.assertThat(pingResponses.length, Matchers.equalTo(1));
+            Assert.assertThat(pingResponses[0].node().id(), Matchers.equalTo("B"));
+            Assert.assertTrue(pingResponses[0].hasJoinedOnce());
+
+            logger.info("ping from B");
+            pingResponses = zenPingB.pingAndWait(TimeValue.timeValueSeconds(1));
+            Assert.assertThat(pingResponses.length, Matchers.equalTo(1));
+            Assert.assertThat(pingResponses[0].node().id(), Matchers.equalTo("A"));
+            Assert.assertFalse(pingResponses[0].hasJoinedOnce());
+
+        } finally {
+            zenPingA.close();
+            zenPingB.close();
+            transportServiceA.close();
+            transportServiceB.close();
+            terminate(threadPool);
+        }
+    }
+
+    @Test @SuppressForbidden(reason = "I bind to wildcard addresses. I am a total nightmare")
+    public void testExternalPing() throws Exception {
+        Settings settings = Settings.EMPTY;
+        settings = buildRandomMulticast(settings);
+
+        final ThreadPool threadPool = new ThreadPool("testExternalPing");
+        final ClusterName clusterName = new ClusterName("test");
+        final TransportService transportServiceA = new TransportService(new LocalTransport(settings, threadPool, Version.CURRENT, new NamedWriteableRegistry()), threadPool).start();
+        final DiscoveryNode nodeA = new DiscoveryNode("A", transportServiceA.boundAddress().publishAddress(), Version.CURRENT);
+
+        MulticastZenPing zenPingA = new MulticastZenPing(threadPool, transportServiceA, clusterName, Version.CURRENT);
+        zenPingA.setPingContextProvider(new PingContextProvider() {
+            @Override
+            public DiscoveryNodes nodes() {
+                return DiscoveryNodes.builder().put(nodeA).localNodeId("A").build();
+            }
+
+            @Override
+            public NodeService nodeService() {
+                return null;
+            }
+
+            @Override
+            public boolean nodeHasJoinedClusterOnce() {
+                return false;
+            }
+        });
+        zenPingA.start();
+
+        MulticastSocket multicastSocket = null;
+        try {
+            Loggers.getLogger(MulticastZenPing.class).setLevel("TRACE");
+            multicastSocket = new MulticastSocket(54328);
+            multicastSocket.setReceiveBufferSize(2048);
+            multicastSocket.setSendBufferSize(2048);
+            multicastSocket.setSoTimeout(60000);
+
+            DatagramPacket datagramPacket = new DatagramPacket(new byte[2048], 2048, InetAddress.getByName("224.2.2.4"), 54328);
+            XContentBuilder builder = XContentFactory.jsonBuilder().startObject().startObject("request").field("cluster_name", "test").endObject().endObject();
+            datagramPacket.setData(builder.bytes().toBytes());
+            multicastSocket.send(datagramPacket);
+            Thread.sleep(100);
+        } finally {
+            Loggers.getLogger(MulticastZenPing.class).setLevel("INFO");
+            if (multicastSocket != null) multicastSocket.close();
+            zenPingA.close();
+            terminate(threadPool);
+        }
+    }
+}
diff --git a/plugins/pom.xml b/plugins/pom.xml
index 5bc7f5f..63ba87d 100644
--- a/plugins/pom.xml
+++ b/plugins/pom.xml
@@ -22,6 +22,7 @@
     <properties>
         <elasticsearch.assembly.descriptor>${elasticsearch.tools.directory}/plugin-metadata/plugin-assembly.xml</elasticsearch.assembly.descriptor>
         <elasticsearch.assembly.appendId>false</elasticsearch.assembly.appendId>
+        <elasticsearch.plugin.name>${project.artifactId}</elasticsearch.plugin.name>
         <elasticsearch.plugin.jvm>true</elasticsearch.plugin.jvm>
         <elasticsearch.plugin.isolated>true</elasticsearch.plugin.isolated>
         <elasticsearch.plugin.site>false</elasticsearch.plugin.site>
@@ -282,6 +283,9 @@
                     <include>api/indices.refresh.json</include>
                     <include>api/nodes.info.json</include>
                     <include>api/count.json</include>
+                    <!-- used in repository plugin REST tests -->
+                    <include>api/snapshot.create_repository.json</include>
+                    <include>api/snapshot.get_repository.json</include>
                 </includes>
             </testResource>
             <!-- shared test resources like log4j.properties -->
@@ -365,6 +369,7 @@
                                 <rules>
                                     <requireProperty>
                                         <property>elasticsearch.plugin.classname</property>
+                                        <property>elasticsearch.plugin.name</property>
                                     </requireProperty>
                                 </rules>
                             </configuration>
@@ -429,6 +434,7 @@
         <module>cloud-azure</module>
         <module>cloud-aws</module>
         <module>delete-by-query</module>
+        <module>discovery-multicast</module>
         <module>lang-python</module>
         <module>lang-javascript</module>
         <module>mapper-murmur3</module>
diff --git a/plugins/site-example/src/test/java/org/elasticsearch/example/SiteContentsIT.java b/plugins/site-example/src/test/java/org/elasticsearch/example/SiteContentsIT.java
index b873e29..c92a0ba 100644
--- a/plugins/site-example/src/test/java/org/elasticsearch/example/SiteContentsIT.java
+++ b/plugins/site-example/src/test/java/org/elasticsearch/example/SiteContentsIT.java
@@ -22,6 +22,7 @@ package org.elasticsearch.example;
 import org.apache.http.impl.client.CloseableHttpClient;
 import org.apache.http.impl.client.HttpClients;
 import org.apache.http.impl.conn.PoolingHttpClientConnectionManager;
+import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ExternalTestCluster;
 import org.elasticsearch.test.TestCluster;
@@ -46,7 +47,7 @@ public class SiteContentsIT extends ESIntegTestCase {
             for (InetSocketAddress address :  externalCluster.httpAddresses()) {
                 RestResponse restResponse = new RestResponse(
                         new HttpRequestBuilder(httpClient)
-                        .host(address.getHostName()).port(address.getPort())
+                        .host(NetworkAddress.formatAddress(address.getAddress())).port(address.getPort())
                         .path("/_plugin/site-example/")
                         .method("GET").execute());
                 assertEquals(200, restResponse.getStatusCode());
diff --git a/pom.xml b/pom.xml
index d82b7ca..8295e8e 100644
--- a/pom.xml
+++ b/pom.xml
@@ -103,6 +103,7 @@
         <tests.rest.spec></tests.rest.spec>
         <tests.rest.load_packaged></tests.rest.load_packaged>
         <tests.network></tests.network>
+        <tests.multicast></tests.multicast>
         <tests.cluster></tests.cluster>
         <tests.filter></tests.filter>
         <env.ES_TEST_LOCAL></env.ES_TEST_LOCAL>
@@ -874,10 +875,16 @@
                     <version>2.4.1</version>
                 </plugin>
                 <plugin>
-                    <!-- We just declare which plugin version to use. Each project can have then its own settings -->
                     <groupId>org.apache.maven.plugins</groupId>
                     <artifactId>maven-resources-plugin</artifactId>
                     <version>2.7</version>
+                    <!-- add some additonal binary types to prevent maven from 
+                         screwing them up with resource filtering  -->
+                    <configuration>
+                        <nonFilteredFileExtensions>
+                            <nonFilteredFileExtension>ico</nonFilteredFileExtension>
+                        </nonFilteredFileExtensions>
+                    </configuration>
                 </plugin>
                 <plugin>
                     <groupId>org.apache.maven.plugins</groupId>
diff --git a/qa/smoke-test-multinode/rest-api-spec/test/smoke_test_multinode/10_basic.yaml b/qa/smoke-test-multinode/rest-api-spec/test/smoke_test_multinode/10_basic.yaml
index f365a61..99446f2 100644
--- a/qa/smoke-test-multinode/rest-api-spec/test/smoke_test_multinode/10_basic.yaml
+++ b/qa/smoke-test-multinode/rest-api-spec/test/smoke_test_multinode/10_basic.yaml
@@ -2,18 +2,10 @@
 # If the local machine which is running the test is low on disk space
 # We can have one unassigned shard
 ---
-"cluster health basic test, one index":
-  - do:
-      indices.create:
-        index: test_index
-        body:
-          settings:
-            index:
-              number_of_replicas: 1
-
+"cluster health basic test, wait for both nodes to join":
   - do:
       cluster.health:
-        wait_for_status: yellow
+        wait_for_nodes: 2
 
   - is_true:   cluster_name
   - is_false:  timed_out
diff --git a/qa/smoke-test-plugins/pom.xml b/qa/smoke-test-plugins/pom.xml
index 9714885..beae9eb 100644
--- a/qa/smoke-test-plugins/pom.xml
+++ b/qa/smoke-test-plugins/pom.xml
@@ -314,6 +314,14 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
+                   <artifactId>discovery-multicast</artifactId>
+                   <version>${elasticsearch.version}</version>
+                   <type>zip</type>
+                   <overWrite>true</overWrite>
+                 </artifactItem>
+
+                 <artifactItem>
+                   <groupId>org.elasticsearch.plugin</groupId>
                    <artifactId>lang-python</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/20_tar_package.bats b/qa/vagrant/src/test/resources/packaging/scripts/20_tar_package.bats
index b5daaec..61210d2 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/20_tar_package.bats
+++ b/qa/vagrant/src/test/resources/packaging/scripts/20_tar_package.bats
@@ -31,37 +31,31 @@
 # Load test utilities
 load packaging_test_utils
 
-# Cleans everything for the 1st execution
 setup() {
-    if [ "$BATS_TEST_NUMBER" -eq 1 ]; then
-        clean_before_test
-    fi
+    skip_not_tar_gz
 }
 
 ##################################
 # Install TAR GZ package
 ##################################
 @test "[TAR] tar command is available" {
-    skip_not_tar_gz
+    # Cleans everything for the 1st execution
+    clean_before_test
     run tar --version
     [ "$status" -eq 0 ]
 }
 
 @test "[TAR] archive is available" {
-    skip_not_tar_gz
     count=$(find . -type f -name 'elasticsearch*.tar.gz' | wc -l)
     [ "$count" -eq 1 ]
 }
 
 @test "[TAR] archive is not installed" {
-    skip_not_tar_gz
     count=$(find /tmp -type d -name 'elasticsearch*' | wc -l)
     [ "$count" -eq 0 ]
 }
 
 @test "[TAR] install archive" {
-    skip_not_tar_gz
-
     # Install the archive
     install_archive
 
@@ -73,8 +67,6 @@ setup() {
 # Check that the archive is correctly installed
 ##################################
 @test "[TAR] verify archive installation" {
-    skip_not_tar_gz
-
     verify_archive_installation "/tmp/elasticsearch"
 }
 
@@ -82,14 +74,11 @@ setup() {
 # Check that Elasticsearch is working
 ##################################
 @test "[TAR] test elasticsearch" {
-    skip_not_tar_gz
-
     start_elasticsearch_service
 
     run_elasticsearch_tests
 
     stop_elasticsearch_service
 
-    run rm -rf "/tmp/elasticsearch"
-    [ "$status" -eq 0 ]
+    rm -rf "/tmp/elasticsearch"
 }
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash b/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
index c3773b5..cbb6838 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
@@ -242,34 +242,27 @@ install_archive() {
         eshome="$1"
     fi
 
-    run tar -xzvf elasticsearch*.tar.gz -C "$eshome" >&2
-    [ "$status" -eq 0 ]
+    tar -xzvf elasticsearch*.tar.gz -C "$eshome"
 
-    run find "$eshome" -depth -type d -name 'elasticsearch*' -exec mv {} "$eshome/elasticsearch" \;
-    [ "$status" -eq 0 ]
+    find "$eshome" -depth -type d -name 'elasticsearch*' -exec mv {} "$eshome/elasticsearch" \;
 
     # ES cannot run as root so create elasticsearch user & group if needed
     if ! getent group "elasticsearch" > /dev/null 2>&1 ; then
         if is_dpkg; then
-            run addgroup --system "elasticsearch"
-            [ "$status" -eq 0 ]
+            addgroup --system "elasticsearch"
         else
-            run groupadd -r "elasticsearch"
-            [ "$status" -eq 0 ]
+            groupadd -r "elasticsearch"
         fi
     fi
     if ! id "elasticsearch" > /dev/null 2>&1 ; then
         if is_dpkg; then
-            run adduser --quiet --system --no-create-home --ingroup "elasticsearch" --disabled-password --shell /bin/false "elasticsearch"
-            [ "$status" -eq 0 ]
+            adduser --quiet --system --no-create-home --ingroup "elasticsearch" --disabled-password --shell /bin/false "elasticsearch"
         else
-            run useradd --system -M --gid "elasticsearch" --shell /sbin/nologin --comment "elasticsearch user" "elasticsearch"
-            [ "$status" -eq 0 ]
+            useradd --system -M --gid "elasticsearch" --shell /sbin/nologin --comment "elasticsearch user" "elasticsearch"
         fi
     fi
 
-    run chown -R elasticsearch:elasticsearch "$eshome/elasticsearch"
-    [ "$status" -eq 0 ]
+    chown -R elasticsearch:elasticsearch "$eshome/elasticsearch"
 }
 
 
@@ -354,11 +347,12 @@ clean_before_test() {
 }
 
 start_elasticsearch_service() {
-
     if [ -f "/tmp/elasticsearch/bin/elasticsearch" ]; then
-        run /bin/su -s /bin/sh -c '/tmp/elasticsearch/bin/elasticsearch -d -p /tmp/elasticsearch/elasticsearch.pid' elasticsearch
-        [ "$status" -eq 0 ]
-
+        # su and the Elasticsearch init script work together to break bats.
+        # sudo isolates bats enough from the init script so everything continues
+        # to tick along
+        sudo -u elasticsearch /tmp/elasticsearch/bin/elasticsearch -d \
+            -p /tmp/elasticsearch/elasticsearch.pid
     elif is_systemd; then
         run systemctl daemon-reload
         [ "$status" -eq 0 ]
@@ -383,9 +377,8 @@ start_elasticsearch_service() {
         pid=$(cat /tmp/elasticsearch/elasticsearch.pid)
         [ "x$pid" != "x" ] && [ "$pid" -gt 0 ]
 
-        run  ps $pid
-        [ "$status" -eq 0 ]
-
+        echo "Looking for elasticsearch pid...."
+        ps $pid
     elif is_systemd; then
         run systemctl is-active elasticsearch.service
         [ "$status" -eq 0 ]
@@ -400,14 +393,11 @@ start_elasticsearch_service() {
 }
 
 stop_elasticsearch_service() {
-
     if [ -r "/tmp/elasticsearch/elasticsearch.pid" ]; then
         pid=$(cat /tmp/elasticsearch/elasticsearch.pid)
         [ "x$pid" != "x" ] && [ "$pid" -gt 0 ]
 
-        run kill -SIGTERM $pid
-        [ "$status" -eq 0 ]
-
+        kill -SIGTERM $pid
     elif is_systemd; then
         run systemctl stop elasticsearch.service
         [ "$status" -eq 0 ]
@@ -428,36 +418,63 @@ stop_elasticsearch_service() {
 
 # Waits for Elasticsearch to reach a given status (defaults to "green")
 wait_for_elasticsearch_status() {
-    local status="green"
+    local desired_status="green"
     if [ "x$1" != "x" ]; then
         status="$1"
     fi
 
-    # Try to connect to elasticsearch and wait for expected status
-    wget --quiet --retry-connrefused --waitretry=1 --timeout=60 \
-         --output-document=/dev/null "http://localhost:9200/_cluster/health?wait_for_status=$status&timeout=60s" || true
+    echo "Making sure elasticsearch is up..."
+    wget -O - --retry-connrefused --waitretry=1 --timeout=60 http://localhost:9200 || {
+          echo "Looks like elasticsearch never started. Here is its log:"
+          if [ -r "/tmp/elasticsearch/elasticsearch.pid" ]; then
+              cat /tmp/elasticsearch/log/elasticsearch.log
+          else
+              if [ -e '/var/log/elasticsearch/elasticsearch.log' ]; then
+                  cat /var/log/elasticsearch/elasticsearch.log
+              else
+                  echo "The elasticsearch log doesn't exist. Maybe /vag/log/messages has something:"
+                  tail -n20 /var/log/messages
+              fi
+          fi
+          false
+    }
+
+    echo "Tring to connect to elasticsearch and wait for expected status..."
+    curl -sS "http://localhost:9200/_cluster/health?wait_for_status=$desired_status&timeout=60s&pretty"
+    if [ $? -eq 0 ]; then
+        echo "Connected"
+    else
+        echo "Unable to connect to Elastisearch"
+        false
+    fi
 
-    # Checks the cluster health
-    curl -XGET 'http://localhost:9200/_cat/health?h=status&v=false'
-    if [ $? -ne 0 ]; then
-        echo "error when checking cluster health" >&2
-        exit 1
+    echo "Checking that the cluster health matches the waited for status..."
+    run curl -sS -XGET 'http://localhost:9200/_cat/health?h=status&v=false'
+    if [ "$status" -ne 0 ]; then
+        echo "error when checking cluster health. code=$status output="
+        echo $output
+        false
     fi
+    echo $output | grep $desired_status || {
+        echo "unexpected status:  '$output' wanted '$desired_status'"
+        false
+    }
 }
 
 # Executes some very basic Elasticsearch tests
 run_elasticsearch_tests() {
+    # TODO this assertion is the same the one made when waiting for
+    # elasticsearch to start
     run curl -XGET 'http://localhost:9200/_cat/health?h=status&v=false'
     [ "$status" -eq 0 ]
     echo "$output" | grep -w "green"
 
-    run curl -XPOST 'http://localhost:9200/library/book/1?refresh=true' -d '{"title": "Elasticsearch - The Definitive Guide"}' 2>&1
-    [ "$status" -eq 0 ]
+    curl -s -XPOST 'http://localhost:9200/library/book/1?refresh=true&pretty' -d '{
+      "title": "Elasticsearch - The Definitive Guide"
+    }'
 
-    run curl -XGET 'http://localhost:9200/_cat/count?h=count&v=false'
-    [ "$status" -eq 0 ]
-    echo "$output" | grep -w "1"
+    curl -s -XGET 'http://localhost:9200/_cat/count?h=count&v=false&pretty' |
+      grep -w "1"
 
-    run curl -XDELETE 'http://localhost:9200/_all'
-    [ "$status" -eq 0 ]
+    curl -s -XDELETE 'http://localhost:9200/_all'
 }
