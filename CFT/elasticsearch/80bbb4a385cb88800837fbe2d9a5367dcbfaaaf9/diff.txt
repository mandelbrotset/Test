diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
index 1bc72fa..00f34f4 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
@@ -49,7 +49,6 @@ import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchService;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.internal.DefaultSearchContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.internal.ShardSearchLocalRequest;
@@ -77,20 +76,17 @@ public class TransportValidateQueryAction extends TransportBroadcastAction<Valid
 
     private final BigArrays bigArrays;
 
-    private final FetchPhase fetchPhase;
-
     @Inject
     public TransportValidateQueryAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
-            TransportService transportService, IndicesService indicesService, ScriptService scriptService,
-            PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, ActionFilters actionFilters,
-            IndexNameExpressionResolver indexNameExpressionResolver, FetchPhase fetchPhase) {
+                                        TransportService transportService, IndicesService indicesService,
+                                        ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
+                                        BigArrays bigArrays, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {
         super(settings, ValidateQueryAction.NAME, threadPool, clusterService, transportService, actionFilters,
                 indexNameExpressionResolver, ValidateQueryRequest::new, ShardValidateQueryRequest::new, ThreadPool.Names.SEARCH);
         this.indicesService = indicesService;
         this.scriptService = scriptService;
         this.pageCacheRecycler = pageCacheRecycler;
         this.bigArrays = bigArrays;
-        this.fetchPhase = fetchPhase;
     }
 
     @Override
@@ -168,7 +164,8 @@ public class TransportValidateQueryAction extends TransportBroadcastAction<Valid
     protected ShardValidateQueryResponse shardOperation(ShardValidateQueryRequest request) {
         IndexService indexService = indicesService.indexServiceSafe(request.shardId().getIndex());
         IndexShard indexShard = indexService.getShard(request.shardId().id());
-        final QueryShardContext queryShardContext = indexShard.getQueryShardContext();
+        final QueryShardContext queryShardContext = indexService.newQueryShardContext();
+        queryShardContext.setTypes(request.types());
 
         boolean valid;
         String explanation = null;
@@ -176,9 +173,11 @@ public class TransportValidateQueryAction extends TransportBroadcastAction<Valid
         Engine.Searcher searcher = indexShard.acquireSearcher("validate_query");
 
         DefaultSearchContext searchContext = new DefaultSearchContext(0,
-                new ShardSearchLocalRequest(request.types(), request.nowInMillis(), request.filteringAliases()), null, searcher,
-                indexService, indexShard, scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(),
-                parseFieldMatcher, SearchService.NO_TIMEOUT, fetchPhase);
+                new ShardSearchLocalRequest(request.types(), request.nowInMillis(), request.filteringAliases()),
+                null, searcher, indexService, indexShard,
+                scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher,
+                SearchService.NO_TIMEOUT
+        );
         SearchContext.setCurrent(searchContext);
         try {
             searchContext.parsedQuery(queryShardContext.toQuery(request.query()));
diff --git a/core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java b/core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java
index 5caef9f..3ebef7c 100644
--- a/core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java
+++ b/core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java
@@ -44,7 +44,6 @@ import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchService;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.internal.DefaultSearchContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.internal.ShardSearchLocalRequest;
@@ -69,20 +68,17 @@ public class TransportExplainAction extends TransportSingleShardAction<ExplainRe
 
     private final BigArrays bigArrays;
 
-    private final FetchPhase fetchPhase;
-
     @Inject
     public TransportExplainAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
-            TransportService transportService, IndicesService indicesService, ScriptService scriptService,
-            PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, ActionFilters actionFilters,
-            IndexNameExpressionResolver indexNameExpressionResolver, FetchPhase fetchPhase) {
+                                  TransportService transportService, IndicesService indicesService,
+                                  ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
+                                  BigArrays bigArrays, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {
         super(settings, ExplainAction.NAME, threadPool, clusterService, transportService, actionFilters, indexNameExpressionResolver,
                 ExplainRequest::new, ThreadPool.Names.GET);
         this.indicesService = indicesService;
         this.scriptService = scriptService;
         this.pageCacheRecycler = pageCacheRecycler;
         this.bigArrays = bigArrays;
-        this.fetchPhase = fetchPhase;
     }
 
     @Override
@@ -115,14 +111,17 @@ public class TransportExplainAction extends TransportSingleShardAction<ExplainRe
             return new ExplainResponse(shardId.getIndexName(), request.type(), request.id(), false);
         }
 
-        SearchContext context = new DefaultSearchContext(0,
-                new ShardSearchLocalRequest(new String[] { request.type() }, request.nowInMillis, request.filteringAlias()), null,
-                result.searcher(), indexService, indexShard, scriptService, pageCacheRecycler, bigArrays,
-                threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher, SearchService.NO_TIMEOUT, fetchPhase);
+        SearchContext context = new DefaultSearchContext(
+                0, new ShardSearchLocalRequest(new String[]{request.type()}, request.nowInMillis, request.filteringAlias()),
+                null, result.searcher(), indexService, indexShard,
+                scriptService, pageCacheRecycler,
+                bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher,
+                SearchService.NO_TIMEOUT
+        );
         SearchContext.setCurrent(context);
 
         try {
-            context.parsedQuery(indexShard.getQueryShardContext().toQuery(request.query()));
+            context.parsedQuery(context.getQueryShardContext().toQuery(request.query()));
             context.preProcess();
             int topLevelDocId = result.docIdAndVersion().docId + result.docIdAndVersion().context.docBase;
             Explanation explanation = context.searcher().explain(context.query(), topLevelDocId);
diff --git a/core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java
index 987c9f7..e4b3a04 100644
--- a/core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java
@@ -26,8 +26,7 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.sort.SortBuilder;
 
@@ -153,26 +152,16 @@ public class PercolateRequestBuilder extends BroadcastOperationRequestBuilder<Pe
     }
 
     /**
-     * Delegates to
-     * {@link PercolateSourceBuilder#addAggregation(AggregatorBuilder)}
+     * Delegates to {@link PercolateSourceBuilder#addAggregation(AbstractAggregationBuilder)}
      */
-    public PercolateRequestBuilder addAggregation(AggregatorBuilder<?> aggregationBuilder) {
+    public PercolateRequestBuilder addAggregation(AbstractAggregationBuilder aggregationBuilder) {
         sourceBuilder().addAggregation(aggregationBuilder);
         return this;
     }
 
     /**
-     * Delegates to
-     * {@link PercolateSourceBuilder#addAggregation(PipelineAggregatorFactory)}
-     */
-    public PercolateRequestBuilder addAggregation(PipelineAggregatorFactory aggregationBuilder) {
-        sourceBuilder().addAggregation(aggregationBuilder);
-        return this;
-    }
-
-    /**
-     * Sets the percolate request definition directly on the request. This will
-     * overwrite any definitions set by any of the delegate methods.
+     * Sets the percolate request definition directly on the request.
+     * This will overwrite any definitions set by any of the delegate methods.
      */
     public PercolateRequestBuilder setSource(PercolateSourceBuilder source) {
         sourceBuilder = source;
diff --git a/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java b/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java
index 254648d..b080039 100644
--- a/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java
@@ -29,8 +29,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.sort.ScoreSortBuilder;
 import org.elasticsearch.search.sort.SortBuilder;
@@ -52,8 +51,7 @@ public class PercolateSourceBuilder extends ToXContentToBytes {
     private List<SortBuilder> sorts;
     private Boolean trackScores;
     private HighlightBuilder highlightBuilder;
-    private List<AggregatorBuilder<?>> aggregationFactorys;
-    private List<PipelineAggregatorFactory> pipelineAggregationFactorys;
+    private List<AbstractAggregationBuilder> aggregations;
 
     /**
      * Sets the document to run the percolate queries against.
@@ -125,22 +123,11 @@ public class PercolateSourceBuilder extends ToXContentToBytes {
     /**
      * Add an aggregation definition.
      */
-    public PercolateSourceBuilder addAggregation(AggregatorBuilder<?> aggregationBuilder) {
-        if (aggregationFactorys == null) {
-            aggregationFactorys = new ArrayList<>();
+    public PercolateSourceBuilder addAggregation(AbstractAggregationBuilder aggregationBuilder) {
+        if (aggregations == null) {
+            aggregations = new ArrayList<>();
         }
-        aggregationFactorys.add(aggregationBuilder);
-        return this;
-    }
-
-    /**
-     * Add an aggregation definition.
-     */
-    public PercolateSourceBuilder addAggregation(PipelineAggregatorFactory aggregationBuilder) {
-        if (pipelineAggregationFactorys == null) {
-            pipelineAggregationFactorys = new ArrayList<>();
-        }
-        pipelineAggregationFactorys.add(aggregationBuilder);
+        aggregations.add(aggregationBuilder);
         return this;
     }
 
@@ -172,18 +159,11 @@ public class PercolateSourceBuilder extends ToXContentToBytes {
         if (highlightBuilder != null) {
             highlightBuilder.toXContent(builder, params);
         }
-        if (aggregationFactorys != null || pipelineAggregationFactorys != null) {
+        if (aggregations != null) {
             builder.field("aggregations");
             builder.startObject();
-            if (aggregationFactorys != null) {
-                for (AggregatorBuilder<?> aggregation : aggregationFactorys) {
-                    aggregation.toXContent(builder, params);
-                }
-            }
-            if (pipelineAggregationFactorys != null) {
-                for (PipelineAggregatorFactory aggregation : pipelineAggregationFactorys) {
-                    aggregation.toXContent(builder, params);
-                }
+            for (AbstractAggregationBuilder aggregation : aggregations) {
+                aggregation.toXContent(builder, params);
             }
             builder.endObject();
         }
diff --git a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
index 6ba24d8..07d7b2f 100644
--- a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
@@ -28,8 +28,7 @@ import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
 import org.elasticsearch.search.highlight.HighlightBuilder;
@@ -372,17 +371,9 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     }
 
     /**
-     * Adds an aggregation to the search operation.
+     * Adds an get to the search operation.
      */
-    public SearchRequestBuilder addAggregation(AggregatorBuilder<?> aggregation) {
-        sourceBuilder().aggregation(aggregation);
-        return this;
-    }
-
-    /**
-     * Adds an aggregation to the search operation.
-     */
-    public SearchRequestBuilder addAggregation(PipelineAggregatorFactory aggregation) {
+    public SearchRequestBuilder addAggregation(AbstractAggregationBuilder aggregation) {
         sourceBuilder().aggregation(aggregation);
         return this;
     }
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
index ed99f9b..a1468f4 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
@@ -33,6 +33,7 @@ import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.logging.log4j.LogConfigurator;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.monitor.jvm.JvmInfo;
@@ -148,10 +149,11 @@ final class Bootstrap {
     }
 
     private void setup(boolean addShutdownHook, Settings settings, Environment environment) throws Exception {
-        initializeNatives(environment.tmpFile(),
-                          settings.getAsBoolean("bootstrap.mlockall", false),
-                          settings.getAsBoolean("bootstrap.seccomp", true),
-                          settings.getAsBoolean("bootstrap.ctrlhandler", true));
+        initializeNatives(
+                environment.tmpFile(),
+                BootstrapSettings.MLOCKALL_SETTING.get(settings),
+                BootstrapSettings.SECCOMP_SETTING.get(settings),
+                BootstrapSettings.CTRLHANDLER_SETTING.get(settings));
 
         // initialize probes before the security manager is installed
         initializeProbes();
@@ -186,22 +188,11 @@ final class Bootstrap {
         node = new Node(nodeSettings);
     }
 
-    /**
-     * option for elasticsearch.yml etc to turn off our security manager completely,
-     * for example if you want to have your own configuration or just disable.
-     */
-    // TODO: remove this: http://www.openbsd.org/papers/hackfest2015-pledge/mgp00005.jpg
-    static final String SECURITY_SETTING = "security.manager.enabled";
-    /**
-     * option for elasticsearch.yml to fully respect the system policy, including bad defaults
-     * from java.
-     */
-    // TODO: remove this hack when insecure defaults are removed from java
-    static final String SECURITY_FILTER_BAD_DEFAULTS_SETTING = "security.manager.filter_bad_defaults";
+
 
     private void setupSecurity(Settings settings, Environment environment) throws Exception {
-        if (settings.getAsBoolean(SECURITY_SETTING, true)) {
-            Security.configure(environment, settings.getAsBoolean(SECURITY_FILTER_BAD_DEFAULTS_SETTING, true));
+        if (BootstrapSettings.SECURITY_MANAGER_ENABLED_SETTING.get(settings)) {
+            Security.configure(environment, BootstrapSettings.SECURITY_FILTER_BAD_DEFAULTS_SETTING.get(settings));
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/BootstrapSettings.java b/core/src/main/java/org/elasticsearch/bootstrap/BootstrapSettings.java
new file mode 100644
index 0000000..9122504
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/bootstrap/BootstrapSettings.java
@@ -0,0 +1,46 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.bootstrap;
+
+import org.elasticsearch.common.settings.Setting;
+import org.elasticsearch.common.settings.Setting.Scope;
+
+public final class BootstrapSettings {
+
+    private BootstrapSettings() {
+    }
+
+    // TODO: remove this: http://www.openbsd.org/papers/hackfest2015-pledge/mgp00005.jpg
+    /**
+     * option to turn off our security manager completely, for example
+     * if you want to have your own configuration or just disable
+     */
+    public static final Setting<Boolean> SECURITY_MANAGER_ENABLED_SETTING =
+            Setting.boolSetting("security.manager.enabled", true, false, Scope.CLUSTER);
+
+    // TODO: remove this hack when insecure defaults are removed from java
+    public static final Setting<Boolean> SECURITY_FILTER_BAD_DEFAULTS_SETTING =
+            Setting.boolSetting("security.manager.filter_bad_defaults", true, false, Scope.CLUSTER);
+
+    public static final Setting<Boolean> MLOCKALL_SETTING = Setting.boolSetting("bootstrap.mlockall", false, false, Scope.CLUSTER);
+    public static final Setting<Boolean> SECCOMP_SETTING = Setting.boolSetting("bootstrap.seccomp", true, false, Scope.CLUSTER);
+    public static final Setting<Boolean> CTRLHANDLER_SETTING = Setting.boolSetting("bootstrap.ctrlhandler", true, false, Scope.CLUSTER);
+
+}
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/JavaVersion.java b/core/src/main/java/org/elasticsearch/bootstrap/JavaVersion.java
index bf8e996..c37f373 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/JavaVersion.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/JavaVersion.java
@@ -19,28 +19,26 @@
 
 package org.elasticsearch.bootstrap;
 
-import org.elasticsearch.common.Strings;
-
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.List;
+import java.util.Objects;
+import java.util.stream.Collectors;
 
 class JavaVersion implements Comparable<JavaVersion> {
     private final List<Integer> version;
 
     public List<Integer> getVersion() {
-        return Collections.unmodifiableList(version);
+        return version;
     }
 
     private JavaVersion(List<Integer> version) {
-        this.version = version;
+        this.version = Collections.unmodifiableList(version);
     }
 
     public static JavaVersion parse(String value) {
-        if (value == null) {
-            throw new NullPointerException("value");
-        }
-        if ("".equals(value)) {
+        Objects.requireNonNull(value);
+        if (!isValid(value)) {
             throw new IllegalArgumentException("value");
         }
 
@@ -79,6 +77,6 @@ class JavaVersion implements Comparable<JavaVersion> {
 
     @Override
     public String toString() {
-        return Strings.collectionToDelimitedString(version, ".");
+        return version.stream().map(v -> Integer.toString(v)).collect(Collectors.joining("."));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
index fbf3446..62f3ad8 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
@@ -53,7 +53,6 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.env.Environment;
-import org.elasticsearch.index.Index;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.NodeServicesProvider;
 import org.elasticsearch.index.mapper.DocumentMapper;
@@ -336,7 +335,7 @@ public class MetaDataCreateIndexService extends AbstractComponent {
                                 }
                             }
 
-                            QueryShardContext queryShardContext = indexService.getQueryShardContext();
+                            final QueryShardContext queryShardContext = indexService.newQueryShardContext();
                             for (Alias alias : request.aliases()) {
                                 if (Strings.hasLength(alias.filter())) {
                                     aliasValidator.validateAliasFilter(alias.name(), alias.filter(), queryShardContext);
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java
index fe53f20..52154bd 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java
@@ -117,7 +117,7 @@ public class MetaDataIndexAliasesService extends AbstractComponent {
                                     indices.put(indexMetaData.getIndex().getName(), indexService);
                                 }
 
-                                aliasValidator.validateAliasFilter(aliasAction.alias(), filter, indexService.getQueryShardContext());
+                                aliasValidator.validateAliasFilter(aliasAction.alias(), filter, indexService.newQueryShardContext());
                             }
                             AliasMetaData newAliasMd = AliasMetaData.newAliasMetaDataBuilder(
                                     aliasAction.alias())
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
index 1087fe8..6d10cac 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
@@ -39,8 +39,6 @@ import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
 import org.elasticsearch.search.rescore.RescoreBuilder;
 import org.elasticsearch.tasks.Task;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
 
@@ -666,20 +664,6 @@ public abstract class StreamInput extends InputStream {
     }
 
     /**
-     * Reads a {@link AggregatorBuilder} from the current stream
-     */
-    public AggregatorBuilder readAggregatorFactory() throws IOException {
-        return readNamedWriteable(AggregatorBuilder.class);
-    }
-
-    /**
-     * Reads a {@link PipelineAggregatorFactory} from the current stream
-     */
-    public PipelineAggregatorFactory readPipelineAggregatorFactory() throws IOException {
-        return readNamedWriteable(PipelineAggregatorFactory.class);
-    }
-
-    /**
      * Reads a {@link QueryBuilder} from the current stream
      */
     public QueryBuilder readQuery() throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
index 5e297ea..77d4005 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
@@ -38,8 +38,6 @@ import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
 import org.elasticsearch.search.rescore.RescoreBuilder;
 import org.elasticsearch.tasks.Task;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.joda.time.ReadableInstant;
 
 import java.io.EOFException;
@@ -643,20 +641,6 @@ public abstract class StreamOutput extends OutputStream {
     }
 
     /**
-     * Writes a {@link AggregatorBuilder} to the current stream
-     */
-    public void writeAggregatorFactory(AggregatorBuilder factory) throws IOException {
-        writeNamedWriteable(factory);
-    }
-
-    /**
-     * Writes a {@link PipelineAggregatorFactory} to the current stream
-     */
-    public void writePipelineAggregatorFactory(PipelineAggregatorFactory factory) throws IOException {
-        writeNamedWriteable(factory);
-    }
-
-    /**
      * Writes a {@link QueryBuilder} to the current stream
      */
     public void writeQuery(QueryBuilder queryBuilder) throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/common/network/Cidrs.java b/core/src/main/java/org/elasticsearch/common/network/Cidrs.java
index f0bd4fb..d055724 100644
--- a/core/src/main/java/org/elasticsearch/common/network/Cidrs.java
+++ b/core/src/main/java/org/elasticsearch/common/network/Cidrs.java
@@ -113,8 +113,4 @@ public final class Cidrs {
         assert octets.length == 4;
         return octetsToString(octets) + "/" + networkMask;
     }
-
-    public static String createCIDR(long ipAddress, int networkMask) {
-        return octetsToCIDR(longToOctets(ipAddress), networkMask);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/rounding/Rounding.java b/core/src/main/java/org/elasticsearch/common/rounding/Rounding.java
index 7e94ebb..89a2679 100644
--- a/core/src/main/java/org/elasticsearch/common/rounding/Rounding.java
+++ b/core/src/main/java/org/elasticsearch/common/rounding/Rounding.java
@@ -19,13 +19,11 @@
 package org.elasticsearch.common.rounding;
 
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A strategy for rounding long values.
@@ -63,12 +61,6 @@ public abstract class Rounding implements Streamable {
      */
     public abstract long nextRoundingValue(long value);
 
-    @Override
-    public abstract boolean equals(Object obj);
-
-    @Override
-    public abstract int hashCode();
-
     /**
      * Rounding strategy which is based on an interval
      *
@@ -78,8 +70,6 @@ public abstract class Rounding implements Streamable {
 
         final static byte ID = 0;
 
-        public static final ParseField INTERVAL_FIELD = new ParseField("interval");
-
         private long interval;
 
         public Interval() { // for serialization
@@ -136,31 +126,12 @@ public abstract class Rounding implements Streamable {
         public void writeTo(StreamOutput out) throws IOException {
             out.writeVLong(interval);
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(interval);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            Interval other = (Interval) obj;
-            return Objects.equals(interval, other.interval);
-        }
     }
 
     public static class FactorRounding extends Rounding {
 
         final static byte ID = 7;
 
-        public static final ParseField FACTOR_FIELD = new ParseField("factor");
-
         private Rounding rounding;
 
         private float factor;
@@ -195,7 +166,7 @@ public abstract class Rounding implements Streamable {
 
         @Override
         public void readFrom(StreamInput in) throws IOException {
-            rounding = Rounding.Streams.read(in);
+            rounding = (TimeZoneRounding) Rounding.Streams.read(in);
             factor = in.readFloat();
         }
 
@@ -204,32 +175,12 @@ public abstract class Rounding implements Streamable {
             Rounding.Streams.write(rounding, out);
             out.writeFloat(factor);
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(rounding, factor);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            FactorRounding other = (FactorRounding) obj;
-            return Objects.equals(rounding, other.rounding)
-                    && Objects.equals(factor, other.factor);
-        }
     }
 
     public static class OffsetRounding extends Rounding {
 
         final static byte ID = 8;
 
-        public static final ParseField OFFSET_FIELD = new ParseField("offset");
-
         private Rounding rounding;
 
         private long offset;
@@ -273,24 +224,6 @@ public abstract class Rounding implements Streamable {
             Rounding.Streams.write(rounding, out);
             out.writeLong(offset);
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(rounding, offset);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            OffsetRounding other = (OffsetRounding) obj;
-            return Objects.equals(rounding, other.rounding)
-                    && Objects.equals(offset, other.offset);
-        }
     }
 
     public static class Streams {
diff --git a/core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java b/core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java
index 4189e41..1e6bbb6 100644
--- a/core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java
+++ b/core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.common.rounding;
 
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.TimeValue;
@@ -28,13 +27,10 @@ import org.joda.time.DateTimeZone;
 import org.joda.time.DurationField;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  */
 public abstract class TimeZoneRounding extends Rounding {
-    public static final ParseField INTERVAL_FIELD = new ParseField("interval");
-    public static final ParseField TIME_ZONE_FIELD = new ParseField("time_zone");
 
     public static Builder builder(DateTimeUnit unit) {
         return new Builder(unit);
@@ -161,24 +157,6 @@ public abstract class TimeZoneRounding extends Rounding {
             out.writeByte(unit.id());
             out.writeString(timeZone.getID());
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(unit, timeZone);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            TimeUnitRounding other = (TimeUnitRounding) obj;
-            return Objects.equals(unit, other.unit)
-                    && Objects.equals(timeZone, other.timeZone);
-        }
     }
 
     static class TimeIntervalRounding extends TimeZoneRounding {
@@ -236,23 +214,5 @@ public abstract class TimeZoneRounding extends Rounding {
             out.writeVLong(interval);
             out.writeString(timeZone.getID());
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(interval, timeZone);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            TimeIntervalRounding other = (TimeIntervalRounding) obj;
-            return Objects.equals(interval, other.interval)
-                    && Objects.equals(timeZone, other.timeZone);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java b/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
index e4b92d8..033423b 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
@@ -88,6 +88,7 @@ import org.elasticsearch.transport.TransportService;
 import org.elasticsearch.transport.TransportSettings;
 import org.elasticsearch.transport.netty.NettyTransport;
 import org.elasticsearch.tribe.TribeService;
+import org.elasticsearch.bootstrap.BootstrapSettings;
 
 import java.util.Arrays;
 import java.util.Collections;
@@ -378,6 +379,11 @@ public final class ClusterSettings extends AbstractScopedSettings {
             PageCacheRecycler.WEIGHT_LONG_SETTING,
             PageCacheRecycler.WEIGHT_OBJECTS_SETTING,
             PageCacheRecycler.TYPE_SETTING,
-            PluginsService.MANDATORY_SETTING
+            PluginsService.MANDATORY_SETTING,
+            BootstrapSettings.SECURITY_MANAGER_ENABLED_SETTING,
+            BootstrapSettings.SECURITY_FILTER_BAD_DEFAULTS_SETTING,
+            BootstrapSettings.MLOCKALL_SETTING,
+            BootstrapSettings.SECCOMP_SETTING,
+            BootstrapSettings.CTRLHANDLER_SETTING
         )));
 }
diff --git a/core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java b/core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java
index 4197da9..157bbfb 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java
@@ -117,7 +117,7 @@ public final class IndexScopedSettings extends AbstractScopedSettings {
         UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING,
         EnableAllocationDecider.INDEX_ROUTING_REBALANCE_ENABLE_SETTING,
         EnableAllocationDecider.INDEX_ROUTING_ALLOCATION_ENABLE_SETTING,
-        IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING,
+        IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING,
         IndexFieldDataService.INDEX_FIELDDATA_CACHE_KEY,
         FieldMapper.IGNORE_MALFORMED_SETTING,
         FieldMapper.COERCE_SETTING,
diff --git a/core/src/main/java/org/elasticsearch/common/settings/Setting.java b/core/src/main/java/org/elasticsearch/common/settings/Setting.java
index 17d670b..22485ec 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/Setting.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/Setting.java
@@ -256,8 +256,8 @@ public class Setting<T> extends ToXContentToBytes {
      * this is used for settings that depend on each other... see {@link org.elasticsearch.common.settings.AbstractScopedSettings#addSettingsUpdateConsumer(Setting, Setting, BiConsumer)} and it's
      * usage for details.
      */
-    static <A, B> AbstractScopedSettings.SettingUpdater<Tuple<A, B>> compoundUpdater(final BiConsumer<A,B> consumer, final Setting<A> aSettting, final Setting<B> bSetting, ESLogger logger) {
-        final AbstractScopedSettings.SettingUpdater<A> aSettingUpdater = aSettting.newUpdater(null, logger);
+    static <A, B> AbstractScopedSettings.SettingUpdater<Tuple<A, B>> compoundUpdater(final BiConsumer<A,B> consumer, final Setting<A> aSetting, final Setting<B> bSetting, ESLogger logger) {
+        final AbstractScopedSettings.SettingUpdater<A> aSettingUpdater = aSetting.newUpdater(null, logger);
         final AbstractScopedSettings.SettingUpdater<B> bSettingUpdater = bSetting.newUpdater(null, logger);
         return new AbstractScopedSettings.SettingUpdater<Tuple<A, B>>() {
             @Override
diff --git a/core/src/main/java/org/elasticsearch/env/NodeEnvironment.java b/core/src/main/java/org/elasticsearch/env/NodeEnvironment.java
index 80c427c..5eecafa 100644
--- a/core/src/main/java/org/elasticsearch/env/NodeEnvironment.java
+++ b/core/src/main/java/org/elasticsearch/env/NodeEnvironment.java
@@ -47,6 +47,7 @@ import org.elasticsearch.index.store.FsDirectoryService;
 import org.elasticsearch.monitor.fs.FsInfo;
 import org.elasticsearch.monitor.fs.FsProbe;
 import org.elasticsearch.monitor.jvm.JvmInfo;
+import org.elasticsearch.monitor.process.ProcessProbe;
 
 import java.io.Closeable;
 import java.io.IOException;
@@ -221,6 +222,7 @@ public class NodeEnvironment extends AbstractComponent implements Closeable {
 
         maybeLogPathDetails();
         maybeLogHeapDetails();
+        maybeWarnFileDescriptors();
 
         applySegmentInfosTrace(settings);
     }
@@ -313,6 +315,20 @@ public class NodeEnvironment extends AbstractComponent implements Closeable {
         logger.info("heap size [{}], compressed ordinary object pointers [{}]", maxHeapSize, useCompressedOops);
     }
 
+    private void maybeWarnFileDescriptors() {
+        long maxFileDescriptorCount = ProcessProbe.getInstance().getMaxFileDescriptorCount();
+        if (maxFileDescriptorCount == -1) {
+            return;
+        }
+        int fileDescriptorCountThreshold = (1 << 16);
+        if (maxFileDescriptorCount < fileDescriptorCountThreshold) {
+            logger.warn(
+                    "max file descriptors [{}] for elasticsearch process likely too low, consider increasing to at least [{}]",
+                    maxFileDescriptorCount,
+                    fileDescriptorCountThreshold);
+        }
+    }
+
     @SuppressForbidden(reason = "System.out.*")
     static void applySegmentInfosTrace(Settings settings) {
         if (ENABLE_LUCENE_SEGMENT_INFOS_TRACE_SETTING.get(settings)) {
diff --git a/core/src/main/java/org/elasticsearch/index/IndexService.java b/core/src/main/java/org/elasticsearch/index/IndexService.java
index 86925c3..543a5fc 100644
--- a/core/src/main/java/org/elasticsearch/index/IndexService.java
+++ b/core/src/main/java/org/elasticsearch/index/IndexService.java
@@ -131,7 +131,7 @@ public final class IndexService extends AbstractIndexComponent implements IndexC
         this.indexSettings = indexSettings;
         this.analysisService = registry.build(indexSettings);
         this.similarityService = similarityService;
-        this.mapperService = new MapperService(indexSettings, analysisService, similarityService, mapperRegistry, IndexService.this::getQueryShardContext);
+        this.mapperService = new MapperService(indexSettings, analysisService, similarityService, mapperRegistry, IndexService.this::newQueryShardContext);
         this.indexFieldData = new IndexFieldDataService(indexSettings, indicesFieldDataCache, nodeServicesProvider.getCircuitBreakerService(), mapperService);
         this.shardStoreDeleter = shardStoreDeleter;
         this.eventListener = eventListener;
@@ -417,7 +417,10 @@ public final class IndexService extends AbstractIndexComponent implements IndexC
         return indexSettings;
     }
 
-    public QueryShardContext getQueryShardContext() {
+    /**
+     * Creates a new QueryShardContext. The context has not types set yet, if types are required set them via {@link QueryShardContext#setTypes(String...)}
+     */
+    public QueryShardContext newQueryShardContext() {
         return new QueryShardContext(indexSettings, nodeServicesProvider.getClient(), indexCache.bitsetFilterCache(), indexFieldData, mapperService(), similarityService(), nodeServicesProvider.getScriptService(), nodeServicesProvider.getIndicesQueriesRegistry());
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/IndexSettings.java b/core/src/main/java/org/elasticsearch/index/IndexSettings.java
index d6a4e90..da15d29 100644
--- a/core/src/main/java/org/elasticsearch/index/IndexSettings.java
+++ b/core/src/main/java/org/elasticsearch/index/IndexSettings.java
@@ -82,7 +82,7 @@ public final class IndexSettings {
     public static final Setting<Integer> MAX_RESULT_WINDOW_SETTING = Setting.intSetting("index.max_result_window", 10000, 1, true, Setting.Scope.INDEX);
     public static final TimeValue DEFAULT_REFRESH_INTERVAL = new TimeValue(1, TimeUnit.SECONDS);
     public static final Setting<TimeValue> INDEX_REFRESH_INTERVAL_SETTING = Setting.timeSetting("index.refresh_interval", DEFAULT_REFRESH_INTERVAL, new TimeValue(-1, TimeUnit.MILLISECONDS), true, Setting.Scope.INDEX);
-    public static final Setting<ByteSizeValue> INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING = Setting.byteSizeSetting("index.translog.flush_threshold_size", new ByteSizeValue(512, ByteSizeUnit.MB), true, Setting.Scope.INDEX);
+    public static final Setting<ByteSizeValue> INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING = Setting.byteSizeSetting("index.translog.flush_threshold_size", new ByteSizeValue(512, ByteSizeUnit.MB), true, Setting.Scope.INDEX);
 
 
     /**
@@ -197,7 +197,7 @@ public final class IndexSettings {
         this.durability = scopedSettings.get(INDEX_TRANSLOG_DURABILITY_SETTING);
         syncInterval = INDEX_TRANSLOG_SYNC_INTERVAL_SETTING.get(settings);
         refreshInterval = scopedSettings.get(INDEX_REFRESH_INTERVAL_SETTING);
-        flushThresholdSize = scopedSettings.get(INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING);
+        flushThresholdSize = scopedSettings.get(INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING);
         mergeSchedulerConfig = new MergeSchedulerConfig(this);
         gcDeletesInMillis = scopedSettings.get(INDEX_GC_DELETES_SETTING).getMillis();
         warmerEnabled = scopedSettings.get(INDEX_WARMER_ENABLED_SETTING);
@@ -222,7 +222,7 @@ public final class IndexSettings {
         scopedSettings.addSettingsUpdateConsumer(MAX_RESULT_WINDOW_SETTING, this::setMaxResultWindow);
         scopedSettings.addSettingsUpdateConsumer(INDEX_WARMER_ENABLED_SETTING, this::setEnableWarmer);
         scopedSettings.addSettingsUpdateConsumer(INDEX_GC_DELETES_SETTING, this::setGCDeletes);
-        scopedSettings.addSettingsUpdateConsumer(INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING, this::setTranslogFlushThresholdSize);
+        scopedSettings.addSettingsUpdateConsumer(INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING, this::setTranslogFlushThresholdSize);
         scopedSettings.addSettingsUpdateConsumer(INDEX_REFRESH_INTERVAL_SETTING, this::setRefreshInterval);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/SearchSlowLog.java b/core/src/main/java/org/elasticsearch/index/SearchSlowLog.java
index 200aa1c..df3139f 100644
--- a/core/src/main/java/org/elasticsearch/index/SearchSlowLog.java
+++ b/core/src/main/java/org/elasticsearch/index/SearchSlowLog.java
@@ -136,11 +136,11 @@ public final class SearchSlowLog {
         public String toString() {
             StringBuilder sb = new StringBuilder();
             sb.append("took[").append(TimeValue.timeValueNanos(tookInNanos)).append("], took_millis[").append(TimeUnit.NANOSECONDS.toMillis(tookInNanos)).append("], ");
-            if (context.types() == null) {
+            if (context.getQueryShardContext().getTypes() == null) {
                 sb.append("types[], ");
             } else {
                 sb.append("types[");
-                Strings.arrayToDelimitedString(context.types(), ",", sb);
+                Strings.arrayToDelimitedString(context.getQueryShardContext().getTypes(), ",", sb);
                 sb.append("], ");
             }
             if (context.groupStats() == null) {
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
index f95db85..20aeb34 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
@@ -49,7 +49,6 @@ public class DocumentMapperParser {
 
     final MapperService mapperService;
     final AnalysisService analysisService;
-    private static final ESLogger logger = Loggers.getLogger(DocumentMapperParser.class);
     private final SimilarityService similarityService;
     private final Supplier<QueryShardContext> queryShardContextSupplier;
 
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java
index a983850..fd35398 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java
@@ -421,8 +421,6 @@ public abstract class FieldMapper extends Mapper implements Cloneable {
             builder.field("index_options", indexOptionToString(fieldType().indexOptions()));
         }
 
-        doXContentAnalyzers(builder, includeDefaults);
-
         if (fieldType().similarity() != null) {
             builder.field("similarity", fieldType().similarity().name());
         } else if (includeDefaults) {
@@ -439,15 +437,26 @@ public abstract class FieldMapper extends Mapper implements Cloneable {
         }
     }
 
-    protected void doXContentAnalyzers(XContentBuilder builder, boolean includeDefaults) throws IOException {
+    protected final void doXContentAnalyzers(XContentBuilder builder, boolean includeDefaults) throws IOException {
+        if (fieldType.tokenized() == false) {
+            return;
+        }
         if (fieldType().indexAnalyzer() == null) {
             if (includeDefaults) {
                 builder.field("analyzer", "default");
             }
-        } else if (includeDefaults || fieldType().indexAnalyzer().name().startsWith("_") == false && fieldType().indexAnalyzer().name().equals("default") == false) {
-            builder.field("analyzer", fieldType().indexAnalyzer().name());
-            if (fieldType().searchAnalyzer().name().equals(fieldType().indexAnalyzer().name()) == false) {
-                builder.field("search_analyzer", fieldType().searchAnalyzer().name());
+        } else {
+            boolean hasDefaultIndexAnalyzer = fieldType().indexAnalyzer().name().equals("default");
+            boolean hasDifferentSearchAnalyzer = fieldType().searchAnalyzer().name().equals(fieldType().indexAnalyzer().name()) == false;
+            boolean hasDifferentSearchQuoteAnalyzer = fieldType().searchAnalyzer().name().equals(fieldType().searchQuoteAnalyzer().name()) == false;
+            if (includeDefaults || hasDefaultIndexAnalyzer == false || hasDifferentSearchAnalyzer || hasDifferentSearchQuoteAnalyzer) {
+                builder.field("analyzer", fieldType().indexAnalyzer().name());
+                if (hasDifferentSearchAnalyzer || hasDifferentSearchQuoteAnalyzer) {
+                    builder.field("search_analyzer", fieldType().searchAnalyzer().name());
+                    if (hasDifferentSearchQuoteAnalyzer) {
+                        builder.field("search_quote_analyzer", fieldType().searchQuoteAnalyzer().name());
+                    }
+                }
             }
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java
index 918731d..05ef8c8 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java
@@ -404,6 +404,7 @@ public class StringFieldMapper extends FieldMapper implements AllFieldMapper.Inc
     @Override
     protected void doXContentBody(XContentBuilder builder, boolean includeDefaults, Params params) throws IOException {
         super.doXContentBody(builder, includeDefaults, params);
+        doXContentAnalyzers(builder, includeDefaults);
 
         if (includeDefaults || fieldType().nullValue() != null) {
             builder.field("null_value", fieldType().nullValue());
@@ -417,16 +418,7 @@ public class StringFieldMapper extends FieldMapper implements AllFieldMapper.Inc
         if (includeDefaults || positionIncrementGap != POSITION_INCREMENT_GAP_USE_ANALYZER) {
             builder.field("position_increment_gap", positionIncrementGap);
         }
-        NamedAnalyzer searchQuoteAnalyzer = fieldType().searchQuoteAnalyzer();
-        if (searchQuoteAnalyzer != null && !searchQuoteAnalyzer.name().equals(fieldType().searchAnalyzer().name())) {
-            builder.field("search_quote_analyzer", searchQuoteAnalyzer.name());
-        } else if (includeDefaults) {
-            if (searchQuoteAnalyzer == null) {
-                builder.field("search_quote_analyzer", "default");
-            } else {
-                builder.field("search_quote_analyzer", searchQuoteAnalyzer.name());
-            }
-        }
+
         if (includeDefaults || ignoreAbove != Defaults.IGNORE_ABOVE) {
             builder.field("ignore_above", ignoreAbove);
         }
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java
index 3589215..c84883f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java
@@ -26,6 +26,7 @@ import org.apache.lucene.search.MatchNoDocsQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.join.JoinUtil;
 import org.apache.lucene.search.join.ScoreMode;
+import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lucene.search.Queries;
@@ -205,13 +206,15 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
 
     @Override
     protected Query doToQuery(QueryShardContext context) throws IOException {
-        String[] previousTypes = QueryShardContext.setTypesWithPrevious(type);
         Query innerQuery;
+        final String[] previousTypes = context.getTypes();
+        context.setTypes(type);
         try {
             innerQuery = query.toQuery(context);
         } finally {
-            QueryShardContext.setTypes(previousTypes);
+            context.setTypes(previousTypes);
         }
+
         if (innerQuery == null) {
             return null;
         }
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java
index 0ad734e..173d6aa 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java
@@ -22,6 +22,7 @@ import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.join.ScoreMode;
+import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lucene.search.Queries;
@@ -119,11 +120,12 @@ public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBu
     @Override
     protected Query doToQuery(QueryShardContext context) throws IOException {
         Query innerQuery;
-        String[] previousTypes = QueryShardContext.setTypesWithPrevious(type);
+        String[] previousTypes = context.getTypes();
+        context.setTypes(type);
         try {
             innerQuery = query.toQuery(context);
         } finally {
-            QueryShardContext.setTypes(previousTypes);
+            context.setTypes(previousTypes);
         }
 
         if (innerQuery == null) {
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java b/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
index a9d6a98..4701cdf 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
@@ -30,6 +30,7 @@ import org.elasticsearch.client.Client;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentFactory;
@@ -74,7 +75,6 @@ import static java.util.Collections.unmodifiableMap;
  */
 public class QueryShardContext {
 
-    private static final ThreadLocal<String[]> typesContext = new ThreadLocal<>();
     private final MapperService mapperService;
     private final ScriptService scriptService;
     private final SimilarityService similarityService;
@@ -82,23 +82,14 @@ public class QueryShardContext {
     private final IndexFieldDataService indexFieldDataService;
     private final IndexSettings indexSettings;
     private final Client client;
+    private String[] types = Strings.EMPTY_ARRAY;
 
-    public static void setTypes(String[] types) {
-        typesContext.set(types);
+    public void setTypes(String... types) {
+        this.types = types;
     }
 
-    public static String[] getTypes() {
-        return typesContext.get();
-    }
-
-    public static String[] setTypesWithPrevious(String... types) {
-        String[] old = typesContext.get();
-        setTypes(types);
-        return old;
-    }
-
-    public static void removeTypes() {
-        typesContext.remove();
+    public String[] getTypes() {
+        return types;
     }
 
     private final Map<String, Query> namedQueries = new HashMap<>();
@@ -126,6 +117,7 @@ public class QueryShardContext {
 
     public QueryShardContext(QueryShardContext source) {
         this(source.indexSettings, source.client, source.bitsetFilterCache, source.indexFieldDataService, source.mapperService, source.similarityService, source.scriptService, source.indicesQueriesRegistry);
+        this.types = source.getTypes();
     }
 
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java b/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java
index 8504ff5..890961d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java
+++ b/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java
@@ -61,7 +61,7 @@ public class NestedInnerQueryParseSupport {
     private ObjectMapper parentObjectMapper;
 
     public NestedInnerQueryParseSupport(XContentParser parser, SearchContext searchContext) {
-        shardContext = searchContext.indexShard().getQueryShardContext();
+        shardContext = searchContext.getQueryShardContext();
         parseContext = shardContext.parseContext();
         shardContext.reset(parser);
 
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
index bbd0095..f2f1add 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -27,7 +27,6 @@ import org.apache.lucene.index.Term;
 import org.apache.lucene.search.QueryCachingPolicy;
 import org.apache.lucene.search.UsageTrackingQueryCachingPolicy;
 import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.util.CloseableThreadLocal;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.ThreadInterruptedException;
 import org.elasticsearch.ElasticsearchException;
@@ -158,7 +157,6 @@ public class IndexShard extends AbstractIndexShardComponent {
     private final IndicesQueryCache indicesQueryCache;
     private final IndexEventListener indexEventListener;
     private final IndexSettings idxSettings;
-    private final NodeServicesProvider provider;
 
     /** How many bytes we are currently moving to disk, via either IndexWriter.flush or refresh.  IndexingMemoryController polls this
      *  across all shards to decide if throttling is necessary because moving bytes to disk is falling behind vs incoming documents
@@ -253,9 +251,9 @@ public class IndexShard extends AbstractIndexShardComponent {
 
         this.engineConfig = newEngineConfig(translogConfig, cachingPolicy);
         this.suspendableRefContainer = new SuspendableRefContainer();
-        this.provider = provider;
         this.searcherWrapper = indexSearcherWrapper;
-        this.percolatorQueriesRegistry = new PercolatorQueriesRegistry(shardId, indexSettings, newQueryShardContext());
+        QueryShardContext queryShardContext = new QueryShardContext(idxSettings, provider.getClient(), indexCache.bitsetFilterCache(), indexFieldDataService, mapperService, similarityService, provider.getScriptService(), provider.getIndicesQueriesRegistry());
+        this.percolatorQueriesRegistry = new PercolatorQueriesRegistry(shardId, indexSettings, queryShardContext);
     }
 
     public Store store() {
@@ -814,7 +812,7 @@ public class IndexShard extends AbstractIndexShardComponent {
                         engine.flushAndClose();
                     }
                 } finally { // playing safe here and close the engine even if the above succeeds - close can be called multiple times
-                    IOUtils.close(engine, percolatorQueriesRegistry, queryShardContextCache);
+                    IOUtils.close(engine, percolatorQueriesRegistry);
                 }
             }
         }
@@ -1514,25 +1512,6 @@ public class IndexShard extends AbstractIndexShardComponent {
         }
     }
 
-    private CloseableThreadLocal<QueryShardContext> queryShardContextCache = new CloseableThreadLocal<QueryShardContext>() {
-        // TODO We should get rid of this threadlocal but I think it should be a sep change
-        @Override
-        protected QueryShardContext initialValue() {
-            return newQueryShardContext();
-        }
-    };
-
-    private QueryShardContext newQueryShardContext() {
-        return new QueryShardContext(idxSettings, provider.getClient(), indexCache.bitsetFilterCache(), indexFieldDataService, mapperService, similarityService, provider.getScriptService(), provider.getIndicesQueriesRegistry());
-    }
-
-    /**
-     * Returns a threadlocal QueryShardContext for this shard.
-     */
-    public QueryShardContext getQueryShardContext() {
-        return queryShardContextCache.get();
-    }
-
     EngineFactory getEngineFactory() {
         return engineFactory;
     }
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java
index f24452b..105dd3c 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java
@@ -93,6 +93,14 @@ public class RecoverySource extends AbstractComponent implements IndexEventListe
             logger.debug("delaying recovery of {} as source node {} is unknown", request.shardId(), request.targetNode());
             throw new DelayRecoveryException("source node does not have the node [" + request.targetNode() + "] in its state yet..");
         }
+
+        ShardRouting routingEntry = shard.routingEntry();
+        if (request.recoveryType() == RecoveryState.Type.PRIMARY_RELOCATION &&
+            (routingEntry.relocating() == false || routingEntry.relocatingNodeId().equals(request.targetNode().getId()) == false)) {
+            logger.debug("delaying recovery of {} as source shard is not marked yet as relocating to {}", request.shardId(), request.targetNode());
+            throw new DelayRecoveryException("source shard is not marked yet as relocating to [" + request.targetNode() + "]");
+        }
+
         ShardRouting targetShardRouting = null;
         for (ShardRouting shardRouting : node) {
             if (shardRouting.shardId().equals(request.shardId())) {
diff --git a/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java b/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
index 291dc09..7572ebc 100644
--- a/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
+++ b/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
@@ -18,8 +18,6 @@
  */
 package org.elasticsearch.percolator;
 
-import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
-
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.LeafReaderContext;
@@ -33,7 +31,6 @@ import org.elasticsearch.action.percolate.PercolateShardRequest;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.text.Text;
 import org.elasticsearch.common.util.BigArrays;
@@ -47,6 +44,7 @@ import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.ParsedQuery;
+import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.script.ScriptService;
@@ -54,7 +52,6 @@ import org.elasticsearch.search.SearchHitField;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -75,11 +72,11 @@ import org.elasticsearch.search.query.QuerySearchResult;
 import org.elasticsearch.search.rescore.RescoreSearchContext;
 import org.elasticsearch.search.suggest.SuggestionSearchContext;
 
+import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Set;
 
 /**
  */
@@ -101,14 +98,11 @@ public class PercolateContext extends SearchContext {
     private final long originNanoTime = System.nanoTime();
     private final long startTime;
     private final boolean onlyCount;
-    private String[] types;
-
     private Engine.Searcher docSearcher;
     private Engine.Searcher engineSearcher;
     private ContextIndexSearcher searcher;
 
     private SearchContextHighlight highlight;
-    private SearchLookup searchLookup;
     private ParsedQuery parsedQuery;
     private Query query;
     private Query percolateQuery;
@@ -117,20 +111,19 @@ public class PercolateContext extends SearchContext {
     private QuerySearchResult querySearchResult;
     private Sort sort;
     private final Map<String, FetchSubPhaseContext> subPhaseContexts = new HashMap<>();
+    private final QueryShardContext queryShardContext;
     private final Map<Class<?>, Collector> queryCollectors = new HashMap<>();
-    private final FetchPhase fetchPhase;
+    private SearchLookup searchLookup;
 
     public PercolateContext(PercolateShardRequest request, SearchShardTarget searchShardTarget, IndexShard indexShard,
-            IndexService indexService, PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, ScriptService scriptService,
-            Query aliasFilter, ParseFieldMatcher parseFieldMatcher, FetchPhase fetchPhase) {
+                            IndexService indexService, PageCacheRecycler pageCacheRecycler,
+                            BigArrays bigArrays, ScriptService scriptService, Query aliasFilter, ParseFieldMatcher parseFieldMatcher) {
         super(parseFieldMatcher);
         this.indexShard = indexShard;
         this.indexService = indexService;
-        this.fetchPhase = fetchPhase;
         this.fieldDataService = indexService.fieldData();
         this.mapperService = indexService.mapperService();
         this.searchShardTarget = searchShardTarget;
-        this.types = new String[]{request.documentType()};
         this.pageCacheRecycler = pageCacheRecycler;
         this.bigArrays = bigArrays.withCircuitBreaking();
         this.querySearchResult = new QuerySearchResult(0, searchShardTarget);
@@ -141,10 +134,12 @@ public class PercolateContext extends SearchContext {
         this.aliasFilter = aliasFilter;
         this.startTime = request.getStartTime();
         this.onlyCount = request.onlyCount();
+        queryShardContext = indexService.newQueryShardContext();
+        queryShardContext.setTypes(request.documentType());
     }
 
     // for testing:
-    PercolateContext(PercolateShardRequest request, SearchShardTarget searchShardTarget, MapperService mapperService) {
+    PercolateContext(PercolateShardRequest request, SearchShardTarget searchShardTarget, MapperService mapperService, QueryShardContext queryShardContext) {
         super(null);
         this.searchShardTarget = searchShardTarget;
         this.mapperService = mapperService;
@@ -158,7 +153,7 @@ public class PercolateContext extends SearchContext {
         this.startTime = 0;
         this.numberOfShards = 0;
         this.onlyCount = true;
-        this.fetchPhase = null;
+        this.queryShardContext = queryShardContext;
     }
 
     public IndexSearcher docSearcher() {
@@ -167,10 +162,10 @@ public class PercolateContext extends SearchContext {
 
     public void initialize(Engine.Searcher docSearcher, ParsedDocument parsedDocument) {
         this.docSearcher = docSearcher;
-
         IndexReader indexReader = docSearcher.reader();
         LeafReaderContext atomicReaderContext = indexReader.leaves().get(0);
-        LeafSearchLookup leafLookup = lookup().getLeafSearchLookup(atomicReaderContext);
+        this.searchLookup = new SearchLookup(mapperService(), fieldData(),  queryShardContext.getTypes());
+        LeafSearchLookup leafLookup = searchLookup.getLeafSearchLookup(atomicReaderContext);
         leafLookup.setDocument(0);
         leafLookup.source().setSource(parsedDocument.source());
 
@@ -237,10 +232,10 @@ public class PercolateContext extends SearchContext {
 
     @Override
     public SearchLookup lookup() {
-        if (searchLookup == null) {
-            searchLookup = new SearchLookup(mapperService(), fieldData(), types);
-        }
-        return searchLookup;
+        // we cache this since it's really just a single document lookup - check the init method for details
+        assert searchLookup != null : "context is not initialized";
+        assert Arrays.equals(searchLookup.doc().getTypes(), getQueryShardContext().getTypes()) : "types mismatch - can't return lookup";
+        return this.searchLookup;
     }
 
     @Override
@@ -271,16 +266,6 @@ public class PercolateContext extends SearchContext {
     }
 
     @Override
-    public String[] types() {
-        return types;
-    }
-
-    public void types(String[] types) {
-        this.types = types;
-        searchLookup = new SearchLookup(mapperService(), fieldData(), types);
-    }
-
-    @Override
     public IndexFieldDataService fieldData() {
         return fieldDataService;
     }
@@ -347,11 +332,6 @@ public class PercolateContext extends SearchContext {
     }
 
     @Override
-    public boolean hasTypes() {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
     public float queryBoost() {
         throw new UnsupportedOperationException();
     }
@@ -665,11 +645,6 @@ public class PercolateContext extends SearchContext {
     }
 
     @Override
-    public FetchPhase fetchPhase() {
-        return fetchPhase;
-    }
-
-    @Override
     public MappedFieldType smartNameFieldType(String name) {
         return mapperService().fullName(name);
     }
@@ -695,6 +670,11 @@ public class PercolateContext extends SearchContext {
     }
 
     @Override
+    public QueryShardContext getQueryShardContext() {
+        return queryShardContext;
+    }
+
+    @Override
     public Profilers getProfilers() {
         throw new UnsupportedOperationException();
     }
diff --git a/core/src/main/java/org/elasticsearch/percolator/PercolateDocumentParser.java b/core/src/main/java/org/elasticsearch/percolator/PercolateDocumentParser.java
index 946d30e..50db3ce 100644
--- a/core/src/main/java/org/elasticsearch/percolator/PercolateDocumentParser.java
+++ b/core/src/main/java/org/elasticsearch/percolator/PercolateDocumentParser.java
@@ -23,8 +23,6 @@ import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.action.percolate.PercolateShardRequest;
-import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
-import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
@@ -35,7 +33,6 @@ import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.mapper.DocumentMapperForType;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.ParsedDocument;
-import org.elasticsearch.index.mapper.SourceToParse;
 import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.aggregations.AggregationPhase;
@@ -60,7 +57,7 @@ public class PercolateDocumentParser {
         this.aggregationPhase = aggregationPhase;
     }
 
-    public ParsedDocument parse(PercolateShardRequest request, PercolateContext context, MapperService mapperService, QueryShardContext queryShardContext) {
+    public ParsedDocument parse(final PercolateShardRequest request, final PercolateContext context, final MapperService mapperService) {
         BytesReference source = request.source();
         if (source == null || source.length() == 0) {
             if (request.docSource() != null && request.docSource().length() != 0) {
@@ -73,13 +70,13 @@ public class PercolateDocumentParser {
         // TODO: combine all feature parse elements into one map
         Map<String, ? extends SearchParseElement> hlElements = highlightPhase.parseElements();
         Map<String, ? extends SearchParseElement> aggregationElements = aggregationPhase.parseElements();
-
+        final QueryShardContext queryShardContext = context.getQueryShardContext();
         ParsedDocument doc = null;
         // Some queries (function_score query when for decay functions) rely on a SearchContext being set:
         // We switch types because this context needs to be in the context of the percolate queries in the shard and
         // not the in memory percolate doc
-        String[] previousTypes = context.types();
-        context.types(new String[]{PercolatorService.TYPE_NAME});
+        final String[] previousTypes = queryShardContext.getTypes();
+        queryShardContext.setTypes(PercolatorService.TYPE_NAME);
         try (XContentParser parser = XContentFactory.xContent(source).createParser(source);) {
             String currentFieldName = null;
             XContentParser.Token token;
@@ -176,7 +173,7 @@ public class PercolateDocumentParser {
         } catch (Throwable e) {
             throw new ElasticsearchParseException("failed to parse request", e);
         } finally {
-            context.types(previousTypes);
+            queryShardContext.setTypes(previousTypes);
         }
 
         if (request.docSource() != null && request.docSource().length() != 0) {
diff --git a/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java b/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
index 01755f2..3fb2b34 100644
--- a/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
+++ b/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.percolator;
 
-
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.memory.ExtendedMemoryIndex;
@@ -71,7 +70,6 @@ import org.elasticsearch.search.aggregations.InternalAggregations;
 import org.elasticsearch.search.aggregations.bucket.global.GlobalAggregator;
 import org.elasticsearch.search.aggregations.pipeline.SiblingPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.highlight.HighlightField;
 import org.elasticsearch.search.highlight.HighlightPhase;
 import org.elasticsearch.search.internal.SearchContext;
@@ -100,25 +98,23 @@ public class PercolatorService extends AbstractComponent implements Releasable {
     private final HighlightPhase highlightPhase;
     private final AggregationPhase aggregationPhase;
     private final PageCacheRecycler pageCacheRecycler;
+    private final ParseFieldMatcher parseFieldMatcher;
     private final CloseableThreadLocal<MemoryIndex> cache;
     private final IndexNameExpressionResolver indexNameExpressionResolver;
     private final PercolateDocumentParser percolateDocumentParser;
 
     private final PercolatorIndex single;
     private final PercolatorIndex multi;
-    private final ParseFieldMatcher parseFieldMatcher;
-    private final FetchPhase fetchPhase;
 
     @Inject
     public PercolatorService(Settings settings, IndexNameExpressionResolver indexNameExpressionResolver, IndicesService indicesService,
                              PageCacheRecycler pageCacheRecycler, BigArrays bigArrays,
                              HighlightPhase highlightPhase, ClusterService clusterService,
                              AggregationPhase aggregationPhase, ScriptService scriptService,
-                             PercolateDocumentParser percolateDocumentParser, FetchPhase fetchPhase) {
+                             PercolateDocumentParser percolateDocumentParser) {
         super(settings);
         this.indexNameExpressionResolver = indexNameExpressionResolver;
         this.percolateDocumentParser = percolateDocumentParser;
-        this.fetchPhase = fetchPhase;
         this.parseFieldMatcher = new ParseFieldMatcher(settings);
         this.indicesService = indicesService;
         this.pageCacheRecycler = pageCacheRecycler;
@@ -145,7 +141,7 @@ public class PercolatorService extends AbstractComponent implements Releasable {
             long finalCount = 0;
             for (PercolateShardResponse shardResponse : shardResponses) {
                 finalCount += shardResponse.topDocs().totalHits;
-    }
+            }
 
             InternalAggregations reducedAggregations = reduceAggregations(shardResponses);
             return new PercolatorService.ReduceResult(finalCount, reducedAggregations);
@@ -173,8 +169,8 @@ public class PercolatorService extends AbstractComponent implements Releasable {
     }
 
     public PercolateShardResponse percolate(PercolateShardRequest request) throws IOException {
-        IndexService percolateIndexService = indicesService.indexServiceSafe(request.shardId().getIndex());
-        IndexShard indexShard = percolateIndexService.getShard(request.shardId().id());
+        final IndexService percolateIndexService = indicesService.indexServiceSafe(request.shardId().getIndex());
+        final IndexShard indexShard = percolateIndexService.getShard(request.shardId().id());
         indexShard.readAllowed(); // check if we can read the shard...
         PercolatorQueriesRegistry percolateQueryRegistry = indexShard.percolateRegistry();
         percolateQueryRegistry.prePercolate();
@@ -187,16 +183,15 @@ public class PercolatorService extends AbstractComponent implements Releasable {
                 indexShard.shardId().getIndex().getName(),
                 request.indices()
         );
-        Query aliasFilter = percolateIndexService.aliasFilter(indexShard.getQueryShardContext(), filteringAliases);
+        Query aliasFilter = percolateIndexService.aliasFilter(percolateIndexService.newQueryShardContext(), filteringAliases);
 
-        SearchShardTarget searchShardTarget = new SearchShardTarget(clusterService.localNode().id(), request.shardId().getIndex(),
-                request.shardId().id());
-        final PercolateContext context = new PercolateContext(request, searchShardTarget, indexShard, percolateIndexService,
-                pageCacheRecycler, bigArrays, scriptService, aliasFilter, parseFieldMatcher, fetchPhase);
+        SearchShardTarget searchShardTarget = new SearchShardTarget(clusterService.localNode().id(), request.shardId().getIndex(), request.shardId().id());
+        final PercolateContext context = new PercolateContext(
+                request, searchShardTarget, indexShard, percolateIndexService, pageCacheRecycler, bigArrays, scriptService, aliasFilter, parseFieldMatcher
+        );
         SearchContext.setCurrent(context);
         try {
-            ParsedDocument parsedDocument = percolateDocumentParser.parse(request, context, percolateIndexService.mapperService(), percolateIndexService.getQueryShardContext());
-
+            ParsedDocument parsedDocument = percolateDocumentParser.parse(request, context, percolateIndexService.mapperService());
             if (context.searcher().getIndexReader().maxDoc() == 0) {
                 return new PercolateShardResponse(Lucene.EMPTY_TOP_DOCS, Collections.emptyMap(), Collections.emptyMap(), context);
             }
@@ -220,14 +215,15 @@ public class PercolatorService extends AbstractComponent implements Releasable {
             if (context.aggregations() != null) {
                 AggregationContext aggregationContext = new AggregationContext(context);
                 context.aggregations().aggregationContext(aggregationContext);
-                Aggregator[] aggregators = context.aggregations().factories().createTopLevelAggregators();
+
+                Aggregator[] aggregators = context.aggregations().factories().createTopLevelAggregators(aggregationContext);
                 List<Aggregator> aggregatorCollectors = new ArrayList<>(aggregators.length);
                 for (int i = 0; i < aggregators.length; i++) {
                     if (!(aggregators[i] instanceof GlobalAggregator)) {
                         Aggregator aggregator = aggregators[i];
                         aggregatorCollectors.add(aggregator);
+                    }
                 }
-            }
                 context.aggregations().aggregators(aggregators);
                 aggregatorCollector = BucketCollector.wrap(aggregatorCollectors);
                 aggregatorCollector.preCollection();
@@ -249,14 +245,14 @@ public class PercolatorService extends AbstractComponent implements Releasable {
         }
         if (context.percolateQuery() != null || context.aliasFilter() != null) {
             BooleanQuery.Builder bq = new BooleanQuery.Builder();
-                        if (context.percolateQuery() != null) {
+            if (context.percolateQuery() != null) {
                 bq.add(context.percolateQuery(), MUST);
-                        }
+            }
             if (context.aliasFilter() != null) {
                 bq.add(context.aliasFilter(), FILTER);
-                        }
+            }
             builder.setPercolateQuery(bq.build());
-                    }
+        }
         PercolatorQuery percolatorQuery = builder.build();
 
         if (context.isOnlyCount() || context.size() == 0) {
@@ -265,20 +261,20 @@ public class PercolatorService extends AbstractComponent implements Releasable {
             if (aggregatorCollector != null) {
                 aggregatorCollector.postCollection();
                 aggregationPhase.execute(context);
-                        }
+            }
             return new PercolateShardResponse(new TopDocs(collector.getTotalHits(), Lucene.EMPTY_SCORE_DOCS, 0f), Collections.emptyMap(), Collections.emptyMap(), context);
         } else {
             int size = context.size();
             if (size > context.searcher().getIndexReader().maxDoc()) {
                 // prevent easy OOM if more than the total number of docs that exist is requested...
                 size = context.searcher().getIndexReader().maxDoc();
-        }
+            }
             TopScoreDocCollector collector = TopScoreDocCollector.create(size);
             context.searcher().search(percolatorQuery, MultiCollector.wrap(collector, aggregatorCollector));
             if (aggregatorCollector != null) {
                 aggregatorCollector.postCollection();
                 aggregationPhase.execute(context);
-    }
+            }
 
             TopDocs topDocs = collector.topDocs();
             Map<Integer, String> ids = new HashMap<>(topDocs.scoreDocs.length);
@@ -287,7 +283,7 @@ public class PercolatorService extends AbstractComponent implements Releasable {
                 if (context.trackScores() == false) {
                     // No sort or tracking scores was provided, so use special value to indicate to not show the scores:
                     scoreDoc.score = NO_SCORE;
-        }
+                }
 
                 int segmentIdx = ReaderUtil.subIndex(scoreDoc.doc, context.searcher().getIndexReader().leaves());
                 LeafReaderContext atomicReaderContext = context.searcher().getIndexReader().leaves().get(segmentIdx);
@@ -300,18 +296,18 @@ public class PercolatorService extends AbstractComponent implements Releasable {
                     Query query = queriesRegistry.getPercolateQueries().get(new BytesRef(id));
                     context.parsedQuery(new ParsedQuery(query));
                     context.hitContext().cache().clear();
-                                highlightPhase.hitExecute(context, context.hitContext());
+                    highlightPhase.hitExecute(context, context.hitContext());
                     hls.put(scoreDoc.doc, context.hitContext().hit().getHighlightFields());
-                            }
-                        }
-            return new PercolateShardResponse(topDocs, ids, hls, context);
                 }
             }
+            return new PercolateShardResponse(topDocs, ids, hls, context);
+        }
+    }
 
     @Override
     public void close() {
         cache.close();
-        }
+    }
 
     private InternalAggregations reduceAggregations(List<PercolateShardResponse> shardResults) {
         if (shardResults.get(0).aggregations() == null) {
@@ -332,12 +328,12 @@ public class PercolatorService extends AbstractComponent implements Releasable {
                 for (SiblingPipelineAggregator pipelineAggregator : pipelineAggregators) {
                     InternalAggregation newAgg = pipelineAggregator.doReduce(new InternalAggregations(newAggs), new InternalAggregation.ReduceContext(bigArrays, scriptService));
                     newAggs.add(newAgg);
-            }
+                }
                 aggregations = new InternalAggregations(newAggs);
-        }
+            }
         }
         return aggregations;
-        }
+    }
 
     public final static class ReduceResult {
 
diff --git a/core/src/main/java/org/elasticsearch/script/Script.java b/core/src/main/java/org/elasticsearch/script/Script.java
index dc0e383..fb2226e 100644
--- a/core/src/main/java/org/elasticsearch/script/Script.java
+++ b/core/src/main/java/org/elasticsearch/script/Script.java
@@ -34,24 +34,12 @@ import org.elasticsearch.script.ScriptService.ScriptType;
 
 import java.io.IOException;
 import java.util.Map;
-import java.util.function.Supplier;
 
 /**
  * Script holds all the parameters necessary to compile or find in cache and then execute a script.
  */
 public class Script implements ToXContent, Streamable {
 
-    /**
-     * A {@link Supplier} implementation for use when reading a {@link Script}
-     * using {@link StreamInput#readOptionalStreamable(Supplier)}
-     */
-    public static final Supplier<Script> SUPPLIER = new Supplier<Script>() {
-
-        @Override
-        public Script get() {
-            return new Script();
-        }
-    };
     public static final ScriptType DEFAULT_TYPE = ScriptType.INLINE;
     private static final ScriptParser PARSER = new ScriptParser();
 
@@ -86,7 +74,7 @@ public class Script implements ToXContent, Streamable {
 
     /**
      * Constructor for Script.
-     *
+     * 
      * @param script
      *            The cache key of the script to be compiled/executed. For
      *            inline scripts this is the actual script source code. For
@@ -124,7 +112,7 @@ public class Script implements ToXContent, Streamable {
 
     /**
      * Method for getting the type.
-     *
+     * 
      * @return The type of script -- inline, indexed, or file.
      */
     public ScriptType getType() {
@@ -133,7 +121,7 @@ public class Script implements ToXContent, Streamable {
 
     /**
      * Method for getting language.
-     *
+     * 
      * @return The language of the script to be compiled/executed.
      */
     public String getLang() {
@@ -142,7 +130,7 @@ public class Script implements ToXContent, Streamable {
 
     /**
      * Method for getting the parameters.
-     *
+     * 
      * @return The map of parameters the script will be executed with.
      */
     public Map<String, Object> getParams() {
diff --git a/core/src/main/java/org/elasticsearch/search/SearchModule.java b/core/src/main/java/org/elasticsearch/search/SearchModule.java
index 6d663a3..739b970 100644
--- a/core/src/main/java/org/elasticsearch/search/SearchModule.java
+++ b/core/src/main/java/org/elasticsearch/search/SearchModule.java
@@ -96,7 +96,6 @@ import org.elasticsearch.index.query.functionscore.script.ScriptScoreFunctionPar
 import org.elasticsearch.index.query.functionscore.weight.WeightBuilder;
 import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.action.SearchServiceTransportAction;
-import org.elasticsearch.search.aggregations.AggregationBinaryParseElement;
 import org.elasticsearch.search.aggregations.AggregationParseElement;
 import org.elasticsearch.search.aggregations.AggregationPhase;
 import org.elasticsearch.search.aggregations.Aggregator;
@@ -128,7 +127,6 @@ import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanc
 import org.elasticsearch.search.aggregations.bucket.range.geodistance.InternalGeoDistance;
 import org.elasticsearch.search.aggregations.bucket.range.ipv4.InternalIPv4Range;
 import org.elasticsearch.search.aggregations.bucket.range.ipv4.IpRangeParser;
-import org.elasticsearch.search.aggregations.bucket.sampler.DiversifiedSamplerParser;
 import org.elasticsearch.search.aggregations.bucket.sampler.InternalSampler;
 import org.elasticsearch.search.aggregations.bucket.sampler.SamplerParser;
 import org.elasticsearch.search.aggregations.bucket.sampler.UnmappedSampler;
@@ -209,26 +207,20 @@ import org.elasticsearch.search.controller.SearchPhaseController;
 import org.elasticsearch.search.dfs.DfsPhase;
 import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSubPhase;
-import org.elasticsearch.search.fetch.FieldsParseElement;
 import org.elasticsearch.search.fetch.explain.ExplainFetchSubPhase;
 import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsFetchSubPhase;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsFetchSubPhase;
 import org.elasticsearch.search.fetch.matchedqueries.MatchedQueriesFetchSubPhase;
 import org.elasticsearch.search.fetch.parent.ParentFieldSubFetchPhase;
 import org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase;
-import org.elasticsearch.search.fetch.script.ScriptFieldsParseElement;
-import org.elasticsearch.search.fetch.source.FetchSourceParseElement;
 import org.elasticsearch.search.fetch.source.FetchSourceSubPhase;
 import org.elasticsearch.search.fetch.version.VersionFetchSubPhase;
 import org.elasticsearch.search.highlight.HighlightPhase;
 import org.elasticsearch.search.highlight.Highlighter;
-import org.elasticsearch.search.highlight.HighlighterParseElement;
 import org.elasticsearch.search.highlight.Highlighters;
 import org.elasticsearch.search.query.QueryPhase;
 import org.elasticsearch.search.rescore.QueryRescorerBuilder;
 import org.elasticsearch.search.rescore.RescoreBuilder;
-import org.elasticsearch.search.sort.SortParseElement;
 import org.elasticsearch.search.suggest.Suggester;
 import org.elasticsearch.search.suggest.Suggesters;
 
@@ -245,8 +237,8 @@ import java.util.function.Supplier;
  */
 public class SearchModule extends AbstractModule {
 
-    private final Set<Aggregator.Parser> aggParsers = new HashSet<>();
-    private final Set<PipelineAggregator.Parser> pipelineAggParsers = new HashSet<>();
+    private final Set<Class<? extends Aggregator.Parser>> aggParsers = new HashSet<>();
+    private final Set<Class<? extends PipelineAggregator.Parser>> pipelineAggParsers = new HashSet<>();
     private final Highlighters highlighters = new Highlighters();
     private final Suggesters suggesters = new Suggesters();
     /**
@@ -261,8 +253,8 @@ public class SearchModule extends AbstractModule {
      */
     private final List<Supplier<QueryParser<?>>> queryParsers = new ArrayList<>();
     private final Set<Class<? extends FetchSubPhase>> fetchSubPhases = new HashSet<>();
-    private final Set<SignificanceHeuristicParser> heuristicParsers = new HashSet<>();
-    private final Set<MovAvgModel.AbstractModelParser> modelParsers = new HashSet<>();
+    private final Set<Class<? extends SignificanceHeuristicParser>> heuristicParsers = new HashSet<>();
+    private final Set<Class<? extends MovAvgModel.AbstractModelParser>> modelParsers = new HashSet<>();
 
     private final Settings settings;
     private final NamedWriteableRegistry namedWriteableRegistry;
@@ -307,11 +299,11 @@ public class SearchModule extends AbstractModule {
         fetchSubPhases.add(subPhase);
     }
 
-    public void registerHeuristicParser(SignificanceHeuristicParser parser) {
+    public void registerHeuristicParser(Class<? extends SignificanceHeuristicParser> parser) {
         heuristicParsers.add(parser);
     }
 
-    public void registerModelParser(MovAvgModel.AbstractModelParser parser) {
+    public void registerModelParser(Class<? extends MovAvgModel.AbstractModelParser> parser) {
         modelParsers.add(parser);
     }
 
@@ -320,22 +312,21 @@ public class SearchModule extends AbstractModule {
      *
      * @param parser The parser for the custom aggregator.
      */
-    public void registerAggregatorParser(Aggregator.Parser parser) {
+    public void registerAggregatorParser(Class<? extends Aggregator.Parser> parser) {
         aggParsers.add(parser);
     }
 
-    public void registerPipelineParser(PipelineAggregator.Parser parser) {
+    public void registerPipelineParser(Class<? extends PipelineAggregator.Parser> parser) {
         pipelineAggParsers.add(parser);
     }
 
     @Override
     protected void configure() {
-        IndicesQueriesRegistry indicesQueriesRegistry = buildQueryParserRegistry();
-        bind(IndicesQueriesRegistry.class).toInstance(indicesQueriesRegistry);
         configureSearch();
-        configureAggs(indicesQueriesRegistry);
+        configureAggs();
         configureHighlighters();
         configureSuggesters();
+        bind(IndicesQueriesRegistry.class).toInstance(buildQueryParserRegistry());
         configureFetchSubPhase();
         configureShapes();
         configureRescorers();
@@ -380,67 +371,75 @@ public class SearchModule extends AbstractModule {
        highlighters.bind(binder());
     }
 
-    protected void configureAggs(IndicesQueriesRegistry indicesQueriesRegistry) {
-
-        MovAvgModelParserMapper movAvgModelParserMapper = new MovAvgModelParserMapper(modelParsers);
-
-        SignificanceHeuristicParserMapper significanceHeuristicParserMapper = new SignificanceHeuristicParserMapper(heuristicParsers);
-
-        registerAggregatorParser(new AvgParser());
-        registerAggregatorParser(new SumParser());
-        registerAggregatorParser(new MinParser());
-        registerAggregatorParser(new MaxParser());
-        registerAggregatorParser(new StatsParser());
-        registerAggregatorParser(new ExtendedStatsParser());
-        registerAggregatorParser(new ValueCountParser());
-        registerAggregatorParser(new PercentilesParser());
-        registerAggregatorParser(new PercentileRanksParser());
-        registerAggregatorParser(new CardinalityParser());
-        registerAggregatorParser(new GlobalParser());
-        registerAggregatorParser(new MissingParser());
-        registerAggregatorParser(new FilterParser());
-        registerAggregatorParser(new FiltersParser(indicesQueriesRegistry));
-        registerAggregatorParser(new SamplerParser());
-        registerAggregatorParser(new DiversifiedSamplerParser());
-        registerAggregatorParser(new TermsParser());
-        registerAggregatorParser(new SignificantTermsParser(significanceHeuristicParserMapper, indicesQueriesRegistry));
-        registerAggregatorParser(new RangeParser());
-        registerAggregatorParser(new DateRangeParser());
-        registerAggregatorParser(new IpRangeParser());
-        registerAggregatorParser(new HistogramParser());
-        registerAggregatorParser(new DateHistogramParser());
-        registerAggregatorParser(new GeoDistanceParser());
-        registerAggregatorParser(new GeoHashGridParser());
-        registerAggregatorParser(new NestedParser());
-        registerAggregatorParser(new ReverseNestedParser());
-        registerAggregatorParser(new TopHitsParser(new SortParseElement(), new FetchSourceParseElement(), new HighlighterParseElement(),
-                new FieldDataFieldsParseElement(), new ScriptFieldsParseElement(), new FieldsParseElement()));
-        registerAggregatorParser(new GeoBoundsParser());
-        registerAggregatorParser(new GeoCentroidParser());
-        registerAggregatorParser(new ScriptedMetricParser());
-        registerAggregatorParser(new ChildrenParser());
-
-        registerPipelineParser(new DerivativeParser());
-        registerPipelineParser(new MaxBucketParser());
-        registerPipelineParser(new MinBucketParser());
-        registerPipelineParser(new AvgBucketParser());
-        registerPipelineParser(new SumBucketParser());
-        registerPipelineParser(new StatsBucketParser());
-        registerPipelineParser(new ExtendedStatsBucketParser());
-        registerPipelineParser(new PercentilesBucketParser());
-        registerPipelineParser(new MovAvgParser(movAvgModelParserMapper));
-        registerPipelineParser(new CumulativeSumParser());
-        registerPipelineParser(new BucketScriptParser());
-        registerPipelineParser(new BucketSelectorParser());
-        registerPipelineParser(new SerialDiffParser());
-
-        AggregatorParsers aggregatorParsers = new AggregatorParsers(aggParsers, pipelineAggParsers, namedWriteableRegistry);
-        AggregationParseElement aggParseElement = new AggregationParseElement(aggregatorParsers, indicesQueriesRegistry);
-        AggregationBinaryParseElement aggBinaryParseElement = new AggregationBinaryParseElement(aggregatorParsers, indicesQueriesRegistry);
-        AggregationPhase aggPhase = new AggregationPhase(aggParseElement, aggBinaryParseElement);
-        bind(AggregatorParsers.class).toInstance(aggregatorParsers);
-        bind(AggregationParseElement.class).toInstance(aggParseElement);
-        bind(AggregationPhase.class).toInstance(aggPhase);
+    protected void configureAggs() {
+        Multibinder<Aggregator.Parser> multibinderAggParser = Multibinder.newSetBinder(binder(), Aggregator.Parser.class);
+        multibinderAggParser.addBinding().to(AvgParser.class);
+        multibinderAggParser.addBinding().to(SumParser.class);
+        multibinderAggParser.addBinding().to(MinParser.class);
+        multibinderAggParser.addBinding().to(MaxParser.class);
+        multibinderAggParser.addBinding().to(StatsParser.class);
+        multibinderAggParser.addBinding().to(ExtendedStatsParser.class);
+        multibinderAggParser.addBinding().to(ValueCountParser.class);
+        multibinderAggParser.addBinding().to(PercentilesParser.class);
+        multibinderAggParser.addBinding().to(PercentileRanksParser.class);
+        multibinderAggParser.addBinding().to(CardinalityParser.class);
+        multibinderAggParser.addBinding().to(GlobalParser.class);
+        multibinderAggParser.addBinding().to(MissingParser.class);
+        multibinderAggParser.addBinding().to(FilterParser.class);
+        multibinderAggParser.addBinding().to(FiltersParser.class);
+        multibinderAggParser.addBinding().to(SamplerParser.class);
+        multibinderAggParser.addBinding().to(TermsParser.class);
+        multibinderAggParser.addBinding().to(SignificantTermsParser.class);
+        multibinderAggParser.addBinding().to(RangeParser.class);
+        multibinderAggParser.addBinding().to(DateRangeParser.class);
+        multibinderAggParser.addBinding().to(IpRangeParser.class);
+        multibinderAggParser.addBinding().to(HistogramParser.class);
+        multibinderAggParser.addBinding().to(DateHistogramParser.class);
+        multibinderAggParser.addBinding().to(GeoDistanceParser.class);
+        multibinderAggParser.addBinding().to(GeoHashGridParser.class);
+        multibinderAggParser.addBinding().to(NestedParser.class);
+        multibinderAggParser.addBinding().to(ReverseNestedParser.class);
+        multibinderAggParser.addBinding().to(TopHitsParser.class);
+        multibinderAggParser.addBinding().to(GeoBoundsParser.class);
+        multibinderAggParser.addBinding().to(GeoCentroidParser.class);
+        multibinderAggParser.addBinding().to(ScriptedMetricParser.class);
+        multibinderAggParser.addBinding().to(ChildrenParser.class);
+        for (Class<? extends Aggregator.Parser> parser : aggParsers) {
+            multibinderAggParser.addBinding().to(parser);
+        }
+
+        Multibinder<PipelineAggregator.Parser> multibinderPipelineAggParser = Multibinder.newSetBinder(binder(), PipelineAggregator.Parser.class);
+        multibinderPipelineAggParser.addBinding().to(DerivativeParser.class);
+        multibinderPipelineAggParser.addBinding().to(MaxBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(MinBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(AvgBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(SumBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(StatsBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(ExtendedStatsBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(PercentilesBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(MovAvgParser.class);
+        multibinderPipelineAggParser.addBinding().to(CumulativeSumParser.class);
+        multibinderPipelineAggParser.addBinding().to(BucketScriptParser.class);
+        multibinderPipelineAggParser.addBinding().to(BucketSelectorParser.class);
+        multibinderPipelineAggParser.addBinding().to(SerialDiffParser.class);
+        for (Class<? extends PipelineAggregator.Parser> parser : pipelineAggParsers) {
+            multibinderPipelineAggParser.addBinding().to(parser);
+        }
+        bind(AggregatorParsers.class).asEagerSingleton();
+        bind(AggregationParseElement.class).asEagerSingleton();
+        bind(AggregationPhase.class).asEagerSingleton();
+
+        Multibinder<SignificanceHeuristicParser> heuristicParserMultibinder = Multibinder.newSetBinder(binder(), SignificanceHeuristicParser.class);
+        for (Class<? extends SignificanceHeuristicParser> clazz : heuristicParsers) {
+            heuristicParserMultibinder.addBinding().to(clazz);
+        }
+        bind(SignificanceHeuristicParserMapper.class);
+
+        Multibinder<MovAvgModel.AbstractModelParser> modelParserMultibinder = Multibinder.newSetBinder(binder(), MovAvgModel.AbstractModelParser.class);
+        for (Class<? extends MovAvgModel.AbstractModelParser> clazz : modelParsers) {
+            modelParserMultibinder.addBinding().to(clazz);
+        }
+        bind(MovAvgModelParserMapper.class);
     }
 
     protected void configureSearch() {
diff --git a/core/src/main/java/org/elasticsearch/search/SearchService.java b/core/src/main/java/org/elasticsearch/search/SearchService.java
index 8fe07e7..e0b30a2 100644
--- a/core/src/main/java/org/elasticsearch/search/SearchService.java
+++ b/core/src/main/java/org/elasticsearch/search/SearchService.java
@@ -534,10 +534,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
 
         Engine.Searcher engineSearcher = searcher == null ? indexShard.acquireSearcher("search") : searcher;
 
-        DefaultSearchContext context = new DefaultSearchContext(idGenerator.incrementAndGet(), request, shardTarget, engineSearcher,
-                indexService,
-                indexShard, scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher,
-                defaultSearchTimeout, fetchPhase);
+        DefaultSearchContext context = new DefaultSearchContext(idGenerator.incrementAndGet(), request, shardTarget, engineSearcher, indexService, indexShard, scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher, defaultSearchTimeout);
         SearchContext.setCurrent(context);
 
         try {
@@ -649,8 +646,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
         if (source == null) {
             return;
         }
-        final IndexShard indexShard = context.indexShard();
-        QueryShardContext queryShardContext = indexShard.getQueryShardContext();
+        QueryShardContext queryShardContext = context.getQueryShardContext();
         context.from(source.from());
         context.size(source.size());
         ObjectFloatHashMap<String> indexBoostMap = source.indexBoost();
@@ -731,7 +727,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
                     // ignore
                 }
                 XContentLocation location = completeAggregationsParser != null ? completeAggregationsParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse aggregation source [" + sSource + "]", location, e);
+                throw new SearchParseException(context, "failed to parse rescore source [" + sSource + "]", location, e);
             }
         }
         if (source.suggest() != null) {
@@ -754,7 +750,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
         if (source.rescores() != null) {
             try {
                 for (RescoreBuilder<?> rescore : source.rescores()) {
-                    context.addRescore(rescore.build(context.indexShard().getQueryShardContext()));
+                    context.addRescore(rescore.build(context.getQueryShardContext()));
                 }
             } catch (IOException e) {
                 throw new SearchContextException(context, "failed to create RescoreSearchContext", e);
@@ -779,7 +775,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
         if (source.highlighter() != null) {
             HighlightBuilder highlightBuilder = source.highlighter();
             try {
-                context.highlight(highlightBuilder.build(context.indexShard().getQueryShardContext()));
+                context.highlight(highlightBuilder.build(context.getQueryShardContext()));
             } catch (IOException e) {
                 throw new SearchContextException(context, "failed to create SearchContextHighlighter", e);
             }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AbstractAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/AbstractAggregationBuilder.java
new file mode 100644
index 0000000..31ee3fd
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AbstractAggregationBuilder.java
@@ -0,0 +1,45 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations;
+
+import org.elasticsearch.common.xcontent.ToXContent;
+
+/**
+ * Base structure for aggregation builders.
+ */
+public abstract class AbstractAggregationBuilder implements ToXContent {
+
+    private final String name;
+    protected final String type;
+
+    /**
+     * Sole constructor, typically used by sub-classes.
+     */
+    protected AbstractAggregationBuilder(String name, String type) {
+        this.name = name;
+        this.type = type;
+    }
+
+    /**
+     * Return the name of the aggregation that is being built.
+     */
+    public String getName() {
+        return name;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBinaryParseElement.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBinaryParseElement.java
index d1fd5b1..f2a9f08 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBinaryParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBinaryParseElement.java
@@ -21,7 +21,6 @@ package org.elasticsearch.search.aggregations;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.internal.SearchContext;
 
 /**
@@ -30,8 +29,8 @@ import org.elasticsearch.search.internal.SearchContext;
 public class AggregationBinaryParseElement extends AggregationParseElement {
 
     @Inject
-    public AggregationBinaryParseElement(AggregatorParsers aggregatorParsers, IndicesQueriesRegistry queriesRegistry) {
-        super(aggregatorParsers, queriesRegistry);
+    public AggregationBinaryParseElement(AggregatorParsers aggregatorParsers) {
+        super(aggregatorParsers);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilder.java
new file mode 100644
index 0000000..ed1ba6c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilder.java
@@ -0,0 +1,147 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations;
+
+import org.elasticsearch.ElasticsearchGenerationException;
+import org.elasticsearch.client.Requests;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * A base class for all bucket aggregation builders.
+ */
+public abstract class AggregationBuilder<B extends AggregationBuilder<B>> extends AbstractAggregationBuilder {
+
+    private List<AbstractAggregationBuilder> aggregations;
+    private BytesReference aggregationsBinary;
+    private Map<String, Object> metaData;
+
+    /**
+     * Sole constructor, typically used by sub-classes.
+     */
+    protected AggregationBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    /**
+     * Add a sub get to this bucket get.
+     */
+    @SuppressWarnings("unchecked")
+    public B subAggregation(AbstractAggregationBuilder aggregation) {
+        if (aggregations == null) {
+            aggregations = new ArrayList<>();
+        }
+        aggregations.add(aggregation);
+        return (B) this;
+    }
+
+    /**
+     * Sets a raw (xcontent / json) sub addAggregation.
+     */
+    public B subAggregation(byte[] aggregationsBinary) {
+        return subAggregation(aggregationsBinary, 0, aggregationsBinary.length);
+    }
+
+    /**
+     * Sets a raw (xcontent / json) sub addAggregation.
+     */
+    public B subAggregation(byte[] aggregationsBinary, int aggregationsBinaryOffset, int aggregationsBinaryLength) {
+        return subAggregation(new BytesArray(aggregationsBinary, aggregationsBinaryOffset, aggregationsBinaryLength));
+    }
+
+    /**
+     * Sets a raw (xcontent / json) sub addAggregation.
+     */
+    @SuppressWarnings("unchecked")
+    public B subAggregation(BytesReference aggregationsBinary) {
+        this.aggregationsBinary = aggregationsBinary;
+        return (B) this;
+    }
+
+    /**
+     * Sets a raw (xcontent / json) sub addAggregation.
+     */
+    public B subAggregation(XContentBuilder aggs) {
+        return subAggregation(aggs.bytes());
+    }
+
+    /**
+     * Sets a raw (xcontent / json) sub addAggregation.
+     */
+    public B subAggregation(Map<String, Object> aggs) {
+        try {
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(aggs);
+            return subAggregation(builder);
+        } catch (IOException e) {
+            throw new ElasticsearchGenerationException("Failed to generate [" + aggs + "]", e);
+        }
+    }
+
+    /**
+     * Sets the meta data to be included in the aggregation response
+     */
+    public B setMetaData(Map<String, Object> metaData) {
+        this.metaData = metaData;
+        return (B)this;
+    }
+
+    @Override
+    public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(getName());
+
+        if (this.metaData != null) {
+            builder.field("meta", this.metaData);
+        }
+        builder.field(type);
+        internalXContent(builder, params);
+
+        if (aggregations != null || aggregationsBinary != null) {
+
+            if (aggregations != null) {
+                builder.startObject("aggregations");
+                for (AbstractAggregationBuilder subAgg : aggregations) {
+                    subAgg.toXContent(builder, params);
+                }
+                builder.endObject();
+            }
+
+            if (aggregationsBinary != null) {
+                if (XContentFactory.xContentType(aggregationsBinary) == builder.contentType()) {
+                    builder.rawField("aggregations", aggregationsBinary);
+                } else {
+                    builder.field("aggregations_binary", aggregationsBinary);
+                }
+            }
+
+        }
+
+        return builder.endObject();
+    }
+
+    protected abstract XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException;
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java
index c4f477e..dd9c5a3 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java
@@ -19,68 +19,64 @@
 package org.elasticsearch.search.aggregations;
 
 import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.bucket.children.Children;
-import org.elasticsearch.search.aggregations.bucket.children.ParentToChildrenAggregator.ChildrenAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.children.ChildrenBuilder;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
-import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator.FilterAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.filters.Filters;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.FiltersAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.KeyedFilter;
+import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGrid;
-import org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridParser.GeoGridAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridBuilder;
 import org.elasticsearch.search.aggregations.bucket.global.Global;
-import org.elasticsearch.search.aggregations.bucket.global.GlobalAggregator.GlobalAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.global.GlobalBuilder;
+import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramBuilder;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.DateHistogramAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.HistogramAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramBuilder;
 import org.elasticsearch.search.aggregations.bucket.missing.Missing;
-import org.elasticsearch.search.aggregations.bucket.missing.MissingAggregator.MissingAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.missing.MissingBuilder;
 import org.elasticsearch.search.aggregations.bucket.nested.Nested;
-import org.elasticsearch.search.aggregations.bucket.nested.NestedAggregator.NestedAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.nested.NestedBuilder;
 import org.elasticsearch.search.aggregations.bucket.nested.ReverseNested;
-import org.elasticsearch.search.aggregations.bucket.nested.ReverseNestedAggregator.ReverseNestedAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.nested.ReverseNestedBuilder;
 import org.elasticsearch.search.aggregations.bucket.range.Range;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.RangeAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.range.date.DateRangeAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanceParser.GeoDistanceAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.range.ipv4.IPv4RangeAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.range.RangeBuilder;
+import org.elasticsearch.search.aggregations.bucket.range.date.DateRangeBuilder;
+import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanceBuilder;
+import org.elasticsearch.search.aggregations.bucket.range.ipv4.IPv4RangeBuilder;
 import org.elasticsearch.search.aggregations.bucket.sampler.Sampler;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator.DiversifiedAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator.SamplerAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms;
-import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsBuilder;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
-import org.elasticsearch.search.aggregations.metrics.avg.AvgAggregator.AvgAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.avg.AvgBuilder;
 import org.elasticsearch.search.aggregations.metrics.cardinality.Cardinality;
-import org.elasticsearch.search.aggregations.metrics.cardinality.CardinalityAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.cardinality.CardinalityBuilder;
 import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBounds;
-import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsAggregator.GeoBoundsAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsBuilder;
 import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroid;
-import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator.GeoCentroidAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidBuilder;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
-import org.elasticsearch.search.aggregations.metrics.max.MaxAggregator.MaxAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.max.MaxBuilder;
 import org.elasticsearch.search.aggregations.metrics.min.Min;
-import org.elasticsearch.search.aggregations.metrics.min.MinAggregator.MinAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.min.MinBuilder;
 import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanks;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksBuilder;
 import org.elasticsearch.search.aggregations.metrics.percentiles.Percentiles;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesBuilder;
 import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetric;
-import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.ScriptedMetricAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricBuilder;
 import org.elasticsearch.search.aggregations.metrics.stats.Stats;
-import org.elasticsearch.search.aggregations.metrics.stats.StatsAggregator.StatsAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.stats.StatsBuilder;
 import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStats;
-import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStatsBuilder;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
-import org.elasticsearch.search.aggregations.metrics.sum.SumAggregator.SumAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.sum.SumBuilder;
 import org.elasticsearch.search.aggregations.metrics.tophits.TopHits;
-import org.elasticsearch.search.aggregations.metrics.tophits.TopHitsAggregator.TopHitsAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.tophits.TopHitsBuilder;
 import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCount;
-import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCountAggregator.ValueCountAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCountBuilder;
 
 /**
  * Utility class to create aggregations.
@@ -93,234 +89,217 @@ public class AggregationBuilders {
     /**
      * Create a new {@link ValueCount} aggregation with the given name.
      */
-    public static ValueCountAggregatorBuilder count(String name) {
-        return new ValueCountAggregatorBuilder(name, null);
+    public static ValueCountBuilder count(String name) {
+        return new ValueCountBuilder(name);
     }
 
     /**
      * Create a new {@link Avg} aggregation with the given name.
      */
-    public static AvgAggregatorBuilder avg(String name) {
-        return new AvgAggregatorBuilder(name);
+    public static AvgBuilder avg(String name) {
+        return new AvgBuilder(name);
     }
 
     /**
      * Create a new {@link Max} aggregation with the given name.
      */
-    public static MaxAggregatorBuilder max(String name) {
-        return new MaxAggregatorBuilder(name);
+    public static MaxBuilder max(String name) {
+        return new MaxBuilder(name);
     }
 
     /**
      * Create a new {@link Min} aggregation with the given name.
      */
-    public static MinAggregatorBuilder min(String name) {
-        return new MinAggregatorBuilder(name);
+    public static MinBuilder min(String name) {
+        return new MinBuilder(name);
     }
 
     /**
      * Create a new {@link Sum} aggregation with the given name.
      */
-    public static SumAggregatorBuilder sum(String name) {
-        return new SumAggregatorBuilder(name);
+    public static SumBuilder sum(String name) {
+        return new SumBuilder(name);
     }
 
     /**
      * Create a new {@link Stats} aggregation with the given name.
      */
-    public static StatsAggregatorBuilder stats(String name) {
-        return new StatsAggregatorBuilder(name);
+    public static StatsBuilder stats(String name) {
+        return new StatsBuilder(name);
     }
 
     /**
      * Create a new {@link ExtendedStats} aggregation with the given name.
      */
-    public static ExtendedStatsAggregatorBuilder extendedStats(String name) {
-        return new ExtendedStatsAggregatorBuilder(name);
+    public static ExtendedStatsBuilder extendedStats(String name) {
+        return new ExtendedStatsBuilder(name);
     }
 
     /**
      * Create a new {@link Filter} aggregation with the given name.
      */
-    public static FilterAggregatorBuilder filter(String name, QueryBuilder<?> filter) {
-        return new FilterAggregatorBuilder(name, filter);
+    public static FilterAggregationBuilder filter(String name) {
+        return new FilterAggregationBuilder(name);
     }
 
     /**
      * Create a new {@link Filters} aggregation with the given name.
      */
-    public static FiltersAggregatorBuilder filters(String name, KeyedFilter... filters) {
-        return new FiltersAggregatorBuilder(name, filters);
-    }
-
-    /**
-     * Create a new {@link Filters} aggregation with the given name.
-     */
-    public static FiltersAggregatorBuilder filters(String name, QueryBuilder<?>... filters) {
-        return new FiltersAggregatorBuilder(name, filters);
-    }
-
-    /**
-     * Create a new {@link Sampler} aggregation with the given name.
-     */
-    public static SamplerAggregatorBuilder sampler(String name) {
-        return new SamplerAggregatorBuilder(name);
+    public static FiltersAggregationBuilder filters(String name) {
+        return new FiltersAggregationBuilder(name);
     }
 
     /**
      * Create a new {@link Sampler} aggregation with the given name.
      */
-    public static DiversifiedAggregatorBuilder diversifiedSampler(String name) {
-        return new DiversifiedAggregatorBuilder(name);
+    public static SamplerAggregationBuilder sampler(String name) {
+        return new SamplerAggregationBuilder(name);
     }
 
     /**
      * Create a new {@link Global} aggregation with the given name.
      */
-    public static GlobalAggregatorBuilder global(String name) {
-        return new GlobalAggregatorBuilder(name);
+    public static GlobalBuilder global(String name) {
+        return new GlobalBuilder(name);
     }
 
     /**
      * Create a new {@link Missing} aggregation with the given name.
      */
-    public static MissingAggregatorBuilder missing(String name) {
-        return new MissingAggregatorBuilder(name, null);
+    public static MissingBuilder missing(String name) {
+        return new MissingBuilder(name);
     }
 
     /**
      * Create a new {@link Nested} aggregation with the given name.
      */
-    public static NestedAggregatorBuilder nested(String name, String path) {
-        return new NestedAggregatorBuilder(name, path);
+    public static NestedBuilder nested(String name) {
+        return new NestedBuilder(name);
     }
 
     /**
      * Create a new {@link ReverseNested} aggregation with the given name.
      */
-    public static ReverseNestedAggregatorBuilder reverseNested(String name) {
-        return new ReverseNestedAggregatorBuilder(name);
+    public static ReverseNestedBuilder reverseNested(String name) {
+        return new ReverseNestedBuilder(name);
     }
 
     /**
      * Create a new {@link Children} aggregation with the given name.
      */
-    public static ChildrenAggregatorBuilder children(String name, String childType) {
-        return new ChildrenAggregatorBuilder(name, childType);
+    public static ChildrenBuilder children(String name) {
+        return new ChildrenBuilder(name);
     }
 
     /**
      * Create a new {@link GeoDistance} aggregation with the given name.
      */
-    public static GeoDistanceAggregatorBuilder geoDistance(String name, GeoPoint origin) {
-        return new GeoDistanceAggregatorBuilder(name, origin);
+    public static GeoDistanceBuilder geoDistance(String name) {
+        return new GeoDistanceBuilder(name);
     }
 
     /**
      * Create a new {@link Histogram} aggregation with the given name.
      */
-    public static HistogramAggregatorBuilder histogram(String name) {
-        return new HistogramAggregatorBuilder(name);
+    public static HistogramBuilder histogram(String name) {
+        return new HistogramBuilder(name);
     }
 
     /**
      * Create a new {@link GeoHashGrid} aggregation with the given name.
      */
-    public static GeoGridAggregatorBuilder geohashGrid(String name) {
-        return new GeoGridAggregatorBuilder(name);
+    public static GeoHashGridBuilder geohashGrid(String name) {
+        return new GeoHashGridBuilder(name);
     }
 
     /**
      * Create a new {@link SignificantTerms} aggregation with the given name.
      */
-    public static SignificantTermsAggregatorBuilder significantTerms(String name) {
-        return new SignificantTermsAggregatorBuilder(name, null);
+    public static SignificantTermsBuilder significantTerms(String name) {
+        return new SignificantTermsBuilder(name);
     }
 
     /**
-     * Create a new {@link DateHistogramAggregatorBuilder} aggregation with the given
-     * name.
+     * Create a new {@link DateHistogramBuilder} aggregation with the given name.
      */
-    public static DateHistogramAggregatorBuilder dateHistogram(String name) {
-        return new DateHistogramAggregatorBuilder(name);
+    public static DateHistogramBuilder dateHistogram(String name) {
+        return new DateHistogramBuilder(name);
     }
 
     /**
      * Create a new {@link Range} aggregation with the given name.
      */
-    public static RangeAggregatorBuilder range(String name) {
-        return new RangeAggregatorBuilder(name);
+    public static RangeBuilder range(String name) {
+        return new RangeBuilder(name);
     }
 
     /**
-     * Create a new {@link DateRangeAggregatorBuilder} aggregation with the
-     * given name.
+     * Create a new {@link DateRangeBuilder} aggregation with the given name.
      */
-    public static DateRangeAggregatorBuilder dateRange(String name) {
-        return new DateRangeAggregatorBuilder(name);
+    public static DateRangeBuilder dateRange(String name) {
+        return new DateRangeBuilder(name);
     }
 
     /**
-     * Create a new {@link IPv4RangeAggregatorBuilder} aggregation with the
-     * given name.
+     * Create a new {@link IPv4RangeBuilder} aggregation with the given name.
      */
-    public static IPv4RangeAggregatorBuilder ipRange(String name) {
-        return new IPv4RangeAggregatorBuilder(name);
+    public static IPv4RangeBuilder ipRange(String name) {
+        return new IPv4RangeBuilder(name);
     }
 
     /**
      * Create a new {@link Terms} aggregation with the given name.
      */
-    public static TermsAggregatorBuilder terms(String name) {
-        return new TermsAggregatorBuilder(name, null);
+    public static TermsBuilder terms(String name) {
+        return new TermsBuilder(name);
     }
 
     /**
      * Create a new {@link Percentiles} aggregation with the given name.
      */
-    public static PercentilesAggregatorBuilder percentiles(String name) {
-        return new PercentilesAggregatorBuilder(name);
+    public static PercentilesBuilder percentiles(String name) {
+        return new PercentilesBuilder(name);
     }
 
     /**
      * Create a new {@link PercentileRanks} aggregation with the given name.
      */
-    public static PercentileRanksAggregatorBuilder percentileRanks(String name) {
-        return new PercentileRanksAggregatorBuilder(name);
+    public static PercentileRanksBuilder percentileRanks(String name) {
+        return new PercentileRanksBuilder(name);
     }
 
     /**
      * Create a new {@link Cardinality} aggregation with the given name.
      */
-    public static CardinalityAggregatorBuilder cardinality(String name) {
-        return new CardinalityAggregatorBuilder(name, null);
+    public static CardinalityBuilder cardinality(String name) {
+        return new CardinalityBuilder(name);
     }
 
     /**
      * Create a new {@link TopHits} aggregation with the given name.
      */
-    public static TopHitsAggregatorBuilder topHits(String name) {
-        return new TopHitsAggregatorBuilder(name);
+    public static TopHitsBuilder topHits(String name) {
+        return new TopHitsBuilder(name);
     }
 
     /**
      * Create a new {@link GeoBounds} aggregation with the given name.
      */
-    public static GeoBoundsAggregatorBuilder geoBounds(String name) {
-        return new GeoBoundsAggregatorBuilder(name);
+    public static GeoBoundsBuilder geoBounds(String name) {
+        return new GeoBoundsBuilder(name);
     }
 
     /**
      * Create a new {@link GeoCentroid} aggregation with the given name.
      */
-    public static GeoCentroidAggregatorBuilder geoCentroid(String name) {
-        return new GeoCentroidAggregatorBuilder(name);
+    public static GeoCentroidBuilder geoCentroid(String name) {
+        return new GeoCentroidBuilder(name);
     }
 
     /**
      * Create a new {@link ScriptedMetric} aggregation with the given name.
      */
-    public static ScriptedMetricAggregatorBuilder scriptedMetric(String name) {
-        return new ScriptedMetricAggregatorBuilder(name);
+    public static ScriptedMetricBuilder scriptedMetric(String name) {
+        return new ScriptedMetricBuilder(name);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationParseElement.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationParseElement.java
index fee445d..767cbfb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationParseElement.java
@@ -20,10 +20,7 @@ package org.elasticsearch.search.aggregations;
 
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.SearchParseElement;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
 import org.elasticsearch.search.internal.SearchContext;
 
 /**
@@ -52,23 +49,15 @@ import org.elasticsearch.search.internal.SearchContext;
 public class AggregationParseElement implements SearchParseElement {
 
     private final AggregatorParsers aggregatorParsers;
-    private IndicesQueriesRegistry queriesRegistry;
 
     @Inject
-    public AggregationParseElement(AggregatorParsers aggregatorParsers, IndicesQueriesRegistry queriesRegistry) {
+    public AggregationParseElement(AggregatorParsers aggregatorParsers) {
         this.aggregatorParsers = aggregatorParsers;
-        this.queriesRegistry = queriesRegistry;
     }
 
     @Override
     public void parse(XContentParser parser, SearchContext context) throws Exception {
-        QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-        parseContext.reset(parser);
-        parseContext.parseFieldMatcher(context.parseFieldMatcher());
-        AggregatorFactories.Builder builders = aggregatorParsers.parseAggregators(parser, parseContext);
-        AggregationContext aggContext = new AggregationContext(context);
-        AggregatorFactories factories = builders.build(aggContext, null);
-        factories.validate();
+        AggregatorFactories factories = aggregatorParsers.parseAggregators(parser, context);
         context.aggregations(new SearchContextAggregations(factories));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
index d707ff4..050ffa6 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
@@ -77,7 +77,7 @@ public class AggregationPhase implements SearchPhase {
             Aggregator[] aggregators;
             try {
                 AggregatorFactories factories = context.aggregations().factories();
-                aggregators = factories.createTopLevelAggregators();
+                aggregators = factories.createTopLevelAggregators(aggregationContext);
                 for (int i = 0; i < aggregators.length; i++) {
                     if (aggregators[i] instanceof GlobalAggregator == false) {
                         collectors.add(aggregators[i]);
@@ -124,7 +124,7 @@ public class AggregationPhase implements SearchPhase {
         if (!globals.isEmpty()) {
             BucketCollector globalsCollector = BucketCollector.wrap(globals);
             Query query = Queries.newMatchAllQuery();
-            Query searchFilter = context.searchFilter(context.types());
+            Query searchFilter = context.searchFilter(context.getQueryShardContext().getTypes());
 
             if (searchFilter != null) {
                 BooleanQuery filtered = new BooleanQuery.Builder()
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java
index 374ac14..8ee4d1f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java
@@ -22,14 +22,11 @@ package org.elasticsearch.search.aggregations;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -43,7 +40,7 @@ public abstract class Aggregator extends BucketCollector implements Releasable {
     /**
      * Parses the aggregation request and creates the appropriate aggregator factory for it.
      *
-     * @see AggregatorBuilder
+     * @see AggregatorFactory
     */
     public interface Parser {
 
@@ -62,13 +59,8 @@ public abstract class Aggregator extends BucketCollector implements Releasable {
          * @return                  The resolved aggregator factory or {@code null} in case the aggregation should be skipped
          * @throws java.io.IOException      When parsing fails
          */
-        AggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException;
+        AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException;
 
-        /**
-         * @return an empty {@link AggregatorBuilder} instance for this parser
-         *         that can be used for deserialization
-         */
-        AggregatorBuilder<?> getFactoryPrototypes();
     }
 
     /**
@@ -115,7 +107,7 @@ public abstract class Aggregator extends BucketCollector implements Releasable {
     public abstract InternalAggregation buildEmptyAggregation();
 
     /** Aggregation mode for sub aggregations. */
-    public enum SubAggCollectionMode implements Writeable<SubAggCollectionMode> {
+    public enum SubAggCollectionMode {
 
         /**
          * Creates buckets and delegates to child aggregators in a single pass over
@@ -151,19 +143,5 @@ public abstract class Aggregator extends BucketCollector implements Releasable {
             }
             throw new ElasticsearchParseException("no [{}] found for value [{}]", KEY.getPreferredName(), value);
         }
-
-        @Override
-        public SubAggCollectionMode readFrom(StreamInput in) throws IOException {
-            int ordinal = in.readVInt();
-            if (ordinal < 0 || ordinal >= values().length) {
-                throw new IOException("Unknown SubAggCollectionMode ordinal [" + ordinal + "]");
-            }
-            return values()[ordinal];
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVInt(ordinal());
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorBuilder.java
deleted file mode 100644
index 70527a6..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorBuilder.java
+++ /dev/null
@@ -1,182 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations;
-
-
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.NamedWriteable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.Objects;
-
-/**
- * A factory that knows how to create an {@link Aggregator} of a specific type.
- */
-public abstract class AggregatorBuilder<AB extends AggregatorBuilder<AB>> extends ToXContentToBytes
-        implements NamedWriteable<AB>, ToXContent {
-
-    protected String name;
-    protected Type type;
-    protected AggregatorFactories.Builder factoriesBuilder = AggregatorFactories.builder();
-    protected Map<String, Object> metaData;
-
-    /**
-     * Constructs a new aggregator factory.
-     *
-     * @param name  The aggregation name
-     * @param type  The aggregation type
-     */
-    public AggregatorBuilder(String name, Type type) {
-        this.name = name;
-        this.type = type;
-    }
-
-    /**
-     * Add a sub aggregation to this aggregation.
-     */
-    @SuppressWarnings("unchecked")
-    public AB subAggregation(AggregatorBuilder<?> aggregation) {
-        factoriesBuilder.addAggregator(aggregation);
-        return (AB) this;
-    }
-
-    /**
-     * Add a sub aggregation to this aggregation.
-     */
-    @SuppressWarnings("unchecked")
-    public AB subAggregation(PipelineAggregatorFactory aggregation) {
-        factoriesBuilder.addPipelineAggregator(aggregation);
-        return (AB) this;
-    }
-
-    /**
-     * Registers sub-factories with this factory. The sub-factory will be
-     * responsible for the creation of sub-aggregators under the aggregator
-     * created by this factory.
-     *
-     * @param subFactories
-     *            The sub-factories
-     * @return this factory (fluent interface)
-     */
-    @SuppressWarnings("unchecked")
-    public AB subAggregations(AggregatorFactories.Builder subFactories) {
-        this.factoriesBuilder = subFactories;
-        return (AB) this;
-    }
-
-    public AB setMetaData(Map<String, Object> metaData) {
-        this.metaData = metaData;
-        return (AB) this;
-    }
-
-    public String getType() {
-        return type.name();
-    }
-
-    public final AggregatorFactory<?> build(AggregationContext context, AggregatorFactory<?> parent) throws IOException {
-        AggregatorFactory<?> factory = doBuild(context, parent, factoriesBuilder);
-        return factory;
-    }
-
-    protected abstract AggregatorFactory<?> doBuild(AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subfactoriesBuilder) throws IOException;
-
-    @Override
-    public final AB readFrom(StreamInput in) throws IOException {
-        String name = in.readString();
-        AB factory = doReadFrom(name, in);
-        factory.factoriesBuilder = AggregatorFactories.Builder.PROTOTYPE.readFrom(in);
-        factory.metaData = in.readMap();
-        return factory;
-    }
-
-    protected abstract AB doReadFrom(String name, StreamInput in) throws IOException;
-
-    @Override
-    public final void writeTo(StreamOutput out) throws IOException {
-        out.writeString(name);
-        doWriteTo(out);
-        factoriesBuilder.writeTo(out);
-        out.writeMap(metaData);
-    }
-
-    protected abstract void doWriteTo(StreamOutput out) throws IOException;
-
-    @Override
-    public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(name);
-
-        if (this.metaData != null) {
-            builder.field("meta", this.metaData);
-        }
-        builder.field(type.name());
-        internalXContent(builder, params);
-
-        if (factoriesBuilder != null && (factoriesBuilder.count()) > 0) {
-            builder.field("aggregations");
-            factoriesBuilder.toXContent(builder, params);
-
-        }
-
-        return builder.endObject();
-    }
-
-    protected abstract XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException;
-
-    @Override
-    public String getWriteableName() {
-        return type.stream().toUtf8();
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(factoriesBuilder, metaData, name, type, doHashCode());
-    }
-
-    protected abstract int doHashCode();
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null)
-            return false;
-        if (getClass() != obj.getClass())
-            return false;
-        AggregatorBuilder<AB> other = (AggregatorBuilder<AB>) obj;
-        if (!Objects.equals(name, other.name))
-            return false;
-        if (!Objects.equals(type, other.type))
-            return false;
-        if (!Objects.equals(metaData, other.metaData))
-            return false;
-        if (!Objects.equals(factoriesBuilder, other.factoriesBuilder))
-            return false;
-        return doEquals(obj);
-    }
-
-    protected abstract boolean doEquals(Object obj);
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java
index 1f3c24d..6a1cd27 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java
@@ -18,11 +18,6 @@
  */
 package org.elasticsearch.search.aggregations;
 
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
@@ -36,7 +31,6 @@ import java.util.HashSet;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.Set;
 
 /**
@@ -44,20 +38,17 @@ import java.util.Set;
  */
 public class AggregatorFactories {
 
-    public static final AggregatorFactories EMPTY = new AggregatorFactories(null, new AggregatorFactory<?>[0],
-            new ArrayList<PipelineAggregatorFactory>());
+    public static final AggregatorFactories EMPTY = new Empty();
 
-    private AggregatorFactory<?> parent;
-    private AggregatorFactory<?>[] factories;
+    private AggregatorFactory parent;
+    private AggregatorFactory[] factories;
     private List<PipelineAggregatorFactory> pipelineAggregatorFactories;
 
     public static Builder builder() {
         return new Builder();
     }
 
-    private AggregatorFactories(AggregatorFactory<?> parent, AggregatorFactory<?>[] factories,
-            List<PipelineAggregatorFactory> pipelineAggregators) {
-        this.parent = parent;
+    private AggregatorFactories(AggregatorFactory[] factories, List<PipelineAggregatorFactory> pipelineAggregators) {
         this.factories = factories;
         this.pipelineAggregatorFactories = pipelineAggregators;
     }
@@ -75,46 +66,42 @@ public class AggregatorFactories {
      * buckets.
      */
     public Aggregator[] createSubAggregators(Aggregator parent) throws IOException {
-        Aggregator[] aggregators = new Aggregator[countAggregators()];
+        Aggregator[] aggregators = new Aggregator[count()];
         for (int i = 0; i < factories.length; ++i) {
             // TODO: sometimes even sub aggregations always get called with bucket 0, eg. if
             // you have a terms agg under a top-level filter agg. We should have a way to
             // propagate the fact that only bucket 0 will be collected with single-bucket
             // aggs
             final boolean collectsFromSingleBucket = false;
-            aggregators[i] = factories[i].create(parent, collectsFromSingleBucket);
+            aggregators[i] = factories[i].create(parent.context(), parent, collectsFromSingleBucket);
         }
         return aggregators;
     }
 
-    public Aggregator[] createTopLevelAggregators() throws IOException {
+    public Aggregator[] createTopLevelAggregators(AggregationContext ctx) throws IOException {
         // These aggregators are going to be used with a single bucket ordinal, no need to wrap the PER_BUCKET ones
         Aggregator[] aggregators = new Aggregator[factories.length];
         for (int i = 0; i < factories.length; i++) {
             // top-level aggs only get called with bucket 0
             final boolean collectsFromSingleBucket = true;
-            aggregators[i] = factories[i].create(null, collectsFromSingleBucket);
+            aggregators[i] = factories[i].create(ctx, null, collectsFromSingleBucket);
         }
         return aggregators;
     }
 
-    /**
-     * @return the number of sub-aggregator factories not including pipeline
-     *         aggregator factories
-     */
-    public int countAggregators() {
+    public int count() {
         return factories.length;
     }
 
-    /**
-     * @return the number of pipeline aggregator factories
-     */
-    public int countPipelineAggregators() {
-        return pipelineAggregatorFactories.size();
+    void setParent(AggregatorFactory parent) {
+        this.parent = parent;
+        for (AggregatorFactory factory : factories) {
+            factory.parent = parent;
+        }
     }
 
     public void validate() {
-        for (AggregatorFactory<?> factory : factories) {
+        for (AggregatorFactory factory : factories) {
             factory.validate();
         }
         for (PipelineAggregatorFactory factory : pipelineAggregatorFactories) {
@@ -122,24 +109,39 @@ public class AggregatorFactories {
         }
     }
 
-    public static class Builder extends ToXContentToBytes implements Writeable<Builder> {
+    private final static class Empty extends AggregatorFactories {
 
-        public final static Builder PROTOTYPE = new Builder();
+        private static final AggregatorFactory[] EMPTY_FACTORIES = new AggregatorFactory[0];
+        private static final Aggregator[] EMPTY_AGGREGATORS = new Aggregator[0];
+        private static final List<PipelineAggregatorFactory> EMPTY_PIPELINE_AGGREGATORS = new ArrayList<>();
 
-        private final Set<String> names = new HashSet<>();
-        private final List<AggregatorBuilder<?>> aggregatorBuilders = new ArrayList<>();
-        private final List<PipelineAggregatorFactory> pipelineAggregatorFactories = new ArrayList<>();
-        private boolean skipResolveOrder;
+        private Empty() {
+            super(EMPTY_FACTORIES, EMPTY_PIPELINE_AGGREGATORS);
+        }
+
+        @Override
+        public Aggregator[] createSubAggregators(Aggregator parent) {
+            return EMPTY_AGGREGATORS;
+        }
 
-        public Builder addAggregators(AggregatorFactories factories) {
-            throw new UnsupportedOperationException("This needs to be removed");
+        @Override
+        public Aggregator[] createTopLevelAggregators(AggregationContext ctx) {
+            return EMPTY_AGGREGATORS;
         }
 
-        public Builder addAggregator(AggregatorBuilder<?> factory) {
+    }
+
+    public static class Builder {
+
+        private final Set<String> names = new HashSet<>();
+        private final List<AggregatorFactory> factories = new ArrayList<>();
+        private final List<PipelineAggregatorFactory> pipelineAggregatorFactories = new ArrayList<>();
+
+        public Builder addAggregator(AggregatorFactory factory) {
             if (!names.add(factory.name)) {
                 throw new IllegalArgumentException("Two sibling aggregations cannot have the same name: [" + factory.name + "]");
             }
-            aggregatorBuilders.add(factory);
+            factories.add(factory);
             return this;
         }
 
@@ -148,39 +150,21 @@ public class AggregatorFactories {
             return this;
         }
 
-        /**
-         * FOR TESTING ONLY
-         */
-        Builder skipResolveOrder() {
-            this.skipResolveOrder = true;
-            return this;
-        }
-
-        public AggregatorFactories build(AggregationContext context, AggregatorFactory<?> parent) throws IOException {
-            if (aggregatorBuilders.isEmpty() && pipelineAggregatorFactories.isEmpty()) {
+        public AggregatorFactories build() {
+            if (factories.isEmpty() && pipelineAggregatorFactories.isEmpty()) {
                 return EMPTY;
             }
-            List<PipelineAggregatorFactory> orderedpipelineAggregators = null;
-            if (skipResolveOrder) {
-                orderedpipelineAggregators = new ArrayList<>(pipelineAggregatorFactories);
-            } else {
-                orderedpipelineAggregators = resolvePipelineAggregatorOrder(this.pipelineAggregatorFactories, this.aggregatorBuilders);
-            }
-            AggregatorFactory<?>[] aggFactories = new AggregatorFactory<?>[aggregatorBuilders.size()];
-            for (int i = 0; i < aggregatorBuilders.size(); i++) {
-                aggFactories[i] = aggregatorBuilders.get(i).build(context, parent);
-            }
-            return new AggregatorFactories(parent, aggFactories, orderedpipelineAggregators);
+            List<PipelineAggregatorFactory> orderedpipelineAggregators = resolvePipelineAggregatorOrder(this.pipelineAggregatorFactories, this.factories);
+            return new AggregatorFactories(factories.toArray(new AggregatorFactory[factories.size()]), orderedpipelineAggregators);
         }
 
-        private List<PipelineAggregatorFactory> resolvePipelineAggregatorOrder(List<PipelineAggregatorFactory> pipelineAggregatorFactories,
-                List<AggregatorBuilder<?>> aggFactories) {
+        private List<PipelineAggregatorFactory> resolvePipelineAggregatorOrder(List<PipelineAggregatorFactory> pipelineAggregatorFactories, List<AggregatorFactory> aggFactories) {
             Map<String, PipelineAggregatorFactory> pipelineAggregatorFactoriesMap = new HashMap<>();
             for (PipelineAggregatorFactory factory : pipelineAggregatorFactories) {
                 pipelineAggregatorFactoriesMap.put(factory.getName(), factory);
             }
-            Map<String, AggregatorBuilder<?>> aggFactoriesMap = new HashMap<>();
-            for (AggregatorBuilder<?> aggFactory : aggFactories) {
+            Map<String, AggregatorFactory> aggFactoriesMap = new HashMap<>();
+            for (AggregatorFactory aggFactory : aggFactories) {
                 aggFactoriesMap.put(aggFactory.name, aggFactory);
             }
             List<PipelineAggregatorFactory> orderedPipelineAggregatorrs = new LinkedList<>();
@@ -194,7 +178,7 @@ public class AggregatorFactories {
             return orderedPipelineAggregatorrs;
         }
 
-        private void resolvePipelineAggregatorOrder(Map<String, AggregatorBuilder<?>> aggFactoriesMap,
+        private void resolvePipelineAggregatorOrder(Map<String, AggregatorFactory> aggFactoriesMap,
                 Map<String, PipelineAggregatorFactory> pipelineAggregatorFactoriesMap,
                 List<PipelineAggregatorFactory> orderedPipelineAggregators, List<PipelineAggregatorFactory> unmarkedFactories, Set<PipelineAggregatorFactory> temporarilyMarked,
                 PipelineAggregatorFactory factory) {
@@ -209,7 +193,7 @@ public class AggregatorFactories {
                     if (bucketsPath.equals("_count") || bucketsPath.equals("_key")) {
                         continue;
                     } else if (aggFactoriesMap.containsKey(firstAggName)) {
-                        AggregatorBuilder<?> aggFactory = aggFactoriesMap.get(firstAggName);
+                        AggregatorFactory aggFactory = aggFactoriesMap.get(firstAggName);
                         for (int i = 1; i < bucketsPathElements.size(); i++) {
                             PathElement pathElement = bucketsPathElements.get(i);
                             String aggName = pathElement.name;
@@ -218,9 +202,9 @@ public class AggregatorFactories {
                             } else {
                                 // Check the non-pipeline sub-aggregator
                                 // factories
-                                AggregatorBuilder<?>[] subFactories = aggFactory.factoriesBuilder.getAggregatorFactories();
+                                AggregatorFactory[] subFactories = aggFactory.factories.factories;
                                 boolean foundSubFactory = false;
-                                for (AggregatorBuilder<?> subFactory : subFactories) {
+                                for (AggregatorFactory subFactory : subFactories) {
                                     if (aggName.equals(subFactory.name)) {
                                         aggFactory = subFactory;
                                         foundSubFactory = true;
@@ -229,7 +213,7 @@ public class AggregatorFactories {
                                 }
                                 // Check the pipeline sub-aggregator factories
                                 if (!foundSubFactory && (i == bucketsPathElements.size() - 1)) {
-                                    List<PipelineAggregatorFactory> subPipelineFactories = aggFactory.factoriesBuilder.pipelineAggregatorFactories;
+                                    List<PipelineAggregatorFactory> subPipelineFactories = aggFactory.factories.pipelineAggregatorFactories;
                                     for (PipelineAggregatorFactory subFactory : subPipelineFactories) {
                                         if (aggName.equals(subFactory.name())) {
                                             foundSubFactory = true;
@@ -261,80 +245,12 @@ public class AggregatorFactories {
             }
         }
 
-        AggregatorBuilder<?>[] getAggregatorFactories() {
-            return this.aggregatorBuilders.toArray(new AggregatorBuilder<?>[this.aggregatorBuilders.size()]);
+        AggregatorFactory[] getAggregatorFactories() {
+            return this.factories.toArray(new AggregatorFactory[this.factories.size()]);
         }
 
         List<PipelineAggregatorFactory> getPipelineAggregatorFactories() {
             return this.pipelineAggregatorFactories;
         }
-
-        public int count() {
-            return aggregatorBuilders.size() + pipelineAggregatorFactories.size();
-        }
-
-        @Override
-        public Builder readFrom(StreamInput in) throws IOException {
-            Builder builder = new Builder();
-            int factoriesSize = in.readVInt();
-            for (int i = 0; i < factoriesSize; i++) {
-                AggregatorBuilder<?> factory = in.readAggregatorFactory();
-                builder.addAggregator(factory);
-            }
-            int pipelineFactoriesSize = in.readVInt();
-            for (int i = 0; i < pipelineFactoriesSize; i++) {
-                PipelineAggregatorFactory factory = in.readPipelineAggregatorFactory();
-                builder.addPipelineAggregator(factory);
-            }
-            return builder;
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVInt(this.aggregatorBuilders.size());
-            for (AggregatorBuilder<?> factory : aggregatorBuilders) {
-                out.writeAggregatorFactory(factory);
-            }
-            out.writeVInt(this.pipelineAggregatorFactories.size());
-            for (PipelineAggregatorFactory factory : pipelineAggregatorFactories) {
-                out.writePipelineAggregatorFactory(factory);
-            }
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            if (aggregatorBuilders != null) {
-                for (AggregatorBuilder<?> subAgg : aggregatorBuilders) {
-                    subAgg.toXContent(builder, params);
-                }
-            }
-            if (pipelineAggregatorFactories != null) {
-                for (PipelineAggregatorFactory subAgg : pipelineAggregatorFactories) {
-                    subAgg.toXContent(builder, params);
-                }
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(aggregatorBuilders, pipelineAggregatorFactories);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null)
-                return false;
-            if (getClass() != obj.getClass())
-                return false;
-            Builder other = (Builder) obj;
-            if (!Objects.equals(aggregatorBuilders, other.aggregatorBuilders))
-                return false;
-            if (!Objects.equals(pipelineAggregatorFactories, other.pipelineAggregatorFactories))
-                return false;
-            return true;
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java
index 72cf70e..680e3ef 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java
@@ -16,17 +16,13 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-
 package org.elasticsearch.search.aggregations;
 
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.Scorer;
-import org.elasticsearch.action.support.ToXContentToBytes;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.ObjectArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
 import org.elasticsearch.search.internal.SearchContext.Lifetime;
@@ -35,39 +31,39 @@ import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
-// NORELEASE remove ToXContentToBytes here when agg refactoring complete
-public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extends ToXContentToBytes {
-
-    // NORELEASE remove this method when agg refactoring complete
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        throw new UnsupportedOperationException("This should never be called");
-    }
+/**
+ * A factory that knows how to create an {@link Aggregator} of a specific type.
+ */
+public abstract class AggregatorFactory {
 
     protected String name;
-    protected Type type;
-    protected AggregatorFactory<?> parent;
+    protected String type;
+    protected AggregatorFactory parent;
     protected AggregatorFactories factories = AggregatorFactories.EMPTY;
     protected Map<String, Object> metaData;
-    private AggregationContext context;
 
     /**
      * Constructs a new aggregator factory.
      *
-     * @param name
-     *            The aggregation name
-     * @param type
-     *            The aggregation type
-     * @throws IOException
-     *             if an error occurs creating the factory
+     * @param name  The aggregation name
+     * @param type  The aggregation type
      */
-    public AggregatorFactory(String name, Type type, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
+    public AggregatorFactory(String name, String type) {
         this.name = name;
         this.type = type;
-        this.context = context;
-        this.factories = subFactoriesBuilder.build(context, this);
-        this.metaData = metaData;
+    }
+
+    /**
+     * Registers sub-factories with this factory. The sub-factory will be responsible for the creation of sub-aggregators under the
+     * aggregator created by this factory.
+     *
+     * @param subFactories  The sub-factories
+     * @return  this factory (fluent interface)
+     */
+    public AggregatorFactory subFactories(AggregatorFactories subFactories) {
+        this.factories = subFactories;
+        this.factories.setParent(this);
+        return this;
     }
 
     public String name() {
@@ -75,15 +71,18 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extend
     }
 
     /**
-     * Validates the state of this factory (makes sure the factory is properly
-     * configured)
+     * Validates the state of this factory (makes sure the factory is properly configured)
      */
     public final void validate() {
         doValidate();
         factories.validate();
     }
 
-    public void doValidate() {
+    /**
+     * @return  The parent factory if one exists (will always return {@code null} for top level aggregator factories).
+     */
+    public AggregatorFactory parent() {
+        return parent;
     }
 
     protected abstract Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
@@ -92,33 +91,32 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extend
     /**
      * Creates the aggregator
      *
-     * @param parent
-     *            The parent aggregator (if this is a top level factory, the
-     *            parent will be {@code null})
-     * @param collectsFromSingleBucket
-     *            If true then the created aggregator will only be collected
-     *            with <tt>0</tt> as a bucket ordinal. Some factories can take
-     *            advantage of this in order to return more optimized
-     *            implementations.
+     * @param context               The aggregation context
+     * @param parent                The parent aggregator (if this is a top level factory, the parent will be {@code null})
+     * @param collectsFromSingleBucket  If true then the created aggregator will only be collected with <tt>0</tt> as a bucket ordinal.
+     *                              Some factories can take advantage of this in order to return more optimized implementations.
      *
-     * @return The created aggregator
+     * @return                      The created aggregator
      */
-    public final Aggregator create(Aggregator parent, boolean collectsFromSingleBucket) throws IOException {
+    public final Aggregator create(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket) throws IOException {
         return createInternal(context, parent, collectsFromSingleBucket, this.factories.createPipelineAggregators(), this.metaData);
     }
 
-    public String getType() {
-        return type.name();
+    public void doValidate() {
     }
 
+    public void setMetaData(Map<String, Object> metaData) {
+        this.metaData = metaData;
+    }
+
+
+
     /**
-     * Utility method. Given an {@link AggregatorFactory} that creates
-     * {@link Aggregator}s that only know how to collect bucket <tt>0</tt>, this
-     * returns an aggregator that can collect any bucket.
+     * Utility method. Given an {@link AggregatorFactory} that creates {@link Aggregator}s that only know how
+     * to collect bucket <tt>0</tt>, this returns an aggregator that can collect any bucket.
      */
-    protected static Aggregator asMultiBucketAggregator(final AggregatorFactory<?> factory, final AggregationContext context,
-            final Aggregator parent) throws IOException {
-        final Aggregator first = factory.create(parent, true);
+    protected static Aggregator asMultiBucketAggregator(final AggregatorFactory factory, final AggregationContext context, final Aggregator parent) throws IOException {
+        final Aggregator first = factory.create(context, parent, true);
         final BigArrays bigArrays = context.bigArrays();
         return new Aggregator() {
 
@@ -199,7 +197,7 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extend
                         if (collector == null) {
                             Aggregator aggregator = aggregators.get(bucket);
                             if (aggregator == null) {
-                                aggregator = factory.create(parent, true);
+                                aggregator = factory.create(context, parent, true);
                                 aggregator.preCollection();
                                 aggregators.set(bucket, aggregator);
                             }
@@ -236,4 +234,4 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extend
         };
     }
 
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
index bb46d13..f38138f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
@@ -18,14 +18,13 @@
  */
 package org.elasticsearch.search.aggregations;
 
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.HashMap;
@@ -55,20 +54,15 @@ public class AggregatorParsers {
      *            ).
      */
     @Inject
-    public AggregatorParsers(Set<Aggregator.Parser> aggParsers, Set<PipelineAggregator.Parser> pipelineAggregatorParsers,
-            NamedWriteableRegistry namedWriteableRegistry) {
+    public AggregatorParsers(Set<Aggregator.Parser> aggParsers, Set<PipelineAggregator.Parser> pipelineAggregatorParsers) {
         Map<String, Aggregator.Parser> aggParsersBuilder = new HashMap<>(aggParsers.size());
         for (Aggregator.Parser parser : aggParsers) {
             aggParsersBuilder.put(parser.type(), parser);
-            AggregatorBuilder<?> factoryPrototype = parser.getFactoryPrototypes();
-            namedWriteableRegistry.registerPrototype(AggregatorBuilder.class, factoryPrototype);
         }
         this.aggParsers = unmodifiableMap(aggParsersBuilder);
         Map<String, PipelineAggregator.Parser> pipelineAggregatorParsersBuilder = new HashMap<>(pipelineAggregatorParsers.size());
         for (PipelineAggregator.Parser parser : pipelineAggregatorParsers) {
             pipelineAggregatorParsersBuilder.put(parser.type(), parser);
-            PipelineAggregatorFactory factoryPrototype = parser.getFactoryPrototype();
-            namedWriteableRegistry.registerPrototype(PipelineAggregatorFactory.class, factoryPrototype);
         }
         this.pipelineAggregatorParsers = unmodifiableMap(pipelineAggregatorParsersBuilder);
     }
@@ -99,50 +93,48 @@ public class AggregatorParsers {
      * Parses the aggregation request recursively generating aggregator factories in turn.
      *
      * @param parser    The input xcontent that will be parsed.
-     * @param parseContext   The parse context.
+     * @param context   The search context.
      *
      * @return          The parsed aggregator factories.
      *
      * @throws IOException When parsing fails for unknown reasons.
      */
-    public AggregatorFactories.Builder parseAggregators(XContentParser parser, QueryParseContext parseContext) throws IOException {
-        return parseAggregators(parser, parseContext, 0);
+    public AggregatorFactories parseAggregators(XContentParser parser, SearchContext context) throws IOException {
+        return parseAggregators(parser, context, 0);
     }
 
 
-    private AggregatorFactories.Builder parseAggregators(XContentParser parser, QueryParseContext parseContext, int level)
-            throws IOException {
+    private AggregatorFactories parseAggregators(XContentParser parser, SearchContext context, int level) throws IOException {
         Matcher validAggMatcher = VALID_AGG_NAME.matcher("");
         AggregatorFactories.Builder factories = new AggregatorFactories.Builder();
 
         XContentParser.Token token = null;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token != XContentParser.Token.FIELD_NAME) {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " in [aggs]: aggregations definitions must start with the name of the aggregation.");
+                throw new SearchParseException(context, "Unexpected token " + token
+                        + " in [aggs]: aggregations definitions must start with the name of the aggregation.", parser.getTokenLocation());
             }
             final String aggregationName = parser.currentName();
             if (!validAggMatcher.reset(aggregationName).matches()) {
-                throw new ParsingException(parser.getTokenLocation(), "Invalid aggregation name [" + aggregationName
-                        + "]. Aggregation names must be alpha-numeric and can only contain '_' and '-'");
+                throw new SearchParseException(context, "Invalid aggregation name [" + aggregationName
+                        + "]. Aggregation names must be alpha-numeric and can only contain '_' and '-'", parser.getTokenLocation());
             }
 
             token = parser.nextToken();
             if (token != XContentParser.Token.START_OBJECT) {
-                throw new ParsingException(parser.getTokenLocation(), "Aggregation definition for [" + aggregationName + " starts with a ["
-                        + token + "], expected a [" + XContentParser.Token.START_OBJECT + "].");
+                throw new SearchParseException(context, "Aggregation definition for [" + aggregationName + " starts with a [" + token
+                        + "], expected a [" + XContentParser.Token.START_OBJECT + "].", parser.getTokenLocation());
             }
 
-            AggregatorBuilder<?> aggFactory = null;
+            AggregatorFactory aggFactory = null;
             PipelineAggregatorFactory pipelineAggregatorFactory = null;
-            AggregatorFactories.Builder subFactories = null;
+            AggregatorFactories subFactories = null;
 
             Map<String, Object> metaData = null;
 
             while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                 if (token != XContentParser.Token.FIELD_NAME) {
-                    throw new ParsingException(
-                            parser.getTokenLocation(), "Expected [" + XContentParser.Token.FIELD_NAME + "] under a ["
+                    throw new SearchParseException(context, "Expected [" + XContentParser.Token.FIELD_NAME + "] under a ["
                             + XContentParser.Token.START_OBJECT + "], but got a [" + token + "] in [" + aggregationName + "]",
                             parser.getTokenLocation());
                 }
@@ -151,8 +143,7 @@ public class AggregatorParsers {
                 token = parser.nextToken();
                 if ("aggregations_binary".equals(fieldName)) {
                     if (subFactories != null) {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Found two sub aggregation definitions under [" + aggregationName + "]",
+                        throw new SearchParseException(context, "Found two sub aggregation definitions under [" + aggregationName + "]",
                                 parser.getTokenLocation());
                     }
                     XContentParser binaryParser = null;
@@ -160,17 +151,17 @@ public class AggregatorParsers {
                         byte[] source = parser.binaryValue();
                         binaryParser = XContentFactory.xContent(source).createParser(source);
                     } else {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Expected [" + XContentParser.Token.VALUE_STRING + " or " + XContentParser.Token.VALUE_EMBEDDED_OBJECT
-                                        + "] for [" + fieldName + "], but got a [" + token + "] in [" + aggregationName + "]");
+                        throw new SearchParseException(context, "Expected [" + XContentParser.Token.VALUE_STRING + " or "
+                                + XContentParser.Token.VALUE_EMBEDDED_OBJECT + "] for [" + fieldName + "], but got a [" + token + "] in ["
+                                + aggregationName + "]", parser.getTokenLocation());
                     }
                     XContentParser.Token binaryToken = binaryParser.nextToken();
                     if (binaryToken != XContentParser.Token.START_OBJECT) {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Expected [" + XContentParser.Token.START_OBJECT + "] as first token when parsing [" + fieldName
-                                        + "], but got a [" + binaryToken + "] in [" + aggregationName + "]");
+                        throw new SearchParseException(context, "Expected [" + XContentParser.Token.START_OBJECT
+                                + "] as first token when parsing [" + fieldName + "], but got a [" + binaryToken + "] in ["
+                                + aggregationName + "]", parser.getTokenLocation());
                     }
-                    subFactories = parseAggregators(binaryParser, parseContext, level + 1);
+                    subFactories = parseAggregators(binaryParser, context, level + 1);
                 } else if (token == XContentParser.Token.START_OBJECT) {
                     switch (fieldName) {
                     case "meta":
@@ -179,66 +170,73 @@ public class AggregatorParsers {
                     case "aggregations":
                     case "aggs":
                         if (subFactories != null) {
-                            throw new ParsingException(parser.getTokenLocation(),
-                                    "Found two sub aggregation definitions under [" + aggregationName + "]");
+                            throw new SearchParseException(context,
+                                    "Found two sub aggregation definitions under [" + aggregationName + "]", parser.getTokenLocation());
                         }
-                        subFactories = parseAggregators(parser, parseContext, level + 1);
+                        subFactories = parseAggregators(parser, context, level + 1);
                         break;
                     default:
                         if (aggFactory != null) {
-                            throw new ParsingException(parser.getTokenLocation(), "Found two aggregation type definitions in ["
-                                    + aggregationName + "]: [" + aggFactory.type + "] and [" + fieldName + "]");
+                            throw new SearchParseException(context, "Found two aggregation type definitions in [" + aggregationName
+                                    + "]: [" + aggFactory.type + "] and [" + fieldName + "]", parser.getTokenLocation());
                         }
                         if (pipelineAggregatorFactory != null) {
-                            throw new ParsingException(parser.getTokenLocation(), "Found two aggregation type definitions in ["
-                                    + aggregationName + "]: [" + pipelineAggregatorFactory + "] and [" + fieldName + "]");
+                            throw new SearchParseException(context, "Found two aggregation type definitions in [" + aggregationName
+                                    + "]: [" + pipelineAggregatorFactory + "] and [" + fieldName + "]", parser.getTokenLocation());
                         }
 
                         Aggregator.Parser aggregatorParser = parser(fieldName);
                         if (aggregatorParser == null) {
                             PipelineAggregator.Parser pipelineAggregatorParser = pipelineAggregator(fieldName);
                             if (pipelineAggregatorParser == null) {
-                                throw new ParsingException(parser.getTokenLocation(),
-                                        "Could not find aggregator type [" + fieldName + "] in [" + aggregationName + "]");
+                                throw new SearchParseException(context, "Could not find aggregator type [" + fieldName + "] in ["
+                                        + aggregationName + "]", parser.getTokenLocation());
                             } else {
-                                pipelineAggregatorFactory = pipelineAggregatorParser.parse(aggregationName, parser, parseContext);
+                                pipelineAggregatorFactory = pipelineAggregatorParser.parse(aggregationName, parser, context);
                             }
                         } else {
-                            aggFactory = aggregatorParser.parse(aggregationName, parser, parseContext);
+                            aggFactory = aggregatorParser.parse(aggregationName, parser, context);
                         }
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.START_OBJECT + "] under ["
-                            + fieldName + "], but got a [" + token + "] in [" + aggregationName + "]");
+                    throw new SearchParseException(context, "Expected [" + XContentParser.Token.START_OBJECT + "] under [" + fieldName
+                            + "], but got a [" + token + "] in [" + aggregationName + "]", parser.getTokenLocation());
                 }
             }
 
             if (aggFactory == null && pipelineAggregatorFactory == null) {
-                throw new ParsingException(parser.getTokenLocation(), "Missing definition for aggregation [" + aggregationName + "]",
+                throw new SearchParseException(context, "Missing definition for aggregation [" + aggregationName + "]",
                         parser.getTokenLocation());
             } else if (aggFactory != null) {
                 assert pipelineAggregatorFactory == null;
-                if (metaData != null) {
+            if (metaData != null) {
                     aggFactory.setMetaData(metaData);
-                }
+            }
 
-                if (subFactories != null) {
-                    aggFactory.subAggregations(subFactories);
-                }
+            if (subFactories != null) {
+                    aggFactory.subFactories(subFactories);
+            }
+
+            if (level == 0) {
+                    aggFactory.validate();
+            }
 
                 factories.addAggregator(aggFactory);
             } else {
                 assert pipelineAggregatorFactory != null;
                 if (subFactories != null) {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Aggregation [" + aggregationName + "] cannot define sub-aggregations",
+                    throw new SearchParseException(context, "Aggregation [" + aggregationName + "] cannot define sub-aggregations",
                             parser.getTokenLocation());
                 }
+                if (level == 0) {
+                    pipelineAggregatorFactory
+                            .validate(null, factories.getAggregatorFactories(), factories.getPipelineAggregatorFactories());
+                }
                 factories.addPipelineAggregator(pipelineAggregatorFactory);
             }
         }
 
-        return factories;
+        return factories.build();
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/ValuesSourceAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/ValuesSourceAggregationBuilder.java
new file mode 100644
index 0000000..2d3c0cf
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/ValuesSourceAggregationBuilder.java
@@ -0,0 +1,112 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.ScriptService.ScriptType;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * A base class for all bucket aggregation builders that are based on values (either script generated or field data values)
+ */
+public abstract class ValuesSourceAggregationBuilder<B extends ValuesSourceAggregationBuilder<B>> extends AggregationBuilder<B> {
+
+    private Script script;
+    private String field;
+    @Deprecated
+    private String scriptString;
+    @Deprecated
+    private String lang;
+    @Deprecated
+    private Map<String, Object> params;
+    private Object missing;
+
+    /**
+     * Constructs a new builder.
+     *
+     * @param name  The name of the aggregation.
+     * @param type  The type of the aggregation.
+     */
+    protected ValuesSourceAggregationBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    /**
+     * Sets the field from which the values will be extracted.
+     *
+     * @param field     The name of the field
+     * @return          This builder (fluent interface support)
+     */
+    @SuppressWarnings("unchecked")
+    public B field(String field) {
+        this.field = field;
+        return (B) this;
+    }
+
+    /**
+     * Sets the script which generates the values. If the script is configured along with the field (as in {@link #field(String)}), then
+     * this script will be treated as a {@code value script}. A <i>value script</i> will be applied on the values that are extracted from
+     * the field data (you can refer to that value in the script using the {@code _value} reserved variable). If only the script is configured
+     * (and the no field is configured next to it), then the script will be responsible to generate the values that will be aggregated.
+     *
+     * @param script    The configured script.
+     * @return          This builder (fluent interface support)
+     */
+    @SuppressWarnings("unchecked")
+    public B script(Script script) {
+        this.script = script;
+        return (B) this;
+    }
+
+    /**
+     * Configure the value to use when documents miss a value.
+     */
+    public B missing(Object missingValue) {
+        this.missing = missingValue;
+        return (B) this;
+    }
+
+    @Override
+    protected final XContentBuilder internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
+        builder.startObject();
+        if (field != null) {
+            builder.field("field", field);
+        }
+
+        if (script == null) {
+            if (scriptString != null) {
+                builder.field("script", new Script(scriptString, ScriptType.INLINE, lang, params));
+            }
+        } else {
+            builder.field("script", script);
+        }
+        if (missing != null) {
+            builder.field("missing", missing);
+        }
+
+        doInternalXContent(builder, builderParams);
+        return builder.endObject();
+    }
+
+    protected abstract XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException;
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenAggregatorFactory.java
deleted file mode 100644
index 9c46a7b..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenAggregatorFactory.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.children;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Bytes.ParentChild;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class ChildrenAggregatorFactory
-        extends ValuesSourceAggregatorFactory<ValuesSource.Bytes.WithOrdinals.ParentChild, ChildrenAggregatorFactory> {
-
-    private final String parentType;
-    private final Query parentFilter;
-    private final Query childFilter;
-
-    public ChildrenAggregatorFactory(String name, Type type, ValuesSourceConfig<ParentChild> config, String parentType, Query childFilter,
-            Query parentFilter, AggregationContext context, AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder,
-            Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.parentType = parentType;
-        this.childFilter = childFilter;
-        this.parentFilter = parentFilter;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new NonCollectingAggregator(name, aggregationContext, parent, pipelineAggregators, metaData) {
-
-            @Override
-            public InternalAggregation buildEmptyAggregation() {
-                return new InternalChildren(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
-            }
-
-        };
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Bytes.WithOrdinals.ParentChild valuesSource, AggregationContext aggregationContext,
-            Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        long maxOrd = valuesSource.globalMaxOrd(aggregationContext.searchContext().searcher(), parentType);
-        return new ParentToChildrenAggregator(name, factories, aggregationContext, parent, parentType, childFilter, parentFilter,
-                valuesSource, maxOrd, pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenBuilder.java
new file mode 100644
index 0000000..dc2670c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenBuilder.java
@@ -0,0 +1,58 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.bucket.children;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Children} aggregation.
+ */
+public class ChildrenBuilder extends AggregationBuilder<ChildrenBuilder> {
+
+    private String childType;
+
+    /**
+     * Sole constructor.
+     */
+    public ChildrenBuilder(String name) {
+        super(name, InternalChildren.TYPE.name());
+    }
+
+    /**
+     * Set the type of children documents. This parameter is compulsory.
+     */
+    public ChildrenBuilder childType(String childType) {
+        this.childType = childType;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (childType == null) {
+            throw new SearchSourceBuilderException("child_type must be set on children aggregation [" + getName() + "]");
+        }
+        builder.field("type", childType);
+        return builder.endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java
index 9246ee6..438e872 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java
@@ -18,11 +18,18 @@
  */
 package org.elasticsearch.search.aggregations.bucket.children;
 
-import org.elasticsearch.common.ParsingException;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.FieldContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -37,7 +44,7 @@ public class ChildrenParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         String childType = null;
 
         XContentParser.Token token;
@@ -49,25 +56,45 @@ public class ChildrenParser implements Aggregator.Parser {
                 if ("type".equals(currentFieldName)) {
                     childType = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + aggregationName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (childType == null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Missing [child_type] field for children aggregation [" + aggregationName + "]");
+            throw new SearchParseException(context, "Missing [child_type] field for children aggregation [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
+        ValuesSourceConfig<ValuesSource.Bytes.WithOrdinals.ParentChild> config = new ValuesSourceConfig<>(ValuesSource.Bytes.WithOrdinals.ParentChild.class);
+        DocumentMapper childDocMapper = context.mapperService().documentMapper(childType);
 
-        return new ParentToChildrenAggregator.ChildrenAggregatorBuilder(aggregationName, childType);
+        String parentType = null;
+        Query parentFilter = null;
+        Query childFilter = null;
+        if (childDocMapper != null) {
+            ParentFieldMapper parentFieldMapper = childDocMapper.parentFieldMapper();
+            if (!parentFieldMapper.active()) {
+                throw new SearchParseException(context, "[children] no [_parent] field not configured that points to a parent type", parser.getTokenLocation());
             }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new ParentToChildrenAggregator.ChildrenAggregatorBuilder(null, null);
+            parentType = parentFieldMapper.type();
+            DocumentMapper parentDocMapper = context.mapperService().documentMapper(parentType);
+            if (parentDocMapper != null) {
+                // TODO: use the query API
+                parentFilter = parentDocMapper.typeFilter();
+                childFilter = childDocMapper.typeFilter();
+                ParentChildIndexFieldData parentChildIndexFieldData = context.fieldData().getForField(parentFieldMapper.fieldType());
+                config.fieldContext(new FieldContext(parentFieldMapper.fieldType().name(), parentChildIndexFieldData, parentFieldMapper.fieldType()));
+            } else {
+                config.unmapped(true);
+            }
+        } else {
+            config.unmapped(true);
+        }
+        return new ParentToChildrenAggregator.Factory(aggregationName, config, parentType, parentFilter, childFilter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java
index 386eb9c..63819b9 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java
@@ -27,48 +27,31 @@ import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.util.LongArray;
 import org.elasticsearch.common.util.LongObjectPagedHashMap;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
-import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
+import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.FieldContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Bytes.ParentChild;
 
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 // The RecordingPerReaderBucketCollector assumes per segment recording which isn't the case for this
 // aggregation, for this reason that collector can't be used
 public class ParentToChildrenAggregator extends SingleBucketAggregator {
 
-    static final ParseField TYPE_FIELD = new ParseField("type");
-
     private final String parentType;
     private final Weight childFilter;
     private final Weight parentFilter;
@@ -192,88 +175,39 @@ public class ParentToChildrenAggregator extends SingleBucketAggregator {
         Releasables.close(parentOrdToBuckets, parentOrdToOtherBuckets);
     }
 
-    public static class ChildrenAggregatorBuilder extends ValuesSourceAggregatorBuilder<ParentChild, ChildrenAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.Bytes.WithOrdinals.ParentChild> {
 
-        private String parentType;
-        private final String childType;
-        private Query parentFilter;
-        private Query childFilter;
+        private final String parentType;
+        private final Query parentFilter;
+        private final Query childFilter;
 
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param childType
-         *            the type of children documents
-         */
-        public ChildrenAggregatorBuilder(String name, String childType) {
-            super(name, InternalChildren.TYPE, ValuesSourceType.BYTES, ValueType.STRING);
-            this.childType = childType;
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Bytes.WithOrdinals.ParentChild> config, String parentType, Query parentFilter, Query childFilter) {
+            super(name, InternalChildren.TYPE.name(), config);
+            this.parentType = parentType;
+            this.parentFilter = parentFilter;
+            this.childFilter = childFilter;
         }
 
         @Override
-        protected ValuesSourceAggregatorFactory<ParentChild, ?> innerBuild(AggregationContext context,
-                ValuesSourceConfig<ParentChild> config, AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new ChildrenAggregatorFactory(name, type, config, parentType, childFilter, parentFilter, context, parent,
-                    subFactoriesBuilder, metaData);
-        }
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            return new NonCollectingAggregator(name, aggregationContext, parent, pipelineAggregators, metaData) {
 
-        @Override
-        protected ValuesSourceConfig<ParentChild> resolveConfig(AggregationContext aggregationContext) {
-            ValuesSourceConfig<ParentChild> config = new ValuesSourceConfig<>(ValuesSourceType.BYTES);
-            DocumentMapper childDocMapper = aggregationContext.searchContext().mapperService().documentMapper(childType);
-
-            if (childDocMapper != null) {
-                ParentFieldMapper parentFieldMapper = childDocMapper.parentFieldMapper();
-                if (!parentFieldMapper.active()) {
-                    throw new SearchParseException(aggregationContext.searchContext(),
-                            "[children] no [_parent] field not configured that points to a parent type", null); // NOCOMMIT fix exception args
-                }
-                parentType = parentFieldMapper.type();
-                DocumentMapper parentDocMapper = aggregationContext.searchContext().mapperService().documentMapper(parentType);
-                if (parentDocMapper != null) {
-                    parentFilter = parentDocMapper.typeFilter();
-                    childFilter = childDocMapper.typeFilter();
-                    ParentChildIndexFieldData parentChildIndexFieldData = aggregationContext.searchContext().fieldData()
-                            .getForField(parentFieldMapper.fieldType());
-                    config.fieldContext(new FieldContext(parentFieldMapper.fieldType().name(), parentChildIndexFieldData,
-                            parentFieldMapper.fieldType()));
-                } else {
-                    config.unmapped(true);
+                @Override
+                public InternalAggregation buildEmptyAggregation() {
+                    return new InternalChildren(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
                 }
-            } else {
-                config.unmapped(true);
-            }
-            return config;
-        }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(TYPE_FIELD.getPreferredName(), childType);
-            return builder;
-        }
-
-        @Override
-        protected ChildrenAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            String childType = in.readString();
-            ChildrenAggregatorBuilder factory = new ChildrenAggregatorBuilder(name, childType);
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeString(childType);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(childType);
+            };
         }
 
         @Override
-        protected boolean innerEquals(Object obj) {
-            ChildrenAggregatorBuilder other = (ChildrenAggregatorBuilder) obj;
-            return Objects.equals(childType, other.childType);
+        protected Aggregator doCreateInternal(ValuesSource.Bytes.WithOrdinals.ParentChild valuesSource,
+                AggregationContext aggregationContext, Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            long maxOrd = valuesSource.globalMaxOrd(aggregationContext.searchContext().searcher(), parentType);
+            return new ParentToChildrenAggregator(name, factories, aggregationContext, parent, parentType, childFilter, parentFilter,
+                    valuesSource, maxOrd, pipelineAggregators, metaData);
         }
 
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregationBuilder.java
new file mode 100644
index 0000000..45d40bd
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregationBuilder.java
@@ -0,0 +1,60 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.filter;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Filter} aggregation.
+ */
+public class FilterAggregationBuilder extends AggregationBuilder<FilterAggregationBuilder> {
+
+    private QueryBuilder filter;
+
+    /**
+     * Sole constructor.
+     */
+    public FilterAggregationBuilder(String name) {
+        super(name, InternalFilter.TYPE.name());
+    }
+
+    /**
+     * Set the filter to use, only documents that match this filter will fall
+     * into the bucket defined by this {@link Filter} aggregation.
+     */
+    public FilterAggregationBuilder filter(QueryBuilder filter) {
+        this.filter = filter;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (filter == null) {
+            throw new SearchSourceBuilderException("filter must be set on filter aggregation [" + getName() + "]");
+        }
+        filter.toXContent(builder, params);
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java
index 9fae8bf..30c34c3 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java
@@ -19,17 +19,14 @@
 package org.elasticsearch.search.aggregations.bucket.filter;
 
 import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
@@ -40,7 +37,6 @@ import org.elasticsearch.search.aggregations.support.AggregationContext;
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Aggregate all docs that match a filter.
@@ -85,57 +81,31 @@ public class FilterAggregator extends SingleBucketAggregator {
         return new InternalFilter(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
     }
 
-    public static class FilterAggregatorBuilder extends AggregatorBuilder<FilterAggregatorBuilder> {
+    public static class Factory extends AggregatorFactory {
 
-        private QueryBuilder<?> filter;
+        private final Query filter;
 
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param filter
-         *            Set the filter to use, only documents that match this
-         *            filter will fall into the bucket defined by this
-         *            {@link Filter} aggregation.
-         */
-        public FilterAggregatorBuilder(String name, QueryBuilder<?> filter) {
-            super(name, InternalFilter.TYPE);
+        public Factory(String name, Query filter) {
+            super(name, InternalFilter.TYPE.name());
             this.filter = filter;
         }
 
-        @Override
-        protected AggregatorFactory<?> doBuild(AggregationContext context, AggregatorFactory<?> parent,
-                AggregatorFactories.Builder subFactoriesBuilder) throws IOException {
-            return new FilterAggregatorFactory(name, type, filter, context, parent, subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (filter != null) {
-                filter.toXContent(builder, params);
-            }
-            return builder;
-        }
+        // TODO: refactor in order to initialize the factory once with its parent,
+        // the context, etc. and then have a no-arg lightweight create method
+        // (since create may be called thousands of times)
 
-        @Override
-        protected FilterAggregatorBuilder doReadFrom(String name, StreamInput in) throws IOException {
-            FilterAggregatorBuilder factory = new FilterAggregatorBuilder(name, in.readQuery());
-            return factory;
-        }
+        private IndexSearcher searcher;
+        private Weight weight;
 
         @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeQuery(filter);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(filter);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            FilterAggregatorBuilder other = (FilterAggregatorBuilder) obj;
-            return Objects.equals(filter, other.filter);
+        public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            IndexSearcher contextSearcher = context.searchContext().searcher();
+            if (searcher != contextSearcher) {
+                searcher = contextSearcher;
+                weight = contextSearcher.createNormalizedWeight(filter, false);
+            }
+            return new FilterAggregator(name, weight, factories, context, parent, pipelineAggregators, metaData);
         }
 
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregatorFactory.java
deleted file mode 100644
index f1659ae..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregatorFactory.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.filter;
-
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.Weight;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class FilterAggregatorFactory extends AggregatorFactory<FilterAggregatorFactory> {
-
-    private final Weight weight;
-
-    public FilterAggregatorFactory(String name, Type type, QueryBuilder<?> filterBuilder, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactoriesBuilder, metaData);
-        IndexSearcher contextSearcher = context.searchContext().searcher();
-        Query filter = filterBuilder.toQuery(context.searchContext().indexShard().getQueryShardContext());
-        weight = contextSearcher.createNormalizedWeight(filter, false);
-    }
-
-    @Override
-    public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new FilterAggregator(name, weight, factories, context, parent, pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java
index 972d29a..d532dc4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java
@@ -18,13 +18,12 @@
  */
 package org.elasticsearch.search.aggregations.bucket.filter;
 
-import org.elasticsearch.common.ParsingException;
+import org.apache.lucene.search.MatchAllDocsQuery;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.MatchAllQueryBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -39,22 +38,10 @@ public class FilterParser implements Aggregator.Parser {
     }
 
     @Override
-    public FilterAggregator.FilterAggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context)
-            throws IOException {
-        QueryBuilder<?> filter = context.parseInnerQueryBuilder();
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        ParsedQuery filter = context.getQueryShardContext().parseInnerFilter(parser);
 
-        if (filter == null) {
-            throw new ParsingException(null, "filter cannot be null in filter aggregation [{}]", aggregationName);
-        }
-
-        FilterAggregator.FilterAggregatorBuilder factory = new FilterAggregator.FilterAggregatorBuilder(aggregationName,
-                filter == null ? new MatchAllQueryBuilder() : filter);
-        return factory;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new FilterAggregator.FilterAggregatorBuilder(null, null);
+        return new FilterAggregator.Factory(aggregationName, filter == null ? new MatchAllDocsQuery() : filter.query());
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregationBuilder.java
new file mode 100644
index 0000000..6f61a89
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregationBuilder.java
@@ -0,0 +1,126 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.filters;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Builder for the {@link Filters} aggregation.
+ */
+public class FiltersAggregationBuilder extends AggregationBuilder<FiltersAggregationBuilder> {
+
+    private Map<String, QueryBuilder> keyedFilters = null;
+    private List<QueryBuilder> nonKeyedFilters = null;
+    private Boolean otherBucket;
+    private String otherBucketKey;
+
+    /**
+     * Sole constructor.
+     */
+    public FiltersAggregationBuilder(String name) {
+        super(name, InternalFilters.TYPE.name());
+    }
+
+    /**
+     * Add a new filter with the given key.
+     * NOTE: if a filter was already defined for this key, then this filter will replace it.
+     * NOTE: the same {@link FiltersAggregationBuilder} cannot have both keyed and non-keyed filters
+     */
+    public FiltersAggregationBuilder filter(String key, QueryBuilder filter) {
+        if (keyedFilters == null) {
+            keyedFilters = new LinkedHashMap<>();
+        }
+        keyedFilters.put(key, filter);
+        return this;
+    }
+
+    /**
+     * Add a new filter with no key.
+     * NOTE: the same {@link FiltersAggregationBuilder} cannot have both keyed and non-keyed filters.
+     */
+    public FiltersAggregationBuilder filter(QueryBuilder filter) {
+        if (nonKeyedFilters == null) {
+            nonKeyedFilters = new ArrayList<>();
+        }
+        nonKeyedFilters.add(filter);
+        return this;
+    }
+
+    /**
+     * Include a bucket for documents not matching any filter
+     */
+    public FiltersAggregationBuilder otherBucket(boolean otherBucket) {
+        this.otherBucket = otherBucket;
+        return this;
+    }
+
+    /**
+     * The key to use for the bucket for documents not matching any filter. Will
+     * implicitly enable the other bucket if set.
+     */
+    public FiltersAggregationBuilder otherBucketKey(String otherBucketKey) {
+        this.otherBucketKey = otherBucketKey;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (keyedFilters == null && nonKeyedFilters == null) {
+            throw new SearchSourceBuilderException("At least one filter must be set on filter aggregation [" + getName() + "]");
+        }
+        if (keyedFilters != null && nonKeyedFilters != null) {
+            throw new SearchSourceBuilderException("Cannot add both keyed and non-keyed filters to filters aggregation");
+        }
+
+        if (keyedFilters != null) {
+            builder.startObject(FiltersParser.FILTERS_FIELD.getPreferredName());
+            for (Map.Entry<String, QueryBuilder> entry : keyedFilters.entrySet()) {
+                builder.field(entry.getKey());
+                entry.getValue().toXContent(builder, params);
+            }
+            builder.endObject();
+        }
+        if (nonKeyedFilters != null) {
+            builder.startArray(FiltersParser.FILTERS_FIELD.getPreferredName());
+            for (QueryBuilder filterBuilder : nonKeyedFilters) {
+                filterBuilder.toXContent(builder, params);
+            }
+            builder.endArray();
+
+        }
+        if (otherBucketKey != null) {
+            builder.field(FiltersParser.OTHER_BUCKET_KEY_FIELD.getPreferredName(), otherBucketKey);
+        }
+        if (otherBucket != null) {
+            builder.field(FiltersParser.OTHER_BUCKET_FIELD.getPreferredName(), otherBucket);
+        }
+        return builder.endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java
index 0a7ae5e..c16089e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java
@@ -20,100 +20,41 @@
 package org.elasticsearch.search.aggregations.bucket.filters;
 
 import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregations;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class FiltersAggregator extends BucketsAggregator {
 
-    public static final ParseField FILTERS_FIELD = new ParseField("filters");
-    public static final ParseField OTHER_BUCKET_FIELD = new ParseField("other_bucket");
-    public static final ParseField OTHER_BUCKET_KEY_FIELD = new ParseField("other_bucket_key");
+    static class KeyedFilter {
 
-    public static class KeyedFilter implements Writeable<KeyedFilter>, ToXContent {
+        final String key;
+        final Query filter;
 
-        static final KeyedFilter PROTOTYPE = new KeyedFilter(null, null);
-        private final String key;
-        private final QueryBuilder<?> filter;
-
-        public KeyedFilter(String key, QueryBuilder<?> filter) {
+        KeyedFilter(String key, Query filter) {
             this.key = key;
             this.filter = filter;
         }
-
-        public String key() {
-            return key;
-        }
-
-        public QueryBuilder<?> filter() {
-            return filter;
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(key, filter);
-            return builder;
-        }
-
-        @Override
-        public KeyedFilter readFrom(StreamInput in) throws IOException {
-            String key = in.readString();
-            QueryBuilder<?> filter = in.readQuery();
-            return new KeyedFilter(key, filter);
-    }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeString(key);
-            out.writeQuery(filter);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(key, filter);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            KeyedFilter other = (KeyedFilter) obj;
-            return Objects.equals(key, other.key)
-                    && Objects.equals(filter, other.filter);
-        }
     }
 
     private final String[] keys;
@@ -198,159 +139,45 @@ public class FiltersAggregator extends BucketsAggregator {
         return owningBucketOrdinal * totalNumKeys + filterOrd;
     }
 
-    public static class FiltersAggregatorBuilder extends AggregatorBuilder<FiltersAggregatorBuilder> {
+    public static class Factory extends AggregatorFactory {
 
         private final List<KeyedFilter> filters;
-        private final boolean keyed;
-        private boolean otherBucket = false;
-        private String otherBucketKey = "_other_";
-
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param filters
-         *            the KeyedFilters to use with this aggregation.
-         */
-        public FiltersAggregatorBuilder(String name, KeyedFilter... filters) {
-            this(name, Arrays.asList(filters));
-        }
+        private final String[] keys;
+        private boolean keyed;
+        private String otherBucketKey;
 
-        private FiltersAggregatorBuilder(String name, List<KeyedFilter> filters) {
-            super(name, InternalFilters.TYPE);
+        public Factory(String name, List<KeyedFilter> filters, boolean keyed, String otherBucketKey) {
+            super(name, InternalFilters.TYPE.name());
             this.filters = filters;
-            this.keyed = true;
-        }
-
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param filters
-         *            the filters to use with this aggregation
-         */
-        public FiltersAggregatorBuilder(String name, QueryBuilder<?>... filters) {
-            super(name, InternalFilters.TYPE);
-            List<KeyedFilter> keyedFilters = new ArrayList<>(filters.length);
-            for (int i = 0; i < filters.length; i++) {
-                keyedFilters.add(new KeyedFilter(String.valueOf(i), filters[i]));
-            }
-            this.filters = keyedFilters;
-            this.keyed = false;
-        }
-
-        /**
-         * Set whether to include a bucket for documents not matching any filter
-         */
-        public FiltersAggregatorBuilder otherBucket(boolean otherBucket) {
-            this.otherBucket = otherBucket;
-            return this;
-        }
-
-        /**
-         * Get whether to include a bucket for documents not matching any filter
-         */
-        public boolean otherBucket() {
-            return otherBucket;
-        }
-
-        /**
-         * Set the key to use for the bucket for documents not matching any
-         * filter.
-         */
-        public FiltersAggregatorBuilder otherBucketKey(String otherBucketKey) {
+            this.keyed = keyed;
             this.otherBucketKey = otherBucketKey;
-            return this;
-        }
-
-        /**
-         * Get the key to use for the bucket for documents not matching any
-         * filter.
-         */
-        public String otherBucketKey() {
-            return otherBucketKey;
-        }
-
-        @Override
-        protected AggregatorFactory<?> doBuild(AggregationContext context, AggregatorFactory<?> parent, Builder subFactoriesBuilder)
-                throws IOException {
-            return new FiltersAggregatorFactory(name, type, filters, keyed, otherBucket, otherBucketKey, context, parent,
-                    subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            if (keyed) {
-                builder.startObject(FILTERS_FIELD.getPreferredName());
-                for (KeyedFilter keyedFilter : filters) {
-                    builder.field(keyedFilter.key(), keyedFilter.filter());
-                }
-                builder.endObject();
-            } else {
-                builder.startArray(FILTERS_FIELD.getPreferredName());
-                for (KeyedFilter keyedFilter : filters) {
-                    builder.value(keyedFilter.filter());
-                }
-                builder.endArray();
+            this.keys = new String[filters.size()];
+            for (int i = 0; i < filters.size(); ++i) {
+                KeyedFilter keyedFilter = filters.get(i);
+                this.keys[i] = keyedFilter.key;
             }
-            builder.field(OTHER_BUCKET_FIELD.getPreferredName(), otherBucket);
-            builder.field(OTHER_BUCKET_KEY_FIELD.getPreferredName(), otherBucketKey);
-            builder.endObject();
-            return builder;
         }
 
-        @Override
-        protected FiltersAggregatorBuilder doReadFrom(String name, StreamInput in) throws IOException {
-            FiltersAggregatorBuilder factory;
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<KeyedFilter> filters = new ArrayList<>(size);
-                for (int i = 0; i < size; i++) {
-                    filters.add(KeyedFilter.PROTOTYPE.readFrom(in));
-                }
-                factory = new FiltersAggregatorBuilder(name, filters);
-            } else {
-                int size = in.readVInt();
-                QueryBuilder<?>[] filters = new QueryBuilder<?>[size];
-                for (int i = 0; i < size; i++) {
-                    filters[i] = in.readQuery();
-                }
-                factory = new FiltersAggregatorBuilder(name, filters);
-            }
-            factory.otherBucket = in.readBoolean();
-            factory.otherBucketKey = in.readString();
-            return factory;
-        }
+        // TODO: refactor in order to initialize the factory once with its parent,
+        // the context, etc. and then have a no-arg lightweight create method
+        // (since create may be called thousands of times)
+
+        private IndexSearcher searcher;
+        private Weight[] weights;
 
         @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeBoolean(keyed);
-            if (keyed) {
-                out.writeVInt(filters.size());
-                for (KeyedFilter keyedFilter : filters) {
-                    keyedFilter.writeTo(out);
-                }
-            } else {
-                out.writeVInt(filters.size());
-                for (KeyedFilter keyedFilter : filters) {
-                    out.writeQuery(keyedFilter.filter());
+        public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            IndexSearcher contextSearcher = context.searchContext().searcher();
+            if (searcher != contextSearcher) {
+                searcher = contextSearcher;
+                weights = new Weight[filters.size()];
+                for (int i = 0; i < filters.size(); ++i) {
+                    KeyedFilter keyedFilter = filters.get(i);
+                    this.weights[i] = contextSearcher.createNormalizedWeight(keyedFilter.filter, false);
                 }
             }
-            out.writeBoolean(otherBucket);
-            out.writeString(otherBucketKey);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(filters, keyed, otherBucket, otherBucketKey);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            FiltersAggregatorBuilder other = (FiltersAggregatorBuilder) obj;
-            return Objects.equals(filters, other.filters)
-                    && Objects.equals(keyed, other.keyed)
-                    && Objects.equals(otherBucket, other.otherBucket)
-                    && Objects.equals(otherBucketKey, other.otherBucketKey);
+            return new FiltersAggregator(name, factories, keys, weights, keyed, otherBucketKey, context, parent, pipelineAggregators, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregatorFactory.java
deleted file mode 100644
index 2564fa5..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregatorFactory.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.filters;
-
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.Weight;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.KeyedFilter;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class FiltersAggregatorFactory extends AggregatorFactory<FiltersAggregatorFactory> {
-
-    private final String[] keys;
-    private final Weight[] weights;
-    private final boolean keyed;
-    private final boolean otherBucket;
-    private final String otherBucketKey;
-
-    public FiltersAggregatorFactory(String name, Type type, List<KeyedFilter> filters, boolean keyed, boolean otherBucket,
-            String otherBucketKey, AggregationContext context, AggregatorFactory<?> parent, AggregatorFactories.Builder subFactories,
-            Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactories, metaData);
-        this.keyed = keyed;
-        this.otherBucket = otherBucket;
-        this.otherBucketKey = otherBucketKey;
-        IndexSearcher contextSearcher = context.searchContext().searcher();
-        weights = new Weight[filters.size()];
-        keys = new String[filters.size()];
-        for (int i = 0; i < filters.size(); ++i) {
-            KeyedFilter keyedFilter = filters.get(i);
-            this.keys[i] = keyedFilter.key();
-            Query filter = keyedFilter.filter().toFilter(context.searchContext().indexShard().getQueryShardContext());
-            this.weights[i] = contextSearcher.createNormalizedWeight(filter, false);
-        }
-    }
-
-    @Override
-    public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new FiltersAggregator(name, factories, keys, weights, keyed, otherBucket ? otherBucketKey : null, context, parent,
-                pipelineAggregators, metaData);
-    }
-
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java
index be9eb26..b7e5e6f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java
@@ -20,15 +20,13 @@
 package org.elasticsearch.search.aggregations.bucket.filters;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
+import org.elasticsearch.index.query.ParsedQuery;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -42,12 +40,6 @@ public class FiltersParser implements Aggregator.Parser {
     public static final ParseField FILTERS_FIELD = new ParseField("filters");
     public static final ParseField OTHER_BUCKET_FIELD = new ParseField("other_bucket");
     public static final ParseField OTHER_BUCKET_KEY_FIELD = new ParseField("other_bucket_key");
-    private final IndicesQueriesRegistry queriesRegistry;
-
-    @Inject
-    public FiltersParser(IndicesQueriesRegistry queriesRegistry) {
-        this.queriesRegistry = queriesRegistry;
-    }
 
     @Override
     public String type() {
@@ -55,15 +47,15 @@ public class FiltersParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-        List<FiltersAggregator.KeyedFilter> keyedFilters = null;
-        List<QueryBuilder<?>> nonKeyedFilters = null;
+        List<FiltersAggregator.KeyedFilter> filters = new ArrayList<>();
 
         XContentParser.Token token = null;
         String currentFieldName = null;
+        Boolean keyed = null;
         String otherBucketKey = null;
-        Boolean otherBucket = false;
+        boolean otherBucket = false;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -71,53 +63,50 @@ public class FiltersParser implements Aggregator.Parser {
                 if (context.parseFieldMatcher().match(currentFieldName, OTHER_BUCKET_FIELD)) {
                     otherBucket = parser.booleanValue();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.VALUE_STRING) {
                 if (context.parseFieldMatcher().match(currentFieldName, OTHER_BUCKET_KEY_FIELD)) {
                     otherBucketKey = parser.text();
+                    otherBucket = true;
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (context.parseFieldMatcher().match(currentFieldName, FILTERS_FIELD)) {
-                    keyedFilters = new ArrayList<>();
+                    keyed = true;
                     String key = null;
                     while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                         if (token == XContentParser.Token.FIELD_NAME) {
                             key = parser.currentName();
                         } else {
-                            QueryParseContext queryParseContext = new QueryParseContext(queriesRegistry);
-                            queryParseContext.reset(parser);
-                            queryParseContext.parseFieldMatcher(context.parseFieldMatcher());
-                            QueryBuilder<?> filter = queryParseContext.parseInnerQueryBuilder();
-                            keyedFilters
-                                    .add(new FiltersAggregator.KeyedFilter(key, filter == null ? QueryBuilders.matchAllQuery() : filter));
+                            ParsedQuery filter = context.getQueryShardContext().parseInnerFilter(parser);
+                            filters.add(new FiltersAggregator.KeyedFilter(key, filter == null ? Queries.newMatchAllQuery() : filter.query()));
                         }
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, FILTERS_FIELD)) {
-                    nonKeyedFilters = new ArrayList<>();
+                    keyed = false;
+                    int idx = 0;
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        QueryParseContext queryParseContext = new QueryParseContext(queriesRegistry);
-                        queryParseContext.reset(parser);
-                        queryParseContext.parseFieldMatcher(context.parseFieldMatcher());
-                        QueryBuilder<?> filter = queryParseContext.parseInnerQueryBuilder();
-                        nonKeyedFilters.add(filter == null ? QueryBuilders.matchAllQuery() : filter);
+                        ParsedQuery filter = context.getQueryShardContext().parseInnerFilter(parser);
+                        filters.add(new FiltersAggregator.KeyedFilter(String.valueOf(idx), filter == null ? Queries.newMatchAllQuery()
+                                : filter.query()));
+                        idx++;
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
             }
         }
 
@@ -125,24 +114,7 @@ public class FiltersParser implements Aggregator.Parser {
             otherBucketKey = "_other_";
         }
 
-        FiltersAggregator.FiltersAggregatorBuilder factory;
-        if (keyedFilters != null) {
-            factory = new FiltersAggregator.FiltersAggregatorBuilder(aggregationName, keyedFilters.toArray(new FiltersAggregator.KeyedFilter[keyedFilters.size()]));
-        } else {
-            factory = new FiltersAggregator.FiltersAggregatorBuilder(aggregationName, nonKeyedFilters.toArray(new QueryBuilder<?>[nonKeyedFilters.size()]));
-        }
-        if (otherBucket != null) {
-            factory.otherBucket(otherBucket);
-        }
-        if (otherBucketKey != null) {
-            factory.otherBucketKey(otherBucketKey);
-        }
-        return factory;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new FiltersAggregator.FiltersAggregatorBuilder(null, new FiltersAggregator.KeyedFilter[0]);
+        return new FiltersAggregator.Factory(aggregationName, filters, keyed, otherBucketKey);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregator.java
index c00f90c..ef2915f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregator.java
@@ -46,10 +46,10 @@ public class GeoHashGridAggregator extends BucketsAggregator {
 
     private final int requiredSize;
     private final int shardSize;
-    private final GeoHashGridParser.GeoGridAggregatorBuilder.CellIdSource valuesSource;
+    private final GeoHashGridParser.GeoGridFactory.CellIdSource valuesSource;
     private final LongHash bucketOrds;
 
-    public GeoHashGridAggregator(String name, AggregatorFactories factories, GeoHashGridParser.GeoGridAggregatorBuilder.CellIdSource valuesSource,
+    public GeoHashGridAggregator(String name, AggregatorFactories factories, GeoHashGridParser.GeoGridFactory.CellIdSource valuesSource,
             int requiredSize, int shardSize, AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
             Map<String, Object> metaData) throws IOException {
         super(name, factories, aggregationContext, parent, pipelineAggregators, metaData);
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregatorFactory.java
deleted file mode 100644
index 22a4c6f..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregatorFactory.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.geogrid;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridParser.GeoGridAggregatorBuilder.CellIdSource;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.GeoPoint;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-public class GeoHashGridAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, GeoHashGridAggregatorFactory> {
-
-    private final int precision;
-    private final int requiredSize;
-    private final int shardSize;
-
-    public GeoHashGridAggregatorFactory(String name, Type type, ValuesSourceConfig<GeoPoint> config, int precision, int requiredSize,
-            int shardSize, AggregationContext context, AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder,
-            Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.precision = precision;
-        this.requiredSize = requiredSize;
-        this.shardSize = shardSize;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        final InternalAggregation aggregation = new InternalGeoHashGrid(name, requiredSize,
-                Collections.<InternalGeoHashGrid.Bucket> emptyList(), pipelineAggregators, metaData);
-        return new NonCollectingAggregator(name, aggregationContext, parent, pipelineAggregators, metaData) {
-            @Override
-            public InternalAggregation buildEmptyAggregation() {
-                return aggregation;
-            }
-        };
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(final ValuesSource.GeoPoint valuesSource, AggregationContext aggregationContext,
-            Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        if (collectsFromSingleBucket == false) {
-            return asMultiBucketAggregator(this, aggregationContext, parent);
-        }
-        CellIdSource cellIdSource = new CellIdSource(valuesSource, precision);
-        return new GeoHashGridAggregator(name, factories, cellIdSource, requiredSize, shardSize, aggregationContext, parent,
-                pipelineAggregators, metaData);
-
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridBuilder.java
new file mode 100644
index 0000000..a1f12f4
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridBuilder.java
@@ -0,0 +1,97 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.bucket.geogrid;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Creates an aggregation based on bucketing points into GeoHashes
+ */
+public class GeoHashGridBuilder extends AggregationBuilder<GeoHashGridBuilder> {
+
+
+    private String field;
+    private int precision = GeoHashGridParams.DEFAULT_PRECISION;
+    private int requiredSize = GeoHashGridParams.DEFAULT_MAX_NUM_CELLS;
+    private int shardSize = 0;
+
+    /**
+     * Sole constructor.
+     */
+    public GeoHashGridBuilder(String name) {
+        super(name, InternalGeoHashGrid.TYPE.name());
+    }
+
+    /**
+     * Set the field to use to get geo points.
+     */
+    public GeoHashGridBuilder field(String field) {
+        this.field = field;
+        return this;
+    }
+
+    /**
+     * Set the geohash precision to use for this aggregation. The higher the
+     * precision, the more fine-grained this aggregation will be.
+     */
+    public GeoHashGridBuilder precision(int precision) {
+        this.precision = GeoHashGridParams.checkPrecision(precision);
+        return this;
+    }
+
+    /**
+     * Set the number of buckets to return.
+     */
+    public GeoHashGridBuilder size(int requiredSize) {
+        this.requiredSize = requiredSize;
+        return this;
+    }
+
+    /**
+     * Expert: Set the number of buckets to get on each shard to improve
+     * accuracy.
+     */
+    public GeoHashGridBuilder shardSize(int shardSize) {
+        this.shardSize = shardSize;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (field != null) {
+            builder.field("field", field);
+        }
+        if (precision != GeoHashGridParams.DEFAULT_PRECISION) {
+            builder.field(GeoHashGridParams.FIELD_PRECISION.getPreferredName(), precision);
+        }
+        if (requiredSize != GeoHashGridParams.DEFAULT_MAX_NUM_CELLS) {
+            builder.field(GeoHashGridParams.FIELD_SIZE.getPreferredName(), requiredSize);
+        }
+        if (shardSize != 0) {
+            builder.field(GeoHashGridParams.FIELD_SHARD_SIZE.getPreferredName(), shardSize);
+        }
+
+        return builder.endObject();
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
index b67edfe..6473b5a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
@@ -21,204 +21,133 @@ package org.elasticsearch.search.aggregations.bucket.geogrid;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.SortedNumericDocValues;
 import org.apache.lucene.util.GeoHashUtils;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.index.fielddata.SortedBinaryDocValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortingNumericDocValues;
 import org.elasticsearch.index.query.GeoBoundingBoxQueryBuilder;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
+import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.BucketUtils;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.GeoPointValuesSourceParser;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
+import java.util.Collections;
+import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Aggregates Geo information into cells determined by geohashes of a given precision.
  * WARNING - for high-precision geohashes it may prove necessary to use a {@link GeoBoundingBoxQueryBuilder}
  * aggregation to focus in on a smaller area to avoid generating too many buckets and using too much RAM
  */
-public class GeoHashGridParser extends GeoPointValuesSourceParser {
-
-    public static final int DEFAULT_PRECISION = 5;
-    public static final int DEFAULT_MAX_NUM_CELLS = 10000;
-
-    public GeoHashGridParser() {
-        super(false, false);
-    }
+public class GeoHashGridParser implements Aggregator.Parser {
 
     @Override
     public String type() {
         return InternalGeoHashGrid.TYPE.name();
     }
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new GeoGridAggregatorBuilder(null);
-    }
 
     @Override
-    protected GeoGridAggregatorBuilder createFactory(
-            String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        GeoGridAggregatorBuilder factory = new GeoGridAggregatorBuilder(aggregationName);
-        Integer precision = (Integer) otherOptions.get(GeoHashGridParams.FIELD_PRECISION);
-        if (precision != null) {
-            factory.precision(precision);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoHashGrid.TYPE, context).build();
+
+        int precision = GeoHashGridParams.DEFAULT_PRECISION;
+        int requiredSize = GeoHashGridParams.DEFAULT_MAX_NUM_CELLS;
+        int shardSize = -1;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_NUMBER ||
+                    token == XContentParser.Token.VALUE_STRING) { //Be lenient and also allow numbers enclosed in quotes
+                if (context.parseFieldMatcher().match(currentFieldName, GeoHashGridParams.FIELD_PRECISION)) {
+                    precision = GeoHashGridParams.checkPrecision(parser.intValue());
+                } else if (context.parseFieldMatcher().match(currentFieldName, GeoHashGridParams.FIELD_SIZE)) {
+                    requiredSize = parser.intValue();
+                } else if (context.parseFieldMatcher().match(currentFieldName, GeoHashGridParams.FIELD_SHARD_SIZE)) {
+                    shardSize = parser.intValue();
                 }
-        Integer size = (Integer) otherOptions.get(GeoHashGridParams.FIELD_SIZE);
-        if (size != null) {
-            factory.size(size);
-        }
-        Integer shardSize = (Integer) otherOptions.get(GeoHashGridParams.FIELD_SHARD_SIZE);
-        if (shardSize != null) {
-            factory.shardSize(shardSize);
-        }
-        return factory;
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.VALUE_NUMBER || token == XContentParser.Token.VALUE_STRING) {
-            if (parseFieldMatcher.match(currentFieldName, GeoHashGridParams.FIELD_PRECISION)) {
-                otherOptions.put(GeoHashGridParams.FIELD_PRECISION, parser.intValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, GeoHashGridParams.FIELD_SIZE)) {
-                otherOptions.put(GeoHashGridParams.FIELD_SIZE, parser.intValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, GeoHashGridParams.FIELD_SHARD_SIZE)) {
-                otherOptions.put(GeoHashGridParams.FIELD_SHARD_SIZE, parser.intValue());
-                return true;
-        }
-        }
-        return false;
-        }
-
-    public static class GeoGridAggregatorBuilder extends ValuesSourceAggregatorBuilder<ValuesSource.GeoPoint, GeoGridAggregatorBuilder> {
-
-        private int precision = DEFAULT_PRECISION;
-        private int requiredSize = DEFAULT_MAX_NUM_CELLS;
-        private int shardSize = -1;
-
-        public GeoGridAggregatorBuilder(String name) {
-            super(name, InternalGeoHashGrid.TYPE, ValuesSourceType.GEOPOINT, ValueType.GEOPOINT);
-    }
-
-        public GeoGridAggregatorBuilder precision(int precision) {
-            this.precision = GeoHashGridParams.checkPrecision(precision);
-            return this;
+            } else if (token != XContentParser.Token.START_OBJECT) {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
         }
 
-        public int precision() {
-            return precision;
+        if (shardSize == 0) {
+            shardSize = Integer.MAX_VALUE;
         }
 
-        public GeoGridAggregatorBuilder size(int size) {
-            this.requiredSize = size;
-            return this;
+        if (requiredSize == 0) {
+            requiredSize = Integer.MAX_VALUE;
         }
 
-        public int size() {
-            return requiredSize;
+        if (shardSize < 0) {
+            //Use default heuristic to avoid any wrong-ranking caused by distributed counting
+            shardSize = BucketUtils.suggestShardSideQueueSize(requiredSize, context.numberOfShards());
         }
 
-        public GeoGridAggregatorBuilder shardSize(int shardSize) {
-            this.shardSize = shardSize;
-            return this;
+        if (shardSize < requiredSize) {
+            shardSize = requiredSize;
         }
 
-        public int shardSize() {
-            return shardSize;
-        }
+        return new GeoGridFactory(aggregationName, vsParser.config(), precision, requiredSize, shardSize);
 
-        @Override
-        protected ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, ?> innerBuild(AggregationContext context,
-                ValuesSourceConfig<ValuesSource.GeoPoint> config, AggregatorFactory<?> parent, Builder subFactoriesBuilder)
-                        throws IOException {
-            int shardSize = this.shardSize;
-            if (shardSize == 0) {
-                shardSize = Integer.MAX_VALUE;
-            }
-
-            int requiredSize = this.requiredSize;
-            if (requiredSize == 0) {
-                requiredSize = Integer.MAX_VALUE;
-            }
+    }
 
-            if (shardSize < 0) {
-                // Use default heuristic to avoid any wrong-ranking caused by distributed counting
-                shardSize = BucketUtils.suggestShardSideQueueSize(requiredSize, context.searchContext().numberOfShards());
-            }
 
-            if (shardSize < requiredSize) {
-                shardSize = requiredSize;
-            }
-            return new GeoHashGridAggregatorFactory(name, type, config, precision, requiredSize, shardSize, context, parent,
-                    subFactoriesBuilder, metaData);
-        }
+    static class GeoGridFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint> {
 
-        @Override
-        protected GeoGridAggregatorBuilder innerReadFrom(
-                String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            GeoGridAggregatorBuilder factory = new GeoGridAggregatorBuilder(name);
-            factory.precision = in.readVInt();
-            factory.requiredSize = in.readVInt();
-            factory.shardSize = in.readVInt();
-            return factory;
-        }
+        private final int precision;
+        private final int requiredSize;
+        private final int shardSize;
 
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(precision);
-            out.writeVInt(requiredSize);
-            out.writeVInt(shardSize);
+        public GeoGridFactory(String name, ValuesSourceConfig<ValuesSource.GeoPoint> config, int precision, int requiredSize, int shardSize) {
+            super(name, InternalGeoHashGrid.TYPE.name(), config);
+            this.precision = precision;
+            this.requiredSize = requiredSize;
+            this.shardSize = shardSize;
         }
 
         @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(GeoHashGridParams.FIELD_PRECISION.getPreferredName(), precision);
-            builder.field(GeoHashGridParams.FIELD_SIZE.getPreferredName(), requiredSize);
-            builder.field(GeoHashGridParams.FIELD_SHARD_SIZE.getPreferredName(), shardSize);
-            return builder;
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            final InternalAggregation aggregation = new InternalGeoHashGrid(name, requiredSize,
+                    Collections.<InternalGeoHashGrid.Bucket> emptyList(), pipelineAggregators, metaData);
+            return new NonCollectingAggregator(name, aggregationContext, parent, pipelineAggregators, metaData) {
+                public InternalAggregation buildEmptyAggregation() {
+                    return aggregation;
+                }
+            };
         }
 
         @Override
-        protected boolean innerEquals(Object obj) {
-            GeoGridAggregatorBuilder other = (GeoGridAggregatorBuilder) obj;
-            if (precision != other.precision) {
-                return false;
-            }
-            if (requiredSize != other.requiredSize) {
-                return false;
-            }
-            if (shardSize != other.shardSize) {
-                return false;
+        protected Aggregator doCreateInternal(final ValuesSource.GeoPoint valuesSource, AggregationContext aggregationContext,
+                Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            if (collectsFromSingleBucket == false) {
+                return asMultiBucketAggregator(this, aggregationContext, parent);
             }
-            return true;
-        }
+            CellIdSource cellIdSource = new CellIdSource(valuesSource, precision);
+            return new GeoHashGridAggregator(name, factories, cellIdSource, requiredSize, shardSize, aggregationContext, parent, pipelineAggregators,
+                    metaData);
 
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(precision, requiredSize, shardSize);
         }
 
         private static class CellValues extends SortingNumericDocValues {
@@ -278,4 +207,4 @@ public class GeoHashGridParser extends GeoPointValuesSourceParser {
 
         }
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java
index 42b9bcc..63f47e1 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java
@@ -19,16 +19,13 @@
 package org.elasticsearch.search.aggregations.bucket.global;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationExecutionException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
@@ -71,43 +68,23 @@ public class GlobalAggregator extends SingleBucketAggregator {
         throw new UnsupportedOperationException("global aggregations cannot serve as sub-aggregations, hence should never be called on #buildEmptyAggregations");
     }
 
-    public static class GlobalAggregatorBuilder extends AggregatorBuilder<GlobalAggregatorBuilder> {
+    public static class Factory extends AggregatorFactory {
 
-        public GlobalAggregatorBuilder(String name) {
-            super(name, InternalGlobal.TYPE);
+        public Factory(String name) {
+            super(name, InternalGlobal.TYPE.name());
         }
 
         @Override
-        protected AggregatorFactory<?> doBuild(AggregationContext context, AggregatorFactory<?> parent, Builder subFactoriesBuilder)
-                throws IOException {
-            return new GlobalAggregatorFactory(name, type, context, parent, subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected GlobalAggregatorBuilder doReadFrom(String name, StreamInput in) throws IOException {
-            return new GlobalAggregatorBuilder(name);
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            // Nothing to write
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            return true;
-        }
-
-        @Override
-        protected int doHashCode() {
-            return 0;
+        public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            if (parent != null) {
+                throw new AggregationExecutionException("Aggregation [" + parent.name() + "] cannot have a global " +
+                        "sub-aggregation [" + name + "]. Global aggregations can only be defined as top level aggregations");
+            }
+            if (collectsFromSingleBucket == false) {
+                throw new IllegalStateException();
+            }
+            return new GlobalAggregator(name, factories, context, pipelineAggregators, metaData);
         }
 
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregatorFactory.java
deleted file mode 100644
index 1f54055..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregatorFactory.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.global;
-
-import org.elasticsearch.search.aggregations.AggregationExecutionException;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class GlobalAggregatorFactory extends AggregatorFactory<GlobalAggregatorFactory> {
-
-    public GlobalAggregatorFactory(String name, Type type, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactories, Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactories, metaData);
-    }
-
-    @Override
-    public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        if (parent != null) {
-            throw new AggregationExecutionException("Aggregation [" + parent.name() + "] cannot have a global " + "sub-aggregation [" + name
-                    + "]. Global aggregations can only be defined as top level aggregations");
-        }
-        if (collectsFromSingleBucket == false) {
-            throw new IllegalStateException();
-        }
-        return new GlobalAggregator(name, factories, context, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalBuilder.java
new file mode 100644
index 0000000..3e9f2ba
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalBuilder.java
@@ -0,0 +1,43 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.global;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Global} aggregation.
+ */
+public class GlobalBuilder extends AggregationBuilder<GlobalBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public GlobalBuilder(String name) {
+        super(name, InternalGlobal.TYPE.name());
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        return builder.startObject().endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalParser.java
index cc33153..c70cf0f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalParser.java
@@ -19,9 +19,9 @@
 package org.elasticsearch.search.aggregations.bucket.global;
 
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -36,14 +36,9 @@ public class GlobalParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         parser.nextToken();
-        return new GlobalAggregator.GlobalAggregatorBuilder(aggregationName);
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new GlobalAggregator.GlobalAggregatorBuilder(null);
+        return new GlobalAggregator.Factory(aggregationName);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramAggregatorFactory.java
deleted file mode 100644
index 5c1dadf..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramAggregatorFactory.java
+++ /dev/null
@@ -1,112 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.histogram;
-
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.rounding.Rounding;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-public abstract class AbstractHistogramAggregatorFactory<AF extends AbstractHistogramAggregatorFactory<AF>>
-        extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, AF> {
-
-    protected final long interval;
-    protected final long offset;
-    protected final InternalOrder order;
-    protected final boolean keyed;
-    protected final long minDocCount;
-    protected final ExtendedBounds extendedBounds;
-    private final InternalHistogram.Factory<?> histogramFactory;
-
-    public AbstractHistogramAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, long interval, long offset,
-            InternalOrder order, boolean keyed, long minDocCount, ExtendedBounds extendedBounds,
-            InternalHistogram.Factory<?> histogramFactory, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.interval = interval;
-        this.offset = offset;
-        this.order = order;
-        this.keyed = keyed;
-        this.minDocCount = minDocCount;
-        this.extendedBounds = extendedBounds;
-        this.histogramFactory = histogramFactory;
-    }
-
-    public long minDocCount() {
-        return minDocCount;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        Rounding rounding = createRounding();
-        return new HistogramAggregator(name, factories, rounding, order, keyed, minDocCount, extendedBounds, null, config.formatter(),
-                histogramFactory, aggregationContext, parent, pipelineAggregators, metaData);
-    }
-
-    protected Rounding createRounding() {
-        if (interval < 1) {
-            throw new ParsingException(null, "[interval] must be 1 or greater for histogram aggregation [" + name() + "]: " + interval);
-        }
-
-        Rounding rounding = new Rounding.Interval(interval);
-        if (offset != 0) {
-            rounding = new Rounding.OffsetRounding(rounding, offset);
-        }
-        return rounding;
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        if (collectsFromSingleBucket == false) {
-            return asMultiBucketAggregator(this, aggregationContext, parent);
-        }
-        Rounding rounding = createRounding();
-        // we need to round the bounds given by the user and we have to do it
-        // for every aggregator we create
-        // as the rounding is not necessarily an idempotent operation.
-        // todo we need to think of a better structure to the factory/agtor
-        // code so we won't need to do that
-        ExtendedBounds roundedBounds = null;
-        if (extendedBounds != null) {
-            // we need to process & validate here using the parser
-            extendedBounds.processAndValidate(name, aggregationContext.searchContext(), config.parser());
-            roundedBounds = extendedBounds.round(rounding);
-        }
-        return new HistogramAggregator(name, factories, rounding, order, keyed, minDocCount, roundedBounds, valuesSource,
-                config.formatter(), histogramFactory, aggregationContext, parent, pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregatorFactory.java
deleted file mode 100644
index 949542a..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregatorFactory.java
+++ /dev/null
@@ -1,97 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.histogram;
-
-import org.elasticsearch.common.rounding.DateTimeUnit;
-import org.elasticsearch.common.rounding.Rounding;
-import org.elasticsearch.common.rounding.TimeZoneRounding;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static java.util.Collections.unmodifiableMap;
-
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-public class DateHistogramAggregatorFactory extends AbstractHistogramAggregatorFactory<DateHistogramAggregatorFactory> {
-    public static final Map<String, DateTimeUnit> DATE_FIELD_UNITS;
-    private final DateHistogramInterval dateHistogramInterval;
-
-    static {
-        Map<String, DateTimeUnit> dateFieldUnits = new HashMap<>();
-        dateFieldUnits.put("year", DateTimeUnit.YEAR_OF_CENTURY);
-        dateFieldUnits.put("1y", DateTimeUnit.YEAR_OF_CENTURY);
-        dateFieldUnits.put("quarter", DateTimeUnit.QUARTER);
-        dateFieldUnits.put("1q", DateTimeUnit.QUARTER);
-        dateFieldUnits.put("month", DateTimeUnit.MONTH_OF_YEAR);
-        dateFieldUnits.put("1M", DateTimeUnit.MONTH_OF_YEAR);
-        dateFieldUnits.put("week", DateTimeUnit.WEEK_OF_WEEKYEAR);
-        dateFieldUnits.put("1w", DateTimeUnit.WEEK_OF_WEEKYEAR);
-        dateFieldUnits.put("day", DateTimeUnit.DAY_OF_MONTH);
-        dateFieldUnits.put("1d", DateTimeUnit.DAY_OF_MONTH);
-        dateFieldUnits.put("hour", DateTimeUnit.HOUR_OF_DAY);
-        dateFieldUnits.put("1h", DateTimeUnit.HOUR_OF_DAY);
-        dateFieldUnits.put("minute", DateTimeUnit.MINUTES_OF_HOUR);
-        dateFieldUnits.put("1m", DateTimeUnit.MINUTES_OF_HOUR);
-        dateFieldUnits.put("second", DateTimeUnit.SECOND_OF_MINUTE);
-        dateFieldUnits.put("1s", DateTimeUnit.SECOND_OF_MINUTE);
-        DATE_FIELD_UNITS = unmodifiableMap(dateFieldUnits);
-    }
-
-    public DateHistogramAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, long interval,
-            DateHistogramInterval dateHistogramInterval, long offset, InternalOrder order, boolean keyed, long minDocCount,
-            ExtendedBounds extendedBounds, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, interval, offset, order, keyed, minDocCount, extendedBounds, InternalDateHistogram.HISTOGRAM_FACTORY,
-                context, parent, subFactoriesBuilder, metaData);
-        this.dateHistogramInterval = dateHistogramInterval;
-    }
-
-    @Override
-    protected Rounding createRounding() {
-        TimeZoneRounding.Builder tzRoundingBuilder;
-        if (dateHistogramInterval != null) {
-            DateTimeUnit dateTimeUnit = DATE_FIELD_UNITS.get(dateHistogramInterval.toString());
-            if (dateTimeUnit != null) {
-                tzRoundingBuilder = TimeZoneRounding.builder(dateTimeUnit);
-            } else {
-                // the interval is a time value?
-                tzRoundingBuilder = TimeZoneRounding.builder(
-                        TimeValue.parseTimeValue(dateHistogramInterval.toString(), null, getClass().getSimpleName() + ".interval"));
-            }
-        } else {
-            // the interval is an integer time value in millis?
-            tzRoundingBuilder = TimeZoneRounding.builder(TimeValue.timeValueMillis(interval));
-        }
-        if (timeZone() != null) {
-            tzRoundingBuilder.timeZone(timeZone());
-        }
-        Rounding rounding = tzRoundingBuilder.offset(offset).build();
-        return rounding;
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java
new file mode 100644
index 0000000..67caf37
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java
@@ -0,0 +1,186 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.histogram;
+
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+import org.joda.time.DateTime;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@code DateHistogram} aggregation.
+ */
+public class DateHistogramBuilder extends ValuesSourceAggregationBuilder<DateHistogramBuilder> {
+
+    private Object interval;
+    private Histogram.Order order;
+    private Long minDocCount;
+    private Object extendedBoundsMin;
+    private Object extendedBoundsMax;
+    private String timeZone;
+    private String format;
+    private String offset;
+
+    /**
+     * Sole constructor.
+     */
+    public DateHistogramBuilder(String name) {
+        super(name, InternalDateHistogram.TYPE.name());
+    }
+
+    /**
+     * Set the interval in milliseconds.
+     */
+    public DateHistogramBuilder interval(long interval) {
+        this.interval = interval;
+        return this;
+    }
+
+    /**
+     * Set the interval.
+     */
+    public DateHistogramBuilder interval(DateHistogramInterval interval) {
+        this.interval = interval;
+        return this;
+    }
+
+    /**
+     * Set the order by which the buckets will be returned.
+     */
+    public DateHistogramBuilder order(Histogram.Order order) {
+        this.order = order;
+        return this;
+    }
+
+    /**
+     * Set the minimum document count per bucket. Buckets with less documents
+     * than this min value will not be returned.
+     */
+    public DateHistogramBuilder minDocCount(long minDocCount) {
+        this.minDocCount = minDocCount;
+        return this;
+    }
+
+    /**
+     * Set the timezone in which to translate dates before computing buckets.
+     */
+    public DateHistogramBuilder timeZone(String timeZone) {
+        this.timeZone = timeZone;
+        return this;
+    }
+
+    /**
+     * @param offset sets the offset of time intervals in this histogram
+     * @return the current builder
+     */
+    public DateHistogramBuilder offset(String offset) {
+       this.offset = offset;
+       return this;
+    }
+
+    /**
+     * Set the format to use for dates.
+     */
+    public DateHistogramBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    /**
+     * Set extended bounds for the histogram. In case the lower value in the
+     * histogram would be greater than <code>min</code> or the upper value would
+     * be less than <code>max</code>, empty buckets will be generated.
+     */
+    public DateHistogramBuilder extendedBounds(Long min, Long max) {
+        extendedBoundsMin = min;
+        extendedBoundsMax = max;
+        return this;
+    }
+
+    /**
+     * Set extended bounds for the histogram. In case the lower value in the
+     * histogram would be greater than <code>min</code> or the upper value would
+     * be less than <code>max</code>, empty buckets will be generated.
+     */
+    public DateHistogramBuilder extendedBounds(String min, String max) {
+        extendedBoundsMin = min;
+        extendedBoundsMax = max;
+        return this;
+    }
+
+    /**
+     * Set extended bounds for the histogram. In case the lower value in the
+     * histogram would be greater than <code>min</code> or the upper value would
+     * be less than <code>max</code>, empty buckets will be generated.
+     */
+    public DateHistogramBuilder extendedBounds(DateTime min, DateTime max) {
+        extendedBoundsMin = min;
+        extendedBoundsMax = max;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (interval == null) {
+            throw new SearchSourceBuilderException("[interval] must be defined for histogram aggregation [" + getName() + "]");
+        }
+        if (interval instanceof Number) {
+            interval = TimeValue.timeValueMillis(((Number) interval).longValue()).toString();
+        }
+        builder.field("interval", interval);
+
+        if (minDocCount != null) {
+            builder.field("min_doc_count", minDocCount);
+        }
+
+        if (order != null) {
+            builder.field("order");
+            order.toXContent(builder, params);
+        }
+
+        if (timeZone != null) {
+            builder.field("time_zone", timeZone);
+        }
+
+        if (offset != null) {
+            builder.field("offset", offset);
+        }
+
+        if (format != null) {
+            builder.field("format", format);
+        }
+
+        if (extendedBoundsMin != null || extendedBoundsMax != null) {
+            builder.startObject(DateHistogramParser.EXTENDED_BOUNDS.getPreferredName());
+            if (extendedBoundsMin != null) {
+                builder.field("min", extendedBoundsMin);
+            }
+            if (extendedBoundsMax != null) {
+                builder.field("max", extendedBoundsMax);
+            }
+            builder.endObject();
+        }
+
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java
index cd2c854..7e99b38 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java
@@ -19,17 +19,10 @@
 
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
-import java.util.Objects;
-
 /**
  * The interval the date histogram is based on.
  */
-public class DateHistogramInterval implements Writeable<DateHistogramInterval> {
+public class DateHistogramInterval {
 
     public static final DateHistogramInterval SECOND = new DateHistogramInterval("1s");
     public static final DateHistogramInterval MINUTE = new DateHistogramInterval("1m");
@@ -40,10 +33,6 @@ public class DateHistogramInterval implements Writeable<DateHistogramInterval> {
     public static final DateHistogramInterval QUARTER = new DateHistogramInterval("1q");
     public static final DateHistogramInterval YEAR = new DateHistogramInterval("1y");
 
-    public static final DateHistogramInterval readFromStream(StreamInput in) throws IOException {
-        return SECOND.readFrom(in);
-    }
-
     public static DateHistogramInterval seconds(int sec) {
         return new DateHistogramInterval(sec + "s");
     }
@@ -74,31 +63,4 @@ public class DateHistogramInterval implements Writeable<DateHistogramInterval> {
     public String toString() {
         return expression;
     }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(expression);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        DateHistogramInterval other = (DateHistogramInterval) obj;
-        return Objects.equals(expression, other.expression);
-    }
-
-    @Override
-    public DateHistogramInterval readFrom(StreamInput in) throws IOException {
-        return new DateHistogramInterval(in.readString());
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(expression);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
index cbd2e4f..694abf2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
@@ -19,23 +19,55 @@
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.rounding.DateTimeUnit;
 import org.elasticsearch.common.rounding.Rounding;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.DateHistogramAggregatorBuilder;
+import org.elasticsearch.common.rounding.TimeZoneRounding;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
+import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  *
  */
-public class DateHistogramParser extends HistogramParser {
+public class DateHistogramParser implements Aggregator.Parser {
+
+    static final ParseField EXTENDED_BOUNDS = new ParseField("extended_bounds");
+    static final ParseField OFFSET = new ParseField("offset");
+    static final ParseField INTERVAL = new ParseField("interval");
+
+    public static final Map<String, DateTimeUnit> DATE_FIELD_UNITS;
 
-    public DateHistogramParser() {
-        super(true);
+    static {
+        Map<String, DateTimeUnit> dateFieldUnits = new HashMap<>();
+        dateFieldUnits.put("year", DateTimeUnit.YEAR_OF_CENTURY);
+        dateFieldUnits.put("1y", DateTimeUnit.YEAR_OF_CENTURY);
+        dateFieldUnits.put("quarter", DateTimeUnit.QUARTER);
+        dateFieldUnits.put("1q", DateTimeUnit.QUARTER);
+        dateFieldUnits.put("month", DateTimeUnit.MONTH_OF_YEAR);
+        dateFieldUnits.put("1M", DateTimeUnit.MONTH_OF_YEAR);
+        dateFieldUnits.put("week", DateTimeUnit.WEEK_OF_WEEKYEAR);
+        dateFieldUnits.put("1w", DateTimeUnit.WEEK_OF_WEEKYEAR);
+        dateFieldUnits.put("day", DateTimeUnit.DAY_OF_MONTH);
+        dateFieldUnits.put("1d", DateTimeUnit.DAY_OF_MONTH);
+        dateFieldUnits.put("hour", DateTimeUnit.HOUR_OF_DAY);
+        dateFieldUnits.put("1h", DateTimeUnit.HOUR_OF_DAY);
+        dateFieldUnits.put("minute", DateTimeUnit.MINUTES_OF_HOUR);
+        dateFieldUnits.put("1m", DateTimeUnit.MINUTES_OF_HOUR);
+        dateFieldUnits.put("second", DateTimeUnit.SECOND_OF_MINUTE);
+        dateFieldUnits.put("1s", DateTimeUnit.SECOND_OF_MINUTE);
+        DATE_FIELD_UNITS = unmodifiableMap(dateFieldUnits);
     }
 
     @Override
@@ -44,47 +76,127 @@ public class DateHistogramParser extends HistogramParser {
     }
 
     @Override
-    protected Object parseStringInterval(String text) {
-        return new DateHistogramInterval(text);
-    }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-    @Override
-    protected DateHistogramAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        HistogramAggregator.DateHistogramAggregatorBuilder factory = new HistogramAggregator.DateHistogramAggregatorBuilder(aggregationName);
-        Object interval = otherOptions.get(Rounding.Interval.INTERVAL_FIELD);
-        if (interval == null) {
-            throw new ParsingException(null, "Missing required field [interval] for histogram aggregation [" + aggregationName + "]");
-        } else if (interval instanceof Long) {
-            factory.interval((Long) interval);
-        } else if (interval instanceof DateHistogramInterval) {
-            factory.dateHistogramInterval((DateHistogramInterval) interval);
-        }
-        Long offset = (Long) otherOptions.get(Rounding.OffsetRounding.OFFSET_FIELD);
-        if (offset != null) {
-            factory.offset(offset);
-        }
+        ValuesSourceParser vsParser = ValuesSourceParser.numeric(aggregationName, InternalDateHistogram.TYPE, context)
+                .targetValueType(ValueType.DATE)
+                .formattable(true)
+                .timezoneAware(true)
+                .build();
 
-        ExtendedBounds extendedBounds = (ExtendedBounds) otherOptions.get(ExtendedBounds.EXTENDED_BOUNDS_FIELD);
-        if (extendedBounds != null) {
-            factory.extendedBounds(extendedBounds);
-        }
-        Boolean keyed = (Boolean) otherOptions.get(HistogramAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
+        boolean keyed = false;
+        long minDocCount = 0;
+        ExtendedBounds extendedBounds = null;
+        InternalOrder order = (InternalOrder) Histogram.Order.KEY_ASC;
+        String interval = null;
+        long offset = 0;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_STRING) {
+                if (context.parseFieldMatcher().match(currentFieldName, OFFSET)) {
+                    offset = parseOffset(parser.text());
+                } else if (context.parseFieldMatcher().match(currentFieldName, INTERVAL)) {
+                    interval = parser.text();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                if ("min_doc_count".equals(currentFieldName) || "minDocCount".equals(currentFieldName)) {
+                    minDocCount = parser.longValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.START_OBJECT) {
+                if ("order".equals(currentFieldName)) {
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            currentFieldName = parser.currentName();
+                        } else if (token == XContentParser.Token.VALUE_STRING) {
+                            String dir = parser.text();
+                            boolean asc = "asc".equals(dir);
+                            order = resolveOrder(currentFieldName, asc);
+                            //TODO should we throw an error if the value is not "asc" or "desc"???
+                        }
+                    }
+                } else if (context.parseFieldMatcher().match(currentFieldName, EXTENDED_BOUNDS)) {
+                    extendedBounds = new ExtendedBounds();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            currentFieldName = parser.currentName();
+                        } else if (token == XContentParser.Token.VALUE_STRING) {
+                            if ("min".equals(currentFieldName)) {
+                                extendedBounds.minAsStr = parser.text();
+                            } else if ("max".equals(currentFieldName)) {
+                                extendedBounds.maxAsStr = parser.text();
+                            } else {
+                                throw new SearchParseException(context, "Unknown extended_bounds key for a " + token + " in aggregation ["
+                                        + aggregationName + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                            }
+                        } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                            if ("min".equals(currentFieldName)) {
+                                extendedBounds.min = parser.longValue();
+                            } else if ("max".equals(currentFieldName)) {
+                                extendedBounds.max = parser.longValue();
+                            } else {
+                                throw new SearchParseException(context, "Unknown extended_bounds key for a " + token + " in aggregation ["
+                                        + aggregationName + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                            }
+                        } else {
+                            throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                    + currentFieldName + "].", parser.getTokenLocation());
+                        }
+                    }
+
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
         }
-        Long minDocCount = (Long) otherOptions.get(HistogramAggregator.MIN_DOC_COUNT_FIELD);
-        if (minDocCount != null) {
-            factory.minDocCount(minDocCount);
+
+        if (interval == null) {
+            throw new SearchParseException(context,
+                    "Missing required field [interval] for histogram aggregation [" + aggregationName + "]", parser.getTokenLocation());
         }
-        InternalOrder order = (InternalOrder) otherOptions.get(HistogramAggregator.ORDER_FIELD);
-        if (order != null) {
-            factory.order(order);
+
+        TimeZoneRounding.Builder tzRoundingBuilder;
+        DateTimeUnit dateTimeUnit = DATE_FIELD_UNITS.get(interval);
+        if (dateTimeUnit != null) {
+            tzRoundingBuilder = TimeZoneRounding.builder(dateTimeUnit);
+        } else {
+            // the interval is a time value?
+            tzRoundingBuilder = TimeZoneRounding.builder(TimeValue.parseTimeValue(interval, null, getClass().getSimpleName() + ".interval"));
         }
-        return factory;
+
+        Rounding rounding = tzRoundingBuilder
+                .timeZone(vsParser.input().timezone())
+                .offset(offset).build();
+
+        ValuesSourceConfig config = vsParser.config();
+        return new HistogramAggregator.Factory(aggregationName, config, rounding, order, keyed, minDocCount, extendedBounds,
+                new InternalDateHistogram.Factory());
+
     }
 
-    static InternalOrder resolveOrder(String key, boolean asc) {
+    private static InternalOrder resolveOrder(String key, boolean asc) {
         if ("_key".equals(key) || "_time".equals(key)) {
             return (InternalOrder) (asc ? InternalOrder.KEY_ASC : InternalOrder.KEY_DESC);
         }
@@ -94,13 +206,11 @@ public class DateHistogramParser extends HistogramParser {
         return new InternalOrder.Aggregation(key, asc);
     }
 
-    @Override
-    protected long parseStringOffset(String offset) throws IOException {
-        return DateHistogramAggregatorBuilder.parseStringOffset(offset);
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return HistogramAggregator.DateHistogramAggregatorBuilder.PROTOTYPE;
+    private long parseOffset(String offset) throws IOException {
+        if (offset.charAt(0) == '-') {
+            return -TimeValue.parseTimeValue(offset.substring(1), null, getClass().getSimpleName() + ".parseOffset").millis();
+        }
+        int beginIndex = offset.charAt(0) == '+' ? 1 : 0;
+        return TimeValue.parseTimeValue(offset.substring(beginIndex), null, getClass().getSimpleName() + ".parseOffset").millis();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java
index 91a3d23..c703058 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java
@@ -19,32 +19,19 @@
 
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.rounding.Rounding;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.support.format.ValueParser;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  *
  */
-public class ExtendedBounds implements ToXContent {
-
-    static final ParseField EXTENDED_BOUNDS_FIELD = new ParseField("extended_bounds");
-    static final ParseField MIN_FIELD = new ParseField("min");
-    static final ParseField MAX_FIELD = new ParseField("max");
-
-    private static final ExtendedBounds PROTOTYPE = new ExtendedBounds();
+public class ExtendedBounds {
 
     Long min;
     Long max;
@@ -54,16 +41,11 @@ public class ExtendedBounds implements ToXContent {
 
     ExtendedBounds() {} //for serialization
 
-    public ExtendedBounds(Long min, Long max) {
+    ExtendedBounds(Long min, Long max) {
         this.min = min;
         this.max = max;
     }
 
-    public ExtendedBounds(String minAsStr, String maxAsStr) {
-        this.minAsStr = minAsStr;
-        this.maxAsStr = maxAsStr;
-    }
-
     void processAndValidate(String aggName, SearchContext context, ValueParser parser) {
         assert parser != null;
         if (minAsStr != null) {
@@ -95,8 +77,6 @@ public class ExtendedBounds implements ToXContent {
         } else {
             out.writeBoolean(false);
         }
-        out.writeOptionalString(minAsStr);
-        out.writeOptionalString(maxAsStr);
     }
 
     static ExtendedBounds readFrom(StreamInput in) throws IOException {
@@ -107,79 +87,6 @@ public class ExtendedBounds implements ToXContent {
         if (in.readBoolean()) {
             bounds.max = in.readLong();
         }
-        bounds.minAsStr = in.readOptionalString();
-        bounds.maxAsStr = in.readOptionalString();
         return bounds;
     }
-
-    public ExtendedBounds fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher, String aggregationName)
-            throws IOException {
-        XContentParser.Token token = null;
-        String currentFieldName = null;
-        ExtendedBounds extendedBounds = new ExtendedBounds();
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token == XContentParser.Token.VALUE_STRING) {
-                if ("min".equals(currentFieldName)) {
-                    extendedBounds.minAsStr = parser.text();
-                } else if ("max".equals(currentFieldName)) {
-                    extendedBounds.maxAsStr = parser.text();
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown extended_bounds key for a " + token
-                            + " in aggregation [" + aggregationName + "]: [" + currentFieldName + "].");
-                }
-            } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                if (parseFieldMatcher.match(currentFieldName, MIN_FIELD)) {
-                    extendedBounds.min = parser.longValue(true);
-                } else if (parseFieldMatcher.match(currentFieldName, MAX_FIELD)) {
-                    extendedBounds.max = parser.longValue(true);
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown extended_bounds key for a " + token
-                            + " in aggregation [" + aggregationName + "]: [" + currentFieldName + "].");
-                }
-            }
-        }
-        return extendedBounds;
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(EXTENDED_BOUNDS_FIELD.getPreferredName());
-        if (min != null) {
-            builder.field(MIN_FIELD.getPreferredName(), min);
-        } else {
-            builder.field(MIN_FIELD.getPreferredName(), minAsStr);
-        }
-        if (max != null) {
-            builder.field(MAX_FIELD.getPreferredName(), max);
-        } else {
-            builder.field(MAX_FIELD.getPreferredName(), maxAsStr);
-        }
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(min, max);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        ExtendedBounds other = (ExtendedBounds) obj;
-        return Objects.equals(min, other.min)
-                && Objects.equals(min, other.min);
-    }
-
-    public static ExtendedBounds parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, String aggregationName)
-            throws IOException {
-        return PROTOTYPE.fromXContent(parser, parseFieldMatcher, aggregationName);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java
index 6c81952..d2ca0a9 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java
@@ -21,31 +21,21 @@ package org.elasticsearch.search.aggregations.bucket.histogram;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.SortedNumericDocValues;
 import org.apache.lucene.util.CollectionUtil;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.internal.Nullable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.rounding.Rounding;
-import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.LongHash;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -53,14 +43,9 @@ import java.util.ArrayList;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 public class HistogramAggregator extends BucketsAggregator {
 
-    public static final ParseField ORDER_FIELD = new ParseField("order");
-    public static final ParseField KEYED_FIELD = new ParseField("keyed");
-    public static final ParseField MIN_DOC_COUNT_FIELD = new ParseField("min_doc_count");
-
     private final ValuesSource.Numeric valuesSource;
     private final ValueFormatter formatter;
     private final Rounding rounding;
@@ -159,274 +144,57 @@ public class HistogramAggregator extends BucketsAggregator {
         Releasables.close(bucketOrds);
     }
 
-    public static class HistogramAggregatorBuilder extends AbstractBuilder<HistogramAggregatorBuilder> {
-        public static final HistogramAggregatorBuilder PROTOTYPE = new HistogramAggregatorBuilder("");
-
-        public HistogramAggregatorBuilder(String name) {
-            super(name, InternalHistogram.HISTOGRAM_FACTORY);
-        }
-
-        @Override
-        protected HistogramAggregatorBuilder createFactoryFromStream(String name, StreamInput in) throws IOException {
-            return new HistogramAggregatorBuilder(name);
-        }
-
-        @Override
-        protected HistogramAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new HistogramAggregatorFactory(name, type, config, interval, offset, order, keyed, minDocCount, extendedBounds, context,
-                    parent, subFactoriesBuilder, metaData);
-        }
-
-    }
-
-    public static abstract class AbstractBuilder<AB extends AbstractBuilder<AB>>
-            extends ValuesSourceAggregatorBuilder<ValuesSource.Numeric, AB> {
-
-        protected long interval;
-        protected long offset = 0;
-        protected InternalOrder order = (InternalOrder) Histogram.Order.KEY_ASC;
-        protected boolean keyed = false;
-        protected long minDocCount = 0;
-        protected ExtendedBounds extendedBounds;
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric> {
 
-        private AbstractBuilder(String name, InternalHistogram.Factory<?> histogramFactory) {
-            super(name, histogramFactory.type(), ValuesSourceType.NUMERIC, histogramFactory.valueType());
-        }
-
-        public long interval() {
-            return interval;
-        }
-
-        public AB interval(long interval) {
-            this.interval = interval;
-            return (AB) this;
-        }
-
-        public long offset() {
-            return offset;
-        }
-
-        public AB offset(long offset) {
-            this.offset = offset;
-            return (AB) this;
-        }
-
-        public Histogram.Order order() {
-            return order;
-        }
-
-        public AB order(Histogram.Order order) {
-            this.order = (InternalOrder) order;
-            return (AB) this;
-        }
+        private final Rounding rounding;
+        private final InternalOrder order;
+        private final boolean keyed;
+        private final long minDocCount;
+        private final ExtendedBounds extendedBounds;
+        private final InternalHistogram.Factory<?> histogramFactory;
 
-        public boolean keyed() {
-            return keyed;
-        }
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> config,
+                       Rounding rounding, InternalOrder order, boolean keyed, long minDocCount,
+                       ExtendedBounds extendedBounds, InternalHistogram.Factory<?> histogramFactory) {
 
-        public AB keyed(boolean keyed) {
+            super(name, histogramFactory.type(), config);
+            this.rounding = rounding;
+            this.order = order;
             this.keyed = keyed;
-            return (AB) this;
-        }
-
-        public long minDocCount() {
-            return minDocCount;
-        }
-
-        public AB minDocCount(long minDocCount) {
             this.minDocCount = minDocCount;
-            return (AB) this;
-        }
-
-        public ExtendedBounds extendedBounds() {
-            return extendedBounds;
-        }
-
-        public AB extendedBounds(ExtendedBounds extendedBounds) {
             this.extendedBounds = extendedBounds;
-            return (AB) this;
-        }
-
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-
-            builder.field(Rounding.Interval.INTERVAL_FIELD.getPreferredName());
-            doXContentInterval(builder, params);
-            builder.field(Rounding.OffsetRounding.OFFSET_FIELD.getPreferredName(), offset);
-
-            if (order != null) {
-                builder.field(ORDER_FIELD.getPreferredName());
-                order.toXContent(builder, params);
-            }
-
-            builder.field(KEYED_FIELD.getPreferredName(), keyed);
-
-            builder.field(MIN_DOC_COUNT_FIELD.getPreferredName(), minDocCount);
-
-            if (extendedBounds != null) {
-                extendedBounds.toXContent(builder, params);
-            }
-
-            return builder;
-        }
-
-        protected XContentBuilder doXContentInterval(XContentBuilder builder, Params params) throws IOException {
-            builder.value(interval);
-            return builder;
-        }
-
-        @Override
-        public String getWriteableName() {
-            return InternalHistogram.TYPE.name();
-        }
-
-        @Override
-        protected AB innerReadFrom(String name, ValuesSourceType valuesSourceType, ValueType targetValueType, StreamInput in)
-                throws IOException {
-            AbstractBuilder<AB> factory = createFactoryFromStream(name, in);
-            factory.interval = in.readVLong();
-            factory.offset = in.readVLong();
-            if (in.readBoolean()) {
-                factory.order = InternalOrder.Streams.readOrder(in);
-            }
-            factory.keyed = in.readBoolean();
-            factory.minDocCount = in.readVLong();
-            if (in.readBoolean()) {
-                factory.extendedBounds = ExtendedBounds.readFrom(in);
-            }
-            return (AB) factory;
-        }
-
-        protected abstract AB createFactoryFromStream(String name, StreamInput in) throws IOException;
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            writeFactoryToStream(out);
-            out.writeVLong(interval);
-            out.writeVLong(offset);
-            boolean hasOrder = order != null;
-            out.writeBoolean(hasOrder);
-            if (hasOrder) {
-                InternalOrder.Streams.writeOrder(order, out);
-            }
-            out.writeBoolean(keyed);
-            out.writeVLong(minDocCount);
-            boolean hasExtendedBounds = extendedBounds != null;
-            out.writeBoolean(hasExtendedBounds);
-            if (hasExtendedBounds) {
-                extendedBounds.writeTo(out);
-            }
-        }
-
-        protected void writeFactoryToStream(StreamOutput out) throws IOException {
-            // Default impl does nothing
-    }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(interval, offset, order, keyed, minDocCount, extendedBounds);
-    }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            AbstractBuilder other = (AbstractBuilder) obj;
-            return Objects.equals(interval, other.interval)
-                    && Objects.equals(offset, other.offset)
-                    && Objects.equals(order, other.order)
-                    && Objects.equals(keyed, other.keyed)
-                    && Objects.equals(minDocCount, other.minDocCount)
-                    && Objects.equals(extendedBounds, other.extendedBounds);
-        }
-    }
-
-    public static class DateHistogramAggregatorBuilder extends AbstractBuilder<DateHistogramAggregatorBuilder> {
-
-        public static final DateHistogramAggregatorBuilder PROTOTYPE = new DateHistogramAggregatorBuilder("");
-
-        private DateHistogramInterval dateHistogramInterval;
-
-        public DateHistogramAggregatorBuilder(String name) {
-            super(name, InternalDateHistogram.HISTOGRAM_FACTORY);
-        }
-
-        /**
-         * Set the interval.
-         */
-        public DateHistogramAggregatorBuilder dateHistogramInterval(DateHistogramInterval dateHistogramInterval) {
-            this.dateHistogramInterval = dateHistogramInterval;
-            return this;
-        }
-
-        public DateHistogramAggregatorBuilder offset(String offset) {
-            return offset(parseStringOffset(offset));
-        }
-
-        protected static long parseStringOffset(String offset) {
-            if (offset.charAt(0) == '-') {
-                return -TimeValue.parseTimeValue(offset.substring(1), null, DateHistogramAggregatorBuilder.class.getSimpleName() + ".parseOffset")
-                        .millis();
-            }
-            int beginIndex = offset.charAt(0) == '+' ? 1 : 0;
-            return TimeValue.parseTimeValue(offset.substring(beginIndex), null, DateHistogramAggregatorBuilder.class.getSimpleName() + ".parseOffset")
-                    .millis();
-        }
-
-        public DateHistogramInterval dateHistogramInterval() {
-            return dateHistogramInterval;
-        }
-
-        @Override
-        protected DateHistogramAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new DateHistogramAggregatorFactory(name, type, config, interval, dateHistogramInterval, offset, order, keyed,
-                    minDocCount, extendedBounds, context, parent, subFactoriesBuilder, metaData);
+            this.histogramFactory = histogramFactory;
         }
 
-        @Override
-        public String getWriteableName() {
-            return InternalDateHistogram.TYPE.name();
+        public long minDocCount() {
+            return minDocCount;
         }
 
         @Override
-        protected XContentBuilder doXContentInterval(XContentBuilder builder, Params params) throws IOException {
-            if (dateHistogramInterval == null) {
-                super.doXContentInterval(builder, params);
-            } else {
-                builder.value(dateHistogramInterval.toString());
-            }
-            return builder;
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            return new HistogramAggregator(name, factories, rounding, order, keyed, minDocCount, extendedBounds, null, config.formatter(),
+                    histogramFactory, aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected DateHistogramAggregatorBuilder createFactoryFromStream(String name, StreamInput in)
-                throws IOException {
-            DateHistogramAggregatorBuilder factory = new DateHistogramAggregatorBuilder(name);
-            if (in.readBoolean()) {
-                factory.dateHistogramInterval = DateHistogramInterval.readFromStream(in);
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            if (collectsFromSingleBucket == false) {
+                return asMultiBucketAggregator(this, aggregationContext, parent);
             }
-            return factory;
-        }
-
-        @Override
-        protected void writeFactoryToStream(StreamOutput out) throws IOException {
-            boolean hasDateInterval = dateHistogramInterval != null;
-            out.writeBoolean(hasDateInterval);
-            if (hasDateInterval) {
-                dateHistogramInterval.writeTo(out);
+            // we need to round the bounds given by the user and we have to do it for every aggregator we crate
+            // as the rounding is not necessarily an idempotent operation.
+            // todo we need to think of a better structure to the factory/agtor code so we won't need to do that
+            ExtendedBounds roundedBounds = null;
+            if (extendedBounds != null) {
+                // we need to process & validate here using the parser
+                extendedBounds.processAndValidate(name, aggregationContext.searchContext(), config.parser());
+                roundedBounds = extendedBounds.round(rounding);
             }
+            return new HistogramAggregator(name, factories, rounding, order, keyed, minDocCount, roundedBounds, valuesSource,
+                    config.formatter(), histogramFactory, aggregationContext, parent, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(super.innerHashCode(), dateHistogramInterval);
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            DateHistogramAggregatorBuilder other = (DateHistogramAggregatorBuilder) obj;
-            return super.innerEquals(obj)
-                    && Objects.equals(dateHistogramInterval, other.dateHistogramInterval);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregatorFactory.java
deleted file mode 100644
index a862aad..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregatorFactory.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.histogram;
-
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-
-import java.io.IOException;
-import java.util.Map;
-
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-public class HistogramAggregatorFactory extends AbstractHistogramAggregatorFactory<HistogramAggregatorFactory> {
-
-    public HistogramAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, long interval, long offset,
-            InternalOrder order, boolean keyed, long minDocCount, ExtendedBounds extendedBounds, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, interval, offset, order, keyed, minDocCount, extendedBounds, InternalHistogram.HISTOGRAM_FACTORY, context,
-                parent, subFactoriesBuilder, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBuilder.java
new file mode 100644
index 0000000..064e046
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBuilder.java
@@ -0,0 +1,134 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.histogram;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Histogram} aggregation.
+ */
+public class HistogramBuilder extends ValuesSourceAggregationBuilder<HistogramBuilder> {
+
+    private Long interval;
+    private Histogram.Order order;
+    private Long minDocCount;
+    private Long extendedBoundsMin;
+    private Long extendedBoundsMax;
+    private Long offset;
+
+    /**
+     * Constructs a new histogram aggregation builder.
+     *
+     * @param name  The name of the aggregation (will serve as the unique identifier for the aggregation result in the response)
+     */
+    public HistogramBuilder(String name) {
+        super(name, InternalHistogram.TYPE.name());
+    }
+
+    /**
+     * Sets the interval for the histogram.
+     *
+     * @param interval  The interval for the histogram
+     * @return          This builder
+     */
+    public HistogramBuilder interval(long interval) {
+        this.interval = interval;
+        return this;
+    }
+
+    /**
+     * Sets the order by which the buckets will be returned.
+     *
+     * @param order The order by which the buckets will be returned
+     * @return      This builder
+     */
+    public HistogramBuilder order(Histogram.Order order) {
+        this.order = order;
+        return this;
+    }
+
+    /**
+     * Sets the minimum document count per bucket. Buckets with less documents than this min value will not be returned.
+     *
+     * @param minDocCount   The minimum document count per bucket
+     * @return              This builder
+     */
+    public HistogramBuilder minDocCount(long minDocCount) {
+        this.minDocCount = minDocCount;
+        return this;
+    }
+
+    /**
+     * Set extended bounds for the histogram. In case the lower value in the
+     * histogram would be greater than <code>min</code> or the upper value would
+     * be less than <code>max</code>, empty buckets will be generated.
+     */
+    public HistogramBuilder extendedBounds(Long min, Long max) {
+        extendedBoundsMin = min;
+        extendedBoundsMax = max;
+        return this;
+    }
+
+    /**
+     * Set the offset to apply to shift bucket boundaries.
+     */
+    public HistogramBuilder offset(long offset) {
+        this.offset = offset;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (interval == null) {
+            throw new SearchSourceBuilderException("[interval] must be defined for histogram aggregation [" + getName() + "]");
+        }
+        builder.field("interval", interval);
+
+        if (order != null) {
+            builder.field("order");
+            order.toXContent(builder, params);
+        }
+
+        if (offset != null) {
+            builder.field("offset", offset);
+        }
+
+        if (minDocCount != null) {
+            builder.field("min_doc_count", minDocCount);
+        }
+
+        if (extendedBoundsMin != null || extendedBoundsMax != null) {
+            builder.startObject(HistogramParser.EXTENDED_BOUNDS.getPreferredName());
+            if (extendedBoundsMin != null) {
+                builder.field("min", extendedBoundsMin);
+            }
+            if (extendedBoundsMax != null) {
+                builder.field("max", extendedBoundsMax);
+            }
+            builder.endObject();
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java
index 83f00d9..c738251 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java
@@ -19,31 +19,24 @@
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.rounding.Rounding;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.aggregations.support.format.ValueParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  * Parses the histogram request
  */
-public class HistogramParser extends NumericValuesSourceParser {
+public class HistogramParser implements Aggregator.Parser {
 
-    public HistogramParser() {
-        super(true, true, false);
-    }
-
-    protected HistogramParser(boolean timezoneAware) {
-        super(true, true, timezoneAware);
-    }
+    static final ParseField EXTENDED_BOUNDS = new ParseField("extended_bounds");
 
     @Override
     public String type() {
@@ -51,105 +44,100 @@ public class HistogramParser extends NumericValuesSourceParser {
     }
 
     @Override
-    protected HistogramAggregator.AbstractBuilder<?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        HistogramAggregator.HistogramAggregatorBuilder factory = new HistogramAggregator.HistogramAggregatorBuilder(aggregationName);
-        Long interval = (Long) otherOptions.get(Rounding.Interval.INTERVAL_FIELD);
-        if (interval == null) {
-            throw new ParsingException(null, "Missing required field [interval] for histogram aggregation [" + aggregationName + "]");
-        } else {
-            factory.interval(interval);
-        }
-        Long offset = (Long) otherOptions.get(Rounding.OffsetRounding.OFFSET_FIELD);
-        if (offset != null) {
-            factory.offset(offset);
-        }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-        ExtendedBounds extendedBounds = (ExtendedBounds) otherOptions.get(ExtendedBounds.EXTENDED_BOUNDS_FIELD);
-        if (extendedBounds != null) {
-            factory.extendedBounds(extendedBounds);
-        }
-        Boolean keyed = (Boolean) otherOptions.get(HistogramAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        Long minDocCount = (Long) otherOptions.get(HistogramAggregator.MIN_DOC_COUNT_FIELD);
-        if (minDocCount != null) {
-            factory.minDocCount(minDocCount);
-        }
-        InternalOrder order = (InternalOrder) otherOptions.get(HistogramAggregator.ORDER_FIELD);
-        if (order != null) {
-            factory.order(order);
-        }
-        return factory;
-    }
+        ValuesSourceParser vsParser = ValuesSourceParser.numeric(aggregationName, InternalHistogram.TYPE, context)
+                .targetValueType(ValueType.NUMERIC)
+                .formattable(true)
+                .build();
 
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token.isValue()) {
-            if (parseFieldMatcher.match(currentFieldName, Rounding.Interval.INTERVAL_FIELD)) {
-                if (token == XContentParser.Token.VALUE_STRING) {
-                    otherOptions.put(Rounding.Interval.INTERVAL_FIELD, parseStringInterval(parser.text()));
-                    return true;
-                } else {
-                    otherOptions.put(Rounding.Interval.INTERVAL_FIELD, parser.longValue());
-                    return true;
-                }
-            } else if (parseFieldMatcher.match(currentFieldName, HistogramAggregator.MIN_DOC_COUNT_FIELD)) {
-                otherOptions.put(HistogramAggregator.MIN_DOC_COUNT_FIELD, parser.longValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, HistogramAggregator.KEYED_FIELD)) {
-                otherOptions.put(HistogramAggregator.KEYED_FIELD, parser.booleanValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, Rounding.OffsetRounding.OFFSET_FIELD)) {
-                if (token == XContentParser.Token.VALUE_STRING) {
-                    otherOptions.put(Rounding.OffsetRounding.OFFSET_FIELD, parseStringOffset(parser.text()));
-                    return true;
+        boolean keyed = false;
+        long minDocCount = 0;
+        InternalOrder order = (InternalOrder) InternalOrder.KEY_ASC;
+        long interval = -1;
+        ExtendedBounds extendedBounds = null;
+        long offset = 0;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token.isValue()) {
+                if ("interval".equals(currentFieldName)) {
+                    interval = parser.longValue();
+                } else if ("min_doc_count".equals(currentFieldName) || "minDocCount".equals(currentFieldName)) {
+                    minDocCount = parser.longValue();
+                } else if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else if ("offset".equals(currentFieldName)) {
+                    offset = parser.longValue();
                 } else {
-                    otherOptions.put(Rounding.OffsetRounding.OFFSET_FIELD, parser.longValue());
-                    return true;
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-            } else {
-                return false;
-            }
-        } else if (token == XContentParser.Token.START_OBJECT) {
-            if (parseFieldMatcher.match(currentFieldName, HistogramAggregator.ORDER_FIELD)) {
-                InternalOrder order = null;
-                while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                    if (token == XContentParser.Token.FIELD_NAME) {
-                        currentFieldName = parser.currentName();
-                    } else if (token == XContentParser.Token.VALUE_STRING) {
-                        String dir = parser.text();
-                        boolean asc = "asc".equals(dir);
-                        if (!asc && !"desc".equals(dir)) {
-                            throw new ParsingException(parser.getTokenLocation(), "Unknown order direction in aggregation ["
-                                    + aggregationName + "]: [" + dir
-                                    + "]. Should be either [asc] or [desc]");
+            } else if (token == XContentParser.Token.START_OBJECT) {
+                if ("order".equals(currentFieldName)) {
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            currentFieldName = parser.currentName();
+                        } else if (token == XContentParser.Token.VALUE_STRING) {
+                            String dir = parser.text();
+                            boolean asc = "asc".equals(dir);
+                            if (!asc && !"desc".equals(dir)) {
+                                throw new SearchParseException(context, "Unknown order direction [" + dir + "] in aggregation ["
+                                        + aggregationName + "]. Should be either [asc] or [desc]", parser.getTokenLocation());
+                            }
+                            order = resolveOrder(currentFieldName, asc);
                         }
-                        order = resolveOrder(currentFieldName, asc);
                     }
+                } else if (context.parseFieldMatcher().match(currentFieldName, EXTENDED_BOUNDS)) {
+                    extendedBounds = new ExtendedBounds();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            currentFieldName = parser.currentName();
+                        } else if (token.isValue()) {
+                            if ("min".equals(currentFieldName)) {
+                                extendedBounds.min = parser.longValue(true);
+                            } else if ("max".equals(currentFieldName)) {
+                                extendedBounds.max = parser.longValue(true);
+                            } else {
+                                throw new SearchParseException(context, "Unknown extended_bounds key for a " + token + " in aggregation ["
+                                        + aggregationName + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                            }
+                        }
+                    }
+
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-                otherOptions.put(HistogramAggregator.ORDER_FIELD, order);
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, ExtendedBounds.EXTENDED_BOUNDS_FIELD)) {
-                ExtendedBounds extendedBounds = ExtendedBounds.parse(parser, parseFieldMatcher, aggregationName);
-                otherOptions.put(ExtendedBounds.EXTENDED_BOUNDS_FIELD, extendedBounds);
-                return true;
             } else {
-                return false;
+                throw new SearchParseException(context, "Unexpected token " + token + " in aggregation [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
-        } else {
-            return false;
         }
-    }
 
-    protected Object parseStringInterval(String interval) {
-        return Long.valueOf(interval);
-    }
+        if (interval < 1) {
+            throw new SearchParseException(context,
+                    "Missing required field [interval] for histogram aggregation [" + aggregationName + "]", parser.getTokenLocation());
+        }
+
+        Rounding rounding = new Rounding.Interval(interval);
+        if (offset != 0) {
+            rounding = new Rounding.OffsetRounding((Rounding.Interval) rounding, offset);
+        }
+
+        if (extendedBounds != null) {
+            // with numeric histogram, we can process here and fail fast if the bounds are invalid
+            extendedBounds.processAndValidate(aggregationName, context, ValueParser.RAW);
+        }
+
+        return new HistogramAggregator.Factory(aggregationName, vsParser.config(), rounding, order, keyed, minDocCount, extendedBounds,
+                new InternalHistogram.Factory());
 
-    protected long parseStringOffset(String offset) throws IOException {
-        return Long.valueOf(offset);
     }
 
     static InternalOrder resolveOrder(String key, boolean asc) {
@@ -161,9 +149,4 @@ public class HistogramParser extends NumericValuesSourceParser {
         }
         return new InternalOrder.Aggregation(key, asc);
     }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return HistogramAggregator.HistogramAggregatorBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java
index 9808eed..1651886 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java
@@ -21,7 +21,6 @@ package org.elasticsearch.search.aggregations.bucket.histogram;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.InternalAggregations;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
@@ -31,7 +30,6 @@ import org.joda.time.DateTimeZone;
  */
 public class InternalDateHistogram {
 
-    public static final Factory HISTOGRAM_FACTORY = new Factory();
     final static Type TYPE = new Type("date_histogram", "dhisto");
 
     static class Bucket extends InternalHistogram.Bucket {
@@ -67,13 +65,8 @@ public class InternalDateHistogram {
         }
 
         @Override
-        public Type type() {
-            return TYPE;
-        }
-
-        @Override
-        public ValueType valueType() {
-            return ValueType.DATE;
+        public String type() {
+            return TYPE.name();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java
index c24ab0d..faca359 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java
@@ -34,7 +34,6 @@ import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;
 import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -52,7 +51,6 @@ import java.util.Map;
 public class InternalHistogram<B extends InternalHistogram.Bucket> extends InternalMultiBucketAggregation<InternalHistogram, B> implements
         Histogram {
 
-    public static final Factory<Bucket> HISTOGRAM_FACTORY = new Factory<Bucket>();
     final static Type TYPE = new Type("histogram", "histo");
 
     private final static AggregationStreams.Stream STREAM = new AggregationStreams.Stream() {
@@ -237,12 +235,8 @@ public class InternalHistogram<B extends InternalHistogram.Bucket> extends Inter
         protected Factory() {
         }
 
-        public Type type() {
-            return TYPE;
-        }
-
-        public ValueType valueType() {
-            return ValueType.NUMERIC;
+        public String type() {
+            return TYPE.name();
         }
 
         public InternalHistogram<B> create(String name, List<B> buckets, InternalOrder order, long minDocCount,
@@ -511,7 +505,7 @@ public class InternalHistogram<B extends InternalHistogram.Bucket> extends Inter
     }
 
     @SuppressWarnings("unchecked")
-    protected static <B extends InternalHistogram.Bucket> Factory<B> resolveFactory(String factoryType) {
+    private static <B extends InternalHistogram.Bucket> Factory<B> resolveFactory(String factoryType) {
         if (factoryType.equals(InternalDateHistogram.TYPE.name())) {
             return (Factory<B>) new InternalDateHistogram.Factory();
         } else if (factoryType.equals(TYPE.name())) {
@@ -523,7 +517,7 @@ public class InternalHistogram<B extends InternalHistogram.Bucket> extends Inter
 
     @Override
     protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(factory.type().name());
+        out.writeString(factory.type());
         InternalOrder.Streams.writeOrder(order, out);
         out.writeVLong(minDocCount);
         if (minDocCount == 0) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java
index d19a839..9d503a8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java
@@ -25,7 +25,6 @@ import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;
 
 import java.io.IOException;
 import java.util.Comparator;
-import java.util.Objects;
 
 /**
  * An internal {@link Histogram.Order} strategy which is identified by a unique id.
@@ -65,25 +64,6 @@ class InternalOrder extends Histogram.Order {
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         return builder.startObject().field(key, asc ? "asc" : "desc").endObject();
     }
-    
-    @Override
-    public int hashCode() {
-        return Objects.hash(id, key, asc);
-    }
-    
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        InternalOrder other = (InternalOrder) obj;
-        return Objects.equals(id, other.id)
-                && Objects.equals(key, other.key)
-                && Objects.equals(asc, other.asc);
-    }
 
     static class Aggregation extends InternalOrder {
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java
index 758b6c9..1ae7341 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java
@@ -20,25 +20,17 @@ package org.elasticsearch.search.aggregations.bucket.missing;
 
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 
 import java.io.IOException;
 import java.util.List;
@@ -89,42 +81,22 @@ public class MissingAggregator extends SingleBucketAggregator {
         return new InternalMissing(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
     }
 
-    public static class MissingAggregatorBuilder extends ValuesSourceAggregatorBuilder<ValuesSource, MissingAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource>  {
 
-        public MissingAggregatorBuilder(String name, ValueType targetValueType) {
-            super(name, InternalMissing.TYPE, ValuesSourceType.ANY, targetValueType);
+        public Factory(String name, ValuesSourceConfig valueSourceConfig) {
+            super(name, InternalMissing.TYPE.name(), valueSourceConfig);
         }
 
         @Override
-        protected ValuesSourceAggregatorFactory<ValuesSource, ?> innerBuild(AggregationContext context,
-                ValuesSourceConfig<ValuesSource> config, AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new MissingAggregatorFactory(name, type, config, context, parent, subFactoriesBuilder, metaData);
+        protected MissingAggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            return new MissingAggregator(name, factories, null, aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected MissingAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new MissingAggregatorBuilder(name, targetValueType);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
+        protected MissingAggregator doCreateInternal(ValuesSource valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new MissingAggregator(name, factories, valuesSource, aggregationContext, parent, pipelineAggregators, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregatorFactory.java
deleted file mode 100644
index 4da7037..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregatorFactory.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.missing;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class MissingAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource, MissingAggregatorFactory> {
-
-    public MissingAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource> config, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected MissingAggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new MissingAggregator(name, factories, null, aggregationContext, parent, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected MissingAggregator doCreateInternal(ValuesSource valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new MissingAggregator(name, factories, valuesSource, aggregationContext, parent, pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingBuilder.java
new file mode 100644
index 0000000..9f51fd0
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingBuilder.java
@@ -0,0 +1,57 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.missing;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Missing} aggregation.
+ */
+public class MissingBuilder extends AggregationBuilder<MissingBuilder> {
+
+    private String field;
+
+    /**
+     * Sole constructor.
+     */
+    public MissingBuilder(String name) {
+        super(name, InternalMissing.TYPE.name());
+    }
+
+    /**
+     * Set the field to count missing values on.
+     */
+    public MissingBuilder field(String field) {
+        this.field = field;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (field != null) {
+            builder.field("field", field);
+        }
+        return builder.endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java
index 43c611b..6ecdc12 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java
@@ -18,22 +18,19 @@
  */
 package org.elasticsearch.search.aggregations.bucket.missing;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
-public class MissingParser extends AnyValuesSourceParser {
-
-    public MissingParser() {
-        super(true, true);
-    }
+/**
+ *
+ */
+public class MissingParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -41,19 +38,25 @@ public class MissingParser extends AnyValuesSourceParser {
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected MissingAggregator.MissingAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new MissingAggregator.MissingAggregatorBuilder(aggregationName, targetValueType);
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new MissingAggregator.MissingAggregatorBuilder(null, null);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, InternalMissing.TYPE, context)
+                .scriptable(false)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+        }
+
+        return new MissingAggregator.Factory(aggregationName, vsParser.config());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java
index 4b18620..fa23cf8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java
@@ -28,20 +28,16 @@ import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.search.join.BitSetProducer;
 import org.apache.lucene.util.BitSet;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
+import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
+import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
@@ -49,15 +45,12 @@ import org.elasticsearch.search.aggregations.support.AggregationContext;
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class NestedAggregator extends SingleBucketAggregator {
 
-    static final ParseField PATH_FIELD = new ParseField("path");
-
     private BitSetProducer parentFilter;
     private final Query childFilter;
 
@@ -127,7 +120,7 @@ public class NestedAggregator extends SingleBucketAggregator {
             }
         };
     }
-
+        
     @Override
     public InternalAggregation buildAggregation(long owningBucketOrdinal) throws IOException {
         return new InternalNested(name, bucketDocCount(owningBucketOrdinal), bucketAggregations(owningBucketOrdinal), pipelineAggregators(),
@@ -150,64 +143,42 @@ public class NestedAggregator extends SingleBucketAggregator {
         return null;
     }
 
-    public static class NestedAggregatorBuilder extends AggregatorBuilder<NestedAggregatorBuilder> {
+    public static class Factory extends AggregatorFactory {
 
         private final String path;
 
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param path
-         *            the path to use for this nested aggregation. The path must
-         *            match the path to a nested object in the mappings.
-         */
-        public NestedAggregatorBuilder(String name, String path) {
-            super(name, InternalNested.TYPE);
+        public Factory(String name, String path) {
+            super(name, InternalNested.TYPE.name());
             this.path = path;
         }
 
-        /**
-         * Get the path to use for this nested aggregation.
-         */
-        public String path() {
-            return path;
-        }
-
         @Override
-        protected AggregatorFactory<?> doBuild(AggregationContext context, AggregatorFactory<?> parent, Builder subFactoriesBuilder)
-                throws IOException {
-            return new NestedAggregatorFactory(name, type, path, context, parent, subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            builder.field(PATH_FIELD.getPreferredName(), path);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected NestedAggregatorBuilder doReadFrom(String name, StreamInput in) throws IOException {
-            String path = in.readString();
-            NestedAggregatorBuilder factory = new NestedAggregatorBuilder(name, path);
-            return factory;
+        public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            if (collectsFromSingleBucket == false) {
+                return asMultiBucketAggregator(this, context, parent);
+            }
+            ObjectMapper objectMapper = context.searchContext().getObjectMapper(path);
+            if (objectMapper == null) {
+                return new Unmapped(name, context, parent, pipelineAggregators, metaData);
+            }
+            if (!objectMapper.nested().isNested()) {
+                throw new AggregationExecutionException("[nested] nested path [" + path + "] is not nested");
+            }
+            return new NestedAggregator(name, factories, objectMapper, context, parent, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeString(path);
-        }
+        private final static class Unmapped extends NonCollectingAggregator {
 
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(path);
-        }
+            public Unmapped(String name, AggregationContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                    throws IOException {
+                super(name, context, parent, pipelineAggregators, metaData);
+            }
 
-        @Override
-        protected boolean doEquals(Object obj) {
-            NestedAggregatorBuilder other = (NestedAggregatorBuilder) obj;
-            return Objects.equals(path, other.path);
+            @Override
+            public InternalAggregation buildEmptyAggregation() {
+                return new InternalNested(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
+            }
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorFactory.java
deleted file mode 100644
index 4e1506e..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorFactory.java
+++ /dev/null
@@ -1,76 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.nested;
-
-import org.elasticsearch.index.mapper.object.ObjectMapper;
-import org.elasticsearch.search.aggregations.AggregationExecutionException;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class NestedAggregatorFactory extends AggregatorFactory<NestedAggregatorFactory> {
-
-    private final String path;
-
-    public NestedAggregatorFactory(String name, Type type, String path, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactories, Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactories, metaData);
-        this.path = path;
-    }
-
-    @Override
-    public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        if (collectsFromSingleBucket == false) {
-            return asMultiBucketAggregator(this, context, parent);
-        }
-        ObjectMapper objectMapper = context.searchContext().getObjectMapper(path);
-        if (objectMapper == null) {
-            return new Unmapped(name, context, parent, pipelineAggregators, metaData);
-        }
-        if (!objectMapper.nested().isNested()) {
-            throw new AggregationExecutionException("[nested] nested path [" + path + "] is not nested");
-        }
-        return new NestedAggregator(name, factories, objectMapper, context, parent, pipelineAggregators, metaData);
-    }
-
-    private final static class Unmapped extends NonCollectingAggregator {
-
-        public Unmapped(String name, AggregationContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
-                Map<String, Object> metaData) throws IOException {
-            super(name, context, parent, pipelineAggregators, metaData);
-        }
-
-        @Override
-        public InternalAggregation buildEmptyAggregation() {
-            return new InternalNested(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
-        }
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedBuilder.java
new file mode 100644
index 0000000..b375f09
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedBuilder.java
@@ -0,0 +1,61 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.nested;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Nested} aggregation.
+ */
+public class NestedBuilder extends AggregationBuilder<NestedBuilder> {
+
+    private String path;
+
+    /**
+     * Sole constructor.
+     */
+    public NestedBuilder(String name) {
+        super(name, InternalNested.TYPE.name());
+    }
+
+    /**
+     * Set the path to use for this nested aggregation. The path must match
+     * the path to a nested object in the mappings. This parameter is
+     * compulsory.
+     */
+    public NestedBuilder path(String path) {
+        this.path = path;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (path == null) {
+            throw new SearchSourceBuilderException("nested path must be set on nested aggregation [" + getName() + "]");
+        }
+        builder.field("path", path);
+        return builder.endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java
index e96775e..ddf6bf1 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java
@@ -18,11 +18,11 @@
  */
 package org.elasticsearch.search.aggregations.bucket.nested;
 
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -37,7 +37,7 @@ public class NestedParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         String path = null;
 
         XContentParser.Token token;
@@ -46,27 +46,24 @@ public class NestedParser implements Aggregator.Parser {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.VALUE_STRING) {
-                if (context.parseFieldMatcher().match(currentFieldName, NestedAggregator.PATH_FIELD)) {
+                if ("path".equals(currentFieldName)) {
                     path = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + aggregationName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (path == null) {
             // "field" doesn't exist, so we fall back to the context of the ancestors
-            throw new ParsingException(parser.getTokenLocation(), "Missing [path] field for nested aggregation [" + aggregationName + "]");
+            throw new SearchParseException(context, "Missing [path] field for nested aggregation [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
-        return new NestedAggregator.NestedAggregatorBuilder(aggregationName, path);
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new NestedAggregator.NestedAggregatorBuilder(null, null);
+        return new NestedAggregator.Factory(aggregationName, path);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java
index b823af5..1b9363c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java
@@ -24,20 +24,17 @@ import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.join.BitSetProducer;
 import org.apache.lucene.util.BitSet;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
+import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
@@ -45,15 +42,12 @@ import org.elasticsearch.search.aggregations.support.AggregationContext;
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class ReverseNestedAggregator extends SingleBucketAggregator {
 
-    static final ParseField PATH_FIELD = new ParseField("path");
-
     private final Query parentFilter;
     private final BitSetProducer parentBitsetProducer;
 
@@ -100,6 +94,15 @@ public class ReverseNestedAggregator extends SingleBucketAggregator {
         };
     }
 
+    private static NestedAggregator findClosestNestedAggregator(Aggregator parent) {
+        for (; parent != null; parent = parent.parent()) {
+            if (parent instanceof NestedAggregator) {
+                return (NestedAggregator) parent;
+            }
+        }
+        return null;
+    }
+
     @Override
     public InternalAggregation buildAggregation(long owningBucketOrdinal) throws IOException {
         return new InternalReverseNested(name, bucketDocCount(owningBucketOrdinal), bucketAggregations(owningBucketOrdinal), pipelineAggregators(),
@@ -115,68 +118,51 @@ public class ReverseNestedAggregator extends SingleBucketAggregator {
         return parentFilter;
     }
 
-    public static class ReverseNestedAggregatorBuilder extends AggregatorBuilder<ReverseNestedAggregatorBuilder> {
-
-        private String path;
+    public static class Factory extends AggregatorFactory {
 
-        public ReverseNestedAggregatorBuilder(String name) {
-            super(name, InternalReverseNested.TYPE);
-        }
+        private final String path;
 
-        /**
-         * Set the path to use for this nested aggregation. The path must match
-         * the path to a nested object in the mappings. If it is not specified
-         * then this aggregation will go back to the root document.
-         */
-        public ReverseNestedAggregatorBuilder path(String path) {
+        public Factory(String name, String path) {
+            super(name, InternalReverseNested.TYPE.name());
             this.path = path;
-            return this;
-        }
-
-        /**
-         * Get the path to use for this nested aggregation.
-         */
-        public String path() {
-            return path;
         }
 
         @Override
-        protected AggregatorFactory<?> doBuild(AggregationContext context, AggregatorFactory<?> parent, Builder subFactoriesBuilder)
-                throws IOException {
-            return new ReverseNestedAggregatorFactory(name, type, path, context, parent, subFactoriesBuilder, metaData);
-        }
+        public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            // Early validation
+            NestedAggregator closestNestedAggregator = findClosestNestedAggregator(parent);
+            if (closestNestedAggregator == null) {
+                throw new SearchParseException(context.searchContext(), "Reverse nested aggregation [" + name
+                        + "] can only be used inside a [nested] aggregation", null);
+            }
 
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
+            final ObjectMapper objectMapper;
             if (path != null) {
-                builder.field(PATH_FIELD.getPreferredName(), path);
+                objectMapper = context.searchContext().getObjectMapper(path);
+                if (objectMapper == null) {
+                    return new Unmapped(name, context, parent, pipelineAggregators, metaData);
+                }
+                if (!objectMapper.nested().isNested()) {
+                    throw new AggregationExecutionException("[reverse_nested] nested path [" + path + "] is not nested");
+                }
+            } else {
+                objectMapper = null;
             }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected ReverseNestedAggregatorBuilder doReadFrom(String name, StreamInput in) throws IOException {
-            ReverseNestedAggregatorBuilder factory = new ReverseNestedAggregatorBuilder(name);
-            factory.path = in.readOptionalString();
-            return factory;
+            return new ReverseNestedAggregator(name, factories, objectMapper, context, parent, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(path);
-        }
+        private final static class Unmapped extends NonCollectingAggregator {
 
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(path);
-        }
+            public Unmapped(String name, AggregationContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                    throws IOException {
+                super(name, context, parent, pipelineAggregators, metaData);
+            }
 
-        @Override
-        protected boolean doEquals(Object obj) {
-            ReverseNestedAggregatorBuilder other = (ReverseNestedAggregatorBuilder) obj;
-            return Objects.equals(path, other.path);
+            @Override
+            public InternalAggregation buildEmptyAggregation() {
+                return new InternalReverseNested(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
+            }
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregatorFactory.java
deleted file mode 100644
index 6ccbe93..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregatorFactory.java
+++ /dev/null
@@ -1,94 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.nested;
-
-import org.elasticsearch.index.mapper.object.ObjectMapper;
-import org.elasticsearch.search.SearchParseException;
-import org.elasticsearch.search.aggregations.AggregationExecutionException;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class ReverseNestedAggregatorFactory extends AggregatorFactory<ReverseNestedAggregatorFactory> {
-
-    private final String path;
-
-    public ReverseNestedAggregatorFactory(String name, Type type, String path, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactories, Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactories, metaData);
-        this.path = path;
-    }
-
-    @Override
-    public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        // Early validation
-        NestedAggregator closestNestedAggregator = findClosestNestedAggregator(parent);
-        if (closestNestedAggregator == null) {
-            throw new SearchParseException(context.searchContext(),
-                    "Reverse nested aggregation [" + name + "] can only be used inside a [nested] aggregation", null);
-        }
-
-        final ObjectMapper objectMapper;
-        if (path != null) {
-            objectMapper = context.searchContext().getObjectMapper(path);
-            if (objectMapper == null) {
-                return new Unmapped(name, context, parent, pipelineAggregators, metaData);
-            }
-            if (!objectMapper.nested().isNested()) {
-                throw new AggregationExecutionException("[reverse_nested] nested path [" + path + "] is not nested");
-            }
-        } else {
-            objectMapper = null;
-        }
-        return new ReverseNestedAggregator(name, factories, objectMapper, context, parent, pipelineAggregators, metaData);
-    }
-
-    private static NestedAggregator findClosestNestedAggregator(Aggregator parent) {
-        for (; parent != null; parent = parent.parent()) {
-            if (parent instanceof NestedAggregator) {
-                return (NestedAggregator) parent;
-            }
-        }
-        return null;
-    }
-
-    private final static class Unmapped extends NonCollectingAggregator {
-
-        public Unmapped(String name, AggregationContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
-                Map<String, Object> metaData) throws IOException {
-            super(name, context, parent, pipelineAggregators, metaData);
-        }
-
-        @Override
-        public InternalAggregation buildEmptyAggregation() {
-            return new InternalReverseNested(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedBuilder.java
new file mode 100644
index 0000000..591655e
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedBuilder.java
@@ -0,0 +1,59 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.nested;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link ReverseNested} aggregation.
+ */
+public class ReverseNestedBuilder extends AggregationBuilder<ReverseNestedBuilder> {
+
+    private String path;
+
+    /**
+     * Sole constructor.
+     */
+    public ReverseNestedBuilder(String name) {
+        super(name, InternalReverseNested.TYPE.name());
+    }
+
+    /**
+     * Set the path to use for this nested aggregation. The path must match
+     * the path to a nested object in the mappings. If it is not specified
+     * then this aggregation will go back to the root document.
+     */
+    public ReverseNestedBuilder path(String path) {
+        this.path = path;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (path != null) {
+            builder.field("path", path);
+        }
+        return builder.endObject();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedParser.java
index 59f3232..80ab9f5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedParser.java
@@ -18,11 +18,11 @@
  */
 package org.elasticsearch.search.aggregations.bucket.nested;
 
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -37,7 +37,7 @@ public class ReverseNestedParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         String path = null;
 
         XContentParser.Token token;
@@ -49,24 +49,15 @@ public class ReverseNestedParser implements Aggregator.Parser {
                 if ("path".equals(currentFieldName)) {
                     path = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + aggregationName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
 
-        ReverseNestedAggregator.ReverseNestedAggregatorBuilder factory = new ReverseNestedAggregator.ReverseNestedAggregatorBuilder(
-                aggregationName);
-        if (path != null) {
-            factory.path(path);
-        }
-        return factory;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new ReverseNestedAggregator.ReverseNestedAggregatorBuilder(null);
+        return new ReverseNestedAggregator.Factory(aggregationName, path);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeAggregatorFactory.java
deleted file mode 100644
index ae54799..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeAggregatorFactory.java
+++ /dev/null
@@ -1,70 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Unmapped;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class AbstractRangeAggregatorFactory<AF extends AbstractRangeAggregatorFactory<AF, R>, R extends Range>
-        extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, AF> {
-
-    private final InternalRange.Factory<?, ?> rangeFactory;
-    private final List<R> ranges;
-    private final boolean keyed;
-
-    public AbstractRangeAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, List<R> ranges, boolean keyed,
-            InternalRange.Factory<?, ?> rangeFactory, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.ranges = ranges;
-        this.keyed = keyed;
-        this.rangeFactory = rangeFactory;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new Unmapped(name, ranges, keyed, config.format(), aggregationContext, parent, rangeFactory, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new RangeAggregator(name, factories, valuesSource, config.format(), rangeFactory, ranges, keyed, aggregationContext, parent,
-                pipelineAggregators, metaData);
-    }
-
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeBuilder.java
new file mode 100644
index 0000000..c4f0c76
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeBuilder.java
@@ -0,0 +1,81 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.range;
+
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ *
+ */
+public abstract class AbstractRangeBuilder<B extends AbstractRangeBuilder<B>> extends ValuesSourceAggregationBuilder<B> {
+
+    protected static class Range implements ToXContent {
+
+        private String key;
+        private Object from;
+        private Object to;
+
+        public Range(String key, Object from, Object to) {
+            this.key = key;
+            this.from = from;
+            this.to = to;
+        }
+
+        @Override
+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+            builder.startObject();
+            if (key != null) {
+                builder.field("key", key);
+            }
+            if (from != null) {
+                builder.field("from", from);
+            }
+            if (to != null) {
+                builder.field("to", to);
+            }
+            return builder.endObject();
+        }
+    }
+
+    protected List<Range> ranges = new ArrayList<>();
+
+    protected AbstractRangeBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (ranges.isEmpty()) {
+            throw new SearchSourceBuilderException("at least one range must be defined for range aggregation [" + getName() + "]");
+        }
+        builder.startArray("ranges");
+        for (Range range : ranges) {
+            range.toXContent(builder, params);
+        }
+        return builder.endArray();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java
index d96e860..5303d7f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java
@@ -29,8 +29,6 @@ import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;
 import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -223,16 +221,8 @@ public class InternalRange<B extends InternalRange.Bucket, R extends InternalRan
 
     public static class Factory<B extends Bucket, R extends InternalRange<B, R>> {
 
-        public Type type() {
-            return TYPE;
-        }
-
-        public ValuesSourceType getValueSourceType() {
-            return ValuesSourceType.NUMERIC;
-        }
-
-        public ValueType getValueType() {
-            return ValueType.NUMERIC;
+        public String type() {
+            return TYPE.name();
         }
 
         public R create(String name, List<B> ranges, ValueFormatter formatter, boolean keyed, List<PipelineAggregator> pipelineAggregators,
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java
index 118a6f3..125fca4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java
@@ -20,33 +20,20 @@ package org.elasticsearch.search.aggregations.bucket.range;
 
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.InPlaceMergeSorter;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregations;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueParser;
@@ -56,38 +43,21 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class RangeAggregator extends BucketsAggregator {
 
-    public static final ParseField RANGES_FIELD = new ParseField("ranges");
-    public static final ParseField KEYED_FIELD = new ParseField("keyed");
+    public static class Range {
 
-    public static class Range implements Writeable<Range>, ToXContent {
+        public String key;
+        public double from = Double.NEGATIVE_INFINITY;
+        String fromAsStr;
+        public double to = Double.POSITIVE_INFINITY;
+        String toAsStr;
 
-        public static final Range PROTOTYPE = new Range(null, -1, null, -1, null);
-        public static final ParseField KEY_FIELD = new ParseField("key");
-        public static final ParseField FROM_FIELD = new ParseField("from");
-        public static final ParseField TO_FIELD = new ParseField("to");
-
-        protected String key;
-        protected double from = Double.NEGATIVE_INFINITY;
-        protected String fromAsStr;
-        protected double to = Double.POSITIVE_INFINITY;
-        protected String toAsStr;
-
-        public Range(String key, Double from, Double to) {
-            this(key, from == null ? Double.NEGATIVE_INFINITY : from, null, to == null ? Double.POSITIVE_INFINITY : to, null);
-        }
-
-        public Range(String key, String from, String to) {
-            this(key, Double.NEGATIVE_INFINITY, from, Double.POSITIVE_INFINITY, to);
-        }
-
-        protected Range(String key, double from, String fromAsStr, double to, String toAsStr) {
+        public Range(String key, double from, String fromAsStr, double to, String toAsStr) {
             this.key = key;
             this.from = from;
             this.fromAsStr = fromAsStr;
@@ -113,99 +83,6 @@ public class RangeAggregator extends BucketsAggregator {
                 to = parser.parseDouble(toAsStr, context);
             }
         }
-
-        @Override
-        public Range readFrom(StreamInput in) throws IOException {
-            String key = in.readOptionalString();
-            String fromAsStr = in.readOptionalString();
-            String toAsStr = in.readOptionalString();
-            double from = in.readDouble();
-            double to = in.readDouble();
-            return new Range(key, from, fromAsStr, to, toAsStr);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(key);
-            out.writeOptionalString(fromAsStr);
-            out.writeOptionalString(toAsStr);
-            out.writeDouble(from);
-            out.writeDouble(to);
-        }
-
-        public Range fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-
-            XContentParser.Token token;
-            String currentFieldName = null;
-            double from = Double.NEGATIVE_INFINITY;
-            String fromAsStr = null;
-            double to = Double.POSITIVE_INFINITY;
-            String toAsStr = null;
-            String key = null;
-            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                if (token == XContentParser.Token.FIELD_NAME) {
-                    currentFieldName = parser.currentName();
-                } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                    if (parseFieldMatcher.match(currentFieldName, FROM_FIELD)) {
-                        from = parser.doubleValue();
-                    } else if (parseFieldMatcher.match(currentFieldName, TO_FIELD)) {
-                        to = parser.doubleValue();
-                    }
-                } else if (token == XContentParser.Token.VALUE_STRING) {
-                    if (parseFieldMatcher.match(currentFieldName, FROM_FIELD)) {
-                        fromAsStr = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, TO_FIELD)) {
-                        toAsStr = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, KEY_FIELD)) {
-                        key = parser.text();
-                    }
-                }
-            }
-            return new Range(key, from, fromAsStr, to, toAsStr);
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            if (key != null) {
-                builder.field(KEY_FIELD.getPreferredName(), key);
-            }
-            if (Double.isFinite(from)) {
-                builder.field(FROM_FIELD.getPreferredName(), from);
-            }
-            if (Double.isFinite(to)) {
-                builder.field(TO_FIELD.getPreferredName(), to);
-            }
-            if (fromAsStr != null) {
-                builder.field(FROM_FIELD.getPreferredName(), fromAsStr);
-            }
-            if (toAsStr != null) {
-                builder.field(TO_FIELD.getPreferredName(), toAsStr);
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(key, from, fromAsStr, to, toAsStr);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            Range other = (Range) obj;
-            return Objects.equals(key, other.key)
-                    && Objects.equals(from, other.from)
-                    && Objects.equals(fromAsStr, other.fromAsStr)
-                    && Objects.equals(to, other.to)
-                    && Objects.equals(toAsStr, other.toAsStr);
-        }
     }
 
     final ValuesSource.Numeric valuesSource;
@@ -217,7 +94,7 @@ public class RangeAggregator extends BucketsAggregator {
     final double[] maxTo;
 
     public RangeAggregator(String name, AggregatorFactories factories, ValuesSource.Numeric valuesSource, ValueFormat format,
-            InternalRange.Factory rangeFactory, List<? extends Range> ranges, boolean keyed, AggregationContext aggregationContext,
+            InternalRange.Factory rangeFactory, List<Range> ranges, boolean keyed, AggregationContext aggregationContext,
             Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
 
         super(name, factories, aggregationContext, parent, pipelineAggregators, metaData);
@@ -368,13 +245,12 @@ public class RangeAggregator extends BucketsAggregator {
 
     public static class Unmapped extends NonCollectingAggregator {
 
-        private final List<? extends RangeAggregator.Range> ranges;
+        private final List<RangeAggregator.Range> ranges;
         private final boolean keyed;
         private final InternalRange.Factory factory;
         private final ValueFormatter formatter;
 
-        public Unmapped(String name, List<? extends RangeAggregator.Range> ranges, boolean keyed, ValueFormat format,
-                AggregationContext context,
+        public Unmapped(String name, List<RangeAggregator.Range> ranges, boolean keyed, ValueFormat format, AggregationContext context,
                 Aggregator parent, InternalRange.Factory factory, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
                 throws IOException {
 
@@ -400,162 +276,29 @@ public class RangeAggregator extends BucketsAggregator {
         }
     }
 
-    public static abstract class AbstractBuilder<AB extends AbstractBuilder<AB, R>, R extends Range>
-            extends ValuesSourceAggregatorBuilder<ValuesSource.Numeric, AB> {
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric> {
 
-        protected final InternalRange.Factory<?, ?> rangeFactory;
-        protected List<R> ranges = new ArrayList<>();
-        protected boolean keyed = false;
+        private final InternalRange.Factory rangeFactory;
+        private final List<Range> ranges;
+        private final boolean keyed;
 
-        protected AbstractBuilder(String name, InternalRange.Factory<?, ?> rangeFactory) {
-            super(name, rangeFactory.type(), rangeFactory.getValueSourceType(), rangeFactory.getValueType());
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valueSourceConfig, InternalRange.Factory rangeFactory, List<Range> ranges, boolean keyed) {
+            super(name, rangeFactory.type(), valueSourceConfig);
             this.rangeFactory = rangeFactory;
-        }
-
-        public AB addRange(R range) {
-            ranges.add(range);
-            return (AB) this;
-        }
-
-        public List<R> ranges() {
-            return ranges;
-        }
-
-        public AB keyed(boolean keyed) {
+            this.ranges = ranges;
             this.keyed = keyed;
-            return (AB) this;
-        }
-
-        public boolean keyed() {
-            return keyed;
-        }
-
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(RANGES_FIELD.getPreferredName(), ranges);
-            builder.field(KEYED_FIELD.getPreferredName(), keyed);
-            return builder;
-        }
-
-        @Override
-        protected AB innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            AbstractBuilder<AB, R> factory = createFactoryFromStream(name, in);
-            factory.keyed = in.readBoolean();
-            return (AB) factory;
-        }
-
-        protected abstract AbstractBuilder<AB, R> createFactoryFromStream(String name, StreamInput in) throws IOException;
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(ranges.size());
-            for (Range range : ranges) {
-                range.writeTo(out);
-            }
-            out.writeBoolean(keyed);
         }
 
         @Override
-        protected int innerHashCode() {
-            return Objects.hash(ranges, keyed);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            return new Unmapped(name, ranges, keyed, config.format(), aggregationContext, parent, rangeFactory, pipelineAggregators, metaData);
         }
 
         @Override
-        protected boolean innerEquals(Object obj) {
-            AbstractBuilder<AB, R> other = (AbstractBuilder<AB, R>) obj;
-            return Objects.equals(ranges, other.ranges)
-                    && Objects.equals(keyed, other.keyed);
-        }
-    }
-
-    public static class RangeAggregatorBuilder extends AbstractBuilder<RangeAggregatorBuilder, Range> {
-
-        public RangeAggregatorBuilder(String name) {
-            super(name, InternalRange.FACTORY);
-        }
-
-        /**
-         * Add a new range to this aggregation.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param from
-         *            the lower bound on the distances, inclusive
-         * @param to
-         *            the upper bound on the distances, exclusive
-         */
-        public RangeAggregatorBuilder addRange(String key, double from, double to) {
-            addRange(new Range(key, from, to));
-            return this;
-        }
-
-        /**
-         * Same as {@link #addRange(String, double, double)} but the key will be
-         * automatically generated based on <code>from</code> and
-         * <code>to</code>.
-         */
-        public RangeAggregatorBuilder addRange(double from, double to) {
-            return addRange(null, from, to);
-        }
-
-        /**
-         * Add a new range with no lower bound.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param to
-         *            the upper bound on the distances, exclusive
-         */
-        public RangeAggregatorBuilder addUnboundedTo(String key, double to) {
-            addRange(new Range(key, null, to));
-            return this;
-        }
-
-        /**
-         * Same as {@link #addUnboundedTo(String, double)} but the key will be
-         * computed automatically.
-         */
-        public RangeAggregatorBuilder addUnboundedTo(double to) {
-            return addUnboundedTo(null, to);
-        }
-
-        /**
-         * Add a new range with no upper bound.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param from
-         *            the lower bound on the distances, inclusive
-         */
-        public RangeAggregatorBuilder addUnboundedFrom(String key, double from) {
-            addRange(new Range(key, from, null));
-            return this;
-        }
-
-        /**
-         * Same as {@link #addUnboundedFrom(String, double)} but the key will be
-         * computed automatically.
-         */
-        public RangeAggregatorBuilder addUnboundedFrom(double from) {
-            return addUnboundedFrom(null, from);
-        }
-
-        @Override
-        protected RangeAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new RangeAggregatorFactory(name, type, config, ranges, keyed, rangeFactory, context, parent, subFactoriesBuilder,
-                    metaData);
-        }
-
-        @Override
-        protected RangeAggregatorBuilder createFactoryFromStream(String name, StreamInput in) throws IOException {
-            int size = in.readVInt();
-            RangeAggregatorBuilder factory = new RangeAggregatorBuilder(name);
-            for (int i = 0; i < size; i++) {
-                factory.addRange(Range.PROTOTYPE.readFrom(in));
-            }
-            return factory;
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new RangeAggregator(name, factories, valuesSource, config.format(), rangeFactory, ranges, keyed, aggregationContext, parent, pipelineAggregators, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregatorFactory.java
deleted file mode 100644
index 5dec4c4..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregatorFactory.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range;
-
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.range.InternalRange.Factory;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class RangeAggregatorFactory extends AbstractRangeAggregatorFactory<RangeAggregatorFactory, RangeAggregator.Range> {
-
-    public RangeAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, List<Range> ranges, boolean keyed,
-            Factory<?, ?> rangeFactory, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, ranges, keyed, rangeFactory, context, parent, subFactoriesBuilder, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeBuilder.java
new file mode 100644
index 0000000..c772397
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeBuilder.java
@@ -0,0 +1,115 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.bucket.range;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link org.elasticsearch.search.aggregations.bucket.range.AbstractRangeBuilder.Range} aggregation.
+ */
+public class RangeBuilder extends AbstractRangeBuilder<RangeBuilder> {
+
+    private String format;
+
+    /**
+     * Sole constructor.
+     */
+    public RangeBuilder(String name) {
+        super(name, InternalRange.TYPE.name());
+    }
+
+    /**
+     * Add a new range to this aggregation.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     * @param to   the upper bound on the distances, exclusive
+     */
+    public RangeBuilder addRange(String key, double from, double to) {
+        ranges.add(new Range(key, from, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addRange(String, double, double)} but the key will be
+     * automatically generated based on <code>from</code> and <code>to</code>.
+     */
+    public RangeBuilder addRange(double from, double to) {
+        return addRange(null, from, to);
+    }
+
+    /**
+     * Add a new range with no lower bound.
+     *
+     * @param key the key to use for this range in the response
+     * @param to  the upper bound on the distances, exclusive
+     */
+    public RangeBuilder addUnboundedTo(String key, double to) {
+        ranges.add(new Range(key, null, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedTo(String, double)} but the key will be
+     * computed automatically.
+     */
+    public RangeBuilder addUnboundedTo(double to) {
+        return addUnboundedTo(null, to);
+    }
+
+    /**
+     * Add a new range with no upper bound.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     */
+    public RangeBuilder addUnboundedFrom(String key, double from) {
+        ranges.add(new Range(key, from, null));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedFrom(String, double)} but the key will be
+     * computed automatically.
+     */
+    public RangeBuilder addUnboundedFrom(double from) {
+        return addUnboundedFrom(null, from);
+    }
+
+    /**
+     * Set the format to use to display values.
+     */
+    public RangeBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.doInternalXContent(builder, params);
+        if (format != null) {
+            builder.field("format", format);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java
index 084939c..e30b84b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java
@@ -18,33 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.bucket.range;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Map;
 
 /**
  *
  */
-public class RangeParser extends NumericValuesSourceParser {
-
-    public RangeParser() {
-        this(true, true, false);
-    }
-
-    protected RangeParser(boolean scriptable, boolean formattable, boolean timezoneAware) {
-        super(scriptable, formattable, timezoneAware);
-    }
+public class RangeParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -52,49 +41,75 @@ public class RangeParser extends NumericValuesSourceParser {
     }
 
     @Override
-    protected RangeAggregator.AbstractBuilder<?, ?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        RangeAggregator.RangeAggregatorBuilder factory = new RangeAggregator.RangeAggregatorBuilder(aggregationName);
-        List<? extends Range> ranges = (List<? extends Range>) otherOptions.get(RangeAggregator.RANGES_FIELD);
-        for (Range range : ranges) {
-            factory.addRange(range);
-        }
-        Boolean keyed = (Boolean) otherOptions.get(RangeAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        return factory;
-    }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.START_ARRAY) {
-            if (parseFieldMatcher.match(currentFieldName, RangeAggregator.RANGES_FIELD)) {
-                List<Range> ranges = new ArrayList<>();
-                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                    Range range = parseRange(parser, parseFieldMatcher);
-                    ranges.add(range);
+        List<RangeAggregator.Range> ranges = null;
+        boolean keyed = false;
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalRange.TYPE, context)
+                .formattable(true)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if ("ranges".equals(currentFieldName)) {
+                    ranges = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        double from = Double.NEGATIVE_INFINITY;
+                        String fromAsStr = null;
+                        double to = Double.POSITIVE_INFINITY;
+                        String toAsStr = null;
+                        String key = null;
+                        String toOrFromOrKey = null;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                toOrFromOrKey = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    from = parser.doubleValue();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    to = parser.doubleValue();
+                                }
+                            } else if (token == XContentParser.Token.VALUE_STRING) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    fromAsStr = parser.text();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    toAsStr = parser.text();
+                                } else if ("key".equals(toOrFromOrKey)) {
+                                    key = parser.text();
+                                }
+                            }
+                        }
+                        ranges.add(new RangeAggregator.Range(key, from, fromAsStr, to, toAsStr));
+                    }
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-                otherOptions.put(RangeAggregator.RANGES_FIELD, ranges);
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, RangeAggregator.KEYED_FIELD)) {
-                boolean keyed = parser.booleanValue();
-                otherOptions.put(RangeAggregator.KEYED_FIELD, keyed);
-                return true;
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
-        return false;
-    }
 
-    protected Range parseRange(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-        return Range.PROTOTYPE.fromXContent(parser, parseFieldMatcher);
-    }
+        if (ranges == null) {
+            throw new SearchParseException(context, "Missing [ranges] in ranges aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
+        }
 
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new RangeAggregator.RangeAggregatorBuilder(null);
+        return new RangeAggregator.Factory(aggregationName, vsParser.config(), InternalRange.FACTORY, ranges, keyed);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorBuilder.java
deleted file mode 100644
index f07e251..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorBuilder.java
+++ /dev/null
@@ -1,266 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range.date;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.AbstractBuilder;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.joda.time.DateTime;
-
-import java.io.IOException;
-
-public class DateRangeAggregatorBuilder extends AbstractBuilder<DateRangeAggregatorBuilder, RangeAggregator.Range> {
-
-    public DateRangeAggregatorBuilder(String name) {
-        super(name, InternalDateRange.FACTORY);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return InternalDateRange.TYPE.name();
-    }
-
-    /**
-     * Add a new range to this aggregation.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the dates, inclusive
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorBuilder addRange(String key, String from, String to) {
-        addRange(new Range(key, from, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addRange(String, String, String)} but the key will be
-     * automatically generated based on <code>from</code> and <code>to</code>.
-     */
-    public DateRangeAggregatorBuilder addRange(String from, String to) {
-        return addRange(null, from, to);
-    }
-
-    /**
-     * Add a new range with no lower bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorBuilder addUnboundedTo(String key, String to) {
-        addRange(new Range(key, null, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedTo(String, String)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorBuilder addUnboundedTo(String to) {
-        return addUnboundedTo(null, to);
-    }
-
-    /**
-     * Add a new range with no upper bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the distances, inclusive
-     */
-    public DateRangeAggregatorBuilder addUnboundedFrom(String key, String from) {
-        addRange(new Range(key, from, null));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedFrom(String, String)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorBuilder addUnboundedFrom(String from) {
-        return addUnboundedFrom(null, from);
-    }
-
-    /**
-     * Add a new range to this aggregation.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the dates, inclusive
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorBuilder addRange(String key, double from, double to) {
-        addRange(new Range(key, from, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addRange(String, double, double)} but the key will be
-     * automatically generated based on <code>from</code> and <code>to</code>.
-     */
-    public DateRangeAggregatorBuilder addRange(double from, double to) {
-        return addRange(null, from, to);
-    }
-
-    /**
-     * Add a new range with no lower bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorBuilder addUnboundedTo(String key, double to) {
-        addRange(new Range(key, null, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedTo(String, double)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorBuilder addUnboundedTo(double to) {
-        return addUnboundedTo(null, to);
-    }
-
-    /**
-     * Add a new range with no upper bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the distances, inclusive
-     */
-    public DateRangeAggregatorBuilder addUnboundedFrom(String key, double from) {
-        addRange(new Range(key, from, null));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedFrom(String, double)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorBuilder addUnboundedFrom(double from) {
-        return addUnboundedFrom(null, from);
-    }
-
-    /**
-     * Add a new range to this aggregation.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the dates, inclusive
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorBuilder addRange(String key, DateTime from, DateTime to) {
-        addRange(new Range(key, convertDateTime(from), convertDateTime(to)));
-        return this;
-    }
-
-    private Double convertDateTime(DateTime dateTime) {
-        if (dateTime == null) {
-            return null;
-        } else {
-            return (double) dateTime.getMillis();
-        }
-    }
-
-    /**
-     * Same as {@link #addRange(String, DateTime, DateTime)} but the key will be
-     * automatically generated based on <code>from</code> and <code>to</code>.
-     */
-    public DateRangeAggregatorBuilder addRange(DateTime from, DateTime to) {
-        return addRange(null, from, to);
-    }
-
-    /**
-     * Add a new range with no lower bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param to
-     *            the upper bound on the dates, exclusive
-     */
-    public DateRangeAggregatorBuilder addUnboundedTo(String key, DateTime to) {
-        addRange(new Range(key, null, convertDateTime(to)));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedTo(String, DateTime)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorBuilder addUnboundedTo(DateTime to) {
-        return addUnboundedTo(null, to);
-    }
-
-    /**
-     * Add a new range with no upper bound.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the distances, inclusive
-     */
-    public DateRangeAggregatorBuilder addUnboundedFrom(String key, DateTime from) {
-        addRange(new Range(key, convertDateTime(from), null));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedFrom(String, DateTime)} but the key will be
-     * computed automatically.
-     */
-    public DateRangeAggregatorBuilder addUnboundedFrom(DateTime from) {
-        return addUnboundedFrom(null, from);
-    }
-
-    @Override
-    protected DateRangeAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-            AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-        return new DateRangeAggregatorFactory(name, type, config, ranges, keyed, rangeFactory, context, parent, subFactoriesBuilder,
-                metaData);
-    }
-
-    @Override
-    protected DateRangeAggregatorBuilder createFactoryFromStream(String name, StreamInput in) throws IOException {
-        int size = in.readVInt();
-        DateRangeAggregatorBuilder factory = new DateRangeAggregatorBuilder(name);
-        for (int i = 0; i < size; i++) {
-            factory.addRange(Range.PROTOTYPE.readFrom(in));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorFactory.java
deleted file mode 100644
index d3bb7ac..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorFactory.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range.date;
-
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.range.AbstractRangeAggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.range.InternalRange.Factory;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class DateRangeAggregatorFactory extends AbstractRangeAggregatorFactory<DateRangeAggregatorFactory, Range> {
-
-    public DateRangeAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, List<Range> ranges, boolean keyed,
-            Factory<?, ?> rangeFactory, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, ranges, keyed, rangeFactory, context, parent, subFactoriesBuilder, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeBuilder.java
new file mode 100644
index 0000000..4bd5758
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeBuilder.java
@@ -0,0 +1,114 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.bucket.range.date;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.bucket.range.AbstractRangeBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@code DateRange} aggregation.
+ */
+public class DateRangeBuilder extends AbstractRangeBuilder<DateRangeBuilder> {
+
+    private String format;
+
+    /**
+     * Sole constructor.
+     */
+    public DateRangeBuilder(String name) {
+        super(name, InternalDateRange.TYPE.name());
+    }
+
+    /**
+     * Add a new range to this aggregation.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     * @param to   the upper bound on the distances, exclusive
+     */
+    public DateRangeBuilder addRange(String key, Object from, Object to) {
+        ranges.add(new Range(key, from, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addRange(String, Object, Object)} but the key will be
+     * automatically generated based on <code>from</code> and <code>to</code>.
+     */
+    public DateRangeBuilder addRange(Object from, Object to) {
+        return addRange(null, from, to);
+    }
+
+    /**
+     * Add a new range with no lower bound.
+     *
+     * @param key the key to use for this range in the response
+     * @param to  the upper bound on the distances, exclusive
+     */
+    public DateRangeBuilder addUnboundedTo(String key, Object to) {
+        ranges.add(new Range(key, null, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedTo(String, Object)} but the key will be
+     * computed automatically.
+     */
+    public DateRangeBuilder addUnboundedTo(Object to) {
+        return addUnboundedTo(null, to);
+    }
+
+    /**
+     * Add a new range with no upper bound.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     */
+    public DateRangeBuilder addUnboundedFrom(String key, Object from) {
+        ranges.add(new Range(key, from, null));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedFrom(String, Object)} but the key will be
+     * computed automatically.
+     */
+    public DateRangeBuilder addUnboundedFrom(Object from) {
+        return addUnboundedFrom(null, from);
+    }
+
+    /**
+     * Set the format to use to display values.
+     */
+    public DateRangeBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.doInternalXContent(builder, params);
+        if (format != null) {
+            builder.field("format", format);
+        }
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java
index 928cfa1..940e20a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java
@@ -18,25 +18,24 @@
  */
 package org.elasticsearch.search.aggregations.bucket.range.date;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.bucket.range.RangeParser;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
+import java.io.IOException;
+import java.util.ArrayList;
 import java.util.List;
-import java.util.Map;
 
 /**
  *
  */
-public class DateRangeParser extends RangeParser {
-
-    public DateRangeParser() {
-        super(true, true, true);
-    }
+public class DateRangeParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -44,22 +43,78 @@ public class DateRangeParser extends RangeParser {
     }
 
     @Override
-    protected DateRangeAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        DateRangeAggregatorBuilder factory = new DateRangeAggregatorBuilder(aggregationName);
-        List<Range> ranges = (List<Range>) otherOptions.get(RangeAggregator.RANGES_FIELD);
-        for (Range range : ranges) {
-            factory.addRange(range);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalDateRange.TYPE, context)
+                .targetValueType(ValueType.DATE)
+                .formattable(true)
+                .build();
+
+        List<RangeAggregator.Range> ranges = null;
+        boolean keyed = false;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if ("ranges".equals(currentFieldName)) {
+                    ranges = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        double from = Double.NEGATIVE_INFINITY;
+                        String fromAsStr = null;
+                        double to = Double.POSITIVE_INFINITY;
+                        String toAsStr = null;
+                        String key = null;
+                        String toOrFromOrKey = null;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                toOrFromOrKey = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    from = parser.doubleValue();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    to = parser.doubleValue();
+                                } else {
+                                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName
+                                            + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                                }
+                            } else if (token == XContentParser.Token.VALUE_STRING) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    fromAsStr = parser.text();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    toAsStr = parser.text();
+                                } else if ("key".equals(toOrFromOrKey)) {
+                                    key = parser.text();
+                                } else {
+                                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                                }
+                            }
+                        }
+                        ranges.add(new RangeAggregator.Range(key, from, fromAsStr, to, toAsStr));
+                    }
+                }
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
         }
-        Boolean keyed = (Boolean) otherOptions.get(RangeAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
+
+        if (ranges == null) {
+            throw new SearchParseException(context, "Missing [ranges] in ranges aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
-        return factory;
-    }
 
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new DateRangeAggregatorBuilder(null);
+        return new RangeAggregator.Factory(aggregationName, vsParser.config(), InternalDateRange.FACTORY, ranges, keyed);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java
index 88568bc..ac2c18e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java
@@ -26,7 +26,6 @@ import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
@@ -116,13 +115,8 @@ public class InternalDateRange extends InternalRange<InternalDateRange.Bucket, I
     public static class Factory extends InternalRange.Factory<InternalDateRange.Bucket, InternalDateRange> {
 
         @Override
-        public Type type() {
-            return TYPE;
-        }
-
-        @Override
-        public ValueType getValueType() {
-            return ValueType.DATE;
+        public String type() {
+            return TYPE.name();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceBuilder.java
new file mode 100644
index 0000000..ae8fc22
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceBuilder.java
@@ -0,0 +1,260 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.range.geodistance;
+
+import org.elasticsearch.common.geo.GeoDistance;
+import org.elasticsearch.common.geo.GeoPoint;
+import org.elasticsearch.common.unit.DistanceUnit;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Locale;
+
+/**
+ * Builder for the {@link GeoDistance} aggregation.
+ */
+public class GeoDistanceBuilder extends AggregationBuilder<GeoDistanceBuilder> {
+
+    /**
+     * A range of values.
+     */
+    public static class Range implements ToXContent {
+
+        private String key;
+        private Double from;
+        private Double to;
+
+        /**
+         * Create a new range.
+         * @param key   the identifier of this range
+         * @param from  the lower bound (inclusive)
+         * @param to    the upper bound (exclusive)
+         */
+        public Range(String key, Double from, Double to) {
+            this.key = key;
+            this.from = from;
+            this.to = to;
+        }
+
+        @Override
+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+            builder.startObject();
+            if (from != null) {
+                builder.field("from", from.doubleValue());
+            }
+            if (to != null) {
+                builder.field("to", to.doubleValue());
+            }
+            if (key != null) {
+                builder.field("key", key);
+            }
+            return builder.endObject();
+        }
+
+    }
+
+    private String field;
+    private DistanceUnit unit;
+    private GeoDistance distanceType;
+    private GeoPoint point;
+
+    private List<Range> ranges = new ArrayList<>();
+
+    /**
+     * Sole constructor.
+     */
+    public GeoDistanceBuilder(String name) {
+        super(name, InternalGeoDistance.TYPE.name());
+    }
+
+    /**
+     * Set the field to use to compute distances.
+     */
+    public GeoDistanceBuilder field(String field) {
+        this.field = field;
+        return this;
+    }
+
+    /**
+     * Set the unit to use for distances, default is kilometers.
+     */
+    public GeoDistanceBuilder unit(DistanceUnit unit) {
+        this.unit = unit;
+        return this;
+    }
+
+    /**
+     * Set the {@link GeoDistance distance type} to use, defaults to
+     * {@link GeoDistance#SLOPPY_ARC}.
+     */
+    public GeoDistanceBuilder distanceType(GeoDistance distanceType) {
+        this.distanceType = distanceType;
+        return this;
+    }
+
+    /**
+     * Set the point to calculate distances from using a
+     * <code>lat,lon</code> notation or geohash.
+     */
+    public GeoDistanceBuilder point(String latLon) {
+        return point(GeoPoint.parseFromLatLon(latLon));
+    }
+
+    /**
+     * Set the point to calculate distances from.
+     */
+    public GeoDistanceBuilder point(GeoPoint point) {
+        this.point = point;
+        return this;
+    }
+
+    /**
+     * Set the point to calculate distances from using its geohash.
+     */
+    public GeoDistanceBuilder geohash(String geohash) {
+        if (this.point == null) {
+            this.point = new GeoPoint();
+        }
+        this.point.resetFromGeoHash(geohash);
+        return this;
+    }
+
+    /**
+     * Set the latitude of the point to calculate distances from.
+     */
+    public GeoDistanceBuilder lat(double lat) {
+        if (this.point == null) {
+            point = new GeoPoint();
+        }
+        point.resetLat(lat);
+        return this;
+    }
+
+    /**
+     * Set the longitude of the point to calculate distances from.
+     */
+    public GeoDistanceBuilder lon(double lon) {
+        if (this.point == null) {
+            point = new GeoPoint();
+        }
+        point.resetLon(lon);
+        return this;
+    }
+
+    /**
+     * Add a new range to this aggregation.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     * @param to   the upper bound on the distances, exclusive
+     */
+    public GeoDistanceBuilder addRange(String key, double from, double to) {
+        ranges.add(new Range(key, from, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addRange(String, double, double)} but the key will be
+     * automatically generated based on <code>from</code> and <code>to</code>.
+     */
+    public GeoDistanceBuilder addRange(double from, double to) {
+        return addRange(null, from, to);
+    }
+
+    /**
+     * Add a new range with no lower bound.
+     *
+     * @param key the key to use for this range in the response
+     * @param to  the upper bound on the distances, exclusive
+     */
+    public GeoDistanceBuilder addUnboundedTo(String key, double to) {
+        ranges.add(new Range(key, null, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedTo(String, double)} but the key will be
+     * computed automatically.
+     */
+    public GeoDistanceBuilder addUnboundedTo(double to) {
+        return addUnboundedTo(null, to);
+    }
+
+    /**
+     * Add a new range with no upper bound.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     */
+    public GeoDistanceBuilder addUnboundedFrom(String key, double from) {
+        ranges.add(new Range(key, from, null));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedFrom(String, double)} but the key will be
+     * computed automatically.
+     */
+    public GeoDistanceBuilder addUnboundedFrom(double from) {
+        return addUnboundedFrom(null, from);
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (ranges.isEmpty()) {
+            throw new SearchSourceBuilderException("at least one range must be defined for geo_distance aggregation [" + getName() + "]");
+        }
+        if (point == null) {
+            throw new SearchSourceBuilderException("center point must be defined for geo_distance aggregation [" + getName() + "]");
+        }
+
+        if (field != null) {
+            builder.field("field", field);
+        }
+
+        if (unit != null) {
+            builder.field("unit", unit);
+        }
+
+        if (distanceType != null) {
+            builder.field("distance_type", distanceType.name().toLowerCase(Locale.ROOT));
+        }
+
+        builder.startObject("center")
+                .field("lat", point.lat())
+                .field("lon", point.lon())
+                .endObject();
+
+        builder.startArray("ranges");
+        for (Range range : ranges) {
+            range.toXContent(builder, params);
+        }
+        builder.endArray();
+
+        return builder.endObject();
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java
index 9c4fde8..e110cc1 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java
@@ -18,372 +18,230 @@
  */
 package org.elasticsearch.search.aggregations.bucket.range.geodistance;
 
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.SortedNumericDocValues;
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.geo.GeoDistance;
+import org.elasticsearch.common.geo.GeoDistance.FixedSourceDistance;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.DistanceUnit;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.index.fielddata.MultiGeoPointValues;
+import org.elasticsearch.index.fielddata.SortedBinaryDocValues;
+import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
 import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.GeoPointValuesSourceParser;
+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Unmapped;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
 import org.elasticsearch.search.aggregations.support.GeoPointParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
-public class GeoDistanceParser extends GeoPointValuesSourceParser {
+public class GeoDistanceParser implements Aggregator.Parser {
 
     private static final ParseField ORIGIN_FIELD = new ParseField("origin", "center", "point", "por");
-    private static final ParseField UNIT_FIELD = new ParseField("unit");
-    private static final ParseField DISTANCE_TYPE_FIELD = new ParseField("distance_type");
-
-    private GeoPointParser geoPointParser = new GeoPointParser(InternalGeoDistance.TYPE, ORIGIN_FIELD);
-
-    public GeoDistanceParser() {
-        super(true, false);
-    }
 
     @Override
     public String type() {
         return InternalGeoDistance.TYPE.name();
     }
 
-    public static class Range extends RangeAggregator.Range {
-
-        static final Range PROTOTYPE = new Range(null, null, null);
-
-        public Range(String key, Double from, Double to) {
-            super(key(key, from, to), from, to);
-        }
-
-        private static String key(String key, Double from, Double to) {
-            if (key != null) {
-                return key;
-            }
-            StringBuilder sb = new StringBuilder();
-            sb.append((from == null || from == 0) ? "*" : from);
-            sb.append("-");
-            sb.append((to == null || Double.isInfinite(to)) ? "*" : to);
-            return sb.toString();
-        }
-
-        @Override
-        public Range readFrom(StreamInput in) throws IOException {
-            String key = in.readOptionalString();
-            double from = in.readDouble();
-            double to = in.readDouble();
-            return new Range(key, from, to);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(key);
-            out.writeDouble(from);
-            out.writeDouble(to);
-        }
-
-    }
-
-    @Override
-    protected GeoDistanceAggregatorBuilder createFactory(
-            String aggregationName, ValuesSourceType valuesSourceType, ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        GeoPoint origin = (GeoPoint) otherOptions.get(ORIGIN_FIELD);
-        GeoDistanceAggregatorBuilder factory = new GeoDistanceAggregatorBuilder(aggregationName, origin);
-        List<Range> ranges = (List<Range>) otherOptions.get(RangeAggregator.RANGES_FIELD);
-        for (Range range : ranges) {
-            factory.addRange(range);
-        }
-        Boolean keyed = (Boolean) otherOptions.get(RangeAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        DistanceUnit unit = (DistanceUnit) otherOptions.get(UNIT_FIELD);
-        if (unit != null) {
-            factory.unit(unit);
-        }
-        GeoDistance distanceType = (GeoDistance) otherOptions.get(DISTANCE_TYPE_FIELD);
-        if (distanceType != null) {
-            factory.distanceType(distanceType);
+    private static String key(String key, double from, double to) {
+        if (key != null) {
+            return key;
         }
-        return factory;
+        StringBuilder sb = new StringBuilder();
+        sb.append(from == 0 ? "*" : from);
+        sb.append("-");
+        sb.append(Double.isInfinite(to) ? "*" : to);
+        return sb.toString();
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (geoPointParser.token(aggregationName, currentFieldName, token, parser, parseFieldMatcher, otherOptions)) {
-            return true;
-        } else if (token == XContentParser.Token.VALUE_STRING) {
-            if (parseFieldMatcher.match(currentFieldName, UNIT_FIELD)) {
-                DistanceUnit unit = DistanceUnit.fromString(parser.text());
-                otherOptions.put(UNIT_FIELD, unit);
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, DISTANCE_TYPE_FIELD)) {
-                GeoDistance distanceType = GeoDistance.fromString(parser.text());
-                otherOptions.put(DISTANCE_TYPE_FIELD, distanceType);
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, RangeAggregator.KEYED_FIELD)) {
-                boolean keyed = parser.booleanValue();
-                otherOptions.put(RangeAggregator.KEYED_FIELD, keyed);
-                return true;
-            }
-        } else if (token == XContentParser.Token.START_ARRAY) {
-            if (parseFieldMatcher.match(currentFieldName, RangeAggregator.RANGES_FIELD)) {
-                List<Range> ranges = new ArrayList<>();
-                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                    String fromAsStr = null;
-                    String toAsStr = null;
-                    double from = 0.0;
-                    double to = Double.POSITIVE_INFINITY;
-                    String key = null;
-                    String toOrFromOrKey = null;
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        if (token == XContentParser.Token.FIELD_NAME) {
-                            toOrFromOrKey = parser.currentName();
-                        } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                            if (parseFieldMatcher.match(toOrFromOrKey, Range.FROM_FIELD)) {
-                                from = parser.doubleValue();
-                            } else if (parseFieldMatcher.match(toOrFromOrKey, Range.TO_FIELD)) {
-                                to = parser.doubleValue();
-                            }
-                        } else if (token == XContentParser.Token.VALUE_STRING) {
-                            if (parseFieldMatcher.match(toOrFromOrKey, Range.KEY_FIELD)) {
-                                key = parser.text();
-                            } else if (parseFieldMatcher.match(toOrFromOrKey, Range.FROM_FIELD)) {
-                                fromAsStr = parser.text();
-                            } else if (parseFieldMatcher.match(toOrFromOrKey, Range.TO_FIELD)) {
-                                toAsStr = parser.text();
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.GeoPoint> vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoDistance.TYPE, context).build();
+
+        GeoPointParser geoPointParser = new GeoPointParser(aggregationName, InternalGeoDistance.TYPE, context, ORIGIN_FIELD);
+
+        List<RangeAggregator.Range> ranges = null;
+        DistanceUnit unit = DistanceUnit.DEFAULT;
+        GeoDistance distanceType = GeoDistance.DEFAULT;
+        boolean keyed = false;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (geoPointParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_STRING) {
+                if ("unit".equals(currentFieldName)) {
+                    unit = DistanceUnit.fromString(parser.text());
+                } else if ("distance_type".equals(currentFieldName) || "distanceType".equals(currentFieldName)) {
+                    distanceType = GeoDistance.fromString(parser.text());
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if ("ranges".equals(currentFieldName)) {
+                    ranges = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        String fromAsStr = null;
+                        String toAsStr = null;
+                        double from = 0.0;
+                        double to = Double.POSITIVE_INFINITY;
+                        String key = null;
+                        String toOrFromOrKey = null;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                toOrFromOrKey = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    from = parser.doubleValue();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    to = parser.doubleValue();
+                                }
+                            } else if (token == XContentParser.Token.VALUE_STRING) {
+                                if ("key".equals(toOrFromOrKey)) {
+                                    key = parser.text();
+                                } else if ("from".equals(toOrFromOrKey)) {
+                                    fromAsStr = parser.text();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    toAsStr = parser.text();
+                                }
                             }
                         }
+                        ranges.add(new RangeAggregator.Range(key(key, from, to), from, fromAsStr, to, toAsStr));
                     }
-                    if (fromAsStr != null || toAsStr != null) {
-                        ranges.add(new Range(key, Double.parseDouble(fromAsStr), Double.parseDouble(toAsStr)));
-                    } else {
-                        ranges.add(new Range(key, from, to));
-                    }
+                } else  {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-                otherOptions.put(RangeAggregator.RANGES_FIELD, ranges);
-                return true;
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
             }
         }
-        return false;
-    }
-
-    public static class GeoDistanceAggregatorBuilder extends ValuesSourceAggregatorBuilder<ValuesSource.GeoPoint, GeoDistanceAggregatorBuilder> {
-
-        private final GeoPoint origin;
-        private final InternalRange.Factory rangeFactory;
-        private List<Range> ranges = new ArrayList<>();
-        private DistanceUnit unit = DistanceUnit.DEFAULT;
-        private GeoDistance distanceType = GeoDistance.DEFAULT;
-        private boolean keyed = false;
-
-        public GeoDistanceAggregatorBuilder(String name, GeoPoint origin) {
-            this(name, origin, InternalGeoDistance.FACTORY);
-        }
-
-        private GeoDistanceAggregatorBuilder(String name, GeoPoint origin, InternalRange.Factory rangeFactory) {
-            super(name, rangeFactory.type(), rangeFactory.getValueSourceType(), rangeFactory.getValueType());
-            this.origin = origin;
-            this.rangeFactory = rangeFactory;
-        }
 
-        public GeoDistanceAggregatorBuilder addRange(Range range) {
-            ranges.add(range);
-            return this;
+        if (ranges == null) {
+            throw new SearchParseException(context, "Missing [ranges] in geo_distance aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
-        /**
-         * Add a new range to this aggregation.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param from
-         *            the lower bound on the distances, inclusive
-         * @param to
-         *            the upper bound on the distances, exclusive
-         */
-        public GeoDistanceAggregatorBuilder addRange(String key, double from, double to) {
-            ranges.add(new Range(key, from, to));
-            return this;
+        GeoPoint origin = geoPointParser.geoPoint();
+        if (origin == null) {
+            throw new SearchParseException(context, "Missing [origin] in geo_distance aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
-        /**
-         * Same as {@link #addRange(String, double, double)} but the key will be
-         * automatically generated based on <code>from</code> and
-         * <code>to</code>.
-         */
-        public GeoDistanceAggregatorBuilder addRange(double from, double to) {
-            return addRange(null, from, to);
-        }
-
-        /**
-         * Add a new range with no lower bound.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param to
-         *            the upper bound on the distances, exclusive
-         */
-        public GeoDistanceAggregatorBuilder addUnboundedTo(String key, double to) {
-            ranges.add(new Range(key, null, to));
-            return this;
-        }
-
-        /**
-         * Same as {@link #addUnboundedTo(String, double)} but the key will be
-         * computed automatically.
-         */
-        public GeoDistanceAggregatorBuilder addUnboundedTo(double to) {
-            return addUnboundedTo(null, to);
-        }
-
-        /**
-         * Add a new range with no upper bound.
-         *
-         * @param key
-         *            the key to use for this range in the response
-         * @param from
-         *            the lower bound on the distances, inclusive
-         */
-        public GeoDistanceAggregatorBuilder addUnboundedFrom(String key, double from) {
-            addRange(new Range(key, from, null));
-            return this;
-        }
-
-        /**
-         * Same as {@link #addUnboundedFrom(String, double)} but the key will be
-         * computed automatically.
-         */
-        public GeoDistanceAggregatorBuilder addUnboundedFrom(double from) {
-            return addUnboundedFrom(null, from);
-        }
+        return new GeoDistanceFactory(aggregationName, vsParser.config(), InternalGeoDistance.FACTORY, origin, unit, distanceType, ranges, keyed);
+    }
 
-        public List<Range> range() {
-            return ranges;
-        }
+    private static class GeoDistanceFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint> {
 
-        @Override
-        public String getWriteableName() {
-            return InternalGeoDistance.TYPE.name();
-        }
+        private final GeoPoint origin;
+        private final DistanceUnit unit;
+        private final GeoDistance distanceType;
+        private final InternalRange.Factory rangeFactory;
+        private final List<RangeAggregator.Range> ranges;
+        private final boolean keyed;
 
-        public GeoDistanceAggregatorBuilder unit(DistanceUnit unit) {
+        public GeoDistanceFactory(String name, ValuesSourceConfig<ValuesSource.GeoPoint> valueSourceConfig,
+                                  InternalRange.Factory rangeFactory, GeoPoint origin, DistanceUnit unit, GeoDistance distanceType,
+                                  List<RangeAggregator.Range> ranges, boolean keyed) {
+            super(name, rangeFactory.type(), valueSourceConfig);
+            this.origin = origin;
             this.unit = unit;
-            return this;
-        }
-
-        public DistanceUnit unit() {
-            return unit;
-        }
-
-        public GeoDistanceAggregatorBuilder distanceType(GeoDistance distanceType) {
             this.distanceType = distanceType;
-            return this;
-        }
-
-        public GeoDistance distanceType() {
-            return distanceType;
-        }
-
-        public GeoDistanceAggregatorBuilder keyed(boolean keyed) {
+            this.rangeFactory = rangeFactory;
+            this.ranges = ranges;
             this.keyed = keyed;
-            return this;
-        }
-
-        public boolean keyed() {
-            return keyed;
         }
 
         @Override
-        protected ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, ?> innerBuild(AggregationContext context,
-                ValuesSourceConfig<ValuesSource.GeoPoint> config, AggregatorFactory<?> parent, Builder subFactoriesBuilder)
-                        throws IOException {
-            return new GeoDistanceRangeAggregatorFactory(name, type, config, origin, ranges, unit, distanceType, keyed, context, parent,
-                    subFactoriesBuilder, metaData);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            return new Unmapped(name, ranges, keyed, config.format(), aggregationContext, parent, rangeFactory, pipelineAggregators,
+                    metaData);
         }
 
         @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(ORIGIN_FIELD.getPreferredName(), origin);
-            builder.field(RangeAggregator.RANGES_FIELD.getPreferredName(), ranges);
-            builder.field(RangeAggregator.KEYED_FIELD.getPreferredName(), keyed);
-            builder.field(UNIT_FIELD.getPreferredName(), unit);
-            builder.field(DISTANCE_TYPE_FIELD.getPreferredName(), distanceType);
-            return builder;
-        }
+        protected Aggregator doCreateInternal(final ValuesSource.GeoPoint valuesSource, AggregationContext aggregationContext,
+                Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData)
+                throws IOException {
+            DistanceSource distanceSource = new DistanceSource(valuesSource, distanceType, origin, unit);
+            return new RangeAggregator(name, factories, distanceSource, config.format(), rangeFactory, ranges, keyed, aggregationContext,
+                    parent,
+                    pipelineAggregators, metaData);
+        }
+
+        private static class DistanceSource extends ValuesSource.Numeric {
+
+            private final ValuesSource.GeoPoint source;
+            private final GeoDistance distanceType;
+            private final DistanceUnit unit;
+            private final org.elasticsearch.common.geo.GeoPoint origin;
+
+            public DistanceSource(ValuesSource.GeoPoint source, GeoDistance distanceType, org.elasticsearch.common.geo.GeoPoint origin, DistanceUnit unit) {
+                this.source = source;
+                // even if the geo points are unique, there's no guarantee the distances are
+                this.distanceType = distanceType;
+                this.unit = unit;
+                this.origin = origin;
+            }
 
-        @Override
-        protected GeoDistanceAggregatorBuilder innerReadFrom(
-                String name, ValuesSourceType valuesSourceType, ValueType targetValueType, StreamInput in) throws IOException {
-            GeoPoint origin = new GeoPoint(in.readDouble(), in.readDouble());
-            int size = in.readVInt();
-            GeoDistanceAggregatorBuilder factory = new GeoDistanceAggregatorBuilder(name, origin);
-            for (int i = 0; i < size; i++) {
-                factory.addRange(Range.PROTOTYPE.readFrom(in));
+            @Override
+            public boolean isFloatingPoint() {
+                return true;
             }
-            factory.keyed = in.readBoolean();
-            factory.distanceType = GeoDistance.readGeoDistanceFrom(in);
-            factory.unit = DistanceUnit.readDistanceUnit(in);
-            return factory;
-        }
 
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDouble(origin.lat());
-            out.writeDouble(origin.lon());
-            out.writeVInt(ranges.size());
-            for (Range range : ranges) {
-                range.writeTo(out);
+            @Override
+            public SortedNumericDocValues longValues(LeafReaderContext ctx) {
+                throw new UnsupportedOperationException();
             }
-            out.writeBoolean(keyed);
-            distanceType.writeTo(out);
-            DistanceUnit.writeDistanceUnit(out, unit);
-        }
 
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(origin, ranges, keyed, distanceType, unit);
-        }
+            @Override
+            public SortedNumericDoubleValues doubleValues(LeafReaderContext ctx) {
+                final MultiGeoPointValues geoValues = source.geoPointValues(ctx);
+                final FixedSourceDistance distance = distanceType.fixedSourceDistance(origin.getLat(), origin.getLon(), unit);
+                return GeoDistance.distanceValues(geoValues, distance);
+            }
 
-        @Override
-        protected boolean innerEquals(Object obj) {
-            GeoDistanceAggregatorBuilder other = (GeoDistanceAggregatorBuilder) obj;
-            return Objects.equals(origin, other.origin)
-                    && Objects.equals(ranges, other.ranges)
-                    && Objects.equals(keyed, other.keyed)
-                    && Objects.equals(distanceType, other.distanceType)
-                    && Objects.equals(unit, other.unit);
-        }
+            @Override
+            public SortedBinaryDocValues bytesValues(LeafReaderContext ctx) {
+                throw new UnsupportedOperationException();
+            }
 
-    }
+        }
 
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new GeoDistanceAggregatorBuilder(null, null);
     }
 
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceRangeAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceRangeAggregatorFactory.java
deleted file mode 100644
index b5f3fb5..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceRangeAggregatorFactory.java
+++ /dev/null
@@ -1,128 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range.geodistance;
-
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.index.SortedNumericDocValues;
-import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.geo.GeoDistance.FixedSourceDistance;
-import org.elasticsearch.common.unit.DistanceUnit;
-import org.elasticsearch.index.fielddata.MultiGeoPointValues;
-import org.elasticsearch.index.fielddata.SortedBinaryDocValues;
-import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Unmapped;
-import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanceParser.Range;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class GeoDistanceRangeAggregatorFactory
-        extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, GeoDistanceRangeAggregatorFactory> {
-
-    private final InternalRange.Factory<InternalGeoDistance.Bucket, InternalGeoDistance> rangeFactory = InternalGeoDistance.FACTORY;
-    private final GeoPoint origin;
-    private final List<Range> ranges;
-    private final DistanceUnit unit;
-    private final GeoDistance distanceType;
-    private final boolean keyed;
-
-    public GeoDistanceRangeAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource.GeoPoint> config, GeoPoint origin,
-            List<Range> ranges, DistanceUnit unit, GeoDistance distanceType, boolean keyed, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.origin = origin;
-        this.ranges = ranges;
-        this.unit = unit;
-        this.distanceType = distanceType;
-        this.keyed = keyed;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new Unmapped(name, ranges, keyed, config.format(), aggregationContext, parent, rangeFactory, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(final ValuesSource.GeoPoint valuesSource, AggregationContext aggregationContext,
-            Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators,
-            Map<String, Object> metaData)
-            throws IOException {
-        DistanceSource distanceSource = new DistanceSource(valuesSource, distanceType, origin, unit);
-        return new RangeAggregator(name, factories, distanceSource, config.format(), rangeFactory, ranges, keyed, aggregationContext,
-                parent,
-                pipelineAggregators, metaData);
-    }
-
-    private static class DistanceSource extends ValuesSource.Numeric {
-
-        private final ValuesSource.GeoPoint source;
-        private final GeoDistance distanceType;
-        private final DistanceUnit unit;
-        private final org.elasticsearch.common.geo.GeoPoint origin;
-
-        public DistanceSource(ValuesSource.GeoPoint source, GeoDistance distanceType, org.elasticsearch.common.geo.GeoPoint origin,
-                DistanceUnit unit) {
-            this.source = source;
-            // even if the geo points are unique, there's no guarantee the
-            // distances are
-            this.distanceType = distanceType;
-            this.unit = unit;
-            this.origin = origin;
-        }
-
-        @Override
-        public boolean isFloatingPoint() {
-            return true;
-        }
-
-        @Override
-        public SortedNumericDocValues longValues(LeafReaderContext ctx) {
-            throw new UnsupportedOperationException();
-        }
-
-        @Override
-        public SortedNumericDoubleValues doubleValues(LeafReaderContext ctx) {
-            final MultiGeoPointValues geoValues = source.geoPointValues(ctx);
-            final FixedSourceDistance distance = distanceType.fixedSourceDistance(origin.getLat(), origin.getLon(), unit);
-            return GeoDistance.distanceValues(geoValues, distance);
-        }
-
-        @Override
-        public SortedBinaryDocValues bytesValues(LeafReaderContext ctx) {
-            throw new UnsupportedOperationException();
-        }
-
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java
index 2c16d93..c5c67df 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java
@@ -26,8 +26,6 @@ import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -106,18 +104,8 @@ public class InternalGeoDistance extends InternalRange<InternalGeoDistance.Bucke
     public static class Factory extends InternalRange.Factory<InternalGeoDistance.Bucket, InternalGeoDistance> {
 
         @Override
-        public Type type() {
-            return TYPE;
-        }
-
-        @Override
-        public ValuesSourceType getValueSourceType() {
-            return ValuesSourceType.GEOPOINT;
-        }
-
-        @Override
-        public ValueType getValueType() {
-            return ValueType.GEOPOINT;
+        public String type() {
+            return TYPE.name();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeAggregatorBuilder.java
deleted file mode 100644
index f4b7a78..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeAggregatorBuilder.java
+++ /dev/null
@@ -1,280 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range.ipv4;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.network.Cidrs;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.AbstractBuilder;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-
-import java.io.IOException;
-import java.util.Objects;
-
-public class IPv4RangeAggregatorBuilder extends AbstractBuilder<IPv4RangeAggregatorBuilder, IPv4RangeAggregatorBuilder.Range> {
-
-    public IPv4RangeAggregatorBuilder(String name) {
-        super(name, InternalIPv4Range.FACTORY);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return InternalIPv4Range.TYPE.name();
-    }
-
-    /**
-     * Add a new range to this aggregation.
-     *
-     * @param key
-     *            the key to use for this range in the response
-     * @param from
-     *            the lower bound on the distances, inclusive
-     * @param to
-     *            the upper bound on the distances, exclusive
-     */
-    public IPv4RangeAggregatorBuilder addRange(String key, String from, String to) {
-        addRange(new Range(key, from, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addMaskRange(String, String)} but uses the mask itself as
-     * a key.
-     */
-    public IPv4RangeAggregatorBuilder addMaskRange(String key, String mask) {
-        return addRange(new Range(key, mask));
-    }
-
-    /**
-     * Same as {@link #addMaskRange(String, String)} but uses the mask itself as
-     * a key.
-     */
-    public IPv4RangeAggregatorBuilder addMaskRange(String mask) {
-        return addRange(new Range(mask, mask));
-    }
-
-    /**
-     * Same as {@link #addRange(String, String, String)} but the key will be
-     * automatically generated.
-     */
-    public IPv4RangeAggregatorBuilder addRange(String from, String to) {
-        return addRange(null, from, to);
-    }
-
-    /**
-     * Same as {@link #addRange(String, String, String)} but there will be no
-     * lower bound.
-     */
-    public IPv4RangeAggregatorBuilder addUnboundedTo(String key, String to) {
-        addRange(new Range(key, null, to));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedTo(String, String)} but the key will be
-     * generated automatically.
-     */
-    public IPv4RangeAggregatorBuilder addUnboundedTo(String to) {
-        return addUnboundedTo(null, to);
-    }
-
-    /**
-     * Same as {@link #addRange(String, String, String)} but there will be no
-     * upper bound.
-     */
-    public IPv4RangeAggregatorBuilder addUnboundedFrom(String key, String from) {
-        addRange(new Range(key, from, null));
-        return this;
-    }
-
-    /**
-     * Same as {@link #addUnboundedFrom(String, String)} but the key will be
-     * generated automatically.
-     */
-    public IPv4RangeAggregatorBuilder addUnboundedFrom(String from) {
-        return addUnboundedFrom(null, from);
-    }
-
-    @Override
-    protected Ipv4RangeAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-            AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-        return new Ipv4RangeAggregatorFactory(name, type, config, ranges, keyed, rangeFactory, context, parent, subFactoriesBuilder,
-                metaData);
-    }
-
-    @Override
-    protected IPv4RangeAggregatorBuilder createFactoryFromStream(String name, StreamInput in) throws IOException {
-        int size = in.readVInt();
-        IPv4RangeAggregatorBuilder factory = new IPv4RangeAggregatorBuilder(name);
-        for (int i = 0; i < size; i++) {
-            factory.addRange(Range.PROTOTYPE.readFrom(in));
-        }
-        return factory;
-    }
-
-    public static class Range extends RangeAggregator.Range {
-
-        static final Range PROTOTYPE = new Range(null, -1, null, -1, null, null);
-        static final ParseField MASK_FIELD = new ParseField("mask");
-
-        private String cidr;
-
-        public Range(String key, double from, double to) {
-            super(key, from, to);
-        }
-
-        public Range(String key, String from, String to) {
-            super(key, from, to);
-        }
-
-        public Range(String key, String cidr) {
-            super(key, -1, null, -1, null);
-            this.cidr = cidr;
-            if (cidr != null) {
-                parseMaskRange();
-            }
-        }
-
-        private Range(String key, double from, String fromAsStr, double to, String toAsStr, String cidr) {
-            super(key, from, fromAsStr, to, toAsStr);
-            this.cidr = cidr;
-            if (cidr != null) {
-                parseMaskRange();
-            }
-        }
-
-        public String mask() {
-            return cidr;
-        }
-
-        private void parseMaskRange() throws IllegalArgumentException {
-            long[] fromTo = Cidrs.cidrMaskToMinMax(cidr);
-            from = fromTo[0] == 0 ? Double.NEGATIVE_INFINITY : fromTo[0];
-            to = fromTo[1] == InternalIPv4Range.MAX_IP ? Double.POSITIVE_INFINITY : fromTo[1];
-            if (key == null) {
-                key = cidr;
-            }
-        }
-
-        @Override
-        public Range fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-
-            XContentParser.Token token;
-            String currentFieldName = null;
-            double from = Double.NEGATIVE_INFINITY;
-            String fromAsStr = null;
-            double to = Double.POSITIVE_INFINITY;
-            String toAsStr = null;
-            String key = null;
-            String cidr = null;
-            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                if (token == XContentParser.Token.FIELD_NAME) {
-                    currentFieldName = parser.currentName();
-                } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                    if (parseFieldMatcher.match(currentFieldName, FROM_FIELD)) {
-                        from = parser.doubleValue();
-                    } else if (parseFieldMatcher.match(currentFieldName, TO_FIELD)) {
-                        to = parser.doubleValue();
-                    }
-                } else if (token == XContentParser.Token.VALUE_STRING) {
-                    if (parseFieldMatcher.match(currentFieldName, FROM_FIELD)) {
-                        fromAsStr = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, TO_FIELD)) {
-                        toAsStr = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, KEY_FIELD)) {
-                        key = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, MASK_FIELD)) {
-                        cidr = parser.text();
-                    }
-                }
-            }
-            return new Range(key, from, fromAsStr, to, toAsStr, cidr);
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            if (key != null) {
-                builder.field(KEY_FIELD.getPreferredName(), key);
-            }
-            if (cidr != null) {
-                builder.field(MASK_FIELD.getPreferredName(), cidr);
-            } else {
-                if (Double.isFinite(from)) {
-                    builder.field(FROM_FIELD.getPreferredName(), from);
-                }
-                if (Double.isFinite(to)) {
-                    builder.field(TO_FIELD.getPreferredName(), to);
-                }
-                if (fromAsStr != null) {
-                    builder.field(FROM_FIELD.getPreferredName(), fromAsStr);
-                }
-                if (toAsStr != null) {
-                    builder.field(TO_FIELD.getPreferredName(), toAsStr);
-                }
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        public Range readFrom(StreamInput in) throws IOException {
-            String key = in.readOptionalString();
-            String fromAsStr = in.readOptionalString();
-            String toAsStr = in.readOptionalString();
-            double from = in.readDouble();
-            double to = in.readDouble();
-            String mask = in.readOptionalString();
-            return new Range(key, from, fromAsStr, to, toAsStr, mask);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(key);
-            out.writeOptionalString(fromAsStr);
-            out.writeOptionalString(toAsStr);
-            out.writeDouble(from);
-            out.writeDouble(to);
-            out.writeOptionalString(cidr);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(super.hashCode(), cidr);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            return super.equals(obj)
-                    && Objects.equals(cidr, ((Range) obj).cidr);
-        }
-
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeBuilder.java
new file mode 100644
index 0000000..5ac3f2a
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeBuilder.java
@@ -0,0 +1,110 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.range.ipv4;
+
+import org.elasticsearch.common.network.Cidrs;
+import org.elasticsearch.search.aggregations.bucket.range.AbstractRangeBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilderException;
+
+/**
+ * Builder for the {@code IPv4Range} aggregation.
+ */
+public class IPv4RangeBuilder extends AbstractRangeBuilder<IPv4RangeBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public IPv4RangeBuilder(String name) {
+        super(name, InternalIPv4Range.TYPE.name());
+    }
+
+    /**
+     * Add a new range to this aggregation.
+     *
+     * @param key  the key to use for this range in the response
+     * @param from the lower bound on the distances, inclusive
+     * @param to   the upper bound on the distances, exclusive
+     */
+    public IPv4RangeBuilder addRange(String key, String from, String to) {
+        ranges.add(new Range(key, from, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addMaskRange(String, String)} but uses the mask itself as a key.
+     */
+    public IPv4RangeBuilder addMaskRange(String mask) {
+        return addMaskRange(mask, mask);
+    }
+
+    /**
+     * Add a range based on a CIDR mask.
+     */
+    public IPv4RangeBuilder addMaskRange(String key, String mask) {
+        long[] fromTo;
+        try {
+            fromTo = Cidrs.cidrMaskToMinMax(mask);
+        } catch (IllegalArgumentException e) {
+            throw new SearchSourceBuilderException("invalid CIDR mask [" + mask + "] in ip_range aggregation [" + getName() + "]", e);
+        }
+        ranges.add(new Range(key, fromTo[0] == 0 ? null : fromTo[0], fromTo[1] == InternalIPv4Range.MAX_IP ? null : fromTo[1]));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addRange(String, String, String)} but the key will be
+     * automatically generated.
+     */
+    public IPv4RangeBuilder addRange(String from, String to) {
+        return addRange(null, from, to);
+    }
+
+    /**
+     * Same as {@link #addRange(String, String, String)} but there will be no lower bound.
+     */
+    public IPv4RangeBuilder addUnboundedTo(String key, String to) {
+        ranges.add(new Range(key, null, to));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedTo(String, String)} but the key will be
+     * generated automatically.
+     */
+    public IPv4RangeBuilder addUnboundedTo(String to) {
+        return addUnboundedTo(null, to);
+    }
+
+    /**
+     * Same as {@link #addRange(String, String, String)} but there will be no upper bound.
+     */
+    public IPv4RangeBuilder addUnboundedFrom(String key, String from) {
+        ranges.add(new Range(key, from, null));
+        return this;
+    }
+
+    /**
+     * Same as {@link #addUnboundedFrom(String, String)} but the key will be
+     * generated automatically.
+     */
+    public IPv4RangeBuilder addUnboundedFrom(String from) {
+        return addUnboundedFrom(null, from);
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java
index a6c3ed3..e20d1ac 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java
@@ -26,7 +26,6 @@ import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -112,13 +111,8 @@ public class InternalIPv4Range extends InternalRange<InternalIPv4Range.Bucket, I
     public static class Factory extends InternalRange.Factory<InternalIPv4Range.Bucket, InternalIPv4Range> {
 
         @Override
-        public Type type() {
-            return TYPE;
-        }
-
-        @Override
-        public ValueType getValueType() {
-            return ValueType.IP;
+        public String type() {
+            return TYPE.name();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java
index 0381ff0..dc1e2a6 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java
@@ -18,28 +18,25 @@
  */
 package org.elasticsearch.search.aggregations.bucket.range.ipv4;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.network.Cidrs;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.bucket.range.RangeParser;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.List;
-import java.util.Map;
 
 /**
  *
  */
-public class IpRangeParser extends RangeParser {
-
-    public IpRangeParser() {
-        super(true, false, false);
-    }
+public class IpRangeParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -47,29 +44,99 @@ public class IpRangeParser extends RangeParser {
     }
 
     @Override
-    protected Range parseRange(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-        return IPv4RangeAggregatorBuilder.Range.PROTOTYPE.fromXContent(parser, parseFieldMatcher);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalIPv4Range.TYPE, context)
+                .targetValueType(ValueType.IP)
+                .formattable(false)
+                .build();
+
+        List<RangeAggregator.Range> ranges = null;
+        boolean keyed = false;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if ("ranges".equals(currentFieldName)) {
+                    ranges = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        double from = Double.NEGATIVE_INFINITY;
+                        String fromAsStr = null;
+                        double to = Double.POSITIVE_INFINITY;
+                        String toAsStr = null;
+                        String key = null;
+                        String mask = null;
+                        String toOrFromOrMaskOrKey = null;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                toOrFromOrMaskOrKey = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if ("from".equals(toOrFromOrMaskOrKey)) {
+                                    from = parser.doubleValue();
+                                } else if ("to".equals(toOrFromOrMaskOrKey)) {
+                                    to = parser.doubleValue();
+                                }
+                            } else if (token == XContentParser.Token.VALUE_STRING) {
+                                if ("from".equals(toOrFromOrMaskOrKey)) {
+                                    fromAsStr = parser.text();
+                                } else if ("to".equals(toOrFromOrMaskOrKey)) {
+                                    toAsStr = parser.text();
+                                } else if ("key".equals(toOrFromOrMaskOrKey)) {
+                                    key = parser.text();
+                                } else if ("mask".equals(toOrFromOrMaskOrKey)) {
+                                    mask = parser.text();
+                                }
+                            }
+                        }
+                        RangeAggregator.Range range = new RangeAggregator.Range(key, from, fromAsStr, to, toAsStr);
+                        if (mask != null) {
+                            parseMaskRange(mask, range, aggregationName, context);
+                        }
+                        ranges.add(range);
+                    }
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
+        }
 
-    @Override
-    protected IPv4RangeAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        IPv4RangeAggregatorBuilder factory = new IPv4RangeAggregatorBuilder(aggregationName);
-        List<IPv4RangeAggregatorBuilder.Range> ranges = (List<IPv4RangeAggregatorBuilder.Range>) otherOptions
-                .get(RangeAggregator.RANGES_FIELD);
-        for (IPv4RangeAggregatorBuilder.Range range : ranges) {
-            factory.addRange(range);
+        if (ranges == null) {
+            throw new SearchParseException(context, "Missing [ranges] in ranges aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
-        Boolean keyed = (Boolean) otherOptions.get(RangeAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
+
+        return new RangeAggregator.Factory(aggregationName, vsParser.config(), InternalIPv4Range.FACTORY, ranges, keyed);
+    }
+
+    private static void parseMaskRange(String cidr, RangeAggregator.Range range, String aggregationName, SearchContext ctx) {
+        long[] fromTo;
+        try {
+            fromTo = Cidrs.cidrMaskToMinMax(cidr);
+        } catch (IllegalArgumentException e) {
+            throw new SearchParseException(ctx, "invalid CIDR mask [" + cidr + "] in aggregation [" + aggregationName + "]",
+                    null, e);
         }
-        return factory;
+        range.from = fromTo[0] == 0 ? Double.NEGATIVE_INFINITY : fromTo[0];
+        range.to = fromTo[1] == InternalIPv4Range.MAX_IP ? Double.POSITIVE_INFINITY : fromTo[1];
+        if (range.key == null) {
+            range.key = cidr;
         }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new IPv4RangeAggregatorBuilder(null);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/Ipv4RangeAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/Ipv4RangeAggregatorFactory.java
deleted file mode 100644
index 1c05935..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/Ipv4RangeAggregatorFactory.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range.ipv4;
-
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.range.AbstractRangeAggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.range.InternalRange.Factory;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class Ipv4RangeAggregatorFactory
-        extends AbstractRangeAggregatorFactory<Ipv4RangeAggregatorFactory, IPv4RangeAggregatorBuilder.Range> {
-
-    public Ipv4RangeAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config,
-            List<IPv4RangeAggregatorBuilder.Range> ranges, boolean keyed, Factory<?, ?> rangeFactory, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, ranges, keyed, rangeFactory, context, parent, subFactoriesBuilder, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedAggregatorFactory.java
deleted file mode 100644
index 42527a9..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedAggregatorFactory.java
+++ /dev/null
@@ -1,101 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.sampler;
-
-import org.elasticsearch.search.aggregations.AggregationExecutionException;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator.ExecutionMode;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class DiversifiedAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource, DiversifiedAggregatorFactory> {
-
-    private final int shardSize;
-    private final int maxDocsPerValue;
-    private final String executionHint;
-
-    public DiversifiedAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource> config, int shardSize, int maxDocsPerValue,
-            String executionHint, AggregationContext context, AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder,
-            Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.shardSize = shardSize;
-        this.maxDocsPerValue = maxDocsPerValue;
-        this.executionHint = executionHint;
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource valuesSource, AggregationContext context, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-
-        if (valuesSource instanceof ValuesSource.Numeric) {
-            return new DiversifiedNumericSamplerAggregator(name, shardSize, factories, context, parent, pipelineAggregators, metaData,
-                    (Numeric) valuesSource, maxDocsPerValue);
-        }
-
-        if (valuesSource instanceof ValuesSource.Bytes) {
-            ExecutionMode execution = null;
-            if (executionHint != null) {
-                execution = ExecutionMode.fromString(executionHint, context.searchContext().parseFieldMatcher());
-            }
-
-            // In some cases using ordinals is just not supported: override
-            // it
-            if (execution == null) {
-                execution = ExecutionMode.GLOBAL_ORDINALS;
-            }
-            if ((execution.needsGlobalOrdinals()) && (!(valuesSource instanceof ValuesSource.Bytes.WithOrdinals))) {
-                execution = ExecutionMode.MAP;
-            }
-            return execution.create(name, factories, shardSize, maxDocsPerValue, valuesSource, context, parent, pipelineAggregators,
-                    metaData);
-        }
-
-        throw new AggregationExecutionException("Sampler aggregation cannot be applied to field [" + config.fieldContext().field()
-                + "]. It can only be applied to numeric or string fields.");
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        final UnmappedSampler aggregation = new UnmappedSampler(name, pipelineAggregators, metaData);
-
-        return new NonCollectingAggregator(name, aggregationContext, parent, factories, pipelineAggregators, metaData) {
-            @Override
-            public InternalAggregation buildEmptyAggregation() {
-                return aggregation;
-            }
-        };
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedSamplerParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedSamplerParser.java
deleted file mode 100644
index fa48ab2..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedSamplerParser.java
+++ /dev/null
@@ -1,94 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations.bucket.sampler;
-
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
-
-/**
- *
- */
-public class DiversifiedSamplerParser extends AnyValuesSourceParser {
-
-    public DiversifiedSamplerParser() {
-        super(true, false);
-    }
-
-    @Override
-    public String type() {
-        return SamplerAggregator.DiversifiedAggregatorBuilder.TYPE.name();
-    }
-
-    @Override
-    protected SamplerAggregator.DiversifiedAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        SamplerAggregator.DiversifiedAggregatorBuilder factory = new SamplerAggregator.DiversifiedAggregatorBuilder(aggregationName);
-        Integer shardSize = (Integer) otherOptions.get(SamplerAggregator.SHARD_SIZE_FIELD);
-        if (shardSize != null) {
-            factory.shardSize(shardSize);
-        }
-        Integer maxDocsPerValue = (Integer) otherOptions.get(SamplerAggregator.MAX_DOCS_PER_VALUE_FIELD);
-        if (maxDocsPerValue != null) {
-            factory.maxDocsPerValue(maxDocsPerValue);
-        }
-        String executionHint = (String) otherOptions.get(SamplerAggregator.EXECUTION_HINT_FIELD);
-        if (executionHint != null) {
-            factory.executionHint(executionHint);
-        }
-        return factory;
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.VALUE_NUMBER) {
-            if (parseFieldMatcher.match(currentFieldName, SamplerAggregator.SHARD_SIZE_FIELD)) {
-                int shardSize = parser.intValue();
-                otherOptions.put(SamplerAggregator.SHARD_SIZE_FIELD, shardSize);
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SamplerAggregator.MAX_DOCS_PER_VALUE_FIELD)) {
-                int maxDocsPerValue = parser.intValue();
-                otherOptions.put(SamplerAggregator.MAX_DOCS_PER_VALUE_FIELD, maxDocsPerValue);
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_STRING) {
-            if (parseFieldMatcher.match(currentFieldName, SamplerAggregator.EXECUTION_HINT_FIELD)) {
-                String executionHint = parser.text();
-                otherOptions.put(SamplerAggregator.EXECUTION_HINT_FIELD, executionHint);
-                return true;
-            }
-        }
-        return false;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new SamplerAggregator.DiversifiedAggregatorBuilder(null);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregationBuilder.java
new file mode 100644
index 0000000..a623735
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregationBuilder.java
@@ -0,0 +1,80 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.sampler;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Sampler} aggregation.
+ */
+public class SamplerAggregationBuilder extends ValuesSourceAggregationBuilder<SamplerAggregationBuilder> {
+
+    private int shardSize = SamplerParser.DEFAULT_SHARD_SAMPLE_SIZE;
+
+    int maxDocsPerValue = SamplerParser.MAX_DOCS_PER_VALUE_DEFAULT;
+    String executionHint = null;
+
+    /**
+     * Sole constructor.
+     */
+    public SamplerAggregationBuilder(String name) {
+        super(name, InternalSampler.TYPE.name());
+    }
+
+    /**
+     * Set the max num docs to be returned from each shard.
+     */
+    public SamplerAggregationBuilder shardSize(int shardSize) {
+        this.shardSize = shardSize;
+        return this;
+    }
+
+    public SamplerAggregationBuilder maxDocsPerValue(int maxDocsPerValue) {
+        this.maxDocsPerValue = maxDocsPerValue;
+        return this;
+    }
+
+    public SamplerAggregationBuilder executionHint(String executionHint) {
+        this.executionHint = executionHint;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        // builder.startObject();
+        if (shardSize != SamplerParser.DEFAULT_SHARD_SAMPLE_SIZE) {
+            builder.field(SamplerParser.SHARD_SIZE_FIELD.getPreferredName(), shardSize);
+        }
+
+        if (maxDocsPerValue != SamplerParser.MAX_DOCS_PER_VALUE_DEFAULT) {
+            builder.field(SamplerParser.MAX_DOCS_PER_VALUE_FIELD.getPreferredName(), maxDocsPerValue);
+        }
+        if (executionHint != null) {
+            builder.field(SamplerParser.EXECUTION_HINT_FIELD.getPreferredName(), executionHint);
+        }
+
+        return builder;
+    }
+
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java
index f9c156e..8cb9809 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java
@@ -21,34 +21,27 @@ package org.elasticsearch.search.aggregations.bucket.sampler;
 import org.apache.lucene.index.LeafReaderContext;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
+import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.BestDocsDeferringCollector;
 import org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Aggregate on only the top-scoring docs on a shard.
@@ -62,10 +55,6 @@ import java.util.Objects;
  */
 public class SamplerAggregator extends SingleBucketAggregator {
 
-    public static final ParseField SHARD_SIZE_FIELD = new ParseField("shard_size");
-    public static final ParseField MAX_DOCS_PER_VALUE_FIELD = new ParseField("max_docs_per_value");
-    public static final ParseField EXECUTION_HINT_FIELD = new ParseField("execution_hint");
-
 
     public enum ExecutionMode {
 
@@ -191,175 +180,82 @@ public class SamplerAggregator extends SingleBucketAggregator {
         return new InternalSampler(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
     }
 
-    public static class SamplerAggregatorBuilder extends AggregatorBuilder<SamplerAggregatorBuilder> {
+    public static class Factory extends AggregatorFactory {
 
-        public static final int DEFAULT_SHARD_SAMPLE_SIZE = 100;
+        private int shardSize;
 
-        private int shardSize = DEFAULT_SHARD_SAMPLE_SIZE;
-
-        public SamplerAggregatorBuilder(String name) {
-            super(name, InternalSampler.TYPE);
-        }
-
-        /**
-         * Set the max num docs to be returned from each shard.
-         */
-        public SamplerAggregatorBuilder shardSize(int shardSize) {
+        public Factory(String name, int shardSize) {
+            super(name, InternalSampler.TYPE.name());
             this.shardSize = shardSize;
-            return this;
-        }
-
-        /**
-         * Get the max num docs to be returned from each shard.
-         */
-        public int shardSize() {
-            return shardSize;
-        }
-
-        @Override
-        protected SamplerAggregatorFactory doBuild(AggregationContext context, AggregatorFactory<?> parent, Builder subFactoriesBuilder)
-                throws IOException {
-            return new SamplerAggregatorFactory(name, type, shardSize, context, parent, subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            builder.field(SHARD_SIZE_FIELD.getPreferredName(), shardSize);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected SamplerAggregatorBuilder doReadFrom(String name, StreamInput in) throws IOException {
-            SamplerAggregatorBuilder factory = new SamplerAggregatorBuilder(name);
-            factory.shardSize = in.readVInt();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(shardSize);
         }
 
         @Override
-        protected int doHashCode() {
-            return Objects.hash(shardSize);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            SamplerAggregatorBuilder other = (SamplerAggregatorBuilder) obj;
-            return Objects.equals(shardSize, other.shardSize);
+        public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new SamplerAggregator(name, shardSize, factories, context, parent, pipelineAggregators, metaData);
         }
 
     }
 
-    public static class DiversifiedAggregatorBuilder extends ValuesSourceAggregatorBuilder<ValuesSource, DiversifiedAggregatorBuilder> {
-
-        public static final Type TYPE = new Type("diversified_sampler");
-
-        public static final int MAX_DOCS_PER_VALUE_DEFAULT = 1;
-
-        private int shardSize = SamplerAggregatorBuilder.DEFAULT_SHARD_SAMPLE_SIZE;
-        private int maxDocsPerValue = MAX_DOCS_PER_VALUE_DEFAULT;
-        private String executionHint = null;
+    public static class DiversifiedFactory extends ValuesSourceAggregatorFactory<ValuesSource> {
 
-        public DiversifiedAggregatorBuilder(String name) {
-            super(name, TYPE, ValuesSourceType.ANY, null);
-        }
+        private int shardSize;
+        private int maxDocsPerValue;
+        private String executionHint;
 
-        /**
-         * Set the max num docs to be returned from each shard.
-         */
-        public DiversifiedAggregatorBuilder shardSize(int shardSize) {
+        public DiversifiedFactory(String name, int shardSize, String executionHint, ValuesSourceConfig vsConfig, int maxDocsPerValue) {
+            super(name, InternalSampler.TYPE.name(), vsConfig);
             this.shardSize = shardSize;
-            return this;
-        }
-
-        /**
-         * Get the max num docs to be returned from each shard.
-         */
-        public int shardSize() {
-            return shardSize;
-        }
-
-        /**
-         * Set the max num docs to be returned per value.
-         */
-        public DiversifiedAggregatorBuilder maxDocsPerValue(int maxDocsPerValue) {
             this.maxDocsPerValue = maxDocsPerValue;
-            return this;
-        }
-
-        /**
-         * Get the max num docs to be returned per value.
-         */
-        public int maxDocsPerValue() {
-            return maxDocsPerValue;
-        }
-
-        /**
-         * Set the execution hint.
-         */
-        public DiversifiedAggregatorBuilder executionHint(String executionHint) {
             this.executionHint = executionHint;
-            return this;
-        }
-
-        /**
-         * Get the execution hint.
-         */
-        public String executionHint() {
-            return executionHint;
         }
 
         @Override
-        protected ValuesSourceAggregatorFactory<ValuesSource, ?> innerBuild(AggregationContext context,
-                ValuesSourceConfig<ValuesSource> config, AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new DiversifiedAggregatorFactory(name, TYPE, config, shardSize, maxDocsPerValue, executionHint, context, parent,
-                    subFactoriesBuilder, metaData);
-        }
+        protected Aggregator doCreateInternal(ValuesSource valuesSource, AggregationContext context, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(SHARD_SIZE_FIELD.getPreferredName(), shardSize);
-            builder.field(MAX_DOCS_PER_VALUE_FIELD.getPreferredName(), maxDocsPerValue);
-            if (executionHint != null) {
-                builder.field(EXECUTION_HINT_FIELD.getPreferredName(), executionHint);
+            if (valuesSource instanceof ValuesSource.Numeric) {
+                return new DiversifiedNumericSamplerAggregator(name, shardSize, factories, context, parent, pipelineAggregators, metaData,
+                        (Numeric) valuesSource, maxDocsPerValue);
             }
-            return builder;
-        }
 
-        @Override
-        protected DiversifiedAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            DiversifiedAggregatorBuilder factory = new DiversifiedAggregatorBuilder(name);
-            factory.shardSize = in.readVInt();
-            factory.maxDocsPerValue = in.readVInt();
-            factory.executionHint = in.readOptionalString();
-            return factory;
-        }
+            if (valuesSource instanceof ValuesSource.Bytes) {
+                ExecutionMode execution = null;
+                if (executionHint != null) {
+                    execution = ExecutionMode.fromString(executionHint, context.searchContext().parseFieldMatcher());
+                }
 
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(shardSize);
-            out.writeVInt(maxDocsPerValue);
-            out.writeOptionalString(executionHint);
-        }
+                // In some cases using ordinals is just not supported: override
+                // it
+                if(execution==null){
+                    execution = ExecutionMode.GLOBAL_ORDINALS;
+                }
+                if ((execution.needsGlobalOrdinals()) && (!(valuesSource instanceof ValuesSource.Bytes.WithOrdinals))) {
+                    execution = ExecutionMode.MAP;
+                }
+                return execution.create(name, factories, shardSize, maxDocsPerValue, valuesSource, context, parent, pipelineAggregators,
+                        metaData);
+            }
 
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(shardSize, maxDocsPerValue, executionHint);
+            throw new AggregationExecutionException("Sampler aggregation cannot be applied to field [" + config.fieldContext().field() +
+                    "]. It can only be applied to numeric or string fields.");
         }
 
         @Override
-        protected boolean innerEquals(Object obj) {
-            DiversifiedAggregatorBuilder other = (DiversifiedAggregatorBuilder) obj;
-            return Objects.equals(shardSize, other.shardSize)
-                    && Objects.equals(maxDocsPerValue, other.maxDocsPerValue)
-                    && Objects.equals(executionHint, other.executionHint);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            final UnmappedSampler aggregation = new UnmappedSampler(name, pipelineAggregators, metaData);
+
+            return new NonCollectingAggregator(name, aggregationContext, parent, factories, pipelineAggregators, metaData) {
+                @Override
+                public InternalAggregation buildEmptyAggregation() {
+                    return aggregation;
+                }
+            };
         }
+
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregatorFactory.java
deleted file mode 100644
index cce3a8738..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregatorFactory.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.sampler;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class SamplerAggregatorFactory extends AggregatorFactory<SamplerAggregatorFactory> {
-
-    private final int shardSize;
-
-    public SamplerAggregatorFactory(String name, Type type, int shardSize, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactories, Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactories, metaData);
-        this.shardSize = shardSize;
-    }
-
-    @Override
-    public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new SamplerAggregator(name, shardSize, factories, context, parent, pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java
index 0f96eb3..498a7cb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java
@@ -18,12 +18,14 @@
  */
 package org.elasticsearch.search.aggregations.bucket.sampler;
 
-
-import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -32,44 +34,73 @@ import java.io.IOException;
  */
 public class SamplerParser implements Aggregator.Parser {
 
+    public static final int DEFAULT_SHARD_SAMPLE_SIZE = 100;
+    public static final ParseField SHARD_SIZE_FIELD = new ParseField("shard_size");
+    public static final ParseField MAX_DOCS_PER_VALUE_FIELD = new ParseField("max_docs_per_value");
+    public static final ParseField EXECUTION_HINT_FIELD = new ParseField("execution_hint");
+    public static final boolean DEFAULT_USE_GLOBAL_ORDINALS = false;
+    public static final int MAX_DOCS_PER_VALUE_DEFAULT = 1;
+
+
     @Override
     public String type() {
         return InternalSampler.TYPE.name();
     }
 
     @Override
-    public AggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
         XContentParser.Token token;
         String currentFieldName = null;
-        Integer shardSize = null;
+        String executionHint = null;
+        int shardSize = DEFAULT_SHARD_SAMPLE_SIZE;
+        int maxDocsPerValue = MAX_DOCS_PER_VALUE_DEFAULT;
+        ValuesSourceParser vsParser = null;
+        boolean diversityChoiceMade = false;
+
+        vsParser = ValuesSourceParser.any(aggregationName, InternalSampler.TYPE, context).scriptable(true).formattable(false).build();
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
             } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                if (context.parseFieldMatcher().match(currentFieldName, SamplerAggregator.SHARD_SIZE_FIELD)) {
+                if (context.parseFieldMatcher().match(currentFieldName, SHARD_SIZE_FIELD)) {
                     shardSize = parser.intValue();
+                } else if (context.parseFieldMatcher().match(currentFieldName, MAX_DOCS_PER_VALUE_FIELD)) {
+                    diversityChoiceMade = true;
+                    maxDocsPerValue = parser.intValue();
+                } else {
+                    throw new SearchParseException(context, "Unsupported property \"" + currentFieldName + "\" for aggregation \""
+                            + aggregationName, parser.getTokenLocation());
+                }
+            } else if (!vsParser.token(currentFieldName, token, parser)) {
+                if (context.parseFieldMatcher().match(currentFieldName, EXECUTION_HINT_FIELD)) {
+                    executionHint = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unsupported property \"" + currentFieldName + "\" for aggregation \"" + aggregationName);
+                    throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                            parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unsupported property \"" + currentFieldName + "\" for aggregation \"" + aggregationName);
+                throw new SearchParseException(context, "Unsupported property \"" + currentFieldName + "\" for aggregation \""
+                        + aggregationName, parser.getTokenLocation());
             }
         }
 
-        SamplerAggregator.SamplerAggregatorBuilder factory = new SamplerAggregator.SamplerAggregatorBuilder(aggregationName);
-        if (shardSize != null) {
-            factory.shardSize(shardSize);
+        ValuesSourceConfig vsConfig = vsParser.config();
+        if (vsConfig.valid()) {
+            return new SamplerAggregator.DiversifiedFactory(aggregationName, shardSize, executionHint, vsConfig, maxDocsPerValue);
+        } else {
+            if (diversityChoiceMade) {
+                throw new SearchParseException(context, "Sampler aggregation has " + MAX_DOCS_PER_VALUE_FIELD.getPreferredName()
+                        + " setting but no \"field\" or \"script\" setting to provide values for aggregation \"" + aggregationName + "\"",
+                        parser.getTokenLocation());
+
+            }
+            return new SamplerAggregator.Factory(aggregationName, shardSize);
         }
-        return factory;
     }
 
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new SamplerAggregator.SamplerAggregatorBuilder(null);
-    }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/GlobalOrdinalsSignificantTermsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/GlobalOrdinalsSignificantTermsAggregator.java
index 7409a61..e489799 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/GlobalOrdinalsSignificantTermsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/GlobalOrdinalsSignificantTermsAggregator.java
@@ -27,7 +27,6 @@ import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
 import org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
@@ -48,17 +47,15 @@ public class GlobalOrdinalsSignificantTermsAggregator extends GlobalOrdinalsStri
 
     protected long numCollectedDocs;
     protected final SignificantTermsAggregatorFactory termsAggFactory;
-    private final SignificanceHeuristic significanceHeuristic;
 
     public GlobalOrdinalsSignificantTermsAggregator(String name, AggregatorFactories factories,
             ValuesSource.Bytes.WithOrdinals.FieldData valuesSource, BucketCountThresholds bucketCountThresholds,
             IncludeExclude.OrdinalsFilter includeExclude, AggregationContext aggregationContext, Aggregator parent,
-            SignificanceHeuristic significanceHeuristic, SignificantTermsAggregatorFactory termsAggFactory,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            SignificantTermsAggregatorFactory termsAggFactory, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+            throws IOException {
 
         super(name, factories, valuesSource, null, bucketCountThresholds, includeExclude, aggregationContext, parent,
                 SubAggCollectionMode.DEPTH_FIRST, false, pipelineAggregators, metaData);
-        this.significanceHeuristic = significanceHeuristic;
         this.termsAggFactory = termsAggFactory;
     }
 
@@ -120,7 +117,7 @@ public class GlobalOrdinalsSignificantTermsAggregator extends GlobalOrdinalsStri
             // that are for this shard only
             // Back at the central reducer these properties will be updated with
             // global stats
-            spare.updateScore(significanceHeuristic);
+            spare.updateScore(termsAggFactory.getSignificanceHeuristic());
             spare = (SignificantStringTerms.Bucket) ordered.insertWithOverflow(spare);
         }
 
@@ -134,7 +131,7 @@ public class GlobalOrdinalsSignificantTermsAggregator extends GlobalOrdinalsStri
         }
 
         return new SignificantStringTerms(subsetSize, supersetSize, name, bucketCountThresholds.getRequiredSize(),
-                bucketCountThresholds.getMinDocCount(), significanceHeuristic, Arrays.asList(list), pipelineAggregators(),
+                bucketCountThresholds.getMinDocCount(), termsAggFactory.getSignificanceHeuristic(), Arrays.asList(list), pipelineAggregators(),
                 metaData());
     }
 
@@ -145,7 +142,7 @@ public class GlobalOrdinalsSignificantTermsAggregator extends GlobalOrdinalsStri
         IndexReader topReader = searcher.getIndexReader();
         int supersetSize = topReader.numDocs();
         return new SignificantStringTerms(0, supersetSize, name, bucketCountThresholds.getRequiredSize(),
-                bucketCountThresholds.getMinDocCount(), significanceHeuristic,
+                bucketCountThresholds.getMinDocCount(), termsAggFactory.getSignificanceHeuristic(),
                 Collections.<InternalSignificantTerms.Bucket> emptyList(), pipelineAggregators(), metaData());
     }
 
@@ -160,11 +157,10 @@ public class GlobalOrdinalsSignificantTermsAggregator extends GlobalOrdinalsStri
 
         public WithHash(String name, AggregatorFactories factories, ValuesSource.Bytes.WithOrdinals.FieldData valuesSource,
                 BucketCountThresholds bucketCountThresholds, IncludeExclude.OrdinalsFilter includeExclude,
-                AggregationContext aggregationContext, Aggregator parent, SignificanceHeuristic significanceHeuristic,
-                SignificantTermsAggregatorFactory termsAggFactory, List<PipelineAggregator> pipelineAggregators,
-                Map<String, Object> metaData) throws IOException {
-            super(name, factories, valuesSource, bucketCountThresholds, includeExclude, aggregationContext, parent, significanceHeuristic,
-                    termsAggFactory, pipelineAggregators, metaData);
+                AggregationContext aggregationContext, Aggregator parent, SignificantTermsAggregatorFactory termsAggFactory,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            super(name, factories, valuesSource, bucketCountThresholds, includeExclude, aggregationContext, parent, termsAggFactory,
+                    pipelineAggregators, metaData);
             bucketOrds = new LongHash(1, aggregationContext.bigArrays());
         }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java
index 226c6d5..9e8ad33 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java
@@ -228,7 +228,7 @@ public class SignificantLongTerms extends InternalSignificantTerms<SignificantLo
         out.writeVLong(minDocCount);
         out.writeVLong(subsetSize);
         out.writeVLong(supersetSize);
-        SignificanceHeuristicStreams.writeTo(significanceHeuristic, out);
+        significanceHeuristic.writeTo(out);
         out.writeVInt(buckets.size());
         for (InternalSignificantTerms.Bucket bucket : buckets) {
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTermsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTermsAggregator.java
index abf9bf6..5e102ce 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTermsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTermsAggregator.java
@@ -25,7 +25,6 @@ import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
 import org.elasticsearch.search.aggregations.bucket.terms.LongTermsAggregator;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
@@ -47,19 +46,16 @@ public class SignificantLongTermsAggregator extends LongTermsAggregator {
 
     public SignificantLongTermsAggregator(String name, AggregatorFactories factories, ValuesSource.Numeric valuesSource,
             ValueFormat format, BucketCountThresholds bucketCountThresholds, AggregationContext aggregationContext, Aggregator parent,
-            SignificanceHeuristic significanceHeuristic, SignificantTermsAggregatorFactory termsAggFactory,
-            IncludeExclude.LongFilter includeExclude,
+            SignificantTermsAggregatorFactory termsAggFactory, IncludeExclude.LongFilter includeExclude,
             List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
 
         super(name, factories, valuesSource, format, null, bucketCountThresholds, aggregationContext, parent,
                 SubAggCollectionMode.DEPTH_FIRST, false, includeExclude, pipelineAggregators, metaData);
-        this.significanceHeuristic = significanceHeuristic;
         this.termsAggFactory = termsAggFactory;
     }
 
     protected long numCollectedDocs;
     private final SignificantTermsAggregatorFactory termsAggFactory;
-    private final SignificanceHeuristic significanceHeuristic;
 
     @Override
     public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,
@@ -99,7 +95,7 @@ public class SignificantLongTermsAggregator extends LongTermsAggregator {
             spare.supersetSize = supersetSize;
             // During shard-local down-selection we use subset/superset stats that are for this shard only
             // Back at the central reducer these properties will be updated with global stats
-            spare.updateScore(significanceHeuristic);
+            spare.updateScore(termsAggFactory.getSignificanceHeuristic());
 
             spare.bucketOrd = i;
             spare = (SignificantLongTerms.Bucket) ordered.insertWithOverflow(spare);
@@ -112,7 +108,7 @@ public class SignificantLongTermsAggregator extends LongTermsAggregator {
             list[i] = bucket;
         }
         return new SignificantLongTerms(subsetSize, supersetSize, name, formatter, bucketCountThresholds.getRequiredSize(),
-                bucketCountThresholds.getMinDocCount(), significanceHeuristic, Arrays.asList(list), pipelineAggregators(),
+                bucketCountThresholds.getMinDocCount(), termsAggFactory.getSignificanceHeuristic(), Arrays.asList(list), pipelineAggregators(),
                 metaData());
     }
 
@@ -123,7 +119,7 @@ public class SignificantLongTermsAggregator extends LongTermsAggregator {
         IndexReader topReader = searcher.getIndexReader();
         int supersetSize = topReader.numDocs();
         return new SignificantLongTerms(0, supersetSize, name, formatter, bucketCountThresholds.getRequiredSize(),
-                bucketCountThresholds.getMinDocCount(), significanceHeuristic,
+                bucketCountThresholds.getMinDocCount(), termsAggFactory.getSignificanceHeuristic(),
                 Collections.<InternalSignificantTerms.Bucket> emptyList(), pipelineAggregators(), metaData());
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java
index b047947..6c1ca0a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java
@@ -214,7 +214,7 @@ public class SignificantStringTerms extends InternalSignificantTerms<Significant
         out.writeVLong(minDocCount);
         out.writeVLong(subsetSize);
         out.writeVLong(supersetSize);
-        SignificanceHeuristicStreams.writeTo(significanceHeuristic, out);
+        significanceHeuristic.writeTo(out);
         out.writeVInt(buckets.size());
         for (InternalSignificantTerms.Bucket bucket : buckets) {
             bucket.writeTo(out);
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTermsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTermsAggregator.java
index d818fb0..1218bba 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTermsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTermsAggregator.java
@@ -26,7 +26,6 @@ import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
 import org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
@@ -47,16 +46,16 @@ public class SignificantStringTermsAggregator extends StringTermsAggregator {
 
     protected long numCollectedDocs;
     protected final SignificantTermsAggregatorFactory termsAggFactory;
-    private final SignificanceHeuristic significanceHeuristic;
 
     public SignificantStringTermsAggregator(String name, AggregatorFactories factories, ValuesSource valuesSource,
-            BucketCountThresholds bucketCountThresholds, IncludeExclude.StringFilter includeExclude, AggregationContext aggregationContext,
-            Aggregator parent, SignificanceHeuristic significanceHeuristic, SignificantTermsAggregatorFactory termsAggFactory,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            BucketCountThresholds bucketCountThresholds,
+            IncludeExclude.StringFilter includeExclude, AggregationContext aggregationContext, Aggregator parent,
+ SignificantTermsAggregatorFactory termsAggFactory, List<PipelineAggregator> pipelineAggregators,
+            Map<String, Object> metaData)
+            throws IOException {
 
         super(name, factories, valuesSource, null, bucketCountThresholds, includeExclude, aggregationContext, parent,
                 SubAggCollectionMode.DEPTH_FIRST, false, pipelineAggregators, metaData);
-        this.significanceHeuristic = significanceHeuristic;
         this.termsAggFactory = termsAggFactory;
     }
 
@@ -101,7 +100,7 @@ public class SignificantStringTermsAggregator extends StringTermsAggregator {
             // that are for this shard only
             // Back at the central reducer these properties will be updated with
             // global stats
-            spare.updateScore(significanceHeuristic);
+            spare.updateScore(termsAggFactory.getSignificanceHeuristic());
 
             spare.bucketOrd = i;
             spare = (SignificantStringTerms.Bucket) ordered.insertWithOverflow(spare);
@@ -117,7 +116,7 @@ public class SignificantStringTermsAggregator extends StringTermsAggregator {
         }
 
         return new SignificantStringTerms(subsetSize, supersetSize, name, bucketCountThresholds.getRequiredSize(),
-                bucketCountThresholds.getMinDocCount(), significanceHeuristic, Arrays.asList(list), pipelineAggregators(),
+                bucketCountThresholds.getMinDocCount(), termsAggFactory.getSignificanceHeuristic(), Arrays.asList(list), pipelineAggregators(),
                 metaData());
     }
 
@@ -128,7 +127,7 @@ public class SignificantStringTermsAggregator extends StringTermsAggregator {
         IndexReader topReader = searcher.getIndexReader();
         int supersetSize = topReader.numDocs();
         return new SignificantStringTerms(0, supersetSize, name, bucketCountThresholds.getRequiredSize(),
-                bucketCountThresholds.getMinDocCount(), significanceHeuristic,
+                bucketCountThresholds.getMinDocCount(), termsAggFactory.getSignificanceHeuristic(),
                 Collections.<InternalSignificantTerms.Bucket> emptyList(), pipelineAggregators(), metaData());
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorBuilder.java
deleted file mode 100644
index ce2edfd..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorBuilder.java
+++ /dev/null
@@ -1,241 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations.bucket.significant;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.index.FilterableTermsEnum;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicStreams;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import java.io.IOException;
-import java.util.Objects;
-
-/**
- *
- */
-public class SignificantTermsAggregatorBuilder extends ValuesSourceAggregatorBuilder<ValuesSource, SignificantTermsAggregatorBuilder> {
-
-    static final ParseField BACKGROUND_FILTER = new ParseField("background_filter");
-    static final ParseField HEURISTIC = new ParseField("significance_heuristic");
-
-    static final TermsAggregator.BucketCountThresholds DEFAULT_BUCKET_COUNT_THRESHOLDS = new TermsAggregator.BucketCountThresholds(
-            3, 0, 10, -1);
-
-    private IncludeExclude includeExclude = null;
-    private String executionHint = null;
-    private String indexedFieldName;
-    private MappedFieldType fieldType;
-    private FilterableTermsEnum termsEnum;
-    private int numberOfAggregatorsCreated = 0;
-    private QueryBuilder<?> filterBuilder = null;
-    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new BucketCountThresholds(DEFAULT_BUCKET_COUNT_THRESHOLDS);
-    private SignificanceHeuristic significanceHeuristic = JLHScore.PROTOTYPE;
-
-    protected TermsAggregator.BucketCountThresholds getBucketCountThresholds() {
-        return new TermsAggregator.BucketCountThresholds(bucketCountThresholds);
-    }
-
-    public SignificantTermsAggregatorBuilder(String name, ValueType valueType) {
-        super(name, SignificantStringTerms.TYPE, ValuesSourceType.ANY, valueType);
-    }
-
-    public TermsAggregator.BucketCountThresholds bucketCountThresholds() {
-        return bucketCountThresholds;
-    }
-
-    public SignificantTermsAggregatorBuilder bucketCountThresholds(TermsAggregator.BucketCountThresholds bucketCountThresholds) {
-        this.bucketCountThresholds = bucketCountThresholds;
-        return this;
-    }
-
-    /**
-     * Sets the size - indicating how many term buckets should be returned
-     * (defaults to 10)
-     */
-    public SignificantTermsAggregatorBuilder size(int size) {
-        bucketCountThresholds.setRequiredSize(size);
-        return this;
-    }
-
-    /**
-     * Sets the shard_size - indicating the number of term buckets each shard
-     * will return to the coordinating node (the node that coordinates the
-     * search execution). The higher the shard size is, the more accurate the
-     * results are.
-     */
-    public SignificantTermsAggregatorBuilder shardSize(int shardSize) {
-        bucketCountThresholds.setShardSize(shardSize);
-        return this;
-    }
-
-    /**
-     * Set the minimum document count terms should have in order to appear in
-     * the response.
-     */
-    public SignificantTermsAggregatorBuilder minDocCount(long minDocCount) {
-        bucketCountThresholds.setMinDocCount(minDocCount);
-        return this;
-    }
-
-    /**
-     * Set the minimum document count terms should have on the shard in order to
-     * appear in the response.
-     */
-    public SignificantTermsAggregatorBuilder shardMinDocCount(long shardMinDocCount) {
-        bucketCountThresholds.setShardMinDocCount(shardMinDocCount);
-        return this;
-    }
-
-    /**
-     * Expert: sets an execution hint to the aggregation.
-     */
-    public SignificantTermsAggregatorBuilder executionHint(String executionHint) {
-        this.executionHint = executionHint;
-        return this;
-    }
-
-    /**
-     * Expert: gets an execution hint to the aggregation.
-     */
-    public String executionHint() {
-        return executionHint;
-    }
-
-    public SignificantTermsAggregatorBuilder backgroundFilter(QueryBuilder<?> filterBuilder) {
-        this.filterBuilder = filterBuilder;
-        return this;
-    }
-
-    public QueryBuilder<?> backgroundFilter() {
-        return filterBuilder;
-    }
-
-    /**
-     * Set terms to include and exclude from the aggregation results
-     */
-    public SignificantTermsAggregatorBuilder includeExclude(IncludeExclude includeExclude) {
-        this.includeExclude = includeExclude;
-        return this;
-    }
-
-    /**
-     * Get terms to include and exclude from the aggregation results
-     */
-    public IncludeExclude includeExclude() {
-        return includeExclude;
-    }
-
-    public SignificantTermsAggregatorBuilder significanceHeuristic(SignificanceHeuristic significanceHeuristic) {
-        this.significanceHeuristic = significanceHeuristic;
-        return this;
-    }
-
-    public SignificanceHeuristic significanceHeuristic() {
-        return significanceHeuristic;
-    }
-
-    @Override
-    protected ValuesSourceAggregatorFactory<ValuesSource, ?> innerBuild(AggregationContext context, ValuesSourceConfig<ValuesSource> config,
-            AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-        return new SignificantTermsAggregatorFactory(name, type, config, includeExclude, executionHint, filterBuilder,
-                bucketCountThresholds, significanceHeuristic, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        bucketCountThresholds.toXContent(builder, params);
-        if (executionHint != null) {
-            builder.field(TermsAggregatorBuilder.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
-        }
-        if (filterBuilder != null) {
-            builder.field(BACKGROUND_FILTER.getPreferredName(), filterBuilder);
-        }
-        if (includeExclude != null) {
-            includeExclude.toXContent(builder, params);
-        }
-        significanceHeuristic.toXContent(builder, params);
-        return builder;
-    }
-
-    @Override
-    protected SignificantTermsAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        SignificantTermsAggregatorBuilder factory = new SignificantTermsAggregatorBuilder(name, targetValueType);
-        factory.bucketCountThresholds = BucketCountThresholds.readFromStream(in);
-        factory.executionHint = in.readOptionalString();
-        if (in.readBoolean()) {
-            factory.filterBuilder = in.readQuery();
-        }
-        if (in.readBoolean()) {
-            factory.includeExclude = IncludeExclude.readFromStream(in);
-        }
-        factory.significanceHeuristic = SignificanceHeuristicStreams.read(in);
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        bucketCountThresholds.writeTo(out);
-        out.writeOptionalString(executionHint);
-        boolean hasfilterBuilder = filterBuilder != null;
-        out.writeBoolean(hasfilterBuilder);
-        if (hasfilterBuilder) {
-            out.writeQuery(filterBuilder);
-        }
-        boolean hasIncExc = includeExclude != null;
-        out.writeBoolean(hasIncExc);
-        if (hasIncExc) {
-            includeExclude.writeTo(out);
-        }
-        SignificanceHeuristicStreams.writeTo(significanceHeuristic, out);
-    }
-
-    @Override
-    protected int innerHashCode() {
-        return Objects.hash(bucketCountThresholds, executionHint, filterBuilder, includeExclude, significanceHeuristic);
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        SignificantTermsAggregatorBuilder other = (SignificantTermsAggregatorBuilder) obj;
-        return Objects.equals(bucketCountThresholds, other.bucketCountThresholds)
-                && Objects.equals(executionHint, other.executionHint)
-                && Objects.equals(filterBuilder, other.filterBuilder)
-                && Objects.equals(includeExclude, other.includeExclude)
-                && Objects.equals(significanceHeuristic, other.significanceHeuristic);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java
index 217287e..399e857 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java
@@ -16,7 +16,6 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-
 package org.elasticsearch.search.aggregations.bucket.significant;
 
 import org.apache.lucene.index.IndexReader;
@@ -31,18 +30,13 @@ import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.lucene.index.FilterableTermsEnum;
 import org.elasticsearch.common.lucene.index.FreqTermsEnum;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.BucketUtils;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
@@ -55,104 +49,119 @@ import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
-public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource, SignificantTermsAggregatorFactory>
-        implements Releasable {
+/**
+ *
+ */
+public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource> implements Releasable {
+
+    public SignificanceHeuristic getSignificanceHeuristic() {
+        return significanceHeuristic;
+    }
+
+    public enum ExecutionMode {
 
+        MAP(new ParseField("map")) {
+
+            @Override
+            Aggregator create(String name, AggregatorFactories factories, ValuesSource valuesSource,
+                    TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
+                    AggregationContext aggregationContext, Aggregator parent, SignificantTermsAggregatorFactory termsAggregatorFactory,
+                    List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+                final IncludeExclude.StringFilter filter = includeExclude == null ? null : includeExclude.convertToStringFilter();
+                return new SignificantStringTermsAggregator(name, factories, valuesSource, bucketCountThresholds, filter,
+                        aggregationContext, parent, termsAggregatorFactory, pipelineAggregators, metaData);
+            }
+
+        },
+        GLOBAL_ORDINALS(new ParseField("global_ordinals")) {
+
+            @Override
+            Aggregator create(String name, AggregatorFactories factories, ValuesSource valuesSource,
+                              TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
+                    AggregationContext aggregationContext, Aggregator parent, SignificantTermsAggregatorFactory termsAggregatorFactory,
+                    List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+                ValuesSource.Bytes.WithOrdinals valueSourceWithOrdinals = (ValuesSource.Bytes.WithOrdinals) valuesSource;
+                IndexSearcher indexSearcher = aggregationContext.searchContext().searcher();
+                final IncludeExclude.OrdinalsFilter filter = includeExclude == null ? null : includeExclude.convertToOrdinalsFilter();
+                return new GlobalOrdinalsSignificantTermsAggregator(name, factories,
+                        (ValuesSource.Bytes.WithOrdinals.FieldData) valuesSource, bucketCountThresholds, filter, aggregationContext,
+                        parent, termsAggregatorFactory, pipelineAggregators, metaData);
+            }
+
+        },
+        GLOBAL_ORDINALS_HASH(new ParseField("global_ordinals_hash")) {
+
+            @Override
+            Aggregator create(String name, AggregatorFactories factories, ValuesSource valuesSource,
+                              TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
+                    AggregationContext aggregationContext, Aggregator parent, SignificantTermsAggregatorFactory termsAggregatorFactory,
+                    List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+                final IncludeExclude.OrdinalsFilter filter = includeExclude == null ? null : includeExclude.convertToOrdinalsFilter();
+                return new GlobalOrdinalsSignificantTermsAggregator.WithHash(name, factories,
+                        (ValuesSource.Bytes.WithOrdinals.FieldData) valuesSource, bucketCountThresholds, filter,
+ aggregationContext,
+                        parent, termsAggregatorFactory, pipelineAggregators, metaData);
+            }
+        };
+
+        public static ExecutionMode fromString(String value, ParseFieldMatcher parseFieldMatcher) {
+            for (ExecutionMode mode : values()) {
+                if (parseFieldMatcher.match(value, mode.parseField)) {
+                    return mode;
+                }
+            }
+            throw new IllegalArgumentException("Unknown `execution_hint`: [" + value + "], expected any of " + values());
+        }
+
+        private final ParseField parseField;
+
+        ExecutionMode(ParseField parseField) {
+            this.parseField = parseField;
+        }
+
+        abstract Aggregator create(String name, AggregatorFactories factories, ValuesSource valuesSource,
+                                   TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
+                AggregationContext aggregationContext, Aggregator parent, SignificantTermsAggregatorFactory termsAggregatorFactory,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException;
+
+        @Override
+        public String toString() {
+            return parseField.getPreferredName();
+        }
+    }
     private final IncludeExclude includeExclude;
     private final String executionHint;
     private String indexedFieldName;
     private MappedFieldType fieldType;
     private FilterableTermsEnum termsEnum;
-    private int numberOfAggregatorsCreated;
-    private final QueryBuilder<?> filterBuilder;
+    private int numberOfAggregatorsCreated = 0;
+    private final Query filter;
     private final TermsAggregator.BucketCountThresholds bucketCountThresholds;
     private final SignificanceHeuristic significanceHeuristic;
 
-    public SignificantTermsAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource> config, IncludeExclude includeExclude,
-            String executionHint, QueryBuilder<?> filterBuilder, TermsAggregator.BucketCountThresholds bucketCountThresholds,
-            SignificanceHeuristic significanceHeuristic, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
+    protected TermsAggregator.BucketCountThresholds getBucketCountThresholds() {
+        return new TermsAggregator.BucketCountThresholds(bucketCountThresholds);
+    }
+
+    public SignificantTermsAggregatorFactory(String name, ValuesSourceConfig valueSourceConfig, TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
+                                             String executionHint, Query filter, SignificanceHeuristic significanceHeuristic) {
+
+        super(name, SignificantStringTerms.TYPE.name(), valueSourceConfig);
+        this.bucketCountThresholds = bucketCountThresholds;
         this.includeExclude = includeExclude;
         this.executionHint = executionHint;
-        this.filterBuilder = filterBuilder;
-        this.bucketCountThresholds = bucketCountThresholds;
         this.significanceHeuristic = significanceHeuristic;
-        this.significanceHeuristic.initialize(context.searchContext());
-        setFieldInfo();
-
-    }
-
-    private void setFieldInfo() {
-        if (!config.unmapped()) {
+        if (!valueSourceConfig.unmapped()) {
             this.indexedFieldName = config.fieldContext().field();
             fieldType = SearchContext.current().smartNameFieldType(indexedFieldName);
         }
-    }
-
-    /**
-     * Creates the TermsEnum (if not already created) and must be called before
-     * any calls to getBackgroundFrequency
-     *
-     * @param context
-     *            The aggregation context
-     * @return The number of documents in the index (after an optional filter
-     *         might have been applied)
-     */
-    public long prepareBackground(AggregationContext context) {
-        if (termsEnum != null) {
-            // already prepared - return
-            return termsEnum.getNumDocs();
-        }
-        SearchContext searchContext = context.searchContext();
-        IndexReader reader = searchContext.searcher().getIndexReader();
-        Query filter = null;
-        try {
-            if (filterBuilder != null) {
-                filter = filterBuilder.toFilter(context.searchContext().indexShard().getQueryShardContext());
-            }
-        } catch (IOException e) {
-            throw new ElasticsearchException("failed to create filter: " + filterBuilder.toString(), e);
-        }
-        try {
-            if (numberOfAggregatorsCreated == 1) {
-                // Setup a termsEnum for sole use by one aggregator
-                termsEnum = new FilterableTermsEnum(reader, indexedFieldName, PostingsEnum.NONE, filter);
-            } else {
-                // When we have > 1 agg we have possibility of duplicate term
-                // frequency lookups
-                // and so use a TermsEnum that caches results of all term
-                // lookups
-                termsEnum = new FreqTermsEnum(reader, indexedFieldName, true, false, filter, searchContext.bigArrays());
-            }
-        } catch (IOException e) {
-            throw new ElasticsearchException("failed to build terms enumeration", e);
-        }
-        return termsEnum.getNumDocs();
-    }
-
-    public long getBackgroundFrequency(BytesRef termBytes) {
-        assert termsEnum != null; // having failed to find a field in the index
-                                  // we don't expect any calls for frequencies
-        long result = 0;
-        try {
-            if (termsEnum.seekExact(termBytes)) {
-                result = termsEnum.docFreq();
-            }
-        } catch (IOException e) {
-            throw new ElasticsearchException("IOException loading background document frequency info", e);
-        }
-        return result;
-    }
-
-    public long getBackgroundFrequency(long term) {
-        BytesRef indexedVal = fieldType.indexedValueForSearch(term);
-        return getBackgroundFrequency(indexedVal);
+        this.filter = filter;
     }
 
     @Override
     protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            List<PipelineAggregator> pipelineAggregators,
+            Map<String, Object> metaData) throws IOException {
         final InternalAggregation aggregation = new UnmappedSignificantTerms(name, bucketCountThresholds.getRequiredSize(),
                 bucketCountThresholds.getMinDocCount(), pipelineAggregators, metaData);
         return new NonCollectingAggregator(name, aggregationContext, parent, pipelineAggregators, metaData) {
@@ -166,29 +175,12 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
     @Override
     protected Aggregator doCreateInternal(ValuesSource valuesSource, AggregationContext aggregationContext, Aggregator parent,
             boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
+            throws IOException {
         if (collectsFromSingleBucket == false) {
             return asMultiBucketAggregator(this, aggregationContext, parent);
         }
 
         numberOfAggregatorsCreated++;
-        BucketCountThresholds bucketCountThresholds = new BucketCountThresholds(this.bucketCountThresholds);
-        if (bucketCountThresholds.getShardSize() == SignificantTermsAggregatorBuilder.DEFAULT_BUCKET_COUNT_THRESHOLDS.getShardSize()) {
-            // The user has not made a shardSize selection .
-            // Use default heuristic to avoid any wrong-ranking caused by
-            // distributed counting
-            // but request double the usual amount.
-            // We typically need more than the number of "top" terms requested
-            // by other aggregations
-            // as the significance algorithm is in less of a position to
-            // down-select at shard-level -
-            // some of the things we want to find have only one occurrence on
-            // each shard and as
-            // such are impossible to differentiate from non-significant terms
-            // at that early stage.
-            bucketCountThresholds.setShardSize(2 * BucketUtils.suggestShardSideQueueSize(bucketCountThresholds.getRequiredSize(),
-                    aggregationContext.searchContext().numberOfShards()));
-        }
 
         if (valuesSource instanceof ValuesSource.Bytes) {
             ExecutionMode execution = null;
@@ -206,13 +198,14 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
                 }
             }
             assert execution != null;
-            return execution.create(name, factories, valuesSource, bucketCountThresholds, includeExclude, aggregationContext, parent,
-                    significanceHeuristic, this, pipelineAggregators, metaData);
+            return execution.create(name, factories, valuesSource, bucketCountThresholds, includeExclude, aggregationContext, parent, this,
+                    pipelineAggregators, metaData);
         }
 
+
         if ((includeExclude != null) && (includeExclude.isRegexBased())) {
-            throw new AggregationExecutionException("Aggregation [" + name + "] cannot support regular expression style include/exclude "
-                    + "settings as they can only be applied to string fields. Use an array of numeric values for include/exclude clauses used to filter numeric fields");
+            throw new AggregationExecutionException("Aggregation [" + name + "] cannot support regular expression style include/exclude " +
+                    "settings as they can only be applied to string fields. Use an array of numeric values for include/exclude clauses used to filter numeric fields");
         }
 
         if (valuesSource instanceof ValuesSource.Numeric) {
@@ -225,87 +218,57 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
                 longFilter = includeExclude.convertToLongFilter();
             }
             return new SignificantLongTermsAggregator(name, factories, (ValuesSource.Numeric) valuesSource, config.format(),
-                    bucketCountThresholds, aggregationContext, parent, significanceHeuristic, this, longFilter, pipelineAggregators,
-                    metaData);
+                    bucketCountThresholds, aggregationContext, parent, this, longFilter, pipelineAggregators, metaData);
         }
 
-        throw new AggregationExecutionException("sigfnificant_terms aggregation cannot be applied to field ["
-                + config.fieldContext().field() + "]. It can only be applied to numeric or string fields.");
+        throw new AggregationExecutionException("sigfnificant_terms aggregation cannot be applied to field [" + config.fieldContext().field() +
+                "]. It can only be applied to numeric or string fields.");
     }
 
-    public enum ExecutionMode {
-
-        MAP(new ParseField("map")) {
-
-            @Override
-            Aggregator create(String name, AggregatorFactories factories, ValuesSource valuesSource,
-                    TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
-                    AggregationContext aggregationContext, Aggregator parent, SignificanceHeuristic significanceHeuristic,
-                    SignificantTermsAggregatorFactory termsAggregatorFactory, List<PipelineAggregator> pipelineAggregators,
-                    Map<String, Object> metaData) throws IOException {
-                final IncludeExclude.StringFilter filter = includeExclude == null ? null : includeExclude.convertToStringFilter();
-                return new SignificantStringTermsAggregator(name, factories, valuesSource, bucketCountThresholds, filter,
-                        aggregationContext, parent, significanceHeuristic, termsAggregatorFactory, pipelineAggregators, metaData);
-            }
-
-        },
-        GLOBAL_ORDINALS(new ParseField("global_ordinals")) {
-
-            @Override
-            Aggregator create(String name, AggregatorFactories factories, ValuesSource valuesSource,
-                    TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
-                    AggregationContext aggregationContext, Aggregator parent, SignificanceHeuristic significanceHeuristic,
-                    SignificantTermsAggregatorFactory termsAggregatorFactory, List<PipelineAggregator> pipelineAggregators,
-                    Map<String, Object> metaData) throws IOException {
-                ValuesSource.Bytes.WithOrdinals valueSourceWithOrdinals = (ValuesSource.Bytes.WithOrdinals) valuesSource;
-                IndexSearcher indexSearcher = aggregationContext.searchContext().searcher();
-                final IncludeExclude.OrdinalsFilter filter = includeExclude == null ? null : includeExclude.convertToOrdinalsFilter();
-                return new GlobalOrdinalsSignificantTermsAggregator(name, factories,
-                        (ValuesSource.Bytes.WithOrdinals.FieldData) valuesSource, bucketCountThresholds, filter, aggregationContext, parent,
-                        significanceHeuristic, termsAggregatorFactory, pipelineAggregators, metaData);
-            }
-
-        },
-        GLOBAL_ORDINALS_HASH(new ParseField("global_ordinals_hash")) {
-
-            @Override
-            Aggregator create(String name, AggregatorFactories factories, ValuesSource valuesSource,
-                    TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
-                    AggregationContext aggregationContext, Aggregator parent, SignificanceHeuristic significanceHeuristic,
-                    SignificantTermsAggregatorFactory termsAggregatorFactory, List<PipelineAggregator> pipelineAggregators,
-                    Map<String, Object> metaData) throws IOException {
-                final IncludeExclude.OrdinalsFilter filter = includeExclude == null ? null : includeExclude.convertToOrdinalsFilter();
-                return new GlobalOrdinalsSignificantTermsAggregator.WithHash(name, factories,
-                        (ValuesSource.Bytes.WithOrdinals.FieldData) valuesSource, bucketCountThresholds, filter, aggregationContext, parent,
-                        significanceHeuristic, termsAggregatorFactory, pipelineAggregators, metaData);
-            }
-        };
-
-        public static ExecutionMode fromString(String value, ParseFieldMatcher parseFieldMatcher) {
-            for (ExecutionMode mode : values()) {
-                if (parseFieldMatcher.match(value, mode.parseField)) {
-                    return mode;
-                }
+    /**
+     * Creates the TermsEnum (if not already created) and must be called before any calls to getBackgroundFrequency
+     * @param context The aggregation context
+     * @return The number of documents in the index (after an optional filter might have been applied)
+     */
+    public long prepareBackground(AggregationContext context) {
+        if (termsEnum != null) {
+            // already prepared - return
+            return termsEnum.getNumDocs();
+        }
+        SearchContext searchContext = context.searchContext();
+        IndexReader reader = searchContext.searcher().getIndexReader();
+        try {
+            if (numberOfAggregatorsCreated == 1) {
+                // Setup a termsEnum for sole use by one aggregator
+                termsEnum = new FilterableTermsEnum(reader, indexedFieldName, PostingsEnum.NONE, filter);
+            } else {
+                // When we have > 1 agg we have possibility of duplicate term frequency lookups
+                // and so use a TermsEnum that caches results of all term lookups
+                termsEnum = new FreqTermsEnum(reader, indexedFieldName, true, false, filter, searchContext.bigArrays());
             }
-            throw new IllegalArgumentException("Unknown `execution_hint`: [" + value + "], expected any of " + values());
+        } catch (IOException e) {
+            throw new ElasticsearchException("failed to build terms enumeration", e);
         }
+        return termsEnum.getNumDocs();
+    }
 
-        private final ParseField parseField;
-
-        ExecutionMode(ParseField parseField) {
-            this.parseField = parseField;
+    public long getBackgroundFrequency(BytesRef termBytes) {
+        assert termsEnum != null; // having failed to find a field in the index we don't expect any calls for frequencies
+        long result = 0;
+        try {
+            if (termsEnum.seekExact(termBytes)) {
+                result = termsEnum.docFreq();
+            }
+        } catch (IOException e) {
+            throw new ElasticsearchException("IOException loading background document frequency info", e);
         }
+        return result;
+    }
 
-        abstract Aggregator create(String name, AggregatorFactories factories, ValuesSource valuesSource,
-                TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
-                AggregationContext aggregationContext, Aggregator parent, SignificanceHeuristic significanceHeuristic,
-                SignificantTermsAggregatorFactory termsAggregatorFactory, List<PipelineAggregator> pipelineAggregators,
-                Map<String, Object> metaData) throws IOException;
 
-        @Override
-        public String toString() {
-            return parseField.getPreferredName();
-        }
+    public long getBackgroundFrequency(long term) {
+        BytesRef indexedVal = fieldType.indexedValueForSearch(term);
+        return getBackgroundFrequency(indexedVal);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java
new file mode 100644
index 0000000..b67ce2a
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java
@@ -0,0 +1,277 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.significant;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.search.aggregations.AggregationBuilder;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicBuilder;
+import org.elasticsearch.search.aggregations.bucket.terms.AbstractTermsParametersParser;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
+
+import java.io.IOException;
+
+/**
+ * Creates an aggregation that finds interesting or unusual occurrences of terms in a result set.
+ * <p>
+ * This feature is marked as experimental, and may be subject to change in the future.  If you
+ * use this feature, please let us know your experience with it!
+ */
+public class SignificantTermsBuilder extends AggregationBuilder<SignificantTermsBuilder> {
+
+    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new TermsAggregator.BucketCountThresholds(-1, -1, -1, -1);
+
+    private String field;
+    private String executionHint;
+    private String includePattern;
+    private int includeFlags;
+    private String excludePattern;
+    private int excludeFlags;
+    private String[] includeTerms = null;
+    private String[] excludeTerms = null;
+    private QueryBuilder filterBuilder;
+    private SignificanceHeuristicBuilder significanceHeuristicBuilder;
+
+    /**
+     * Sole constructor.
+     */
+    public SignificantTermsBuilder(String name) {
+        super(name, SignificantStringTerms.TYPE.name());
+    }
+
+    /**
+     * Set the field to fetch significant terms from.
+     */
+    public SignificantTermsBuilder field(String field) {
+        this.field = field;
+        return this;
+    }
+
+    /**
+     * Set the number of significant terms to retrieve.
+     */
+    public SignificantTermsBuilder size(int requiredSize) {
+        bucketCountThresholds.setRequiredSize(requiredSize);
+        return this;
+    }
+
+    /**
+     * Expert: Set the number of significant terms to retrieve on each shard.
+     */
+    public SignificantTermsBuilder shardSize(int shardSize) {
+        bucketCountThresholds.setShardSize(shardSize);
+        return this;
+    }
+
+    /**
+     * Only return significant terms that belong to at least <code>minDocCount</code> documents.
+     */
+    public SignificantTermsBuilder minDocCount(int minDocCount) {
+        bucketCountThresholds.setMinDocCount(minDocCount);
+        return this;
+    }
+    
+    /**
+     * Set the background filter to compare to. Defaults to the whole index.
+     */
+    public SignificantTermsBuilder backgroundFilter(QueryBuilder filter) {
+        this.filterBuilder = filter;
+        return this;
+    }
+    
+    /**
+     * Expert: set the minimum number of documents that a term should match to
+     * be retrieved from a shard.
+     */
+    public SignificantTermsBuilder shardMinDocCount(int shardMinDocCount) {
+        bucketCountThresholds.setShardMinDocCount(shardMinDocCount);
+        return this;
+    }
+
+    /**
+     * Expert: give an execution hint to this aggregation.
+     */
+    public SignificantTermsBuilder executionHint(String executionHint) {
+        this.executionHint = executionHint;
+        return this;
+    }
+
+    /**
+     * Define a regular expression that will determine what terms should be aggregated. The regular expression is based
+     * on the {@link java.util.regex.Pattern} class.
+     *
+     * @see #include(String, int)
+     */
+    public SignificantTermsBuilder include(String regex) {
+        return include(regex, 0);
+    }
+
+    /**
+     * Define a regular expression that will determine what terms should be aggregated. The regular expression is based
+     * on the {@link java.util.regex.Pattern} class.
+     *
+     * @see java.util.regex.Pattern#compile(String, int)
+     */
+    public SignificantTermsBuilder include(String regex, int flags) {
+        if (includeTerms != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of strings or a regex, not both");
+        }
+        this.includePattern = regex;
+        this.includeFlags = flags;
+        return this;
+    }
+    
+    /**
+     * Define a set of terms that should be aggregated.
+     */
+    public SignificantTermsBuilder include(String [] terms) {
+        if (includePattern != null) {
+            throw new IllegalArgumentException("include clause must be an array of exact values or a regex, not both");
+        }
+        this.includeTerms = terms;
+        return this;
+    }    
+    
+    /**
+     * Define a set of terms that should be aggregated.
+     */
+    public SignificantTermsBuilder include(long [] terms) {
+        if (includePattern != null) {
+            throw new IllegalArgumentException("include clause must be an array of exact values or a regex, not both");
+        }
+        this.includeTerms = longsArrToStringArr(terms);
+        return this;
+    }     
+    
+    private String[] longsArrToStringArr(long[] terms) {
+        String[] termsAsString = new String[terms.length];
+        for (int i = 0; i < terms.length; i++) {
+            termsAsString[i] = Long.toString(terms[i]);
+        }
+        return termsAsString;
+    }      
+    
+
+    /**
+     * Define a regular expression that will filter out terms that should be excluded from the aggregation. The regular
+     * expression is based on the {@link java.util.regex.Pattern} class.
+     *
+     * @see #exclude(String, int)
+     */
+    public SignificantTermsBuilder exclude(String regex) {
+        return exclude(regex, 0);
+    }
+
+    /**
+     * Define a regular expression that will filter out terms that should be excluded from the aggregation. The regular
+     * expression is based on the {@link java.util.regex.Pattern} class.
+     *
+     * @see java.util.regex.Pattern#compile(String, int)
+     */
+    public SignificantTermsBuilder exclude(String regex, int flags) {
+        if (excludeTerms != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of strings or a regex, not both");
+        }
+        this.excludePattern = regex;
+        this.excludeFlags = flags;
+        return this;
+    }
+    
+    /**
+     * Define a set of terms that should not be aggregated.
+     */
+    public SignificantTermsBuilder exclude(String [] terms) {
+        if (excludePattern != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of strings or a regex, not both");
+        }
+        this.excludeTerms = terms;
+        return this;
+    }    
+    
+    
+    /**
+     * Define a set of terms that should not be aggregated.
+     */
+    public SignificantTermsBuilder exclude(long [] terms) {
+        if (excludePattern != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of longs or a regex, not both");
+        }
+        this.excludeTerms = longsArrToStringArr(terms);
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (field != null) {
+            builder.field("field", field);
+        }
+        bucketCountThresholds.toXContent(builder);
+        if (executionHint != null) {
+            builder.field(AbstractTermsParametersParser.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
+        }
+        if (includePattern != null) {
+            if (includeFlags == 0) {
+                builder.field("include", includePattern);
+            } else {
+                builder.startObject("include")
+                        .field("pattern", includePattern)
+                        .field("flags", includeFlags)
+                        .endObject();
+            }
+        }
+        if (includeTerms != null) {
+            builder.array("include", includeTerms);
+        }
+        
+        if (excludePattern != null) {
+            if (excludeFlags == 0) {
+                builder.field("exclude", excludePattern);
+            } else {
+                builder.startObject("exclude")
+                        .field("pattern", excludePattern)
+                        .field("flags", excludeFlags)
+                        .endObject();
+            }
+        }
+        if (excludeTerms != null) {
+            builder.array("exclude", excludeTerms);
+        }
+        
+        if (filterBuilder != null) {
+            builder.field(SignificantTermsParametersParser.BACKGROUND_FILTER.getPreferredName());
+            filterBuilder.toXContent(builder, params); 
+        }
+        if (significanceHeuristicBuilder != null) {
+            significanceHeuristicBuilder.toXContent(builder, params);
+        }
+
+        return builder.endObject();
+    }
+
+    /**
+     * Expert: set the {@link SignificanceHeuristic} to use.
+     */
+    public SignificantTermsBuilder significanceHeuristic(SignificanceHeuristicBuilder significanceHeuristicBuilder) {
+        this.significanceHeuristicBuilder = significanceHeuristicBuilder;
+        return this;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParametersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParametersParser.java
new file mode 100644
index 0000000..3019765
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParametersParser.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.search.aggregations.bucket.significant;
+
+import org.apache.lucene.search.Query;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParser;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParserMapper;
+import org.elasticsearch.search.aggregations.bucket.terms.AbstractTermsParametersParser;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+
+
+public class SignificantTermsParametersParser extends AbstractTermsParametersParser {
+
+    private static final TermsAggregator.BucketCountThresholds DEFAULT_BUCKET_COUNT_THRESHOLDS = new TermsAggregator.BucketCountThresholds(3, 0, 10, -1);
+    private final SignificanceHeuristicParserMapper significanceHeuristicParserMapper;
+
+    public SignificantTermsParametersParser(SignificanceHeuristicParserMapper significanceHeuristicParserMapper) {
+        this.significanceHeuristicParserMapper = significanceHeuristicParserMapper;
+    }
+
+    public Query getFilter() {
+        return filter;
+    }
+
+    private Query filter = null;
+
+    private SignificanceHeuristic significanceHeuristic;
+
+    @Override
+    public TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds() {
+        return new TermsAggregator.BucketCountThresholds(DEFAULT_BUCKET_COUNT_THRESHOLDS);
+    }
+
+    static final ParseField BACKGROUND_FILTER = new ParseField("background_filter");
+
+    @Override
+    public void parseSpecial(String aggregationName, XContentParser parser, SearchContext context, XContentParser.Token token, String currentFieldName) throws IOException {
+
+        if (token == XContentParser.Token.START_OBJECT) {
+            SignificanceHeuristicParser significanceHeuristicParser = significanceHeuristicParserMapper.get(currentFieldName);
+            if (significanceHeuristicParser != null) {
+                significanceHeuristic = significanceHeuristicParser.parse(parser, context.parseFieldMatcher(), context);
+            } else if (context.parseFieldMatcher().match(currentFieldName, BACKGROUND_FILTER)) {
+                filter = context.getQueryShardContext().parseInnerFilter(parser).query();
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
+            }
+        } else {
+            throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName
+                    + "].", parser.getTokenLocation());
+        }
+    }
+
+    public SignificanceHeuristic getSignificanceHeuristic() {
+        return significanceHeuristic;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java
index 12b181a..28e0fb5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java
@@ -18,41 +18,31 @@
  */
 package org.elasticsearch.search.aggregations.bucket.significant;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.BucketUtils;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParser;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParserMapper;
-import org.elasticsearch.search.aggregations.bucket.terms.AbstractTermsParser;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  *
  */
-public class SignificantTermsParser extends AbstractTermsParser {
+public class SignificantTermsParser implements Aggregator.Parser {
+
     private final SignificanceHeuristicParserMapper significanceHeuristicParserMapper;
-    private final IndicesQueriesRegistry queriesRegistry;
 
     @Inject
-    public SignificantTermsParser(SignificanceHeuristicParserMapper significanceHeuristicParserMapper,
-            IndicesQueriesRegistry queriesRegistry) {
+    public SignificantTermsParser(SignificanceHeuristicParserMapper significanceHeuristicParserMapper) {
         this.significanceHeuristicParserMapper = significanceHeuristicParserMapper;
-        this.queriesRegistry = queriesRegistry;
     }
 
     @Override
@@ -61,58 +51,32 @@ public class SignificantTermsParser extends AbstractTermsParser {
     }
 
     @Override
-    protected SignificantTermsAggregatorBuilder doCreateFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, BucketCountThresholds bucketCountThresholds, SubAggCollectionMode collectMode, String executionHint,
-            IncludeExclude incExc, Map<ParseField, Object> otherOptions) {
-        SignificantTermsAggregatorBuilder factory = new SignificantTermsAggregatorBuilder(aggregationName, targetValueType);
-        if (bucketCountThresholds != null) {
-            factory.bucketCountThresholds(bucketCountThresholds);
-        }
-        if (executionHint != null) {
-            factory.executionHint(executionHint);
-        }
-        if (incExc != null) {
-            factory.includeExclude(incExc);
-        }
-        QueryBuilder<?> backgroundFilter = (QueryBuilder<?>) otherOptions.get(SignificantTermsAggregatorBuilder.BACKGROUND_FILTER);
-        if (backgroundFilter != null) {
-            factory.backgroundFilter(backgroundFilter);
-        }
-        SignificanceHeuristic significanceHeuristic = (SignificanceHeuristic) otherOptions.get(SignificantTermsAggregatorBuilder.HEURISTIC);
-        if (significanceHeuristic != null) {
-            factory.significanceHeuristic(significanceHeuristic);
-        }
-        return factory;
-    }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        SignificantTermsParametersParser aggParser = new SignificantTermsParametersParser(significanceHeuristicParserMapper);
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, SignificantStringTerms.TYPE, context)
+                .scriptable(false)
+                .formattable(true)
+                .build();
+        IncludeExclude.Parser incExcParser = new IncludeExclude.Parser();
+        aggParser.parse(aggregationName, parser, context, vsParser, incExcParser);
 
-    @Override
-    public boolean parseSpecial(String aggregationName, XContentParser parser, ParseFieldMatcher parseFieldMatcher, Token token,
-            String currentFieldName, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.START_OBJECT) {
-            SignificanceHeuristicParser significanceHeuristicParser = significanceHeuristicParserMapper.get(currentFieldName);
-            if (significanceHeuristicParser != null) {
-                SignificanceHeuristic significanceHeuristic = significanceHeuristicParser.parse(parser, parseFieldMatcher);
-                otherOptions.put(SignificantTermsAggregatorBuilder.HEURISTIC, significanceHeuristic);
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SignificantTermsAggregatorBuilder.BACKGROUND_FILTER)) {
-                QueryParseContext queryParseContext = new QueryParseContext(queriesRegistry);
-                queryParseContext.reset(parser);
-                queryParseContext.parseFieldMatcher(parseFieldMatcher);
-                QueryBuilder<?> filter = queryParseContext.parseInnerQueryBuilder();
-                otherOptions.put(SignificantTermsAggregatorBuilder.BACKGROUND_FILTER, filter);
-                return true;
-            }
+        TermsAggregator.BucketCountThresholds bucketCountThresholds = aggParser.getBucketCountThresholds();
+        if (bucketCountThresholds.getShardSize() == aggParser.getDefaultBucketCountThresholds().getShardSize()) {
+            //The user has not made a shardSize selection .
+            //Use default heuristic to avoid any wrong-ranking caused by distributed counting
+            //but request double the usual amount.
+            //We typically need more than the number of "top" terms requested by other aggregations
+            //as the significance algorithm is in less of a position to down-select at shard-level -
+            //some of the things we want to find have only one occurrence on each shard and as
+            // such are impossible to differentiate from non-significant terms at that early stage.
+            bucketCountThresholds.setShardSize(2 * BucketUtils.suggestShardSideQueueSize(bucketCountThresholds.getRequiredSize(), context.numberOfShards()));
         }
-        return false;
-    }
 
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new SignificantTermsAggregatorBuilder(null, null);
-    }
-
-    @Override
-    protected BucketCountThresholds getDefaultBucketCountThresholds() {
-        return new TermsAggregator.BucketCountThresholds(SignificantTermsAggregatorBuilder.DEFAULT_BUCKET_COUNT_THRESHOLDS);
+        bucketCountThresholds.ensureValidity();
+        SignificanceHeuristic significanceHeuristic = aggParser.getSignificanceHeuristic();
+        if (significanceHeuristic == null) {
+            significanceHeuristic = JLHScore.INSTANCE;
+        }
+        return new SignificantTermsAggregatorFactory(aggregationName, vsParser.config(), bucketCountThresholds, aggParser.getIncludeExclude(), aggParser.getExecutionHint(), aggParser.getFilter(), significanceHeuristic);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java
index bcad058..c7569db 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java
@@ -58,9 +58,9 @@ public class UnmappedSignificantTerms extends InternalSignificantTerms<UnmappedS
     UnmappedSignificantTerms() {} // for serialization
 
     public UnmappedSignificantTerms(String name, int requiredSize, long minDocCount, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) {
-        //We pass zero for index/subset sizes because for the purpose of significant term analysis
-        // we assume an unmapped index's size is irrelevant to the proceedings.
-        super(0, 0, name, requiredSize, minDocCount, JLHScore.PROTOTYPE, BUCKETS, pipelineAggregators, metaData);
+        //We pass zero for index/subset sizes because for the purpose of significant term analysis 
+        // we assume an unmapped index's size is irrelevant to the proceedings. 
+        super(0, 0, name, requiredSize, minDocCount, JLHScore.INSTANCE, BUCKETS, pipelineAggregators, metaData);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java
index c68e47a..cc9303a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java
@@ -23,14 +23,13 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
 public class ChiSquare extends NXYSignificanceHeuristic {
 
-    static final ChiSquare PROTOTYPE = new ChiSquare(false, false);
-
     protected static final ParseField NAMES_FIELD = new ParseField("chi_square");
 
     public ChiSquare(boolean includeNegatives, boolean backgroundIsSuperset) {
@@ -52,6 +51,18 @@ public class ChiSquare extends NXYSignificanceHeuristic {
         return result;
     }
 
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return new ChiSquare(in.readBoolean(), in.readBoolean());
+        }
+
+        @Override
+        public String getName() {
+            return NAMES_FIELD.getPreferredName();
+        }
+    };
+
     /**
      * Calculates Chi^2
      * see "Information Retrieval", Manning et al., Eq. 13.19
@@ -69,21 +80,9 @@ public class ChiSquare extends NXYSignificanceHeuristic {
     }
 
     @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return new ChiSquare(in.readBoolean(), in.readBoolean());
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName());
-        super.build(builder);
-        builder.endObject();
-        return builder;
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
+        super.writeTo(out);
     }
 
     public static class ChiSquareParser extends NXYParser {
@@ -107,7 +106,7 @@ public class ChiSquare extends NXYSignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName());
+            builder.startObject(STREAM.getName());
             super.build(builder);
             builder.endObject();
             return builder;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
index 6da05ed..99ee7c7 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
@@ -29,13 +29,12 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
 public class GND extends NXYSignificanceHeuristic {
 
-    static final GND PROTOTYPE = new GND(false);
-
     protected static final ParseField NAMES_FIELD = new ParseField("gnd");
 
     public GND(boolean backgroundIsSuperset) {
@@ -58,6 +57,18 @@ public class GND extends NXYSignificanceHeuristic {
         return result;
     }
 
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return new GND(in.readBoolean());
+        }
+
+        @Override
+        public String getName() {
+            return NAMES_FIELD.getPreferredName();
+        }
+    };
+
     /**
      * Calculates Google Normalized Distance, as described in "The Google Similarity Distance", Cilibrasi and Vitanyi, 2007
      * link: http://arxiv.org/pdf/cs/0412098v3.pdf
@@ -87,28 +98,11 @@ public class GND extends NXYSignificanceHeuristic {
     }
 
     @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return new GND(in.readBoolean());
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
         out.writeBoolean(backgroundIsSuperset);
     }
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName());
-        builder.field(BACKGROUND_IS_SUPERSET.getPreferredName(), backgroundIsSuperset);
-        builder.endObject();
-        return builder;
-    }
-
     public static class GNDParser extends NXYParser {
 
         @Override
@@ -122,7 +116,7 @@ public class GND extends NXYSignificanceHeuristic {
         }
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             String givenName = parser.currentName();
             boolean backgroundIsSuperset = true;
@@ -149,7 +143,7 @@ public class GND extends NXYSignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName());
+            builder.startObject(STREAM.getName());
             builder.field(BACKGROUND_IS_SUPERSET.getPreferredName(), backgroundIsSuperset);
             builder.endObject();
             return builder;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
index c4327cd..97264e7 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
@@ -22,43 +22,38 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
 public class JLHScore extends SignificanceHeuristic {
 
-    public static final JLHScore PROTOTYPE = new JLHScore();
+    public static final JLHScore INSTANCE = new JLHScore();
 
-    protected static final ParseField NAMES_FIELD = new ParseField("jlh");
+    protected static final String[] NAMES = {"jlh"};
 
-    public JLHScore() {
-    }
+    private JLHScore() {}
 
-    @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return PROTOTYPE;
-    }
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return readFrom(in);
+        }
 
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-    }
+        @Override
+        public String getName() {
+            return NAMES[0];
+        }
+    };
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
-        return builder;
+    public static SignificanceHeuristic readFrom(StreamInput in) throws IOException {
+        return INSTANCE;
     }
 
     /**
@@ -106,21 +101,26 @@ public class JLHScore extends SignificanceHeuristic {
         return absoluteProbabilityChange * relativeProbabilityChange;
     }
 
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
+    }
+
     public static class JLHScoreParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             // move to the closing bracket
             if (!parser.nextToken().equals(XContentParser.Token.END_OBJECT)) {
-                throw new ElasticsearchParseException("failed to parse [jlh] significance heuristic. expected an empty object, but found [{}] instead", parser.currentToken());
+                throw new ElasticsearchParseException("failed to parse [jhl] significance heuristic. expected an empty object, but found [{}] instead", parser.currentToken());
             }
-            return PROTOTYPE;
+            return new JLHScore();
         }
 
         @Override
         public String[] getNames() {
-            return NAMES_FIELD.getAllNamesIncludedDeprecated();
+            return NAMES;
         }
     }
 
@@ -128,7 +128,7 @@ public class JLHScore extends SignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
+            builder.startObject(STREAM.getName()).endObject();
             return builder;
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java
index d20b5f3..b4529b8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java
@@ -23,14 +23,13 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
 public class MutualInformation extends NXYSignificanceHeuristic {
 
-    static final MutualInformation PROTOTYPE = new MutualInformation(false, false);
-
     protected static final ParseField NAMES_FIELD = new ParseField("mutual_information");
 
     private static final double log2 = Math.log(2.0);
@@ -54,6 +53,18 @@ public class MutualInformation extends NXYSignificanceHeuristic {
         return result;
     }
 
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return new MutualInformation(in.readBoolean(), in.readBoolean());
+        }
+
+        @Override
+        public String getName() {
+            return NAMES_FIELD.getPreferredName();
+        }
+    };
+
     /**
      * Calculates mutual information
      * see "Information Retrieval", Manning et al., Eq. 13.17
@@ -102,21 +113,9 @@ public class MutualInformation extends NXYSignificanceHeuristic {
     }
 
     @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return new MutualInformation(in.readBoolean(), in.readBoolean());
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName());
-        super.build(builder);
-        builder.endObject();
-        return builder;
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
+        super.writeTo(out);
     }
 
     public static class MutualInformationParser extends NXYParser {
@@ -140,7 +139,7 @@ public class MutualInformation extends NXYSignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName());
+            builder.startObject(STREAM.getName());
             super.build(builder);
             builder.endObject();
             return builder;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
index d3f98f2..c6a6924 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
@@ -28,6 +28,7 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -135,15 +136,10 @@ public abstract class NXYSignificanceHeuristic extends SignificanceHeuristic {
         }
     }
 
-    protected void build(XContentBuilder builder) throws IOException {
-        builder.field(INCLUDE_NEGATIVES_FIELD.getPreferredName(), includeNegatives).field(BACKGROUND_IS_SUPERSET.getPreferredName(),
-                backgroundIsSuperset);
-    }
-
     public static abstract class NXYParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             String givenName = parser.currentName();
             boolean includeNegatives = false;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
index 648c126..aceae8c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
@@ -22,43 +22,38 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
 public class PercentageScore extends SignificanceHeuristic {
 
-    public static final PercentageScore PROTOTYPE = new PercentageScore();
+    public static final PercentageScore INSTANCE = new PercentageScore();
 
-    protected static final ParseField NAMES_FIELD = new ParseField("percentage");
+    protected static final String[] NAMES = {"percentage"};
 
-    public PercentageScore() {
-    }
+    private PercentageScore() {}
 
-    @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return PROTOTYPE;
-    }
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return readFrom(in);
+        }
 
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-    }
+        @Override
+        public String getName() {
+            return NAMES[0];
+        }
+    };
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
-        return builder;
+    public static SignificanceHeuristic readFrom(StreamInput in) throws IOException {
+        return INSTANCE;
     }
 
     /**
@@ -75,21 +70,26 @@ public class PercentageScore extends SignificanceHeuristic {
         return (double) subsetFreq / (double) supersetFreq;
    }
 
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
+    }
+
     public static class PercentageScoreParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             // move to the closing bracket
             if (!parser.nextToken().equals(XContentParser.Token.END_OBJECT)) {
                 throw new ElasticsearchParseException("failed to parse [percentage] significance heuristic. expected an empty object, but got [{}] instead", parser.currentToken());
             }
-            return PROTOTYPE;
+            return new PercentageScore();
         }
 
         @Override
         public String[] getNames() {
-            return NAMES_FIELD.getAllNamesIncludedDeprecated();
+            return NAMES;
         }
     }
 
@@ -97,7 +97,7 @@ public class PercentageScore extends SignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
+            builder.startObject(STREAM.getName()).endObject();
             return builder;
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
index 64a437d..a160451 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
@@ -44,12 +44,9 @@ import java.io.IOException;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
-import java.util.Objects;
 
 public class ScriptHeuristic extends SignificanceHeuristic {
 
-    static final ScriptHeuristic PROTOTYPE = new ScriptHeuristic(null);
-
     protected static final ParseField NAMES_FIELD = new ParseField("script_heuristic");
     private final LongAccessor subsetSizeHolder;
     private final LongAccessor supersetSizeHolder;
@@ -58,11 +55,31 @@ public class ScriptHeuristic extends SignificanceHeuristic {
     ExecutableScript searchScript = null;
     Script script;
 
-    public ScriptHeuristic(Script script) {
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            Script script = Script.readScript(in);
+            return new ScriptHeuristic(null, script);
+        }
+
+        @Override
+        public String getName() {
+            return NAMES_FIELD.getPreferredName();
+        }
+    };
+
+    public ScriptHeuristic(ExecutableScript searchScript, Script script) {
         subsetSizeHolder = new LongAccessor();
         supersetSizeHolder = new LongAccessor();
         subsetDfHolder = new LongAccessor();
         supersetDfHolder = new LongAccessor();
+        this.searchScript = searchScript;
+        if (searchScript != null) {
+            searchScript.setNextVar("_subset_freq", subsetDfHolder);
+            searchScript.setNextVar("_subset_size", subsetSizeHolder);
+            searchScript.setNextVar("_superset_freq", supersetDfHolder);
+            searchScript.setNextVar("_superset_size", supersetSizeHolder);
+        }
         this.script = script;
 
 
@@ -70,16 +87,7 @@ public class ScriptHeuristic extends SignificanceHeuristic {
 
     @Override
     public void initialize(InternalAggregation.ReduceContext context) {
-        initialize(context.scriptService());
-    }
-
-    @Override
-    public void initialize(SearchContext context) {
-        initialize(context.scriptService());
-    }
-
-    public void initialize(ScriptService scriptService) {
-        searchScript = scriptService.executable(script, ScriptContext.Standard.AGGS, Collections.emptyMap());
+        searchScript = context.scriptService().executable(script, ScriptContext.Standard.AGGS, Collections.emptyMap());
         searchScript.setNextVar("_subset_freq", subsetDfHolder);
         searchScript.setNextVar("_subset_size", subsetSizeHolder);
         searchScript.setNextVar("_superset_freq", supersetDfHolder);
@@ -113,54 +121,20 @@ public class ScriptHeuristic extends SignificanceHeuristic {
     }
 
     @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        Script script = Script.readScript(in);
-        return new ScriptHeuristic(script);
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
         script.writeTo(out);
     }
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params builderParams) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName());
-        builder.field(ScriptField.SCRIPT.getPreferredName());
-        script.toXContent(builder, builderParams);
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(script);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        ScriptHeuristic other = (ScriptHeuristic) obj;
-        return Objects.equals(script, other.script);
-    }
-
     public static class ScriptHeuristicParser implements SignificanceHeuristicParser {
+        private final ScriptService scriptService;
 
-        public ScriptHeuristicParser() {
+        public ScriptHeuristicParser(ScriptService scriptService) {
+            this.scriptService = scriptService;
         }
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             String heuristicName = parser.currentName();
             Script script = null;
@@ -199,7 +173,13 @@ public class ScriptHeuristic extends SignificanceHeuristic {
             if (script == null) {
                 throw new ElasticsearchParseException("failed to parse [{}] significance heuristic. no script found in script_heuristic", heuristicName);
             }
-            return new ScriptHeuristic(script);
+            ExecutableScript searchScript;
+            try {
+                searchScript = scriptService.executable(script, ScriptContext.Standard.AGGS, Collections.emptyMap());
+            } catch (Exception e) {
+                throw new ElasticsearchParseException("failed to parse [{}] significance heuristic. the script [{}] could not be loaded", e, script, heuristicName);
+            }
+            return new ScriptHeuristic(searchScript, script);
         }
 
         @Override
@@ -219,7 +199,7 @@ public class ScriptHeuristic extends SignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params builderParams) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName());
+            builder.startObject(STREAM.getName());
             builder.field(ScriptField.SCRIPT.getPreferredName());
             script.toXContent(builder, builderParams);
             builder.endObject();
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java
index 972696b..4f12277 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java
@@ -20,12 +20,12 @@
 package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 
-import org.elasticsearch.common.io.stream.NamedWriteable;
-import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.internal.SearchContext;
 
-public abstract class SignificanceHeuristic implements NamedWriteable<SignificanceHeuristic>, ToXContent {
+import java.io.IOException;
+
+public abstract class SignificanceHeuristic {
     /**
      * @param subsetFreq   The frequency of the term in the selected sample
      * @param subsetSize   The size of the selected sample (typically number of docs)
@@ -35,6 +35,8 @@ public abstract class SignificanceHeuristic implements NamedWriteable<Significan
      */
     public abstract double getScore(long subsetFreq, long subsetSize, long supersetFreq, long supersetSize);
 
+    abstract public void writeTo(StreamOutput out) throws IOException;
+
     protected void checkFrequencyValidity(long subsetFreq, long subsetSize, long supersetFreq, long supersetSize, String scoreFunctionName) {
         if (subsetFreq < 0 || subsetSize < 0 || supersetFreq < 0 || supersetSize < 0) {
             throw new IllegalArgumentException("Frequencies of subset and superset must be positive in " + scoreFunctionName + ".getScore()");
@@ -50,8 +52,4 @@ public abstract class SignificanceHeuristic implements NamedWriteable<Significan
     public void initialize(InternalAggregation.ReduceContext reduceContext) {
 
     }
-
-    public void initialize(SearchContext context) {
-
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java
index aa430dc..92baa43 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java
@@ -23,12 +23,13 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
 public interface SignificanceHeuristicParser {
 
-    SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException,
+    SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context) throws IOException,
             ParsingException;
 
     String[] getNames();
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParserMapper.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParserMapper.java
index a9b385c..07e74ec 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParserMapper.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParserMapper.java
@@ -21,6 +21,8 @@
 package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.script.ScriptService;
+
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
@@ -31,14 +33,14 @@ public class SignificanceHeuristicParserMapper {
     protected final Map<String, SignificanceHeuristicParser> significanceHeuristicParsers;
 
     @Inject
-    public SignificanceHeuristicParserMapper(Set<SignificanceHeuristicParser> parsers) {
+    public SignificanceHeuristicParserMapper(Set<SignificanceHeuristicParser> parsers, ScriptService scriptService) {
         Map<String, SignificanceHeuristicParser> map = new HashMap<>();
         add(map, new JLHScore.JLHScoreParser());
         add(map, new PercentageScore.PercentageScoreParser());
         add(map, new MutualInformation.MutualInformationParser());
         add(map, new ChiSquare.ChiSquareParser());
         add(map, new GND.GNDParser());
-        add(map, new ScriptHeuristic.ScriptHeuristicParser());
+        add(map, new ScriptHeuristic.ScriptHeuristicParser(scriptService));
         for (SignificanceHeuristicParser parser : parsers) {
             add(map, parser);
         }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicStreams.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicStreams.java
index 2ffe5ec..198f129 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicStreams.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicStreams.java
@@ -19,7 +19,6 @@
 package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 
 import java.io.IOException;
 import java.util.Collections;
@@ -32,26 +31,21 @@ import java.util.Map;
  */
 public class SignificanceHeuristicStreams {
 
-    private static Map<String, SignificanceHeuristic> STREAMS = Collections.emptyMap();
+    private static Map<String, Stream> STREAMS = Collections.emptyMap();
 
     static {
-        HashMap<String, SignificanceHeuristic> map = new HashMap<>();
-        map.put(JLHScore.NAMES_FIELD.getPreferredName(), JLHScore.PROTOTYPE);
-        map.put(PercentageScore.NAMES_FIELD.getPreferredName(), PercentageScore.PROTOTYPE);
-        map.put(MutualInformation.NAMES_FIELD.getPreferredName(), MutualInformation.PROTOTYPE);
-        map.put(GND.NAMES_FIELD.getPreferredName(), GND.PROTOTYPE);
-        map.put(ChiSquare.NAMES_FIELD.getPreferredName(), ChiSquare.PROTOTYPE);
-        map.put(ScriptHeuristic.NAMES_FIELD.getPreferredName(), ScriptHeuristic.PROTOTYPE);
+        HashMap<String, Stream> map = new HashMap<>();
+        map.put(JLHScore.STREAM.getName(), JLHScore.STREAM);
+        map.put(PercentageScore.STREAM.getName(), PercentageScore.STREAM);
+        map.put(MutualInformation.STREAM.getName(), MutualInformation.STREAM);
+        map.put(GND.STREAM.getName(), GND.STREAM);
+        map.put(ChiSquare.STREAM.getName(), ChiSquare.STREAM);
+        map.put(ScriptHeuristic.STREAM.getName(), ScriptHeuristic.STREAM);
         STREAMS = Collections.unmodifiableMap(map);
     }
 
     public static SignificanceHeuristic read(StreamInput in) throws IOException {
-        return stream(in.readString()).readFrom(in);
-    }
-
-    public static void writeTo(SignificanceHeuristic significanceHeuristic, StreamOutput out) throws IOException {
-        out.writeString(significanceHeuristic.getWriteableName());
-        significanceHeuristic.writeTo(out);
+        return stream(in.readString()).readResult(in);
     }
 
     /**
@@ -65,18 +59,17 @@ public class SignificanceHeuristicStreams {
     }
 
     /**
-     * Registers the given prototype.
+     * Registers the given stream and associate it with the given types.
      *
-     * @param prototype
-     *            The prototype to register
+     * @param stream The stream to register
      */
-    public static synchronized void registerPrototype(SignificanceHeuristic prototype) {
-        if (STREAMS.containsKey(prototype.getWriteableName())) {
-            throw new IllegalArgumentException("Can't register stream with name [" + prototype.getWriteableName() + "] more than once");
+    public static synchronized void registerStream(Stream stream) {
+        if (STREAMS.containsKey(stream.getName())) {
+            throw new IllegalArgumentException("Can't register stream with name [" + stream.getName() + "] more than once");
         }
-        HashMap<String, SignificanceHeuristic> map = new HashMap<>();
+        HashMap<String, Stream> map = new HashMap<>();
         map.putAll(STREAMS);
-        map.put(prototype.getWriteableName(), prototype);
+        map.put(stream.getName(), stream);
         STREAMS = Collections.unmodifiableMap(map);
     }
 
@@ -86,7 +79,7 @@ public class SignificanceHeuristicStreams {
      * @param name The given name
      * @return The associated stream
      */
-    private static synchronized SignificanceHeuristic stream(String name) {
+    private static synchronized Stream stream(String name) {
         return STREAMS.get(name);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java
new file mode 100644
index 0000000..891526c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java
@@ -0,0 +1,111 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.search.aggregations.bucket.terms;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
+import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+
+public abstract class AbstractTermsParametersParser {
+
+    public static final ParseField EXECUTION_HINT_FIELD_NAME = new ParseField("execution_hint");
+    public static final ParseField SHARD_SIZE_FIELD_NAME = new ParseField("shard_size");
+    public static final ParseField MIN_DOC_COUNT_FIELD_NAME = new ParseField("min_doc_count");
+    public static final ParseField SHARD_MIN_DOC_COUNT_FIELD_NAME = new ParseField("shard_min_doc_count");
+    public static final ParseField REQUIRED_SIZE_FIELD_NAME = new ParseField("size");
+    public static final ParseField SHOW_TERM_DOC_COUNT_ERROR = new ParseField("show_term_doc_count_error");
+    
+
+    //These are the results of the parsing.
+    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new TermsAggregator.BucketCountThresholds();
+
+    private String executionHint = null;
+    
+    private SubAggCollectionMode collectMode = SubAggCollectionMode.DEPTH_FIRST;
+
+
+    IncludeExclude includeExclude;
+
+    public TermsAggregator.BucketCountThresholds getBucketCountThresholds() {return bucketCountThresholds;}
+
+    //These are the results of the parsing.
+
+    public String getExecutionHint() {
+        return executionHint;
+    }
+
+    public IncludeExclude getIncludeExclude() {
+        return includeExclude;
+    }
+    
+    public SubAggCollectionMode getCollectionMode() {
+        return collectMode;
+    }
+
+    public void parse(String aggregationName, XContentParser parser, SearchContext context, ValuesSourceParser vsParser, IncludeExclude.Parser incExcParser) throws IOException {
+        bucketCountThresholds = getDefaultBucketCountThresholds();
+        XContentParser.Token token;
+        String currentFieldName = null;
+
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (incExcParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_STRING) {
+                if (context.parseFieldMatcher().match(currentFieldName, EXECUTION_HINT_FIELD_NAME)) {
+                    executionHint = parser.text();
+                } else if(context.parseFieldMatcher().match(currentFieldName, SubAggCollectionMode.KEY)){
+                    collectMode = SubAggCollectionMode.parse(parser.text(), context.parseFieldMatcher());
+                } else if (context.parseFieldMatcher().match(currentFieldName, REQUIRED_SIZE_FIELD_NAME)) {
+                    bucketCountThresholds.setRequiredSize(parser.intValue());
+                } else {
+                    parseSpecial(aggregationName, parser, context, token, currentFieldName);
+                }
+            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                if (context.parseFieldMatcher().match(currentFieldName, REQUIRED_SIZE_FIELD_NAME)) {
+                    bucketCountThresholds.setRequiredSize(parser.intValue());
+                } else if (context.parseFieldMatcher().match(currentFieldName, SHARD_SIZE_FIELD_NAME)) {
+                    bucketCountThresholds.setShardSize(parser.intValue());
+                } else if (context.parseFieldMatcher().match(currentFieldName, MIN_DOC_COUNT_FIELD_NAME)) {
+                    bucketCountThresholds.setMinDocCount(parser.intValue());
+                } else if (context.parseFieldMatcher().match(currentFieldName, SHARD_MIN_DOC_COUNT_FIELD_NAME)) {
+                    bucketCountThresholds.setShardMinDocCount(parser.longValue());
+                } else {
+                    parseSpecial(aggregationName, parser, context, token, currentFieldName);
+                }
+            } else {
+                parseSpecial(aggregationName, parser, context, token, currentFieldName);
+            }
+        }
+        includeExclude = incExcParser.includeExclude();
+    }
+
+    public abstract void parseSpecial(String aggregationName, XContentParser parser, SearchContext context, XContentParser.Token token, String currentFieldName) throws IOException;
+
+    protected abstract TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds();
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParser.java
deleted file mode 100644
index a15c7d2..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParser.java
+++ /dev/null
@@ -1,130 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.terms;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
-
-public abstract class AbstractTermsParser extends AnyValuesSourceParser {
-
-    public static final ParseField EXECUTION_HINT_FIELD_NAME = new ParseField("execution_hint");
-    public static final ParseField SHARD_SIZE_FIELD_NAME = new ParseField("shard_size");
-    public static final ParseField MIN_DOC_COUNT_FIELD_NAME = new ParseField("min_doc_count");
-    public static final ParseField SHARD_MIN_DOC_COUNT_FIELD_NAME = new ParseField("shard_min_doc_count");
-    public static final ParseField REQUIRED_SIZE_FIELD_NAME = new ParseField("size");
-
-    public IncludeExclude.Parser incExcParser = new IncludeExclude.Parser();
-
-    protected AbstractTermsParser() {
-        super(true, true);
-    }
-
-    @Override
-    protected final ValuesSourceAggregatorBuilder<ValuesSource, ?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        BucketCountThresholds bucketCountThresholds = getDefaultBucketCountThresholds();
-        Integer requiredSize = (Integer) otherOptions.get(REQUIRED_SIZE_FIELD_NAME);
-        if (requiredSize != null && requiredSize != -1) {
-            bucketCountThresholds.setRequiredSize(requiredSize);
-        }
-        Integer shardSize = (Integer) otherOptions.get(SHARD_SIZE_FIELD_NAME);
-        if (shardSize != null && shardSize != -1) {
-            bucketCountThresholds.setShardSize(shardSize);
-        }
-        Long minDocCount = (Long) otherOptions.get(MIN_DOC_COUNT_FIELD_NAME);
-        if (minDocCount != null && minDocCount != -1) {
-            bucketCountThresholds.setMinDocCount(minDocCount);
-        }
-        Long shardMinDocCount = (Long) otherOptions.get(SHARD_MIN_DOC_COUNT_FIELD_NAME);
-        if (shardMinDocCount != null && shardMinDocCount != -1) {
-            bucketCountThresholds.setShardMinDocCount(shardMinDocCount);
-        }
-        SubAggCollectionMode collectMode = (SubAggCollectionMode) otherOptions.get(SubAggCollectionMode.KEY);
-        String executionHint = (String) otherOptions.get(EXECUTION_HINT_FIELD_NAME);
-        IncludeExclude incExc = incExcParser.createIncludeExclude(otherOptions);
-        return doCreateFactory(aggregationName, valuesSourceType, targetValueType, bucketCountThresholds, collectMode, executionHint,
-                incExc,
-                otherOptions);
-    }
-
-    protected abstract ValuesSourceAggregatorBuilder<ValuesSource, ?> doCreateFactory(String aggregationName,
-            ValuesSourceType valuesSourceType,
-            ValueType targetValueType, BucketCountThresholds bucketCountThresholds, SubAggCollectionMode collectMode, String executionHint,
-            IncludeExclude incExc, Map<ParseField, Object> otherOptions);
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (incExcParser.token(currentFieldName, token, parser, parseFieldMatcher, otherOptions)) {
-            return true;
-        } else if (token == XContentParser.Token.VALUE_STRING) {
-            if (parseFieldMatcher.match(currentFieldName, EXECUTION_HINT_FIELD_NAME)) {
-                otherOptions.put(EXECUTION_HINT_FIELD_NAME, parser.text());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SubAggCollectionMode.KEY)) {
-                otherOptions.put(SubAggCollectionMode.KEY, SubAggCollectionMode.parse(parser.text(), parseFieldMatcher));
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, REQUIRED_SIZE_FIELD_NAME)) {
-                otherOptions.put(REQUIRED_SIZE_FIELD_NAME, parser.intValue());
-                return true;
-            } else if (parseSpecial(aggregationName, parser, parseFieldMatcher, token, currentFieldName, otherOptions)) {
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_NUMBER) {
-            if (parseFieldMatcher.match(currentFieldName, REQUIRED_SIZE_FIELD_NAME)) {
-                otherOptions.put(REQUIRED_SIZE_FIELD_NAME, parser.intValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SHARD_SIZE_FIELD_NAME)) {
-                otherOptions.put(SHARD_SIZE_FIELD_NAME, parser.intValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, MIN_DOC_COUNT_FIELD_NAME)) {
-                otherOptions.put(MIN_DOC_COUNT_FIELD_NAME, parser.longValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SHARD_MIN_DOC_COUNT_FIELD_NAME)) {
-                otherOptions.put(SHARD_MIN_DOC_COUNT_FIELD_NAME, parser.longValue());
-                return true;
-            } else if (parseSpecial(aggregationName, parser, parseFieldMatcher, token, currentFieldName, otherOptions)) {
-                return true;
-            }
-        } else if (parseSpecial(aggregationName, parser, parseFieldMatcher, token, currentFieldName, otherOptions)) {
-            return true;
-        }
-        return false;
-    }
-
-    public abstract boolean parseSpecial(String aggregationName, XContentParser parser, ParseFieldMatcher parseFieldMatcher,
-            XContentParser.Token token, String currentFieldName, Map<ParseField, Object> otherOptions) throws IOException;
-
-    protected abstract TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds();
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/GlobalOrdinalsStringTermsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/GlobalOrdinalsStringTermsAggregator.java
index 91e949e..1e7a004 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/GlobalOrdinalsStringTermsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/GlobalOrdinalsStringTermsAggregator.java
@@ -347,7 +347,7 @@ public class GlobalOrdinalsStringTermsAggregator extends AbstractStringTermsAggr
                 Map<String, Object> metaData) throws IOException {
             super(name, factories, valuesSource, order, bucketCountThresholds, null, aggregationContext, parent, collectionMode,
                     showTermDocCountError, pipelineAggregators, metaData);
-            assert factories == null || factories.countAggregators() == 0;
+            assert factories == null || factories.count() == 0;
             this.segmentDocCounts = context.bigArrays().newIntArray(1, true);
         }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java
index f3f87c0..4e3e28a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java
@@ -38,7 +38,6 @@ import java.util.Comparator;
 import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.List;
-import java.util.Objects;
 
 /**
  *
@@ -96,7 +95,7 @@ class InternalOrder extends Terms.Order {
     public static boolean isCountDesc(Terms.Order order) {
         if (order == COUNT_DESC) {
             return true;
-        } else if (order instanceof CompoundOrder) {
+        }else if (order instanceof CompoundOrder) {
             // check if its a compound order with count desc and the tie breaker (term asc)
             CompoundOrder compoundOrder = (CompoundOrder) order;
             if (compoundOrder.orderElements.size() == 2 && compoundOrder.orderElements.get(0) == COUNT_DESC && compoundOrder.orderElements.get(1) == TERM_ASC) {
@@ -106,23 +105,6 @@ class InternalOrder extends Terms.Order {
         return false;
     }
 
-    public static boolean isTermOrder(Terms.Order order) {
-        if (order == TERM_ASC) {
-            return true;
-        } else if (order == TERM_DESC) {
-            return true;
-        } else if (order instanceof CompoundOrder) {
-            // check if its a compound order with only a single element ordering
-            // by term
-            CompoundOrder compoundOrder = (CompoundOrder) order;
-            if (compoundOrder.orderElements.size() == 1 && compoundOrder.orderElements.get(0) == TERM_ASC
-                    || compoundOrder.orderElements.get(0) == TERM_DESC) {
-                return true;
-            }
-        }
-        return false;
-    }
-
     final byte id;
     final String key;
     final boolean asc;
@@ -281,23 +263,6 @@ class InternalOrder extends Terms.Order {
             return new CompoundOrderComparator(orderElements, aggregator);
         }
 
-        @Override
-        public int hashCode() {
-            return Objects.hash(orderElements);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            CompoundOrder other = (CompoundOrder) obj;
-            return Objects.equals(orderElements, other.orderElements);
-        }
-
         public static class CompoundOrderComparator implements Comparator<Terms.Bucket> {
 
             private List<Terms.Order> compoundOrder;
@@ -341,7 +306,7 @@ class InternalOrder extends Terms.Order {
         }
 
         public static Terms.Order readOrder(StreamInput in) throws IOException {
-            return readOrder(in, false);
+            return readOrder(in, true);
         }
 
         public static Terms.Order readOrder(StreamInput in, boolean absoluteOrder) throws IOException {
@@ -367,22 +332,4 @@ class InternalOrder extends Terms.Order {
             }
         }
     }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(id, asc);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        InternalOrder other = (InternalOrder) obj;
-        return Objects.equals(id, other.id)
-                && Objects.equals(asc, other.asc);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java
index 6e5aaac..31285d2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java
@@ -188,7 +188,7 @@ public abstract class InternalTerms<A extends InternalTerms, B extends InternalT
             }
             otherDocCount += terms.getSumOfOtherDocCounts();
             final long thisAggDocCountError;
-            if (terms.buckets.size() < this.shardSize || InternalOrder.isTermOrder(order)) {
+            if (terms.buckets.size() < this.shardSize || this.order == InternalOrder.TERM_ASC || this.order == InternalOrder.TERM_DESC) {
                 thisAggDocCountError = 0;
             } else if (InternalOrder.isCountDesc(this.order)) {
                 thisAggDocCountError = terms.buckets.get(terms.buckets.size() - 1).docCount;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java
index 16fb7f1..224e42c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java
@@ -82,7 +82,7 @@ public interface Terms extends MultiBucketsAggregation {
      * Get the bucket for the given term, or null if there is no such bucket.
      */
     Bucket getBucketByKey(String term);
-
+    
     /**
      * Get an upper bound of the error on document counts in this aggregation.
      */
@@ -166,11 +166,5 @@ public interface Terms extends MultiBucketsAggregation {
 
         abstract byte id();
 
-        @Override
-        public abstract int hashCode();
-
-        @Override
-        public abstract boolean equals(Object obj);
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java
index 699793e..7971d1f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java
@@ -21,10 +21,7 @@
 package org.elasticsearch.search.aggregations.bucket.terms;
 
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.Explicit;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
@@ -39,136 +36,99 @@ import java.io.IOException;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.Set;
 
 public abstract class TermsAggregator extends BucketsAggregator {
 
-    public static class BucketCountThresholds implements Writeable<BucketCountThresholds>, ToXContent {
-
-        private static final BucketCountThresholds PROTOTYPE = new BucketCountThresholds(-1, -1, -1, -1);
-
-        private long minDocCount;
-        private long shardMinDocCount;
-        private int requiredSize;
-        private int shardSize;
-
-        public static BucketCountThresholds readFromStream(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
-        }
+    public static class BucketCountThresholds {
+        private Explicit<Long> minDocCount;
+        private Explicit<Long> shardMinDocCount;
+        private Explicit<Integer> requiredSize;
+        private Explicit<Integer> shardSize;
 
         public BucketCountThresholds(long minDocCount, long shardMinDocCount, int requiredSize, int shardSize) {
-            this.minDocCount = minDocCount;
-            this.shardMinDocCount = shardMinDocCount;
-            this.requiredSize = requiredSize;
-            this.shardSize = shardSize;
+            this.minDocCount = new Explicit<>(minDocCount, false);
+            this.shardMinDocCount =  new Explicit<>(shardMinDocCount, false);
+            this.requiredSize = new Explicit<>(requiredSize, false);
+            this.shardSize = new Explicit<>(shardSize, false);
+        }
+        public BucketCountThresholds() {
+            this(-1, -1, -1, -1);
         }
 
         public BucketCountThresholds(BucketCountThresholds bucketCountThresholds) {
-            this(bucketCountThresholds.minDocCount, bucketCountThresholds.shardMinDocCount, bucketCountThresholds.requiredSize,
-                    bucketCountThresholds.shardSize);
+            this(bucketCountThresholds.minDocCount.value(), bucketCountThresholds.shardMinDocCount.value(), bucketCountThresholds.requiredSize.value(), bucketCountThresholds.shardSize.value());
         }
 
         public void ensureValidity() {
 
-            if (shardSize == 0) {
+            if (shardSize.value() == 0) {
                 setShardSize(Integer.MAX_VALUE);
             }
 
-            if (requiredSize == 0) {
+            if (requiredSize.value() == 0) {
                 setRequiredSize(Integer.MAX_VALUE);
             }
             // shard_size cannot be smaller than size as we need to at least fetch <size> entries from every shards in order to return <size>
-            if (shardSize < requiredSize) {
-                setShardSize(requiredSize);
+            if (shardSize.value() < requiredSize.value()) {
+                setShardSize(requiredSize.value());
             }
 
             // shard_min_doc_count should not be larger than min_doc_count because this can cause buckets to be removed that would match the min_doc_count criteria
-            if (shardMinDocCount > minDocCount) {
-                setShardMinDocCount(minDocCount);
+            if (shardMinDocCount.value() > minDocCount.value()) {
+                setShardMinDocCount(minDocCount.value());
             }
 
-            if (requiredSize < 0 || minDocCount < 0) {
+            if (requiredSize.value() < 0 || minDocCount.value() < 0) {
                 throw new ElasticsearchException("parameters [requiredSize] and [minDocCount] must be >=0 in terms aggregation.");
             }
         }
 
         public long getShardMinDocCount() {
-            return shardMinDocCount;
+            return shardMinDocCount.value();
         }
 
         public void setShardMinDocCount(long shardMinDocCount) {
-            this.shardMinDocCount = shardMinDocCount;
+            this.shardMinDocCount = new Explicit<>(shardMinDocCount, true);
         }
 
         public long getMinDocCount() {
-            return minDocCount;
+            return minDocCount.value();
         }
 
         public void setMinDocCount(long minDocCount) {
-            this.minDocCount = minDocCount;
+            this.minDocCount = new Explicit<>(minDocCount, true);
         }
 
         public int getRequiredSize() {
-            return requiredSize;
+            return requiredSize.value();
         }
 
         public void setRequiredSize(int requiredSize) {
-            this.requiredSize = requiredSize;
+            this.requiredSize = new Explicit<>(requiredSize, true);
         }
 
         public int getShardSize() {
-            return shardSize;
+            return shardSize.value();
         }
 
         public void setShardSize(int shardSize) {
-            this.shardSize = shardSize;
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(TermsAggregatorBuilder.REQUIRED_SIZE_FIELD_NAME.getPreferredName(), requiredSize);
-            builder.field(TermsAggregatorBuilder.SHARD_SIZE_FIELD_NAME.getPreferredName(), shardSize);
-            builder.field(TermsAggregatorBuilder.MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), minDocCount);
-            builder.field(TermsAggregatorBuilder.SHARD_MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), shardMinDocCount);
-            return builder;
+            this.shardSize = new Explicit<>(shardSize, true);
         }
 
-        @Override
-        public BucketCountThresholds readFrom(StreamInput in) throws IOException {
-            int requiredSize = in.readInt();
-            int shardSize = in.readInt();
-            long minDocCount = in.readLong();
-            long shardMinDocCount = in.readLong();
-            return new BucketCountThresholds(minDocCount, shardMinDocCount, requiredSize, shardSize);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeInt(requiredSize);
-            out.writeInt(shardSize);
-            out.writeLong(minDocCount);
-            out.writeLong(shardMinDocCount);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(requiredSize, shardSize, minDocCount, shardMinDocCount);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
+        public void toXContent(XContentBuilder builder) throws IOException {
+            if (requiredSize.explicit()) {
+                builder.field(AbstractTermsParametersParser.REQUIRED_SIZE_FIELD_NAME.getPreferredName(), requiredSize.value());
+            }
+            if (shardSize.explicit()) {
+                builder.field(AbstractTermsParametersParser.SHARD_SIZE_FIELD_NAME.getPreferredName(), shardSize.value());
+            }
+            if (minDocCount.explicit()) {
+                builder.field(AbstractTermsParametersParser.MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), minDocCount.value());
             }
-            if (getClass() != obj.getClass()) {
-                return false;
+            if (shardMinDocCount.explicit()) {
+                builder.field(AbstractTermsParametersParser.SHARD_MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), shardMinDocCount.value());
             }
-            BucketCountThresholds other = (BucketCountThresholds) obj;
-            return Objects.equals(requiredSize, other.requiredSize)
-                    && Objects.equals(shardSize, other.shardSize)
-                    && Objects.equals(minDocCount, other.minDocCount)
-                    && Objects.equals(shardMinDocCount, other.shardMinDocCount);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorBuilder.java
deleted file mode 100644
index 2bfdcfe..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorBuilder.java
+++ /dev/null
@@ -1,271 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations.bucket.terms;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import java.io.IOException;
-import java.util.List;
-import java.util.Objects;
-
-/**
- *
- */
-/**
- *
- */
-public class TermsAggregatorBuilder extends ValuesSourceAggregatorBuilder<ValuesSource, TermsAggregatorBuilder> {
-
-    public static final ParseField EXECUTION_HINT_FIELD_NAME = new ParseField("execution_hint");
-    public static final ParseField SHARD_SIZE_FIELD_NAME = new ParseField("shard_size");
-    public static final ParseField MIN_DOC_COUNT_FIELD_NAME = new ParseField("min_doc_count");
-    public static final ParseField SHARD_MIN_DOC_COUNT_FIELD_NAME = new ParseField("shard_min_doc_count");
-    public static final ParseField REQUIRED_SIZE_FIELD_NAME = new ParseField("size");
-
-    static final TermsAggregator.BucketCountThresholds DEFAULT_BUCKET_COUNT_THRESHOLDS = new TermsAggregator.BucketCountThresholds(1, 0, 10,
-            -1);
-    public static final ParseField SHOW_TERM_DOC_COUNT_ERROR = new ParseField("show_term_doc_count_error");
-    public static final ParseField ORDER_FIELD = new ParseField("order");
-
-    private Terms.Order order = Terms.Order.compound(Terms.Order.count(false), Terms.Order.term(true));
-    private IncludeExclude includeExclude = null;
-    private String executionHint = null;
-    private SubAggCollectionMode collectMode = SubAggCollectionMode.DEPTH_FIRST;
-    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new TermsAggregator.BucketCountThresholds(
-            DEFAULT_BUCKET_COUNT_THRESHOLDS);
-    private boolean showTermDocCountError = false;
-
-    public TermsAggregatorBuilder(String name, ValueType valueType) {
-        super(name, StringTerms.TYPE, ValuesSourceType.ANY, valueType);
-    }
-
-    public TermsAggregator.BucketCountThresholds bucketCountThresholds() {
-        return bucketCountThresholds;
-    }
-
-    public TermsAggregatorBuilder bucketCountThresholds(TermsAggregator.BucketCountThresholds bucketCountThresholds) {
-        this.bucketCountThresholds = bucketCountThresholds;
-        return this;
-    }
-
-    /**
-     * Sets the size - indicating how many term buckets should be returned
-     * (defaults to 10)
-     */
-    public TermsAggregatorBuilder size(int size) {
-        bucketCountThresholds.setRequiredSize(size);
-        return this;
-    }
-
-    /**
-     * Sets the shard_size - indicating the number of term buckets each shard
-     * will return to the coordinating node (the node that coordinates the
-     * search execution). The higher the shard size is, the more accurate the
-     * results are.
-     */
-    public TermsAggregatorBuilder shardSize(int shardSize) {
-        bucketCountThresholds.setShardSize(shardSize);
-        return this;
-    }
-
-    /**
-     * Set the minimum document count terms should have in order to appear in
-     * the response.
-     */
-    public TermsAggregatorBuilder minDocCount(long minDocCount) {
-        bucketCountThresholds.setMinDocCount(minDocCount);
-        return this;
-    }
-
-    /**
-     * Set the minimum document count terms should have on the shard in order to
-     * appear in the response.
-     */
-    public TermsAggregatorBuilder shardMinDocCount(long shardMinDocCount) {
-        bucketCountThresholds.setShardMinDocCount(shardMinDocCount);
-        return this;
-    }
-
-    /**
-     * Sets the order in which the buckets will be returned.
-     */
-    public TermsAggregatorBuilder order(Terms.Order order) {
-        this.order = order;
-        return this;
-    }
-
-    /**
-     * Sets the order in which the buckets will be returned.
-     */
-    public TermsAggregatorBuilder order(List<Terms.Order> orders) {
-        order(Terms.Order.compound(orders));
-        return this;
-    }
-
-    /**
-     * Gets the order in which the buckets will be returned.
-     */
-    public Terms.Order order() {
-        return order;
-    }
-
-    /**
-     * Expert: sets an execution hint to the aggregation.
-     */
-    public TermsAggregatorBuilder executionHint(String executionHint) {
-        this.executionHint = executionHint;
-        return this;
-    }
-
-    /**
-     * Expert: gets an execution hint to the aggregation.
-     */
-    public String executionHint() {
-        return executionHint;
-    }
-
-    /**
-     * Expert: set the collection mode.
-     */
-    public TermsAggregatorBuilder collectMode(SubAggCollectionMode mode) {
-        this.collectMode = mode;
-        return this;
-    }
-
-    /**
-     * Expert: get the collection mode.
-     */
-    public SubAggCollectionMode collectMode() {
-        return collectMode;
-    }
-
-    /**
-     * Set terms to include and exclude from the aggregation results
-     */
-    public TermsAggregatorBuilder includeExclude(IncludeExclude includeExclude) {
-        this.includeExclude = includeExclude;
-        return this;
-    }
-
-    /**
-     * Get terms to include and exclude from the aggregation results
-     */
-    public IncludeExclude includeExclude() {
-        return includeExclude;
-    }
-
-    /**
-     * Get whether doc count error will be return for individual terms
-     */
-    public boolean showTermDocCountError() {
-        return showTermDocCountError;
-    }
-
-    /**
-     * Set whether doc count error will be return for individual terms
-     */
-    public TermsAggregatorBuilder showTermDocCountError(boolean showTermDocCountError) {
-        this.showTermDocCountError = showTermDocCountError;
-        return this;
-    }
-
-    @Override
-    protected ValuesSourceAggregatorFactory<ValuesSource, ?> innerBuild(AggregationContext context, ValuesSourceConfig<ValuesSource> config,
-            AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-        return new TermsAggregatorFactory(name, type, config, order, includeExclude, executionHint, collectMode,
- bucketCountThresholds,
-                showTermDocCountError, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        bucketCountThresholds.toXContent(builder, params);
-        builder.field(SHOW_TERM_DOC_COUNT_ERROR.getPreferredName(), showTermDocCountError);
-        if (executionHint != null) {
-            builder.field(TermsAggregatorBuilder.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
-        }
-        builder.field(ORDER_FIELD.getPreferredName());
-        order.toXContent(builder, params);
-        builder.field(SubAggCollectionMode.KEY.getPreferredName(), collectMode.parseField().getPreferredName());
-        if (includeExclude != null) {
-            includeExclude.toXContent(builder, params);
-        }
-        return builder;
-    }
-
-    @Override
-    protected TermsAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        TermsAggregatorBuilder factory = new TermsAggregatorBuilder(name, targetValueType);
-        factory.bucketCountThresholds = BucketCountThresholds.readFromStream(in);
-        factory.collectMode = SubAggCollectionMode.BREADTH_FIRST.readFrom(in);
-        factory.executionHint = in.readOptionalString();
-        if (in.readBoolean()) {
-            factory.includeExclude = IncludeExclude.readFromStream(in);
-        }
-        factory.order = InternalOrder.Streams.readOrder(in);
-        factory.showTermDocCountError = in.readBoolean();
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        bucketCountThresholds.writeTo(out);
-        collectMode.writeTo(out);
-        out.writeOptionalString(executionHint);
-        boolean hasIncExc = includeExclude != null;
-        out.writeBoolean(hasIncExc);
-        if (hasIncExc) {
-            includeExclude.writeTo(out);
-        }
-        InternalOrder.Streams.writeOrder(order, out);
-        out.writeBoolean(showTermDocCountError);
-    }
-
-    @Override
-    protected int innerHashCode() {
-        return Objects.hash(bucketCountThresholds, collectMode, executionHint, includeExclude, order, showTermDocCountError);
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        TermsAggregatorBuilder other = (TermsAggregatorBuilder) obj;
-        return Objects.equals(bucketCountThresholds, other.bucketCountThresholds)
-                && Objects.equals(collectMode, other.collectMode)
-                && Objects.equals(executionHint, other.executionHint)
-                && Objects.equals(includeExclude, other.includeExclude)
-                && Objects.equals(order, other.order)
-                && Objects.equals(showTermDocCountError, other.showTermDocCountError);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java
index cdab6dd..270dc00 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java
@@ -16,7 +16,6 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-
 package org.elasticsearch.search.aggregations.bucket.terms;
 
 import org.apache.lucene.search.IndexSearcher;
@@ -24,14 +23,10 @@ import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.bucket.BucketUtils;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
@@ -43,144 +38,10 @@ import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
-public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource, TermsAggregatorFactory> {
-
-    private final Terms.Order order;
-    private final IncludeExclude includeExclude;
-    private final String executionHint;
-    private final SubAggCollectionMode collectMode;
-    private final TermsAggregator.BucketCountThresholds bucketCountThresholds;
-    private boolean showTermDocCountError;
-
-    public TermsAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource> config, Terms.Order order,
-            IncludeExclude includeExclude, String executionHint, SubAggCollectionMode collectMode,
-            TermsAggregator.BucketCountThresholds bucketCountThresholds, boolean showTermDocCountError, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.order = order;
-        this.includeExclude = includeExclude;
-        this.executionHint = executionHint;
-        this.collectMode = collectMode;
-        this.bucketCountThresholds = bucketCountThresholds;
-        this.showTermDocCountError = showTermDocCountError;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        final InternalAggregation aggregation = new UnmappedTerms(name, order, bucketCountThresholds.getRequiredSize(),
-                bucketCountThresholds.getShardSize(), bucketCountThresholds.getMinDocCount(), pipelineAggregators, metaData);
-        return new NonCollectingAggregator(name, aggregationContext, parent, factories, pipelineAggregators, metaData) {
-            {
-                // even in the case of an unmapped aggregator, validate the
-                // order
-                InternalOrder.validate(order, this);
-            }
-
-            @Override
-            public InternalAggregation buildEmptyAggregation() {
-                return aggregation;
-            }
-        };
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        if (collectsFromSingleBucket == false) {
-            return asMultiBucketAggregator(this, aggregationContext, parent);
-        }
-        BucketCountThresholds bucketCountThresholds = new BucketCountThresholds(this.bucketCountThresholds);
-        if (!(order == InternalOrder.TERM_ASC || order == InternalOrder.TERM_DESC)
-                && bucketCountThresholds.getShardSize() == TermsAggregatorBuilder.DEFAULT_BUCKET_COUNT_THRESHOLDS.getShardSize()) {
-            // The user has not made a shardSize selection. Use default
-            // heuristic to avoid any wrong-ranking caused by distributed
-            // counting
-            bucketCountThresholds.setShardSize(BucketUtils.suggestShardSideQueueSize(bucketCountThresholds.getRequiredSize(),
-                    aggregationContext.searchContext().numberOfShards()));
-        }
-        bucketCountThresholds.ensureValidity();
-        if (valuesSource instanceof ValuesSource.Bytes) {
-            ExecutionMode execution = null;
-            if (executionHint != null) {
-                execution = ExecutionMode.fromString(executionHint, aggregationContext.searchContext().parseFieldMatcher());
-            }
-
-            // In some cases, using ordinals is just not supported: override it
-            if (!(valuesSource instanceof ValuesSource.Bytes.WithOrdinals)) {
-                execution = ExecutionMode.MAP;
-            }
-
-            final long maxOrd;
-            final double ratio;
-            if (execution == null || execution.needsGlobalOrdinals()) {
-                ValuesSource.Bytes.WithOrdinals valueSourceWithOrdinals = (ValuesSource.Bytes.WithOrdinals) valuesSource;
-                IndexSearcher indexSearcher = aggregationContext.searchContext().searcher();
-                maxOrd = valueSourceWithOrdinals.globalMaxOrd(indexSearcher);
-                ratio = maxOrd / ((double) indexSearcher.getIndexReader().numDocs());
-            } else {
-                maxOrd = -1;
-                ratio = -1;
-            }
-
-            // Let's try to use a good default
-            if (execution == null) {
-                // if there is a parent bucket aggregator the number of
-                // instances of this aggregator is going
-                // to be unbounded and most instances may only aggregate few
-                // documents, so use hashed based
-                // global ordinals to keep the bucket ords dense.
-                if (Aggregator.descendsFromBucketAggregator(parent)) {
-                    execution = ExecutionMode.GLOBAL_ORDINALS_HASH;
-                } else {
-                    if (factories == AggregatorFactories.EMPTY) {
-                        if (ratio <= 0.5 && maxOrd <= 2048) {
-                            // 0.5: At least we need reduce the number of global
-                            // ordinals look-ups by half
-                            // 2048: GLOBAL_ORDINALS_LOW_CARDINALITY has
-                            // additional memory usage, which directly linked to
-                            // maxOrd, so we need to limit.
-                            execution = ExecutionMode.GLOBAL_ORDINALS_LOW_CARDINALITY;
-                        } else {
-                            execution = ExecutionMode.GLOBAL_ORDINALS;
-                        }
-                    } else {
-                        execution = ExecutionMode.GLOBAL_ORDINALS;
-                    }
-                }
-            }
-
-            return execution.create(name, factories, valuesSource, order, bucketCountThresholds, includeExclude, aggregationContext, parent,
-                    collectMode, showTermDocCountError, pipelineAggregators, metaData);
-        }
-
-        if ((includeExclude != null) && (includeExclude.isRegexBased())) {
-            throw new AggregationExecutionException("Aggregation [" + name + "] cannot support regular expression style include/exclude "
-                    + "settings as they can only be applied to string fields. Use an array of numeric values for include/exclude clauses used to filter numeric fields");
-        }
-
-        if (valuesSource instanceof ValuesSource.Numeric) {
-            IncludeExclude.LongFilter longFilter = null;
-            if (((ValuesSource.Numeric) valuesSource).isFloatingPoint()) {
-                if (includeExclude != null) {
-                    longFilter = includeExclude.convertToDoubleFilter();
-                }
-                return new DoubleTermsAggregator(name, factories, (ValuesSource.Numeric) valuesSource, config.format(), order,
-                        bucketCountThresholds, aggregationContext, parent, collectMode, showTermDocCountError, longFilter,
-                        pipelineAggregators, metaData);
-            }
-            if (includeExclude != null) {
-                longFilter = includeExclude.convertToLongFilter();
-            }
-            return new LongTermsAggregator(name, factories, (ValuesSource.Numeric) valuesSource, config.format(), order,
-                    bucketCountThresholds, aggregationContext, parent, collectMode, showTermDocCountError, longFilter, pipelineAggregators,
-                    metaData);
-        }
-
-        throw new AggregationExecutionException("terms aggregation cannot be applied to field [" + config.fieldContext().field()
-                + "]. It can only be applied to numeric or string fields.");
-    }
+/**
+ *
+ */
+public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource> {
 
     public enum ExecutionMode {
 
@@ -191,7 +52,7 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
                     TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
                     AggregationContext aggregationContext, Aggregator parent, SubAggCollectionMode subAggCollectMode,
                     boolean showTermDocCountError, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                            throws IOException {
+                    throws IOException {
                 final IncludeExclude.StringFilter filter = includeExclude == null ? null : includeExclude.convertToStringFilter();
                 return new StringTermsAggregator(name, factories, valuesSource, order, bucketCountThresholds, filter, aggregationContext,
                         parent, subAggCollectMode, showTermDocCountError, pipelineAggregators, metaData);
@@ -210,7 +71,7 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
                     TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
                     AggregationContext aggregationContext, Aggregator parent, SubAggCollectionMode subAggCollectMode,
                     boolean showTermDocCountError, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                            throws IOException {
+                    throws IOException {
                 final IncludeExclude.OrdinalsFilter filter = includeExclude == null ? null : includeExclude.convertToOrdinalsFilter();
                 return new GlobalOrdinalsStringTermsAggregator(name, factories, (ValuesSource.Bytes.WithOrdinals) valuesSource, order,
                         bucketCountThresholds, filter, aggregationContext, parent, subAggCollectMode, showTermDocCountError,
@@ -230,11 +91,11 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
                     TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
                     AggregationContext aggregationContext, Aggregator parent, SubAggCollectionMode subAggCollectMode,
                     boolean showTermDocCountError, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                            throws IOException {
+                    throws IOException {
                 final IncludeExclude.OrdinalsFilter filter = includeExclude == null ? null : includeExclude.convertToOrdinalsFilter();
-                return new GlobalOrdinalsStringTermsAggregator.WithHash(name, factories, (ValuesSource.Bytes.WithOrdinals) valuesSource,
-                        order, bucketCountThresholds, filter, aggregationContext, parent, subAggCollectMode, showTermDocCountError,
-                        pipelineAggregators, metaData);
+                return new GlobalOrdinalsStringTermsAggregator.WithHash(name, factories,
+                        (ValuesSource.Bytes.WithOrdinals) valuesSource, order, bucketCountThresholds, filter, aggregationContext,
+                        parent, subAggCollectMode, showTermDocCountError, pipelineAggregators, metaData);
             }
 
             @Override
@@ -249,10 +110,10 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
                     TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
                     AggregationContext aggregationContext, Aggregator parent, SubAggCollectionMode subAggCollectMode,
                     boolean showTermDocCountError, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                            throws IOException {
-                if (includeExclude != null || factories.countAggregators() > 0
-                // we need the FieldData impl to be able to extract the
-                // segment to global ord mapping
+                    throws IOException {
+                if (includeExclude != null || factories.count() > 0
+                        // we need the FieldData impl to be able to extract the
+                        // segment to global ord mapping
                         || valuesSource.getClass() != ValuesSource.Bytes.FieldData.class) {
                     return GLOBAL_ORDINALS.create(name, factories, valuesSource, order, bucketCountThresholds, includeExclude,
                             aggregationContext, parent, subAggCollectMode, showTermDocCountError, pipelineAggregators, metaData);
@@ -287,7 +148,7 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
                 TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
                 AggregationContext aggregationContext, Aggregator parent, SubAggCollectionMode subAggCollectMode,
                 boolean showTermDocCountError, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                        throws IOException;
+                throws IOException;
 
         abstract boolean needsGlobalOrdinals();
 
@@ -297,4 +158,127 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
         }
     }
 
+    private final Terms.Order order;
+    private final IncludeExclude includeExclude;
+    private final String executionHint;
+    private final SubAggCollectionMode collectMode;
+    private final TermsAggregator.BucketCountThresholds bucketCountThresholds;
+    private final boolean showTermDocCountError;
+
+    public TermsAggregatorFactory(String name, ValuesSourceConfig config, Terms.Order order,
+            TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude, String executionHint,
+            SubAggCollectionMode executionMode, boolean showTermDocCountError) {
+        super(name, StringTerms.TYPE.name(), config);
+        this.order = order;
+        this.includeExclude = includeExclude;
+        this.executionHint = executionHint;
+        this.bucketCountThresholds = bucketCountThresholds;
+        this.collectMode = executionMode;
+        this.showTermDocCountError = showTermDocCountError;
+    }
+
+    @Override
+    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+        final InternalAggregation aggregation = new UnmappedTerms(name, order, bucketCountThresholds.getRequiredSize(),
+                bucketCountThresholds.getShardSize(), bucketCountThresholds.getMinDocCount(), pipelineAggregators, metaData);
+        return new NonCollectingAggregator(name, aggregationContext, parent, factories, pipelineAggregators, metaData) {
+            {
+                // even in the case of an unmapped aggregator, validate the order
+                InternalOrder.validate(order, this);
+            }
+
+            @Override
+            public InternalAggregation buildEmptyAggregation() {
+                return aggregation;
+            }
+        };
+    }
+
+    @Override
+    protected Aggregator doCreateInternal(ValuesSource valuesSource, AggregationContext aggregationContext, Aggregator parent,
+            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+            throws IOException {
+        if (collectsFromSingleBucket == false) {
+            return asMultiBucketAggregator(this, aggregationContext, parent);
+        }
+        if (valuesSource instanceof ValuesSource.Bytes) {
+            ExecutionMode execution = null;
+            if (executionHint != null) {
+                execution = ExecutionMode.fromString(executionHint, aggregationContext.searchContext().parseFieldMatcher());
+            }
+
+            // In some cases, using ordinals is just not supported: override it
+            if (!(valuesSource instanceof ValuesSource.Bytes.WithOrdinals)) {
+                execution = ExecutionMode.MAP;
+            }
+
+            final long maxOrd;
+            final double ratio;
+            if (execution == null || execution.needsGlobalOrdinals()) {
+                ValuesSource.Bytes.WithOrdinals valueSourceWithOrdinals = (ValuesSource.Bytes.WithOrdinals) valuesSource;
+                IndexSearcher indexSearcher = aggregationContext.searchContext().searcher();
+                maxOrd = valueSourceWithOrdinals.globalMaxOrd(indexSearcher);
+                ratio = maxOrd / ((double) indexSearcher.getIndexReader().numDocs());
+            } else {
+                maxOrd = -1;
+                ratio = -1;
+            }
+
+            // Let's try to use a good default
+            if (execution == null) {
+                // if there is a parent bucket aggregator the number of instances of this aggregator is going
+                // to be unbounded and most instances may only aggregate few documents, so use hashed based
+                // global ordinals to keep the bucket ords dense.
+                if (Aggregator.descendsFromBucketAggregator(parent)) {
+                    execution = ExecutionMode.GLOBAL_ORDINALS_HASH;
+                } else {
+                    if (factories == AggregatorFactories.EMPTY) {
+                        if (ratio <= 0.5 && maxOrd <= 2048) {
+                            // 0.5: At least we need reduce the number of global ordinals look-ups by half
+                            // 2048: GLOBAL_ORDINALS_LOW_CARDINALITY has additional memory usage, which directly linked to maxOrd, so we need to limit.
+                            execution = ExecutionMode.GLOBAL_ORDINALS_LOW_CARDINALITY;
+                        } else {
+                            execution = ExecutionMode.GLOBAL_ORDINALS;
+                        }
+                    } else {
+                        execution = ExecutionMode.GLOBAL_ORDINALS;
+                    }
+                }
+            }
+
+            return execution.create(name, factories, valuesSource, order, bucketCountThresholds, includeExclude, aggregationContext,
+                    parent, collectMode, showTermDocCountError, pipelineAggregators, metaData);
+        }
+
+        if ((includeExclude != null) && (includeExclude.isRegexBased())) {
+            throw new AggregationExecutionException(
+                    "Aggregation ["
+                            + name
+                            + "] cannot support regular expression style include/exclude "
+                            + "settings as they can only be applied to string fields. Use an array of numeric values for include/exclude clauses used to filter numeric fields");
+        }
+
+        if (valuesSource instanceof ValuesSource.Numeric) {
+            IncludeExclude.LongFilter longFilter = null;
+            if (((ValuesSource.Numeric) valuesSource).isFloatingPoint()) {
+                if (includeExclude != null) {
+                    longFilter = includeExclude.convertToDoubleFilter();
+                }
+                return new DoubleTermsAggregator(name, factories, (ValuesSource.Numeric) valuesSource, config.format(), order,
+                        bucketCountThresholds, aggregationContext, parent, collectMode, showTermDocCountError, longFilter,
+                        pipelineAggregators, metaData);
+            }
+            if (includeExclude != null) {
+                longFilter = includeExclude.convertToLongFilter();
+            }
+            return new LongTermsAggregator(name, factories, (ValuesSource.Numeric) valuesSource, config.format(), order,
+                    bucketCountThresholds, aggregationContext, parent, collectMode, showTermDocCountError, longFilter, pipelineAggregators,
+                    metaData);
+        }
+
+        throw new AggregationExecutionException("terms aggregation cannot be applied to field [" + config.fieldContext().field()
+                + "]. It can only be applied to numeric or string fields.");
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java
new file mode 100644
index 0000000..9bc1f7a
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java
@@ -0,0 +1,276 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.terms;
+
+import org.apache.lucene.util.automaton.RegExp;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+
+import java.io.IOException;
+import java.util.Locale;
+
+/**
+ * Builder for the {@link Terms} aggregation.
+ */
+public class TermsBuilder extends ValuesSourceAggregationBuilder<TermsBuilder> {
+
+    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new TermsAggregator.BucketCountThresholds(-1, -1, -1, -1);
+
+    private Terms.ValueType valueType;
+    private Terms.Order order;
+    private String includePattern;
+    private String excludePattern;
+    private String executionHint;
+    private SubAggCollectionMode collectionMode;
+    private Boolean showTermDocCountError;
+    private String[] includeTerms = null;
+    private String[] excludeTerms = null;
+
+    /**
+     * Sole constructor.
+     */
+    public TermsBuilder(String name) {
+        super(name, "terms");
+    }
+
+    /**
+     * Sets the size - indicating how many term buckets should be returned (defaults to 10)
+     */
+    public TermsBuilder size(int size) {
+        bucketCountThresholds.setRequiredSize(size);
+        return this;
+    }
+
+    /**
+     * Sets the shard_size - indicating the number of term buckets each shard will return to the coordinating node (the
+     * node that coordinates the search execution). The higher the shard size is, the more accurate the results are.
+     */
+    public TermsBuilder shardSize(int shardSize) {
+        bucketCountThresholds.setShardSize(shardSize);
+        return this;
+    }
+
+    /**
+     * Set the minimum document count terms should have in order to appear in the response.
+     */
+    public TermsBuilder minDocCount(long minDocCount) {
+        bucketCountThresholds.setMinDocCount(minDocCount);
+        return this;
+    }
+
+    /**
+     * Set the minimum document count terms should have on the shard in order to appear in the response.
+     */
+    public TermsBuilder shardMinDocCount(long shardMinDocCount) {
+        bucketCountThresholds.setShardMinDocCount(shardMinDocCount);
+        return this;
+    }
+
+    /**
+     * Define a regular expression that will determine what terms should be aggregated. The regular expression is based
+     * on the {@link RegExp} class.
+     *
+     * @see RegExp#RegExp(String)
+     */
+    public TermsBuilder include(String regex) {
+        if (includeTerms != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of strings or a regex, not both");
+        }
+        this.includePattern = regex;
+        return this;
+    }
+    
+    /**
+     * Define a set of terms that should be aggregated.
+     */
+    public TermsBuilder include(String [] terms) {
+        if (includePattern != null) {
+            throw new IllegalArgumentException("include clause must be an array of exact values or a regex, not both");
+        }
+        this.includeTerms = terms;
+        return this;
+    }    
+    
+    /**
+     * Define a set of terms that should be aggregated.
+     */
+    public TermsBuilder include(long [] terms) {
+        if (includePattern != null) {
+            throw new IllegalArgumentException("include clause must be an array of exact values or a regex, not both");
+        }
+        this.includeTerms = longsArrToStringArr(terms);
+        return this;
+    }     
+    
+    private String[] longsArrToStringArr(long[] terms) {
+        String[] termsAsString = new String[terms.length];
+        for (int i = 0; i < terms.length; i++) {
+            termsAsString[i] = Long.toString(terms[i]);
+        }
+        return termsAsString;
+    }      
+    
+
+    /**
+     * Define a set of terms that should be aggregated.
+     */
+    public TermsBuilder include(double [] terms) {
+        if (includePattern != null) {
+            throw new IllegalArgumentException("include clause must be an array of exact values or a regex, not both");
+        }
+        this.includeTerms = doubleArrToStringArr(terms);
+        return this;
+    }
+
+    private String[] doubleArrToStringArr(double[] terms) {
+        String[] termsAsString = new String[terms.length];
+        for (int i = 0; i < terms.length; i++) {
+            termsAsString[i] = Double.toString(terms[i]);
+        }
+        return termsAsString;
+    }    
+
+    /**
+     * Define a regular expression that will filter out terms that should be excluded from the aggregation. The regular
+     * expression is based on the {@link RegExp} class.
+     *
+     * @see RegExp#RegExp(String)
+     */
+    public TermsBuilder exclude(String regex) {
+        if (excludeTerms != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of exact values or a regex, not both");
+        }
+        this.excludePattern = regex;
+        return this;
+    }
+    
+    /**
+     * Define a set of terms that should not be aggregated.
+     */
+    public TermsBuilder exclude(String [] terms) {
+        if (excludePattern != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of exact values or a regex, not both");
+        }
+        this.excludeTerms = terms;
+        return this;
+    }    
+    
+    
+    /**
+     * Define a set of terms that should not be aggregated.
+     */
+    public TermsBuilder exclude(long [] terms) {
+        if (excludePattern != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of exact values or a regex, not both");
+        }
+        this.excludeTerms = longsArrToStringArr(terms);
+        return this;
+    }
+
+    /**
+     * Define a set of terms that should not be aggregated.
+     */
+    public TermsBuilder exclude(double [] terms) {
+        if (excludePattern != null) {
+            throw new IllegalArgumentException("exclude clause must be an array of exact values or a regex, not both");
+        }
+        this.excludeTerms = doubleArrToStringArr(terms);
+        return this;
+    }    
+    
+    
+
+    /**
+     * When using scripts, the value type indicates the types of the values the script is generating.
+     */
+    public TermsBuilder valueType(Terms.ValueType valueType) {
+        this.valueType = valueType;
+        return this;
+    }
+
+    /**
+     * Defines the order in which the buckets will be returned.
+     */
+    public TermsBuilder order(Terms.Order order) {
+        this.order = order;
+        return this;
+    }
+
+    /**
+     * Expert: provide an execution hint to the aggregation.
+     */
+    public TermsBuilder executionHint(String executionHint) {
+        this.executionHint = executionHint;
+        return this;
+    }
+
+    /**
+     * Expert: set the collection mode.
+     */
+    public TermsBuilder collectMode(SubAggCollectionMode mode) {
+        this.collectionMode = mode;
+        return this;
+    }
+
+    /**
+     * Expert: return document count errors per term in the response.
+     */
+    public TermsBuilder showTermDocCountError(boolean showTermDocCountError) {
+        this.showTermDocCountError = showTermDocCountError;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+
+        bucketCountThresholds.toXContent(builder);
+
+        if (showTermDocCountError != null) {
+            builder.field(AbstractTermsParametersParser.SHOW_TERM_DOC_COUNT_ERROR.getPreferredName(), showTermDocCountError);
+        }
+        if (executionHint != null) {
+            builder.field(AbstractTermsParametersParser.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
+        }
+        if (valueType != null) {
+            builder.field("value_type", valueType.name().toLowerCase(Locale.ROOT));
+        }
+        if (order != null) {
+            builder.field("order");
+            order.toXContent(builder, params);
+        }
+        if (collectionMode != null) {
+            builder.field(SubAggCollectionMode.KEY.getPreferredName(), collectionMode.parseField().getPreferredName());
+        }
+        if (includeTerms != null) {
+            builder.array("include", includeTerms);
+        }
+        if (includePattern != null) {
+            builder.field("include", includePattern);
+        }
+        if (excludeTerms != null) {
+            builder.array("exclude", excludeTerms);
+        }
+        if (excludePattern != null) {
+            builder.field("exclude", excludePattern);
+        }
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParametersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParametersParser.java
new file mode 100644
index 0000000..c8138b7
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParametersParser.java
@@ -0,0 +1,144 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.search.aggregations.bucket.terms;
+
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+
+
+public class TermsParametersParser extends AbstractTermsParametersParser {
+
+    private static final TermsAggregator.BucketCountThresholds DEFAULT_BUCKET_COUNT_THRESHOLDS = new TermsAggregator.BucketCountThresholds(1, 0, 10, -1);
+
+    public List<OrderElement> getOrderElements() {
+        return orderElements;
+    }
+    
+    public boolean showTermDocCountError() {
+        return showTermDocCountError;
+    }
+
+    List<OrderElement> orderElements;
+    private boolean showTermDocCountError = false;
+
+    public TermsParametersParser() {
+        orderElements = new ArrayList<>(1);
+        orderElements.add(new OrderElement("_count", false));
+    }
+
+    @Override
+    public void parseSpecial(String aggregationName, XContentParser parser, SearchContext context, XContentParser.Token token, String currentFieldName) throws IOException {
+        if (token == XContentParser.Token.START_OBJECT) {
+            if ("order".equals(currentFieldName)) {
+                this.orderElements = Collections.singletonList(parseOrderParam(aggregationName, parser, context));
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
+            }
+        } else if (token == XContentParser.Token.START_ARRAY) {
+            if ("order".equals(currentFieldName)) {
+                orderElements = new ArrayList<>();
+                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                    if (token == XContentParser.Token.START_OBJECT) {
+                        OrderElement orderParam = parseOrderParam(aggregationName, parser, context);
+                        orderElements.add(orderParam);
+                    } else {
+                        throw new SearchParseException(context, "Order elements must be of type object in [" + aggregationName + "].",
+                                parser.getTokenLocation());
+                    }
+                }
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
+            }
+        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+            if (context.parseFieldMatcher().match(currentFieldName, SHOW_TERM_DOC_COUNT_ERROR)) {
+                showTermDocCountError = parser.booleanValue();
+            }
+        } else {
+            throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName
+                    + "].", parser.getTokenLocation());
+        }
+    }
+
+    private OrderElement parseOrderParam(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        XContentParser.Token token;
+        OrderElement orderParam = null;
+        String orderKey = null;
+        boolean orderAsc = false;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                orderKey = parser.currentName();
+            } else if (token == XContentParser.Token.VALUE_STRING) {
+                String dir = parser.text();
+                if ("asc".equalsIgnoreCase(dir)) {
+                    orderAsc = true;
+                } else if ("desc".equalsIgnoreCase(dir)) {
+                    orderAsc = false;
+                } else {
+                    throw new SearchParseException(context, "Unknown terms order direction [" + dir + "] in terms aggregation ["
+                            + aggregationName + "]", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " for [order] in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+        }
+        if (orderKey == null) {
+            throw new SearchParseException(context, "Must specify at least one field for [order] in [" + aggregationName + "].",
+                    parser.getTokenLocation());
+        } else {
+            orderParam = new OrderElement(orderKey, orderAsc);
+        }
+        return orderParam;
+    }
+
+    static class OrderElement {
+        private final String key;
+        private final boolean asc;
+
+        public OrderElement(String key, boolean asc) {
+            this.key = key;
+            this.asc = asc;
+        }
+
+        public String key() {
+            return key;
+        }
+
+        public boolean asc() {
+            return asc;
+        }
+
+        
+    }
+
+    @Override
+    public TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds() {
+        return new TermsAggregator.BucketCountThresholds(DEFAULT_BUCKET_COUNT_THRESHOLDS);
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java
index 1a5f854..478309d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java
@@ -18,30 +18,24 @@
  */
 package org.elasticsearch.search.aggregations.bucket.terms;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.BucketUtils;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsParametersParser.OrderElement;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
-import java.util.Map;
 
 /**
  *
  */
-public class TermsParser extends AbstractTermsParser {
-
+public class TermsParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -49,123 +43,37 @@ public class TermsParser extends AbstractTermsParser {
     }
 
     @Override
-    protected TermsAggregatorBuilder doCreateFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, BucketCountThresholds bucketCountThresholds, SubAggCollectionMode collectMode, String executionHint,
-            IncludeExclude incExc, Map<ParseField, Object> otherOptions) {
-        TermsAggregatorBuilder factory = new TermsAggregatorBuilder(aggregationName, targetValueType);
-        List<OrderElement> orderElements = (List<OrderElement>) otherOptions.get(TermsAggregatorBuilder.ORDER_FIELD);
-        if (orderElements != null) {
-            List<Terms.Order> orders = new ArrayList<>(orderElements.size());
-            for (OrderElement orderElement : orderElements) {
-                orders.add(resolveOrder(orderElement.key(), orderElement.asc()));
-            }
-            factory.order(orders);
-        }
-        if (bucketCountThresholds != null) {
-            factory.bucketCountThresholds(bucketCountThresholds);
-        }
-        if (collectMode != null) {
-            factory.collectMode(collectMode);
-        }
-        if (executionHint != null) {
-            factory.executionHint(executionHint);
-        }
-        if (incExc != null) {
-            factory.includeExclude(incExc);
-        }
-        Boolean showTermDocCountError = (Boolean) otherOptions.get(TermsAggregatorBuilder.SHOW_TERM_DOC_COUNT_ERROR);
-        if (showTermDocCountError != null) {
-            factory.showTermDocCountError(showTermDocCountError);
-        }
-        return factory;
-    }
-
-    @Override
-    public boolean parseSpecial(String aggregationName, XContentParser parser, ParseFieldMatcher parseFieldMatcher, Token token,
-            String currentFieldName, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.START_OBJECT) {
-            if (parseFieldMatcher.match(currentFieldName, TermsAggregatorBuilder.ORDER_FIELD)) {
-                otherOptions.put(TermsAggregatorBuilder.ORDER_FIELD, Collections.singletonList(parseOrderParam(aggregationName, parser)));
-                return true;
-            }
-        } else if (token == XContentParser.Token.START_ARRAY) {
-            if (parseFieldMatcher.match(currentFieldName, TermsAggregatorBuilder.ORDER_FIELD)) {
-                List<OrderElement> orderElements = new ArrayList<>();
-                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                    if (token == XContentParser.Token.START_OBJECT) {
-                        OrderElement orderParam = parseOrderParam(aggregationName, parser);
-                        orderElements.add(orderParam);
-                    } else {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Order elements must be of type object in [" + aggregationName + "] found token of type [" + token + "].");
-                    }
-                }
-                otherOptions.put(TermsAggregatorBuilder.ORDER_FIELD, orderElements);
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, TermsAggregatorBuilder.SHOW_TERM_DOC_COUNT_ERROR)) {
-                otherOptions.put(TermsAggregatorBuilder.SHOW_TERM_DOC_COUNT_ERROR, parser.booleanValue());
-                return true;
-            }
-        }
-        return false;
-    }
-
-    private OrderElement parseOrderParam(String aggregationName, XContentParser parser) throws IOException {
-        XContentParser.Token token;
-        OrderElement orderParam = null;
-        String orderKey = null;
-        boolean orderAsc = false;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                orderKey = parser.currentName();
-            } else if (token == XContentParser.Token.VALUE_STRING) {
-                String dir = parser.text();
-                if ("asc".equalsIgnoreCase(dir)) {
-                    orderAsc = true;
-                } else if ("desc".equalsIgnoreCase(dir)) {
-                    orderAsc = false;
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown terms order direction [" + dir + "] in terms aggregation [" + aggregationName + "]");
-                }
-            } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " for [order] in [" + aggregationName + "].");
-            }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        TermsParametersParser aggParser = new TermsParametersParser();
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, StringTerms.TYPE, context).scriptable(true).formattable(true).build();
+        IncludeExclude.Parser incExcParser = new IncludeExclude.Parser();
+        aggParser.parse(aggregationName, parser, context, vsParser, incExcParser);
+
+        List<OrderElement> orderElements = aggParser.getOrderElements();
+        List<Terms.Order> orders = new ArrayList<>(orderElements.size());
+        for (OrderElement orderElement : orderElements) {
+            orders.add(resolveOrder(orderElement.key(), orderElement.asc()));
         }
-        if (orderKey == null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Must specify at least one field for [order] in [" + aggregationName + "].");
-        } else {
-            orderParam = new OrderElement(orderKey, orderAsc);
+        Terms.Order order;
+        if (orders.size() == 1 && (orders.get(0) == InternalOrder.TERM_ASC || orders.get(0) == InternalOrder.TERM_DESC))
+        {
+            // If order is only terms order then we don't need compound ordering
+            order = orders.get(0);
         }
-        return orderParam;
-    }
-
-    static class OrderElement {
-        private final String key;
-        private final boolean asc;
-
-        public OrderElement(String key, boolean asc) {
-            this.key = key;
-            this.asc = asc;
+        else
+        {
+            // for all other cases we need compound order so term order asc can be added to make the order deterministic
+            order = Order.compound(orders);
         }
-
-        public String key() {
-            return key;
+        TermsAggregator.BucketCountThresholds bucketCountThresholds = aggParser.getBucketCountThresholds();
+        if (!(order == InternalOrder.TERM_ASC || order == InternalOrder.TERM_DESC)
+                && bucketCountThresholds.getShardSize() == aggParser.getDefaultBucketCountThresholds().getShardSize()) {
+            // The user has not made a shardSize selection. Use default heuristic to avoid any wrong-ranking caused by distributed counting
+            bucketCountThresholds.setShardSize(BucketUtils.suggestShardSideQueueSize(bucketCountThresholds.getRequiredSize(),
+                    context.numberOfShards()));
         }
-
-        public boolean asc() {
-            return asc;
-        }
-
-    }
-
-    @Override
-    public TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds() {
-        return new TermsAggregator.BucketCountThresholds(TermsAggregatorBuilder.DEFAULT_BUCKET_COUNT_THRESHOLDS);
+        bucketCountThresholds.ensureValidity();
+        return new TermsAggregatorFactory(aggregationName, vsParser.config(), order, bucketCountThresholds, aggParser.getIncludeExclude(), aggParser.getExecutionHint(), aggParser.getCollectionMode(), aggParser.showTermDocCountError());
     }
 
     static Terms.Order resolveOrder(String key, boolean asc) {
@@ -178,9 +86,4 @@ public class TermsParser extends AbstractTermsParser {
         return Order.aggregation(key, asc);
     }
 
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new TermsAggregatorBuilder(null, null);
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java
index eee9d4c..9c33a98 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java
@@ -20,7 +20,6 @@ package org.elasticsearch.search.aggregations.bucket.terms.support;
 
 import com.carrotsearch.hppc.LongHashSet;
 import com.carrotsearch.hppc.LongSet;
-
 import org.apache.lucene.index.RandomAccessOrds;
 import org.apache.lucene.index.SortedSetDocValues;
 import org.apache.lucene.index.Terms;
@@ -35,22 +34,12 @@ import org.apache.lucene.util.automaton.CompiledAutomaton;
 import org.apache.lucene.util.automaton.Operations;
 import org.apache.lucene.util.automaton.RegExp;
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Bytes.WithOrdinals;
 
 import java.io.IOException;
-import java.util.Collections;
 import java.util.HashSet;
-import java.util.Map;
-import java.util.Objects;
 import java.util.Set;
 import java.util.SortedSet;
 import java.util.TreeSet;
@@ -59,16 +48,7 @@ import java.util.TreeSet;
  * Defines the include/exclude regular expression filtering for string terms aggregation. In this filtering logic,
  * exclusion has precedence, where the {@code include} is evaluated first and then the {@code exclude}.
  */
-public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
-
-    private static final IncludeExclude PROTOTYPE = new IncludeExclude(Collections.emptySortedSet(), Collections.emptySortedSet());
-    private static final ParseField INCLUDE_FIELD = new ParseField("include");
-    private static final ParseField EXCLUDE_FIELD = new ParseField("exclude");
-    private static final ParseField PATTERN_FIELD = new ParseField("pattern");
-
-    public static IncludeExclude readFromStream(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
+public class IncludeExclude {
 
     // The includeValue and excludeValue ByteRefs which are the result of the parsing
     // process are converted into a LongFilter when used on numeric fields
@@ -227,10 +207,6 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
         this.excludeValues = null;
     }
 
-    public IncludeExclude(String include, String exclude) {
-        this(include == null ? null : new RegExp(include), exclude == null ? null : new RegExp(exclude));
-    }
-
     /**
      * @param includeValues   The terms to be included
      * @param excludeValues   The terms to be excluded
@@ -245,51 +221,6 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
         this.excludeValues = excludeValues;
     }
 
-    public IncludeExclude(String[] includeValues, String[] excludeValues) {
-        this(convertToBytesRefSet(includeValues), convertToBytesRefSet(excludeValues));
-    }
-
-    public IncludeExclude(double[] includeValues, double[] excludeValues) {
-        this(convertToBytesRefSet(includeValues), convertToBytesRefSet(excludeValues));
-    }
-
-    public IncludeExclude(long[] includeValues, long[] excludeValues) {
-        this(convertToBytesRefSet(includeValues), convertToBytesRefSet(excludeValues));
-    }
-
-    private static SortedSet<BytesRef> convertToBytesRefSet(String[] values) {
-        SortedSet<BytesRef> returnSet = null;
-        if (values != null) {
-            returnSet = new TreeSet<>();
-            for (String value : values) {
-                returnSet.add(new BytesRef(value));
-            }
-        }
-        return returnSet;
-    }
-
-    private static SortedSet<BytesRef> convertToBytesRefSet(double[] values) {
-        SortedSet<BytesRef> returnSet = null;
-        if (values != null) {
-            returnSet = new TreeSet<>();
-            for (double value : values) {
-                returnSet.add(new BytesRef(String.valueOf(value)));
-            }
-        }
-        return returnSet;
-    }
-
-    private static SortedSet<BytesRef> convertToBytesRefSet(long[] values) {
-        SortedSet<BytesRef> returnSet = null;
-        if (values != null) {
-            returnSet = new TreeSet<>();
-            for (long value : values) {
-                returnSet.add(new BytesRef(String.valueOf(value)));
-            }
-        }
-        return returnSet;
-    }
-
     /**
      * Terms adapter around doc values.
      */
@@ -352,14 +283,18 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
 
     public static class Parser {
 
-        public boolean token(String currentFieldName, XContentParser.Token token, XContentParser parser,
-                ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
+        String include = null;
+        String exclude = null;
+        SortedSet<BytesRef> includeValues;
+        SortedSet<BytesRef> excludeValues;
+
+        public boolean token(String currentFieldName, XContentParser.Token token, XContentParser parser) throws IOException {
 
             if (token == XContentParser.Token.VALUE_STRING) {
-                if (parseFieldMatcher.match(currentFieldName, INCLUDE_FIELD)) {
-                    otherOptions.put(INCLUDE_FIELD, parser.text());
-                } else if (parseFieldMatcher.match(currentFieldName, EXCLUDE_FIELD)) {
-                    otherOptions.put(EXCLUDE_FIELD, parser.text());
+                if ("include".equals(currentFieldName)) {
+                    include = parser.text();
+                } else if ("exclude".equals(currentFieldName)) {
+                    exclude = parser.text();
                 } else {
                     return false;
                 }
@@ -367,35 +302,35 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
             }
 
             if (token == XContentParser.Token.START_ARRAY) {
-                if (parseFieldMatcher.match(currentFieldName, INCLUDE_FIELD)) {
-                    otherOptions.put(INCLUDE_FIELD, new TreeSet<>(parseArrayToSet(parser)));
+                if ("include".equals(currentFieldName)) {
+                     includeValues = new TreeSet<>(parseArrayToSet(parser));
                      return true;
                 }
-                if (parseFieldMatcher.match(currentFieldName, EXCLUDE_FIELD)) {
-                    otherOptions.put(EXCLUDE_FIELD, new TreeSet<>(parseArrayToSet(parser)));
+                if ("exclude".equals(currentFieldName)) {
+                      excludeValues = new TreeSet<>(parseArrayToSet(parser));
                       return true;
                 }
                 return false;
             }
 
             if (token == XContentParser.Token.START_OBJECT) {
-                if (parseFieldMatcher.match(currentFieldName, INCLUDE_FIELD)) {
+                if ("include".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                         if (token == XContentParser.Token.FIELD_NAME) {
                             currentFieldName = parser.currentName();
                         } else if (token == XContentParser.Token.VALUE_STRING) {
-                            if (parseFieldMatcher.match(currentFieldName, PATTERN_FIELD)) {
-                                otherOptions.put(INCLUDE_FIELD, parser.text());
+                            if ("pattern".equals(currentFieldName)) {
+                                include = parser.text();
                             }
                         }
                     }
-                } else if (parseFieldMatcher.match(currentFieldName, EXCLUDE_FIELD)) {
+                } else if ("exclude".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                         if (token == XContentParser.Token.FIELD_NAME) {
                             currentFieldName = parser.currentName();
                         } else if (token == XContentParser.Token.VALUE_STRING) {
-                            if (parseFieldMatcher.match(currentFieldName, PATTERN_FIELD)) {
-                                otherOptions.put(EXCLUDE_FIELD, parser.text());
+                            if ("pattern".equals(currentFieldName)) {
+                                exclude = parser.text();
                             }
                         }
                     }
@@ -407,7 +342,6 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
 
             return false;
         }
-
         private Set<BytesRef> parseArrayToSet(XContentParser parser) throws IOException {
             final Set<BytesRef> set = new HashSet<>();
             if (parser.currentToken() != XContentParser.Token.START_ARRAY) {
@@ -422,27 +356,7 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
             return set;
         }
 
-        public IncludeExclude createIncludeExclude(Map<ParseField, Object> otherOptions) {
-            Object includeObject = otherOptions.get(INCLUDE_FIELD);
-            String include = null;
-            SortedSet<BytesRef> includeValues = null;
-            if (includeObject != null) {
-                if (includeObject instanceof String) {
-                    include = (String) includeObject;
-                } else if (includeObject instanceof SortedSet) {
-                    includeValues = (SortedSet<BytesRef>) includeObject;
-                }
-            }
-            Object excludeObject = otherOptions.get(EXCLUDE_FIELD);
-            String exclude = null;
-            SortedSet<BytesRef> excludeValues = null;
-            if (excludeObject != null) {
-                if (excludeObject instanceof String) {
-                    exclude = (String) excludeObject;
-                } else if (excludeObject instanceof SortedSet) {
-                    excludeValues = (SortedSet<BytesRef>) excludeObject;
-                }
-            }
+        public IncludeExclude includeExclude() {
             RegExp includePattern =  include != null ? new RegExp(include) : null;
             RegExp excludePattern = exclude != null ? new RegExp(exclude) : null;
             if (includePattern != null || excludePattern != null) {
@@ -530,111 +444,4 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
         return result;
     }
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        if (include != null) {
-            builder.field(INCLUDE_FIELD.getPreferredName(), include.getOriginalString());
-        }
-        if (includeValues != null) {
-            builder.startArray(INCLUDE_FIELD.getPreferredName());
-            for (BytesRef value : includeValues) {
-                builder.value(value.utf8ToString());
-            }
-            builder.endArray();
-        }
-        if (exclude != null) {
-            builder.field(EXCLUDE_FIELD.getPreferredName(), exclude.getOriginalString());
-        }
-        if (excludeValues != null) {
-            builder.startArray(EXCLUDE_FIELD.getPreferredName());
-            for (BytesRef value : excludeValues) {
-                builder.value(value.utf8ToString());
-            }
-            builder.endArray();
-        }
-        return builder;
-    }
-
-    @Override
-    public IncludeExclude readFrom(StreamInput in) throws IOException {
-        if (in.readBoolean()) {
-            String includeString = in.readOptionalString();
-            RegExp include = null;
-            if (includeString != null) {
-                include = new RegExp(includeString);
-            }
-            String excludeString = in.readOptionalString();
-            RegExp exclude = null;
-            if (excludeString != null) {
-                exclude = new RegExp(excludeString);
-            }
-            return new IncludeExclude(include, exclude);
-        } else {
-            SortedSet<BytesRef> includes = null;
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                includes = new TreeSet<>();
-                for (int i = 0; i < size; i++) {
-                    includes.add(in.readBytesRef());
-                }
-            }
-            SortedSet<BytesRef> excludes = null;
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                excludes = new TreeSet<>();
-                for (int i = 0; i < size; i++) {
-                    excludes.add(in.readBytesRef());
-                }
-            }
-            return new IncludeExclude(includes, excludes);
-        }
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        boolean regexBased = isRegexBased();
-        out.writeBoolean(regexBased);
-        if (regexBased) {
-            out.writeOptionalString(include == null ? null : include.getOriginalString());
-            out.writeOptionalString(exclude == null ? null : exclude.getOriginalString());
-        } else {
-            boolean hasIncludes = includeValues != null;
-            out.writeBoolean(hasIncludes);
-            if (hasIncludes) {
-                out.writeVInt(includeValues.size());
-                for (BytesRef value : includeValues) {
-                    out.writeBytesRef(value);
-                }
-            }
-            boolean hasExcludes = excludeValues != null;
-            out.writeBoolean(hasExcludes);
-            if (hasExcludes) {
-                out.writeVInt(excludeValues.size());
-                for (BytesRef value : excludeValues) {
-                    out.writeBytesRef(value);
-                }
-            }
-        }
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(include == null ? null : include.getOriginalString(), exclude == null ? null : exclude.getOriginalString(),
-                includeValues, excludeValues);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        } if (getClass() != obj.getClass()) {
-            return false;
-        }
-        IncludeExclude other = (IncludeExclude) obj;
-        return Objects.equals(include == null ? null : include.getOriginalString(), other.include == null ? null : other.include.getOriginalString())
-                && Objects.equals(exclude == null ? null : exclude.getOriginalString(), other.exclude == null ? null : other.exclude.getOriginalString())
-                && Objects.equals(includeValues, other.includeValues)
-                && Objects.equals(excludeValues, other.excludeValues);
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/MetricsAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/MetricsAggregationBuilder.java
new file mode 100644
index 0000000..56ae24b
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/MetricsAggregationBuilder.java
@@ -0,0 +1,44 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Base builder for metrics aggregations.
+ */
+public abstract class MetricsAggregationBuilder<B extends MetricsAggregationBuilder<B>> extends AbstractAggregationBuilder {
+
+    public MetricsAggregationBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    @Override
+    public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(getName()).startObject(type);
+        internalXContent(builder, params);
+        return builder.endObject().endObject();
+    }
+
+    protected abstract void internalXContent(XContentBuilder builder, Params params) throws IOException;
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/NumericValuesSourceMetricsAggregatorParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/NumericValuesSourceMetricsAggregatorParser.java
new file mode 100644
index 0000000..6847a9a
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/NumericValuesSourceMetricsAggregatorParser.java
@@ -0,0 +1,70 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.metrics;
+
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+
+/**
+ *
+ */
+public abstract class NumericValuesSourceMetricsAggregatorParser<S extends InternalNumericMetricsAggregation> implements Aggregator.Parser {
+
+    protected final InternalAggregation.Type aggType;
+
+    protected NumericValuesSourceMetricsAggregatorParser(InternalAggregation.Type aggType) {
+        this.aggType = aggType;
+    }
+
+    @Override
+    public String type() {
+        return aggType.name();
+    }
+
+    @Override
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, aggType, context).formattable(true)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (!vsParser.token(currentFieldName, token, parser)) {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+        }
+
+        return createFactory(aggregationName, vsParser.config());
+    }
+
+    protected abstract AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config);
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/ValuesSourceMetricsAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/ValuesSourceMetricsAggregationBuilder.java
new file mode 100644
index 0000000..e675548
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/ValuesSourceMetricsAggregationBuilder.java
@@ -0,0 +1,88 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+
+import java.io.IOException;
+
+/**
+ *
+ */
+public abstract class ValuesSourceMetricsAggregationBuilder<B extends ValuesSourceMetricsAggregationBuilder<B>> extends MetricsAggregationBuilder<B> {
+
+    private String field;
+    private Script script;
+    private String format;
+    private Object missing;
+
+    protected ValuesSourceMetricsAggregationBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    @SuppressWarnings("unchecked")
+    public B field(String field) {
+        this.field = field;
+        return (B) this;
+    }
+
+    /**
+     * The script to use for this aggregation
+     */
+    @SuppressWarnings("unchecked")
+    public B script(Script script) {
+        this.script = script;
+        return (B) this;
+    }
+
+    @SuppressWarnings("unchecked")
+    public B format(String format) {
+        this.format = format;
+        return (B) this;
+    }
+
+    /**
+     * Configure the value to use when documents miss a value.
+     */
+    public B missing(Object missingValue) {
+        this.missing = missingValue;
+        return (B) this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (field != null) {
+            builder.field("field", field);
+        }
+
+        if (script != null) {
+            builder.field("script", script);
+        }
+
+        if (format != null) {
+            builder.field("format", format);
+        }
+
+        if (missing != null) {
+            builder.field("missing", missing);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java
index 0f6657e..17506b7 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java
@@ -19,29 +19,21 @@
 package org.elasticsearch.search.aggregations.metrics.avg;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -121,42 +113,24 @@ public class AvgAggregator extends NumericMetricsAggregator.SingleValue {
         return new InternalAvg(name, 0.0, 0L, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class AvgAggregatorBuilder extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, AvgAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public AvgAggregatorBuilder(String name) {
-            super(name, InternalAvg.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, String type, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, type, valuesSourceConfig);
         }
 
         @Override
-        protected AvgAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new AvgAggregatorFactory(name, type, config, context, parent, subFactoriesBuilder, metaData);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators,
+                Map<String, Object> metaData) throws IOException {
+            return new AvgAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected AvgAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new AvgAggregator.AvgAggregatorBuilder(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new AvgAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregatorFactory.java
deleted file mode 100644
index dec0ee1..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregatorFactory.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.avg;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class AvgAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, AvgAggregatorFactory> {
-
-    public AvgAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new AvgAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new AvgAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgBuilder.java
new file mode 100644
index 0000000..143f39b
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.avg;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Avg} aggregation.
+ */
+public class AvgBuilder extends ValuesSourceMetricsAggregationBuilder<AvgBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public AvgBuilder(String name) {
+        super(name, InternalAvg.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java
index 3897c7f..1c2b2be 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java
@@ -18,46 +18,23 @@
  */
 package org.elasticsearch.search.aggregations.metrics.avg;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class AvgParser extends NumericValuesSourceParser {
+public class AvgParser extends NumericValuesSourceMetricsAggregatorParser<InternalAvg> {
 
     public AvgParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalAvg.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected AvgAggregator.AvgAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new AvgAggregator.AvgAggregatorBuilder(aggregationName);
+        super(InternalAvg.TYPE);
     }
 
     @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new AvgAggregator.AvgAggregatorBuilder(null);
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new AvgAggregator.Factory(aggregationName, type(), config);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorBuilder.java
deleted file mode 100644
index 4f033c3..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorBuilder.java
+++ /dev/null
@@ -1,117 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.cardinality;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import java.io.IOException;
-import java.util.Objects;
-
-public final class CardinalityAggregatorBuilder extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource, CardinalityAggregatorBuilder> {
-
-    public static final ParseField PRECISION_THRESHOLD_FIELD = new ParseField("precision_threshold");
-
-    private Long precisionThreshold = null;
-
-    public CardinalityAggregatorBuilder(String name, ValueType targetValueType) {
-        super(name, InternalCardinality.TYPE, ValuesSourceType.ANY, targetValueType);
-    }
-
-    /**
-     * Set a precision threshold. Higher values improve accuracy but also
-     * increase memory usage.
-     */
-    public CardinalityAggregatorBuilder precisionThreshold(long precisionThreshold) {
-        this.precisionThreshold = precisionThreshold;
-        return this;
-    }
-
-    /**
-     * Get the precision threshold. Higher values improve accuracy but also
-     * increase memory usage. Will return <code>null</code> if the
-     * precisionThreshold has not been set yet.
-     */
-    public Long precisionThreshold() {
-        return precisionThreshold;
-    }
-
-    /**
-     * @deprecated no replacement - values will always be rehashed
-     */
-    @Deprecated
-    public void rehash(boolean rehash) {
-        // Deprecated all values are already rehashed so do nothing
-    }
-
-    @Override
-    protected CardinalityAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<ValuesSource> config,
-            AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-        return new CardinalityAggregatorFactory(name, type, config, precisionThreshold, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected CardinalityAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        CardinalityAggregatorBuilder factory = new CardinalityAggregatorBuilder(name, targetValueType);
-        if (in.readBoolean()) {
-            factory.precisionThreshold = in.readLong();
-        }
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        boolean hasPrecisionThreshold = precisionThreshold != null;
-        out.writeBoolean(hasPrecisionThreshold);
-        if (hasPrecisionThreshold) {
-            out.writeLong(precisionThreshold);
-        }
-    }
-
-    @Override
-    public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        if (precisionThreshold != null) {
-            builder.field(PRECISION_THRESHOLD_FIELD.getPreferredName(), precisionThreshold);
-        }
-        return builder;
-    }
-
-    @Override
-    protected int innerHashCode() {
-        return Objects.hash(precisionThreshold);
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        CardinalityAggregatorBuilder other = (CardinalityAggregatorBuilder) obj;
-        return Objects.equals(precisionThreshold, other.precisionThreshold);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
index 8ad3c95..1b2d5fc 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
@@ -20,9 +20,6 @@
 package org.elasticsearch.search.aggregations.metrics.cardinality;
 
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
@@ -34,34 +31,30 @@ import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
-public class CardinalityAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource, CardinalityAggregatorFactory> {
+final class CardinalityAggregatorFactory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource> {
 
-    private final Long precisionThreshold;
+    private final long precisionThreshold;
 
-    public CardinalityAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource> config, Long precisionThreshold,
-            AggregationContext context, AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder,
-            Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
+    CardinalityAggregatorFactory(String name, ValuesSourceConfig config, long precisionThreshold) {
+        super(name, InternalCardinality.TYPE.name(), config);
         this.precisionThreshold = precisionThreshold;
     }
 
-    @Override
-    protected Aggregator createUnmapped(AggregationContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
-            Map<String, Object> metaData) throws IOException {
-        return new CardinalityAggregator(name, null, precision(parent), config.formatter(), context, parent, pipelineAggregators,
-                metaData);
+    private int precision(Aggregator parent) {
+        return precisionThreshold < 0 ? defaultPrecision(parent) : HyperLogLogPlusPlus.precisionFromThreshold(precisionThreshold);
     }
 
     @Override
-    protected Aggregator doCreateInternal(ValuesSource valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new CardinalityAggregator(name, valuesSource, precision(parent), config.formatter(), aggregationContext, parent,
-                pipelineAggregators, metaData);
+    protected Aggregator createUnmapped(AggregationContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+            throws IOException {
+        return new CardinalityAggregator(name, null, precision(parent), config.formatter(), context, parent, pipelineAggregators, metaData);
     }
 
-    private int precision(Aggregator parent) {
-        return precisionThreshold == null ? defaultPrecision(parent) : HyperLogLogPlusPlus.precisionFromThreshold(precisionThreshold);
+    @Override
+    protected Aggregator doCreateInternal(ValuesSource valuesSource, AggregationContext context, Aggregator parent,
+            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+        return new CardinalityAggregator(name, valuesSource, precision(parent), config.formatter(), context, parent, pipelineAggregators,
+                metaData);
     }
 
     /*
@@ -82,4 +75,5 @@ public class CardinalityAggregatorFactory extends ValuesSourceAggregatorFactory<
 
         return Math.max(precision, HyperLogLogPlusPlus.MIN_PRECISION);
     }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityBuilder.java
new file mode 100644
index 0000000..7680aa0
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityBuilder.java
@@ -0,0 +1,71 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.cardinality;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Cardinality} aggregation.
+ */
+public class CardinalityBuilder extends ValuesSourceMetricsAggregationBuilder<CardinalityBuilder> {
+
+    private Long precisionThreshold;
+    private Boolean rehash;
+
+    /**
+     * Sole constructor.
+     */
+    public CardinalityBuilder(String name) {
+        super(name, InternalCardinality.TYPE.name());
+    }
+
+    /**
+     * Set a precision threshold. Higher values improve accuracy but also
+     * increase memory usage.
+     */
+    public CardinalityBuilder precisionThreshold(long precisionThreshold) {
+        this.precisionThreshold = precisionThreshold;
+        return this;
+    }
+
+    /**
+     * Expert: set to false in case values of this field can already be treated
+     * as 64-bits hash values.
+     */
+    public CardinalityBuilder rehash(boolean rehash) {
+        this.rehash = rehash;
+        return this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.internalXContent(builder, params);
+        if (precisionThreshold != null) {
+            builder.field("precision_threshold", precisionThreshold);
+        }
+        if (rehash != null) {
+            builder.field("rehash", rehash);
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java
index 157d37e..6833945 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java
@@ -20,59 +20,56 @@
 package org.elasticsearch.search.aggregations.metrics.cardinality;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 
-public class CardinalityParser extends AnyValuesSourceParser {
+public class CardinalityParser implements Aggregator.Parser {
 
+    private static final ParseField PRECISION_THRESHOLD = new ParseField("precision_threshold");
     private static final ParseField REHASH = new ParseField("rehash").withAllDeprecated("no replacement - values will always be rehashed");
 
-    public CardinalityParser() {
-        super(true, false);
-    }
-
     @Override
     public String type() {
         return InternalCardinality.TYPE.name();
     }
 
     @Override
-    protected CardinalityAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        CardinalityAggregatorBuilder factory = new CardinalityAggregatorBuilder(aggregationName, targetValueType);
-        Long precisionThreshold = (Long) otherOptions.get(CardinalityAggregatorBuilder.PRECISION_THRESHOLD_FIELD);
-        if (precisionThreshold != null) {
-            factory.precisionThreshold(precisionThreshold);
-        }
-        return factory;
-    }
+    public AggregatorFactory parse(String name, XContentParser parser, SearchContext context) throws IOException {
 
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token.isValue()) {
-            if (parseFieldMatcher.match(currentFieldName, CardinalityAggregatorBuilder.PRECISION_THRESHOLD_FIELD)) {
-                otherOptions.put(CardinalityAggregatorBuilder.PRECISION_THRESHOLD_FIELD, parser.longValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, REHASH)) {
-                // ignore
-                return true;
+        ValuesSourceParser<?> vsParser = ValuesSourceParser.any(name, InternalCardinality.TYPE, context).formattable(false).build();
+
+        long precisionThreshold = -1;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token.isValue()) {
+                if (context.parseFieldMatcher().match(currentFieldName, REHASH)) {
+                    // ignore
+                } else if (context.parseFieldMatcher().match(currentFieldName, PRECISION_THRESHOLD)) {
+                    precisionThreshold = parser.longValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + name + "]: [" + currentFieldName
+                            + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + name + "].", parser.getTokenLocation());
             }
         }
-        return false;
-    }
 
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new CardinalityAggregatorBuilder(null, null);
+        return new CardinalityAggregatorFactory(name, vsParser.config(), precisionThreshold);
+
     }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java
index be2c773..8c6159e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java
@@ -20,38 +20,28 @@
 package org.elasticsearch.search.aggregations.metrics.geobounds;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.MetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 public final class GeoBoundsAggregator extends MetricsAggregator {
 
-    static final ParseField WRAP_LONGITUDE_FIELD = new ParseField("wrap_longitude");
-
     private final ValuesSource.GeoPoint valuesSource;
     private final boolean wrapLongitude;
     DoubleArray tops;
@@ -170,69 +160,32 @@ public final class GeoBoundsAggregator extends MetricsAggregator {
         return new InternalGeoBounds(name, Double.NEGATIVE_INFINITY, Double.POSITIVE_INFINITY, Double.POSITIVE_INFINITY,
                 Double.NEGATIVE_INFINITY, Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY, wrapLongitude, pipelineAggregators(), metaData());
     }
-
+    
     @Override
     public void doClose() {
         Releasables.close(tops, bottoms, posLefts, posRights, negLefts, negRights);
     }
 
-    public static class GeoBoundsAggregatorBuilder extends ValuesSourceAggregatorBuilder<ValuesSource.GeoPoint, GeoBoundsAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint> {
 
-        private boolean wrapLongitude = true;
-
-        public GeoBoundsAggregatorBuilder(String name) {
-            super(name, InternalGeoBounds.TYPE, ValuesSourceType.GEOPOINT, ValueType.GEOPOINT);
-        }
+        private final boolean wrapLongitude;
 
-        /**
-         * Set whether to wrap longitudes. Defaults to true.
-         */
-        public GeoBoundsAggregatorBuilder wrapLongitude(boolean wrapLongitude) {
+        protected Factory(String name, ValuesSourceConfig<ValuesSource.GeoPoint> config, boolean wrapLongitude) {
+            super(name, InternalGeoBounds.TYPE.name(), config);
             this.wrapLongitude = wrapLongitude;
-            return this;
-        }
-
-        /**
-         * Get whether to wrap longitudes.
-         */
-        public boolean wrapLongitude() {
-            return wrapLongitude;
-        }
-
-        @Override
-        protected GeoBoundsAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<ValuesSource.GeoPoint> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new GeoBoundsAggregatorFactory(name, type, config, wrapLongitude, context, parent, subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected GeoBoundsAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            GeoBoundsAggregatorBuilder factory = new GeoBoundsAggregatorBuilder(name);
-            factory.wrapLongitude = in.readBoolean();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeBoolean(wrapLongitude);
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(WRAP_LONGITUDE_FIELD.getPreferredName(), wrapLongitude);
-            return builder;
         }
 
         @Override
-        protected int innerHashCode() {
-            return Objects.hash(wrapLongitude);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new GeoBoundsAggregator(name, aggregationContext, parent, null, wrapLongitude, pipelineAggregators, metaData);
         }
 
         @Override
-        protected boolean innerEquals(Object obj) {
-            GeoBoundsAggregatorBuilder other = (GeoBoundsAggregatorBuilder) obj;
-            return Objects.equals(wrapLongitude, other.wrapLongitude);
+        protected Aggregator doCreateInternal(ValuesSource.GeoPoint valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new GeoBoundsAggregator(name, aggregationContext, parent, valuesSource, wrapLongitude, pipelineAggregators, metaData);
         }
 
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregatorFactory.java
deleted file mode 100644
index e36b03a..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregatorFactory.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.geobounds;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class GeoBoundsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, GeoBoundsAggregatorFactory> {
-
-    private final boolean wrapLongitude;
-
-    public GeoBoundsAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource.GeoPoint> config, boolean wrapLongitude,
-            AggregationContext context, AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder,
-            Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.wrapLongitude = wrapLongitude;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new GeoBoundsAggregator(name, aggregationContext, parent, null, wrapLongitude, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.GeoPoint valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new GeoBoundsAggregator(name, aggregationContext, parent, valuesSource, wrapLongitude, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsBuilder.java
new file mode 100644
index 0000000..582ab8c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsBuilder.java
@@ -0,0 +1,57 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.geobounds;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link GeoBounds} aggregation.
+ */
+public class GeoBoundsBuilder extends ValuesSourceAggregationBuilder<GeoBoundsBuilder> {
+
+    private Boolean wrapLongitude;
+
+    /**
+     * Sole constructor.
+     */
+    public GeoBoundsBuilder(String name) {
+        super(name, InternalGeoBounds.TYPE.name());
+    }
+
+    /**
+     * Set whether to wrap longitudes. Defaults to true.
+     */
+    public GeoBoundsBuilder wrapLongitude(boolean wrapLongitude) {
+        this.wrapLongitude = wrapLongitude;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (wrapLongitude != null) {
+            builder.field("wrap_longitude", wrapLongitude);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsParser.java
index 5c3112f..de1fea2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsParser.java
@@ -19,23 +19,18 @@
 
 package org.elasticsearch.search.aggregations.metrics.geobounds;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.GeoPointValuesSourceParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource.GeoPoint;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
-public class GeoBoundsParser extends GeoPointValuesSourceParser {
-
-    public GeoBoundsParser() {
-        super(false, false);
-    }
+public class GeoBoundsParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -43,31 +38,33 @@ public class GeoBoundsParser extends GeoPointValuesSourceParser {
     }
 
     @Override
-    protected GeoBoundsAggregator.GeoBoundsAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        GeoBoundsAggregator.GeoBoundsAggregatorBuilder factory = new GeoBoundsAggregator.GeoBoundsAggregatorBuilder(aggregationName);
-        Boolean wrapLongitude = (Boolean) otherOptions.get(GeoBoundsAggregator.WRAP_LONGITUDE_FIELD);
-        if (wrapLongitude != null) {
-            factory.wrapLongitude(wrapLongitude);
-        }
-        return factory;
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, GeoBoundsAggregator.WRAP_LONGITUDE_FIELD)) {
-                otherOptions.put(GeoBoundsAggregator.WRAP_LONGITUDE_FIELD, parser.booleanValue());
-                return true;
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        ValuesSourceParser<GeoPoint> vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoBounds.TYPE, context)
+                .targetValueType(ValueType.GEOPOINT)
+                .formattable(true)
+                .build();
+        boolean wrapLongitude = true;
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+                
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("wrap_longitude".equals(currentFieldName) || "wrapLongitude".equals(currentFieldName)) {
+                    wrapLongitude = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
             }
         }
-        return false;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new GeoBoundsAggregator.GeoBoundsAggregatorBuilder(null);
+        return new GeoBoundsAggregator.Factory(aggregationName, vsParser.config(), wrapLongitude);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregator.java
index a4ef9f7..b5739f5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregator.java
@@ -22,27 +22,22 @@ package org.elasticsearch.search.aggregations.metrics.geocentroid;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.GeoUtils;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.MetricsAggregator;
+import org.elasticsearch.search.aggregations.metrics.geobounds.InternalGeoBounds;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
@@ -127,43 +122,22 @@ public final class GeoCentroidAggregator extends MetricsAggregator {
         Releasables.close(centroids, counts);
     }
 
-    public static class GeoCentroidAggregatorBuilder
-            extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.GeoPoint, GeoCentroidAggregatorBuilder> {
-
-        public GeoCentroidAggregatorBuilder(String name) {
-            super(name, InternalGeoCentroid.TYPE, ValuesSourceType.GEOPOINT, ValueType.GEOPOINT);
-        }
-
-        @Override
-        protected GeoCentroidAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<ValuesSource.GeoPoint> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new GeoCentroidAggregatorFactory(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected GeoCentroidAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            return new GeoCentroidAggregatorBuilder(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.GeoPoint> {
+        protected Factory(String name, ValuesSourceConfig<ValuesSource.GeoPoint> config) {
+            super(name, InternalGeoBounds.TYPE.name(), config);
         }
 
         @Override
-        protected int innerHashCode() {
-            return 0;
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                                            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new GeoCentroidAggregator(name, aggregationContext, parent, null, pipelineAggregators, metaData);
         }
 
         @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
+        protected Aggregator doCreateInternal(ValuesSource.GeoPoint valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new GeoCentroidAggregator(name, aggregationContext, parent, valuesSource, pipelineAggregators, metaData);
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregatorFactory.java
deleted file mode 100644
index 81dc514..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregatorFactory.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.geocentroid;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class GeoCentroidAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, GeoCentroidAggregatorFactory> {
-
-    public GeoCentroidAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource.GeoPoint> config,
-            AggregationContext context, AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder,
-            Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new GeoCentroidAggregator(name, aggregationContext, parent, null, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.GeoPoint valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new GeoCentroidAggregator(name, aggregationContext, parent, valuesSource, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidBuilder.java
new file mode 100644
index 0000000..9d6823c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidBuilder.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.geocentroid;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder class for {@link org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator}
+ */
+public class GeoCentroidBuilder extends ValuesSourceMetricsAggregationBuilder<GeoCentroidBuilder> {
+
+    public GeoCentroidBuilder(String name) {
+        super(name, InternalGeoCentroid.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidParser.java
index 4bf7ccf..49a7bc8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidParser.java
@@ -19,26 +19,21 @@
 
 package org.elasticsearch.search.aggregations.metrics.geocentroid;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.GeoPointValuesSourceParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  * Parser class for {@link org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator}
  */
-public class GeoCentroidParser extends GeoPointValuesSourceParser {
-
-    public GeoCentroidParser() {
-        super(true, false);
-    }
+public class GeoCentroidParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -46,19 +41,23 @@ public class GeoCentroidParser extends GeoPointValuesSourceParser {
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected GeoCentroidAggregator.GeoCentroidAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new GeoCentroidAggregator.GeoCentroidAggregatorBuilder(aggregationName);
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new GeoCentroidAggregator.GeoCentroidAggregatorBuilder(null);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        ValuesSourceParser<ValuesSource.GeoPoint> vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoCentroid.TYPE, context)
+                .targetValueType(ValueType.GEOPOINT)
+                .formattable(true)
+                .build();
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
+            }
+        }
+        return new GeoCentroidAggregator.Factory(aggregationName, vsParser.config());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java
index dd5e667..e70fc7f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java
@@ -19,30 +19,22 @@
 package org.elasticsearch.search.aggregations.metrics.max;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.NumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.MultiValueMode;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -122,44 +114,24 @@ public class MaxAggregator extends NumericMetricsAggregator.SingleValue {
         return new InternalMax(name, Double.NEGATIVE_INFINITY, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class MaxAggregatorBuilder extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, MaxAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public MaxAggregatorBuilder(String name) {
-            super(name, InternalMax.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalMax.TYPE.name(), valuesSourceConfig);
         }
 
         @Override
-        protected MaxAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new MaxAggregatorFactory(name, type, config, context, parent, subFactoriesBuilder, metaData);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new MaxAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected MaxAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new MaxAggregator.MaxAggregatorBuilder(name);
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new MaxAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
-
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregatorFactory.java
deleted file mode 100644
index 39785f2..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregatorFactory.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.max;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class MaxAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, MaxAggregatorFactory> {
-
-    public MaxAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new MaxAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new MaxAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxBuilder.java
new file mode 100644
index 0000000..b433573
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.max;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Max} aggregation.
+ */
+public class MaxBuilder extends ValuesSourceMetricsAggregationBuilder<MaxBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public MaxBuilder(String name) {
+        super(name, InternalMax.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java
index ee517b7..a5cdac5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java
@@ -18,46 +18,23 @@
  */
 package org.elasticsearch.search.aggregations.metrics.max;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class MaxParser extends NumericValuesSourceParser {
+public class MaxParser extends NumericValuesSourceMetricsAggregatorParser<InternalMax> {
 
     public MaxParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalMax.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected MaxAggregator.MaxAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new MaxAggregator.MaxAggregatorBuilder(aggregationName);
+        super(InternalMax.TYPE);
     }
 
     @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new MaxAggregator.MaxAggregatorBuilder(null);
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new MaxAggregator.Factory(aggregationName, config);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java
index 70b7050..3a9bd40 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java
@@ -19,30 +19,22 @@
 package org.elasticsearch.search.aggregations.metrics.min;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.NumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.MultiValueMode;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -121,42 +113,23 @@ public class MinAggregator extends NumericMetricsAggregator.SingleValue {
         return new InternalMin(name, Double.POSITIVE_INFINITY, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class MinAggregatorBuilder extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, MinAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public MinAggregatorBuilder(String name) {
-            super(name, InternalMin.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalMin.TYPE.name(), valuesSourceConfig);
         }
 
         @Override
-        protected MinAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new MinAggregatorFactory(name, type, config, context, parent, subFactoriesBuilder, metaData);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new MinAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected MinAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new MinAggregator.MinAggregatorBuilder(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new MinAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregatorFactory.java
deleted file mode 100644
index aade8de..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregatorFactory.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.min;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class MinAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, MinAggregatorFactory> {
-
-    public MinAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new MinAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new MinAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinBuilder.java
new file mode 100644
index 0000000..e5342de
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.min;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Min} aggregation.
+ */
+public class MinBuilder extends ValuesSourceMetricsAggregationBuilder<MinBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public MinBuilder(String name) {
+        super(name, InternalMin.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java
index e43b691..e7dde9c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java
@@ -18,46 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.metrics.min;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class MinParser extends NumericValuesSourceParser {
+public class MinParser extends NumericValuesSourceMetricsAggregatorParser<InternalMin> {
 
     public MinParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalMin.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected MinAggregator.MinAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new MinAggregator.MinAggregatorBuilder(aggregationName);
+        super(InternalMin.TYPE);
     }
 
     @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new MinAggregator.MinAggregatorBuilder(null);
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new MinAggregator.Factory(aggregationName, config);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesBuilder.java
new file mode 100644
index 0000000..c587d64
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesBuilder.java
@@ -0,0 +1,87 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.percentiles;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+import java.io.IOException;
+
+abstract class AbstractPercentilesBuilder<PB extends AbstractPercentilesBuilder<PB>> extends
+        ValuesSourceMetricsAggregationBuilder<PB> {
+
+    private Double compression;
+    private PercentilesMethod method;
+    private Integer numberOfSignificantValueDigits;
+
+    public AbstractPercentilesBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    /**
+     * Expert: Set the method to use to compute the percentiles.
+     */
+    public PB method(PercentilesMethod method) {
+        this.method = method;
+        return (PB) this;
+    }
+
+    /**
+     * Expert: set the compression. Higher values improve accuracy but also
+     * memory usage. Only relevant when using {@link PercentilesMethod#TDIGEST}.
+     */
+    public PB compression(double compression) {
+        this.compression = compression;
+        return (PB) this;
+    }
+
+    /**
+     * Expert: set the number of significant digits in the values. Only relevant
+     * when using {@link PercentilesMethod#HDR}.
+     */
+    public PB numberOfSignificantValueDigits(int numberOfSignificantValueDigits) {
+        this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
+        return (PB) this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.internalXContent(builder, params);
+
+        doInternalXContent(builder, params);
+
+        if (method != null) {
+            builder.startObject(method.getName());
+
+            if (compression != null) {
+                builder.field(AbstractPercentilesParser.COMPRESSION_FIELD.getPreferredName(), compression);
+            }
+
+            if (numberOfSignificantValueDigits != null) {
+                builder.field(AbstractPercentilesParser.NUMBER_SIGNIFICANT_DIGITS_FIELD.getPreferredName(), numberOfSignificantValueDigits);
+            }
+
+            builder.endObject();
+        }
+    }
+
+    protected abstract void doInternalXContent(XContentBuilder builder, Params params) throws IOException;
+
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java
index 0a22b03..0b30040 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java
@@ -21,19 +21,21 @@ package org.elasticsearch.search.aggregations.metrics.percentiles;
 
 import com.carrotsearch.hppc.DoubleArrayList;
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
+import java.util.Arrays;
 
-public abstract class AbstractPercentilesParser extends NumericValuesSourceParser {
+public abstract class AbstractPercentilesParser implements Aggregator.Parser {
 
     public static final ParseField KEYED_FIELD = new ParseField("keyed");
     public static final ParseField METHOD_FIELD = new ParseField("method");
@@ -43,95 +45,139 @@ public abstract class AbstractPercentilesParser extends NumericValuesSourceParse
     private boolean formattable;
 
     public AbstractPercentilesParser(boolean formattable) {
-        super(true, formattable, false);
+        this.formattable = formattable;
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.START_ARRAY) {
-            if (parseFieldMatcher.match(currentFieldName, keysField())) {
-                DoubleArrayList values = new DoubleArrayList(10);
-                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                    double value = parser.doubleValue();
-                    values.add(value);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalTDigestPercentiles.TYPE, context)
+                .formattable(formattable).build();
+
+        double[] keys = null;
+        boolean keyed = true;
+        Double compression = null;
+        Integer numberOfSignificantValueDigits = null;
+        PercentilesMethod method = null;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if (context.parseFieldMatcher().match(currentFieldName, keysField())) {
+                    DoubleArrayList values = new DoubleArrayList(10);
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        double value = parser.doubleValue();
+                        values.add(value);
+                    }
+                    keys = values.toArray();
+                    Arrays.sort(keys);
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-                double[] keys = values.toArray();
-                otherOptions.put(keysField(), keys);
-                return true;
-            } else {
-                return false;
-            }
-        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, KEYED_FIELD)) {
-                boolean keyed = parser.booleanValue();
-                otherOptions.put(KEYED_FIELD, keyed);
-                return true;
-            } else {
-                return false;
-            }
-        } else if (token == XContentParser.Token.START_OBJECT) {
-            PercentilesMethod method = PercentilesMethod.resolveFromName(currentFieldName);
-            if (method == null) {
-                return false;
-            } else {
-                otherOptions.put(METHOD_FIELD, method);
-                switch (method) {
-                case TDIGEST:
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        if (token == XContentParser.Token.FIELD_NAME) {
-                            currentFieldName = parser.currentName();
-                        } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                            if (parseFieldMatcher.match(currentFieldName, COMPRESSION_FIELD)) {
-                                double compression = parser.doubleValue();
-                                otherOptions.put(COMPRESSION_FIELD, compression);
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if (context.parseFieldMatcher().match(currentFieldName, KEYED_FIELD)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.START_OBJECT) {
+                if (method != null) {
+                    throw new SearchParseException(context, "Found multiple methods in [" + aggregationName + "]: [" + currentFieldName
+                            + "]. only one of [" + PercentilesMethod.TDIGEST.getName() + "] and [" + PercentilesMethod.HDR.getName()
+                            + "] may be used.", parser.getTokenLocation());
+                }
+                method = PercentilesMethod.resolveFromName(currentFieldName);
+                if (method == null) {
+                    throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                            parser.getTokenLocation());
+                } else {
+                    switch (method) {
+                    case TDIGEST:
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                currentFieldName = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if (context.parseFieldMatcher().match(currentFieldName, COMPRESSION_FIELD)) {
+                                    compression = parser.doubleValue();
+                                } else {
+                                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName
+                                            + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                                }
                             } else {
-                                return false;
+                                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                        + currentFieldName + "].", parser.getTokenLocation());
                             }
-                        } else {
-                            return false;
                         }
-                    }
-                    break;
-                case HDR:
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        if (token == XContentParser.Token.FIELD_NAME) {
-                            currentFieldName = parser.currentName();
-                        } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                            if (parseFieldMatcher.match(currentFieldName, NUMBER_SIGNIFICANT_DIGITS_FIELD)) {
-                                int numberOfSignificantValueDigits = parser.intValue();
-                                otherOptions.put(NUMBER_SIGNIFICANT_DIGITS_FIELD, numberOfSignificantValueDigits);
+                        break;
+                    case HDR:
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                currentFieldName = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if (context.parseFieldMatcher().match(currentFieldName, NUMBER_SIGNIFICANT_DIGITS_FIELD)) {
+                                    numberOfSignificantValueDigits = parser.intValue();
+                                } else {
+                                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName
+                                            + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                                }
                             } else {
-                                return false;
+                                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                        + currentFieldName + "].", parser.getTokenLocation());
                             }
-                        } else {
-                            return false;
                         }
+                        break;
                     }
-                    break;
                 }
-                return true;
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
-        return false;
-    }
 
-    @Override
-    protected ValuesSourceAggregatorBuilder<Numeric, ?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        PercentilesMethod method = (PercentilesMethod) otherOptions.getOrDefault(METHOD_FIELD, PercentilesMethod.TDIGEST);
-
-        double[] cdfValues = (double[]) otherOptions.get(keysField());
-        Double compression = (Double) otherOptions.get(COMPRESSION_FIELD);
-        Integer numberOfSignificantValueDigits = (Integer) otherOptions.get(NUMBER_SIGNIFICANT_DIGITS_FIELD);
-        Boolean keyed = (Boolean) otherOptions.get(KEYED_FIELD);
-        return buildFactory(aggregationName, cdfValues, method, compression, numberOfSignificantValueDigits, keyed);
+        if (method == null) {
+            method = PercentilesMethod.TDIGEST;
+        }
+
+        switch (method) {
+        case TDIGEST:
+            if (numberOfSignificantValueDigits != null) {
+                throw new SearchParseException(context, "[number_of_significant_value_digits] cannot be used with method [tdigest] in ["
+                        + aggregationName + "].", parser.getTokenLocation());
+            }
+            if (compression == null) {
+                compression = 100.0;
+            }
+            break;
+        case HDR:
+            if (compression != null) {
+                throw new SearchParseException(context, "[compression] cannot be used with method [hdr] in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+            if (numberOfSignificantValueDigits == null) {
+                numberOfSignificantValueDigits = 3;
+            }
+            break;
+
+        default:
+            // Shouldn't get here but if we do, throw a parse exception for
+            // invalid method
+            throw new SearchParseException(context, "Unknown value for [" + currentFieldName + "] in [" + aggregationName + "]: [" + method
+                    + "].", parser.getTokenLocation());
+        }
+
+        return buildFactory(context, aggregationName, vsParser.config(), keys, method, compression,
+                numberOfSignificantValueDigits, keyed);
     }
 
-    protected abstract ValuesSourceAggregatorBuilder<Numeric, ?> buildFactory(String aggregationName, double[] cdfValues,
-            PercentilesMethod method,
-            Double compression,
-            Integer numberOfSignificantValueDigits, Boolean keyed);
+    protected abstract AggregatorFactory buildFactory(SearchContext context, String aggregationName, ValuesSourceConfig<Numeric> config,
+            double[] cdfValues, PercentilesMethod method, Double compression, Integer numberOfSignificantValueDigits, boolean keyed);
 
     protected abstract ParseField keysField();
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksAggregatorBuilder.java
deleted file mode 100644
index d022c91..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksAggregatorBuilder.java
+++ /dev/null
@@ -1,214 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentileRanksAggregatorFactory;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentileRanks;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentileRanksAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder.LeafOnly;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Objects;
-
-public class PercentileRanksAggregatorBuilder extends LeafOnly<ValuesSource.Numeric, PercentileRanksAggregatorBuilder> {
-
-    private double[] values;
-    private PercentilesMethod method = PercentilesMethod.TDIGEST;
-    private int numberOfSignificantValueDigits = 3;
-    private double compression = 100.0;
-    private boolean keyed = false;
-
-    public PercentileRanksAggregatorBuilder(String name) {
-        super(name, InternalTDigestPercentileRanks.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-    }
-
-    /**
-     * Set the values to compute percentiles from.
-     */
-    public PercentileRanksAggregatorBuilder values(double... values) {
-        double[] sortedValues = Arrays.copyOf(values, values.length);
-        Arrays.sort(sortedValues);
-        this.values = sortedValues;
-        return this;
-    }
-
-    /**
-     * Get the values to compute percentiles from.
-     */
-    public double[] values() {
-        return values;
-    }
-
-    /**
-     * Set whether the XContent response should be keyed
-     */
-    public PercentileRanksAggregatorBuilder keyed(boolean keyed) {
-        this.keyed = keyed;
-        return this;
-    }
-
-    /**
-     * Get whether the XContent response should be keyed
-     */
-    public boolean keyed() {
-        return keyed;
-    }
-
-    /**
-     * Expert: set the number of significant digits in the values. Only relevant
-     * when using {@link PercentilesMethod#HDR}.
-     */
-    public PercentileRanksAggregatorBuilder numberOfSignificantValueDigits(int numberOfSignificantValueDigits) {
-        this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
-        return this;
-    }
-
-    /**
-     * Expert: get the number of significant digits in the values. Only relevant
-     * when using {@link PercentilesMethod#HDR}.
-     */
-    public int numberOfSignificantValueDigits() {
-        return numberOfSignificantValueDigits;
-    }
-
-    /**
-     * Expert: set the compression. Higher values improve accuracy but also
-     * memory usage. Only relevant when using {@link PercentilesMethod#TDIGEST}.
-     */
-    public PercentileRanksAggregatorBuilder compression(double compression) {
-        this.compression = compression;
-        return this;
-    }
-
-    /**
-     * Expert: get the compression. Higher values improve accuracy but also
-     * memory usage. Only relevant when using {@link PercentilesMethod#TDIGEST}.
-     */
-    public double compression() {
-        return compression;
-    }
-
-    public PercentileRanksAggregatorBuilder method(PercentilesMethod method) {
-        this.method = method;
-        return this;
-    }
-
-    public PercentilesMethod method() {
-        return method;
-    }
-
-    @Override
-    protected ValuesSourceAggregatorFactory<Numeric, ?> innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-            AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-        switch (method) {
-        case TDIGEST:
-            return new TDigestPercentileRanksAggregatorFactory(name, type, config, values, compression, keyed, context, parent,
-                    subFactoriesBuilder, metaData);
-        case HDR:
-            return new HDRPercentileRanksAggregatorFactory(name, type, config, values, numberOfSignificantValueDigits, keyed, context,
-                    parent, subFactoriesBuilder, metaData);
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-    }
-
-    @Override
-    protected PercentileRanksAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        PercentileRanksAggregatorBuilder factory = new PercentileRanksAggregatorBuilder(name);
-        factory.values = in.readDoubleArray();
-        factory.keyed = in.readBoolean();
-        factory.numberOfSignificantValueDigits = in.readVInt();
-        factory.compression = in.readDouble();
-        factory.method = PercentilesMethod.TDIGEST.readFrom(in);
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        out.writeDoubleArray(values);
-        out.writeBoolean(keyed);
-        out.writeVInt(numberOfSignificantValueDigits);
-        out.writeDouble(compression);
-        method.writeTo(out);
-    }
-
-    @Override
-    protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        builder.field(PercentileRanksParser.VALUES_FIELD.getPreferredName(), values);
-        builder.field(AbstractPercentilesParser.KEYED_FIELD.getPreferredName(), keyed);
-        builder.startObject(method.getName());
-        if (method == PercentilesMethod.TDIGEST) {
-            builder.field(AbstractPercentilesParser.COMPRESSION_FIELD.getPreferredName(), compression);
-        } else {
-            builder.field(AbstractPercentilesParser.NUMBER_SIGNIFICANT_DIGITS_FIELD.getPreferredName(), numberOfSignificantValueDigits);
-        }
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        PercentileRanksAggregatorBuilder other = (PercentileRanksAggregatorBuilder) obj;
-        if (!Objects.equals(method, other.method)) {
-            return false;
-        }
-        boolean equalSettings = false;
-        switch (method) {
-        case HDR:
-            equalSettings = Objects.equals(numberOfSignificantValueDigits, other.numberOfSignificantValueDigits);
-            break;
-        case TDIGEST:
-            equalSettings = Objects.equals(compression, other.compression);
-            break;
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-        return equalSettings
-                && Objects.deepEquals(values, other.values)
-                && Objects.equals(keyed, other.keyed)
-                && Objects.equals(method, other.method);
-    }
-
-    @Override
-    protected int innerHashCode() {
-        switch (method) {
-        case HDR:
-            return Objects.hash(Arrays.hashCode(values), keyed, numberOfSignificantValueDigits, method);
-        case TDIGEST:
-            return Objects.hash(Arrays.hashCode(values), keyed, compression, method);
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksBuilder.java
new file mode 100644
index 0000000..abb9bc5
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksBuilder.java
@@ -0,0 +1,54 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.metrics.percentiles;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link PercentileRanks} aggregation.
+ */
+public class PercentileRanksBuilder extends AbstractPercentilesBuilder<PercentileRanksBuilder> {
+
+    private double[] values;
+
+    /**
+     * Sole constructor.
+     */
+    public PercentileRanksBuilder(String name) {
+        super(name, PercentileRanks.TYPE_NAME);
+    }
+
+    /**
+     * Set the values to compute percentiles from.
+     */
+    public PercentileRanksBuilder percentiles(double... values) {
+        this.values = values;
+        return this;
+    }
+
+    @Override
+    protected void doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+
+        if (values != null) {
+            builder.field(PercentileRanksParser.VALUES_FIELD.getPreferredName(), values);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java
index 400ddb5..51e900a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java
@@ -19,10 +19,14 @@
 package org.elasticsearch.search.aggregations.metrics.percentiles;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentileRanksAggregator;
 import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentileRanks;
+import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentileRanksAggregator;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.internal.SearchContext;
 
 /**
  *
@@ -46,30 +50,19 @@ public class PercentileRanksParser extends AbstractPercentilesParser {
     }
 
     @Override
-    protected ValuesSourceAggregatorBuilder<Numeric, ?> buildFactory(String aggregationName, double[] keys, PercentilesMethod method,
-            Double compression, Integer numberOfSignificantValueDigits, Boolean keyed) {
-        PercentileRanksAggregatorBuilder factory = new PercentileRanksAggregatorBuilder(aggregationName);
-        if (keys != null) {
-            factory.values(keys);
+    protected AggregatorFactory buildFactory(SearchContext context, String aggregationName, ValuesSourceConfig<Numeric> valuesSourceConfig,
+            double[] keys, PercentilesMethod method, Double compression, Integer numberOfSignificantValueDigits, boolean keyed) {
+        if (keys == null) {
+            throw new SearchParseException(context, "Missing token values in [" + aggregationName + "].", null);
         }
-        if (method != null) {
-            factory.method(method);
+        if (method == PercentilesMethod.TDIGEST) {
+            return new TDigestPercentileRanksAggregator.Factory(aggregationName, valuesSourceConfig, keys, compression, keyed);
+        } else if (method == PercentilesMethod.HDR) {
+            return new HDRPercentileRanksAggregator.Factory(aggregationName, valuesSourceConfig, keys, numberOfSignificantValueDigits,
+                    keyed);
+        } else {
+            throw new AssertionError();
         }
-        if (compression != null) {
-            factory.compression(compression);
-        }
-        if (numberOfSignificantValueDigits != null) {
-            factory.numberOfSignificantValueDigits(numberOfSignificantValueDigits);
-        }
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        return factory;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new PercentileRanksAggregatorBuilder(null);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesAggregatorBuilder.java
deleted file mode 100644
index 243028a..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesAggregatorBuilder.java
+++ /dev/null
@@ -1,214 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentilesAggregatorFactory;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentilesAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder.LeafOnly;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Objects;
-
-public class PercentilesAggregatorBuilder extends LeafOnly<ValuesSource.Numeric, PercentilesAggregatorBuilder> {
-
-    private double[] percents = PercentilesParser.DEFAULT_PERCENTS;
-    private PercentilesMethod method = PercentilesMethod.TDIGEST;
-    private int numberOfSignificantValueDigits = 3;
-    private double compression = 100.0;
-    private boolean keyed = false;
-
-    public PercentilesAggregatorBuilder(String name) {
-        super(name, InternalTDigestPercentiles.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-    }
-
-    /**
-     * Set the values to compute percentiles from.
-     */
-    public PercentilesAggregatorBuilder percentiles(double... percents) {
-        double[] sortedPercents = Arrays.copyOf(percents, percents.length);
-        Arrays.sort(sortedPercents);
-        this.percents = sortedPercents;
-        return this;
-    }
-
-    /**
-     * Get the values to compute percentiles from.
-     */
-    public double[] percentiles() {
-        return percents;
-    }
-
-    /**
-     * Set whether the XContent response should be keyed
-     */
-    public PercentilesAggregatorBuilder keyed(boolean keyed) {
-        this.keyed = keyed;
-        return this;
-    }
-
-    /**
-     * Get whether the XContent response should be keyed
-     */
-    public boolean keyed() {
-        return keyed;
-    }
-
-    /**
-     * Expert: set the number of significant digits in the values. Only relevant
-     * when using {@link PercentilesMethod#HDR}.
-     */
-    public PercentilesAggregatorBuilder numberOfSignificantValueDigits(int numberOfSignificantValueDigits) {
-        this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
-        return this;
-    }
-
-    /**
-     * Expert: get the number of significant digits in the values. Only relevant
-     * when using {@link PercentilesMethod#HDR}.
-     */
-    public int numberOfSignificantValueDigits() {
-        return numberOfSignificantValueDigits;
-    }
-
-    /**
-     * Expert: set the compression. Higher values improve accuracy but also
-     * memory usage. Only relevant when using {@link PercentilesMethod#TDIGEST}.
-     */
-    public PercentilesAggregatorBuilder compression(double compression) {
-        this.compression = compression;
-        return this;
-    }
-
-    /**
-     * Expert: get the compression. Higher values improve accuracy but also
-     * memory usage. Only relevant when using {@link PercentilesMethod#TDIGEST}.
-     */
-    public double compression() {
-        return compression;
-    }
-
-    public PercentilesAggregatorBuilder method(PercentilesMethod method) {
-        this.method = method;
-        return this;
-    }
-
-    public PercentilesMethod method() {
-        return method;
-    }
-
-    @Override
-    protected ValuesSourceAggregatorFactory<Numeric, ?> innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-            AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-        switch (method) {
-        case TDIGEST:
-            return new TDigestPercentilesAggregatorFactory(name, type, config, percents, compression, keyed, context, parent,
-                    subFactoriesBuilder, metaData);
-        case HDR:
-            return new HDRPercentilesAggregatorFactory(name, type, config, percents, numberOfSignificantValueDigits, keyed, context, parent,
-                    subFactoriesBuilder, metaData);
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-    }
-
-    @Override
-    protected PercentilesAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        PercentilesAggregatorBuilder factory = new PercentilesAggregatorBuilder(name);
-        factory.percents = in.readDoubleArray();
-        factory.keyed = in.readBoolean();
-        factory.numberOfSignificantValueDigits = in.readVInt();
-        factory.compression = in.readDouble();
-        factory.method = PercentilesMethod.TDIGEST.readFrom(in);
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        out.writeDoubleArray(percents);
-        out.writeBoolean(keyed);
-        out.writeVInt(numberOfSignificantValueDigits);
-        out.writeDouble(compression);
-        method.writeTo(out);
-    }
-
-    @Override
-    protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        builder.field(PercentilesParser.PERCENTS_FIELD.getPreferredName(), percents);
-        builder.field(AbstractPercentilesParser.KEYED_FIELD.getPreferredName(), keyed);
-        builder.startObject(method.getName());
-        if (method == PercentilesMethod.TDIGEST) {
-            builder.field(AbstractPercentilesParser.COMPRESSION_FIELD.getPreferredName(), compression);
-        } else {
-            builder.field(AbstractPercentilesParser.NUMBER_SIGNIFICANT_DIGITS_FIELD.getPreferredName(), numberOfSignificantValueDigits);
-        }
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        PercentilesAggregatorBuilder other = (PercentilesAggregatorBuilder) obj;
-        if (!Objects.equals(method, other.method)) {
-            return false;
-        }
-        boolean equalSettings = false;
-        switch (method) {
-        case HDR:
-            equalSettings = Objects.equals(numberOfSignificantValueDigits, other.numberOfSignificantValueDigits);
-            break;
-        case TDIGEST:
-            equalSettings = Objects.equals(compression, other.compression);
-            break;
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-        return equalSettings
-                && Objects.deepEquals(percents, other.percents)
-                && Objects.equals(keyed, other.keyed)
-                && Objects.equals(method, other.method);
-    }
-
-    @Override
-    protected int innerHashCode() {
-        switch (method) {
-        case HDR:
-            return Objects.hash(Arrays.hashCode(percents), keyed, numberOfSignificantValueDigits, method);
-        case TDIGEST:
-            return Objects.hash(Arrays.hashCode(percents), keyed, compression, method);
-        default:
-            throw new IllegalStateException("Illegal method [" + method.getName() + "]");
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesBuilder.java
new file mode 100644
index 0000000..399f9ea
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesBuilder.java
@@ -0,0 +1,60 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.metrics.percentiles;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
+
+
+/**
+ * Builder for the {@link Percentiles} aggregation.
+ */
+public class PercentilesBuilder extends AbstractPercentilesBuilder<PercentilesBuilder> {
+
+    double[] percentiles;
+    /**
+     * Sole constructor.
+     */
+    public PercentilesBuilder(String name) {
+        super(name, Percentiles.TYPE_NAME);
+    }
+
+    /**
+     * Set the percentiles to compute.
+     */
+    public PercentilesBuilder percentiles(double... percentiles) {
+        for (int i = 0; i < percentiles.length; i++) {
+            if (percentiles[i] < 0 || percentiles[i] > 100) {
+                throw new IllegalArgumentException("the percents in the percentiles aggregation [" +
+                        getName() + "] must be in the [0, 100] range");
+            }
+        }
+        this.percentiles = percentiles;
+        return this;
+    }
+
+    @Override
+    protected void doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (percentiles != null) {
+            builder.field(PercentilesParser.PERCENTS_FIELD.getPreferredName(), percentiles);
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethod.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethod.java
index 44316b6..c593b1f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethod.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethod.java
@@ -19,16 +19,11 @@
 
 package org.elasticsearch.search.aggregations.metrics.percentiles;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
 
 /**
  * An enum representing the methods for calculating percentiles
  */
-public enum PercentilesMethod implements Writeable<PercentilesMethod> {
+public enum PercentilesMethod {
     /**
      * The TDigest method for calculating percentiles
      */
@@ -51,20 +46,6 @@ public enum PercentilesMethod implements Writeable<PercentilesMethod> {
         return name;
     }
 
-    @Override
-    public PercentilesMethod readFrom(StreamInput in) throws IOException {
-        int ordinal = in.readVInt();
-        if (ordinal < 0 || ordinal >= values().length) {
-            throw new IOException("Unknown PercentilesMethod ordinal [" + ordinal + "]");
-        }
-        return values()[ordinal];
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(ordinal());
-    }
-
     /**
      * Returns the {@link PercentilesMethod} for this method name. returns
      * <code>null</code> if no {@link PercentilesMethod} exists for the name.
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java
index 7299ccb..6fbb2cc 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java
@@ -19,10 +19,13 @@
 package org.elasticsearch.search.aggregations.metrics.percentiles;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentilesAggregator;
 import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
+import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentilesAggregator;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.internal.SearchContext;
 
 /**
  *
@@ -35,7 +38,7 @@ public class PercentilesParser extends AbstractPercentilesParser {
         super(true);
     }
 
-    public final static double[] DEFAULT_PERCENTS = new double[] { 1, 5, 25, 50, 75, 95, 99 };
+    private final static double[] DEFAULT_PERCENTS = new double[] { 1, 5, 25, 50, 75, 95, 99 };
 
     @Override
     public String type() {
@@ -48,30 +51,18 @@ public class PercentilesParser extends AbstractPercentilesParser {
     }
 
     @Override
-    protected ValuesSourceAggregatorBuilder<Numeric, ?> buildFactory(String aggregationName, double[] keys, PercentilesMethod method,
-            Double compression, Integer numberOfSignificantValueDigits, Boolean keyed) {
-        PercentilesAggregatorBuilder factory = new PercentilesAggregatorBuilder(aggregationName);
-        if (keys != null) {
-            factory.percentiles(keys);
+    protected AggregatorFactory buildFactory(SearchContext context, String aggregationName, ValuesSourceConfig<Numeric> valuesSourceConfig,
+            double[] keys, PercentilesMethod method, Double compression, Integer numberOfSignificantValueDigits, boolean keyed) {
+        if (keys == null) {
+            keys = DEFAULT_PERCENTS;
         }
-        if (method != null) {
-            factory.method(method);
+        if (method == PercentilesMethod.TDIGEST) {
+            return new TDigestPercentilesAggregator.Factory(aggregationName, valuesSourceConfig, keys, compression, keyed);
+        } else if (method == PercentilesMethod.HDR) {
+            return new HDRPercentilesAggregator.Factory(aggregationName, valuesSourceConfig, keys, numberOfSignificantValueDigits, keyed);
+        } else {
+            throw new AssertionError();
         }
-        if (compression != null) {
-            factory.compression(compression);
-        }
-        if (numberOfSignificantValueDigits != null) {
-            factory.numberOfSignificantValueDigits(numberOfSignificantValueDigits);
-        }
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        return factory;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new PercentilesAggregatorBuilder(null);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java
index 603871e..d132fdb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java
@@ -23,7 +23,10 @@ import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -70,4 +73,34 @@ public class HDRPercentileRanksAggregator extends AbstractHDRPercentilesAggregat
             return InternalHDRPercentileRanks.percentileRank(state, Double.valueOf(name));
         }
     }
+
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
+
+        private final double[] values;
+        private final int numberOfSignificantValueDigits;
+        private final boolean keyed;
+
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig, double[] values,
+                int numberOfSignificantValueDigits, boolean keyed) {
+            super(name, InternalHDRPercentiles.TYPE.name(), valuesSourceConfig);
+            this.values = values;
+            this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
+            this.keyed = keyed;
+        }
+
+        @Override
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new HDRPercentileRanksAggregator(name, null, aggregationContext, parent, values, numberOfSignificantValueDigits, keyed,
+                    config.formatter(), pipelineAggregators, metaData);
+        }
+
+        @Override
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new HDRPercentileRanksAggregator(name, valuesSource, aggregationContext, parent, values, numberOfSignificantValueDigits,
+                    keyed, config.formatter(), pipelineAggregators, metaData);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregatorFactory.java
deleted file mode 100644
index 6a52e86..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregatorFactory.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles.hdr;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class HDRPercentileRanksAggregatorFactory
-        extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, HDRPercentileRanksAggregatorFactory> {
-
-    private final double[] values;
-    private final int numberOfSignificantValueDigits;
-    private final boolean keyed;
-
-    public HDRPercentileRanksAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, double[] values,
-            int numberOfSignificantValueDigits, boolean keyed, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.values = values;
-        this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
-        this.keyed = keyed;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new HDRPercentileRanksAggregator(name, null, aggregationContext, parent, values, numberOfSignificantValueDigits, keyed,
-                config.formatter(), pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new HDRPercentileRanksAggregator(name, valuesSource, aggregationContext, parent, values, numberOfSignificantValueDigits,
-                keyed, config.formatter(), pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java
index f51769d..d1c0a62 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java
@@ -21,9 +21,13 @@ package org.elasticsearch.search.aggregations.metrics.percentiles.hdr;
 import org.HdrHistogram.DoubleHistogram;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -71,4 +75,34 @@ public class HDRPercentilesAggregator extends AbstractHDRPercentilesAggregator {
                 keyed,
                 formatter, pipelineAggregators(), metaData());
     }
+
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
+
+        private final double[] percents;
+        private final int numberOfSignificantValueDigits;
+        private final boolean keyed;
+
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig, double[] percents,
+                int numberOfSignificantValueDigits, boolean keyed) {
+            super(name, InternalTDigestPercentiles.TYPE.name(), valuesSourceConfig);
+            this.percents = percents;
+            this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
+            this.keyed = keyed;
+        }
+
+        @Override
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new HDRPercentilesAggregator(name, null, aggregationContext, parent, percents, numberOfSignificantValueDigits, keyed,
+                    config.formatter(), pipelineAggregators, metaData);
+        }
+
+        @Override
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new HDRPercentilesAggregator(name, valuesSource, aggregationContext, parent, percents, numberOfSignificantValueDigits,
+                    keyed, config.formatter(), pipelineAggregators, metaData);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregatorFactory.java
deleted file mode 100644
index 6441610..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregatorFactory.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles.hdr;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class HDRPercentilesAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, HDRPercentilesAggregatorFactory> {
-
-    private final double[] percents;
-    private final int numberOfSignificantValueDigits;
-    private final boolean keyed;
-
-    public HDRPercentilesAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, double[] percents,
-            int numberOfSignificantValueDigits, boolean keyed, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.percents = percents;
-        this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
-        this.keyed = keyed;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new HDRPercentilesAggregator(name, null, aggregationContext, parent, percents, numberOfSignificantValueDigits, keyed,
-                config.formatter(), pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new HDRPercentilesAggregator(name, valuesSource, aggregationContext, parent, percents, numberOfSignificantValueDigits, keyed,
-                config.formatter(), pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java
index 3db68ca..95c9f06 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java
@@ -22,7 +22,10 @@ import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -65,4 +68,34 @@ public class TDigestPercentileRanksAggregator extends AbstractTDigestPercentiles
             return InternalTDigestPercentileRanks.percentileRank(state, Double.valueOf(name));
         }
     }
+
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
+
+        private final double[] values;
+        private final double compression;
+        private final boolean keyed;
+
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig,
+                double[] values, double compression, boolean keyed) {
+            super(name, InternalTDigestPercentiles.TYPE.name(), valuesSourceConfig);
+            this.values = values;
+            this.compression = compression;
+            this.keyed = keyed;
+        }
+
+        @Override
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new TDigestPercentileRanksAggregator(name, null, aggregationContext, parent, values, compression, keyed, config.formatter(),
+                    pipelineAggregators, metaData);
+        }
+
+        @Override
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new TDigestPercentileRanksAggregator(name, valuesSource, aggregationContext, parent, values, compression, keyed,
+                    config.formatter(), pipelineAggregators, metaData);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregatorFactory.java
deleted file mode 100644
index b75db65..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregatorFactory.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles.tdigest;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class TDigestPercentileRanksAggregatorFactory
-        extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, TDigestPercentileRanksAggregatorFactory> {
-
-    private final double[] percents;
-    private final double compression;
-    private final boolean keyed;
-
-    public TDigestPercentileRanksAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, double[] percents,
-            double compression, boolean keyed, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.percents = percents;
-        this.compression = compression;
-        this.keyed = keyed;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new TDigestPercentileRanksAggregator(name, null, aggregationContext, parent, percents, compression, keyed,
-                config.formatter(), pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new TDigestPercentileRanksAggregator(name, valuesSource, aggregationContext, parent, percents, compression, keyed,
-                config.formatter(), pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java
index 3bf7cff..43800bf 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java
@@ -22,7 +22,10 @@ import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -65,4 +68,34 @@ public class TDigestPercentilesAggregator extends AbstractTDigestPercentilesAggr
     public InternalAggregation buildEmptyAggregation() {
         return new InternalTDigestPercentiles(name, keys, new TDigestState(compression), keyed, formatter, pipelineAggregators(), metaData());
     }
+
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
+
+        private final double[] percents;
+        private final double compression;
+        private final boolean keyed;
+
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig,
+                double[] percents, double compression, boolean keyed) {
+            super(name, InternalTDigestPercentiles.TYPE.name(), valuesSourceConfig);
+            this.percents = percents;
+            this.compression = compression;
+            this.keyed = keyed;
+        }
+
+        @Override
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new TDigestPercentilesAggregator(name, null, aggregationContext, parent, percents, compression, keyed, config.formatter(),
+                    pipelineAggregators, metaData);
+        }
+
+        @Override
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new TDigestPercentilesAggregator(name, valuesSource, aggregationContext, parent, percents, compression, keyed,
+                    config.formatter(), pipelineAggregators, metaData);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregatorFactory.java
deleted file mode 100644
index 3838dc2..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregatorFactory.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles.tdigest;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class TDigestPercentilesAggregatorFactory
-        extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, TDigestPercentilesAggregatorFactory> {
-
-    private final double[] percents;
-    private final double compression;
-    private final boolean keyed;
-
-    public TDigestPercentilesAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, double[] percents,
-            double compression, boolean keyed, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.percents = percents;
-        this.compression = compression;
-        this.keyed = keyed;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new TDigestPercentilesAggregator(name, null, aggregationContext, parent, percents, compression, keyed,
-                config.formatter(), pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new TDigestPercentilesAggregator(name, valuesSource, aggregationContext, parent, percents, compression, keyed,
-                config.formatter(), pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java
index d2d7ccb..68d886a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java
@@ -20,30 +20,30 @@
 package org.elasticsearch.search.aggregations.metrics.scripted;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.script.ExecutableScript;
 import org.elasticsearch.script.LeafSearchScript;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.script.SearchScript;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.MetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.internal.SearchContext;
+
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
+import java.util.Map.Entry;
 
 public class ScriptedMetricAggregator extends MetricsAggregator {
 
@@ -106,7 +106,7 @@ public class ScriptedMetricAggregator extends MetricsAggregator {
         return new InternalScriptedMetric(name, null, reduceScript, pipelineAggregators(), metaData());
     }
 
-    public static class ScriptedMetricAggregatorBuilder extends AggregatorBuilder<ScriptedMetricAggregatorBuilder> {
+    public static class Factory extends AggregatorFactory {
 
         private Script initScript;
         private Script mapScript;
@@ -114,159 +114,79 @@ public class ScriptedMetricAggregator extends MetricsAggregator {
         private Script reduceScript;
         private Map<String, Object> params;
 
-        public ScriptedMetricAggregatorBuilder(String name) {
-            super(name, InternalScriptedMetric.TYPE);
-        }
-
-        /**
-         * Set the <tt>init</tt> script.
-         */
-        public ScriptedMetricAggregatorBuilder initScript(Script initScript) {
+        public Factory(String name, Script initScript, Script mapScript, Script combineScript, Script reduceScript,
+                Map<String, Object> params) {
+            super(name, InternalScriptedMetric.TYPE.name());
             this.initScript = initScript;
-            return this;
-        }
-
-        /**
-         * Get the <tt>init</tt> script.
-         */
-        public Script initScript() {
-            return initScript;
-        }
-
-        /**
-         * Set the <tt>map</tt> script.
-         */
-        public ScriptedMetricAggregatorBuilder mapScript(Script mapScript) {
             this.mapScript = mapScript;
-            return this;
-        }
-
-        /**
-         * Get the <tt>map</tt> script.
-         */
-        public Script mapScript() {
-            return mapScript;
-        }
-
-        /**
-         * Set the <tt>combine</tt> script.
-         */
-        public ScriptedMetricAggregatorBuilder combineScript(Script combineScript) {
             this.combineScript = combineScript;
-            return this;
-        }
-
-        /**
-         * Get the <tt>combine</tt> script.
-         */
-        public Script combineScript() {
-            return combineScript;
-        }
-
-        /**
-         * Set the <tt>reduce</tt> script.
-         */
-        public ScriptedMetricAggregatorBuilder reduceScript(Script reduceScript) {
             this.reduceScript = reduceScript;
-            return this;
-        }
-
-        /**
-         * Get the <tt>reduce</tt> script.
-         */
-        public Script reduceScript() {
-            return reduceScript;
-        }
-
-        /**
-         * Set parameters that will be available in the <tt>init</tt>,
-         * <tt>map</tt> and <tt>combine</tt> phases.
-         */
-        public ScriptedMetricAggregatorBuilder params(Map<String, Object> params) {
             this.params = params;
-            return this;
-        }
-
-        /**
-         * Get parameters that will be available in the <tt>init</tt>,
-         * <tt>map</tt> and <tt>combine</tt> phases.
-         */
-        public Map<String, Object> params() {
-            return params;
         }
 
         @Override
-        protected ScriptedMetricAggregatorFactory doBuild(AggregationContext context, AggregatorFactory<?> parent,
-                Builder subfactoriesBuilder) throws IOException {
-            return new ScriptedMetricAggregatorFactory(name, type, initScript, mapScript, combineScript, reduceScript, params, context,
-                    parent, subfactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
-            builder.startObject();
-            if (initScript != null) {
-                builder.field(ScriptedMetricParser.INIT_SCRIPT_FIELD.getPreferredName(), initScript);
+        public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            if (collectsFromSingleBucket == false) {
+                return asMultiBucketAggregator(this, context, parent);
             }
-
-            if (mapScript != null) {
-                builder.field(ScriptedMetricParser.MAP_SCRIPT_FIELD.getPreferredName(), mapScript);
+            Map<String, Object> params = this.params;
+            if (params != null) {
+                params = deepCopyParams(params, context.searchContext());
+            } else {
+                params = new HashMap<>();
+                params.put("_agg", new HashMap<String, Object>());
             }
-
-            if (combineScript != null) {
-                builder.field(ScriptedMetricParser.COMBINE_SCRIPT_FIELD.getPreferredName(), combineScript);
+            return new ScriptedMetricAggregator(name, insertParams(initScript, params), insertParams(mapScript, params), insertParams(
+                    combineScript, params), deepCopyScript(reduceScript, context.searchContext()), params, context, parent, pipelineAggregators,
+                    metaData);
             }
 
-            if (reduceScript != null) {
-                builder.field(ScriptedMetricParser.REDUCE_SCRIPT_FIELD.getPreferredName(), reduceScript);
+        private static Script insertParams(Script script, Map<String, Object> params) {
+            if (script == null) {
+                return null;
             }
-            if (params != null) {
-                builder.field(ScriptedMetricParser.PARAMS_FIELD.getPreferredName());
-                builder.map(params);
+            return new Script(script.getScript(), script.getType(), script.getLang(), params);
+        }
+
+        private static Script deepCopyScript(Script script, SearchContext context) {
+            if (script != null) {
+                Map<String, Object> params = script.getParams();
+                if (params != null) {
+                    params = deepCopyParams(params, context);
+                }
+                return new Script(script.getScript(), script.getType(), script.getLang(), params);
+            } else {
+                return null;
             }
-            builder.endObject();
-            return builder;
         }
 
-        @Override
-        protected ScriptedMetricAggregatorBuilder doReadFrom(String name, StreamInput in) throws IOException {
-            ScriptedMetricAggregatorBuilder factory = new ScriptedMetricAggregatorBuilder(name);
-            factory.initScript = in.readOptionalStreamable(Script.SUPPLIER);
-            factory.mapScript = in.readOptionalStreamable(Script.SUPPLIER);
-            factory.combineScript = in.readOptionalStreamable(Script.SUPPLIER);
-            factory.reduceScript = in.readOptionalStreamable(Script.SUPPLIER);
-            if (in.readBoolean()) {
-                factory.params = in.readMap();
+        @SuppressWarnings({ "unchecked" })
+        private static <T> T deepCopyParams(T original, SearchContext context) {
+            T clone;
+            if (original instanceof Map) {
+                Map<?, ?> originalMap = (Map<?, ?>) original;
+                Map<Object, Object> clonedMap = new HashMap<>();
+                for (Entry<?, ?> e : originalMap.entrySet()) {
+                    clonedMap.put(deepCopyParams(e.getKey(), context), deepCopyParams(e.getValue(), context));
+                }
+                clone = (T) clonedMap;
+            } else if (original instanceof List) {
+                List<?> originalList = (List<?>) original;
+                List<Object> clonedList = new ArrayList<Object>();
+                for (Object o : originalList) {
+                    clonedList.add(deepCopyParams(o, context));
+                }
+                clone = (T) clonedList;
+            } else if (original instanceof String || original instanceof Integer || original instanceof Long || original instanceof Short
+                    || original instanceof Byte || original instanceof Float || original instanceof Double || original instanceof Character
+                    || original instanceof Boolean) {
+                clone = original;
+            } else {
+                throw new SearchParseException(context, "Can only clone primitives, String, ArrayList, and HashMap. Found: "
+                        + original.getClass().getCanonicalName(), null);
             }
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalStreamable(initScript);
-            out.writeOptionalStreamable(mapScript);
-            out.writeOptionalStreamable(combineScript);
-            out.writeOptionalStreamable(reduceScript);
-            boolean hasParams = params != null;
-            out.writeBoolean(hasParams);
-            if (hasParams) {
-                out.writeMap(params);
-            }
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(initScript, mapScript, combineScript, reduceScript, params);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            ScriptedMetricAggregatorBuilder other = (ScriptedMetricAggregatorBuilder) obj;
-            return Objects.equals(initScript, other.initScript)
-                    && Objects.equals(mapScript, other.mapScript)
-                    && Objects.equals(combineScript, other.combineScript)
-                    && Objects.equals(reduceScript, other.reduceScript)
-                    && Objects.equals(params, other.params);
+            return clone;
         }
 
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregatorFactory.java
deleted file mode 100644
index e19f7b7..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregatorFactory.java
+++ /dev/null
@@ -1,123 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.scripted;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.SearchParseException;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.internal.SearchContext;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-
-public class ScriptedMetricAggregatorFactory extends AggregatorFactory<ScriptedMetricAggregatorFactory> {
-
-    private final Script initScript;
-    private final Script mapScript;
-    private final Script combineScript;
-    private final Script reduceScript;
-    private final Map<String, Object> params;
-
-    public ScriptedMetricAggregatorFactory(String name, Type type, Script initScript, Script mapScript, Script combineScript,
-            Script reduceScript, Map<String, Object> params, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactories, Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactories, metaData);
-        this.initScript = initScript;
-        this.mapScript = mapScript;
-        this.combineScript = combineScript;
-        this.reduceScript = reduceScript;
-        this.params = params;
-    }
-
-    @Override
-    public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        if (collectsFromSingleBucket == false) {
-            return asMultiBucketAggregator(this, context, parent);
-        }
-        Map<String, Object> params = this.params;
-        if (params != null) {
-            params = deepCopyParams(params, context.searchContext());
-        } else {
-            params = new HashMap<>();
-            params.put("_agg", new HashMap<String, Object>());
-        }
-        return new ScriptedMetricAggregator(name, insertParams(initScript, params), insertParams(mapScript, params),
-                insertParams(combineScript, params), deepCopyScript(reduceScript, context.searchContext()), params, context, parent,
-                pipelineAggregators, metaData);
-    }
-
-    private static Script insertParams(Script script, Map<String, Object> params) {
-        if (script == null) {
-            return null;
-        }
-        return new Script(script.getScript(), script.getType(), script.getLang(), params);
-    }
-
-    private static Script deepCopyScript(Script script, SearchContext context) {
-        if (script != null) {
-            Map<String, Object> params = script.getParams();
-            if (params != null) {
-                params = deepCopyParams(params, context);
-            }
-            return new Script(script.getScript(), script.getType(), script.getLang(), params);
-        } else {
-            return null;
-        }
-    }
-
-    @SuppressWarnings({ "unchecked" })
-    private static <T> T deepCopyParams(T original, SearchContext context) {
-        T clone;
-        if (original instanceof Map) {
-            Map<?, ?> originalMap = (Map<?, ?>) original;
-            Map<Object, Object> clonedMap = new HashMap<>();
-            for (Entry<?, ?> e : originalMap.entrySet()) {
-                clonedMap.put(deepCopyParams(e.getKey(), context), deepCopyParams(e.getValue(), context));
-            }
-            clone = (T) clonedMap;
-        } else if (original instanceof List) {
-            List<?> originalList = (List<?>) original;
-            List<Object> clonedList = new ArrayList<Object>();
-            for (Object o : originalList) {
-                clonedList.add(deepCopyParams(o, context));
-            }
-            clone = (T) clonedList;
-        } else if (original instanceof String || original instanceof Integer || original instanceof Long || original instanceof Short
-                || original instanceof Byte || original instanceof Float || original instanceof Double || original instanceof Character
-                || original instanceof Boolean) {
-            clone = original;
-        } else {
-            throw new SearchParseException(context,
-                    "Can only clone primitives, String, ArrayList, and HashMap. Found: " + original.getClass().getCanonicalName(), null);
-        }
-        return clone;
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java
new file mode 100644
index 0000000..803123f
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java
@@ -0,0 +1,112 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.scripted;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.search.aggregations.metrics.MetricsAggregationBuilder;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * Builder for the {@link ScriptedMetric} aggregation.
+ */
+public class ScriptedMetricBuilder extends MetricsAggregationBuilder<ScriptedMetricBuilder> {
+
+    private Script initScript = null;
+    private Script mapScript = null;
+    private Script combineScript = null;
+    private Script reduceScript = null;
+    private Map<String, Object> params = null;
+
+    /**
+     * Sole constructor.
+     */
+    public ScriptedMetricBuilder(String name) {
+        super(name, InternalScriptedMetric.TYPE.name());
+    }
+
+    /**
+     * Set the <tt>init</tt> script.
+     */
+    public ScriptedMetricBuilder initScript(Script initScript) {
+        this.initScript = initScript;
+        return this;
+    }
+
+    /**
+     * Set the <tt>map</tt> script.
+     */
+    public ScriptedMetricBuilder mapScript(Script mapScript) {
+        this.mapScript = mapScript;
+        return this;
+    }
+
+    /**
+     * Set the <tt>combine</tt> script.
+     */
+    public ScriptedMetricBuilder combineScript(Script combineScript) {
+        this.combineScript = combineScript;
+        return this;
+    }
+
+    /**
+     * Set the <tt>reduce</tt> script.
+     */
+    public ScriptedMetricBuilder reduceScript(Script reduceScript) {
+        this.reduceScript = reduceScript;
+        return this;
+    }
+
+    /**
+     * Set parameters that will be available in the <tt>init</tt>, <tt>map</tt>
+     * and <tt>combine</tt> phases.
+     */
+    public ScriptedMetricBuilder params(Map<String, Object> params) {
+        this.params = params;
+        return this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
+
+        if (initScript != null) {
+            builder.field(ScriptedMetricParser.INIT_SCRIPT_FIELD.getPreferredName(), initScript);
+        }
+
+        if (mapScript != null) {
+            builder.field(ScriptedMetricParser.MAP_SCRIPT_FIELD.getPreferredName(), mapScript);
+        }
+
+        if (combineScript != null) {
+            builder.field(ScriptedMetricParser.COMBINE_SCRIPT_FIELD.getPreferredName(), combineScript);
+        }
+
+        if (reduceScript != null) {
+            builder.field(ScriptedMetricParser.REDUCE_SCRIPT_FIELD.getPreferredName(), reduceScript);
+        }
+        if (params != null) {
+            builder.field(ScriptedMetricParser.PARAMS_FIELD.getPreferredName());
+            builder.map(params);
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java
index c2fffde..528078c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java
@@ -20,14 +20,14 @@
 package org.elasticsearch.search.aggregations.metrics.scripted;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptParameterParser;
 import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.HashSet;
@@ -35,7 +35,7 @@ import java.util.Map;
 import java.util.Set;
 
 public class ScriptedMetricParser implements Aggregator.Parser {
-
+    
     public static final String INIT_SCRIPT = "init_script";
     public static final String MAP_SCRIPT = "map_script";
     public static final String COMBINE_SCRIPT = "combine_script";
@@ -54,7 +54,7 @@ public class ScriptedMetricParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         Script initScript = null;
         Script mapScript = null;
         Script combineScript = null;
@@ -87,16 +87,17 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, REDUCE_PARAMS_FIELD)) {
                   reduceParams = parser.map();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token.isValue()) {
                 if (!scriptParameterParser.token(currentFieldName, token, parser, context.parseFieldMatcher())) {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + aggregationName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
 
@@ -106,8 +107,10 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 initScript = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), params);
             }
         } else if (initScript.getParams() != null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "init_script params are not supported. Parameters for the init_script must be specified in the params field on the scripted_metric aggregator not inside the init_script object");
+            throw new SearchParseException(
+                    context,
+                    "init_script params are not supported. Parameters for the init_script must be specified in the params field on the scripted_metric aggregator not inside the init_script object",
+                    parser.getTokenLocation());
         }
 
         if (mapScript == null) { // Didn't find anything using the new API so try using the old one instead
@@ -116,8 +119,10 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 mapScript = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), params);
             }
         } else if (mapScript.getParams() != null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "map_script params are not supported. Parameters for the map_script must be specified in the params field on the scripted_metric aggregator not inside the map_script object");
+            throw new SearchParseException(
+                    context,
+                    "map_script params are not supported. Parameters for the map_script must be specified in the params field on the scripted_metric aggregator not inside the map_script object",
+                    parser.getTokenLocation());
         }
 
         if (combineScript == null) { // Didn't find anything using the new API so try using the old one instead
@@ -126,8 +131,10 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 combineScript = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), params);
             }
         } else if (combineScript.getParams() != null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "combine_script params are not supported. Parameters for the combine_script must be specified in the params field on the scripted_metric aggregator not inside the combine_script object");
+            throw new SearchParseException(
+                    context,
+                    "combine_script params are not supported. Parameters for the combine_script must be specified in the params field on the scripted_metric aggregator not inside the combine_script object",
+                    parser.getTokenLocation());
         }
 
         if (reduceScript == null) { // Didn't find anything using the new API so try using the old one instead
@@ -136,23 +143,11 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 reduceScript = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), reduceParams);
             }
         }
-
+        
         if (mapScript == null) {
-            throw new ParsingException(parser.getTokenLocation(), "map_script field is required in [" + aggregationName + "].");
+            throw new SearchParseException(context, "map_script field is required in [" + aggregationName + "].", parser.getTokenLocation());
         }
-
-        ScriptedMetricAggregator.ScriptedMetricAggregatorBuilder factory = new ScriptedMetricAggregator.ScriptedMetricAggregatorBuilder(aggregationName);
-        factory.initScript(initScript);
-        factory.mapScript(mapScript);
-        factory.combineScript(combineScript);
-        factory.reduceScript(reduceScript);
-        factory.params(params);
-        return factory;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new ScriptedMetricAggregator.ScriptedMetricAggregatorBuilder(null);
+        return new ScriptedMetricAggregator.Factory(aggregationName, initScript, mapScript, combineScript, reduceScript, params);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java
index bdd513e..6e648cb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java
@@ -19,29 +19,21 @@
 package org.elasticsearch.search.aggregations.metrics.stats;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -163,42 +155,23 @@ public class StatsAggregator extends NumericMetricsAggregator.MultiValue {
         return new InternalStats(name, 0, 0, Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class StatsAggregatorBuilder extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, StatsAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public StatsAggregatorBuilder(String name) {
-            super(name, InternalStats.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalStats.TYPE.name(), valuesSourceConfig);
         }
 
         @Override
-        protected StatsAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new StatsAggregatorFactory(name, type, config, context, parent, subFactoriesBuilder, metaData);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new StatsAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected StatsAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new StatsAggregator.StatsAggregatorBuilder(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new StatsAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregatorFactory.java
deleted file mode 100644
index 9c9cc68..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregatorFactory.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.stats;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class StatsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, StatsAggregatorFactory> {
-
-    public StatsAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new StatsAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new StatsAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsBuilder.java
new file mode 100644
index 0000000..3b6e549
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.stats;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Stats} aggregation.
+ */
+public class StatsBuilder extends ValuesSourceMetricsAggregationBuilder<StatsBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public StatsBuilder(String name) {
+        super(name, InternalStats.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java
index e8490dc..86c85e4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java
@@ -18,45 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.metrics.stats;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class StatsParser extends NumericValuesSourceParser {
+public class StatsParser extends NumericValuesSourceMetricsAggregatorParser<InternalStats> {
 
     public StatsParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalStats.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected StatsAggregator.StatsAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new StatsAggregator.StatsAggregatorBuilder(aggregationName);
+        super(InternalStats.TYPE);
     }
 
     @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new StatsAggregator.StatsAggregatorBuilder(null);
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new StatsAggregator.Factory(aggregationName, config);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java
index fab1cd5..86a6481 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java
@@ -19,44 +19,32 @@
 package org.elasticsearch.search.aggregations.metrics.stats.extended;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class ExtendedStatsAggregator extends NumericMetricsAggregator.MultiValue {
 
-    public static final ParseField SIGMA_FIELD = new ParseField("sigma");
-
     final ValuesSource.Numeric valuesSource;
     final ValueFormatter formatter;
     final double sigma;
@@ -200,57 +188,29 @@ public class ExtendedStatsAggregator extends NumericMetricsAggregator.MultiValue
         Releasables.close(counts, maxes, mins, sumOfSqrs, sums);
     }
 
-    public static class ExtendedStatsAggregatorBuilder extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, ExtendedStatsAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        private double sigma = 2.0;
+        private final double sigma;
 
-        public ExtendedStatsAggregatorBuilder(String name) {
-            super(name, InternalExtendedStats.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-        }
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig, double sigma) {
+            super(name, InternalExtendedStats.TYPE.name(), valuesSourceConfig);
 
-        public ExtendedStatsAggregatorBuilder sigma(double sigma) {
             this.sigma = sigma;
-            return this;
-        }
-
-        public double sigma() {
-            return sigma;
-        }
-
-        @Override
-        protected ExtendedStatsAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new ExtendedStatsAggregatorFactory(name, type, config, sigma, context, parent, subFactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected ExtendedStatsAggregatorBuilder innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder factory = new ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder(name);
-            factory.sigma = in.readDouble();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDouble(sigma);
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(SIGMA_FIELD.getPreferredName(), sigma);
-            return builder;
         }
 
         @Override
-        protected int innerHashCode() {
-            return Objects.hash(sigma);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new ExtendedStatsAggregator(name, null, config.formatter(), aggregationContext, parent, sigma, pipelineAggregators,
+                    metaData);
         }
 
         @Override
-        protected boolean innerEquals(Object obj) {
-            ExtendedStatsAggregatorBuilder other = (ExtendedStatsAggregatorBuilder) obj;
-            return Objects.equals(sigma, other.sigma);
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new ExtendedStatsAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, sigma,
+                    pipelineAggregators, metaData);
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregatorFactory.java
deleted file mode 100644
index 16208d9..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregatorFactory.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.stats.extended;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class ExtendedStatsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, ExtendedStatsAggregatorFactory> {
-
-    private final double sigma;
-
-    public ExtendedStatsAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, double sigma,
-            AggregationContext context, AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder,
-            Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-        this.sigma = sigma;
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new ExtendedStatsAggregator(name, null, config.formatter(), aggregationContext, parent, sigma, pipelineAggregators,
-                metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new ExtendedStatsAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, sigma, pipelineAggregators,
-                metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsBuilder.java
new file mode 100644
index 0000000..28f4d73
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsBuilder.java
@@ -0,0 +1,55 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.stats.extended;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link ExtendedStats} aggregation.
+ */
+public class ExtendedStatsBuilder extends ValuesSourceMetricsAggregationBuilder<ExtendedStatsBuilder> {
+
+    private Double sigma;
+
+    /**
+     * Sole constructor.
+     */
+    public ExtendedStatsBuilder(String name) {
+        super(name, InternalExtendedStats.TYPE.name());
+    }
+
+    public ExtendedStatsBuilder sigma(double sigma) {
+        this.sigma = sigma;
+        return this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.internalXContent(builder, params);
+
+        if (sigma != null) {
+            builder.field("sigma", sigma);
+        }
+
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java
index fb8caae..b29ce08 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java
@@ -19,55 +19,65 @@
 package org.elasticsearch.search.aggregations.metrics.stats.extended;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  *
  */
-public class ExtendedStatsParser extends NumericValuesSourceParser {
+public class ExtendedStatsParser  implements Aggregator.Parser {
 
-    public ExtendedStatsParser() {
-        super(true, true, false);
-    }
+    static final ParseField SIGMA = new ParseField("sigma");
 
     @Override
     public String type() {
         return InternalExtendedStats.TYPE.name();
     }
 
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config, double sigma) {
+        return new ExtendedStatsAggregator.Factory(aggregationName, config, sigma);
+    }
+
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (parseFieldMatcher.match(currentFieldName, ExtendedStatsAggregator.SIGMA_FIELD)) {
-            if (token.isValue()) {
-                otherOptions.put(ExtendedStatsAggregator.SIGMA_FIELD, parser.doubleValue());
-                return true;
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalExtendedStats.TYPE, context).formattable(true)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        double sigma = 2.0;
+
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                if (context.parseFieldMatcher().match(currentFieldName, SIGMA)) {
+                    sigma = parser.doubleValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
-        return false;
-    }
 
-    @Override
-    protected ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder factory = new ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder(aggregationName);
-        Double sigma = (Double) otherOptions.get(ExtendedStatsAggregator.SIGMA_FIELD);
-        if (sigma != null) {
-            factory.sigma(sigma);
+        if (sigma < 0) {
+            throw new SearchParseException(context, "[sigma] must not be negative. Value provided was" + sigma, parser.getTokenLocation());
         }
-        return factory;
-    }
 
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder(null);
+        return createFactory(aggregationName, vsParser.config(), sigma);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java
index 7c1548d..8a6b40c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java
@@ -19,28 +19,20 @@
 package org.elasticsearch.search.aggregations.metrics.sum;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -113,42 +105,23 @@ public class SumAggregator extends NumericMetricsAggregator.SingleValue {
         return new InternalSum(name, 0.0, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class SumAggregatorBuilder extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, SumAggregatorBuilder> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public SumAggregatorBuilder(String name) {
-            super(name, InternalSum.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalSum.TYPE.name(), valuesSourceConfig);
         }
 
         @Override
-        protected SumAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<Numeric> config,
-                AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException {
-            return new SumAggregatorFactory(name, type, config, context, parent, subFactoriesBuilder, metaData);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new SumAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected ValuesSourceAggregatorBuilder<Numeric, SumAggregatorBuilder> innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new SumAggregator.SumAggregatorBuilder(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new SumAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregatorFactory.java
deleted file mode 100644
index 4efd4b4..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregatorFactory.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.sum;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class SumAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, SumAggregatorFactory> {
-
-    public SumAggregatorFactory(String name, Type type, ValuesSourceConfig<Numeric> config, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new SumAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new SumAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumBuilder.java
new file mode 100644
index 0000000..53e8780
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.sum;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Sum} aggregation.
+ */
+public class SumBuilder extends ValuesSourceMetricsAggregationBuilder<SumBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public SumBuilder(String name) {
+        super(name, InternalSum.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java
index dd34b3a..b43285a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java
@@ -18,45 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.metrics.sum;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class SumParser extends NumericValuesSourceParser {
+public class SumParser extends NumericValuesSourceMetricsAggregatorParser<InternalSum> {
 
     public SumParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalSum.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected SumAggregator.SumAggregatorBuilder createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new SumAggregator.SumAggregatorBuilder(aggregationName);
+        super(InternalSum.TYPE);
     }
 
     @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new SumAggregator.SumAggregatorBuilder(null);
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new SumAggregator.Factory(aggregationName, config);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java
index c064f9b..82dea48 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java
@@ -30,47 +30,28 @@ import org.apache.lucene.search.TopDocsCollector;
 import org.apache.lucene.search.TopFieldCollector;
 import org.apache.lucene.search.TopFieldDocs;
 import org.apache.lucene.search.TopScoreDocCollector;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.util.LongObjectPagedHashMap;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.script.Script;
+import org.elasticsearch.search.aggregations.AggregationInitializationException;
 import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
 import org.elasticsearch.search.aggregations.metrics.MetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.builder.SearchSourceBuilder.ScriptField;
 import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
-import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.internal.InternalSearchHit;
 import org.elasticsearch.search.internal.InternalSearchHits;
 import org.elasticsearch.search.internal.SubSearchContext;
-import org.elasticsearch.search.sort.SortBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
+
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  */
@@ -200,536 +181,26 @@ public class TopHitsAggregator extends MetricsAggregator {
         Releasables.close(topDocsCollectors);
     }
 
-    public static class TopHitsAggregatorBuilder extends AggregatorBuilder<TopHitsAggregatorBuilder> {
-
-        private int from = 0;
-        private int size = 3;
-        private boolean explain = false;
-        private boolean version = false;
-        private boolean trackScores = false;
-        private List<BytesReference> sorts = null;
-        private HighlightBuilder highlightBuilder;
-        private List<String> fieldNames;
-        private List<String> fieldDataFields;
-        private List<ScriptField> scriptFields;
-        private FetchSourceContext fetchSourceContext;
-
-        public TopHitsAggregatorBuilder(String name) {
-            super(name, InternalTopHits.TYPE);
-        }
-
-        /**
-         * From index to start the search from. Defaults to <tt>0</tt>.
-         */
-        public TopHitsAggregatorBuilder from(int from) {
-            this.from = from;
-            return this;
-        }
-
-        /**
-         * Gets the from index to start the search from.
-         **/
-        public int from() {
-            return from;
-        }
-
-        /**
-         * The number of search hits to return. Defaults to <tt>10</tt>.
-         */
-        public TopHitsAggregatorBuilder size(int size) {
-            this.size = size;
-            return this;
-        }
-
-        /**
-         * Gets the number of search hits to return.
-         */
-        public int size() {
-            return size;
-        }
-
-        /**
-         * Adds a sort against the given field name and the sort ordering.
-         *
-         * @param name
-         *            The name of the field
-         * @param order
-         *            The sort ordering
-         */
-        public TopHitsAggregatorBuilder sort(String name, SortOrder order) {
-            sort(SortBuilders.fieldSort(name).order(order));
-            return this;
-        }
-
-        /**
-         * Add a sort against the given field name.
-         *
-         * @param name
-         *            The name of the field to sort by
-         */
-        public TopHitsAggregatorBuilder sort(String name) {
-            sort(SortBuilders.fieldSort(name));
-            return this;
-        }
-
-        /**
-         * Adds a sort builder.
-         */
-        public TopHitsAggregatorBuilder sort(SortBuilder sort) {
-            try {
-                if (sorts == null) {
-                    sorts = new ArrayList<>();
-                }
-                // NORELEASE when sort has been refactored and made writeable
-                // add the sortBuilcer to the List directly instead of
-                // serialising to XContent
-                XContentBuilder builder = XContentFactory.jsonBuilder();
-                builder.startObject();
-                sort.toXContent(builder, EMPTY_PARAMS);
-                builder.endObject();
-                sorts.add(builder.bytes());
-            } catch (IOException e) {
-                throw new RuntimeException(e);
-            }
-            return this;
-        }
-
-        /**
-         * Adds a sort builder.
-         */
-        public TopHitsAggregatorBuilder sorts(List<BytesReference> sorts) {
-            if (this.sorts == null) {
-                this.sorts = new ArrayList<>();
-            }
-            for (BytesReference sort : sorts) {
-                this.sorts.add(sort);
-            }
-            return this;
-        }
-
-        /**
-         * Gets the bytes representing the sort builders for this request.
-         */
-        public List<BytesReference> sorts() {
-            return sorts;
-        }
-
-        /**
-         * Adds highlight to perform as part of the search.
-         */
-        public TopHitsAggregatorBuilder highlighter(HighlightBuilder highlightBuilder) {
-            this.highlightBuilder = highlightBuilder;
-            return this;
-        }
-
-        /**
-         * Gets the hightlighter builder for this request.
-         */
-        public HighlightBuilder highlighter() {
-            return highlightBuilder;
-        }
+    public static class Factory extends AggregatorFactory {
 
-        /**
-         * Indicates whether the response should contain the stored _source for
-         * every hit
-         */
-        public TopHitsAggregatorBuilder fetchSource(boolean fetch) {
-            if (this.fetchSourceContext == null) {
-                this.fetchSourceContext = new FetchSourceContext(fetch);
-            } else {
-                this.fetchSourceContext.fetchSource(fetch);
-            }
-            return this;
-        }
-
-        /**
-         * Indicate that _source should be returned with every hit, with an
-         * "include" and/or "exclude" set which can include simple wildcard
-         * elements.
-         *
-         * @param include
-         *            An optional include (optionally wildcarded) pattern to
-         *            filter the returned _source
-         * @param exclude
-         *            An optional exclude (optionally wildcarded) pattern to
-         *            filter the returned _source
-         */
-        public TopHitsAggregatorBuilder fetchSource(@Nullable String include, @Nullable String exclude) {
-            fetchSource(include == null ? Strings.EMPTY_ARRAY : new String[] { include },
-                    exclude == null ? Strings.EMPTY_ARRAY : new String[] { exclude });
-            return this;
-        }
-
-        /**
-         * Indicate that _source should be returned with every hit, with an
-         * "include" and/or "exclude" set which can include simple wildcard
-         * elements.
-         *
-         * @param includes
-         *            An optional list of include (optionally wildcarded)
-         *            pattern to filter the returned _source
-         * @param excludes
-         *            An optional list of exclude (optionally wildcarded)
-         *            pattern to filter the returned _source
-         */
-        public TopHitsAggregatorBuilder fetchSource(@Nullable String[] includes, @Nullable String[] excludes) {
-            fetchSourceContext = new FetchSourceContext(includes, excludes);
-            return this;
-        }
-
-        /**
-         * Indicate how the _source should be fetched.
-         */
-        public TopHitsAggregatorBuilder fetchSource(@Nullable FetchSourceContext fetchSourceContext) {
-            this.fetchSourceContext = fetchSourceContext;
-            return this;
-        }
-
-        /**
-         * Gets the {@link FetchSourceContext} which defines how the _source
-         * should be fetched.
-         */
-        public FetchSourceContext fetchSource() {
-            return fetchSourceContext;
-        }
-
-        /**
-         * Adds a field to load and return (note, it must be stored) as part of
-         * the search request. If none are specified, the source of the document
-         * will be return.
-         */
-        public TopHitsAggregatorBuilder field(String name) {
-            if (fieldNames == null) {
-                fieldNames = new ArrayList<>();
-            }
-            fieldNames.add(name);
-            return this;
-        }
-
-        /**
-         * Sets the fields to load and return as part of the search request. If
-         * none are specified, the source of the document will be returned.
-         */
-        public TopHitsAggregatorBuilder fields(List<String> fields) {
-            this.fieldNames = fields;
-            return this;
-        }
-
-        /**
-         * Sets no fields to be loaded, resulting in only id and type to be
-         * returned per field.
-         */
-        public TopHitsAggregatorBuilder noFields() {
-            this.fieldNames = Collections.emptyList();
-            return this;
-        }
-
-        /**
-         * Gets the fields to load and return as part of the search request.
-         */
-        public List<String> fields() {
-            return fieldNames;
-        }
-
-        /**
-         * Adds a field to load from the field data cache and return as part of
-         * the search request.
-         */
-        public TopHitsAggregatorBuilder fieldDataField(String name) {
-            if (fieldDataFields == null) {
-                fieldDataFields = new ArrayList<>();
-            }
-            fieldDataFields.add(name);
-            return this;
-        }
-
-        /**
-         * Adds fields to load from the field data cache and return as part of
-         * the search request.
-         */
-        public TopHitsAggregatorBuilder fieldDataFields(List<String> names) {
-            if (fieldDataFields == null) {
-                fieldDataFields = new ArrayList<>();
-            }
-            fieldDataFields.addAll(names);
-            return this;
-        }
-
-        /**
-         * Gets the field-data fields.
-         */
-        public List<String> fieldDataFields() {
-            return fieldDataFields;
-        }
-
-        /**
-         * Adds a script field under the given name with the provided script.
-         *
-         * @param name
-         *            The name of the field
-         * @param script
-         *            The script
-         */
-        public TopHitsAggregatorBuilder scriptField(String name, Script script) {
-            scriptField(name, script, false);
-            return this;
-        }
+        private final FetchPhase fetchPhase;
+        private final SubSearchContext subSearchContext;
 
-        /**
-         * Adds a script field under the given name with the provided script.
-         *
-         * @param name
-         *            The name of the field
-         * @param script
-         *            The script
-         */
-        public TopHitsAggregatorBuilder scriptField(String name, Script script, boolean ignoreFailure) {
-            if (scriptFields == null) {
-                scriptFields = new ArrayList<>();
-            }
-            scriptFields.add(new ScriptField(name, script, ignoreFailure));
-            return this;
-        }
-
-        public TopHitsAggregatorBuilder scriptFields(List<ScriptField> scriptFields) {
-            if (this.scriptFields == null) {
-                this.scriptFields = new ArrayList<>();
-            }
-            this.scriptFields.addAll(scriptFields);
-            return this;
-        }
-
-        /**
-         * Gets the script fields.
-         */
-        public List<ScriptField> scriptFields() {
-            return scriptFields;
-        }
-
-        /**
-         * Should each {@link org.elasticsearch.search.SearchHit} be returned
-         * with an explanation of the hit (ranking).
-         */
-        public TopHitsAggregatorBuilder explain(boolean explain) {
-            this.explain = explain;
-            return this;
-        }
-
-        /**
-         * Indicates whether each search hit will be returned with an
-         * explanation of the hit (ranking)
-         */
-        public boolean explain() {
-            return explain;
-        }
-
-        /**
-         * Should each {@link org.elasticsearch.search.SearchHit} be returned
-         * with a version associated with it.
-         */
-        public TopHitsAggregatorBuilder version(boolean version) {
-            this.version = version;
-            return this;
-        }
-
-        /**
-         * Indicates whether the document's version will be included in the
-         * search hits.
-         */
-        public boolean version() {
-            return version;
-        }
-
-        /**
-         * Applies when sorting, and controls if scores will be tracked as well.
-         * Defaults to <tt>false</tt>.
-         */
-        public TopHitsAggregatorBuilder trackScores(boolean trackScores) {
-            this.trackScores = trackScores;
-            return this;
-        }
-
-        /**
-         * Indicates whether scores will be tracked for this request.
-         */
-        public boolean trackScores() {
-            return trackScores;
-        }
-        @Override
-        protected AggregatorFactory<?> doBuild(AggregationContext context, AggregatorFactory<?> parent, Builder subfactoriesBuilder)
-                throws IOException {
-            return new TopHitsAggregatorFactory(name, type, from, size, explain, version, trackScores, sorts, highlightBuilder, fieldNames,
-                    fieldDataFields, scriptFields, fetchSourceContext, context, parent, subfactoriesBuilder, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            builder.field(SearchSourceBuilder.FROM_FIELD.getPreferredName(), from);
-            builder.field(SearchSourceBuilder.SIZE_FIELD.getPreferredName(), size);
-            builder.field(SearchSourceBuilder.VERSION_FIELD.getPreferredName(), version);
-            builder.field(SearchSourceBuilder.EXPLAIN_FIELD.getPreferredName(), explain);
-            if (fetchSourceContext != null) {
-                builder.field(SearchSourceBuilder._SOURCE_FIELD.getPreferredName(), fetchSourceContext);
-            }
-            if (fieldNames != null) {
-                if (fieldNames.size() == 1) {
-                    builder.field(SearchSourceBuilder.FIELDS_FIELD.getPreferredName(), fieldNames.get(0));
-                } else {
-                    builder.startArray(SearchSourceBuilder.FIELDS_FIELD.getPreferredName());
-                    for (String fieldName : fieldNames) {
-                        builder.value(fieldName);
-                    }
-                    builder.endArray();
-                }
-            }
-            if (fieldDataFields != null) {
-                builder.startArray(SearchSourceBuilder.FIELDDATA_FIELDS_FIELD.getPreferredName());
-                for (String fieldDataField : fieldDataFields) {
-                    builder.value(fieldDataField);
-                }
-                builder.endArray();
-            }
-            if (scriptFields != null) {
-                builder.startObject(SearchSourceBuilder.SCRIPT_FIELDS_FIELD.getPreferredName());
-                for (ScriptField scriptField : scriptFields) {
-                    scriptField.toXContent(builder, params);
-                }
-                builder.endObject();
-            }
-            if (sorts != null) {
-                builder.startArray(SearchSourceBuilder.SORT_FIELD.getPreferredName());
-                for (BytesReference sort : sorts) {
-                    XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(sort);
-                    parser.nextToken();
-                    builder.copyCurrentStructure(parser);
-                }
-                builder.endArray();
-            }
-            if (trackScores) {
-                builder.field(SearchSourceBuilder.TRACK_SCORES_FIELD.getPreferredName(), true);
-            }
-            if (highlightBuilder != null) {
-                this.highlightBuilder.toXContent(builder, params);
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected TopHitsAggregatorBuilder doReadFrom(String name, StreamInput in) throws IOException {
-            TopHitsAggregatorBuilder factory = new TopHitsAggregatorBuilder(name);
-            factory.explain = in.readBoolean();
-            factory.fetchSourceContext = FetchSourceContext.optionalReadFromStream(in);
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<String> fieldDataFields = new ArrayList<>(size);
-                for (int i = 0; i < size; i++) {
-                    fieldDataFields.add(in.readString());
-                }
-                factory.fieldDataFields = fieldDataFields;
-            }
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<String> fieldNames = new ArrayList<>(size);
-                for (int i = 0; i < size; i++) {
-                    fieldNames.add(in.readString());
-                }
-                factory.fieldNames = fieldNames;
-            }
-            factory.from = in.readVInt();
-            if (in.readBoolean()) {
-                factory.highlightBuilder = HighlightBuilder.PROTOTYPE.readFrom(in);
-            }
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<ScriptField> scriptFields = new ArrayList<>(size);
-                for (int i = 0; i < size; i++) {
-                    scriptFields.add(ScriptField.PROTOTYPE.readFrom(in));
-                }
-                factory.scriptFields = scriptFields;
-            }
-            factory.size = in.readVInt();
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<BytesReference> sorts = new ArrayList<>();
-                for (int i = 0; i < size; i++) {
-                    sorts.add(in.readBytesReference());
-                }
-                factory.sorts = sorts;
-            }
-            factory.trackScores = in.readBoolean();
-            factory.version = in.readBoolean();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeBoolean(explain);
-            FetchSourceContext.optionalWriteToStream(fetchSourceContext, out);
-            boolean hasFieldDataFields = fieldDataFields != null;
-            out.writeBoolean(hasFieldDataFields);
-            if (hasFieldDataFields) {
-                out.writeVInt(fieldDataFields.size());
-                for (String fieldName : fieldDataFields) {
-                    out.writeString(fieldName);
-                }
-            }
-            boolean hasFieldNames = fieldNames != null;
-            out.writeBoolean(hasFieldNames);
-            if (hasFieldNames) {
-                out.writeVInt(fieldNames.size());
-                for (String fieldName : fieldNames) {
-                    out.writeString(fieldName);
-                }
-            }
-            out.writeVInt(from);
-            boolean hasHighlighter = highlightBuilder != null;
-            out.writeBoolean(hasHighlighter);
-            if (hasHighlighter) {
-                highlightBuilder.writeTo(out);
-            }
-            boolean hasScriptFields = scriptFields != null;
-            out.writeBoolean(hasScriptFields);
-            if (hasScriptFields) {
-                out.writeVInt(scriptFields.size());
-                for (ScriptField scriptField : scriptFields) {
-                    scriptField.writeTo(out);
-                }
-            }
-            out.writeVInt(size);
-            boolean hasSorts = sorts != null;
-            out.writeBoolean(hasSorts);
-            if (hasSorts) {
-                out.writeVInt(sorts.size());
-                for (BytesReference sort : sorts) {
-                    out.writeBytesReference(sort);
-                }
-            }
-            out.writeBoolean(trackScores);
-            out.writeBoolean(version);
+        public Factory(String name, FetchPhase fetchPhase, SubSearchContext subSearchContext) {
+            super(name, InternalTopHits.TYPE.name());
+            this.fetchPhase = fetchPhase;
+            this.subSearchContext = subSearchContext;
         }
 
         @Override
-        protected int doHashCode() {
-            return Objects.hash(explain, fetchSourceContext, fieldDataFields, fieldNames, from, highlightBuilder, scriptFields, size, sorts,
-                    trackScores, version);
+        public Aggregator createInternal(AggregationContext aggregationContext, Aggregator parent, boolean collectsFromSingleBucket,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new TopHitsAggregator(fetchPhase, subSearchContext, name, aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected boolean doEquals(Object obj) {
-            TopHitsAggregatorBuilder other = (TopHitsAggregatorBuilder) obj;
-            return Objects.equals(explain, other.explain)
-                    && Objects.equals(fetchSourceContext, other.fetchSourceContext)
-                    && Objects.equals(fieldDataFields, other.fieldDataFields)
-                    && Objects.equals(fieldNames, other.fieldNames)
-                    && Objects.equals(from, other.from)
-                    && Objects.equals(highlightBuilder, other.highlightBuilder)
-                    && Objects.equals(scriptFields, other.scriptFields)
-                    && Objects.equals(size, other.size)
-                    && Objects.equals(sorts, other.sorts)
-                    && Objects.equals(trackScores, other.trackScores)
-                    && Objects.equals(version, other.version);
+        public AggregatorFactory subFactories(AggregatorFactories subFactories) {
+            throw new AggregationInitializationException("Aggregator [" + name + "] of type [" + type + "] cannot accept sub-aggregations");
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregatorFactory.java
deleted file mode 100644
index 565b1b7..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregatorFactory.java
+++ /dev/null
@@ -1,145 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.tophits;
-
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentLocation;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.SearchScript;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.builder.SearchSourceBuilder.ScriptField;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsContext;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsFetchSubPhase;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsContext.FieldDataField;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
-import org.elasticsearch.search.highlight.HighlightBuilder;
-import org.elasticsearch.search.internal.SubSearchContext;
-import org.elasticsearch.search.sort.SortParseElement;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-
-public class TopHitsAggregatorFactory extends AggregatorFactory<TopHitsAggregatorFactory> {
-
-    private static final SortParseElement sortParseElement = new SortParseElement();
-    private final int from;
-    private final int size;
-    private final boolean explain;
-    private final boolean version;
-    private final boolean trackScores;
-    private final List<BytesReference> sorts;
-    private final HighlightBuilder highlightBuilder;
-    private final List<String> fieldNames;
-    private final List<String> fieldDataFields;
-    private final List<ScriptField> scriptFields;
-    private final FetchSourceContext fetchSourceContext;
-
-    public TopHitsAggregatorFactory(String name, Type type, int from, int size, boolean explain, boolean version, boolean trackScores,
-            List<BytesReference> sorts, HighlightBuilder highlightBuilder, List<String> fieldNames, List<String> fieldDataFields,
-            List<ScriptField> scriptFields, FetchSourceContext fetchSourceContext, AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactories, Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactories, metaData);
-        this.from = from;
-        this.size = size;
-        this.explain = explain;
-        this.version = version;
-        this.trackScores = trackScores;
-        this.sorts = sorts;
-        this.highlightBuilder = highlightBuilder;
-        this.fieldNames = fieldNames;
-        this.fieldDataFields = fieldDataFields;
-        this.scriptFields = scriptFields;
-        this.fetchSourceContext = fetchSourceContext;
-    }
-
-    @Override
-    public Aggregator createInternal(AggregationContext aggregationContext, Aggregator parent, boolean collectsFromSingleBucket,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        SubSearchContext subSearchContext = new SubSearchContext(aggregationContext.searchContext());
-        subSearchContext.explain(explain);
-        subSearchContext.version(version);
-        subSearchContext.trackScores(trackScores);
-        subSearchContext.from(from);
-        subSearchContext.size(size);
-        if (sorts != null) {
-            XContentParser completeSortParser = null;
-            try {
-                XContentBuilder completeSortBuilder = XContentFactory.jsonBuilder();
-                completeSortBuilder.startObject();
-                completeSortBuilder.startArray("sort");
-                for (BytesReference sort : sorts) {
-                    XContentParser parser = XContentFactory.xContent(sort).createParser(sort);
-                    parser.nextToken();
-                    completeSortBuilder.copyCurrentStructure(parser);
-                }
-                completeSortBuilder.endArray();
-                completeSortBuilder.endObject();
-                BytesReference completeSortBytes = completeSortBuilder.bytes();
-                completeSortParser = XContentFactory.xContent(completeSortBytes).createParser(completeSortBytes);
-                completeSortParser.nextToken();
-                completeSortParser.nextToken();
-                completeSortParser.nextToken();
-                sortParseElement.parse(completeSortParser, subSearchContext);
-            } catch (Exception e) {
-                XContentLocation location = completeSortParser != null ? completeSortParser.getTokenLocation() : null;
-                throw new ParsingException(location, "failed to parse sort source in aggregation [" + name + "]", e);
-            }
-        }
-        if (fieldNames != null) {
-            subSearchContext.fieldNames().addAll(fieldNames);
-        }
-        if (fieldDataFields != null) {
-            FieldDataFieldsContext fieldDataFieldsContext = subSearchContext
-                    .getFetchSubPhaseContext(FieldDataFieldsFetchSubPhase.CONTEXT_FACTORY);
-            for (String field : fieldDataFields) {
-                fieldDataFieldsContext.add(new FieldDataField(field));
-            }
-            fieldDataFieldsContext.setHitExecutionNeeded(true);
-        }
-        if (scriptFields != null) {
-            for (ScriptField field : scriptFields) {
-                SearchScript searchScript = subSearchContext.scriptService().search(subSearchContext.lookup(), field.script(),
-                        ScriptContext.Standard.SEARCH, Collections.emptyMap());
-                subSearchContext.scriptFields().add(new org.elasticsearch.search.fetch.script.ScriptFieldsContext.ScriptField(
-                        field.fieldName(), searchScript, field.ignoreFailure()));
-            }
-        }
-        if (fetchSourceContext != null) {
-            subSearchContext.fetchSourceContext(fetchSourceContext);
-        }
-        if (highlightBuilder != null) {
-            subSearchContext.highlight(highlightBuilder.build(aggregationContext.searchContext().indexShard().getQueryShardContext()));
-        }
-        return new TopHitsAggregator(aggregationContext.searchContext().fetchPhase(), subSearchContext, name, aggregationContext, parent,
-                pipelineAggregators, metaData);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
new file mode 100644
index 0000000..1efd4a7
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
@@ -0,0 +1,204 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.metrics.tophits;
+
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.search.highlight.HighlightBuilder;
+import org.elasticsearch.search.sort.SortBuilder;
+import org.elasticsearch.search.sort.SortOrder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link TopHits} aggregation.
+ */
+public class TopHitsBuilder extends AbstractAggregationBuilder {
+
+    private SearchSourceBuilder sourceBuilder;
+
+    /**
+     * Sole constructor.
+     */
+    public TopHitsBuilder(String name) {
+        super(name, InternalTopHits.TYPE.name());
+    }
+
+    /**
+     * The index to start to return hits from. Defaults to <tt>0</tt>.
+     */
+    public TopHitsBuilder setFrom(int from) {
+        sourceBuilder().from(from);
+        return this;
+    }
+
+
+    /**
+     * The number of search hits to return. Defaults to <tt>10</tt>.
+     */
+    public TopHitsBuilder setSize(int size) {
+        sourceBuilder().size(size);
+        return this;
+    }
+
+    /**
+     * Applies when sorting, and controls if scores will be tracked as well. Defaults to
+     * <tt>false</tt>.
+     */
+    public TopHitsBuilder setTrackScores(boolean trackScores) {
+        sourceBuilder().trackScores(trackScores);
+        return this;
+    }
+
+    /**
+     * Should each {@link org.elasticsearch.search.SearchHit} be returned with an
+     * explanation of the hit (ranking).
+     */
+    public TopHitsBuilder setExplain(boolean explain) {
+        sourceBuilder().explain(explain);
+        return this;
+    }
+
+    /**
+     * Should each {@link org.elasticsearch.search.SearchHit} be returned with its
+     * version.
+     */
+    public TopHitsBuilder setVersion(boolean version) {
+        sourceBuilder().version(version);
+        return this;
+    }
+
+    /**
+     * Adds a field to loaded and returned.
+     */
+    public TopHitsBuilder addField(String field) {
+        sourceBuilder().field(field);
+        return this;
+    }
+
+    /**
+     * Sets no fields to be loaded, resulting in only id and type to be returned per field.
+     */
+    public TopHitsBuilder setNoFields() {
+        sourceBuilder().noFields();
+        return this;
+    }
+
+    /**
+     * Indicates whether the response should contain the stored _source for every hit
+     */
+    public TopHitsBuilder setFetchSource(boolean fetch) {
+        sourceBuilder().fetchSource(fetch);
+        return this;
+    }
+
+    /**
+     * Indicate that _source should be returned with every hit, with an "include" and/or "exclude" set which can include simple wildcard
+     * elements.
+     *
+     * @param include An optional include (optionally wildcarded) pattern to filter the returned _source
+     * @param exclude An optional exclude (optionally wildcarded) pattern to filter the returned _source
+     */
+    public TopHitsBuilder setFetchSource(@Nullable String include, @Nullable String exclude) {
+        sourceBuilder().fetchSource(include, exclude);
+        return this;
+    }
+
+    /**
+     * Indicate that _source should be returned with every hit, with an "include" and/or "exclude" set which can include simple wildcard
+     * elements.
+     *
+     * @param includes An optional list of include (optionally wildcarded) pattern to filter the returned _source
+     * @param excludes An optional list of exclude (optionally wildcarded) pattern to filter the returned _source
+     */
+    public TopHitsBuilder setFetchSource(@Nullable String[] includes, @Nullable String[] excludes) {
+        sourceBuilder().fetchSource(includes, excludes);
+        return this;
+    }
+
+    /**
+     * Adds a field data based field to load and return. The field does not have to be stored,
+     * but its recommended to use non analyzed or numeric fields.
+     *
+     * @param name The field to get from the field data cache
+     */
+    public TopHitsBuilder addFieldDataField(String name) {
+        sourceBuilder().fieldDataField(name);
+        return this;
+    }
+
+    /**
+     * Adds a script based field to load and return. The field does not have to be stored,
+     * but its recommended to use non analyzed or numeric fields.
+     *
+     * @param name   The name that will represent this value in the return hit
+     * @param script The script to use
+     */
+    public TopHitsBuilder addScriptField(String name, Script script) {
+        sourceBuilder().scriptField(name, script);
+        return this;
+    }
+
+    /**
+     * Adds a sort against the given field name and the sort ordering.
+     *
+     * @param field The name of the field
+     * @param order The sort ordering
+     */
+    public TopHitsBuilder addSort(String field, SortOrder order) {
+        sourceBuilder().sort(field, order);
+        return this;
+    }
+
+    /**
+     * Adds a generic sort builder.
+     *
+     * @see org.elasticsearch.search.sort.SortBuilders
+     */
+    public TopHitsBuilder addSort(SortBuilder sort) {
+        sourceBuilder().sort(sort);
+        return this;
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(getName()).field(type);
+        sourceBuilder().toXContent(builder, params);
+        return builder.endObject();
+    }
+
+    private SearchSourceBuilder sourceBuilder() {
+        if (sourceBuilder == null) {
+            sourceBuilder = new SearchSourceBuilder();
+        }
+        return sourceBuilder;
+    }
+
+    public HighlightBuilder highlighter() {
+        return sourceBuilder().highlighter();
+    }
+
+    public TopHitsBuilder highlighter(HighlightBuilder highlightBuilder) {
+        sourceBuilder().highlighter(highlightBuilder);
+        return this;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java
index f8e0095..50b3482 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java
@@ -18,36 +18,30 @@
  */
 package org.elasticsearch.search.aggregations.metrics.tophits;
 
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.script.Script;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.builder.SearchSourceBuilder.ScriptField;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FieldsParseElement;
 import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
 import org.elasticsearch.search.fetch.script.ScriptFieldsParseElement;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
 import org.elasticsearch.search.fetch.source.FetchSourceParseElement;
-import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.highlight.HighlighterParseElement;
+import org.elasticsearch.search.internal.SearchContext;
+import org.elasticsearch.search.internal.SubSearchContext;
 import org.elasticsearch.search.sort.SortParseElement;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
 
 /**
  *
  */
 public class TopHitsParser implements Aggregator.Parser {
 
+    private final FetchPhase fetchPhase;
     private final SortParseElement sortParseElement;
     private final FetchSourceParseElement sourceParseElement;
     private final HighlighterParseElement highlighterParseElement;
@@ -56,9 +50,10 @@ public class TopHitsParser implements Aggregator.Parser {
     private final FieldsParseElement fieldsParseElement;
 
     @Inject
-    public TopHitsParser(SortParseElement sortParseElement, FetchSourceParseElement sourceParseElement,
+    public TopHitsParser(FetchPhase fetchPhase, SortParseElement sortParseElement, FetchSourceParseElement sourceParseElement,
             HighlighterParseElement highlighterParseElement, FieldDataFieldsParseElement fieldDataFieldsParseElement,
             ScriptFieldsParseElement scriptFieldsParseElement, FieldsParseElement fieldsParseElement) {
+        this.fetchPhase = fetchPhase;
         this.sortParseElement = sortParseElement;
         this.sourceParseElement = sourceParseElement;
         this.highlighterParseElement = highlighterParseElement;
@@ -73,141 +68,74 @@ public class TopHitsParser implements Aggregator.Parser {
     }
 
     @Override
-    public TopHitsAggregator.TopHitsAggregatorBuilder parse(String aggregationName, XContentParser parser, QueryParseContext context)
-            throws IOException {
-        TopHitsAggregator.TopHitsAggregatorBuilder factory = new TopHitsAggregator.TopHitsAggregatorBuilder(aggregationName);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        SubSearchContext subSearchContext = new SubSearchContext(context);
         XContentParser.Token token;
         String currentFieldName = null;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token.isValue()) {
-                if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.FROM_FIELD)) {
-                    factory.from(parser.intValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SIZE_FIELD)) {
-                    factory.size(parser.intValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.VERSION_FIELD)) {
-                    factory.version(parser.booleanValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.EXPLAIN_FIELD)) {
-                    factory.explain(parser.booleanValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.TRACK_SCORES_FIELD)) {
-                    factory.trackScores(parser.booleanValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder._SOURCE_FIELD)) {
-                    factory.fetchSource(FetchSourceContext.parse(parser, context));
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.FIELDS_FIELD)) {
-                    List<String> fieldNames = new ArrayList<>();
-                    fieldNames.add(parser.text());
-                    factory.fields(fieldNames);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SORT_FIELD)) {
-                    factory.sort(parser.text());
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else if (token == XContentParser.Token.START_OBJECT) {
-                if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder._SOURCE_FIELD)) {
-                    factory.fetchSource(FetchSourceContext.parse(parser, context));
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SCRIPT_FIELDS_FIELD)) {
-                    List<ScriptField> scriptFields = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        String scriptFieldName = parser.currentName();
-                        token = parser.nextToken();
-                        if (token == XContentParser.Token.START_OBJECT) {
-                            Script script = null;
-                            boolean ignoreFailure = false;
-                            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                                if (token == XContentParser.Token.FIELD_NAME) {
-                                    currentFieldName = parser.currentName();
-                                } else if (token.isValue()) {
-                                    if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SCRIPT_FIELD)) {
-                                        script = Script.parse(parser, context.parseFieldMatcher());
-                                    } else if (context.parseFieldMatcher().match(currentFieldName,
-                                            SearchSourceBuilder.IGNORE_FAILURE_FIELD)) {
-                                        ignoreFailure = parser.booleanValue();
-                                    } else {
-                                        throw new ParsingException(parser.getTokenLocation(),
-                                                "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                                parser.getTokenLocation());
-                                    }
-                                } else if (token == XContentParser.Token.START_OBJECT) {
-                                    if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SCRIPT_FIELD)) {
-                                        script = Script.parse(parser, context.parseFieldMatcher());
-                                    } else {
-                                        throw new ParsingException(parser.getTokenLocation(),
-                                                "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                                parser.getTokenLocation());
-                                    }
-                                } else {
-                                    throw new ParsingException(parser.getTokenLocation(),
-                                            "Unknown key for a " + token + " in [" + currentFieldName + "].", parser.getTokenLocation());
-                                }
-                            }
-                            scriptFields.add(new ScriptField(scriptFieldName, script, ignoreFailure));
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.START_OBJECT
-                                    + "] in [" + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
+        try {
+            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                if (token == XContentParser.Token.FIELD_NAME) {
+                    currentFieldName = parser.currentName();
+                } else if ("sort".equals(currentFieldName)) {
+                    sortParseElement.parse(parser, subSearchContext);
+                } else if ("_source".equals(currentFieldName)) {
+                    sourceParseElement.parse(parser, subSearchContext);
+                } else if ("fields".equals(currentFieldName)) {
+                    fieldsParseElement.parse(parser, subSearchContext);
+                } else if (token.isValue()) {
+                    switch (currentFieldName) {
+                        case "from":
+                            subSearchContext.from(parser.intValue());
+                            break;
+                        case "size":
+                            subSearchContext.size(parser.intValue());
+                            break;
+                        case "track_scores":
+                        case "trackScores":
+                            subSearchContext.trackScores(parser.booleanValue());
+                            break;
+                        case "version":
+                            subSearchContext.version(parser.booleanValue());
+                            break;
+                        case "explain":
+                            subSearchContext.explain(parser.booleanValue());
+                            break;
+                        default:
+                        throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                + currentFieldName + "].", parser.getTokenLocation());
                     }
-                    factory.scriptFields(scriptFields);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.HIGHLIGHT_FIELD)) {
-                    factory.highlighter(HighlightBuilder.PROTOTYPE.fromXContent(context));
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SORT_FIELD)) {
-                    List<BytesReference> sorts = new ArrayList<>();
-                    XContentBuilder xContentBuilder = XContentFactory.jsonBuilder().copyCurrentStructure(parser);
-                    sorts.add(xContentBuilder.bytes());
-                    factory.sorts(sorts);
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else if (token == XContentParser.Token.START_ARRAY) {
-
-                if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.FIELDS_FIELD)) {
-                    List<String> fieldNames = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            fieldNames.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING
-                                    + "] in [" + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
-                    }
-                    factory.fields(fieldNames);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.FIELDDATA_FIELDS_FIELD)) {
-                    List<String> fieldDataFields = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            fieldDataFields.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING
-                                    + "] in [" + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
+                } else if (token == XContentParser.Token.START_OBJECT) {
+                    switch (currentFieldName) {
+                        case "highlight":
+                            highlighterParseElement.parse(parser, subSearchContext);
+                            break;
+                        case "scriptFields":
+                        case "script_fields":
+                            scriptFieldsParseElement.parse(parser, subSearchContext);
+                            break;
+                        default:
+                        throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                + currentFieldName + "].", parser.getTokenLocation());
                     }
-                    factory.fieldDataFields(fieldDataFields);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SORT_FIELD)) {
-                    List<BytesReference> sorts = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        XContentBuilder xContentBuilder = XContentFactory.jsonBuilder().copyCurrentStructure(parser);
-                        sorts.add(xContentBuilder.bytes());
+                } else if (token == XContentParser.Token.START_ARRAY) {
+                    switch (currentFieldName) {
+                        case "fielddataFields":
+                        case "fielddata_fields":
+                            fieldDataFieldsParseElement.parse(parser, subSearchContext);
+                            break;
+                        default:
+                        throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                + currentFieldName + "].", parser.getTokenLocation());
                     }
-                    factory.sorts(sorts);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder._SOURCE_FIELD)) {
-                    factory.fetchSource(FetchSourceContext.parse(parser, context));
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
+                    throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
                             parser.getTokenLocation());
                 }
-            } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                        parser.getTokenLocation());
             }
+        } catch (Exception e) {
+            throw ExceptionsHelper.convertToElastic(e);
         }
-        return factory;
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new TopHitsAggregator.TopHitsAggregatorBuilder(null);
+        return new TopHitsAggregator.Factory(aggregationName, fetchPhase, subSearchContext);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java
index 16c38e2..2d1d9db 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java
@@ -19,29 +19,20 @@
 package org.elasticsearch.search.aggregations.metrics.valuecount;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedBinaryDocValues;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -117,43 +108,24 @@ public class ValueCountAggregator extends NumericMetricsAggregator.SingleValue {
         Releasables.close(counts);
     }
 
-    public static class ValueCountAggregatorBuilder
-            extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource, ValueCountAggregatorBuilder> {
+    public static class Factory<VS extends ValuesSource> extends ValuesSourceAggregatorFactory.LeafOnly<VS> {
 
-        public ValueCountAggregatorBuilder(String name, ValueType targetValueType) {
-            super(name, InternalValueCount.TYPE, ValuesSourceType.ANY, targetValueType);
+        public Factory(String name, ValuesSourceConfig<VS> config) {
+            super(name, InternalValueCount.TYPE.name(), config);
         }
 
         @Override
-        protected ValueCountAggregatorFactory innerBuild(AggregationContext context, ValuesSourceConfig<ValuesSource> config,
-                AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder) throws IOException {
-            return new ValueCountAggregatorFactory(name, type, config, context, parent, subFactoriesBuilder, metaData);
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new ValueCountAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        protected ValuesSourceAggregatorBuilder<ValuesSource, ValueCountAggregatorBuilder> innerReadFrom(String name,
-                ValuesSourceType valuesSourceType, ValueType targetValueType, StreamInput in) {
-            return new ValueCountAggregator.ValueCountAggregatorBuilder(name, targetValueType);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
+        protected Aggregator doCreateInternal(VS valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new ValueCountAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators,
+                    metaData);
         }
 
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregatorFactory.java
deleted file mode 100644
index ee65dea..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregatorFactory.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.valuecount;
-
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-public class ValueCountAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource, ValueCountAggregatorFactory> {
-
-    public ValueCountAggregatorFactory(String name, Type type, ValuesSourceConfig<ValuesSource> config, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, config, context, parent, subFactoriesBuilder, metaData);
-    }
-
-    @Override
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        return new ValueCountAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
-    }
-
-    @Override
-    protected Aggregator doCreateInternal(ValuesSource valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        return new ValueCountAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountBuilder.java
new file mode 100644
index 0000000..1d37914
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountBuilder.java
@@ -0,0 +1,36 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.valuecount;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link ValueCount} aggregation.
+ */
+public class ValueCountBuilder extends ValuesSourceMetricsAggregationBuilder<ValueCountBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public ValueCountBuilder(String name) {
+        super(name, InternalValueCount.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java
index 043958f..764f6ce 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java
@@ -18,27 +18,19 @@
  */
 package org.elasticsearch.search.aggregations.metrics.valuecount;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  *
  */
-public class ValueCountParser extends AnyValuesSourceParser {
-
-    public ValueCountParser() {
-        super(true, true);
-    }
+public class ValueCountParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -46,19 +38,22 @@ public class ValueCountParser extends AnyValuesSourceParser {
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected ValuesSourceAggregatorBuilder<ValuesSource, ValueCountAggregator.ValueCountAggregatorBuilder> createFactory(
-            String aggregationName, ValuesSourceType valuesSourceType, ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new ValueCountAggregator.ValueCountAggregatorBuilder(aggregationName, targetValueType);
-    }
-
-    @Override
-    public AggregatorBuilder<?> getFactoryPrototypes() {
-        return new ValueCountAggregator.ValueCountAggregatorBuilder(null, null);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, InternalValueCount.TYPE, context)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (!vsParser.token(currentFieldName, token, parser)) {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+        }
+
+        return new ValueCountAggregator.Factory(aggregationName, vsParser.config());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java
index 24d2913..881a8e4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java
@@ -20,17 +20,17 @@
 package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentLocation;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;
 import org.elasticsearch.search.aggregations.InvalidAggregationPathException;
 import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;
 import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativeParser;
 import org.elasticsearch.search.aggregations.support.AggregationPath;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -62,7 +62,7 @@ public class BucketHelpers {
          * @param text    GapPolicy in string format (e.g. "ignore")
          * @return        GapPolicy enum
          */
-        public static GapPolicy parse(QueryParseContext context, String text, XContentLocation tokenLocation) {
+        public static GapPolicy parse(SearchContext context, String text, XContentLocation tokenLocation) {
             GapPolicy result = null;
             for (GapPolicy policy : values()) {
                 if (context.parseFieldMatcher().match(text, policy.parseField)) {
@@ -79,7 +79,7 @@ public class BucketHelpers {
                 for (GapPolicy policy : values()) {
                     validNames.add(policy.getName());
                 }
-                throw new ParsingException(tokenLocation, "Invalid gap policy: [" + text + "], accepted values: " + validNames);
+                throw new SearchParseException(context, "Invalid gap policy: [" + text + "], accepted values: " + validNames, tokenLocation);
             }
             return result;
         }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java
index a3c1805..b2ee037 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java
@@ -25,10 +25,10 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.Map;
@@ -38,7 +38,7 @@ public abstract class PipelineAggregator implements Streamable {
     /**
      * Parses the pipeline aggregation request and creates the appropriate
      * pipeline aggregator factory for it.
-     *
+     * 
      * @see PipelineAggregatorFactory
      */
     public static interface Parser {
@@ -56,7 +56,7 @@ public abstract class PipelineAggregator implements Streamable {
         /**
          * Returns the pipeline aggregator factory with which this parser is
          * associated.
-         *
+         * 
          * @param pipelineAggregatorName
          *            The name of the pipeline aggregation
          * @param parser
@@ -67,13 +67,7 @@ public abstract class PipelineAggregator implements Streamable {
          * @throws java.io.IOException
          *             When parsing fails
          */
-        PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context) throws IOException;
-
-        /**
-         * @return an empty {@link PipelineAggregatorFactory} instance for this
-         *         parser that can be used for deserialization
-         */
-        PipelineAggregatorFactory getFactoryPrototype();
+        PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException;
 
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilder.java
new file mode 100644
index 0000000..56ae321
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilder.java
@@ -0,0 +1,84 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * A base class for all pipeline aggregator builders.
+ */
+public abstract class PipelineAggregatorBuilder<B extends PipelineAggregatorBuilder<B>> extends AbstractAggregationBuilder {
+
+    private String[] bucketsPaths;
+    private Map<String, Object> metaData;
+
+    /**
+     * Sole constructor, typically used by sub-classes.
+     */
+    protected PipelineAggregatorBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    /**
+     * Sets the paths to the buckets to use for this pipeline aggregator
+     */
+    public B setBucketsPaths(String... bucketsPaths) {
+        this.bucketsPaths = bucketsPaths;
+        return (B) this;
+    }
+
+    /**
+     * Sets the meta data to be included in the pipeline aggregator's response
+     */
+    public B setMetaData(Map<String, Object> metaData) {
+        this.metaData = metaData;
+        return (B)this;
+    }
+
+    @Override
+    public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(getName());
+
+        if (this.metaData != null) {
+            builder.field("meta", this.metaData);
+        }
+        builder.startObject(type);
+
+        if (bucketsPaths != null) {
+            builder.startArray(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName());
+            for (String path : bucketsPaths) {
+                builder.value(path);
+            }
+            builder.endArray();
+        }
+
+        internalXContent(builder, params);
+
+        builder.endObject();
+
+        return builder.endObject();
+    }
+
+    protected abstract XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException;
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilders.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilders.java
index adaba99..6fbc6f8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilders.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorBuilders.java
@@ -19,87 +19,74 @@
 
 package org.elasticsearch.search.aggregations.pipeline;
 
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg.AvgBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max.MaxBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.StatsBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended.ExtendedStatsBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum.SumBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.cumulativesum.CumulativeSumPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativePipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.having.BucketSelectorPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.movavg.MovAvgPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.serialdiff.SerialDiffPipelineAggregator;
-
-import java.util.Map;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg.AvgBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max.MaxBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.StatsBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended.ExtendedStatsBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum.SumBucketBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptBuilder;
+import org.elasticsearch.search.aggregations.pipeline.cumulativesum.CumulativeSumBuilder;
+import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativeBuilder;
+import org.elasticsearch.search.aggregations.pipeline.having.BucketSelectorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.movavg.MovAvgBuilder;
+import org.elasticsearch.search.aggregations.pipeline.serialdiff.SerialDiffBuilder;
 
 public final class PipelineAggregatorBuilders {
 
     private PipelineAggregatorBuilders() {
     }
 
-    public static final DerivativePipelineAggregator.Factory derivative(String name, String bucketsPath) {
-        return new DerivativePipelineAggregator.Factory(name, bucketsPath);
+    public static final DerivativeBuilder derivative(String name) {
+        return new DerivativeBuilder(name);
     }
 
-    public static final MaxBucketPipelineAggregator.Factory maxBucket(String name, String bucketsPath) {
-        return new MaxBucketPipelineAggregator.Factory(name, bucketsPath);
+    public static final MaxBucketBuilder maxBucket(String name) {
+        return new MaxBucketBuilder(name);
     }
 
-    public static final MinBucketPipelineAggregator.Factory minBucket(String name, String bucketsPath) {
-        return new MinBucketPipelineAggregator.Factory(name, bucketsPath);
+    public static final MinBucketBuilder minBucket(String name) {
+        return new MinBucketBuilder(name);
     }
 
-    public static final AvgBucketPipelineAggregator.Factory avgBucket(String name, String bucketsPath) {
-        return new AvgBucketPipelineAggregator.Factory(name, bucketsPath);
+    public static final AvgBucketBuilder avgBucket(String name) {
+        return new AvgBucketBuilder(name);
     }
 
-    public static final SumBucketPipelineAggregator.Factory sumBucket(String name, String bucketsPath) {
-        return new SumBucketPipelineAggregator.Factory(name, bucketsPath);
+    public static final SumBucketBuilder sumBucket(String name) {
+        return new SumBucketBuilder(name);
     }
 
-    public static final StatsBucketPipelineAggregator.Factory statsBucket(String name, String bucketsPath) {
-        return new StatsBucketPipelineAggregator.Factory(name, bucketsPath);
+    public static final StatsBucketBuilder statsBucket(String name) {
+        return new StatsBucketBuilder(name);
     }
 
-    public static final ExtendedStatsBucketPipelineAggregator.Factory extendedStatsBucket(String name, String bucketsPath) {
-        return new ExtendedStatsBucketPipelineAggregator.Factory(name, bucketsPath);
+    public static final ExtendedStatsBucketBuilder extendedStatsBucket(String name) {
+        return new ExtendedStatsBucketBuilder(name);
     }
 
-    public static final PercentilesBucketPipelineAggregator.Factory percentilesBucket(String name, String bucketsPath) {
-        return new PercentilesBucketPipelineAggregator.Factory(name, bucketsPath);
+    public static final PercentilesBucketBuilder percentilesBucket(String name) {
+        return new PercentilesBucketBuilder(name);
     }
 
-    public static final MovAvgPipelineAggregator.Factory movingAvg(String name, String bucketsPath) {
-        return new MovAvgPipelineAggregator.Factory(name, bucketsPath);
+    public static final MovAvgBuilder movingAvg(String name) {
+        return new MovAvgBuilder(name);
     }
 
-    public static final BucketScriptPipelineAggregator.Factory bucketScript(String name, Map<String, String> bucketsPathsMap,
-            Script script) {
-        return new BucketScriptPipelineAggregator.Factory(name, bucketsPathsMap, script);
+    public static final BucketScriptBuilder bucketScript(String name) {
+        return new BucketScriptBuilder(name);
     }
 
-    public static final BucketScriptPipelineAggregator.Factory bucketScript(String name, Script script, String... bucketsPaths) {
-        return new BucketScriptPipelineAggregator.Factory(name, script, bucketsPaths);
+    public static final BucketSelectorBuilder having(String name) {
+        return new BucketSelectorBuilder(name);
     }
 
-    public static final BucketSelectorPipelineAggregator.Factory bucketSelector(String name, Map<String, String> bucketsPathsMap,
-            Script script) {
-        return new BucketSelectorPipelineAggregator.Factory(name, bucketsPathsMap, script);
+    public static final CumulativeSumBuilder cumulativeSum(String name) {
+        return new CumulativeSumBuilder(name);
     }
 
-    public static final BucketSelectorPipelineAggregator.Factory bucketSelector(String name, Script script, String... bucketsPaths) {
-        return new BucketSelectorPipelineAggregator.Factory(name, script, bucketsPaths);
-    }
-
-    public static final CumulativeSumPipelineAggregator.Factory cumulativeSum(String name, String bucketsPath) {
-        return new CumulativeSumPipelineAggregator.Factory(name, bucketsPath);
-    }
-
-    public static final SerialDiffPipelineAggregator.Factory diff(String name, String bucketsPath) {
-        return new SerialDiffPipelineAggregator.Factory(name, bucketsPath);
+    public static final SerialDiffBuilder diff(String name) {
+        return new SerialDiffBuilder(name);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java
index eddcb55..6fc0185 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java
@@ -18,25 +18,17 @@
  */
 package org.elasticsearch.search.aggregations.pipeline;
 
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.NamedWriteable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 
 import java.io.IOException;
-import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * A factory that knows how to create an {@link PipelineAggregator} of a
  * specific type.
  */
-public abstract class PipelineAggregatorFactory extends ToXContentToBytes implements NamedWriteable<PipelineAggregatorFactory>, ToXContent {
+public abstract class PipelineAggregatorFactory {
 
     protected String name;
     protected String type;
@@ -61,15 +53,11 @@ public abstract class PipelineAggregatorFactory extends ToXContentToBytes implem
         return name;
     }
 
-    public String type() {
-        return type;
-    }
-
     /**
      * Validates the state of this factory (makes sure the factory is properly
      * configured)
      */
-    public final void validate(AggregatorFactory<?> parent, AggregatorFactory<?>[] factories,
+    public final void validate(AggregatorFactory parent, AggregatorFactory[] factories,
             List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
         doValidate(parent, factories, pipelineAggregatorFactories);
     }
@@ -86,7 +74,7 @@ public abstract class PipelineAggregatorFactory extends ToXContentToBytes implem
         return aggregator;
     }
 
-    public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] factories,
+    public void doValidate(AggregatorFactory parent, AggregatorFactory[] factories,
             List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
     }
 
@@ -102,91 +90,4 @@ public abstract class PipelineAggregatorFactory extends ToXContentToBytes implem
         return bucketsPaths;
     }
 
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(name);
-        out.writeStringArray(bucketsPaths);
-        doWriteTo(out);
-        out.writeMap(metaData);
-    }
-
-    protected abstract void doWriteTo(StreamOutput out) throws IOException;
-
-    @Override
-    public String getWriteableName() {
-        return type;
-    }
-
-    @Override
-    public PipelineAggregatorFactory readFrom(StreamInput in) throws IOException {
-        String name = in.readString();
-        String[] bucketsPaths = in.readStringArray();
-        PipelineAggregatorFactory factory = doReadFrom(name, bucketsPaths, in);
-        factory.metaData = in.readMap();
-        return factory;
-    }
-
-    protected abstract PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException;
-
-    @Override
-    public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(getName());
-
-        if (this.metaData != null) {
-            builder.field("meta", this.metaData);
-        }
-        builder.startObject(type);
-
-        if (!overrideBucketsPath() && bucketsPaths != null) {
-            builder.startArray(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName());
-            for (String path : bucketsPaths) {
-                builder.value(path);
-            }
-            builder.endArray();
-        }
-
-        internalXContent(builder, params);
-
-        builder.endObject();
-
-        return builder.endObject();
-    }
-
-    /**
-     * @return <code>true</code> if the {@link PipelineAggregatorFactory}
-     *         overrides the XContent rendering of the bucketPath option.
-     */
-    protected boolean overrideBucketsPath() {
-        return false;
-    }
-
-    protected abstract XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException;
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(Arrays.hashCode(bucketsPaths), metaData, name, type, doHashCode());
-    }
-
-    protected abstract int doHashCode();
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null)
-            return false;
-        if (getClass() != obj.getClass())
-            return false;
-        PipelineAggregatorFactory other = (PipelineAggregatorFactory) obj;
-        if (!Objects.equals(name, other.name))
-            return false;
-        if (!Objects.equals(type, other.type))
-            return false;
-        if (!Objects.deepEquals(bucketsPaths, other.bucketsPaths))
-            return false;
-        if (!Objects.equals(metaData, other.metaData))
-            return false;
-        return doEquals(obj);
-    }
-
-    protected abstract boolean doEquals(Object obj);
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsBuilder.java
new file mode 100644
index 0000000..88a1f42
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsBuilder.java
@@ -0,0 +1,67 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketParser;
+import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativeParser;
+
+import java.io.IOException;
+
+/**
+ * A builder for building requests for a {@link BucketMetricsPipelineAggregator}
+ */
+public abstract class BucketMetricsBuilder<B extends BucketMetricsBuilder<B>> extends PipelineAggregatorBuilder<B> {
+
+    private String format;
+    private GapPolicy gapPolicy;
+
+    public BucketMetricsBuilder(String name, String type) {
+        super(name, type);
+    }
+
+    public B format(String format) {
+        this.format = format;
+        return (B) this;
+    }
+
+    public B gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return (B) this;
+    }
+
+    @Override
+    protected final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (format != null) {
+            builder.field(MinBucketParser.FORMAT.getPreferredName(), format);
+        }
+        if (gapPolicy != null) {
+            builder.field(DerivativeParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        doInternalXContent(builder, params);
+        return builder;
+    }
+
+    protected void doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsFactory.java
deleted file mode 100644
index fb91030..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsFactory.java
+++ /dev/null
@@ -1,146 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
-import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-import java.util.Objects;
-
-public abstract class BucketMetricsFactory<AF extends BucketMetricsFactory<AF>> extends PipelineAggregatorFactory {
-
-    private String format = null;
-    private GapPolicy gapPolicy = GapPolicy.SKIP;
-
-    public BucketMetricsFactory(String name, String type, String[] bucketsPaths) {
-        super(name, type, bucketsPaths);
-    }
-
-    /**
-     * Sets the format to use on the output of this aggregation.
-     */
-    public AF format(String format) {
-        this.format = format;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the format to use on the output of this aggregation.
-     */
-    public String format() {
-        return format;
-    }
-
-    protected ValueFormatter formatter() {
-        if (format != null) {
-            return ValueFormat.Patternable.Number.format(format).formatter();
-        } else {
-            return ValueFormatter.RAW;
-        }
-    }
-
-    /**
-     * Sets the gap policy to use for this aggregation.
-     */
-    public AF gapPolicy(GapPolicy gapPolicy) {
-        this.gapPolicy = gapPolicy;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the gap policy to use for this aggregation.
-     */
-    public GapPolicy gapPolicy() {
-        return gapPolicy;
-    }
-
-    @Override
-    protected abstract PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException;
-
-    @Override
-    public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
-            List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
-        if (bucketsPaths.length != 1) {
-            throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
-                    + " must contain a single entry for aggregation [" + name + "]");
-        }
-    }
-
-    @Override
-    protected final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-        if (format != null) {
-            builder.field(BucketMetricsParser.FORMAT.getPreferredName(), format);
-        }
-        if (gapPolicy != null) {
-            builder.field(BucketMetricsParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-        }
-        doXContentBody(builder, params);
-        return builder;
-    }
-
-    protected abstract XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException;
-
-    @Override
-    protected final PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-        BucketMetricsFactory factory = innerReadFrom(name, bucketsPaths, in);
-        factory.format = in.readOptionalString();
-        factory.gapPolicy = GapPolicy.readFrom(in);
-        return factory;
-    }
-
-    protected abstract BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException;
-
-    @Override
-    protected final void doWriteTo(StreamOutput out) throws IOException {
-        innerWriteTo(out);
-        out.writeOptionalString(format);
-        gapPolicy.writeTo(out);
-    }
-
-    protected abstract void innerWriteTo(StreamOutput out) throws IOException;
-
-    @Override
-    protected final int doHashCode() {
-        return Objects.hash(format, gapPolicy, innerHashCode());
-    }
-
-    protected abstract int innerHashCode();
-
-    @Override
-    protected final boolean doEquals(Object obj) {
-        BucketMetricsFactory other = (BucketMetricsFactory) obj;
-        return Objects.equals(format, other.format)
-                && Objects.equals(gapPolicy, other.gapPolicy)
-                && innerEquals(other);
-    }
-
-    protected abstract boolean innerEquals(BucketMetricsFactory other);
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java
index db1f62b..3cf084b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java
@@ -20,12 +20,14 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.text.ParseException;
@@ -46,13 +48,12 @@ public abstract class BucketMetricsParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public final PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context)
-            throws IOException {
+    public final PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
         String format = null;
-        GapPolicy gapPolicy = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
         Map<String, Object> leftover = new HashMap<>(5);
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -85,34 +86,34 @@ public abstract class BucketMetricsParser implements PipelineAggregator.Parser {
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Missing required field [" + BUCKETS_PATH.getPreferredName() + "] for aggregation [" + pipelineAggregatorName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for aggregation [" + pipelineAggregatorName + "]", parser.getTokenLocation());
         }
 
-        BucketMetricsFactory factory = null;
+        ValueFormatter formatter = null;
+        if (format != null) {
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
+        }
+
+        PipelineAggregatorFactory factory = null;
         try {
-            factory = buildFactory(pipelineAggregatorName, bucketsPaths[0], leftover);
-            if (format != null) {
-                factory.format(format);
-            }
-            if (gapPolicy != null) {
-                factory.gapPolicy(gapPolicy);
-            }
+            factory = buildFactory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter, leftover);
         } catch (ParseException exception) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Could not parse settings for aggregation [" + pipelineAggregatorName + "].", exception);
+            throw new SearchParseException(context, "Could not parse settings for aggregation ["
+                    + pipelineAggregatorName + "].", null, exception);
         }
 
         if (leftover.size() > 0) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Unexpected tokens " + leftover.keySet() + " in [" + pipelineAggregatorName + "].");
+            throw new SearchParseException(context, "Unexpected tokens " + leftover.keySet() + " in [" + pipelineAggregatorName + "].", null);
         }
         assert(factory != null);
 
         return factory;
     }
 
-    protected abstract BucketMetricsFactory buildFactory(String pipelineAggregatorName, String bucketsPaths,
-            Map<String, Object> unparsedParams) throws ParseException;
+    protected abstract PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) throws ParseException;
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketBuilder.java
new file mode 100644
index 0000000..627cded
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketBuilder.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg;
+
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+public class AvgBucketBuilder extends BucketMetricsBuilder<AvgBucketBuilder> {
+
+    public AvgBucketBuilder(String name) {
+        super(name, AvgBucketPipelineAggregator.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketParser.java
index 431be6d..658284f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg;
 
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -32,11 +33,8 @@ public class AvgBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String bucketsPath, Map<String, Object> unparsedParams) {
-        return new AvgBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPath);
-    }
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new AvgBucketPipelineAggregator.Factory(null, null);
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new AvgBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketPipelineAggregator.java
index e2c74fe..3ab134c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -30,7 +28,6 @@ import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
@@ -89,55 +86,30 @@ public class AvgBucketPipelineAggregator extends BucketMetricsPipelineAggregator
         return new InternalSimpleValue(name(), avgValue, formatter, pipelineAggregators, metadata);
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
 
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new AvgBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new AvgBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
-                List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories, List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            return new Factory(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketBuilder.java
new file mode 100644
index 0000000..8b214d3
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketBuilder.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max;
+
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+public class MaxBucketBuilder extends BucketMetricsBuilder<MaxBucketBuilder> {
+
+    public MaxBucketBuilder(String name) {
+        super(name, MaxBucketPipelineAggregator.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketParser.java
index c75927e..4cd584a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max;
 
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -33,13 +34,9 @@ public class MaxBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String bucketsPath, Map<String, Object> unparsedParams) {
-        return new MaxBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPath);
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new MaxBucketPipelineAggregator.Factory(null, null);
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+                                                     ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new MaxBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketPipelineAggregator.java
index 6018540..95a70af 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -29,7 +27,6 @@ import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.InternalBucketMetricValue;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
@@ -96,23 +93,24 @@ public class MaxBucketPipelineAggregator extends BucketMetricsPipelineAggregator
         return new InternalBucketMetricValue(name(), keys, maxValue, formatter, Collections.emptyList(), metaData());
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
 
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new MaxBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new MaxBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
                 List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
@@ -120,31 +118,6 @@ public class MaxBucketPipelineAggregator extends BucketMetricsPipelineAggregator
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            return new Factory(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketBuilder.java
new file mode 100644
index 0000000..327bf4e
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketBuilder.java
@@ -0,0 +1,31 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min;
+
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+
+public class MinBucketBuilder extends BucketMetricsBuilder<MinBucketBuilder> {
+
+    public MinBucketBuilder(String name) {
+        super(name, MinBucketPipelineAggregator.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketParser.java
index 3e6e7e1..db7bc9b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min;
 
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -32,14 +33,9 @@ public class MinBucketParser extends BucketMetricsParser {
         return MinBucketPipelineAggregator.TYPE.name();
     }
 
-    @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String bucketsPath, Map<String, Object> unparsedParams) {
-        return new MinBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPath);
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new MinBucketPipelineAggregator.Factory(null, null);
-    }
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new MinBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
+    };
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketPipelineAggregator.java
index 576b05d..755b206 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -29,7 +27,6 @@ import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.InternalBucketMetricValue;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
@@ -97,23 +94,24 @@ public class MinBucketPipelineAggregator extends BucketMetricsPipelineAggregator
         return new InternalBucketMetricValue(name(), keys, minValue, formatter, Collections.emptyList(), metaData());
     };
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
 
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new MinBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new MinBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
                 List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
@@ -121,31 +119,6 @@ public class MinBucketPipelineAggregator extends BucketMetricsPipelineAggregator
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            return new Factory(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketBuilder.java
new file mode 100644
index 0000000..9293e14
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketBuilder.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+import java.io.IOException;
+
+public class PercentilesBucketBuilder extends BucketMetricsBuilder<PercentilesBucketBuilder> {
+
+    Double[] percents;
+
+    public PercentilesBucketBuilder(String name) {
+        super(name, PercentilesBucketPipelineAggregator.TYPE.name());
+    }
+
+    public PercentilesBucketBuilder percents(Double[] percents) {
+        this.percents = percents;
+        return this;
+    }
+
+    @Override
+    protected void doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (percents != null) {
+            builder.field(PercentilesBucketParser.PERCENTS.getPreferredName(), (Object[])percents);
+        }
+    }
+
+
+}
+
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketParser.java
index fabe718..7c9da5c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketParser.java
@@ -21,13 +21,15 @@ package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile;
 
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.text.ParseException;
 import java.util.List;
 import java.util.Map;
 
+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+
 
 public class PercentilesBucketParser extends BucketMetricsParser {
 
@@ -39,10 +41,10 @@ public class PercentilesBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String bucketsPath, Map<String, Object> unparsedParams)
-            throws ParseException {
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+                                                     ValueFormatter formatter, Map<String, Object> unparsedParams) throws ParseException {
 
-        double[] percents = null;
+        double[] percents = new double[] { 1.0, 5.0, 25.0, 50.0, 75.0, 95.0, 99.0 };
         int counter = 0;
         Object percentParam = unparsedParams.get(PERCENTS.getPreferredName());
 
@@ -65,16 +67,6 @@ public class PercentilesBucketParser extends BucketMetricsParser {
             }
         }
 
-        PercentilesBucketPipelineAggregator.Factory factory = new PercentilesBucketPipelineAggregator.Factory(pipelineAggregatorName,
-                bucketsPath);
-        if (percents != null) {
-            factory.percents(percents);
-        }
-        return factory;
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new PercentilesBucketPipelineAggregator.Factory(null, null);
+        return new PercentilesBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter, percents);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregator.java
index 7f74301..24e8204 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregator.java
@@ -19,33 +19,28 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile;
 
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
+
+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 
 public class PercentilesBucketPipelineAggregator extends BucketMetricsPipelineAggregator {
 
     public final static Type TYPE = new Type("percentiles_bucket");
-    public final ParseField PERCENTS_FIELD = new ParseField("percents");
 
     public final static PipelineAggregatorStreams.Stream STREAM = new PipelineAggregatorStreams.Stream() {
         @Override
@@ -124,47 +119,27 @@ public class PercentilesBucketPipelineAggregator extends BucketMetricsPipelineAg
         out.writeDoubleArray(percents);
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
-
-        private double[] percents = new double[] { 1.0, 5.0, 25.0, 50.0, 75.0, 95.0, 99.0 };
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
+        private final double[] percents;
 
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter, double[] percents) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Get the percentages to calculate percentiles for in this aggregation
-         */
-        public double[] percents() {
-            return percents;
-        }
-
-        /**
-         * Set the percentages to calculate percentiles for in this aggregation
-         */
-        public Factory percents(double[] percents) {
-            for (Double p : percents) {
-                if (p == null || p < 0.0 || p > 100.0) {
-                    throw new IllegalArgumentException(PercentilesBucketParser.PERCENTS.getPreferredName()
-                            + " must only contain non-null doubles from 0.0-100.0 inclusive");
-                }
-            }
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
             this.percents = percents;
-            return this;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new PercentilesBucketPipelineAggregator(name, percents, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new PercentilesBucketPipelineAggregator(name, percents, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
-                List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
+                               List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
@@ -178,37 +153,6 @@ public class PercentilesBucketPipelineAggregator extends BucketMetricsPipelineAg
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            if (percents != null) {
-                builder.field(PercentilesBucketParser.PERCENTS.getPreferredName(), percents);
-            }
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.percents = in.readDoubleArray();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDoubleArray(percents);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Arrays.hashCode(percents);
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory obj) {
-            Factory other = (Factory) obj;
-            return Objects.deepEquals(percents, other.percents);
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketBuilder.java
new file mode 100644
index 0000000..a8c19db
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketBuilder.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats;
+
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+public class StatsBucketBuilder extends BucketMetricsBuilder<StatsBucketBuilder> {
+
+    public StatsBucketBuilder(String name) {
+        super(name, StatsBucketPipelineAggregator.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketParser.java
index 41a742c..b250447 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats;
 
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -32,12 +33,8 @@ public class StatsBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String bucketsPath, Map<String, Object> unparsedParams) {
-        return new StatsBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPath);
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new StatsBucketPipelineAggregator.Factory(null, null);
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new StatsBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketPipelineAggregator.java
index 0cbcde8..66726ce 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -29,7 +27,6 @@ import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
@@ -95,23 +92,24 @@ public class StatsBucketPipelineAggregator extends BucketMetricsPipelineAggregat
         return new InternalStatsBucket(name(), count, sum, min, max, formatter, pipelineAggregators, metadata);
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
 
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new StatsBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new StatsBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
                 List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(Parser.BUCKETS_PATH.getPreferredName()
@@ -119,31 +117,6 @@ public class StatsBucketPipelineAggregator extends BucketMetricsPipelineAggregat
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            return new Factory(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketBuilder.java
new file mode 100644
index 0000000..25880bd
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketBuilder.java
@@ -0,0 +1,47 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+import java.io.IOException;
+
+public class ExtendedStatsBucketBuilder extends BucketMetricsBuilder<ExtendedStatsBucketBuilder> {
+
+    Double sigma;
+
+    public ExtendedStatsBucketBuilder(String name) {
+        super(name, ExtendedStatsBucketPipelineAggregator.TYPE.name());
+    }
+
+    public ExtendedStatsBucketBuilder sigma(Double sigma) {
+        this.sigma = sigma;
+        return this;
+    }
+
+    @Override
+    protected void doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (sigma != null) {
+            builder.field(ExtendedStatsBucketParser.SIGMA.getPreferredName(), sigma);
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java
index 89457a9..b4d1f18 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java
@@ -20,9 +20,10 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended;
 
 import org.elasticsearch.common.ParseField;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.text.ParseException;
 import java.util.Map;
@@ -36,10 +37,10 @@ public class ExtendedStatsBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String bucketsPath, Map<String, Object> unparsedParams)
-            throws ParseException {
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) throws ParseException {
 
-        Double sigma = null;
+        double sigma = 2.0;
         Object param = unparsedParams.get(SIGMA.getPreferredName());
 
         if (param != null) {
@@ -51,16 +52,6 @@ public class ExtendedStatsBucketParser extends BucketMetricsParser {
                         + param.getClass().getSimpleName() + "` provided instead", 0);
             }
         }
-        ExtendedStatsBucketPipelineAggregator.Factory factory = new ExtendedStatsBucketPipelineAggregator.Factory(pipelineAggregatorName,
-                bucketsPath);
-        if (sigma != null) {
-            factory.sigma(sigma);
-        }
-        return factory;
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new ExtendedStatsBucketPipelineAggregator.Factory(null, null);
+        return new ExtendedStatsBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, sigma, gapPolicy, formatter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketPipelineAggregator.java
index 877b376..6a7f2be 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -29,14 +27,12 @@ import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 public class ExtendedStatsBucketPipelineAggregator extends BucketMetricsPipelineAggregator {
 
@@ -101,45 +97,26 @@ public class ExtendedStatsBucketPipelineAggregator extends BucketMetricsPipeline
         return new InternalExtendedStatsBucket(name(), count, sum, min, max, sumOfSqrs, sigma, formatter, pipelineAggregators, metadata);
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        private double sigma = 2.0;
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
+        private final double sigma;
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
-
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, double sigma, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Set the value of sigma to use when calculating the standard deviation
-         * bounds
-         */
-        public Factory sigma(double sigma) {
-            if (sigma < 0.0) {
-                throw new IllegalArgumentException(ExtendedStatsBucketParser.SIGMA.getPreferredName() + " must be a non-negative double");
-            }
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
             this.sigma = sigma;
-            return this;
-        }
-
-        /**
-         * Get the value of sigma to use when calculating the standard deviation
-         * bounds
-         */
-        public double sigma() {
-            return sigma;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new ExtendedStatsBucketPipelineAggregator(name, bucketsPaths, sigma, gapPolicy(), formatter(), metaData);
+            return new ExtendedStatsBucketPipelineAggregator(name, bucketsPaths, sigma, gapPolicy, formatter, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
                 List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(Parser.BUCKETS_PATH.getPreferredName()
@@ -152,35 +129,6 @@ public class ExtendedStatsBucketPipelineAggregator extends BucketMetricsPipeline
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(ExtendedStatsBucketParser.SIGMA.getPreferredName(), sigma);
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.sigma = in.readDouble();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDouble(sigma);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(sigma);
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(sigma, other.sigma);
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketBuilder.java
new file mode 100644
index 0000000..5b2201c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketBuilder.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum;
+
+import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsBuilder;
+
+public class SumBucketBuilder extends BucketMetricsBuilder<SumBucketBuilder> {
+
+    public SumBucketBuilder(String name) {
+        super(name, SumBucketPipelineAggregator.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketParser.java
index bcf08b3..3fad95d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum;
 
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -32,12 +33,8 @@ public class SumBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String bucketsPath, Map<String, Object> unparsedParams) {
-        return new SumBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPath);
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new SumBucketPipelineAggregator.Factory(null, null);
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new SumBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketPipelineAggregator.java
index 275e9a3..138bd63 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -30,7 +28,6 @@ import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
@@ -85,23 +82,24 @@ public class SumBucketPipelineAggregator extends BucketMetricsPipelineAggregator
         return new InternalSimpleValue(name(), sum, formatter, pipelineAggregators, metadata);
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
 
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new SumBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new SumBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
                 List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
@@ -109,31 +107,6 @@ public class SumBucketPipelineAggregator extends BucketMetricsPipelineAggregator
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            return new Factory(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptBuilder.java
new file mode 100644
index 0000000..ee5fa94
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptBuilder.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.bucketscript;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+
+import java.io.IOException;
+import java.util.Map;
+
+public class BucketScriptBuilder extends PipelineAggregatorBuilder<BucketScriptBuilder> {
+
+    private String format;
+    private GapPolicy gapPolicy;
+    private Script script;
+    private Map<String, String> bucketsPathsMap;
+
+    public BucketScriptBuilder(String name) {
+        super(name, BucketScriptPipelineAggregator.TYPE.name());
+    }
+
+    public BucketScriptBuilder script(Script script) {
+        this.script = script;
+        return this;
+    }
+
+    public BucketScriptBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    public BucketScriptBuilder gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return this;
+    }
+
+    /**
+     * Sets the paths to the buckets to use for this pipeline aggregator
+     */
+    public BucketScriptBuilder setBucketsPathsMap(Map<String, String> bucketsPathsMap) {
+        this.bucketsPathsMap = bucketsPathsMap;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
+        if (script != null) {
+            builder.field(ScriptField.SCRIPT.getPreferredName(), script);
+        }
+        if (format != null) {
+            builder.field(BucketScriptParser.FORMAT.getPreferredName(), format);
+        }
+        if (gapPolicy != null) {
+            builder.field(BucketScriptParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        if (bucketsPathsMap != null) {
+            builder.field(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName(), bucketsPathsMap);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java
index d97d51a..05ff7e9 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java
@@ -20,18 +20,19 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketscript;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -48,13 +49,13 @@ public class BucketScriptParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, QueryParseContext context) throws IOException {
+    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         Script script = null;
         String currentFieldName = null;
         Map<String, String> bucketsPathsMap = null;
         String format = null;
-        GapPolicy gapPolicy = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -70,8 +71,8 @@ public class BucketScriptParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
                     script = Script.parse(parser, context.parseFieldMatcher());
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -85,8 +86,8 @@ public class BucketScriptParser implements PipelineAggregator.Parser {
                         bucketsPathsMap.put("_value" + i, paths.get(i));
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
@@ -98,37 +99,33 @@ public class BucketScriptParser implements PipelineAggregator.Parser {
                         bucketsPathsMap.put(entry.getKey(), String.valueOf(entry.getValue()));
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + reducerName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + reducerName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPathsMap == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for series_arithmetic aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for series_arithmetic aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
         if (script == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + ScriptField.SCRIPT.getPreferredName()
-                    + "] for series_arithmetic aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + ScriptField.SCRIPT.getPreferredName()
+                    + "] for series_arithmetic aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
-        BucketScriptPipelineAggregator.Factory factory = new BucketScriptPipelineAggregator.Factory(reducerName, bucketsPathsMap, script);
+        ValueFormatter formatter = null;
         if (format != null) {
-            factory.format(format);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
         }
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
-        }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new BucketScriptPipelineAggregator.Factory(null, Collections.emptyMap(), null);
+        return new BucketScriptPipelineAggregator.Factory(reducerName, bucketsPathsMap, script, formatter, gapPolicy);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java
index 6edcbc3..4da355f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java
@@ -21,11 +21,9 @@ package org.elasticsearch.search.aggregations.pipeline.bucketscript;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.script.CompiledScript;
 import org.elasticsearch.script.ExecutableScript;
 import org.elasticsearch.script.Script;
-import org.elasticsearch.script.Script.ScriptField;
 import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -39,7 +37,6 @@ import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -49,8 +46,6 @@ import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
@@ -162,124 +157,22 @@ public class BucketScriptPipelineAggregator extends PipelineAggregator {
 
     public static class Factory extends PipelineAggregatorFactory {
 
-        private final Script script;
-        private final Map<String, String> bucketsPathsMap;
-        private String format = null;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
+        private Script script;
+        private final ValueFormatter formatter;
+        private GapPolicy gapPolicy;
+        private Map<String, String> bucketsPathsMap;
 
-        public Factory(String name, Map<String, String> bucketsPathsMap, Script script) {
+        public Factory(String name, Map<String, String> bucketsPathsMap, Script script, ValueFormatter formatter, GapPolicy gapPolicy) {
             super(name, TYPE.name(), bucketsPathsMap.values().toArray(new String[bucketsPathsMap.size()]));
             this.bucketsPathsMap = bucketsPathsMap;
             this.script = script;
-        }
-
-        public Factory(String name, Script script, String... bucketsPaths) {
-            this(name, convertToBucketsPathMap(bucketsPaths), script);
-        }
-
-        private static Map<String, String> convertToBucketsPathMap(String[] bucketsPaths) {
-            Map<String, String> bucketsPathsMap = new HashMap<>();
-            for (int i = 0; i < bucketsPaths.length; i++) {
-                bucketsPathsMap.put("_value" + i, bucketsPaths[i]);
-            }
-            return bucketsPathsMap;
-        }
-
-        /**
-         * Sets the format to use on the output of this aggregation.
-         */
-        public Factory format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        /**
-         * Gets the format to use on the output of this aggregation.
-         */
-        public String format() {
-            return format;
-        }
-
-        protected ValueFormatter formatter() {
-            if (format != null) {
-                return ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                return ValueFormatter.RAW;
-            }
-        }
-
-        /**
-         * Sets the gap policy to use for this aggregation.
-         */
-        public Factory gapPolicy(GapPolicy gapPolicy) {
+            this.formatter = formatter;
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        /**
-         * Gets the gap policy to use for this aggregation.
-         */
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new BucketScriptPipelineAggregator(name, bucketsPathsMap, script, formatter(), gapPolicy, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(BucketScriptParser.BUCKETS_PATH.getPreferredName(), bucketsPathsMap);
-            builder.field(ScriptField.SCRIPT.getPreferredName(), script);
-            if (format != null) {
-                builder.field(BucketScriptParser.FORMAT.getPreferredName(), format);
-            }
-            builder.field(BucketScriptParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            return builder;
-        }
-
-        @Override
-        protected boolean overrideBucketsPath() {
-            return true;
-        }
-
-        @Override
-        protected PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Map<String, String> bucketsPathsMap = new HashMap<String, String>();
-            int mapSize = in.readVInt();
-            for (int i = 0; i < mapSize; i++) {
-                bucketsPathsMap.put(in.readString(), in.readString());
-            }
-            Script script = Script.readScript(in);
-            Factory factory = new Factory(name, bucketsPathsMap, script);
-            factory.format = in.readOptionalString();
-            factory.gapPolicy = GapPolicy.readFrom(in);
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(bucketsPathsMap.size());
-            for (Entry<String, String> e : bucketsPathsMap.entrySet()) {
-                out.writeString(e.getKey());
-                out.writeString(e.getValue());
-            }
-            script.writeTo(out);
-            out.writeOptionalString(format);
-            gapPolicy.writeTo(out);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(bucketsPathsMap, script, format, gapPolicy);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(bucketsPathsMap, other.bucketsPathsMap) && Objects.equals(script, other.script)
-                    && Objects.equals(format, other.format) && Objects.equals(gapPolicy, other.gapPolicy);
+            return new BucketScriptPipelineAggregator(name, bucketsPathsMap, script, formatter, gapPolicy, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumBuilder.java
new file mode 100644
index 0000000..282ded8
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumBuilder.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.cumulativesum;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+
+import java.io.IOException;
+
+public class CumulativeSumBuilder extends PipelineAggregatorBuilder<CumulativeSumBuilder> {
+
+    private String format;
+
+    public CumulativeSumBuilder(String name) {
+        super(name, CumulativeSumPipelineAggregator.TYPE.name());
+    }
+
+    public CumulativeSumBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (format != null) {
+            builder.field(CumulativeSumParser.FORMAT.getPreferredName(), format);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java
index 30a6b21..f3e2ead 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java
@@ -20,11 +20,13 @@
 package org.elasticsearch.search.aggregations.pipeline.cumulativesum;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -42,8 +44,7 @@ public class CumulativeSumParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context)
-            throws IOException {
+    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
@@ -58,8 +59,8 @@ public class CumulativeSumParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
                     bucketsPaths = new String[] { parser.text() };
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -70,31 +71,28 @@ public class CumulativeSumParser implements PipelineAggregator.Parser {
                     }
                     bucketsPaths = paths.toArray(new String[paths.size()]);
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " in [" + pipelineAggregatorName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + pipelineAggregatorName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for derivative aggregation [" + pipelineAggregatorName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for derivative aggregation [" + pipelineAggregatorName + "]", parser.getTokenLocation());
         }
 
-        CumulativeSumPipelineAggregator.Factory factory = new CumulativeSumPipelineAggregator.Factory(pipelineAggregatorName,
-                bucketsPaths[0]);
+        ValueFormatter formatter = null;
         if (format != null) {
-            factory.format(format);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
         }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new CumulativeSumPipelineAggregator.Factory(null, null);
+        return new CumulativeSumPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, formatter);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java
index 62b98e3..49c6f4f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java
@@ -21,21 +21,18 @@ package org.elasticsearch.search.aggregations.pipeline.cumulativesum;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.InternalAggregations;
-import org.elasticsearch.search.aggregations.bucket.histogram.AbstractHistogramAggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -43,7 +40,6 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
@@ -113,55 +109,29 @@ public class CumulativeSumPipelineAggregator extends PipelineAggregator {
 
     public static class Factory extends PipelineAggregatorFactory {
 
-        private String format;
+        private final ValueFormatter formatter;
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
-
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Sets the format to use on the output of this aggregation.
-         */
-        public Factory format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        /**
-         * Gets the format to use on the output of this aggregation.
-         */
-        public String format() {
-            return format;
-        }
-
-        protected ValueFormatter formatter() {
-            if (format != null) {
-                return ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                return ValueFormatter.RAW;
-            }
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new CumulativeSumPipelineAggregator(name, bucketsPaths, formatter(), metaData);
+            return new CumulativeSumPipelineAggregator(name, bucketsPaths, formatter, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories, List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories, List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
             }
-            if (!(parent instanceof AbstractHistogramAggregatorFactory<?>)) {
+            if (!(parent instanceof HistogramAggregator.Factory)) {
                 throw new IllegalStateException("cumulative sum aggregation [" + name
                         + "] must have a histogram or date_histogram as parent");
             } else {
-                AbstractHistogramAggregatorFactory<?> histoParent = (AbstractHistogramAggregatorFactory<?>) parent;
+                HistogramAggregator.Factory histoParent = (HistogramAggregator.Factory) parent;
                 if (histoParent.minDocCount() != 0) {
                     throw new IllegalStateException("parent histogram of cumulative sum aggregation [" + name
                             + "] must have min_doc_count of 0");
@@ -169,35 +139,5 @@ public class CumulativeSumPipelineAggregator extends PipelineAggregator {
             }
         }
 
-        @Override
-        protected final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (format != null) {
-                builder.field(BucketMetricsParser.FORMAT.getPreferredName(), format);
-            }
-            return builder;
-        }
-
-        @Override
-        protected final PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.format = in.readOptionalString();
-            return factory;
-        }
-
-        @Override
-        protected final void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(format);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(format);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(format, other.format);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeBuilder.java
new file mode 100644
index 0000000..50b4578
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeBuilder.java
@@ -0,0 +1,78 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.derivative;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+
+import java.io.IOException;
+
+public class DerivativeBuilder extends PipelineAggregatorBuilder<DerivativeBuilder> {
+
+    private String format;
+    private GapPolicy gapPolicy;
+    private String unit;
+
+    public DerivativeBuilder(String name) {
+        super(name, DerivativePipelineAggregator.TYPE.name());
+    }
+
+    public DerivativeBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    public DerivativeBuilder gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return this;
+    }
+
+    public DerivativeBuilder unit(String unit) {
+        this.unit = unit;
+        return this;
+    }
+
+    /**
+     * Sets the unit using the provided {@link DateHistogramInterval}. This
+     * method is only useful when calculating the derivative using a
+     * `date_histogram`
+     */
+    public DerivativeBuilder unit(DateHistogramInterval unit) {
+        this.unit = unit.toString();
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (format != null) {
+            builder.field(DerivativeParser.FORMAT.getPreferredName(), format);
+        }
+        if (gapPolicy != null) {
+            builder.field(DerivativeParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        if (unit != null) {
+            builder.field(DerivativeParser.UNIT.getPreferredName(), unit);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java
index 7d6abdc..f413987 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java
@@ -20,12 +20,17 @@
 package org.elasticsearch.search.aggregations.pipeline.derivative;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.rounding.DateTimeUnit;
+import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramParser;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -43,14 +48,13 @@ public class DerivativeParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context)
-            throws IOException {
+    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
         String format = null;
         String units = null;
-        GapPolicy gapPolicy = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -65,8 +69,8 @@ public class DerivativeParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, UNIT)) {
                     units = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -77,36 +81,41 @@ public class DerivativeParser implements PipelineAggregator.Parser {
                     }
                     bucketsPaths = paths.toArray(new String[paths.size()]);
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " in [" + pipelineAggregatorName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + pipelineAggregatorName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for derivative aggregation [" + pipelineAggregatorName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for derivative aggregation [" + pipelineAggregatorName + "]", parser.getTokenLocation());
         }
 
-        DerivativePipelineAggregator.Factory factory = new DerivativePipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths[0]);
+        ValueFormatter formatter = null;
         if (format != null) {
-            factory.format(format);
-        }
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
         }
+
+        Long xAxisUnits = null;
         if (units != null) {
-            factory.unit(units);
+            DateTimeUnit dateTimeUnit = DateHistogramParser.DATE_FIELD_UNITS.get(units);
+            if (dateTimeUnit != null) {
+                xAxisUnits = dateTimeUnit.field().getDurationField().getUnitMillis();
+            } else {
+                TimeValue timeValue = TimeValue.parseTimeValue(units, null, getClass().getSimpleName() + ".unit");
+                if (timeValue != null) {
+                    xAxisUnits = timeValue.getMillis();
+                }
+            }
         }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new DerivativePipelineAggregator.Factory(null, null);
+        return new DerivativePipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, formatter, gapPolicy, xAxisUnits);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java
index d5d6387..855fea8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java
@@ -21,24 +21,18 @@ package org.elasticsearch.search.aggregations.pipeline.derivative;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.rounding.DateTimeUnit;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.InternalAggregations;
-import org.elasticsearch.search.aggregations.bucket.histogram.AbstractHistogramAggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramAggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 import org.joda.time.DateTime;
@@ -47,7 +41,6 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
@@ -161,84 +154,33 @@ public class DerivativePipelineAggregator extends PipelineAggregator {
 
     public static class Factory extends PipelineAggregatorFactory {
 
-        private String format;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
-        private String units;
+        private final ValueFormatter formatter;
+        private GapPolicy gapPolicy;
+        private Long xAxisUnits;
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
-
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, ValueFormatter formatter, GapPolicy gapPolicy, Long xAxisUnits) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        public Factory format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        public String format() {
-            return format;
-        }
-
-        public Factory gapPolicy(GapPolicy gapPolicy) {
+            this.formatter = formatter;
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
-        }
-
-        public Factory unit(String units) {
-            this.units = units;
-            return this;
-        }
-
-        public Factory unit(DateHistogramInterval units) {
-            this.units = units.toString();
-            return this;
-        }
-
-        public String unit() {
-            return units;
+            this.xAxisUnits = xAxisUnits;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            ValueFormatter formatter;
-            if (format != null) {
-                formatter = ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                formatter = ValueFormatter.RAW;
-            }
-            Long xAxisUnits = null;
-            if (units != null) {
-                DateTimeUnit dateTimeUnit = DateHistogramAggregatorFactory.DATE_FIELD_UNITS.get(units);
-                if (dateTimeUnit != null) {
-                    xAxisUnits = dateTimeUnit.field().getDurationField().getUnitMillis();
-                } else {
-                    TimeValue timeValue = TimeValue.parseTimeValue(units, null, getClass().getSimpleName() + ".unit");
-                    if (timeValue != null) {
-                        xAxisUnits = timeValue.getMillis();
-                    }
-                }
-            }
             return new DerivativePipelineAggregator(name, bucketsPaths, formatter, gapPolicy, xAxisUnits, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories, List<PipelineAggregatorFactory> pipelineAggregatoractories) {
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories, List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
             }
-            if (!(parent instanceof AbstractHistogramAggregatorFactory<?>)) {
+            if (!(parent instanceof HistogramAggregator.Factory)) {
                 throw new IllegalStateException("derivative aggregation [" + name
                         + "] must have a histogram or date_histogram as parent");
             } else {
-                AbstractHistogramAggregatorFactory<?> histoParent = (AbstractHistogramAggregatorFactory<?>) parent;
+                HistogramAggregator.Factory histoParent = (HistogramAggregator.Factory) parent;
                 if (histoParent.minDocCount() != 0) {
                     throw new IllegalStateException("parent histogram of derivative aggregation [" + name
                             + "] must have min_doc_count of 0");
@@ -246,61 +188,5 @@ public class DerivativePipelineAggregator extends PipelineAggregator {
             }
         }
 
-        @Override
-        protected PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.format = in.readOptionalString();
-            if (in.readBoolean()) {
-                factory.gapPolicy = GapPolicy.readFrom(in);
-            }
-            factory.units = in.readOptionalString();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(format);
-            boolean hasGapPolicy = gapPolicy != null;
-            out.writeBoolean(hasGapPolicy);
-            if (hasGapPolicy) {
-                gapPolicy.writeTo(out);
-            }
-            out.writeOptionalString(units);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (format != null) {
-                builder.field(DerivativeParser.FORMAT.getPreferredName(), format);
-            }
-            if (gapPolicy != null) {
-                builder.field(DerivativeParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            }
-            if (units != null) {
-                builder.field(DerivativeParser.UNIT.getPreferredName(), units);
-            }
-            return builder;
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            if (!Objects.equals(format, other.format)) {
-                return false;
-            }
-            if (!Objects.equals(gapPolicy, other.gapPolicy)) {
-                return false;
-            }
-            if (!Objects.equals(units, other.units)) {
-                return false;
-            }
-            return true;
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(format, gapPolicy, units);
-        }
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorBuilder.java
new file mode 100644
index 0000000..c291c63
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorBuilder.java
@@ -0,0 +1,76 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.having;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+
+import java.io.IOException;
+import java.util.Map;
+
+public class BucketSelectorBuilder extends PipelineAggregatorBuilder<BucketSelectorBuilder> {
+
+    private GapPolicy gapPolicy;
+    private Script script;
+    private Map<String, String> bucketsPathsMap;
+
+    public BucketSelectorBuilder(String name) {
+        super(name, BucketSelectorPipelineAggregator.TYPE.name());
+    }
+
+    public BucketSelectorBuilder script(Script script) {
+        this.script = script;
+        return this;
+    }
+
+    public BucketSelectorBuilder gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return this;
+    }
+
+    /**
+     * Sets the paths to the buckets to use for this pipeline aggregator. The
+     * map given to this method must contain script variable name as keys with
+     * bucket paths values to the metrics to use for each variable.
+     */
+    public BucketSelectorBuilder setBucketsPathsMap(Map<String, String> bucketsPathsMap) {
+        this.bucketsPathsMap = bucketsPathsMap;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
+        if (script != null) {
+            builder.field(ScriptField.SCRIPT.getPreferredName(), script);
+        }
+        if (gapPolicy != null) {
+            builder.field(BucketSelectorParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        if (bucketsPathsMap != null) {
+            builder.field(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName(), bucketsPathsMap);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorParser.java
index cca0166..e2623b5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorParser.java
@@ -20,18 +20,17 @@
 package org.elasticsearch.search.aggregations.pipeline.having;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -48,12 +47,12 @@ public class BucketSelectorParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, QueryParseContext context) throws IOException {
+    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         Script script = null;
         String currentFieldName = null;
         Map<String, String> bucketsPathsMap = null;
-        GapPolicy gapPolicy = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -67,8 +66,8 @@ public class BucketSelectorParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
                     script = Script.parse(parser, context.parseFieldMatcher());
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -82,8 +81,8 @@ public class BucketSelectorParser implements PipelineAggregator.Parser {
                         bucketsPathsMap.put("_value" + i, paths.get(i));
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
@@ -95,36 +94,26 @@ public class BucketSelectorParser implements PipelineAggregator.Parser {
                         bucketsPathsMap.put(entry.getKey(), String.valueOf(entry.getValue()));
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + reducerName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + reducerName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPathsMap == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for bucket_selector aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for bucket_selector aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
         if (script == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + ScriptField.SCRIPT.getPreferredName()
-                    + "] for bucket_selector aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + ScriptField.SCRIPT.getPreferredName()
+                    + "] for bucket_selector aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
-        BucketSelectorPipelineAggregator.Factory factory = new BucketSelectorPipelineAggregator.Factory(reducerName, bucketsPathsMap,
-                script);
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
-        }
-        return factory;
-
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new BucketSelectorPipelineAggregator.Factory(null, Collections.emptyMap(), null);
+        return new BucketSelectorPipelineAggregator.Factory(reducerName, bucketsPathsMap, script, gapPolicy);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java
index 505234b..1032d0f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java
@@ -22,11 +22,9 @@ package org.elasticsearch.search.aggregations.pipeline.having;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.script.CompiledScript;
 import org.elasticsearch.script.ExecutableScript;
 import org.elasticsearch.script.Script;
-import org.elasticsearch.script.Script.ScriptField;
 import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
@@ -37,7 +35,6 @@ import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptParser;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -45,8 +42,6 @@ import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Objects;
 
 import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;
 
@@ -142,96 +137,20 @@ public class BucketSelectorPipelineAggregator extends PipelineAggregator {
     public static class Factory extends PipelineAggregatorFactory {
 
         private Script script;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
+        private GapPolicy gapPolicy;
         private Map<String, String> bucketsPathsMap;
 
-        public Factory(String name, Map<String, String> bucketsPathsMap, Script script) {
+        public Factory(String name, Map<String, String> bucketsPathsMap, Script script, GapPolicy gapPolicy) {
             super(name, TYPE.name(), bucketsPathsMap.values().toArray(new String[bucketsPathsMap.size()]));
             this.bucketsPathsMap = bucketsPathsMap;
             this.script = script;
-        }
-
-        public Factory(String name, Script script, String... bucketsPaths) {
-            this(name, convertToBucketsPathMap(bucketsPaths), script);
-        }
-
-        private static Map<String, String> convertToBucketsPathMap(String[] bucketsPaths) {
-            Map<String, String> bucketsPathsMap = new HashMap<>();
-            for (int i = 0; i < bucketsPaths.length; i++) {
-                bucketsPathsMap.put("_value" + i, bucketsPaths[i]);
-            }
-            return bucketsPathsMap;
-        }
-
-        /**
-         * Sets the gap policy to use for this aggregation.
-         */
-        public Factory gapPolicy(GapPolicy gapPolicy) {
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        /**
-         * Gets the gap policy to use for this aggregation.
-         */
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
             return new BucketSelectorPipelineAggregator(name, bucketsPathsMap, script, gapPolicy, metaData);
         }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(BucketScriptParser.BUCKETS_PATH.getPreferredName(), bucketsPathsMap);
-            builder.field(ScriptField.SCRIPT.getPreferredName(), script);
-            builder.field(BucketScriptParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            return builder;
-        }
-
-        @Override
-        protected boolean overrideBucketsPath() {
-            return true;
-        }
-
-        @Override
-        protected PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Map<String, String> bucketsPathsMap = new HashMap<String, String>();
-            int mapSize = in.readVInt();
-            for (int i = 0; i < mapSize; i++) {
-                bucketsPathsMap.put(in.readString(), in.readString());
-            }
-            Script script = Script.readScript(in);
-            Factory factory = new Factory(name, bucketsPathsMap, script);
-            factory.gapPolicy = GapPolicy.readFrom(in);
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(bucketsPathsMap.size());
-            for (Entry<String, String> e : bucketsPathsMap.entrySet()) {
-                out.writeString(e.getKey());
-                out.writeString(e.getValue());
-            }
-            script.writeTo(out);
-            gapPolicy.writeTo(out);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(bucketsPathsMap, script, gapPolicy);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(bucketsPathsMap, other.bucketsPathsMap) && Objects.equals(script, other.script)
-                    && Objects.equals(gapPolicy, other.gapPolicy);
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgBuilder.java
new file mode 100644
index 0000000..b2dc718
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgBuilder.java
@@ -0,0 +1,148 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.movavg;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelBuilder;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * A builder to create MovingAvg pipeline aggregations
+ */
+public class MovAvgBuilder extends PipelineAggregatorBuilder<MovAvgBuilder> {
+
+    private String format;
+    private GapPolicy gapPolicy;
+    private MovAvgModelBuilder modelBuilder;
+    private Integer window;
+    private Integer predict;
+    private Boolean minimize;
+    private Map<String, Object> settings;
+
+    public MovAvgBuilder(String name) {
+        super(name, MovAvgPipelineAggregator.TYPE.name());
+    }
+
+    public MovAvgBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    /**
+     * Defines what should be done when a gap in the series is discovered
+     *
+     * @param gapPolicy A GapPolicy enum defining the selected policy
+     * @return Returns the builder to continue chaining
+     */
+    public MovAvgBuilder gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return this;
+    }
+
+    /**
+     * Sets a MovAvgModelBuilder for the Moving Average.  The model builder is used to
+     * define what type of moving average you want to use on the series
+     *
+     * @param modelBuilder A MovAvgModelBuilder which has been prepopulated with settings
+     * @return Returns the builder to continue chaining
+     */
+    public MovAvgBuilder modelBuilder(MovAvgModelBuilder modelBuilder) {
+        this.modelBuilder = modelBuilder;
+        return this;
+    }
+
+    /**
+     * Sets the window size for the moving average.  This window will "slide" across the
+     * series, and the values inside that window will be used to calculate the moving avg value
+     *
+     * @param window Size of window
+     * @return Returns the builder to continue chaining
+     */
+    public MovAvgBuilder window(int window) {
+        this.window = window;
+        return this;
+    }
+
+    /**
+     * Sets the number of predictions that should be returned.  Each prediction will be spaced at
+     * the intervals specified in the histogram.  E.g "predict: 2" will return two new buckets at the
+     * end of the histogram with the predicted values.
+     *
+     * @param numPredictions Number of predictions to make
+     * @return Returns the builder to continue chaining
+     */
+    public MovAvgBuilder predict(int numPredictions) {
+        this.predict = numPredictions;
+        return this;
+    }
+
+    /**
+     * Determines if the model should be fit to the data using a cost
+     * minimizing algorithm.
+     *
+     * @param minimize If the model should be fit to the underlying data
+     * @return Returns the builder to continue chaining
+     */
+    public MovAvgBuilder minimize(boolean minimize) {
+        this.minimize = minimize;
+        return this;
+    }
+
+    /**
+     * The hash of settings that should be provided to the model when it is
+     * instantiated
+     */
+    public MovAvgBuilder settings(Map<String, Object> settings) {
+        this.settings = settings;
+        return this;
+    }
+
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (format != null) {
+            builder.field(MovAvgParser.FORMAT.getPreferredName(), format);
+        }
+        if (gapPolicy != null) {
+            builder.field(MovAvgParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        if (modelBuilder != null) {
+            modelBuilder.toXContent(builder, params);
+        }
+        if (window != null) {
+            builder.field(MovAvgParser.WINDOW.getPreferredName(), window);
+        }
+        if (predict != null) {
+            builder.field(MovAvgParser.PREDICT.getPreferredName(), predict);
+        }
+        if (minimize != null) {
+            builder.field(MovAvgParser.MINIMIZE.getPreferredName(), minimize);
+        }
+        if (settings != null) {
+            builder.field(MovAvgParser.SETTINGS.getPreferredName(), settings);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java
index 62bcf61..5856735 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java
@@ -20,15 +20,17 @@
 package org.elasticsearch.search.aggregations.pipeline.movavg;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModel;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelParserMapper;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.text.ParseException;
@@ -57,18 +59,17 @@ public class MovAvgParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context)
-            throws IOException {
+    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
         String format = null;
 
-        GapPolicy gapPolicy = null;
-        Integer window = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
+        int window = 5;
         Map<String, Object> settings = null;
-        String model = null;
-        Integer predict = null;
+        String model = "simple";
+        int predict = 0;
         Boolean minimize = null;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -78,18 +79,20 @@ public class MovAvgParser implements PipelineAggregator.Parser {
                 if (context.parseFieldMatcher().match(currentFieldName, WINDOW)) {
                     window = parser.intValue();
                     if (window <= 0) {
-                        throw new ParsingException(parser.getTokenLocation(), "[" + currentFieldName + "] value must be a positive, "
-                                + "non-zero integer.  Value supplied was [" + predict + "] in [" + pipelineAggregatorName + "].");
+                        throw new SearchParseException(context, "[" + currentFieldName + "] value must be a positive, "
+                                + "non-zero integer.  Value supplied was [" + predict + "] in [" + pipelineAggregatorName + "].",
+                                parser.getTokenLocation());
                     }
                 } else if (context.parseFieldMatcher().match(currentFieldName, PREDICT)) {
                     predict = parser.intValue();
                     if (predict <= 0) {
-                        throw new ParsingException(parser.getTokenLocation(), "[" + currentFieldName + "] value must be a positive integer."
-                                + "  Value supplied was [" + predict + "] in [" + pipelineAggregatorName + "].");
+                        throw new SearchParseException(context, "[" + currentFieldName + "] value must be a positive, "
+                                + "non-zero integer.  Value supplied was [" + predict + "] in [" + pipelineAggregatorName + "].",
+                                parser.getTokenLocation());
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.VALUE_STRING) {
                 if (context.parseFieldMatcher().match(currentFieldName, FORMAT)) {
@@ -101,8 +104,8 @@ public class MovAvgParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, MODEL)) {
                     model = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -113,71 +116,66 @@ public class MovAvgParser implements PipelineAggregator.Parser {
                     }
                     bucketsPaths = paths.toArray(new String[paths.size()]);
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (context.parseFieldMatcher().match(currentFieldName, SETTINGS)) {
                     settings = parser.map();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
                 if (context.parseFieldMatcher().match(currentFieldName, MINIMIZE)) {
                     minimize = parser.booleanValue();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " in [" + pipelineAggregatorName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + pipelineAggregatorName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for movingAvg aggregation [" + pipelineAggregatorName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for movingAvg aggregation [" + pipelineAggregatorName + "]", parser.getTokenLocation());
         }
 
-        MovAvgPipelineAggregator.Factory factory = new MovAvgPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths[0]);
+        ValueFormatter formatter = null;
         if (format != null) {
-            factory.format(format);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
         }
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
-        }
-        if (window != null) {
-            factory.window(window);
-        }
-        if (predict != null) {
-            factory.predict(predict);
+
+        MovAvgModel.AbstractModelParser modelParser = movAvgModelParserMapper.get(model);
+        if (modelParser == null) {
+            throw new SearchParseException(context, "Unknown model [" + model + "] specified.  Valid options are:"
+                    + movAvgModelParserMapper.getAllNames().toString(), parser.getTokenLocation());
         }
-        if (model != null) {
-            MovAvgModel.AbstractModelParser modelParser = movAvgModelParserMapper.get(model);
-            if (modelParser == null) {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unknown model [" + model + "] specified.  Valid options are:" + movAvgModelParserMapper.getAllNames().toString());
-            }
 
-            MovAvgModel movAvgModel;
-            try {
-                movAvgModel = modelParser.parse(settings, pipelineAggregatorName, window, context.parseFieldMatcher());
-            } catch (ParseException exception) {
-                throw new ParsingException(parser.getTokenLocation(), "Could not parse settings for model [" + model + "].", exception);
-            }
-            factory.model(movAvgModel);
+        MovAvgModel movAvgModel;
+        try {
+            movAvgModel = modelParser.parse(settings, pipelineAggregatorName, window, context.parseFieldMatcher());
+        } catch (ParseException exception) {
+            throw new SearchParseException(context, "Could not parse settings for model [" + model + "].", null, exception);
         }
-        if (minimize != null) {
-            factory.minimize(minimize);
+
+        // If the user doesn't set a preference for cost minimization, ask what the model prefers
+        if (minimize == null) {
+            minimize = movAvgModel.minimizeByDefault();
+        } else if (minimize && !movAvgModel.canBeMinimized()) {
+            // If the user asks to minimize, but this model doesn't support it, throw exception
+            throw new SearchParseException(context, "The [" + model + "] model cannot be minimized.", null);
         }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new MovAvgPipelineAggregator.Factory(null, null);
+
+        return new MovAvgPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, formatter, gapPolicy, window, predict,
+                movAvgModel, minimize);
     }
 
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
index 4668905..4f7034b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
@@ -22,14 +22,13 @@ package org.elasticsearch.search.aggregations.pipeline.movavg;
 import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.InternalAggregations;
-import org.elasticsearch.search.aggregations.bucket.histogram.AbstractHistogramAggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
@@ -37,10 +36,7 @@ import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelBuilder;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelStreams;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.SimpleModel;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 import org.joda.time.DateTime;
@@ -50,7 +46,6 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.ListIterator;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
@@ -281,185 +276,41 @@ public class MovAvgPipelineAggregator extends PipelineAggregator {
 
     public static class Factory extends PipelineAggregatorFactory {
 
-        private String format;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
-        private int window = 5;
-        private MovAvgModel model = new SimpleModel();
-        private int predict = 0;
-        private Boolean minimize;
+        private final ValueFormatter formatter;
+        private GapPolicy gapPolicy;
+        private int window;
+        private MovAvgModel model;
+        private int predict;
+        private boolean minimize;
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
-
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, ValueFormatter formatter, GapPolicy gapPolicy,
+                       int window, int predict, MovAvgModel model, boolean minimize) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Sets the format to use on the output of this aggregation.
-         */
-        public Factory format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        /**
-         * Gets the format to use on the output of this aggregation.
-         */
-        public String format() {
-            return format;
-        }
-
-        /**
-         * Sets the GapPolicy to use on the output of this aggregation.
-         */
-        public Factory gapPolicy(GapPolicy gapPolicy) {
+            this.formatter = formatter;
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        /**
-         * Gets the GapPolicy to use on the output of this aggregation.
-         */
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
-        }
-
-        protected ValueFormatter formatter() {
-            if (format != null) {
-                return ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                return ValueFormatter.RAW;
-            }
-        }
-
-        /**
-         * Sets the window size for the moving average. This window will "slide"
-         * across the series, and the values inside that window will be used to
-         * calculate the moving avg value
-         *
-         * @param window
-         *            Size of window
-         */
-        public Factory window(int window) {
             this.window = window;
-            return this;
-        }
-
-        /**
-         * Gets the window size for the moving average. This window will "slide"
-         * across the series, and the values inside that window will be used to
-         * calculate the moving avg value
-         */
-        public int window() {
-            return window;
-        }
-
-        /**
-         * Sets a MovAvgModel for the Moving Average. The model is used to
-         * define what type of moving average you want to use on the series
-         *
-         * @param model
-         *            A MovAvgModel which has been prepopulated with settings
-         */
-        public Factory modelBuilder(MovAvgModelBuilder model) {
-            this.model = model.build();
-            return this;
-        }
-
-        /**
-         * Sets a MovAvgModel for the Moving Average. The model is used to
-         * define what type of moving average you want to use on the series
-         *
-         * @param model
-         *            A MovAvgModel which has been prepopulated with settings
-         */
-        public Factory model(MovAvgModel model) {
             this.model = model;
-            return this;
-        }
-
-        /**
-         * Gets a MovAvgModel for the Moving Average. The model is used to
-         * define what type of moving average you want to use on the series
-         */
-        public MovAvgModel model() {
-            return model;
-        }
-
-        /**
-         * Sets the number of predictions that should be returned. Each
-         * prediction will be spaced at the intervals specified in the
-         * histogram. E.g "predict: 2" will return two new buckets at the end of
-         * the histogram with the predicted values.
-         *
-         * @param predict
-         *            Number of predictions to make
-         */
-        public Factory predict(int predict) {
-            if (predict <= 0) {
-                throw new IllegalArgumentException("predict must be greater than 0. Found [" + predict + "] in [" + name + "]");
-            }
             this.predict = predict;
-            return this;
-        }
-
-        /**
-         * Gets the number of predictions that should be returned. Each
-         * prediction will be spaced at the intervals specified in the
-         * histogram. E.g "predict: 2" will return two new buckets at the end of
-         * the histogram with the predicted values.
-         */
-        public int predict() {
-            return predict;
-        }
-
-        /**
-         * Sets whether the model should be fit to the data using a cost
-         * minimizing algorithm.
-         *
-         * @param minimize
-         *            If the model should be fit to the underlying data
-         */
-        public Factory minimize(boolean minimize) {
             this.minimize = minimize;
-            return this;
-        }
-
-        /**
-         * Gets whether the model should be fit to the data using a cost
-         * minimizing algorithm.
-         */
-        public Boolean minimize() {
-            return minimize;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            // If the user doesn't set a preference for cost minimization, ask
-            // what the model prefers
-            boolean minimize = this.minimize == null ? model.minimizeByDefault() : this.minimize;
-            return new MovAvgPipelineAggregator(name, bucketsPaths, formatter(), gapPolicy, window, predict, model, minimize, metaData);
+            return new MovAvgPipelineAggregator(name, bucketsPaths, formatter, gapPolicy, window, predict, model, minimize, metaData);
         }
 
         @Override
-        public void doValidate(AggregatorFactory<?> parent, AggregatorFactory<?>[] aggFactories,
+        public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
                 List<PipelineAggregatorFactory> pipelineAggregatoractories) {
-            if (minimize != null && minimize && !model.canBeMinimized()) {
-                // If the user asks to minimize, but this model doesn't support
-                // it, throw exception
-                throw new IllegalStateException("The [" + model + "] model cannot be minimized for aggregation [" + name + "]");
-            }
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
             }
-            if (!(parent instanceof AbstractHistogramAggregatorFactory<?>)) {
+            if (!(parent instanceof HistogramAggregator.Factory)) {
                 throw new IllegalStateException("moving average aggregation [" + name
                         + "] must have a histogram or date_histogram as parent");
             } else {
-                AbstractHistogramAggregatorFactory<?> histoParent = (AbstractHistogramAggregatorFactory<?>) parent;
+                HistogramAggregator.Factory histoParent = (HistogramAggregator.Factory) parent;
                 if (histoParent.minDocCount() != 0) {
                     throw new IllegalStateException("parent histogram of moving average aggregation [" + name
                             + "] must have min_doc_count of 0");
@@ -467,60 +318,5 @@ public class MovAvgPipelineAggregator extends PipelineAggregator {
             }
         }
 
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (format != null) {
-                builder.field(MovAvgParser.FORMAT.getPreferredName(), format);
-            }
-            builder.field(MovAvgParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            model.toXContent(builder, params);
-            builder.field(MovAvgParser.WINDOW.getPreferredName(), window);
-            if (predict > 0) {
-                builder.field(MovAvgParser.PREDICT.getPreferredName(), predict);
-            }
-            if (minimize != null) {
-                builder.field(MovAvgParser.MINIMIZE.getPreferredName(), minimize);
-            }
-            return builder;
-        }
-
-        @Override
-        protected PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.format = in.readOptionalString();
-            factory.gapPolicy = GapPolicy.readFrom(in);
-            factory.window = in.readVInt();
-            factory.model = MovAvgModelStreams.read(in);
-            factory.predict = in.readVInt();
-            factory.minimize = in.readOptionalBoolean();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(format);
-            gapPolicy.writeTo(out);
-            out.writeVInt(window);
-            model.writeTo(out);
-            out.writeVInt(predict);
-            out.writeOptionalBoolean(minimize);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(format, gapPolicy, window, model, predict, minimize);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(format, other.format)
-                    && Objects.equals(gapPolicy, other.gapPolicy)
-                    && Objects.equals(window, other.window)
-                    && Objects.equals(model, other.model)
-                    && Objects.equals(predict, other.predict)
-                    && Objects.equals(minimize, other.minimize);
-        }
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java
index edbfa66..84de794 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java
@@ -32,16 +32,13 @@ import java.text.ParseException;
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Calculate a exponentially weighted moving average
  */
 public class EwmaModel extends MovAvgModel {
 
-    private static final EwmaModel PROTOTYPE = new EwmaModel();
     protected static final ParseField NAME_FIELD = new ParseField("ewma");
-    public static final double DEFAULT_ALPHA = 0.3;
 
     /**
      * Controls smoothing of data.  Also known as "level" value.
@@ -51,10 +48,6 @@ public class EwmaModel extends MovAvgModel {
      */
     private final double alpha;
 
-    public EwmaModel() {
-        this(DEFAULT_ALPHA);
-    }
-
     public EwmaModel(double alpha) {
         this.alpha = alpha;
     }
@@ -104,7 +97,7 @@ public class EwmaModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            return new EwmaModel(in.readDouble());
         }
 
         @Override
@@ -114,42 +107,11 @@ public class EwmaModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-        builder.field("alpha", alpha);
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new EwmaModel(in.readDouble());
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
         out.writeDouble(alpha);
     }
 
-    @Override
-    public int hashCode() {
-        return Objects.hash(alpha);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        EwmaModel other = (EwmaModel) obj;
-        return Objects.equals(alpha, other.alpha);
-    }
-
     public static class SingleExpModelParser extends AbstractModelParser {
 
         @Override
@@ -161,7 +123,7 @@ public class EwmaModel extends MovAvgModel {
         public MovAvgModel parse(@Nullable Map<String, Object> settings, String pipelineName, int windowSize,
                                  ParseFieldMatcher parseFieldMatcher) throws ParseException {
 
-            double alpha = parseDoubleParam(settings, "alpha", DEFAULT_ALPHA);
+            double alpha = parseDoubleParam(settings, "alpha", 0.3);
             checkUnrecognizedParams(settings);
             return new EwmaModel(alpha);
         }
@@ -170,7 +132,7 @@ public class EwmaModel extends MovAvgModel {
 
     public static class EWMAModelBuilder implements MovAvgModelBuilder {
 
-        private double alpha = DEFAULT_ALPHA;
+        private Double alpha;
 
         /**
          * Alpha controls the smoothing of the data.  Alpha = 1 retains no memory of past values
@@ -190,16 +152,13 @@ public class EwmaModel extends MovAvgModel {
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
             builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
             builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-            builder.field("alpha", alpha);
+            if (alpha != null) {
+                builder.field("alpha", alpha);
+            }
 
             builder.endObject();
             return builder;
         }
-
-        @Override
-        public MovAvgModel build() {
-            return new EwmaModel(alpha);
-        }
     }
 }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java
index 06ce050..fe0321b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java
@@ -31,17 +31,13 @@ import java.io.IOException;
 import java.text.ParseException;
 import java.util.Collection;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Calculate a doubly exponential weighted moving average
  */
 public class HoltLinearModel extends MovAvgModel {
 
-    private static final HoltLinearModel PROTOTYPE = new HoltLinearModel();
     protected static final ParseField NAME_FIELD = new ParseField("holt");
-    public static final double DEFAULT_ALPHA = 0.3;
-    public static final double DEFAULT_BETA = 0.1;
 
     /**
      * Controls smoothing of data.  Also known as "level" value.
@@ -59,10 +55,6 @@ public class HoltLinearModel extends MovAvgModel {
      */
     private final double beta;
 
-    public HoltLinearModel() {
-        this(DEFAULT_ALPHA, DEFAULT_BETA);
-    }
-
     public HoltLinearModel(double alpha, double beta) {
         this.alpha = alpha;
         this.beta = beta;
@@ -165,7 +157,7 @@ public class HoltLinearModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            return new HoltLinearModel(in.readDouble(), in.readDouble());
         }
 
         @Override
@@ -175,45 +167,12 @@ public class HoltLinearModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-        builder.field("alpha", alpha);
-        builder.field("beta", beta);
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new HoltLinearModel(in.readDouble(), in.readDouble());
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
         out.writeDouble(alpha);
         out.writeDouble(beta);
     }
 
-    @Override
-    public int hashCode() {
-        return Objects.hash(alpha, beta);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        HoltLinearModel other = (HoltLinearModel) obj;
-        return Objects.equals(alpha, other.alpha)
-                && Objects.equals(beta, other.beta);
-    }
-
     public static class DoubleExpModelParser extends AbstractModelParser {
 
         @Override
@@ -225,8 +184,8 @@ public class HoltLinearModel extends MovAvgModel {
         public MovAvgModel parse(@Nullable Map<String, Object> settings, String pipelineName, int windowSize,
                                  ParseFieldMatcher parseFieldMatcher) throws ParseException {
 
-            double alpha = parseDoubleParam(settings, "alpha", DEFAULT_ALPHA);
-            double beta = parseDoubleParam(settings, "beta", DEFAULT_BETA);
+            double alpha = parseDoubleParam(settings, "alpha", 0.3);
+            double beta = parseDoubleParam(settings, "beta", 0.1);
             checkUnrecognizedParams(settings);
             return new HoltLinearModel(alpha, beta);
         }
@@ -235,8 +194,8 @@ public class HoltLinearModel extends MovAvgModel {
     public static class HoltLinearModelBuilder implements MovAvgModelBuilder {
 
 
-        private double alpha = DEFAULT_ALPHA;
-        private double beta = DEFAULT_BETA;
+        private Double alpha;
+        private Double beta;
 
         /**
          * Alpha controls the smoothing of the data.  Alpha = 1 retains no memory of past values
@@ -268,17 +227,18 @@ public class HoltLinearModel extends MovAvgModel {
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
             builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
             builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-            builder.field("alpha", alpha);
-            builder.field("beta", beta);
+
+            if (alpha != null) {
+                builder.field("alpha", alpha);
+            }
+
+            if (beta != null) {
+                builder.field("beta", beta);
+            }
 
             builder.endObject();
             return builder;
         }
-
-        @Override
-        public MovAvgModel build() {
-            return new HoltLinearModel(alpha, beta);
-        }
     }
 }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java
index be8d2fb..55cf6be 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java
@@ -37,7 +37,6 @@ import java.util.Arrays;
 import java.util.Collection;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Calculate a triple exponential weighted moving average
@@ -45,13 +44,6 @@ import java.util.Objects;
 public class HoltWintersModel extends MovAvgModel {
 
     protected static final ParseField NAME_FIELD = new ParseField("holt_winters");
-    public static final double DEFAULT_ALPHA = 0.3;
-    public static final double DEFAULT_BETA = 0.1;
-    public static final double DEFAULT_GAMMA = 0.3;
-    public static final int DEFAULT_PERIOD = 1;
-    public static final SeasonalityType DEFAULT_SEASONALITY_TYPE = SeasonalityType.ADDITIVE;
-    public static final boolean DEFAULT_PAD = false;
-    private static final HoltWintersModel PROTOTYPE = new HoltWintersModel();
 
     /**
      * Controls smoothing of data.  Also known as "level" value.
@@ -167,9 +159,6 @@ public class HoltWintersModel extends MovAvgModel {
         }
     }
 
-    public HoltWintersModel() {
-        this(DEFAULT_ALPHA, DEFAULT_BETA, DEFAULT_GAMMA, DEFAULT_PERIOD, DEFAULT_SEASONALITY_TYPE, DEFAULT_PAD);
-    }
 
     public HoltWintersModel(double alpha, double beta, double gamma, int period, SeasonalityType seasonalityType, boolean pad) {
         this.alpha = alpha;
@@ -284,8 +273,8 @@ public class HoltWintersModel extends MovAvgModel {
             s += vs[i];
             b += (vs[i + period] - vs[i]) / period;
         }
-        s /= period;
-        b /= period;
+        s /= (double) period;
+        b /= (double) period;
         last_s = s;
 
         // Calculate first seasonal
@@ -335,7 +324,14 @@ public class HoltWintersModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            double alpha = in.readDouble();
+            double beta = in.readDouble();
+            double gamma = in.readDouble();
+            int period = in.readVInt();
+            SeasonalityType type = SeasonalityType.readFrom(in);
+            boolean pad = in.readBoolean();
+
+            return new HoltWintersModel(alpha, beta, gamma, period, type, pad);
         }
 
         @Override
@@ -345,26 +341,6 @@ public class HoltWintersModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-        builder.field("alpha", alpha);
-        builder.field("beta", beta);
-        builder.field("gamma", gamma);
-        builder.field("period", period);
-        builder.field("pad", pad);
-        builder.field("type", seasonalityType.getName());
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new HoltWintersModel(in.readDouble(), in.readDouble(), in.readDouble(), in.readVInt(), SeasonalityType.readFrom(in),
-                in.readBoolean());
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
         out.writeDouble(alpha);
@@ -375,28 +351,6 @@ public class HoltWintersModel extends MovAvgModel {
         out.writeBoolean(pad);
     }
 
-    @Override
-    public int hashCode() {
-        return Objects.hash(alpha, beta, gamma, period, seasonalityType, pad);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        HoltWintersModel other = (HoltWintersModel) obj;
-        return Objects.equals(alpha, other.alpha)
-                && Objects.equals(beta, other.beta)
-                && Objects.equals(gamma, other.gamma)
-                && Objects.equals(period, other.period)
-                && Objects.equals(seasonalityType, other.seasonalityType)
-                && Objects.equals(pad, other.pad);
-    }
-
     public static class HoltWintersModelParser extends AbstractModelParser {
 
         @Override
@@ -408,10 +362,10 @@ public class HoltWintersModel extends MovAvgModel {
         public MovAvgModel parse(@Nullable Map<String, Object> settings, String pipelineName, int windowSize,
                                  ParseFieldMatcher parseFieldMatcher) throws ParseException {
 
-            double alpha = parseDoubleParam(settings, "alpha", DEFAULT_ALPHA);
-            double beta = parseDoubleParam(settings, "beta", DEFAULT_BETA);
-            double gamma = parseDoubleParam(settings, "gamma", DEFAULT_GAMMA);
-            int period = parseIntegerParam(settings, "period", DEFAULT_PERIOD);
+            double alpha = parseDoubleParam(settings, "alpha", 0.3);
+            double beta = parseDoubleParam(settings, "beta", 0.1);
+            double gamma = parseDoubleParam(settings, "gamma", 0.3);
+            int period = parseIntegerParam(settings, "period", 1);
 
             if (windowSize < 2 * period) {
                 throw new ParseException("Field [window] must be at least twice as large as the period when " +
@@ -419,7 +373,7 @@ public class HoltWintersModel extends MovAvgModel {
                         + (2 * period), 0);
             }
 
-            SeasonalityType seasonalityType = DEFAULT_SEASONALITY_TYPE;
+            SeasonalityType seasonalityType = SeasonalityType.ADDITIVE;
 
             if (settings != null) {
                 Object value = settings.get("type");
@@ -443,12 +397,12 @@ public class HoltWintersModel extends MovAvgModel {
 
     public static class HoltWintersModelBuilder implements MovAvgModelBuilder {
 
-        private double alpha = DEFAULT_ALPHA;
-        private double beta = DEFAULT_BETA;
-        private double gamma = DEFAULT_GAMMA;
-        private int period = DEFAULT_PERIOD;
-        private SeasonalityType seasonalityType = DEFAULT_SEASONALITY_TYPE;
-        private Boolean pad = null;
+        private Double alpha;
+        private Double beta;
+        private Double gamma;
+        private Integer period;
+        private SeasonalityType seasonalityType;
+        private Boolean pad;
 
         /**
          * Alpha controls the smoothing of the data.  Alpha = 1 retains no memory of past values
@@ -500,24 +454,34 @@ public class HoltWintersModel extends MovAvgModel {
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
             builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
             builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-            builder.field("alpha", alpha);
-            builder.field("beta", beta);
-            builder.field("gamma", gamma);
-            builder.field("period", period);
+
+            if (alpha != null) {
+                builder.field("alpha", alpha);
+            }
+
+            if (beta != null) {
+                builder.field("beta", beta);
+            }
+
+            if (gamma != null) {
+                builder.field("gamma", gamma);
+            }
+
+            if (period != null) {
+                builder.field("period", period);
+            }
+
             if (pad != null) {
                 builder.field("pad", pad);
             }
-            builder.field("type", seasonalityType.getName());
+
+            if (seasonalityType != null) {
+                builder.field("type", seasonalityType.getName());
+            }
 
             builder.endObject();
             return builder;
         }
-
-        @Override
-        public MovAvgModel build() {
-            boolean pad = this.pad == null ? (seasonalityType == SeasonalityType.MULTIPLICATIVE) : this.pad;
-            return new HoltWintersModel(alpha, beta, gamma, period, seasonalityType, pad);
-        }
     }
 }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java
index bc3de8b..264a425 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java
@@ -40,7 +40,6 @@ import java.util.Map;
  */
 public class LinearModel extends MovAvgModel {
 
-    private static final LinearModel PROTOTYPE = new LinearModel();
     protected static final ParseField NAME_FIELD = new ParseField("linear");
 
 
@@ -86,7 +85,7 @@ public class LinearModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            return new LinearModel();
         }
 
         @Override
@@ -96,17 +95,6 @@ public class LinearModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new LinearModel();
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
     }
@@ -132,26 +120,5 @@ public class LinearModel extends MovAvgModel {
             builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
             return builder;
         }
-
-        @Override
-        public MovAvgModel build() {
-            return new LinearModel();
-        }
-    }
-
-    @Override
-    public int hashCode() {
-        return 0;
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        return true;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java
index 92f4615..4bfac9d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java
@@ -22,8 +22,6 @@ package org.elasticsearch.search.aggregations.pipeline.movavg.models;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
 
 import java.io.IOException;
 import java.text.ParseException;
@@ -31,7 +29,7 @@ import java.util.Arrays;
 import java.util.Collection;
 import java.util.Map;
 
-public abstract class MovAvgModel implements Writeable<MovAvgModel>, ToXContent {
+public abstract class MovAvgModel {
 
     /**
      * Should this model be fit to the data via a cost minimizing algorithm by default?
@@ -118,21 +116,13 @@ public abstract class MovAvgModel implements Writeable<MovAvgModel>, ToXContent
      *
      * @param out   Output stream
      */
-    @Override
     public abstract void writeTo(StreamOutput out) throws IOException;
 
     /**
      * Clone the model, returning an exact copy
      */
-    @Override
     public abstract MovAvgModel clone();
 
-    @Override
-    public abstract int hashCode();
-
-    @Override
-    public abstract boolean equals(Object obj);
-
     /**
      * Abstract class which also provides some concrete parsing functionality.
      */
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelBuilder.java
index 759c493..e491d12 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModelBuilder.java
@@ -26,6 +26,4 @@ import org.elasticsearch.common.xcontent.ToXContent;
  * average models are used by the MovAvg aggregation
  */
 public interface MovAvgModelBuilder extends ToXContent {
-
-    public MovAvgModel build();
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java
index 61f8c66..e0c7781 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java
@@ -38,7 +38,6 @@ import java.util.Map;
  */
 public class SimpleModel extends MovAvgModel {
 
-    private static final SimpleModel PROTOTYPE = new SimpleModel();
     protected static final ParseField NAME_FIELD = new ParseField("simple");
 
 
@@ -79,7 +78,7 @@ public class SimpleModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            return new SimpleModel();
         }
 
         @Override
@@ -89,17 +88,6 @@ public class SimpleModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new SimpleModel();
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
     }
@@ -125,26 +113,5 @@ public class SimpleModel extends MovAvgModel {
             builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
             return builder;
         }
-
-        @Override
-        public MovAvgModel build() {
-            return new SimpleModel();
-        }
-    }
-
-    @Override
-    public int hashCode() {
-        return 0;
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        return true;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffBuilder.java
new file mode 100644
index 0000000..052f3f0
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffBuilder.java
@@ -0,0 +1,67 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.pipeline.serialdiff;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilder;
+
+import java.io.IOException;
+
+public class SerialDiffBuilder extends PipelineAggregatorBuilder<SerialDiffBuilder> {
+
+    private String format;
+    private GapPolicy gapPolicy;
+    private Integer lag;
+
+    public SerialDiffBuilder(String name) {
+        super(name, SerialDiffPipelineAggregator.TYPE.name());
+    }
+
+    public SerialDiffBuilder format(String format) {
+        this.format = format;
+        return this;
+    }
+
+    public SerialDiffBuilder gapPolicy(GapPolicy gapPolicy) {
+        this.gapPolicy = gapPolicy;
+        return this;
+    }
+
+    public SerialDiffBuilder lag(Integer lag) {
+        this.lag = lag;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (format != null) {
+            builder.field(SerialDiffParser.FORMAT.getPreferredName(), format);
+        }
+        if (gapPolicy != null) {
+            builder.field(SerialDiffParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
+        }
+        if (lag != null) {
+            builder.field(SerialDiffParser.LAG.getPreferredName(), lag);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java
index fd9abc2..109cbcc 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java
@@ -20,17 +20,20 @@
 package org.elasticsearch.search.aggregations.pipeline.serialdiff;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+
 public class SerialDiffParser implements PipelineAggregator.Parser {
 
     public static final ParseField FORMAT = new ParseField("format");
@@ -43,13 +46,13 @@ public class SerialDiffParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, QueryParseContext context) throws IOException {
+    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
         String format = null;
-        GapPolicy gapPolicy = null;
-        Integer lag = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
+        int lag = 1;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -62,21 +65,20 @@ public class SerialDiffParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, GAP_POLICY)) {
                     gapPolicy = GapPolicy.parse(context, parser.text(), parser.getTokenLocation());
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.VALUE_NUMBER) {
                 if (context.parseFieldMatcher().match(currentFieldName, LAG)) {
                     lag = parser.intValue(true);
                     if (lag <= 0) {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Lag must be a positive, non-zero integer.  Value supplied was" +
+                        throw new SearchParseException(context, "Lag must be a positive, non-zero integer.  Value supplied was" +
                                 lag + " in [" + reducerName + "]: ["
-                                        + currentFieldName + "].");
+                                + currentFieldName + "].", parser.getTokenLocation());
                     }
                 }  else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -87,36 +89,28 @@ public class SerialDiffParser implements PipelineAggregator.Parser {
                     }
                     bucketsPaths = paths.toArray(new String[paths.size()]);
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + reducerName + "].",
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + reducerName + "].",
                         parser.getTokenLocation());
             }
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Missing required field [" + BUCKETS_PATH.getPreferredName() + "] for derivative aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for derivative aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
-        SerialDiffPipelineAggregator.Factory factory = new SerialDiffPipelineAggregator.Factory(reducerName, bucketsPaths[0]);
-        if (lag != null) {
-            factory.lag(lag);
-        }
+        ValueFormatter formatter;
         if (format != null) {
-            factory.format(format);
-        }
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        }  else {
+            formatter = ValueFormatter.RAW;
         }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new SerialDiffPipelineAggregator.Factory(null, null);
+        return new SerialDiffPipelineAggregator.Factory(reducerName, bucketsPaths, formatter, gapPolicy, lag);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
index 1bf4f82..5df97d3 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
@@ -23,18 +23,15 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.InternalAggregations;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -42,10 +39,10 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;
 
 public class SerialDiffPipelineAggregator extends PipelineAggregator {
@@ -147,112 +144,20 @@ public class SerialDiffPipelineAggregator extends PipelineAggregator {
 
     public static class Factory extends PipelineAggregatorFactory {
 
-        private String format;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
-        private int lag = 1;
+        private final ValueFormatter formatter;
+        private GapPolicy gapPolicy;
+        private int lag;
 
-        public Factory(String name, String bucketsPath) {
-            this(name, new String[] { bucketsPath });
-        }
-
-        private Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, @Nullable ValueFormatter formatter, GapPolicy gapPolicy, int lag) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Sets the lag to use when calculating the serial difference.
-         */
-        public Factory lag(int lag) {
-            this.lag = lag;
-            return this;
-        }
-
-        /**
-         * Gets the lag to use when calculating the serial difference.
-         */
-        public int lag() {
-            return lag;
-        }
-
-        /**
-         * Sets the format to use on the output of this aggregation.
-         */
-        public Factory format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        /**
-         * Gets the format to use on the output of this aggregation.
-         */
-        public String format() {
-            return format;
-        }
-
-        /**
-         * Sets the GapPolicy to use on the output of this aggregation.
-         */
-        public Factory gapPolicy(GapPolicy gapPolicy) {
+            this.formatter = formatter;
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        /**
-         * Gets the GapPolicy to use on the output of this aggregation.
-         */
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
-        }
-
-        protected ValueFormatter formatter() {
-            if (format != null) {
-                return ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                return ValueFormatter.RAW;
-            }
+            this.lag = lag;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new SerialDiffPipelineAggregator(name, bucketsPaths, formatter(), gapPolicy, lag, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (format != null) {
-                builder.field(SerialDiffParser.FORMAT.getPreferredName(), format);
-            }
-            builder.field(SerialDiffParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            builder.field(SerialDiffParser.LAG.getPreferredName(), lag);
-            return builder;
-        }
-
-        @Override
-        protected PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.format = in.readOptionalString();
-            factory.gapPolicy = GapPolicy.readFrom(in);
-            factory.lag = in.readVInt();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(format);
-            gapPolicy.writeTo(out);
-            out.writeVInt(lag);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(format, gapPolicy, lag);
-        }
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(format, other.format)
-                    && Objects.equals(gapPolicy, other.gapPolicy)
-                    && Objects.equals(lag, other.lag);
+            return new SerialDiffPipelineAggregator(name, bucketsPaths, formatter, gapPolicy, lag, metaData);
         }
 
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/AbstractValuesSourceParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/AbstractValuesSourceParser.java
deleted file mode 100644
index e7575a4..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/AbstractValuesSourceParser.java
+++ /dev/null
@@ -1,209 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.support;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.Script.ScriptField;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.joda.time.DateTimeZone;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- *
- */
-public abstract class AbstractValuesSourceParser<VS extends ValuesSource>
-        implements Aggregator.Parser {
-    static final ParseField TIME_ZONE = new ParseField("time_zone");
-
-    public abstract static class AnyValuesSourceParser extends AbstractValuesSourceParser<ValuesSource> {
-
-        protected AnyValuesSourceParser(boolean scriptable, boolean formattable) {
-            super(scriptable, formattable, false, ValuesSourceType.ANY, null);
-        }
-    }
-
-    public abstract static class NumericValuesSourceParser extends AbstractValuesSourceParser<ValuesSource.Numeric> {
-
-        protected NumericValuesSourceParser(boolean scriptable, boolean formattable, boolean timezoneAware) {
-            super(scriptable, formattable, timezoneAware, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-        }
-    }
-
-    public abstract static class BytesValuesSourceParser extends AbstractValuesSourceParser<ValuesSource.Bytes> {
-
-        protected BytesValuesSourceParser(boolean scriptable, boolean formattable) {
-            super(scriptable, formattable, false, ValuesSourceType.BYTES, ValueType.STRING);
-        }
-    }
-
-    public abstract static class GeoPointValuesSourceParser extends AbstractValuesSourceParser<ValuesSource.GeoPoint> {
-
-        protected GeoPointValuesSourceParser(boolean scriptable, boolean formattable) {
-            super(scriptable, formattable, false, ValuesSourceType.GEOPOINT, ValueType.GEOPOINT);
-        }
-    }
-
-    private boolean scriptable = true;
-    private boolean formattable = false;
-    private boolean timezoneAware = false;
-    private ValuesSourceType valuesSourceType = null;
-    private ValueType targetValueType = null;
-
-    private AbstractValuesSourceParser(boolean scriptable, boolean formattable, boolean timezoneAware, ValuesSourceType valuesSourceType,
-            ValueType targetValueType) {
-        this.timezoneAware = timezoneAware;
-        this.valuesSourceType = valuesSourceType;
-        this.targetValueType = targetValueType;
-        this.scriptable = scriptable;
-        this.formattable = formattable;
-    }
-
-    @Override
-    public final ValuesSourceAggregatorBuilder<VS, ?> parse(String aggregationName, XContentParser parser, QueryParseContext context)
-            throws IOException {
-
-        String field = null;
-        Script script = null;
-        ValueType valueType = null;
-        String format = null;
-        Object missing = null;
-        DateTimeZone timezone = null;
-        Map<ParseField, Object> otherOptions = new HashMap<>();
-
-        XContentParser.Token token;
-        String currentFieldName = null;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if ("missing".equals(currentFieldName) && token.isValue()) {
-                missing = parser.objectText();
-            } else if (timezoneAware && context.parseFieldMatcher().match(currentFieldName, TIME_ZONE)) {
-                if (token == XContentParser.Token.VALUE_STRING) {
-                    timezone = DateTimeZone.forID(parser.text());
-                } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                    timezone = DateTimeZone.forOffsetHours(parser.intValue());
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-                }
-            } else if (token == XContentParser.Token.VALUE_STRING) {
-                if ("field".equals(currentFieldName)) {
-                    field = parser.text();
-                } else if (formattable && "format".equals(currentFieldName)) {
-                    format = parser.text();
-                } else if (scriptable) {
-                    if ("value_type".equals(currentFieldName) || "valueType".equals(currentFieldName)) {
-                        valueType = ValueType.resolveForScript(parser.text());
-                        if (targetValueType != null && valueType.isNotA(targetValueType)) {
-                            throw new ParsingException(parser.getTokenLocation(),
-                                    type() + " aggregation [" + aggregationName + "] was configured with an incompatible value type ["
-                                            + valueType + "]. [" + type() + "] aggregation can only work on value of type ["
-                                            + targetValueType + "]");
-                        }
-                    } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-                    }
-                } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-                }
-            } else if (scriptable && token == XContentParser.Token.START_OBJECT) {
-                if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
-                    script = Script.parse(parser, context.parseFieldMatcher());
-                } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-                }
-            } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-            }
-        }
-
-        ValuesSourceAggregatorBuilder<VS, ?> factory = createFactory(aggregationName, this.valuesSourceType, this.targetValueType,
-                otherOptions);
-        factory.field(field);
-        factory.script(script);
-        factory.valueType(valueType);
-        factory.format(format);
-        factory.missing(missing);
-        factory.timeZone(timezone);
-        return factory;
-    }
-
-    /**
-     * Creates a {@link ValuesSourceAggregatorBuilder} from the information
-     * gathered by the subclass. Options parsed in
-     * {@link AbstractValuesSourceParser} itself will be added to the factory
-     * after it has been returned by this method.
-     *
-     * @param aggregationName
-     *            the name of the aggregation
-     * @param valuesSourceType
-     *            the type of the {@link ValuesSource}
-     * @param targetValueType
-     *            the target type of the final value output by the aggregation
-     * @param otherOptions
-     *            a {@link Map} containing the extra options parsed by the
-     *            {@link #token(String, String, org.elasticsearch.common.xcontent.XContentParser.Token,
-     *             XContentParser, ParseFieldMatcher, Map)}
-     *            method
-     * @return the created factory
-     */
-    protected abstract ValuesSourceAggregatorBuilder<VS, ?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions);
-
-    /**
-     * Allows subclasses of {@link AbstractValuesSourceParser} to parse extra
-     * parameters and store them in a {@link Map} which will later be passed to
-     * {@link #createFactory(String, ValuesSourceType, ValueType, Map)}.
-     *
-     * @param aggregationName
-     *            the name of the aggregation
-     * @param currentFieldName
-     *            the name of the current field being parsed
-     * @param token
-     *            the current token for the parser
-     * @param parser
-     *            the parser
-     * @param parseFieldMatcher
-     *            the {@link ParseFieldMatcher} to use to match field names
-     * @param otherOptions
-     *            a {@link Map} of options to be populated by successive calls
-     *            to this method which will then be passed to the
-     *            {@link #createFactory(String, ValuesSourceType, ValueType, Map)}
-     *            method
-     * @return <code>true</code> if the current token was correctly parsed,
-     *         <code>false</code> otherwise
-     * @throws IOException
-     *             if an error occurs whilst parsing
-     */
-    protected abstract boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException;
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java
index 14e8481..ee91782 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java
@@ -70,11 +70,13 @@ public class AggregationContext {
             if (config.missing == null) {
                 // otherwise we will have values because of the missing value
                 vs = null;
-            } else if (config.valueSourceType == ValuesSourceType.NUMERIC) {
+            } else if (ValuesSource.Numeric.class.isAssignableFrom(config.valueSourceType)) {
                 vs = (VS) ValuesSource.Numeric.EMPTY;
-            } else if (config.valueSourceType == ValuesSourceType.GEOPOINT) {
+            } else if (ValuesSource.GeoPoint.class.isAssignableFrom(config.valueSourceType)) {
                 vs = (VS) ValuesSource.GeoPoint.EMPTY;
-            } else if (config.valueSourceType == ValuesSourceType.ANY || config.valueSourceType == ValuesSourceType.BYTES) {
+            } else if (ValuesSource.class.isAssignableFrom(config.valueSourceType)
+                    || ValuesSource.Bytes.class.isAssignableFrom(config.valueSourceType)
+                    || ValuesSource.Bytes.WithOrdinals.class.isAssignableFrom(config.valueSourceType)) {
                 vs = (VS) ValuesSource.Bytes.EMPTY;
             } else {
                 throw new SearchParseException(searchContext, "Can't deal with unmapped ValuesSource type " + config.valueSourceType, null);
@@ -130,20 +132,19 @@ public class AggregationContext {
      */
     private <VS extends ValuesSource> VS originalValuesSource(ValuesSourceConfig<VS> config) throws IOException {
         if (config.fieldContext == null) {
-            if (config.valueSourceType == ValuesSourceType.NUMERIC) {
+            if (ValuesSource.Numeric.class.isAssignableFrom(config.valueSourceType)) {
                 return (VS) numericScript(config);
             }
-            if (config.valueSourceType == ValuesSourceType.BYTES) {
+            if (ValuesSource.Bytes.class.isAssignableFrom(config.valueSourceType)) {
                 return (VS) bytesScript(config);
             }
-            throw new AggregationExecutionException("value source of type [" + config.valueSourceType.name()
-                    + "] is not supported by scripts");
+            throw new AggregationExecutionException("value source of type [" + config.valueSourceType.getSimpleName() + "] is not supported by scripts");
         }
 
-        if (config.valueSourceType == ValuesSourceType.NUMERIC) {
+        if (ValuesSource.Numeric.class.isAssignableFrom(config.valueSourceType)) {
             return (VS) numericField(config);
         }
-        if (config.valueSourceType == ValuesSourceType.GEOPOINT) {
+        if (ValuesSource.GeoPoint.class.isAssignableFrom(config.valueSourceType)) {
             return (VS) geoPointField(config);
         }
         // falling back to bytes values
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java
index fd2f363..3dfab20 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java
@@ -19,39 +19,41 @@
 
 package org.elasticsearch.search.aggregations.support;
 
-
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  *
  */
 public class GeoPointParser {
 
+    private final String aggName;
     private final InternalAggregation.Type aggType;
+    private final SearchContext context;
     private final ParseField field;
 
-    public GeoPointParser(InternalAggregation.Type aggType, ParseField field) {
+    GeoPoint point;
+
+    public GeoPointParser(String aggName, InternalAggregation.Type aggType, SearchContext context, ParseField field) {
+        this.aggName = aggName;
         this.aggType = aggType;
+        this.context = context;
         this.field = field;
     }
 
-    public boolean token(String aggName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (!parseFieldMatcher.match(currentFieldName, field)) {
+    public boolean token(String currentFieldName, XContentParser.Token token, XContentParser parser) throws IOException {
+        if (!context.parseFieldMatcher().match(currentFieldName, field)) {
             return false;
         }
         if (token == XContentParser.Token.VALUE_STRING) {
-            GeoPoint point = new GeoPoint();
+            point = new GeoPoint();
             point.resetFromString(parser.text());
-            otherOptions.put(field, point);
             return true;
         }
         if (token == XContentParser.Token.START_ARRAY) {
@@ -63,12 +65,12 @@ public class GeoPointParser {
                 } else if (Double.isNaN(lat)) {
                     lat = parser.doubleValue();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(), "malformed [" + currentFieldName + "] geo point array in ["
-                            + aggName + "] " + aggType + " aggregation. a geo point array must be of the form [lon, lat]");
+                    throw new SearchParseException(context, "malformed [" + currentFieldName + "] geo point array in [" +
+                            aggName + "] " + aggType + " aggregation. a geo point array must be of the form [lon, lat]", 
+                            parser.getTokenLocation());
                 }
             }
-            GeoPoint point = new GeoPoint(lat, lon);
-            otherOptions.put(field, point);
+            point = new GeoPoint(lat, lon);
             return true;
         }
         if (token == XContentParser.Token.START_OBJECT) {
@@ -86,15 +88,17 @@ public class GeoPointParser {
                 }
             }
             if (Double.isNaN(lat) || Double.isNaN(lon)) {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "malformed [" + currentFieldName + "] geo point object. either [lat] or [lon] (or both) are " + "missing in ["
-                                + aggName + "] " + aggType + " aggregation");
+                throw new SearchParseException(context, "malformed [" + currentFieldName + "] geo point object. either [lat] or [lon] (or both) are " +
+                        "missing in [" + aggName + "] " + aggType + " aggregation", parser.getTokenLocation());
             }
-            GeoPoint point = new GeoPoint(lat, lon);
-            otherOptions.put(field, point);
+            point = new GeoPoint(lat, lon);
             return true;
         }
         return false;
     }
 
+    public GeoPoint geoPoint() {
+        return point;
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java
index 688e447..0ae99b4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java
@@ -19,35 +19,26 @@
 
 package org.elasticsearch.search.aggregations.support;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.index.fielddata.IndexFieldData;
 import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
 import org.elasticsearch.index.fielddata.IndexNumericFieldData;
 import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 
-import java.io.IOException;
-
 /**
  *
  */
-public enum ValueType implements Writeable<ValueType> {
+public enum ValueType {
 
     @Deprecated
-    ANY((byte) 0, "any", "any", ValuesSourceType.ANY, IndexFieldData.class, ValueFormat.RAW), 
-    STRING((byte) 1, "string", "string", ValuesSourceType.BYTES,
-            IndexFieldData.class,
+    ANY("any", ValuesSource.class, IndexFieldData.class, ValueFormat.RAW), STRING("string", ValuesSource.Bytes.class, IndexFieldData.class,
             ValueFormat.RAW),
-    LONG((byte) 2, "byte|short|integer|long", "long",
-                    ValuesSourceType.NUMERIC,
-            IndexNumericFieldData.class, ValueFormat.RAW) {
+    LONG("byte|short|integer|long", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    DOUBLE((byte) 3, "float|double", "double", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.RAW) {
+    DOUBLE("float|double", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isNumeric() {
             return true;
@@ -58,31 +49,31 @@ public enum ValueType implements Writeable<ValueType> {
             return true;
         }
     },
-    NUMBER((byte) 4, "number", "number", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.RAW) {
+    NUMBER("number", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    DATE((byte) 5, "date", "date", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.DateTime.DEFAULT) {
+    DATE("date", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.DateTime.DEFAULT) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    IP((byte) 6, "ip", "ip", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.IPv4) {
+    IP("ip", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.IPv4) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    NUMERIC((byte) 7, "numeric", "numeric", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.RAW) {
+    NUMERIC("numeric", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    GEOPOINT((byte) 8, "geo_point", "geo_point", ValuesSourceType.GEOPOINT, IndexGeoPointFieldData.class, ValueFormat.RAW) {
+    GEOPOINT("geo_point", ValuesSource.GeoPoint.class, IndexGeoPointFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isGeoPoint() {
             return true;
@@ -90,17 +81,12 @@ public enum ValueType implements Writeable<ValueType> {
     };
 
     final String description;
-    final ValuesSourceType valuesSourceType;
+    final Class<? extends ValuesSource> valuesSourceType;
     final Class<? extends IndexFieldData> fieldDataType;
     final ValueFormat defaultFormat;
-    private final byte id;
-    private String preferredName;
 
-    private ValueType(byte id, String description, String preferredName, ValuesSourceType valuesSourceType, Class<? extends IndexFieldData> fieldDataType,
-            ValueFormat defaultFormat) {
-        this.id = id;
+    private ValueType(String description, Class<? extends ValuesSource> valuesSourceType, Class<? extends IndexFieldData> fieldDataType, ValueFormat defaultFormat) {
         this.description = description;
-        this.preferredName = preferredName;
         this.valuesSourceType = valuesSourceType;
         this.fieldDataType = fieldDataType;
         this.defaultFormat = defaultFormat;
@@ -110,11 +96,7 @@ public enum ValueType implements Writeable<ValueType> {
         return description;
     }
 
-    public String getPreferredName() {
-        return preferredName;
-    }
-    
-    public ValuesSourceType getValuesSourceType() {
+    public Class<? extends ValuesSource> getValuesSourceType() {
         return valuesSourceType;
     }
 
@@ -123,7 +105,7 @@ public enum ValueType implements Writeable<ValueType> {
     }
 
     public boolean isA(ValueType valueType) {
-        return valueType.valuesSourceType == valuesSourceType &&
+        return valueType.valuesSourceType.isAssignableFrom(valuesSourceType) &&
                 valueType.fieldDataType.isAssignableFrom(fieldDataType);
     }
 
@@ -167,20 +149,4 @@ public enum ValueType implements Writeable<ValueType> {
     public String toString() {
         return description;
     }
-
-    @Override
-    public ValueType readFrom(StreamInput in) throws IOException {
-        byte id = in.readByte();
-        for (ValueType valueType : values()) {
-            if (id == valueType.id) {
-                return valueType;
-            }
-        }
-        throw new IOException("No valueType found for id [" + id + "]");
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeByte(id);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorBuilder.java
deleted file mode 100644
index 4a84507..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorBuilder.java
+++ /dev/null
@@ -1,424 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations.support;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
-import org.elasticsearch.index.fielddata.IndexNumericFieldData;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.core.BooleanFieldMapper;
-import org.elasticsearch.index.mapper.core.DateFieldMapper;
-import org.elasticsearch.index.mapper.core.NumberFieldMapper;
-import org.elasticsearch.index.mapper.ip.IpFieldMapper;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.SearchScript;
-import org.elasticsearch.search.aggregations.AggregationInitializationException;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories.Builder;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
-import org.elasticsearch.search.internal.SearchContext;
-import org.joda.time.DateTimeZone;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-import java.util.Objects;
-
-/**
- *
- */
-public abstract class ValuesSourceAggregatorBuilder<VS extends ValuesSource, AB extends ValuesSourceAggregatorBuilder<VS, AB>>
-        extends AggregatorBuilder<AB> {
-
-    public static abstract class LeafOnly<VS extends ValuesSource, AB extends ValuesSourceAggregatorBuilder<VS, AB>>
-            extends ValuesSourceAggregatorBuilder<VS, AB> {
-
-        protected LeafOnly(String name, Type type, ValuesSourceType valuesSourceType, ValueType targetValueType) {
-            super(name, type, valuesSourceType, targetValueType);
-        }
-
-        @Override
-        public AB subAggregations(Builder subFactories) {
-            throw new AggregationInitializationException("Aggregator [" + name + "] of type [" + type + "] cannot accept sub-aggregations");
-        }
-    }
-
-    private final ValuesSourceType valuesSourceType;
-    private final ValueType targetValueType;
-    private String field = null;
-    private Script script = null;
-    private ValueType valueType = null;
-    private String format = null;
-    private Object missing = null;
-    private DateTimeZone timeZone;
-    protected ValuesSourceConfig<VS> config;
-
-    protected ValuesSourceAggregatorBuilder(String name, Type type, ValuesSourceType valuesSourceType, ValueType targetValueType) {
-        super(name, type);
-        this.valuesSourceType = valuesSourceType;
-        this.targetValueType = targetValueType;
-    }
-
-    /**
-     * Sets the field to use for this aggregation.
-     */
-    public AB field(String field) {
-        this.field = field;
-        return (AB) this;
-    }
-
-    /**
-     * Gets the field to use for this aggregation.
-     */
-    public String field() {
-        return field;
-    }
-
-    /**
-     * Sets the script to use for this aggregation.
-     */
-    public AB script(Script script) {
-        this.script = script;
-        return (AB) this;
-    }
-
-    /**
-     * Gets the script to use for this aggregation.
-     */
-    public Script script() {
-        return script;
-    }
-
-    /**
-     * Sets the {@link ValueType} for the value produced by this aggregation
-     */
-    public AB valueType(ValueType valueType) {
-        this.valueType = valueType;
-        return (AB) this;
-    }
-
-    /**
-     * Gets the {@link ValueType} for the value produced by this aggregation
-     */
-    public ValueType valueType() {
-        return valueType;
-    }
-
-    /**
-     * Sets the format to use for the output of the aggregation.
-     */
-    public AB format(String format) {
-        this.format = format;
-        return (AB) this;
-    }
-
-    /**
-     * Gets the format to use for the output of the aggregation.
-     */
-    public String format() {
-        return format;
-    }
-
-    /**
-     * Sets the value to use when the aggregation finds a missing value in a
-     * document
-     */
-    public AB missing(Object missing) {
-        this.missing = missing;
-        return (AB) this;
-    }
-
-    /**
-     * Gets the value to use when the aggregation finds a missing value in a
-     * document
-     */
-    public Object missing() {
-        return missing;
-    }
-
-    /**
-     * Sets the time zone to use for this aggregation
-     */
-    public AB timeZone(DateTimeZone timeZone) {
-        this.timeZone = timeZone;
-        return (AB) this;
-    }
-
-    /**
-     * Gets the time zone to use for this aggregation
-     */
-    public DateTimeZone timeZone() {
-        return timeZone;
-    }
-
-    @Override
-    protected final ValuesSourceAggregatorFactory<VS, ?> doBuild(AggregationContext context, AggregatorFactory<?> parent,
-            AggregatorFactories.Builder subFactoriesBuilder) throws IOException {
-        ValuesSourceConfig<VS> config = resolveConfig(context);
-        ValuesSourceAggregatorFactory<VS, ?> factory = innerBuild(context, config, parent, subFactoriesBuilder);
-        return factory;
-    }
-
-    protected ValuesSourceConfig<VS> resolveConfig(AggregationContext context) {
-        ValuesSourceConfig<VS> config = config(context);
-        return config;
-    }
-
-    protected abstract ValuesSourceAggregatorFactory<VS, ?> innerBuild(AggregationContext context, ValuesSourceConfig<VS> config,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder) throws IOException;
-
-    public ValuesSourceConfig<VS> config(AggregationContext context) {
-
-        ValueType valueType = this.valueType != null ? this.valueType : targetValueType;
-
-        if (field == null) {
-            if (script == null) {
-                ValuesSourceConfig<VS> config = new ValuesSourceConfig(ValuesSourceType.ANY);
-                config.format = resolveFormat(null, valueType);
-                return config;
-            }
-            ValuesSourceType valuesSourceType = valueType != null ? valueType.getValuesSourceType() : this.valuesSourceType;
-            if (valuesSourceType == null || valuesSourceType == ValuesSourceType.ANY) {
-                // the specific value source type is undefined, but for scripts,
-                // we need to have a specific value source
-                // type to know how to handle the script values, so we fallback
-                // on Bytes
-                valuesSourceType = ValuesSourceType.BYTES;
-            }
-            ValuesSourceConfig<VS> config = new ValuesSourceConfig<VS>(valuesSourceType);
-            config.missing = missing;
-            config.timeZone = timeZone;
-            config.format = resolveFormat(format, valueType);
-            config.script = createScript(script, context.searchContext());
-            config.scriptValueType = valueType;
-            return config;
-        }
-
-        MappedFieldType fieldType = context.searchContext().smartNameFieldType(field);
-        if (fieldType == null) {
-            ValuesSourceType valuesSourceType = valueType != null ? valueType.getValuesSourceType() : this.valuesSourceType;
-            ValuesSourceConfig<VS> config = new ValuesSourceConfig<>(valuesSourceType);
-            config.missing = missing;
-            config.timeZone = timeZone;
-            config.format = resolveFormat(format, valueType);
-            config.unmapped = true;
-            if (valueType != null) {
-                // todo do we really need this for unmapped?
-                config.scriptValueType = valueType;
-            }
-            return config;
-        }
-
-        IndexFieldData<?> indexFieldData = context.searchContext().fieldData().getForField(fieldType);
-
-        ValuesSourceConfig config;
-        if (valuesSourceType == ValuesSourceType.ANY) {
-            if (indexFieldData instanceof IndexNumericFieldData) {
-                config = new ValuesSourceConfig<>(ValuesSourceType.NUMERIC);
-            } else if (indexFieldData instanceof IndexGeoPointFieldData) {
-                config = new ValuesSourceConfig<>(ValuesSourceType.GEOPOINT);
-            } else {
-                config = new ValuesSourceConfig<>(ValuesSourceType.BYTES);
-            }
-        } else {
-            config = new ValuesSourceConfig(valuesSourceType);
-        }
-
-        config.fieldContext = new FieldContext(field, indexFieldData, fieldType);
-        config.missing = missing;
-        config.timeZone = timeZone;
-        config.script = createScript(script, context.searchContext());
-        config.format = resolveFormat(format, this.timeZone, fieldType);
-        return config;
-    }
-
-    private SearchScript createScript(Script script, SearchContext context) {
-        return script == null ? null
-                : context.scriptService().search(context.lookup(), script, ScriptContext.Standard.AGGS, Collections.emptyMap());
-    }
-
-    private static ValueFormat resolveFormat(@Nullable String format, @Nullable ValueType valueType) {
-        if (valueType == null) {
-            return ValueFormat.RAW; // we can't figure it out
-        }
-        ValueFormat valueFormat = valueType.defaultFormat;
-        if (valueFormat != null && valueFormat instanceof ValueFormat.Patternable && format != null) {
-            return ((ValueFormat.Patternable) valueFormat).create(format);
-        }
-        return valueFormat;
-    }
-
-    private static ValueFormat resolveFormat(@Nullable String format, @Nullable DateTimeZone timezone, MappedFieldType fieldType) {
-        if (fieldType instanceof DateFieldMapper.DateFieldType) {
-            return format != null ? ValueFormat.DateTime.format(format, timezone) : ValueFormat.DateTime.mapper(
-                    (DateFieldMapper.DateFieldType) fieldType, timezone);
-        }
-        if (fieldType instanceof IpFieldMapper.IpFieldType) {
-            return ValueFormat.IPv4;
-        }
-        if (fieldType instanceof BooleanFieldMapper.BooleanFieldType) {
-            return ValueFormat.BOOLEAN;
-        }
-        if (fieldType instanceof NumberFieldMapper.NumberFieldType) {
-            return format != null ? ValueFormat.Number.format(format) : ValueFormat.RAW;
-        }
-        return ValueFormat.RAW;
-    }
-
-    // NORELEASE remove this method when agg refactoring is complete
-    protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-            List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        throw new UnsupportedOperationException("This should never be called");
-    }
-
-    // NORELEASE remove this method when agg refactoring is complete
-    protected Aggregator doCreateInternal(VS valuesSource, AggregationContext aggregationContext, Aggregator parent,
-            boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                    throws IOException {
-        throw new UnsupportedOperationException("This should never be called");
-    }
-
-    @Override
-    protected final void doWriteTo(StreamOutput out) throws IOException {
-        valuesSourceType.writeTo(out);
-        boolean hasTargetValueType = targetValueType != null;
-        out.writeBoolean(hasTargetValueType);
-        if (hasTargetValueType) {
-            targetValueType.writeTo(out);
-        }
-        innerWriteTo(out);
-        out.writeOptionalString(field);
-        boolean hasScript = script != null;
-        out.writeBoolean(hasScript);
-        if (hasScript) {
-            script.writeTo(out);
-        }
-        boolean hasValueType = valueType != null;
-        out.writeBoolean(hasValueType);
-        if (hasValueType) {
-            valueType.writeTo(out);
-        }
-        out.writeOptionalString(format);
-        out.writeGenericValue(missing);
-        boolean hasTimeZone = timeZone != null;
-        out.writeBoolean(hasTimeZone);
-        if (hasTimeZone) {
-            out.writeString(timeZone.getID());
-        }
-    }
-
-    protected abstract void innerWriteTo(StreamOutput out) throws IOException;
-
-    @Override
-    protected final AB doReadFrom(String name, StreamInput in) throws IOException {
-        ValuesSourceType valuesSourceType = ValuesSourceType.ANY.readFrom(in);
-        ValueType targetValueType = null;
-        if (in.readBoolean()) {
-            targetValueType = ValueType.STRING.readFrom(in);
-        }
-        ValuesSourceAggregatorBuilder<VS, AB> factory = innerReadFrom(name, valuesSourceType, targetValueType, in);
-        factory.field = in.readOptionalString();
-        if (in.readBoolean()) {
-            factory.script = Script.readScript(in);
-        }
-        if (in.readBoolean()) {
-            factory.valueType = ValueType.STRING.readFrom(in);
-        }
-        factory.format = in.readOptionalString();
-        factory.missing = in.readGenericValue();
-        if (in.readBoolean()) {
-            factory.timeZone = DateTimeZone.forID(in.readString());
-        }
-        return (AB) factory;
-    }
-
-    protected abstract ValuesSourceAggregatorBuilder<VS, AB> innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException;
-
-    @Override
-    public final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        if (field != null) {
-            builder.field("field", field);
-        }
-        if (script != null) {
-            builder.field("script", script);
-        }
-        if (missing != null) {
-            builder.field("missing", missing);
-        }
-        if (format != null) {
-            builder.field("format", format);
-        }
-        if (timeZone != null) {
-            builder.field("time_zone", timeZone);
-        }
-        if (valueType != null) {
-            builder.field("value_type", valueType.getPreferredName());
-        }
-        doXContentBody(builder, params);
-        builder.endObject();
-        return builder;
-    }
-
-    protected abstract XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException;
-
-    @Override
-    protected final int doHashCode() {
-        return Objects.hash(field, format, missing, script, targetValueType, timeZone, valueType, valuesSourceType,
-                innerHashCode());
-    }
-
-    protected abstract int innerHashCode();
-
-    @Override
-    protected final boolean doEquals(Object obj) {
-        ValuesSourceAggregatorBuilder<?, ?> other = (ValuesSourceAggregatorBuilder<?, ?>) obj;
-        if (!Objects.equals(field, other.field))
-            return false;
-        if (!Objects.equals(format, other.format))
-            return false;
-        if (!Objects.equals(missing, other.missing))
-            return false;
-        if (!Objects.equals(script, other.script))
-            return false;
-        if (!Objects.equals(targetValueType, other.targetValueType))
-            return false;
-        if (!Objects.equals(timeZone, other.timeZone))
-            return false;
-        if (!Objects.equals(valueType, other.valueType))
-            return false;
-        if (!Objects.equals(valuesSourceType, other.valuesSourceType))
-            return false;
-        return innerEquals(obj);
-    }
-
-    protected abstract boolean innerEquals(Object obj);
-}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java
index b724bdc..d0eaec2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java
@@ -16,36 +16,44 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-
 package org.elasticsearch.search.aggregations.support;
 
+import org.elasticsearch.search.aggregations.AggregationExecutionException;
+import org.elasticsearch.search.aggregations.AggregationInitializationException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.joda.time.DateTimeZone;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
-// NORELEASE this needs to extend AggregatorFactory<AF> after refactoring is complete
-public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource, AF extends ValuesSourceAggregatorFactory<VS, AF>>
-        extends AggregatorFactory<AF> {
+/**
+ *
+ */
+public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource> extends AggregatorFactory {
+
+    public static abstract class LeafOnly<VS extends ValuesSource> extends ValuesSourceAggregatorFactory<VS> {
+
+        protected LeafOnly(String name, String type, ValuesSourceConfig<VS> valuesSourceConfig) {
+            super(name, type, valuesSourceConfig);
+        }
+
+        @Override
+        public AggregatorFactory subFactories(AggregatorFactories subFactories) {
+            throw new AggregationInitializationException("Aggregator [" + name + "] of type [" + type + "] cannot accept sub-aggregations");
+        }
+    }
 
     protected ValuesSourceConfig<VS> config;
 
-    public ValuesSourceAggregatorFactory(String name, Type type, ValuesSourceConfig<VS> config, AggregationContext context,
-            AggregatorFactory<?> parent, AggregatorFactories.Builder subFactoriesBuilder, Map<String, Object> metaData) throws IOException {
-        super(name, type, context, parent, subFactoriesBuilder, metaData);
+    protected ValuesSourceAggregatorFactory(String name, String type, ValuesSourceConfig<VS> config) {
+        super(name, type);
         this.config = config;
     }
 
-    public DateTimeZone timeZone() {
-        return config.timeZone;
-        }
-
     @Override
     public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
             List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
@@ -56,6 +64,13 @@ public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource, AF
         return doCreateInternal(vs, context, parent, collectsFromSingleBucket, pipelineAggregators, metaData);
     }
 
+    @Override
+    public void doValidate() {
+        if (config == null || !config.valid()) {
+            resolveValuesSourceConfigFromAncestors(name, parent, config.valueSourceType());
+        }
+    }
+
     protected abstract Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
             List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException;
 
@@ -63,4 +78,26 @@ public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource, AF
             boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
             throws IOException;
 
-}
\ No newline at end of file
+    private void resolveValuesSourceConfigFromAncestors(String aggName, AggregatorFactory parent, Class<VS> requiredValuesSourceType) {
+        ValuesSourceConfig config;
+        while (parent != null) {
+            if (parent instanceof ValuesSourceAggregatorFactory) {
+                config = ((ValuesSourceAggregatorFactory) parent).config;
+                if (config != null && config.valid()) {
+                    if (requiredValuesSourceType == null || requiredValuesSourceType.isAssignableFrom(config.valueSourceType)) {
+                        ValueFormat format = config.format;
+                        this.config = config;
+                        // if the user explicitly defined a format pattern, we'll do our best to keep it even when we inherit the
+                        // value source form one of the ancestor aggregations
+                        if (this.config.formatPattern != null && format != null && format instanceof ValueFormat.Patternable) {
+                            this.config.format = ((ValueFormat.Patternable) format).create(this.config.formatPattern);
+                        }
+                        return;
+                    }
+                }
+            }
+            parent = parent.parent();
+        }
+        throw new AggregationExecutionException("could not find the appropriate value context to perform aggregation [" + aggName + "]");
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java
index 79e9efd..a831f2f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java
@@ -22,14 +22,13 @@ import org.elasticsearch.script.SearchScript;
 import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueParser;
-import org.joda.time.DateTimeZone;
 
 /**
  *
  */
 public class ValuesSourceConfig<VS extends ValuesSource> {
 
-    final ValuesSourceType valueSourceType;
+    final Class<VS> valueSourceType;
     FieldContext fieldContext;
     SearchScript script;
     ValueType scriptValueType;
@@ -37,13 +36,12 @@ public class ValuesSourceConfig<VS extends ValuesSource> {
     String formatPattern;
     ValueFormat format = ValueFormat.RAW;
     Object missing;
-    DateTimeZone timeZone;
 
-    public ValuesSourceConfig(ValuesSourceType valueSourceType) {
+    public ValuesSourceConfig(Class<VS> valueSourceType) {
         this.valueSourceType = valueSourceType;
     }
 
-    public ValuesSourceType valueSourceType() {
+    public Class<VS> valueSourceType() {
         return valueSourceType;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java
new file mode 100644
index 0000000..fced5fd
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java
@@ -0,0 +1,297 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.support;
+
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.fielddata.IndexFieldData;
+import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
+import org.elasticsearch.index.fielddata.IndexNumericFieldData;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.core.BooleanFieldMapper;
+import org.elasticsearch.index.mapper.core.DateFieldMapper;
+import org.elasticsearch.index.mapper.core.NumberFieldMapper;
+import org.elasticsearch.index.mapper.ip.IpFieldMapper;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.script.ScriptContext;
+import org.elasticsearch.script.ScriptParameterParser;
+import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
+import org.elasticsearch.script.SearchScript;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.internal.SearchContext;
+import org.joda.time.DateTimeZone;
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ *
+ */
+public class ValuesSourceParser<VS extends ValuesSource> {
+
+    static final ParseField TIME_ZONE = new ParseField("time_zone");
+
+    public static Builder any(String aggName, InternalAggregation.Type aggType, SearchContext context) {
+        return new Builder<>(aggName, aggType, context, ValuesSource.class);
+    }
+
+    public static Builder<ValuesSource.Numeric> numeric(String aggName, InternalAggregation.Type aggType, SearchContext context) {
+        return new Builder<>(aggName, aggType, context, ValuesSource.Numeric.class).targetValueType(ValueType.NUMERIC);
+    }
+
+    public static Builder<ValuesSource.Bytes> bytes(String aggName, InternalAggregation.Type aggType, SearchContext context) {
+        return new Builder<>(aggName, aggType, context, ValuesSource.Bytes.class).targetValueType(ValueType.STRING);
+    }
+
+    public static Builder<ValuesSource.GeoPoint> geoPoint(String aggName, InternalAggregation.Type aggType, SearchContext context) {
+        return new Builder<>(aggName, aggType, context, ValuesSource.GeoPoint.class).targetValueType(ValueType.GEOPOINT).scriptable(false);
+    }
+
+    public static class Input {
+        private String field = null;
+        private Script script = null;
+        @Deprecated
+        private Map<String, Object> params = null; // TODO Remove in 3.0
+        private ValueType valueType = null;
+        private String format = null;
+        private Object missing = null;
+        private DateTimeZone timezone = DateTimeZone.UTC;
+
+        public DateTimeZone timezone() {
+            return this.timezone;
+        }
+    }
+
+    private final String aggName;
+    private final InternalAggregation.Type aggType;
+    private final SearchContext context;
+    private final Class<VS> valuesSourceType;
+
+    private boolean scriptable = true;
+    private boolean formattable = false;
+    private boolean timezoneAware = false;
+    private ValueType targetValueType = null;
+    private ScriptParameterParser scriptParameterParser = new ScriptParameterParser();
+
+    private Input input = new Input();
+
+    private ValuesSourceParser(String aggName, InternalAggregation.Type aggType, SearchContext context, Class<VS> valuesSourceType) {
+        this.aggName = aggName;
+        this.aggType = aggType;
+        this.context = context;
+        this.valuesSourceType = valuesSourceType;
+    }
+
+    public boolean token(String currentFieldName, XContentParser.Token token, XContentParser parser) throws IOException {
+        if ("missing".equals(currentFieldName) && token.isValue()) {
+            input.missing = parser.objectText();
+            return true;
+        }
+        if (token == XContentParser.Token.VALUE_STRING) {
+            if ("field".equals(currentFieldName)) {
+                input.field = parser.text();
+            } else if (formattable && "format".equals(currentFieldName)) {
+                input.format = parser.text();
+            } else if (timezoneAware && context.parseFieldMatcher().match(currentFieldName, TIME_ZONE)) {
+                input.timezone = DateTimeZone.forID(parser.text());
+            } else if (scriptable) {
+                if ("value_type".equals(currentFieldName) || "valueType".equals(currentFieldName)) {
+                    input.valueType = ValueType.resolveForScript(parser.text());
+                    if (targetValueType != null && input.valueType.isNotA(targetValueType)) {
+                        throw new SearchParseException(context, aggType.name() + " aggregation [" + aggName +
+                                "] was configured with an incompatible value type [" + input.valueType + "]. [" + aggType +
+                                "] aggregation can only work on value of type [" + targetValueType + "]",
+                                parser.getTokenLocation());
+                    }
+                } else if (!scriptParameterParser.token(currentFieldName, token, parser, context.parseFieldMatcher())) {
+                    return false;
+                }
+                return true;
+            } else {
+                return false;
+            }
+            return true;
+        }
+        if (token == XContentParser.Token.VALUE_NUMBER) {
+            if (timezoneAware && context.parseFieldMatcher().match(currentFieldName, TIME_ZONE)) {
+                input.timezone = DateTimeZone.forOffsetHours(parser.intValue());
+            } else {
+                return false;
+            }
+            return true;
+        }
+        if (scriptable && token == XContentParser.Token.START_OBJECT) {
+            if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
+                input.script = Script.parse(parser, context.parseFieldMatcher());
+                return true;
+            } else if ("params".equals(currentFieldName)) {
+                input.params = parser.map();
+                return true;
+            }
+            return false;
+        }
+
+        return false;
+    }
+
+    public ValuesSourceConfig<VS> config() {
+
+        if (input.script == null) { // Didn't find anything using the new API so try using the old one instead
+            ScriptParameterValue scriptValue = scriptParameterParser.getDefaultScriptParameterValue();
+            if (scriptValue != null) {
+                if (input.params == null) {
+                    input.params = new HashMap<>();
+                }
+                input.script = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), input.params);
+            }
+        }
+
+        ValueType valueType = input.valueType != null ? input.valueType : targetValueType;
+
+        if (input.field == null) {
+            if (input.script == null) {
+                ValuesSourceConfig<VS> config = new ValuesSourceConfig(ValuesSource.class);
+                config.format = resolveFormat(null, valueType);
+                return config;
+            }
+            Class valuesSourceType = valueType != null ? (Class<VS>) valueType.getValuesSourceType() : this.valuesSourceType;
+            if (valuesSourceType == null || valuesSourceType == ValuesSource.class) {
+                // the specific value source type is undefined, but for scripts, we need to have a specific value source
+                // type to know how to handle the script values, so we fallback on Bytes
+                valuesSourceType = ValuesSource.Bytes.class;
+            }
+            ValuesSourceConfig<VS> config = new ValuesSourceConfig<VS>(valuesSourceType);
+            config.missing = input.missing;
+            config.format = resolveFormat(input.format, valueType);
+            config.script = createScript();
+            config.scriptValueType = valueType;
+            return config;
+        }
+
+        MappedFieldType fieldType = context.smartNameFieldType(input.field);
+        if (fieldType == null) {
+            Class<VS> valuesSourceType = valueType != null ? (Class<VS>) valueType.getValuesSourceType() : this.valuesSourceType;
+            ValuesSourceConfig<VS> config = new ValuesSourceConfig<>(valuesSourceType);
+            config.missing = input.missing;
+            config.format = resolveFormat(input.format, valueType);
+            config.unmapped = true;
+            if (valueType != null) {
+                // todo do we really need this for unmapped?
+                config.scriptValueType = valueType;
+            }
+            return config;
+        }
+
+        IndexFieldData<?> indexFieldData = context.fieldData().getForField(fieldType);
+
+        ValuesSourceConfig config;
+        if (valuesSourceType == ValuesSource.class) {
+            if (indexFieldData instanceof IndexNumericFieldData) {
+                config = new ValuesSourceConfig<>(ValuesSource.Numeric.class);
+            } else if (indexFieldData instanceof IndexGeoPointFieldData) {
+                config = new ValuesSourceConfig<>(ValuesSource.GeoPoint.class);
+            } else {
+                config = new ValuesSourceConfig<>(ValuesSource.Bytes.class);
+            }
+        } else {
+            config = new ValuesSourceConfig(valuesSourceType);
+        }
+
+        config.fieldContext = new FieldContext(input.field, indexFieldData, fieldType);
+        config.missing = input.missing;
+        config.script = createScript();
+        config.format = resolveFormat(input.format, input.timezone, fieldType);
+        return config;
+    }
+
+    private SearchScript createScript() {
+        return input.script == null ? null : context.scriptService().search(context.lookup(), input.script, ScriptContext.Standard.AGGS, Collections.emptyMap());
+    }
+
+    private static ValueFormat resolveFormat(@Nullable String format, @Nullable ValueType valueType) {
+        if (valueType == null) {
+            return ValueFormat.RAW; // we can't figure it out
+        }
+        ValueFormat valueFormat = valueType.defaultFormat;
+        if (valueFormat != null && valueFormat instanceof ValueFormat.Patternable && format != null) {
+            return ((ValueFormat.Patternable) valueFormat).create(format);
+        }
+        return valueFormat;
+    }
+
+    private static ValueFormat resolveFormat(@Nullable String format, @Nullable DateTimeZone timezone,  MappedFieldType fieldType) {
+        if (fieldType instanceof  DateFieldMapper.DateFieldType) {
+            return format != null ? ValueFormat.DateTime.format(format, timezone) : ValueFormat.DateTime.mapper((DateFieldMapper.DateFieldType) fieldType, timezone);
+        }
+        if (fieldType instanceof IpFieldMapper.IpFieldType) {
+            return ValueFormat.IPv4;
+        }
+        if (fieldType instanceof BooleanFieldMapper.BooleanFieldType) {
+            return ValueFormat.BOOLEAN;
+        }
+        if (fieldType instanceof NumberFieldMapper.NumberFieldType) {
+            return format != null ? ValueFormat.Number.format(format) : ValueFormat.RAW;
+        }
+        return ValueFormat.RAW;
+    }
+
+    public Input input() {
+        return this.input;
+    }
+
+    public static class Builder<VS extends ValuesSource> {
+
+        private final ValuesSourceParser<VS> parser;
+
+        private Builder(String aggName, InternalAggregation.Type aggType, SearchContext context, Class<VS> valuesSourceType) {
+            parser = new ValuesSourceParser<>(aggName, aggType, context, valuesSourceType);
+        }
+
+        public Builder<VS> scriptable(boolean scriptable) {
+            parser.scriptable = scriptable;
+            return this;
+        }
+
+        public Builder<VS> formattable(boolean formattable) {
+            parser.formattable = formattable;
+            return this;
+        }
+
+        public Builder<VS> timezoneAware(boolean timezoneAware) {
+            parser.timezoneAware = timezoneAware;
+            return this;
+        }
+
+        public Builder<VS> targetValueType(ValueType valueType) {
+            parser.targetValueType = valueType;
+            return this;
+        }
+
+        public ValuesSourceParser<VS> build() {
+            return parser;
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceType.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceType.java
deleted file mode 100644
index 46b5698..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceType.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.support;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
-
-/*
- * The ordinal values for this class are tested in ValuesSourceTypeTests to
- * ensure that the ordinal for each value does not change and break bwc
- */
-public enum ValuesSourceType implements Writeable<ValuesSourceType> {
-
-    ANY,
-    NUMERIC,
-    BYTES,
-    GEOPOINT;
-
-    @Override
-    public ValuesSourceType readFrom(StreamInput in) throws IOException {
-        int ordinal = in.readVInt();
-        if (ordinal < 0 || ordinal >= values().length) {
-            throw new IOException("Unknown ValuesSourceType ordinal [" + ordinal + "]");
-        }
-        return values()[ordinal];
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(ordinal());
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
index b1ba834..16c00a0 100644
--- a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
@@ -42,8 +42,7 @@ import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.searchafter.SearchAfterBuilder;
-import org.elasticsearch.search.aggregations.AggregatorBuilder;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 import org.elasticsearch.search.highlight.HighlightBuilder;
@@ -411,26 +410,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     /**
      * Add an aggregation to perform as part of the search.
      */
-    public SearchSourceBuilder aggregation(AggregatorBuilder<?> aggregation) {
-        try {
-            if (aggregations == null) {
-                aggregations = new ArrayList<>();
-            }
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            aggregation.toXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            aggregations.add(builder.bytes());
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
-        }
-    }
-
-    /**
-     * Add an aggregation to perform as part of the search.
-     */
-    public SearchSourceBuilder aggregation(PipelineAggregatorFactory aggregation) {
+    public SearchSourceBuilder aggregation(AbstractAggregationBuilder aggregation) {
         try {
             if (aggregations == null) {
                 aggregations = new ArrayList<>();
@@ -1123,7 +1103,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
             this(fieldName, script, false);
         }
 
-        public ScriptField(String fieldName, Script script, boolean ignoreFailure) {
+        private ScriptField(String fieldName, Script script, boolean ignoreFailure) {
             this.fieldName = fieldName;
             this.script = script;
             this.ignoreFailure = ignoreFailure;
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java
index d813dea..077268a 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java
@@ -59,7 +59,7 @@ public class InnerHitsParseElement implements SearchParseElement {
 
     @Override
     public void parse(XContentParser parser, SearchContext searchContext) throws Exception {
-        QueryShardContext context = searchContext.indexShard().getQueryShardContext();
+        QueryShardContext context = searchContext.getQueryShardContext();
         context.reset(parser);
         Map<String, InnerHitsContext.BaseInnerHits> topLevelInnerHits = parseInnerHits(parser, context, searchContext);
         if (topLevelInnerHits != null) {
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java b/core/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java
index 38534ba..b774aca 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java
@@ -54,7 +54,7 @@ public class HighlighterParseElement implements SearchParseElement {
     @Override
     public void parse(XContentParser parser, SearchContext context) throws Exception {
         try {
-            context.highlight(parse(parser, context.indexShard().getQueryShardContext()));
+            context.highlight(parse(parser, context.getQueryShardContext()));
         } catch (IllegalArgumentException ex) {
             throw new SearchParseException(context, "Error while trying to parse Highlighter element in request", parser.getTokenLocation());
         }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
index f8c10b8..32aa45f 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
@@ -29,7 +29,6 @@ import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.FieldDoc;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.util.Counter;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
@@ -53,17 +52,16 @@ import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.AbstractQueryBuilder;
 import org.elasticsearch.index.query.ParsedQuery;
+import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
 import org.elasticsearch.search.fetch.script.ScriptFieldsContext;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 import org.elasticsearch.search.highlight.SearchContextHighlight;
@@ -151,16 +149,17 @@ public class DefaultSearchContext extends SearchContext {
 
     private final Map<String, FetchSubPhaseContext> subPhaseContexts = new HashMap<>();
     private final Map<Class<?>, Collector> queryCollectors = new HashMap<>();
-    private FetchPhase fetchPhase;
-
-    public DefaultSearchContext(long id, ShardSearchRequest request, SearchShardTarget shardTarget, Engine.Searcher engineSearcher,
-            IndexService indexService, IndexShard indexShard, ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
-            BigArrays bigArrays, Counter timeEstimateCounter, ParseFieldMatcher parseFieldMatcher, TimeValue timeout,
-            FetchPhase fetchPhase) {
+    private final QueryShardContext queryShardContext;
+
+    public DefaultSearchContext(long id, ShardSearchRequest request, SearchShardTarget shardTarget,
+                                Engine.Searcher engineSearcher, IndexService indexService, IndexShard indexShard,
+                                ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
+                                BigArrays bigArrays, Counter timeEstimateCounter, ParseFieldMatcher parseFieldMatcher,
+                                TimeValue timeout
+    ) {
         super(parseFieldMatcher);
         this.id = id;
         this.request = request;
-        this.fetchPhase = fetchPhase;
         this.searchType = request.searchType();
         this.shardTarget = shardTarget;
         this.engineSearcher = engineSearcher;
@@ -176,6 +175,8 @@ public class DefaultSearchContext extends SearchContext {
         this.searcher = new ContextIndexSearcher(engineSearcher, indexService.cache().query(), indexShard.getQueryCachingPolicy());
         this.timeEstimateCounter = timeEstimateCounter;
         this.timeoutInMillis = timeout.millis();
+        queryShardContext = indexService.newQueryShardContext();
+        queryShardContext.setTypes(request.types());
     }
 
     @Override
@@ -207,7 +208,7 @@ public class DefaultSearchContext extends SearchContext {
         }
 
         // initialize the filtering alias based on the provided filters
-        aliasFilter = indexService.aliasFilter(indexShard.getQueryShardContext(), request.filteringAliases());
+        aliasFilter = indexService.aliasFilter(queryShardContext, request.filteringAliases());
 
         if (query() == null) {
             parsedQuery(ParsedQuery.parsedMatchAllQuery());
@@ -224,7 +225,7 @@ public class DefaultSearchContext extends SearchContext {
     }
 
     private ParsedQuery buildFilteredQuery() {
-        Query searchFilter = searchFilter(types());
+        Query searchFilter = searchFilter(queryShardContext.getTypes());
         if (searchFilter == null) {
             return originalQuery;
         }
@@ -314,16 +315,6 @@ public class DefaultSearchContext extends SearchContext {
     }
 
     @Override
-    public boolean hasTypes() {
-        return request.types() != null && request.types().length > 0;
-    }
-
-    @Override
-    public String[] types() {
-        return request.types();
-    }
-
-    @Override
     public float queryBoost() {
         return queryBoost;
     }
@@ -742,11 +733,6 @@ public class DefaultSearchContext extends SearchContext {
     }
 
     @Override
-    public FetchPhase fetchPhase() {
-        return fetchPhase;
-    }
-
-    @Override
     public FetchSearchResult fetchResult() {
         return fetchResult;
     }
@@ -772,6 +758,11 @@ public class DefaultSearchContext extends SearchContext {
     }
 
     @Override
+    public QueryShardContext getQueryShardContext() {
+        return queryShardContext;
+    }
+
+    @Override
     public Profilers getProfilers() {
         return profilers;
     }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
index 301baff..c47d32b 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
@@ -35,13 +35,13 @@ import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.ParsedQuery;
+import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -119,16 +119,6 @@ public abstract class FilteredSearchContext extends SearchContext {
     }
 
     @Override
-    public boolean hasTypes() {
-        return in.hasTypes();
-    }
-
-    @Override
-    public String[] types() {
-        return in.types();
-    }
-
-    @Override
     public float queryBoost() {
         return in.queryBoost();
     }
@@ -499,11 +489,6 @@ public abstract class FilteredSearchContext extends SearchContext {
     }
 
     @Override
-    public FetchPhase fetchPhase() {
-        return in.fetchPhase();
-    }
-
-    @Override
     public MappedFieldType smartNameFieldType(String name) {
         return in.smartNameFieldType(name);
     }
@@ -531,4 +516,8 @@ public abstract class FilteredSearchContext extends SearchContext {
     @Override
     public Map<Class<?>, Collector> queryCollectors() { return in.queryCollectors();}
 
+    @Override
+    public QueryShardContext getQueryShardContext() {
+        return in.getQueryShardContext();
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
index f77387a..7bef567 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
@@ -46,7 +46,6 @@ import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -73,12 +72,10 @@ public abstract class SearchContext implements Releasable {
 
     public static void setCurrent(SearchContext value) {
         current.set(value);
-        QueryShardContext.setTypes(value.types());
     }
 
     public static void removeCurrent() {
         current.remove();
-        QueryShardContext.removeTypes();
     }
 
     public static SearchContext current() {
@@ -135,10 +132,6 @@ public abstract class SearchContext implements Releasable {
 
     public abstract int numberOfShards();
 
-    public abstract boolean hasTypes();
-
-    public abstract String[] types();
-
     public abstract float queryBoost();
 
     public abstract SearchContext queryBoost(float queryBoost);
@@ -311,8 +304,6 @@ public abstract class SearchContext implements Releasable {
 
     public abstract QuerySearchResult queryResult();
 
-    public abstract FetchPhase fetchPhase();
-
     public abstract FetchSearchResult fetchResult();
 
     /**
@@ -382,4 +373,6 @@ public abstract class SearchContext implements Releasable {
         CONTEXT
     }
 
+    public abstract QueryShardContext getQueryShardContext();
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/lookup/DocLookup.java b/core/src/main/java/org/elasticsearch/search/lookup/DocLookup.java
index b32a6ae..3eea11a 100644
--- a/core/src/main/java/org/elasticsearch/search/lookup/DocLookup.java
+++ b/core/src/main/java/org/elasticsearch/search/lookup/DocLookup.java
@@ -51,4 +51,8 @@ public class DocLookup {
     public LeafDocLookup getLeafDocLookup(LeafReaderContext context) {
         return new LeafDocLookup(mapperService, fieldDataService, types, context);
     }
+
+    public String[] getTypes() {
+        return types;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/query/PostFilterParseElement.java b/core/src/main/java/org/elasticsearch/search/query/PostFilterParseElement.java
index 3f3093b..6995b6f 100644
--- a/core/src/main/java/org/elasticsearch/search/query/PostFilterParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/query/PostFilterParseElement.java
@@ -30,9 +30,9 @@ public class PostFilterParseElement implements SearchParseElement {
 
     @Override
     public void parse(XContentParser parser, SearchContext context) throws Exception {
-        ParsedQuery postFilter = context.indexShard().getQueryShardContext().parseInnerFilter(parser);
+        ParsedQuery postFilter = context.getQueryShardContext().parseInnerFilter(parser);
         if (postFilter != null) {
             context.parsedPostFilter(postFilter);
         }
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/search/query/QueryParseElement.java b/core/src/main/java/org/elasticsearch/search/query/QueryParseElement.java
index 12a080a..094a29c 100644
--- a/core/src/main/java/org/elasticsearch/search/query/QueryParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/query/QueryParseElement.java
@@ -30,6 +30,6 @@ public class QueryParseElement implements SearchParseElement {
 
     @Override
     public void parse(XContentParser parser, SearchContext context) throws Exception {
-        context.parsedQuery(context.indexShard().getQueryShardContext().parse(parser));
+        context.parsedQuery(context.getQueryShardContext().parse(parser));
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/search/rescore/RescoreParseElement.java b/core/src/main/java/org/elasticsearch/search/rescore/RescoreParseElement.java
index 149db6c..702d366 100644
--- a/core/src/main/java/org/elasticsearch/search/rescore/RescoreParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/rescore/RescoreParseElement.java
@@ -36,10 +36,10 @@ public class RescoreParseElement implements SearchParseElement {
     public void parse(XContentParser parser, SearchContext context) throws Exception {
         if (parser.currentToken() == XContentParser.Token.START_ARRAY) {
             while (parser.nextToken() != XContentParser.Token.END_ARRAY) {
-                context.addRescore(parseSingleRescoreContext(parser, context.indexShard().getQueryShardContext()));
+                context.addRescore(parseSingleRescoreContext(parser, context.getQueryShardContext()));
             }
         } else {
-            context.addRescore(parseSingleRescoreContext(parser, context.indexShard().getQueryShardContext()));
+            context.addRescore(parseSingleRescoreContext(parser, context.getQueryShardContext()));
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java b/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java
index c7fa6fa..7838eac 100644
--- a/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java
+++ b/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java
@@ -33,7 +33,6 @@ import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.text.Text;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.query.ParsedQuery;
-import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.script.CompiledScript;
 import org.elasticsearch.script.ExecutableScript;
@@ -65,7 +64,7 @@ public final class PhraseSuggester extends Suggester<PhraseSuggestionContext> {
     /*
      * More Ideas:
      *   - add ability to find whitespace problems -> we can build a poor mans decompounder with our index based on a automaton?
-     *   - add ability to build different error models maybe based on a confusion matrix?   
+     *   - add ability to build different error models maybe based on a confusion matrix?
      *   - try to combine a token with its subsequent token to find / detect word splits (optional)
      *      - for this to work we need some way to defined the position length of a candidate
      *   - phonetic filters could be interesting here too for candidate selection
@@ -84,8 +83,8 @@ public final class PhraseSuggester extends Suggester<PhraseSuggestionContext> {
             DirectSpellChecker directSpellChecker = SuggestUtils.getDirectSpellChecker(generator);
             Terms terms = MultiFields.getTerms(indexReader, generator.field());
             if (terms !=  null) {
-                gens.add(new DirectCandidateGenerator(directSpellChecker, generator.field(), generator.suggestMode(), 
-                        indexReader, realWordErrorLikelihood, generator.size(), generator.preFilter(), generator.postFilter(), terms));    
+                gens.add(new DirectCandidateGenerator(directSpellChecker, generator.field(), generator.suggestMode(),
+                        indexReader, realWordErrorLikelihood, generator.size(), generator.preFilter(), generator.postFilter(), terms));
             }
         }
         final String suggestField = suggestion.getField();
@@ -119,8 +118,7 @@ public final class PhraseSuggester extends Suggester<PhraseSuggestionContext> {
                     final ExecutableScript executable = scriptService.executable(collateScript, vars);
                     final BytesReference querySource = (BytesReference) executable.run();
                     IndexService indexService = indicesService.indexService(suggestion.getIndex());
-                    IndexShard shard = indexService.getShard(suggestion.getShard());
-                    final ParsedQuery parsedQuery = shard.getQueryShardContext().parse(querySource);
+                    final ParsedQuery parsedQuery = indexService.newQueryShardContext().parse(querySource);
                     collateMatch = Lucene.exists(searcher, parsedQuery.query());
                 }
                 if (!collateMatch && !collatePrune) {
@@ -152,7 +150,7 @@ public final class PhraseSuggester extends Suggester<PhraseSuggestionContext> {
     ScriptService scriptService() {
         return scriptService;
     }
-    
+
     @Override
     public SuggestContextParser getContextParser() {
         return new PhraseSuggestParser(this);
diff --git a/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionContext.java b/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionContext.java
index 04d06ab..736b297 100644
--- a/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionContext.java
+++ b/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionContext.java
@@ -42,7 +42,6 @@ class PhraseSuggestionContext extends SuggestionContext {
     private BytesRef preTag;
     private BytesRef postTag;
     private CompiledScript collateQueryScript;
-    private CompiledScript collateFilterScript;
     private Map<String, Object> collateScriptParams = new HashMap<>(1);
 
     private WordScorer.WordScorerFactory scorer;
diff --git a/core/src/test/java/org/elasticsearch/ESExceptionTests.java b/core/src/test/java/org/elasticsearch/ESExceptionTests.java
index 75a69cd..ad3f063 100644
--- a/core/src/test/java/org/elasticsearch/ESExceptionTests.java
+++ b/core/src/test/java/org/elasticsearch/ESExceptionTests.java
@@ -194,7 +194,7 @@ public class ESExceptionTests extends ESTestCase {
 
     public void testToXContent() throws IOException {
         {
-            ElasticsearchException ex = new SearchParseException(new TestSearchContext(), "foo", new XContentLocation(1,0));
+            ElasticsearchException ex = new SearchParseException(new TestSearchContext(null), "foo", new XContentLocation(1,0));
             XContentBuilder builder = XContentFactory.jsonBuilder();
             builder.startObject();
             ex.toXContent(builder, PARAMS);
diff --git a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
index 57b1fda..606ff09 100644
--- a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
+++ b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
@@ -439,7 +439,7 @@ public class ExceptionSerializationTests extends ESTestCase {
     }
 
     public void testSearchParseException() throws IOException {
-        SearchContext ctx = new TestSearchContext();
+        SearchContext ctx = new TestSearchContext(null);
         SearchParseException ex = serialize(new SearchParseException(ctx, "foo", new XContentLocation(66, 666)));
         assertEquals("foo", ex.getMessage());
         assertEquals(66, ex.getLineNumber());
diff --git a/core/src/test/java/org/elasticsearch/bootstrap/BootstrapSettingsTests.java b/core/src/test/java/org/elasticsearch/bootstrap/BootstrapSettingsTests.java
new file mode 100644
index 0000000..0570a69
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/bootstrap/BootstrapSettingsTests.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.bootstrap;
+
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.test.ESTestCase;
+
+public class BootstrapSettingsTests extends ESTestCase {
+
+    public void testDefaultSettings() {
+        assertTrue(BootstrapSettings.SECURITY_MANAGER_ENABLED_SETTING.get(Settings.EMPTY));
+        assertTrue(BootstrapSettings.SECURITY_FILTER_BAD_DEFAULTS_SETTING.get(Settings.EMPTY));
+        assertFalse(BootstrapSettings.MLOCKALL_SETTING.get(Settings.EMPTY));
+        assertTrue(BootstrapSettings.SECCOMP_SETTING.get(Settings.EMPTY));
+        assertTrue(BootstrapSettings.CTRLHANDLER_SETTING.get(Settings.EMPTY));
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/bootstrap/JavaVersionTests.java b/core/src/test/java/org/elasticsearch/bootstrap/JavaVersionTests.java
index 21bfa05..d2ef349 100644
--- a/core/src/test/java/org/elasticsearch/bootstrap/JavaVersionTests.java
+++ b/core/src/test/java/org/elasticsearch/bootstrap/JavaVersionTests.java
@@ -36,8 +36,10 @@ public class JavaVersionTests extends ESTestCase {
     }
 
     public void testToString() {
-        JavaVersion javaVersion = JavaVersion.parse("1.7.0");
-        assertThat("1.7.0", is(javaVersion.toString()));
+        JavaVersion javaVersion170 = JavaVersion.parse("1.7.0");
+        assertThat(javaVersion170.toString(), is("1.7.0"));
+        JavaVersion javaVersion9 = JavaVersion.parse("9");
+        assertThat(javaVersion9.toString(), is("9"));
     }
 
     public void testCompare() {
diff --git a/core/src/test/java/org/elasticsearch/common/hash/MessageDigestsTests.java b/core/src/test/java/org/elasticsearch/common/hash/MessageDigestsTests.java
index 4063c1b..e3c085f 100644
--- a/core/src/test/java/org/elasticsearch/common/hash/MessageDigestsTests.java
+++ b/core/src/test/java/org/elasticsearch/common/hash/MessageDigestsTests.java
@@ -59,16 +59,14 @@ public class MessageDigestsTests extends ESTestCase {
     }
 
     public void testToHexString() throws Exception {
-        for (int i = 0; i < 1024; i++) {
-            BigInteger expected = BigInteger.probablePrime(256, random());
-            byte[] bytes = expected.toByteArray();
-            String hex = MessageDigests.toHexString(bytes);
-            String zeros = new String(new char[2 * bytes.length]).replace("\0", "0");
-            String expectedAsString = expected.toString(16);
-            String expectedHex = zeros.substring(expectedAsString.length()) + expectedAsString;
-            assertEquals(expectedHex, hex);
-            BigInteger actual = new BigInteger(hex, 16);
-            assertEquals(expected, actual);
-        }
+        BigInteger expected = BigInteger.probablePrime(256, random());
+        byte[] bytes = expected.toByteArray();
+        String hex = MessageDigests.toHexString(bytes);
+        String zeros = new String(new char[2 * bytes.length]).replace("\0", "0");
+        String expectedAsString = expected.toString(16);
+        String expectedHex = zeros.substring(expectedAsString.length()) + expectedAsString;
+        assertEquals(expectedHex, hex);
+        BigInteger actual = new BigInteger(hex, 16);
+        assertEquals(expected, actual);
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/IndexServiceTests.java b/core/src/test/java/org/elasticsearch/index/IndexServiceTests.java
index 252fa9d..3a4020e 100644
--- a/core/src/test/java/org/elasticsearch/index/IndexServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/index/IndexServiceTests.java
@@ -39,7 +39,6 @@ import org.elasticsearch.test.ESSingleNodeTestCase;
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.io.IOException;
-import java.util.Set;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReference;
@@ -87,17 +86,17 @@ public class IndexServiceTests extends ESSingleNodeTestCase {
         assertThat(indexService.getMetaData().getAliases().containsKey("dogs"), equalTo(true));
         assertThat(indexService.getMetaData().getAliases().containsKey("turtles"), equalTo(false));
 
-        assertThat(indexService.aliasFilter(shard.getQueryShardContext(), "cats").toString(), equalTo("animal:cat"));
-        assertThat(indexService.aliasFilter(shard.getQueryShardContext(), "cats", "dogs").toString(), equalTo("animal:cat animal:dog"));
+        assertThat(indexService.aliasFilter(indexService.newQueryShardContext(), "cats").toString(), equalTo("animal:cat"));
+        assertThat(indexService.aliasFilter(indexService.newQueryShardContext(), "cats", "dogs").toString(), equalTo("animal:cat animal:dog"));
 
         // Non-filtering alias should turn off all filters because filters are ORed
-        assertThat(indexService.aliasFilter(shard.getQueryShardContext(), "all"), nullValue());
-        assertThat(indexService.aliasFilter(shard.getQueryShardContext(), "cats", "all"), nullValue());
-        assertThat(indexService.aliasFilter(shard.getQueryShardContext(), "all", "cats"), nullValue());
+        assertThat(indexService.aliasFilter(indexService.newQueryShardContext(), "all"), nullValue());
+        assertThat(indexService.aliasFilter(indexService.newQueryShardContext(), "cats", "all"), nullValue());
+        assertThat(indexService.aliasFilter(indexService.newQueryShardContext(), "all", "cats"), nullValue());
 
         add(indexService, "cats", filter(termQuery("animal", "feline")));
         add(indexService, "dogs", filter(termQuery("animal", "canine")));
-        assertThat(indexService.aliasFilter(shard.getQueryShardContext(), "dogs", "cats").toString(), equalTo("animal:canine animal:feline"));
+        assertThat(indexService.aliasFilter(indexService.newQueryShardContext(), "dogs", "cats").toString(), equalTo("animal:canine animal:feline"));
     }
 
     public void testAliasFilters() throws Exception {
@@ -107,14 +106,14 @@ public class IndexServiceTests extends ESSingleNodeTestCase {
         add(indexService, "cats", filter(termQuery("animal", "cat")));
         add(indexService, "dogs", filter(termQuery("animal", "dog")));
 
-        assertThat(indexService.aliasFilter(shard.getQueryShardContext()), nullValue());
-        assertThat(indexService.aliasFilter(shard.getQueryShardContext(), "dogs").toString(), equalTo("animal:dog"));
-        assertThat(indexService.aliasFilter(shard.getQueryShardContext(), "dogs", "cats").toString(), equalTo("animal:dog animal:cat"));
+        assertThat(indexService.aliasFilter(indexService.newQueryShardContext()), nullValue());
+        assertThat(indexService.aliasFilter(indexService.newQueryShardContext(), "dogs").toString(), equalTo("animal:dog"));
+        assertThat(indexService.aliasFilter(indexService.newQueryShardContext(), "dogs", "cats").toString(), equalTo("animal:dog animal:cat"));
 
         add(indexService, "cats", filter(termQuery("animal", "feline")));
         add(indexService, "dogs", filter(termQuery("animal", "canine")));
 
-        assertThat(indexService.aliasFilter(shard.getQueryShardContext(), "dogs", "cats").toString(), equalTo("animal:canine animal:feline"));
+        assertThat(indexService.aliasFilter(indexService.newQueryShardContext(), "dogs", "cats").toString(), equalTo("animal:canine animal:feline"));
     }
 
     public void testRemovedAliasFilter() throws Exception {
@@ -124,7 +123,7 @@ public class IndexServiceTests extends ESSingleNodeTestCase {
         add(indexService, "cats", filter(termQuery("animal", "cat")));
         remove(indexService, "cats");
         try {
-            indexService.aliasFilter(shard.getQueryShardContext(), "cats");
+            indexService.aliasFilter(indexService.newQueryShardContext(), "cats");
             fail("Expected InvalidAliasNameException");
         } catch (InvalidAliasNameException e) {
             assertThat(e.getMessage(), containsString("Invalid alias name [cats]"));
@@ -139,7 +138,7 @@ public class IndexServiceTests extends ESSingleNodeTestCase {
         add(indexService, "dogs", filter(termQuery("animal", "dog")));
 
         try {
-            indexService.aliasFilter(shard.getQueryShardContext(), "unknown");
+            indexService.aliasFilter(indexService.newQueryShardContext(), "unknown");
             fail();
         } catch (InvalidAliasNameException e) {
             // all is well
diff --git a/core/src/test/java/org/elasticsearch/index/IndexSettingsTests.java b/core/src/test/java/org/elasticsearch/index/IndexSettingsTests.java
index 09fec10..677c835 100644
--- a/core/src/test/java/org/elasticsearch/index/IndexSettingsTests.java
+++ b/core/src/test/java/org/elasticsearch/index/IndexSettingsTests.java
@@ -272,16 +272,16 @@ public class IndexSettingsTests extends ESTestCase {
 
     public void testTranslogFlushSizeThreshold() {
         ByteSizeValue translogFlushThresholdSize = new ByteSizeValue(Math.abs(randomInt()));
-        ByteSizeValue actualValue = ByteSizeValue.parseBytesSizeValue(translogFlushThresholdSize.toString(), IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey());
+        ByteSizeValue actualValue = ByteSizeValue.parseBytesSizeValue(translogFlushThresholdSize.toString(), IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey());
         IndexMetaData metaData = newIndexMeta("index", Settings.settingsBuilder()
             .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-            .put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey(), translogFlushThresholdSize.toString())
+            .put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), translogFlushThresholdSize.toString())
             .build());
         IndexSettings settings = new IndexSettings(metaData, Settings.EMPTY);
         assertEquals(actualValue, settings.getFlushThresholdSize());
         ByteSizeValue newTranslogFlushThresholdSize = new ByteSizeValue(Math.abs(randomInt()));
-        ByteSizeValue actualNewTranslogFlushThresholdSize = ByteSizeValue.parseBytesSizeValue(newTranslogFlushThresholdSize.toString(), IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey());
-        settings.updateIndexMetaData(newIndexMeta("index", Settings.builder().put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey(), newTranslogFlushThresholdSize.toString()).build()));
+        ByteSizeValue actualNewTranslogFlushThresholdSize = ByteSizeValue.parseBytesSizeValue(newTranslogFlushThresholdSize.toString(), IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey());
+        settings.updateIndexMetaData(newIndexMeta("index", Settings.builder().put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), newTranslogFlushThresholdSize.toString()).build()));
         assertEquals(actualNewTranslogFlushThresholdSize, settings.getFlushThresholdSize());
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java b/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
index 5d54f77..4ef513a 100644
--- a/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
+++ b/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
@@ -182,7 +182,7 @@ public class IndexWithShadowReplicasIT extends ESIntegTestCase {
         Settings idxSettings = Settings.builder()
                 .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
                 .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 2)
-                .put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB))
+                .put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB))
                 .put(IndexMetaData.SETTING_DATA_PATH, dataPath.toAbsolutePath().toString())
                 .put(IndexMetaData.SETTING_SHADOW_REPLICAS, true)
                 .put(IndexMetaData.SETTING_SHARED_FILESYSTEM, true)
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java
index f9531c3..b6a7dee 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java
@@ -260,7 +260,7 @@ public class SimpleDateMappingTests extends ESSingleNodeTestCase {
 
         NumericRangeQuery<Long> rangeQuery;
         try {
-            SearchContext.setCurrent(new TestSearchContext());
+            SearchContext.setCurrent(new TestSearchContext(null));
             rangeQuery = (NumericRangeQuery<Long>) defaultMapper.mappers().smartNameFieldMapper("date_field").fieldType().rangeQuery("10:00:00", "11:00:00", true, true).rewrite(null);
         } finally {
             SearchContext.removeCurrent();
@@ -286,7 +286,7 @@ public class SimpleDateMappingTests extends ESSingleNodeTestCase {
 
         NumericRangeQuery<Long> rangeQuery;
         try {
-            SearchContext.setCurrent(new TestSearchContext());
+            SearchContext.setCurrent(new TestSearchContext(null));
             rangeQuery = (NumericRangeQuery<Long>) defaultMapper.mappers().smartNameFieldMapper("date_field").fieldType().rangeQuery("Jan 02 10:00:00", "Jan 02 11:00:00", true, true).rewrite(null);
         } finally {
             SearchContext.removeCurrent();
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/SimpleExternalMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/SimpleExternalMappingTests.java
index 96c099d..bf92991 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/SimpleExternalMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/SimpleExternalMappingTests.java
@@ -63,7 +63,7 @@ public class SimpleExternalMappingTests extends ESSingleNodeTestCase {
                 Collections.singletonMap(ExternalMetadataMapper.CONTENT_TYPE, new ExternalMetadataMapper.TypeParser()));
 
         DocumentMapperParser parser = new DocumentMapperParser(indexService.getIndexSettings(), indexService.mapperService(),
-                indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::getQueryShardContext);
+                indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::newQueryShardContext);
         DocumentMapper documentMapper = parser.parse("type", new CompressedXContent(
                 XContentFactory.jsonBuilder().startObject().startObject("type")
                 .startObject(ExternalMetadataMapper.CONTENT_TYPE)
@@ -109,7 +109,7 @@ public class SimpleExternalMappingTests extends ESSingleNodeTestCase {
         MapperRegistry mapperRegistry = new MapperRegistry(mapperParsers, Collections.emptyMap());
 
         DocumentMapperParser parser = new DocumentMapperParser(indexService.getIndexSettings(), indexService.mapperService(),
-                indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::getQueryShardContext);
+                indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::newQueryShardContext);
 
         DocumentMapper documentMapper = parser.parse("type", new CompressedXContent(
                 XContentFactory.jsonBuilder().startObject().startObject("type").startObject("properties")
@@ -168,7 +168,7 @@ public class SimpleExternalMappingTests extends ESSingleNodeTestCase {
         MapperRegistry mapperRegistry = new MapperRegistry(mapperParsers, Collections.emptyMap());
 
         DocumentMapperParser parser = new DocumentMapperParser(indexService.getIndexSettings(), indexService.mapperService(),
-                indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::getQueryShardContext);
+                indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::newQueryShardContext);
 
         DocumentMapper documentMapper = parser.parse("type", new CompressedXContent(
                 XContentFactory.jsonBuilder().startObject().startObject("type").startObject("properties")
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapperTests.java
index e23ea15..2646d94 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapperTests.java
@@ -236,9 +236,9 @@ public class FieldNamesFieldMapperTests extends ESSingleNodeTestCase {
         IndicesModule indicesModule = new IndicesModule();
         indicesModule.registerMetadataMapper("_dummy", new DummyMetadataFieldMapper.TypeParser());
         final MapperRegistry mapperRegistry = indicesModule.getMapperRegistry();
-        MapperService mapperService = new MapperService(indexService.getIndexSettings(), indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::getQueryShardContext);
+        MapperService mapperService = new MapperService(indexService.getIndexSettings(), indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::newQueryShardContext);
         DocumentMapperParser parser = new DocumentMapperParser(indexService.getIndexSettings(), mapperService,
-                indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::getQueryShardContext);
+                indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::newQueryShardContext);
         String mapping = XContentFactory.jsonBuilder().startObject().startObject("type").endObject().endObject().string();
         DocumentMapper mapper = parser.parse("type", new CompressedXContent(mapping));
         ParsedDocument parsedDocument = mapper.parse("index", "type", "id", new BytesArray("{}"));
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java
index 6114185..f100182 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java
@@ -50,6 +50,7 @@ import org.elasticsearch.test.InternalSettingsPlugin;
 import org.elasticsearch.test.VersionUtils;
 import org.junit.Before;
 
+import java.io.IOException;
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Map;
@@ -280,6 +281,33 @@ public class SimpleStringMappingTests extends ESSingleNodeTestCase {
         }
     }
 
+    public void testSearchAnalyzerSerialization() throws IOException {
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
+                .startObject("properties")
+                    .startObject("field")
+                        .field("type", "string")
+                        .field("analyzer", "standard")
+                        .field("search_analyzer", "keyword")
+                    .endObject()
+                .endObject().endObject().endObject().string();
+
+        DocumentMapper mapper = parser.parse("type", new CompressedXContent(mapping));
+        assertEquals(mapping,  mapper.mappingSource().toString());
+
+        // special case: default index analyzer
+        mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
+                .startObject("properties")
+                    .startObject("field")
+                        .field("type", "string")
+                        .field("analyzer", "default")
+                        .field("search_analyzer", "keyword")
+                    .endObject()
+                .endObject().endObject().endObject().string();
+
+        mapper = parser.parse("type", new CompressedXContent(mapping));
+        assertEquals(mapping,  mapper.mappingSource().toString());
+    }
+
     private Map<String, Object> getSerializedMap(String fieldName, DocumentMapper mapper) throws Exception {
         FieldMapper fieldMapper = mapper.mappers().smartNameFieldMapper(fieldName);
         XContentBuilder builder = JsonXContent.contentBuilder().startObject();
diff --git a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
index 2435a34..e8f20cb 100644
--- a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
@@ -332,15 +332,14 @@ public abstract class AbstractQueryTestCase<QB extends AbstractQueryBuilder<QB>>
     }
 
     protected void setSearchContext(String[] types) {
-        TestSearchContext testSearchContext = new TestSearchContext();
-        testSearchContext.setTypes(types);
+        TestSearchContext testSearchContext = new TestSearchContext(queryShardContext);
+        testSearchContext.getQueryShardContext().setTypes(types);
         SearchContext.setCurrent(testSearchContext);
     }
 
     @After
     public void afterTest() {
         clientInvocationHandler.delegate = null;
-        QueryShardContext.removeTypes();
         SearchContext.removeCurrent();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/query/HasChildQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/HasChildQueryBuilderTests.java
index 8a2792e..fb61886 100644
--- a/core/src/test/java/org/elasticsearch/index/query/HasChildQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/HasChildQueryBuilderTests.java
@@ -84,7 +84,7 @@ public class HasChildQueryBuilderTests extends AbstractQueryTestCase<HasChildQue
     protected void setSearchContext(String[] types) {
         final MapperService mapperService = queryShardContext().getMapperService();
         final IndexFieldDataService fieldData = indexFieldDataService();
-        TestSearchContext testSearchContext = new TestSearchContext() {
+        TestSearchContext testSearchContext = new TestSearchContext(queryShardContext()) {
 
             @Override
             public MapperService mapperService() {
@@ -96,7 +96,7 @@ public class HasChildQueryBuilderTests extends AbstractQueryTestCase<HasChildQue
                 return fieldData; // need to build / parse inner hits sort fields
             }
         };
-        testSearchContext.setTypes(types);
+        testSearchContext.getQueryShardContext().setTypes(types);
         SearchContext.setCurrent(testSearchContext);
     }
 
@@ -230,11 +230,12 @@ public class HasChildQueryBuilderTests extends AbstractQueryTestCase<HasChildQue
     }
     public void testToQueryInnerQueryType() throws IOException {
         String[] searchTypes = new String[]{PARENT_TYPE};
-        QueryShardContext.setTypes(searchTypes);
+        QueryShardContext shardContext = createShardContext();
+        shardContext.setTypes(searchTypes);
         HasChildQueryBuilder hasChildQueryBuilder = new HasChildQueryBuilder(CHILD_TYPE, new IdsQueryBuilder().addIds("id"));
-        Query query = hasChildQueryBuilder.toQuery(createShardContext());
+        Query query = hasChildQueryBuilder.toQuery(shardContext);
         //verify that the context types are still the same as the ones we previously set
-        assertThat(QueryShardContext.getTypes(), equalTo(searchTypes));
+        assertThat(shardContext.getTypes(), equalTo(searchTypes));
         assertLateParsingQuery(query, CHILD_TYPE, "id");
     }
 
@@ -253,7 +254,7 @@ public class HasChildQueryBuilderTests extends AbstractQueryTestCase<HasChildQue
         ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) rewrittenTermsQuery;
         assertThat(constantScoreQuery.getQuery(), instanceOf(BooleanQuery.class));
         BooleanQuery booleanTermsQuery = (BooleanQuery) constantScoreQuery.getQuery();
-        assertThat(booleanTermsQuery.clauses().size(), equalTo(1));
+        assertThat(booleanTermsQuery.clauses().toString(), booleanTermsQuery.clauses().size(), equalTo(1));
         assertThat(booleanTermsQuery.clauses().get(0).getOccur(), equalTo(BooleanClause.Occur.SHOULD));
         assertThat(booleanTermsQuery.clauses().get(0).getQuery(), instanceOf(TermQuery.class));
         TermQuery termQuery = (TermQuery) booleanTermsQuery.clauses().get(0).getQuery();
diff --git a/core/src/test/java/org/elasticsearch/index/query/HasParentQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/HasParentQueryBuilderTests.java
index 8875641..33ac44e 100644
--- a/core/src/test/java/org/elasticsearch/index/query/HasParentQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/HasParentQueryBuilderTests.java
@@ -79,7 +79,7 @@ public class HasParentQueryBuilderTests extends AbstractQueryTestCase<HasParentQ
     protected void setSearchContext(String[] types) {
         final MapperService mapperService = queryShardContext().getMapperService();
         final IndexFieldDataService fieldData = indexFieldDataService();
-        TestSearchContext testSearchContext = new TestSearchContext() {
+        TestSearchContext testSearchContext = new TestSearchContext(queryShardContext()) {
 
             @Override
             public MapperService mapperService() {
@@ -91,7 +91,7 @@ public class HasParentQueryBuilderTests extends AbstractQueryTestCase<HasParentQ
                 return fieldData; // need to build / parse inner hits sort fields
             }
         };
-        testSearchContext.setTypes(types);
+        testSearchContext.getQueryShardContext().setTypes(types);
         SearchContext.setCurrent(testSearchContext);
     }
 
@@ -192,11 +192,12 @@ public class HasParentQueryBuilderTests extends AbstractQueryTestCase<HasParentQ
 
     public void testToQueryInnerQueryType() throws IOException {
         String[] searchTypes = new String[]{CHILD_TYPE};
-        QueryShardContext.setTypes(searchTypes);
+        QueryShardContext shardContext = createShardContext();
+        shardContext.setTypes(searchTypes);
         HasParentQueryBuilder hasParentQueryBuilder = new HasParentQueryBuilder(PARENT_TYPE, new IdsQueryBuilder().addIds("id"));
-        Query query = hasParentQueryBuilder.toQuery(createShardContext());
+        Query query = hasParentQueryBuilder.toQuery(shardContext);
         //verify that the context types are still the same as the ones we previously set
-        assertThat(QueryShardContext.getTypes(), equalTo(searchTypes));
+        assertThat(shardContext.getTypes(), equalTo(searchTypes));
         HasChildQueryBuilderTests.assertLateParsingQuery(query, PARENT_TYPE, "id");
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/query/NestedQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/NestedQueryBuilderTests.java
index 9f202a3..c06bc09 100644
--- a/core/src/test/java/org/elasticsearch/index/query/NestedQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/NestedQueryBuilderTests.java
@@ -60,7 +60,7 @@ public class NestedQueryBuilderTests extends AbstractQueryTestCase<NestedQueryBu
     protected void setSearchContext(String[] types) {
         final MapperService mapperService = queryShardContext().getMapperService();
         final IndexFieldDataService fieldData = indexFieldDataService();
-        TestSearchContext testSearchContext = new TestSearchContext() {
+        TestSearchContext testSearchContext = new TestSearchContext(queryShardContext()) {
 
             @Override
             public MapperService mapperService() {
@@ -72,7 +72,7 @@ public class NestedQueryBuilderTests extends AbstractQueryTestCase<NestedQueryBu
                 return fieldData; // need to build / parse inner hits sort fields
             }
         };
-        testSearchContext.setTypes(types);
+        testSearchContext.getQueryShardContext().setTypes(types);
         SearchContext.setCurrent(testSearchContext);
     }
 
@@ -139,45 +139,45 @@ public class NestedQueryBuilderTests extends AbstractQueryTestCase<NestedQueryBu
 
     public void testFromJson() throws IOException {
         String json =
-                "{\n" + 
-                "  \"nested\" : {\n" + 
-                "    \"query\" : {\n" + 
-                "      \"bool\" : {\n" + 
-                "        \"must\" : [ {\n" + 
-                "          \"match\" : {\n" + 
-                "            \"obj1.name\" : {\n" + 
-                "              \"query\" : \"blue\",\n" + 
-                "              \"type\" : \"boolean\",\n" + 
-                "              \"operator\" : \"OR\",\n" + 
-                "              \"slop\" : 0,\n" + 
-                "              \"prefix_length\" : 0,\n" + 
-                "              \"max_expansions\" : 50,\n" + 
-                "              \"fuzzy_transpositions\" : true,\n" + 
-                "              \"lenient\" : false,\n" + 
-                "              \"zero_terms_query\" : \"NONE\",\n" + 
-                "              \"boost\" : 1.0\n" + 
-                "            }\n" + 
-                "          }\n" + 
-                "        }, {\n" + 
-                "          \"range\" : {\n" + 
-                "            \"obj1.count\" : {\n" + 
-                "              \"from\" : 5,\n" + 
-                "              \"to\" : null,\n" + 
-                "              \"include_lower\" : false,\n" + 
-                "              \"include_upper\" : true,\n" + 
-                "              \"boost\" : 1.0\n" + 
-                "            }\n" + 
-                "          }\n" + 
-                "        } ],\n" + 
-                "        \"disable_coord\" : false,\n" + 
-                "        \"adjust_pure_negative\" : true,\n" + 
-                "        \"boost\" : 1.0\n" + 
-                "      }\n" + 
-                "    },\n" + 
-                "    \"path\" : \"obj1\",\n" + 
-                "    \"score_mode\" : \"avg\",\n" + 
-                "    \"boost\" : 1.0\n" + 
-                "  }\n" + 
+                "{\n" +
+                "  \"nested\" : {\n" +
+                "    \"query\" : {\n" +
+                "      \"bool\" : {\n" +
+                "        \"must\" : [ {\n" +
+                "          \"match\" : {\n" +
+                "            \"obj1.name\" : {\n" +
+                "              \"query\" : \"blue\",\n" +
+                "              \"type\" : \"boolean\",\n" +
+                "              \"operator\" : \"OR\",\n" +
+                "              \"slop\" : 0,\n" +
+                "              \"prefix_length\" : 0,\n" +
+                "              \"max_expansions\" : 50,\n" +
+                "              \"fuzzy_transpositions\" : true,\n" +
+                "              \"lenient\" : false,\n" +
+                "              \"zero_terms_query\" : \"NONE\",\n" +
+                "              \"boost\" : 1.0\n" +
+                "            }\n" +
+                "          }\n" +
+                "        }, {\n" +
+                "          \"range\" : {\n" +
+                "            \"obj1.count\" : {\n" +
+                "              \"from\" : 5,\n" +
+                "              \"to\" : null,\n" +
+                "              \"include_lower\" : false,\n" +
+                "              \"include_upper\" : true,\n" +
+                "              \"boost\" : 1.0\n" +
+                "            }\n" +
+                "          }\n" +
+                "        } ],\n" +
+                "        \"disable_coord\" : false,\n" +
+                "        \"adjust_pure_negative\" : true,\n" +
+                "        \"boost\" : 1.0\n" +
+                "      }\n" +
+                "    },\n" +
+                "    \"path\" : \"obj1\",\n" +
+                "    \"score_mode\" : \"avg\",\n" +
+                "    \"boost\" : 1.0\n" +
+                "  }\n" +
                 "}";
 
         NestedQueryBuilder parsed = (NestedQueryBuilder) parseQuery(json);
diff --git a/core/src/test/java/org/elasticsearch/index/query/ParentIdQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/ParentIdQueryBuilderTests.java
index fdb66c7..e397dac 100644
--- a/core/src/test/java/org/elasticsearch/index/query/ParentIdQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/ParentIdQueryBuilderTests.java
@@ -64,7 +64,7 @@ public class ParentIdQueryBuilderTests extends AbstractQueryTestCase<ParentIdQue
     protected void setSearchContext(String[] types) {
         final MapperService mapperService = queryShardContext().getMapperService();
         final IndexFieldDataService fieldData = indexFieldDataService();
-        TestSearchContext testSearchContext = new TestSearchContext() {
+        TestSearchContext testSearchContext = new TestSearchContext(queryShardContext()) {
 
             @Override
             public MapperService mapperService() {
@@ -76,7 +76,7 @@ public class ParentIdQueryBuilderTests extends AbstractQueryTestCase<ParentIdQue
                 return fieldData; // need to build / parse inner hits sort fields
             }
         };
-        testSearchContext.setTypes(types);
+        testSearchContext.getQueryShardContext().setTypes(types);
         SearchContext.setCurrent(testSearchContext);
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/query/plugin/CustomQueryParserIT.java b/core/src/test/java/org/elasticsearch/index/query/plugin/CustomQueryParserIT.java
index 886be82..1758d95 100644
--- a/core/src/test/java/org/elasticsearch/index/query/plugin/CustomQueryParserIT.java
+++ b/core/src/test/java/org/elasticsearch/index/query/plugin/CustomQueryParserIT.java
@@ -68,7 +68,7 @@ public class CustomQueryParserIT extends ESIntegTestCase {
 
     private static QueryShardContext queryShardContext() {
         IndicesService indicesService = internalCluster().getDataNodeInstance(IndicesService.class);
-        return indicesService.indexServiceSafe("index").getQueryShardContext();
+        return indicesService.indexServiceSafe("index").newQueryShardContext();
     }
 
     //see #11120
diff --git a/core/src/test/java/org/elasticsearch/index/search/MultiMatchQueryTests.java b/core/src/test/java/org/elasticsearch/index/search/MultiMatchQueryTests.java
index 46c8d37..b9da37a 100644
--- a/core/src/test/java/org/elasticsearch/index/search/MultiMatchQueryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/search/MultiMatchQueryTests.java
@@ -69,7 +69,7 @@ public class MultiMatchQueryTests extends ESSingleNodeTestCase {
     }
 
     public void testCrossFieldMultiMatchQuery() throws IOException {
-        QueryShardContext queryShardContext = indexService.getShard(0).getQueryShardContext();
+        QueryShardContext queryShardContext = indexService.newQueryShardContext();
         queryShardContext.setAllowUnmappedFields(true);
         Query parsedQuery = multiMatchQuery("banon").field("name.first", 2).field("name.last", 3).field("foobar").type(MultiMatchQueryBuilder.Type.CROSS_FIELDS).toQuery(queryShardContext);
         try (Engine.Searcher searcher = indexService.getShard(0).acquireSearcher("test")) {
diff --git a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
index 7788312..e5f0ade 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
@@ -704,7 +704,7 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         IndexService test = indicesService.indexService("test");
         IndexShard shard = test.getShardOrNull(0);
         assertFalse(shard.shouldFlush());
-        client().admin().indices().prepareUpdateSettings("test").setSettings(settingsBuilder().put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey(), new ByteSizeValue(133 /* size of the operation + header&footer*/, ByteSizeUnit.BYTES)).build()).get();
+        client().admin().indices().prepareUpdateSettings("test").setSettings(settingsBuilder().put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(133 /* size of the operation + header&footer*/, ByteSizeUnit.BYTES)).build()).get();
         client().prepareIndex("test", "test", "0").setSource("{}").setRefresh(randomBoolean()).get();
         assertFalse(shard.shouldFlush());
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, new ParseContext.Document(), new BytesArray(new byte[]{1}), null);
@@ -720,7 +720,7 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         shard.getEngine().getTranslog().sync();
         long size = shard.getEngine().getTranslog().sizeInBytes();
         logger.info("--> current translog size: [{}] num_ops [{}] generation [{}]", shard.getEngine().getTranslog().sizeInBytes(), shard.getEngine().getTranslog().totalOperations(), shard.getEngine().getTranslog().getGeneration());
-        client().admin().indices().prepareUpdateSettings("test").setSettings(settingsBuilder().put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey(), new ByteSizeValue(size, ByteSizeUnit.BYTES))
+        client().admin().indices().prepareUpdateSettings("test").setSettings(settingsBuilder().put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(size, ByteSizeUnit.BYTES))
                 .build()).get();
         client().prepareDelete("test", "test", "2").get();
         logger.info("--> translog size after delete: [{}] num_ops [{}] generation [{}]", shard.getEngine().getTranslog().sizeInBytes(), shard.getEngine().getTranslog().totalOperations(), shard.getEngine().getTranslog().getGeneration());
@@ -738,7 +738,7 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         IndexService test = indicesService.indexService("test");
         final IndexShard shard = test.getShardOrNull(0);
         assertFalse(shard.shouldFlush());
-        client().admin().indices().prepareUpdateSettings("test").setSettings(settingsBuilder().put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey(), new ByteSizeValue(133/* size of the operation + header&footer*/, ByteSizeUnit.BYTES)).build()).get();
+        client().admin().indices().prepareUpdateSettings("test").setSettings(settingsBuilder().put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(133/* size of the operation + header&footer*/, ByteSizeUnit.BYTES)).build()).get();
         client().prepareIndex("test", "test", "0").setSource("{}").setRefresh(randomBoolean()).get();
         assertFalse(shard.shouldFlush());
         final AtomicBoolean running = new AtomicBoolean(true);
diff --git a/core/src/test/java/org/elasticsearch/index/shard/ShardPathTests.java b/core/src/test/java/org/elasticsearch/index/shard/ShardPathTests.java
index 640ae9e..2a52e8c 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/ShardPathTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/ShardPathTests.java
@@ -109,7 +109,7 @@ public class ShardPathTests extends ESTestCase {
 
     public void testGetRootPaths() throws IOException {
         boolean useCustomDataPath = randomBoolean();
-        final Settings indexSetttings;
+        final Settings indexSettings;
         final Settings nodeSettings;
         Settings.Builder indexSettingsBuilder = settingsBuilder()
                 .put(IndexMetaData.SETTING_INDEX_UUID, "0xDEADBEEF")
@@ -118,7 +118,7 @@ public class ShardPathTests extends ESTestCase {
         if (useCustomDataPath) {
             final Path path = createTempDir();
             final boolean includeNodeId = randomBoolean();
-            indexSetttings = indexSettingsBuilder.put(IndexMetaData.SETTING_DATA_PATH, "custom").build();
+            indexSettings = indexSettingsBuilder.put(IndexMetaData.SETTING_DATA_PATH, "custom").build();
             nodeSettings = settingsBuilder().put(Environment.PATH_SHARED_DATA_SETTING.getKey(), path.toAbsolutePath().toAbsolutePath())
                     .put(NodeEnvironment.ADD_NODE_ID_TO_CUSTOM_PATH.getKey(), includeNodeId).build();
             if (includeNodeId) {
@@ -128,7 +128,7 @@ public class ShardPathTests extends ESTestCase {
             }
         } else {
             customPath = null;
-            indexSetttings = indexSettingsBuilder.build();
+            indexSettings = indexSettingsBuilder.build();
             nodeSettings = Settings.EMPTY;
         }
         try (final NodeEnvironment env = newNodeEnvironment(nodeSettings)) {
@@ -136,7 +136,7 @@ public class ShardPathTests extends ESTestCase {
             Path[] paths = env.availableShardPaths(shardId);
             Path path = randomFrom(paths);
             ShardStateMetaData.FORMAT.write(new ShardStateMetaData(2, true, "0xDEADBEEF", AllocationId.newInitializing()), 2, path);
-            ShardPath shardPath = ShardPath.loadShardPath(logger, env, shardId, IndexSettingsModule.newIndexSettings(shardId.getIndex(), indexSetttings));
+            ShardPath shardPath = ShardPath.loadShardPath(logger, env, shardId, IndexSettingsModule.newIndexSettings(shardId.getIndex(), indexSettings));
             boolean found = false;
             for (Path p : env.nodeDataPaths()) {
                 if (p.equals(shardPath.getRootStatePath())) {
diff --git a/core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java b/core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java
index 0f68030..79f8a0c 100644
--- a/core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java
+++ b/core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java
@@ -146,7 +146,7 @@ public class CorruptedFileIT extends ESIntegTestCase {
                         .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, "1")
                         .put(MergePolicyConfig.INDEX_MERGE_ENABLED, false)
                         .put(MockFSIndexStore.INDEX_CHECK_INDEX_ON_CLOSE_SETTING.getKey(), false) // no checkindex - we corrupt shards on purpose
-                        .put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)) // no translog based flush - it might change the .liv / segments.N files
+                        .put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)) // no translog based flush - it might change the .liv / segments.N files
         ));
         ensureGreen();
         disableAllocation("test");
@@ -250,7 +250,7 @@ public class CorruptedFileIT extends ESIntegTestCase {
                         .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, "0")
                         .put(MergePolicyConfig.INDEX_MERGE_ENABLED, false)
                         .put(MockFSIndexStore.INDEX_CHECK_INDEX_ON_CLOSE_SETTING.getKey(), false) // no checkindex - we corrupt shards on purpose
-                        .put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)) // no translog based flush - it might change the .liv / segments.N files
+                        .put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)) // no translog based flush - it might change the .liv / segments.N files
         ));
         ensureGreen();
         IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocs];
@@ -475,7 +475,7 @@ public class CorruptedFileIT extends ESIntegTestCase {
                         .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, "0") // no replicas for this test
                         .put(MergePolicyConfig.INDEX_MERGE_ENABLED, false)
                         .put(MockFSIndexStore.INDEX_CHECK_INDEX_ON_CLOSE_SETTING.getKey(), false) // no checkindex - we corrupt shards on purpose
-                        .put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)) // no translog based flush - it might change the .liv / segments.N files
+                        .put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)) // no translog based flush - it might change the .liv / segments.N files
         ));
         ensureGreen();
         IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocs];
@@ -529,7 +529,7 @@ public class CorruptedFileIT extends ESIntegTestCase {
                         .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, cluster().numDataNodes() - 1)
                         .put(MergePolicyConfig.INDEX_MERGE_ENABLED, false)
                         .put(MockFSIndexStore.INDEX_CHECK_INDEX_ON_CLOSE_SETTING.getKey(), false) // no checkindex - we corrupt shards on purpose
-                        .put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)) // no translog based flush - it might change the .liv / segments.N files
+                        .put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)) // no translog based flush - it might change the .liv / segments.N files
         ));
         ensureGreen();
         IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocs];
diff --git a/core/src/test/java/org/elasticsearch/index/store/CorruptedTranslogIT.java b/core/src/test/java/org/elasticsearch/index/store/CorruptedTranslogIT.java
index 500cf07..ae158b8 100644
--- a/core/src/test/java/org/elasticsearch/index/store/CorruptedTranslogIT.java
+++ b/core/src/test/java/org/elasticsearch/index/store/CorruptedTranslogIT.java
@@ -167,13 +167,13 @@ public class CorruptedTranslogIT extends ESIntegTestCase {
 
     /** Disables translog flushing for the specified index */
     private static void disableTranslogFlush(String index) {
-        Settings settings = Settings.builder().put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)).build();
+        Settings settings = Settings.builder().put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)).build();
         client().admin().indices().prepareUpdateSettings(index).setSettings(settings).get();
     }
 
     /** Enables translog flushing for the specified index */
     private static void enableTranslogFlush(String index) {
-        Settings settings = Settings.builder().put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey(), new ByteSizeValue(512, ByteSizeUnit.MB)).build();
+        Settings settings = Settings.builder().put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(512, ByteSizeUnit.MB)).build();
         client().admin().indices().prepareUpdateSettings(index).setSettings(settings).get();
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/indices/cache/query/IndicesRequestCacheIT.java b/core/src/test/java/org/elasticsearch/indices/cache/query/IndicesRequestCacheIT.java
index bc646b3..f09e033 100644
--- a/core/src/test/java/org/elasticsearch/indices/cache/query/IndicesRequestCacheIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/cache/query/IndicesRequestCacheIT.java
@@ -26,7 +26,6 @@ import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInter
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.test.ESIntegTestCase;
-import org.joda.time.DateTimeZone;
 
 import java.util.List;
 
@@ -51,19 +50,15 @@ public class IndicesRequestCacheIT extends ESIntegTestCase {
         // which used to not work well with the query cache because of the handles stream output
         // see #9500
         final SearchResponse r1 = client().prepareSearch("index").setSize(0).setSearchType(SearchType.QUERY_THEN_FETCH)
-                .addAggregation(dateHistogram("histo").field("f").timeZone(DateTimeZone.forID("+01:00")).minDocCount(0)
-                        .dateHistogramInterval(DateHistogramInterval.MONTH))
-                .get();
+            .addAggregation(dateHistogram("histo").field("f").timeZone("+01:00").minDocCount(0).interval(DateHistogramInterval.MONTH)).get();
         assertSearchResponse(r1);
 
         // The cached is actually used
         assertThat(client().admin().indices().prepareStats("index").setRequestCache(true).get().getTotal().getRequestCache().getMemorySizeInBytes(), greaterThan(0L));
 
         for (int i = 0; i < 10; ++i) {
-            final SearchResponse r2 = client().prepareSearch("index").setSize(0)
-                    .setSearchType(SearchType.QUERY_THEN_FETCH).addAggregation(dateHistogram("histo").field("f")
-                            .timeZone(DateTimeZone.forID("+01:00")).minDocCount(0).dateHistogramInterval(DateHistogramInterval.MONTH))
-                    .get();
+            final SearchResponse r2 = client().prepareSearch("index").setSize(0).setSearchType(SearchType.QUERY_THEN_FETCH)
+                    .addAggregation(dateHistogram("histo").field("f").timeZone("+01:00").minDocCount(0).interval(DateHistogramInterval.MONTH)).get();
             assertSearchResponse(r2);
             Histogram h1 = r1.getAggregations().get("histo");
             Histogram h2 = r2.getAggregations().get("histo");
diff --git a/core/src/test/java/org/elasticsearch/indices/flush/FlushIT.java b/core/src/test/java/org/elasticsearch/indices/flush/FlushIT.java
index d74e490..60a56c1 100644
--- a/core/src/test/java/org/elasticsearch/indices/flush/FlushIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/flush/FlushIT.java
@@ -154,7 +154,7 @@ public class FlushIT extends ESIntegTestCase {
         createIndex("test");
 
         client().admin().indices().prepareUpdateSettings("test").setSettings(
-                Settings.builder().put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)).put("index.refresh_interval", -1).put("index.number_of_replicas", internalCluster().numDataNodes() - 1))
+                Settings.builder().put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)).put("index.refresh_interval", -1).put("index.number_of_replicas", internalCluster().numDataNodes() - 1))
                 .get();
         ensureGreen();
         final AtomicBoolean stop = new AtomicBoolean(false);
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java b/core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java
index 58b4fe2..91fb8b5 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java
@@ -24,7 +24,6 @@ import org.apache.lucene.search.TermQuery;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.percolate.PercolateShardRequest;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.Index;
@@ -88,10 +87,8 @@ public class PercolateDocumentParserTests extends ESTestCase {
         queryShardContext = new QueryShardContext(indexSettings, null, null, null, mapperService, null, null, indicesQueriesRegistry);
 
         HighlightPhase highlightPhase = new HighlightPhase(Settings.EMPTY, new Highlighters());
-        AggregatorParsers aggregatorParsers = new AggregatorParsers(Collections.emptySet(), Collections.emptySet(),
-                new NamedWriteableRegistry());
-        AggregationPhase aggregationPhase = new AggregationPhase(new AggregationParseElement(aggregatorParsers, indicesQueriesRegistry),
-                new AggregationBinaryParseElement(aggregatorParsers, indicesQueriesRegistry));
+        AggregatorParsers aggregatorParsers = new AggregatorParsers(Collections.emptySet(), Collections.emptySet());
+        AggregationPhase aggregationPhase = new AggregationPhase(new AggregationParseElement(aggregatorParsers), new AggregationBinaryParseElement(aggregatorParsers));
         parser = new PercolateDocumentParser(highlightPhase, new SortParseElement(), aggregationPhase);
 
         request = Mockito.mock(PercolateShardRequest.class);
@@ -107,8 +104,8 @@ public class PercolateDocumentParserTests extends ESTestCase {
                 .endObject();
         Mockito.when(request.source()).thenReturn(source.bytes());
 
-        PercolateContext context = new PercolateContext(request, new SearchShardTarget("_node", new Index("_index", "_na_"), 0), mapperService);
-        ParsedDocument parsedDocument = parser.parse(request, context, mapperService, queryShardContext);
+        PercolateContext context = new PercolateContext(request, new SearchShardTarget("_node", new Index("_index", "_na_"), 0), mapperService, queryShardContext);
+        ParsedDocument parsedDocument = parser.parse(request, context, mapperService);
         assertThat(parsedDocument.rootDoc().get("field1"), equalTo("value1"));
     }
 
@@ -126,8 +123,8 @@ public class PercolateDocumentParserTests extends ESTestCase {
                 .endObject();
         Mockito.when(request.source()).thenReturn(source.bytes());
 
-        PercolateContext context = new PercolateContext(request, new SearchShardTarget("_node", new Index("_index", "_na_"), 0), mapperService);
-        ParsedDocument parsedDocument = parser.parse(request, context, mapperService, queryShardContext);
+        PercolateContext context = new PercolateContext(request, new SearchShardTarget("_node", new Index("_index", "_na_"), 0), mapperService, queryShardContext);
+        ParsedDocument parsedDocument = parser.parse(request, context, mapperService);
         assertThat(parsedDocument.rootDoc().get("field1"), equalTo("value1"));
         assertThat(context.percolateQuery(), equalTo(new TermQuery(new Term("field1", "value1"))));
         assertThat(context.trackScores(), is(true));
@@ -150,8 +147,8 @@ public class PercolateDocumentParserTests extends ESTestCase {
         Mockito.when(request.source()).thenReturn(source.bytes());
         Mockito.when(request.docSource()).thenReturn(docSource.bytes());
 
-        PercolateContext context = new PercolateContext(request, new SearchShardTarget("_node", new Index("_index", "_na_"), 0), mapperService);
-        ParsedDocument parsedDocument = parser.parse(request, context, mapperService, queryShardContext);
+        PercolateContext context = new PercolateContext(request, new SearchShardTarget("_node", new Index("_index", "_na_"), 0), mapperService, queryShardContext);
+        ParsedDocument parsedDocument = parser.parse(request, context, mapperService);
         assertThat(parsedDocument.rootDoc().get("field1"), equalTo("value1"));
         assertThat(context.percolateQuery(), equalTo(new TermQuery(new Term("field1", "value1"))));
         assertThat(context.trackScores(), is(true));
@@ -177,9 +174,9 @@ public class PercolateDocumentParserTests extends ESTestCase {
         Mockito.when(request.source()).thenReturn(source.bytes());
         Mockito.when(request.docSource()).thenReturn(docSource.bytes());
 
-        PercolateContext context = new PercolateContext(request, new SearchShardTarget("_node", new Index("_index", "_na_"), 0), mapperService);
+        PercolateContext context = new PercolateContext(request, new SearchShardTarget("_node", new Index("_index", "_na_"), 0), mapperService, queryShardContext);
         try {
-            parser.parse(request, context, mapperService, queryShardContext);
+            parser.parse(request, context, mapperService);
         } catch (IllegalArgumentException e) {
             assertThat(e.getMessage(), equalTo("Can't specify the document to percolate in the source of the request and as document id"));
         }
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java b/core/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java
index 7bac10d..b8ed2cc 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java
@@ -165,7 +165,7 @@ public class PercolatorAggregationsIT extends ESIntegTestCase {
                 percolateRequestBuilder.setOnlyCount(countOnly);
             }
 
-            percolateRequestBuilder.addAggregation(PipelineAggregatorBuilders.maxBucket("max_a", "a>_count"));
+            percolateRequestBuilder.addAggregation(PipelineAggregatorBuilders.maxBucket("max_a").setBucketsPaths("a>_count"));
 
             PercolateResponse response = percolateRequestBuilder.execute().actionGet();
             assertMatchCount(response, expectedCount[i % numUniqueQueries]);
@@ -245,7 +245,7 @@ public class PercolatorAggregationsIT extends ESIntegTestCase {
                 percolateRequestBuilder.setOnlyCount(countOnly);
             }
 
-            percolateRequestBuilder.addAggregation(PipelineAggregatorBuilders.maxBucket("max_terms", "terms>_count"));
+            percolateRequestBuilder.addAggregation(PipelineAggregatorBuilders.maxBucket("max_terms").setBucketsPaths("terms>_count"));
 
             PercolateResponse response = percolateRequestBuilder.execute().actionGet();
             assertMatchCount(response, numQueries);
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java b/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
index 9bd5a74..be87cc0 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
@@ -1351,12 +1351,7 @@ public class PercolatorIT extends ESIntegTestCase {
         assertThat(convertFromTextArray(response.getMatches(), "test"), arrayContainingInAnyOrder("1", "2", "3", "4", "5"));
 
         PercolateResponse.Match[] matches = response.getMatches();
-        Arrays.sort(matches, new Comparator<PercolateResponse.Match>() {
-            @Override
-            public int compare(PercolateResponse.Match a, PercolateResponse.Match b) {
-                return a.getId().compareTo(b.getId());
-            }
-        });
+        Arrays.sort(matches, (a, b) -> a.getId().compareTo(b.getId()));
 
         assertThat(matches[0].getHighlightFields().get("field1").fragments()[0].string(), equalTo("The quick <em>brown</em> <em>fox</em> jumps over the lazy dog"));
         assertThat(matches[1].getHighlightFields().get("field1").fragments()[0].string(), equalTo("The quick brown fox jumps over the <em>lazy</em> <em>dog</em>"));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationCollectorTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationCollectorTests.java
index 29eb13d..178c7ff 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationCollectorTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationCollectorTests.java
@@ -19,12 +19,9 @@
 
 package org.elasticsearch.search.aggregations;
 
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
 import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESSingleNodeTestCase;
@@ -37,7 +34,7 @@ public class AggregationCollectorTests extends ESSingleNodeTestCase {
         IndexService index = createIndex("idx");
         client().prepareIndex("idx", "type", "1").setSource("f", 5).execute().get();
         client().admin().indices().prepareRefresh("idx").get();
-
+        
         // simple field aggregation, no scores needed
         String fieldAgg = "{ \"my_terms\": {\"terms\": {\"field\": \"f\"}}}";
         assertFalse(needsScores(index, fieldAgg));
@@ -61,16 +58,12 @@ public class AggregationCollectorTests extends ESSingleNodeTestCase {
 
     private boolean needsScores(IndexService index, String agg) throws IOException {
         AggregatorParsers parser = getInstanceFromNode(AggregatorParsers.class);
-        IndicesQueriesRegistry queriesRegistry = getInstanceFromNode(IndicesQueriesRegistry.class);
         XContentParser aggParser = JsonXContent.jsonXContent.createParser(agg);
-        QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-        parseContext.reset(aggParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
         aggParser.nextToken();
         SearchContext searchContext = createSearchContext(index);
-        AggregationContext aggContext = new AggregationContext(searchContext);
-        final AggregatorFactories factories = parser.parseAggregators(aggParser, parseContext).build(aggContext, null);
-        final Aggregator[] aggregators = factories.createTopLevelAggregators();
+        final AggregatorFactories factories = parser.parseAggregators(aggParser, searchContext);
+        AggregationContext aggregationContext = new AggregationContext(searchContext);
+        final Aggregator[] aggregators = factories.createTopLevelAggregators(aggregationContext);
         assertEquals(1, aggregators.length);
         return aggregators[0].needsScores();
     }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java
new file mode 100644
index 0000000..1b8d0bb
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java
@@ -0,0 +1,142 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations;
+
+import org.apache.lucene.util.LuceneTestCase.AwaitsFix;
+import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.client.Requests;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.json.JsonXContent;
+import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
+import org.elasticsearch.test.ESIntegTestCase;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.core.IsNull.notNullValue;
+
+@ESIntegTestCase.SuiteScopeTestCase
+@AwaitsFix(bugUrl = "needs fixing after the search request refactor. Do we need agg binary?")
+// NO RELEASE
+public class AggregationsBinaryIT extends ESIntegTestCase {
+
+    private static final String STRING_FIELD_NAME = "s_value";
+    private static final String INT_FIELD_NAME = "i_value";
+
+    @Override
+    public void setupSuiteScopeCluster() throws Exception {
+        createIndex("idx");
+        List<IndexRequestBuilder> builders = new ArrayList<>();
+        for (int i = 0; i < 5; i++) {
+            builders.add(client().prepareIndex("idx", "type").setSource(
+                    jsonBuilder().startObject().field(STRING_FIELD_NAME, "val" + i).field(INT_FIELD_NAME, i).endObject()));
+        }
+        indexRandom(true, builders);
+        ensureSearchable();
+    }
+
+    public void testAggregationsBinary() throws Exception {
+        TermsBuilder termsBuilder = AggregationBuilders.terms("terms").field(STRING_FIELD_NAME);
+        TermsBuilder subTerm = AggregationBuilders.terms("subterms").field(INT_FIELD_NAME);
+
+        // Create an XContentBuilder from sub aggregation
+        XContentBuilder subTermContentBuilder = JsonXContent.contentBuilder().startObject();
+        subTerm.toXContent(subTermContentBuilder, ToXContent.EMPTY_PARAMS);
+        subTermContentBuilder.endObject();
+
+        // Add sub aggregation as a XContentBuilder (binary_aggregation)
+        termsBuilder.subAggregation(subTermContentBuilder);
+
+        SearchResponse response = client().prepareSearch("idx").setTypes("type").addAggregation(termsBuilder).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("val" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(bucket.getKeyAsString(), equalTo("val" + i));
+            assertThat(bucket.getDocCount(), equalTo(1L));
+            Aggregations subAggs = bucket.getAggregations();
+            assertThat(subAggs, notNullValue());
+            assertThat(subAggs.asList().size(), equalTo(1));
+            Terms subTerms = subAggs.get("subterms");
+            assertThat(subTerms, notNullValue());
+            List<Bucket> subTermsBuckets = subTerms.getBuckets();
+            assertThat(subTermsBuckets, notNullValue());
+            assertThat(subTermsBuckets.size(), equalTo(1));
+            assertThat(((Number) subTermsBuckets.get(0).getKey()).intValue(), equalTo(i));
+            assertThat(subTermsBuckets.get(0).getDocCount(), equalTo(1L));
+        }
+    }
+
+    public void testAggregationsBinarySameContentType() throws Exception {
+        TermsBuilder termsBuilder = AggregationBuilders.terms("terms").field(STRING_FIELD_NAME);
+        TermsBuilder subTerm = AggregationBuilders.terms("subterms").field(INT_FIELD_NAME);
+
+        // Create an XContentBuilder from sub aggregation
+
+        XContentBuilder subTermContentBuilder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+        subTermContentBuilder.startObject();
+        subTerm.toXContent(subTermContentBuilder, ToXContent.EMPTY_PARAMS);
+        subTermContentBuilder.endObject();
+
+        // Add sub aggregation as a XContentBuilder (binary_aggregation)
+        termsBuilder.subAggregation(subTermContentBuilder);
+
+        SearchResponse response = client().prepareSearch("idx").setTypes("type").addAggregation(termsBuilder).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("val" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(bucket.getKeyAsString(), equalTo("val" + i));
+            assertThat(bucket.getDocCount(), equalTo(1L));
+            Aggregations subAggs = bucket.getAggregations();
+            assertThat(subAggs, notNullValue());
+            assertThat(subAggs.asList().size(), equalTo(1));
+            Terms subTerms = subAggs.get("subterms");
+            assertThat(subTerms, notNullValue());
+            List<Bucket> subTermsBuckets = subTerms.getBuckets();
+            assertThat(subTermsBuckets, notNullValue());
+            assertThat(subTermsBuckets.size(), equalTo(1));
+            assertThat(((Number) subTermsBuckets.get(0).getKey()).intValue(), equalTo(i));
+            assertThat(subTermsBuckets.get(0).getDocCount(), equalTo(1L));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java b/core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java
deleted file mode 100644
index 66cb557..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java
+++ /dev/null
@@ -1,350 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.query.AbstractQueryTestCase;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.script.MockScriptEngine;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.ScriptContextRegistry;
-import org.elasticsearch.script.ScriptEngineRegistry;
-import org.elasticsearch.script.ScriptEngineService;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.ScriptSettings;
-import org.elasticsearch.search.SearchModule;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.IndexSettingsModule;
-import org.elasticsearch.test.InternalSettingsPlugin;
-import org.elasticsearch.test.TestSearchContext;
-import org.elasticsearch.test.VersionUtils;
-import org.elasticsearch.test.cluster.TestClusterService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public abstract class BaseAggregationTestCase<AB extends AggregatorBuilder<AB>> extends ESTestCase {
-
-    protected static final String STRING_FIELD_NAME = "mapped_string";
-    protected static final String INT_FIELD_NAME = "mapped_int";
-    protected static final String DOUBLE_FIELD_NAME = "mapped_double";
-    protected static final String BOOLEAN_FIELD_NAME = "mapped_boolean";
-    protected static final String DATE_FIELD_NAME = "mapped_date";
-    protected static final String OBJECT_FIELD_NAME = "mapped_object";
-    protected static final String[] mappedFieldNames = new String[] { STRING_FIELD_NAME, INT_FIELD_NAME,
-            DOUBLE_FIELD_NAME, BOOLEAN_FIELD_NAME, DATE_FIELD_NAME, OBJECT_FIELD_NAME };
-
-    private static Injector injector;
-    private static Index index;
-
-    private static String[] currentTypes;
-
-    protected static String[] getCurrentTypes() {
-        return currentTypes;
-    }
-
-    private static NamedWriteableRegistry namedWriteableRegistry;
-
-    private static AggregatorParsers aggParsers;
-    private static IndicesQueriesRegistry queriesRegistry;
-    private static ParseFieldMatcher parseFieldMatcher;
-
-    protected abstract AB createTestAggregatorBuilder();
-
-    /**
-     * Setup for the whole base test class.
-     */
-    @BeforeClass
-    public static void init() throws IOException {
-        // we have to prefer CURRENT since with the range of versions we support it's rather unlikely to get the current actually.
-        Version version = randomBoolean() ? Version.CURRENT
-                : VersionUtils.randomVersionBetween(random(), Version.V_2_0_0_beta1, Version.CURRENT);
-        Settings settings = Settings.settingsBuilder()
-                .put("node.name", AbstractQueryTestCase.class.toString())
-                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
-                .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false)
-                .build();
-
-        namedWriteableRegistry =  new NamedWriteableRegistry();
-        index = new Index(randomAsciiOfLengthBetween(1, 10), "_na_");
-        Settings indexSettings = Settings.settingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, version).build();
-        final TestClusterService clusterService = new TestClusterService();
-        clusterService.setState(new ClusterState.Builder(clusterService.state()).metaData(new MetaData.Builder()
-                .put(new IndexMetaData.Builder(index.getName()).settings(indexSettings).numberOfShards(1).numberOfReplicas(0))));
-        SettingsModule settingsModule = new SettingsModule(settings);
-        settingsModule.registerSetting(InternalSettingsPlugin.VERSION_CREATED);
-        ScriptModule scriptModule = new ScriptModule() {
-            @Override
-            protected void configure() {
-                Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
-                        // no file watching, so we don't need a
-                        // ResourceWatcherService
-                        .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false).build();
-                MockScriptEngine mockScriptEngine = new MockScriptEngine();
-                Multibinder<ScriptEngineService> multibinder = Multibinder.newSetBinder(binder(), ScriptEngineService.class);
-                multibinder.addBinding().toInstance(mockScriptEngine);
-                Set<ScriptEngineService> engines = new HashSet<>();
-                engines.add(mockScriptEngine);
-                List<ScriptContext.Plugin> customContexts = new ArrayList<>();
-                ScriptEngineRegistry scriptEngineRegistry = new ScriptEngineRegistry(Collections
-                        .singletonList(new ScriptEngineRegistry.ScriptEngineRegistration(MockScriptEngine.class, MockScriptEngine.TYPES)));
-                bind(ScriptEngineRegistry.class).toInstance(scriptEngineRegistry);
-                ScriptContextRegistry scriptContextRegistry = new ScriptContextRegistry(customContexts);
-                bind(ScriptContextRegistry.class).toInstance(scriptContextRegistry);
-                ScriptSettings scriptSettings = new ScriptSettings(scriptEngineRegistry, scriptContextRegistry);
-                bind(ScriptSettings.class).toInstance(scriptSettings);
-                try {
-                    ScriptService scriptService = new ScriptService(settings, new Environment(settings), engines, null,
-                            scriptEngineRegistry, scriptContextRegistry, scriptSettings);
-                    bind(ScriptService.class).toInstance(scriptService);
-                } catch (IOException e) {
-                    throw new IllegalStateException("error while binding ScriptService", e);
-                }
-            }
-        };
-        scriptModule.prepareSettings(settingsModule);
-        injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)),
-                settingsModule,
-                new ThreadPoolModule(new ThreadPool(settings)),
-                scriptModule,
-                new IndicesModule() {
-
-                    @Override
-                    protected void configure() {
-                        bindMapperExtension();
-                    }
-                }, new SearchModule(settings, namedWriteableRegistry) {
-                    @Override
-                    protected void configureSearch() {
-                        // Skip me
-                    }
-                    @Override
-                    protected void configureSuggesters() {
-                        // Skip me
-                    }
-                },
-                new IndexSettingsModule(index, settings),
-
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        bind(ClusterService.class).toProvider(Providers.of(clusterService));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                        bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry);
-                    }
-                }
-        ).createInjector();
-        aggParsers = injector.getInstance(AggregatorParsers.class);
-        //create some random type with some default field, those types will stick around for all of the subclasses
-        currentTypes = new String[randomIntBetween(0, 5)];
-        for (int i = 0; i < currentTypes.length; i++) {
-            String type = randomAsciiOfLengthBetween(1, 10);
-            currentTypes[i] = type;
-        }
-        queriesRegistry = injector.getInstance(IndicesQueriesRegistry.class);
-        parseFieldMatcher = ParseFieldMatcher.STRICT;
-    }
-
-    @AfterClass
-    public static void afterClass() throws Exception {
-        terminate(injector.getInstance(ThreadPool.class));
-        injector = null;
-        index = null;
-        aggParsers = null;
-        currentTypes = null;
-        namedWriteableRegistry = null;
-    }
-
-    @Before
-    public void beforeTest() {
-        //set some random types to be queried as part the search request, before each test
-        String[] types = getRandomTypes();
-        TestSearchContext testSearchContext = new TestSearchContext();
-        testSearchContext.setTypes(types);
-        SearchContext.setCurrent(testSearchContext);
-    }
-
-    @After
-    public void afterTest() {
-        SearchContext.removeCurrent();
-    }
-
-    /**
-     * Generic test that creates new AggregatorFactory from the test
-     * AggregatorFactory and checks both for equality and asserts equality on
-     * the two queries.
-     */
-    public void testFromXContent() throws IOException {
-        AB testAgg = createTestAggregatorBuilder();
-        AggregatorFactories.Builder factoriesBuilder = AggregatorFactories.builder().addAggregator(testAgg);
-        String contentString = factoriesBuilder.toString();
-        XContentParser parser = XContentFactory.xContent(contentString).createParser(contentString);
-        QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-        parseContext.reset(parser);
-        parseContext.parseFieldMatcher(parseFieldMatcher);
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.FIELD_NAME, parser.nextToken());
-        assertEquals(testAgg.name, parser.currentName());
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.FIELD_NAME, parser.nextToken());
-        assertEquals(testAgg.type.name(), parser.currentName());
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        AggregatorBuilder newAgg = aggParsers.parser(testAgg.getType()).parse(testAgg.name, parser, parseContext);
-        assertSame(XContentParser.Token.END_OBJECT, parser.currentToken());
-        assertSame(XContentParser.Token.END_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.END_OBJECT, parser.nextToken());
-        assertNull(parser.nextToken());
-        assertNotNull(newAgg);
-        assertNotSame(newAgg, testAgg);
-        assertEquals(testAgg, newAgg);
-        assertEquals(testAgg.hashCode(), newAgg.hashCode());
-    }
-
-    /**
-     * Test serialization and deserialization of the test AggregatorFactory.
-     */
-
-    public void testSerialization() throws IOException {
-        AB testAgg = createTestAggregatorBuilder();
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            testAgg.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                AggregatorBuilder prototype = (AggregatorBuilder) namedWriteableRegistry.getPrototype(AggregatorBuilder.class, 
-                    testAgg.getWriteableName());
-                AggregatorBuilder deserializedQuery = prototype.readFrom(in);
-                assertEquals(deserializedQuery, testAgg);
-                assertEquals(deserializedQuery.hashCode(), testAgg.hashCode());
-                assertNotSame(deserializedQuery, testAgg);
-            }
-        }
-    }
-
-
-    public void testEqualsAndHashcode() throws IOException {
-        AB firstAgg = createTestAggregatorBuilder();
-        assertFalse("aggregation is equal to null", firstAgg.equals(null));
-        assertFalse("aggregation is equal to incompatible type", firstAgg.equals(""));
-        assertTrue("aggregation is not equal to self", firstAgg.equals(firstAgg));
-        assertThat("same aggregation's hashcode returns different values if called multiple times", firstAgg.hashCode(),
-                equalTo(firstAgg.hashCode()));
-
-        AB secondQuery = copyAggregation(firstAgg);
-        assertTrue("aggregation is not equal to self", secondQuery.equals(secondQuery));
-        assertTrue("aggregation is not equal to its copy", firstAgg.equals(secondQuery));
-        assertTrue("equals is not symmetric", secondQuery.equals(firstAgg));
-        assertThat("aggregation copy's hashcode is different from original hashcode", secondQuery.hashCode(), equalTo(firstAgg.hashCode()));
-
-        AB thirdQuery = copyAggregation(secondQuery);
-        assertTrue("aggregation is not equal to self", thirdQuery.equals(thirdQuery));
-        assertTrue("aggregation is not equal to its copy", secondQuery.equals(thirdQuery));
-        assertThat("aggregation copy's hashcode is different from original hashcode", secondQuery.hashCode(),
-                equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not transitive", firstAgg.equals(thirdQuery));
-        assertThat("aggregation copy's hashcode is different from original hashcode", firstAgg.hashCode(), equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not symmetric", thirdQuery.equals(secondQuery));
-        assertTrue("equals is not symmetric", thirdQuery.equals(firstAgg));
-    }
-
-    // we use the streaming infra to create a copy of the query provided as
-    // argument
-    private AB copyAggregation(AB agg) throws IOException {
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            agg.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                AggregatorBuilder prototype = (AggregatorBuilder) namedWriteableRegistry.getPrototype(AggregatorBuilder.class, 
-                    agg.getWriteableName());
-                @SuppressWarnings("unchecked")
-                AB secondAgg = (AB) prototype.readFrom(in);
-                return secondAgg;
-            }
-        }
-    }
-
-    protected String[] getRandomTypes() {
-        String[] types;
-        if (currentTypes.length > 0 && randomBoolean()) {
-            int numberOfQueryTypes = randomIntBetween(1, currentTypes.length);
-            types = new String[numberOfQueryTypes];
-            for (int i = 0; i < numberOfQueryTypes; i++) {
-                types[i] = randomFrom(currentTypes);
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[] { MetaData.ALL };
-            } else {
-                types = new String[0];
-            }
-        }
-        return types;
-    }
-
-    public String randomNumericField() {
-        int randomInt = randomInt(3);
-        switch (randomInt) {
-        case 0:
-            return DATE_FIELD_NAME;
-        case 1:
-            return DOUBLE_FIELD_NAME;
-        case 2:
-        default:
-            return INT_FIELD_NAME;
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/BasePipelineAggregationTestCase.java b/core/src/test/java/org/elasticsearch/search/aggregations/BasePipelineAggregationTestCase.java
deleted file mode 100644
index 443a5c6..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/BasePipelineAggregationTestCase.java
+++ /dev/null
@@ -1,351 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.query.AbstractQueryTestCase;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.script.MockScriptEngine;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.ScriptContextRegistry;
-import org.elasticsearch.script.ScriptEngineRegistry;
-import org.elasticsearch.script.ScriptEngineService;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.ScriptSettings;
-import org.elasticsearch.search.SearchModule;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.IndexSettingsModule;
-import org.elasticsearch.test.InternalSettingsPlugin;
-import org.elasticsearch.test.TestSearchContext;
-import org.elasticsearch.test.VersionUtils;
-import org.elasticsearch.test.cluster.TestClusterService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public abstract class BasePipelineAggregationTestCase<AF extends PipelineAggregatorFactory> extends ESTestCase {
-
-    protected static final String STRING_FIELD_NAME = "mapped_string";
-    protected static final String INT_FIELD_NAME = "mapped_int";
-    protected static final String DOUBLE_FIELD_NAME = "mapped_double";
-    protected static final String BOOLEAN_FIELD_NAME = "mapped_boolean";
-    protected static final String DATE_FIELD_NAME = "mapped_date";
-    protected static final String OBJECT_FIELD_NAME = "mapped_object";
-    protected static final String[] mappedFieldNames = new String[] { STRING_FIELD_NAME, INT_FIELD_NAME,
-            DOUBLE_FIELD_NAME, BOOLEAN_FIELD_NAME, DATE_FIELD_NAME, OBJECT_FIELD_NAME };
-
-    private static Injector injector;
-    private static Index index;
-
-    private static String[] currentTypes;
-
-    protected static String[] getCurrentTypes() {
-        return currentTypes;
-    }
-
-    private static NamedWriteableRegistry namedWriteableRegistry;
-
-    private static AggregatorParsers aggParsers;
-    private static ParseFieldMatcher parseFieldMatcher;
-    private static IndicesQueriesRegistry queriesRegistry;
-
-    protected abstract AF createTestAggregatorFactory();
-
-    /**
-     * Setup for the whole base test class.
-     */
-    @BeforeClass
-    public static void init() throws IOException {
-     // we have to prefer CURRENT since with the range of versions we support it's rather unlikely to get the current actually.
-        Version version = randomBoolean() ? Version.CURRENT
-                : VersionUtils.randomVersionBetween(random(), Version.V_2_0_0_beta1, Version.CURRENT);
-        Settings settings = Settings.settingsBuilder()
-                .put("node.name", AbstractQueryTestCase.class.toString())
-                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
-                .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false)
-                .build();
-
-        namedWriteableRegistry =  new NamedWriteableRegistry();
-        index = new Index(randomAsciiOfLengthBetween(1, 10), "_na_");
-        Settings indexSettings = Settings.settingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, version).build();
-        final TestClusterService clusterService = new TestClusterService();
-        clusterService.setState(new ClusterState.Builder(clusterService.state()).metaData(new MetaData.Builder()
-                .put(new IndexMetaData.Builder(index.getName()).settings(indexSettings).numberOfShards(1).numberOfReplicas(0))));
-        SettingsModule settingsModule = new SettingsModule(settings);
-        settingsModule.registerSetting(InternalSettingsPlugin.VERSION_CREATED);
-        ScriptModule scriptModule = new ScriptModule() {
-            @Override
-            protected void configure() {
-                Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
-                        // no file watching, so we don't need a
-                        // ResourceWatcherService
-                        .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false).build();
-                MockScriptEngine mockScriptEngine = new MockScriptEngine();
-                Multibinder<ScriptEngineService> multibinder = Multibinder.newSetBinder(binder(), ScriptEngineService.class);
-                multibinder.addBinding().toInstance(mockScriptEngine);
-                Set<ScriptEngineService> engines = new HashSet<>();
-                engines.add(mockScriptEngine);
-                List<ScriptContext.Plugin> customContexts = new ArrayList<>();
-                ScriptEngineRegistry scriptEngineRegistry = new ScriptEngineRegistry(Collections
-                        .singletonList(new ScriptEngineRegistry.ScriptEngineRegistration(MockScriptEngine.class, MockScriptEngine.TYPES)));
-                bind(ScriptEngineRegistry.class).toInstance(scriptEngineRegistry);
-                ScriptContextRegistry scriptContextRegistry = new ScriptContextRegistry(customContexts);
-                bind(ScriptContextRegistry.class).toInstance(scriptContextRegistry);
-                ScriptSettings scriptSettings = new ScriptSettings(scriptEngineRegistry, scriptContextRegistry);
-                bind(ScriptSettings.class).toInstance(scriptSettings);
-                try {
-                    ScriptService scriptService = new ScriptService(settings, new Environment(settings), engines, null,
-                            scriptEngineRegistry, scriptContextRegistry, scriptSettings);
-                    bind(ScriptService.class).toInstance(scriptService);
-                } catch (IOException e) {
-                    throw new IllegalStateException("error while binding ScriptService", e);
-                }
-            }
-        };
-        scriptModule.prepareSettings(settingsModule);
-        injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)),
-                settingsModule,
-                new ThreadPoolModule(new ThreadPool(settings)),
-                scriptModule,
-                new IndicesModule() {
-
-                    @Override
-                    protected void configure() {
-                        bindMapperExtension();
-                    }
-                }, new SearchModule(settings, namedWriteableRegistry) {
-                    @Override
-                    protected void configureSearch() {
-                        // Skip me
-                    }
-                    @Override
-                    protected void configureSuggesters() {
-                        // Skip me
-                    }
-                },
-                new IndexSettingsModule(index, settings),
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        bind(ClusterService.class).toProvider(Providers.of(clusterService));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                        bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry);
-                    }
-                }
-        ).createInjector();
-        aggParsers = injector.getInstance(AggregatorParsers.class);
-        //create some random type with some default field, those types will stick around for all of the subclasses
-        currentTypes = new String[randomIntBetween(0, 5)];
-        for (int i = 0; i < currentTypes.length; i++) {
-            String type = randomAsciiOfLengthBetween(1, 10);
-            currentTypes[i] = type;
-        }
-        queriesRegistry = injector.getInstance(IndicesQueriesRegistry.class);
-        parseFieldMatcher = ParseFieldMatcher.STRICT;
-    }
-
-    @AfterClass
-    public static void afterClass() throws Exception {
-        terminate(injector.getInstance(ThreadPool.class));
-        injector = null;
-        index = null;
-        aggParsers = null;
-        currentTypes = null;
-        namedWriteableRegistry = null;
-    }
-
-    @Before
-    public void beforeTest() {
-        //set some random types to be queried as part the search request, before each test
-        String[] types = getRandomTypes();
-        TestSearchContext testSearchContext = new TestSearchContext();
-        testSearchContext.setTypes(types);
-        SearchContext.setCurrent(testSearchContext);
-    }
-
-    @After
-    public void afterTest() {
-        SearchContext.removeCurrent();
-    }
-
-    /**
-     * Generic test that creates new AggregatorFactory from the test
-     * AggregatorFactory and checks both for equality and asserts equality on
-     * the two queries.
-     */
-
-    public void testFromXContent() throws IOException {
-        AF testAgg = createTestAggregatorFactory();
-        AggregatorFactories.Builder factoriesBuilder = AggregatorFactories.builder().skipResolveOrder().addPipelineAggregator(testAgg);
-        String contentString = factoriesBuilder.toString();
-        System.out.println(contentString);
-        XContentParser parser = XContentFactory.xContent(contentString).createParser(contentString);
-        QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-        parseContext.reset(parser);
-        parseContext.parseFieldMatcher(parseFieldMatcher);
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.FIELD_NAME, parser.nextToken());
-        assertEquals(testAgg.name(), parser.currentName());
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.FIELD_NAME, parser.nextToken());
-        assertEquals(testAgg.type(), parser.currentName());
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        PipelineAggregatorFactory newAgg = aggParsers.pipelineAggregator(testAgg.getWriteableName()).parse(testAgg.name(), parser,
-                parseContext);
-        assertSame(XContentParser.Token.END_OBJECT, parser.currentToken());
-        assertSame(XContentParser.Token.END_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.END_OBJECT, parser.nextToken());
-        assertNull(parser.nextToken());
-        assertNotNull(newAgg);
-        assertNotSame(newAgg, testAgg);
-        assertEquals(testAgg, newAgg);
-        assertEquals(testAgg.hashCode(), newAgg.hashCode());
-    }
-
-    /**
-     * Test serialization and deserialization of the test AggregatorFactory.
-     */
-
-    public void testSerialization() throws IOException {
-        AF testAgg = createTestAggregatorFactory();
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            testAgg.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                PipelineAggregatorFactory prototype = aggParsers.pipelineAggregator(testAgg.getWriteableName()).getFactoryPrototype();
-                PipelineAggregatorFactory deserializedQuery = prototype.readFrom(in);
-                assertEquals(deserializedQuery, testAgg);
-                assertEquals(deserializedQuery.hashCode(), testAgg.hashCode());
-                assertNotSame(deserializedQuery, testAgg);
-            }
-        }
-    }
-
-
-    public void testEqualsAndHashcode() throws IOException {
-        AF firstAgg = createTestAggregatorFactory();
-        assertFalse("aggregation is equal to null", firstAgg.equals(null));
-        assertFalse("aggregation is equal to incompatible type", firstAgg.equals(""));
-        assertTrue("aggregation is not equal to self", firstAgg.equals(firstAgg));
-        assertThat("same aggregation's hashcode returns different values if called multiple times", firstAgg.hashCode(),
-                equalTo(firstAgg.hashCode()));
-
-        AF secondQuery = copyAggregation(firstAgg);
-        assertTrue("aggregation is not equal to self", secondQuery.equals(secondQuery));
-        assertTrue("aggregation is not equal to its copy", firstAgg.equals(secondQuery));
-        assertTrue("equals is not symmetric", secondQuery.equals(firstAgg));
-        assertThat("aggregation copy's hashcode is different from original hashcode", secondQuery.hashCode(), equalTo(firstAgg.hashCode()));
-
-        AF thirdQuery = copyAggregation(secondQuery);
-        assertTrue("aggregation is not equal to self", thirdQuery.equals(thirdQuery));
-        assertTrue("aggregation is not equal to its copy", secondQuery.equals(thirdQuery));
-        assertThat("aggregation copy's hashcode is different from original hashcode", secondQuery.hashCode(),
-                equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not transitive", firstAgg.equals(thirdQuery));
-        assertThat("aggregation copy's hashcode is different from original hashcode", firstAgg.hashCode(), equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not symmetric", thirdQuery.equals(secondQuery));
-        assertTrue("equals is not symmetric", thirdQuery.equals(firstAgg));
-    }
-
-    // we use the streaming infra to create a copy of the query provided as
-    // argument
-    private AF copyAggregation(AF agg) throws IOException {
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            agg.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                PipelineAggregatorFactory prototype = aggParsers.pipelineAggregator(agg.getWriteableName()).getFactoryPrototype();
-                @SuppressWarnings("unchecked")
-                AF secondAgg = (AF) prototype.readFrom(in);
-                return secondAgg;
-            }
-        }
-    }
-
-    protected String[] getRandomTypes() {
-        String[] types;
-        if (currentTypes.length > 0 && randomBoolean()) {
-            int numberOfQueryTypes = randomIntBetween(1, currentTypes.length);
-            types = new String[numberOfQueryTypes];
-            for (int i = 0; i < numberOfQueryTypes; i++) {
-                types[i] = randomFrom(currentTypes);
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[] { MetaData.ALL };
-            } else {
-                types = new String[0];
-            }
-        }
-        return types;
-    }
-
-    public String randomNumericField() {
-        int randomInt = randomInt(3);
-        switch (randomInt) {
-        case 0:
-            return DATE_FIELD_NAME;
-        case 1:
-            return DOUBLE_FIELD_NAME;
-        case 2:
-        default:
-            return INT_FIELD_NAME;
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/MissingValueIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/MissingValueIT.java
index 1821d09..63008bc 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/MissingValueIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/MissingValueIT.java
@@ -147,10 +147,7 @@ public class MissingValueIT extends ESIntegTestCase {
     }
 
     public void testDateHistogram() {
-        SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(
-                        dateHistogram("my_histogram").field("date").dateHistogramInterval(DateHistogramInterval.YEAR).missing("2014-05-07"))
-                .get();
+        SearchResponse response = client().prepareSearch("idx").addAggregation(dateHistogram("my_histogram").field("date").interval(DateHistogramInterval.YEAR).missing("2014-05-07")).get();
         assertSearchResponse(response);
         Histogram histogram = response.getAggregations().get("my_histogram");
         assertEquals(2, histogram.getBuckets().size());
@@ -159,10 +156,7 @@ public class MissingValueIT extends ESIntegTestCase {
         assertEquals("2015-01-01T00:00:00.000Z", histogram.getBuckets().get(1).getKeyAsString());
         assertEquals(1, histogram.getBuckets().get(1).getDocCount());
 
-        response = client().prepareSearch("idx")
-                .addAggregation(
-                        dateHistogram("my_histogram").field("date").dateHistogramInterval(DateHistogramInterval.YEAR).missing("2015-05-07"))
-                .get();
+        response = client().prepareSearch("idx").addAggregation(dateHistogram("my_histogram").field("date").interval(DateHistogramInterval.YEAR).missing("2015-05-07")).get();
         assertSearchResponse(response);
         histogram = response.getAggregations().get("my_histogram");
         assertEquals(1, histogram.getBuckets().size());
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/SubAggCollectionModeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/SubAggCollectionModeTests.java
deleted file mode 100644
index 131144d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/SubAggCollectionModeTests.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class SubAggCollectionModeTests extends ESTestCase {
-
-    public void testValidOrdinals() {
-        assertThat(SubAggCollectionMode.DEPTH_FIRST.ordinal(), equalTo(0));
-        assertThat(SubAggCollectionMode.BREADTH_FIRST.ordinal(), equalTo(1));
-    }
-
-    public void testwriteTo() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            SubAggCollectionMode.DEPTH_FIRST.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(0));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            SubAggCollectionMode.BREADTH_FIRST.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(1));
-            }
-        }
-    }
-
-    public void testReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(0);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(SubAggCollectionMode.BREADTH_FIRST.readFrom(in), equalTo(SubAggCollectionMode.DEPTH_FIRST));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(1);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(SubAggCollectionMode.BREADTH_FIRST.readFrom(in), equalTo(SubAggCollectionMode.BREADTH_FIRST));
-            }
-        }
-    }
-
-    public void testInvalidReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(randomIntBetween(2, Integer.MAX_VALUE));
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                SubAggCollectionMode.BREADTH_FIRST.readFrom(in);
-                fail("Expected IOException");
-            } catch(IOException e) {
-                assertThat(e.getMessage(), containsString("Unknown SubAggCollectionMode ordinal ["));
-            }
-
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java
index ea8fffd..2dcb79b 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java
@@ -134,8 +134,7 @@ public class ChildrenIT extends ESIntegTestCase {
                 .setQuery(matchQuery("randomized", true))
                 .addAggregation(
                         terms("category").field("category").size(0).subAggregation(
-children("to_comment", "comment")
-                                .subAggregation(
+                                children("to_comment").childType("comment").subAggregation(
                                         terms("commenters").field("commenter").size(0).subAggregation(
                                                 topHits("top_comments")
                                         ))
@@ -176,7 +175,7 @@ children("to_comment", "comment")
                 .setQuery(matchQuery("randomized", false))
                 .addAggregation(
                         terms("category").field("category").size(0).subAggregation(
-                        children("to_comment", "comment").subAggregation(topHits("top_comments").sort("_uid", SortOrder.ASC))
+                                children("to_comment").childType("comment").subAggregation(topHits("top_comments").addSort("_uid", SortOrder.ASC))
                         )
                 ).get();
         assertSearchResponse(searchResponse);
@@ -251,7 +250,7 @@ children("to_comment", "comment")
 
         for (int i = 0; i < 10; i++) {
             SearchResponse searchResponse = client().prepareSearch(indexName)
-                    .addAggregation(children("children", "child").subAggregation(sum("counts").field("count")))
+                    .addAggregation(children("children").childType("child").subAggregation(sum("counts").field("count")))
                     .get();
 
             assertNoFailures(searchResponse);
@@ -280,7 +279,7 @@ children("to_comment", "comment")
     public void testNonExistingChildType() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("test")
                 .addAggregation(
-children("non-existing", "xyz")
+                        children("non-existing").childType("xyz")
                 ).get();
         assertSearchResponse(searchResponse);
 
@@ -320,7 +319,8 @@ children("non-existing", "xyz")
 
         SearchResponse response = client().prepareSearch(indexName).setTypes(masterType)
                 .setQuery(hasChildQuery(childType, termQuery("color", "orange")))
-.addAggregation(children("my-refinements", childType)
+                .addAggregation(children("my-refinements")
+                                .childType(childType)
                                 .subAggregation(terms("my-colors").field("color"))
                                 .subAggregation(terms("my-sizes").field("size"))
                 ).get();
@@ -371,7 +371,8 @@ children("non-existing", "xyz")
         SearchResponse response = client().prepareSearch(indexName)
                 .setQuery(matchQuery("name", "europe"))
                 .addAggregation(
-                children(parentType, parentType).subAggregation(children(childType, childType).subAggregation(
+                        children(parentType).childType(parentType).subAggregation(
+                                children(childType).childType(childType).subAggregation(
                                         terms("name").field("name")
                                 )
                         )
@@ -419,7 +420,7 @@ children("non-existing", "xyz")
             .setSize(0)
             .addAggregation(AggregationBuilders.terms("towns").field("town")
                 .subAggregation(AggregationBuilders.terms("parent_names").field("name")
-.subAggregation(AggregationBuilders.children("child_docs", "childType"))
+                    .subAggregation(AggregationBuilders.children("child_docs").childType("childType"))
                 )
             )
             .get();
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenTests.java
deleted file mode 100644
index 3e0941e..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenTests.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.children.ParentToChildrenAggregator;
-import org.elasticsearch.search.aggregations.bucket.children.ParentToChildrenAggregator.ChildrenAggregatorBuilder;
-
-public class ChildrenTests extends BaseAggregationTestCase<ParentToChildrenAggregator.ChildrenAggregatorBuilder> {
-
-    @Override
-    protected ChildrenAggregatorBuilder createTestAggregatorBuilder() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String childType = randomAsciiOfLengthBetween(5, 40);
-        ChildrenAggregatorBuilder factory = new ChildrenAggregatorBuilder(name, childType);
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
index 4e1e84c..0d28bce 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
@@ -39,7 +39,6 @@ import org.elasticsearch.script.ScriptModule;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.SearchScript;
 import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
@@ -163,7 +162,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testSingleValuedField() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -198,7 +197,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testSingleValuedFieldWithTimeZone() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(1).timeZone(DateTimeZone.forID("+01:00"))).execute()
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(1).timeZone("+01:00")).execute()
                 .actionGet();
         DateTimeZone tz = DateTimeZone.forID("+01:00");
         assertSearchResponse(response);
@@ -256,7 +255,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.KEY_ASC))
                 .execute().actionGet();
 
@@ -279,7 +278,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.KEY_DESC))
                 .execute().actionGet();
 
@@ -301,7 +300,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.COUNT_ASC))
                 .execute().actionGet();
 
@@ -323,7 +322,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.COUNT_DESC))
                 .execute().actionGet();
 
@@ -343,7 +342,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testSingleValuedFieldWithSubAggregation() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH)
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH)
                         .subAggregation(sum("sum").field("value")))
                 .execute().actionGet();
 
@@ -398,11 +397,56 @@ public class DateHistogramIT extends ESIntegTestCase {
         assertThat((double) propertiesCounts[2], equalTo(15.0));
     }
 
+    public void testSingleValuedFieldWithSubAggregationInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH)
+                        .subAggregation(max("max")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        List<? extends Bucket> buckets = histo.getBuckets();
+        assertThat(buckets.size(), equalTo(3));
+
+        DateTime key = new DateTime(2012, 1, 1, 0, 0, DateTimeZone.UTC);
+        Histogram.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(1L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) new DateTime(2012, 1, 2, 0, 0, DateTimeZone.UTC).getMillis()));
+
+        key = new DateTime(2012, 2, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) new DateTime(2012, 2, 15, 0, 0, DateTimeZone.UTC).getMillis()));
+
+        key = new DateTime(2012, 3, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) new DateTime(2012, 3, 23, 0, 0, DateTimeZone.UTC).getMillis()));
+    }
+
     public void testSingleValuedFieldOrderedBySubAggregationAsc() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.aggregation("sum", true))
                         .subAggregation(max("sum").field("value")))
                 .execute().actionGet();
@@ -425,7 +469,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.aggregation("sum", false))
                         .subAggregation(max("sum").field("value")))
                 .execute().actionGet();
@@ -444,11 +488,34 @@ public class DateHistogramIT extends ESIntegTestCase {
         }
     }
 
+    public void testSingleValuedFieldOrderedByMultiValuedSubAggregationAscInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateHistogram("histo")
+                        .field("date")
+                        .interval(DateHistogramInterval.MONTH)
+                        .order(Histogram.Order.aggregation("stats", "sum", true))
+                        .subAggregation(stats("stats").field("value")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        assertThat(histo.getBuckets().size(), equalTo(3));
+
+        int i = 0;
+        for (Histogram.Bucket bucket : histo.getBuckets()) {
+            assertThat(((DateTime) bucket.getKey()), equalTo(new DateTime(2012, i + 1, 1, 0, 0, DateTimeZone.UTC)));
+            i++;
+        }
+    }
+
     public void testSingleValuedFieldOrderedByMultiValuedSubAggregationDesc() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.aggregation("stats", "sum", false))
                         .subAggregation(stats("stats").field("value")))
                 .execute().actionGet();
@@ -472,7 +539,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .addAggregation(dateHistogram("histo")
                         .field("date")
                         .script(new Script("", ScriptType.INLINE, FieldValueScriptEngine.NAME, null))
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)).execute().actionGet();
+                        .interval(DateHistogramInterval.MONTH)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -516,7 +583,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testMultiValuedField() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").field("dates").dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").field("dates").interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -560,7 +627,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("dates")
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)
+                        .interval(DateHistogramInterval.MONTH)
                         .order(Histogram.Order.COUNT_DESC))
                 .execute().actionGet();
 
@@ -606,7 +673,61 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .addAggregation(dateHistogram("histo")
                         .field("dates")
                         .script(new Script("", ScriptType.INLINE, FieldValueScriptEngine.NAME, null))
-                        .dateHistogramInterval(DateHistogramInterval.MONTH)).execute().actionGet();
+                        .interval(DateHistogramInterval.MONTH)).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        List<? extends Bucket> buckets = histo.getBuckets();
+        assertThat(buckets.size(), equalTo(4));
+
+        DateTime key = new DateTime(2012, 2, 1, 0, 0, DateTimeZone.UTC);
+        Histogram.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(1L));
+
+        key = new DateTime(2012, 3, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+
+        key = new DateTime(2012, 4, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(5L));
+
+        key = new DateTime(2012, 5, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(3);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+    }
+
+    /**
+     * The script will change to document date values to the following:
+     * <p>
+     * doc 1: [ Feb 2, Mar 3]
+     * doc 2: [ Mar 2, Apr 3]
+     * doc 3: [ Mar 15, Apr 16]
+     * doc 4: [ Apr 2, May 3]
+     * doc 5: [ Apr 15, May 16]
+     * doc 6: [ Apr 23, May 24]
+     */
+    public void testMultiValuedFieldWithValueScriptWithInheritedSubAggregator() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateHistogram("histo")
+                        .field("dates")
+                        .script(new Script("", ScriptType.INLINE, FieldValueScriptEngine.NAME, null))
+                        .interval(DateHistogramInterval.MONTH).subAggregation(max("max"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -622,6 +743,9 @@ public class DateHistogramIT extends ESIntegTestCase {
         assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
         assertThat(((DateTime) bucket.getKey()), equalTo(key));
         assertThat(bucket.getDocCount(), equalTo(1L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat((long) max.getValue(), equalTo(new DateTime(2012, 3, 3, 0, 0, DateTimeZone.UTC).getMillis()));
 
         key = new DateTime(2012, 3, 1, 0, 0, DateTimeZone.UTC);
         bucket = buckets.get(1);
@@ -629,6 +753,9 @@ public class DateHistogramIT extends ESIntegTestCase {
         assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
         assertThat(((DateTime) bucket.getKey()), equalTo(key));
         assertThat(bucket.getDocCount(), equalTo(3L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat((long) max.getValue(), equalTo(new DateTime(2012, 4, 16, 0, 0, DateTimeZone.UTC).getMillis()));
 
         key = new DateTime(2012, 4, 1, 0, 0, DateTimeZone.UTC);
         bucket = buckets.get(2);
@@ -636,6 +763,9 @@ public class DateHistogramIT extends ESIntegTestCase {
         assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
         assertThat(((DateTime) bucket.getKey()), equalTo(key));
         assertThat(bucket.getDocCount(), equalTo(5L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat((long) max.getValue(), equalTo(new DateTime(2012, 5, 24, 0, 0, DateTimeZone.UTC).getMillis()));
 
         key = new DateTime(2012, 5, 1, 0, 0, DateTimeZone.UTC);
         bucket = buckets.get(3);
@@ -643,6 +773,9 @@ public class DateHistogramIT extends ESIntegTestCase {
         assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
         assertThat(((DateTime) bucket.getKey()), equalTo(key));
         assertThat(bucket.getDocCount(), equalTo(3L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat((long) max.getValue(), equalTo(new DateTime(2012, 5, 24, 0, 0, DateTimeZone.UTC).getMillis()));
     }
 
     /**
@@ -655,7 +788,7 @@ public class DateHistogramIT extends ESIntegTestCase {
      */
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").script(new Script("date", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").script(new Script("date", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -688,9 +821,54 @@ public class DateHistogramIT extends ESIntegTestCase {
         assertThat(bucket.getDocCount(), equalTo(3L));
     }
 
+    public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateHistogram("histo")
+                        .script(new Script("date", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).interval(DateHistogramInterval.MONTH)
+                        .subAggregation(max("max"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        List<? extends Bucket> buckets = histo.getBuckets();
+        assertThat(buckets.size(), equalTo(3));
+
+        DateTime key = new DateTime(2012, 1, 1, 0, 0, DateTimeZone.UTC);
+        Histogram.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(1L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) new DateTime(2012, 1, 2, 0, 0, DateTimeZone.UTC).getMillis()));
+
+        key = new DateTime(2012, 2, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) new DateTime(2012, 2, 15, 0, 0, DateTimeZone.UTC).getMillis()));
+
+        key = new DateTime(2012, 3, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) new DateTime(2012, 3, 23, 0, 0, DateTimeZone.UTC).getMillis()));
+    }
+
     public void testScriptMultiValued() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").script(new Script("dates", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").script(new Script("dates", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -741,9 +919,64 @@ public class DateHistogramIT extends ESIntegTestCase {
     [ Mar 23, Apr 24]
      */
 
+    public void testScriptMultiValuedWithAggregatorInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateHistogram("histo")
+                        .script(new Script("dates", ScriptType.INLINE, ExtractFieldScriptEngine.NAME, null)).interval(DateHistogramInterval.MONTH)
+                        .subAggregation(max("max"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        List<? extends Bucket> buckets = histo.getBuckets();
+        assertThat(buckets.size(), equalTo(4));
+
+        DateTime key = new DateTime(2012, 1, 1, 0, 0, DateTimeZone.UTC);
+        Histogram.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(1L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat((long) max.getValue(), equalTo(new DateTime(2012, 2, 3, 0, 0, DateTimeZone.UTC).getMillis()));
+
+        key = new DateTime(2012, 2, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat((long) max.getValue(), equalTo(new DateTime(2012, 3, 16, 0, 0, DateTimeZone.UTC).getMillis()));
+
+        key = new DateTime(2012, 3, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(5L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat((long) max.getValue(), equalTo(new DateTime(2012, 4, 24, 0, 0, DateTimeZone.UTC).getMillis()));
+
+        key = new DateTime(2012, 4, 1, 0, 0, DateTimeZone.UTC);
+        bucket = buckets.get(3);
+        assertThat(bucket, notNullValue());
+        assertThat(bucket.getKeyAsString(), equalTo(getBucketKeyAsString(key)));
+        assertThat(((DateTime) bucket.getKey()), equalTo(key));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat((long) max.getValue(), equalTo(new DateTime(2012, 4, 24, 0, 0, DateTimeZone.UTC).getMillis()));
+    }
+
     public void testUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx_unmapped")
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -756,7 +989,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testPartiallyUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx", "idx_unmapped")
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH))
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -792,8 +1025,7 @@ public class DateHistogramIT extends ESIntegTestCase {
     public void testEmptyAggregation() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(dateHistogram("date_histo").field("value").interval(1)))
+                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(dateHistogram("date_histo").interval(1)))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -827,8 +1059,8 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(dateHistogram("date_histo")
                         .field("date")
-                        .timeZone(DateTimeZone.forID("-02:00"))
-                        .dateHistogramInterval(DateHistogramInterval.DAY)
+                        .timeZone("-02:00")
+                        .interval(DateHistogramInterval.DAY)
                         .format("yyyy-MM-dd:HH-mm-ssZZ"))
                 .execute().actionGet();
 
@@ -921,10 +1153,10 @@ public class DateHistogramIT extends ESIntegTestCase {
             response = client().prepareSearch("idx2")
                     .addAggregation(dateHistogram("histo")
                             .field("date")
-                            .dateHistogramInterval(DateHistogramInterval.days(interval))
+                            .interval(DateHistogramInterval.days(interval))
                             .minDocCount(0)
                                     // when explicitly specifying a format, the extended bounds should be defined by the same format
-                            .extendedBounds(new ExtendedBounds(format(boundsMin, pattern), format(boundsMax, pattern)))
+                            .extendedBounds(format(boundsMin, pattern), format(boundsMax, pattern))
                             .format(pattern))
                     .execute().actionGet();
 
@@ -997,8 +1229,8 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .prepareSearch(index)
                 .setQuery(QueryBuilders.rangeQuery("date").from("now/d").to("now/d").includeLower(true).includeUpper(true).timeZone(timezone.getID()))
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.hours(1)).timeZone(timezone).minDocCount(0)
-                                .extendedBounds(new ExtendedBounds("now/d", "now/d+23h"))
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.hours(1)).timeZone(timezone.getID()).minDocCount(0)
+                                .extendedBounds("now/d", "now/d+23h")
                 ).execute().actionGet();
         assertSearchResponse(response);
 
@@ -1036,7 +1268,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(dateHistogram("date_histo")
                         .field("date")
-                        .dateHistogramInterval(DateHistogramInterval.DAY))
+                        .interval(DateHistogramInterval.DAY))
                 .execute().actionGet();
 
         assertSearchHits(response, "0", "1", "2", "3", "4");
@@ -1055,7 +1287,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     public void testIssue6965() {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(dateHistogram("histo").field("date").timeZone(DateTimeZone.forID("+01:00")).dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0))
+                .addAggregation(dateHistogram("histo").field("date").timeZone("+01:00").interval(DateHistogramInterval.MONTH).minDocCount(0))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -1096,7 +1328,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 client().prepareIndex("test9491", "type").setSource("d", "2014-11-08T13:00:00Z"));
         ensureSearchable("test9491");
         SearchResponse response = client().prepareSearch("test9491")
-                .addAggregation(dateHistogram("histo").field("d").dateHistogramInterval(DateHistogramInterval.YEAR).timeZone(DateTimeZone.forID("Asia/Jerusalem")))
+                .addAggregation(dateHistogram("histo").field("d").interval(DateHistogramInterval.YEAR).timeZone("Asia/Jerusalem"))
                 .execute().actionGet();
         assertSearchResponse(response);
         Histogram histo = response.getAggregations().get("histo");
@@ -1113,7 +1345,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 client().prepareIndex("test8209", "type").setSource("d", "2014-04-30T00:00:00Z"));
         ensureSearchable("test8209");
         SearchResponse response = client().prepareSearch("test8209")
-                .addAggregation(dateHistogram("histo").field("d").dateHistogramInterval(DateHistogramInterval.MONTH).timeZone(DateTimeZone.forID("CET"))
+                .addAggregation(dateHistogram("histo").field("d").interval(DateHistogramInterval.MONTH).timeZone("CET")
                         .minDocCount(0))
                 .execute().actionGet();
         assertSearchResponse(response);
@@ -1131,7 +1363,7 @@ public class DateHistogramIT extends ESIntegTestCase {
     }
 
     /**
-     * see issue #9634, negative dateHistogramInterval in date_histogram should raise exception
+     * see issue #9634, negative interval in date_histogram should raise exception
      */
     public void testExceptionOnNegativeInterval() {
         try {
@@ -1140,12 +1372,12 @@ public class DateHistogramIT extends ESIntegTestCase {
                     .actionGet();
             fail();
         } catch (SearchPhaseExecutionException e) {
-            assertThat(e.toString(), containsString("IllegalArgumentException"));
+            assertThat(e.toString(), containsString("ElasticsearchParseException"));
         }
     }
 
     public void testTimestampField() { // see #11692
-        SearchResponse response = client().prepareSearch("idx").addAggregation(dateHistogram("histo").field("_timestamp").dateHistogramInterval(randomFrom(DateHistogramInterval.DAY, DateHistogramInterval.MONTH))).get();
+        SearchResponse response = client().prepareSearch("idx").addAggregation(dateHistogram("histo").field("_timestamp").interval(randomFrom(DateHistogramInterval.DAY, DateHistogramInterval.MONTH))).get();
         assertSearchResponse(response);
         Histogram histo = response.getAggregations().get("histo");
         assertThat(histo.getBuckets().size(), greaterThan(0));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
index b3b85e0..f240960 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
@@ -100,7 +100,7 @@ public class DateHistogramOffsetIT extends ESIntegTestCase {
                         .field("date")
                         .offset("2h")
                         .format(DATE_FORMAT)
-                        .dateHistogramInterval(DateHistogramInterval.DAY))
+                        .interval(DateHistogramInterval.DAY))
                 .execute().actionGet();
 
         assertThat(response.getHits().getTotalHits(), equalTo(5L));
@@ -122,7 +122,7 @@ public class DateHistogramOffsetIT extends ESIntegTestCase {
                         .field("date")
                         .offset("-2h")
                         .format(DATE_FORMAT)
-                        .dateHistogramInterval(DateHistogramInterval.DAY))
+                        .interval(DateHistogramInterval.DAY))
                 .execute().actionGet();
 
         assertThat(response.getHits().getTotalHits(), equalTo(5L));
@@ -149,7 +149,7 @@ public class DateHistogramOffsetIT extends ESIntegTestCase {
                         .offset("6h")
                         .minDocCount(0)
                         .format(DATE_FORMAT)
-                        .dateHistogramInterval(DateHistogramInterval.DAY))
+                        .interval(DateHistogramInterval.DAY))
                 .execute().actionGet();
 
         assertThat(response.getHits().getTotalHits(), equalTo(24L));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java
deleted file mode 100644
index 0f2411f..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java
+++ /dev/null
@@ -1,110 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
-import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Order;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.DateHistogramAggregatorBuilder;
-
-public class DateHistogramTests extends BaseAggregationTestCase<DateHistogramAggregatorBuilder> {
-
-    @Override
-    protected DateHistogramAggregatorBuilder createTestAggregatorBuilder() {
-        DateHistogramAggregatorBuilder factory = new DateHistogramAggregatorBuilder("foo");
-        factory.field(INT_FIELD_NAME);
-        if (randomBoolean()) {
-            factory.interval(randomIntBetween(1, 100000));
-        } else {
-            if (randomBoolean()) {
-                factory.dateHistogramInterval(randomFrom(DateHistogramInterval.YEAR, DateHistogramInterval.QUARTER,
-                        DateHistogramInterval.MONTH, DateHistogramInterval.WEEK, DateHistogramInterval.DAY, DateHistogramInterval.HOUR,
-                        DateHistogramInterval.MINUTE, DateHistogramInterval.SECOND));
-            } else {
-                int branch = randomInt(4);
-                switch (branch) {
-                case 0:
-                    factory.dateHistogramInterval(DateHistogramInterval.seconds(randomIntBetween(1, 1000)));
-                    break;
-                case 1:
-                    factory.dateHistogramInterval(DateHistogramInterval.minutes(randomIntBetween(1, 1000)));
-                    break;
-                case 2:
-                    factory.dateHistogramInterval(DateHistogramInterval.hours(randomIntBetween(1, 1000)));
-                    break;
-                case 3:
-                    factory.dateHistogramInterval(DateHistogramInterval.days(randomIntBetween(1, 1000)));
-                    break;
-                case 4:
-                    factory.dateHistogramInterval(DateHistogramInterval.weeks(randomIntBetween(1, 1000)));
-                    break;
-                default:
-                    throw new IllegalStateException("invalid branch: " + branch);
-                }
-            }
-        }
-        if (randomBoolean()) {
-            long extendedBoundsMin = randomIntBetween(-100000, 100000);
-            long extendedBoundsMax = randomIntBetween((int) extendedBoundsMin, 200000);
-            factory.extendedBounds(new ExtendedBounds(extendedBoundsMin, extendedBoundsMax));
-        }
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.minDocCount(randomIntBetween(0, 100));
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        if (randomBoolean()) {
-            factory.offset(randomIntBetween(0, 100000));
-        }
-        if (randomBoolean()) {
-            int branch = randomInt(5);
-            switch (branch) {
-            case 0:
-                factory.order(Order.COUNT_ASC);
-                break;
-            case 1:
-                factory.order(Order.COUNT_DESC);
-                break;
-            case 2:
-                factory.order(Order.KEY_ASC);
-                break;
-            case 3:
-                factory.order(Order.KEY_DESC);
-                break;
-            case 4:
-                factory.order(Order.aggregation("foo", true));
-                break;
-            case 5:
-                factory.order(Order.aggregation("foo", false));
-                break;
-            }
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeTests.java
deleted file mode 100644
index 94156fc..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeTests.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.bucket.range.date.DateRangeAggregatorBuilder;
-
-public class DateRangeTests extends BaseAggregationTestCase<DateRangeAggregatorBuilder> {
-
-    @Override
-    protected DateRangeAggregatorBuilder createTestAggregatorBuilder() {
-        int numRanges = randomIntBetween(1, 10);
-        DateRangeAggregatorBuilder factory = new DateRangeAggregatorBuilder("foo");
-        for (int i = 0; i < numRanges; i++) {
-            String key = null;
-            if (randomBoolean()) {
-                key = randomAsciiOfLengthBetween(1, 20);
-            }
-            double from = randomBoolean() ? Double.NEGATIVE_INFINITY : randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE - 1000);
-            double to = randomBoolean() ? Double.POSITIVE_INFINITY
-                    : (Double.isInfinite(from) ? randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE)
-                            : randomIntBetween((int) from, Integer.MAX_VALUE));
-            if (randomBoolean()) {
-                factory.addRange(new Range(key, from, to));
-            } else {
-                String fromAsStr = Double.isInfinite(from) ? null : String.valueOf(from);
-                String toAsStr = Double.isInfinite(to) ? null : String.valueOf(to);
-                factory.addRange(new Range(key, fromAsStr, toAsStr));
-            }
-        }
-        factory.field(INT_FIELD_NAME);
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerIT.java
deleted file mode 100644
index 1b72944..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerIT.java
+++ /dev/null
@@ -1,241 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.action.admin.indices.refresh.RefreshRequest;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchType;
-import org.elasticsearch.index.query.TermQueryBuilder;
-import org.elasticsearch.search.aggregations.bucket.sampler.Sampler;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator.DiversifiedAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorBuilder;
-import org.elasticsearch.search.aggregations.metrics.max.Max;
-import org.elasticsearch.test.ESIntegTestCase;
-
-import java.util.Collection;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.max;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.sampler;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.greaterThanOrEqualTo;
-import static org.hamcrest.Matchers.lessThanOrEqualTo;
-
-/**
- * Tests the Sampler aggregation
- */
-@ESIntegTestCase.SuiteScopeTestCase
-public class DiversifiedSamplerIT extends ESIntegTestCase {
-
-    public static final int NUM_SHARDS = 2;
-
-    public String randomExecutionHint() {
-        return randomBoolean() ? null : randomFrom(SamplerAggregator.ExecutionMode.values()).toString();
-    }
-
-
-    @Override
-    public void setupSuiteScopeCluster() throws Exception {
-        assertAcked(prepareCreate("test").setSettings(SETTING_NUMBER_OF_SHARDS, NUM_SHARDS, SETTING_NUMBER_OF_REPLICAS, 0).addMapping(
-                "book", "author", "type=string,index=not_analyzed", "name", "type=string,index=analyzed", "genre",
-                "type=string,index=not_analyzed", "price", "type=float"));
-        createIndex("idx_unmapped");
-        // idx_unmapped_author is same as main index but missing author field
-        assertAcked(prepareCreate("idx_unmapped_author").setSettings(SETTING_NUMBER_OF_SHARDS, NUM_SHARDS, SETTING_NUMBER_OF_REPLICAS, 0)
-                .addMapping("book", "name", "type=string,index=analyzed", "genre", "type=string,index=not_analyzed", "price",
-                        "type=float"));
-
-        ensureGreen();
-        String data[] = {
-                // "id,cat,name,price,inStock,author_t,series_t,sequence_i,genre_s",
-                "0553573403,book,A Game of Thrones,7.99,true,George R.R. Martin,A Song of Ice and Fire,1,fantasy",
-                "0553579908,book,A Clash of Kings,7.99,true,George R.R. Martin,A Song of Ice and Fire,2,fantasy",
-                "055357342X,book,A Storm of Swords,7.99,true,George R.R. Martin,A Song of Ice and Fire,3,fantasy",
-                "0553293354,book,Foundation,17.99,true,Isaac Asimov,Foundation Novels,1,scifi",
-                "0812521390,book,The Black Company,6.99,false,Glen Cook,The Chronicles of The Black Company,1,fantasy",
-                "0812550706,book,Ender's Game,6.99,true,Orson Scott Card,Ender,1,scifi",
-                "0441385532,book,Jhereg,7.95,false,Steven Brust,Vlad Taltos,1,fantasy",
-                "0380014300,book,Nine Princes In Amber,6.99,true,Roger Zelazny,the Chronicles of Amber,1,fantasy",
-                "0805080481,book,The Book of Three,5.99,true,Lloyd Alexander,The Chronicles of Prydain,1,fantasy",
-                "080508049X,book,The Black Cauldron,5.99,true,Lloyd Alexander,The Chronicles of Prydain,2,fantasy"
-
-            };
-
-        for (int i = 0; i < data.length; i++) {
-            String[] parts = data[i].split(",");
-            client().prepareIndex("test", "book", "" + i)
-                    .setSource("author", parts[5], "name", parts[2], "genre", parts[8], "price", Float.parseFloat(parts[3])).get();
-            client().prepareIndex("idx_unmapped_author", "book", "" + i)
-                    .setSource("name", parts[2], "genre", parts[8], "price", Float.parseFloat(parts[3])).get();
-        }
-        client().admin().indices().refresh(new RefreshRequest("test")).get();
-    }
-
-    public void testIssue10719() throws Exception {
-        // Tests that we can refer to nested elements under a sample in a path
-        // statement
-        boolean asc = randomBoolean();
-        SearchResponse response = client().prepareSearch("test").setTypes("book").setSearchType(SearchType.QUERY_AND_FETCH)
-                .addAggregation(terms("genres")
-                        .field("genre")
-                        .order(Terms.Order.aggregation("sample>max_price.value", asc))
-                        .subAggregation(sampler("sample").shardSize(100)
-                                .subAggregation(max("max_price").field("price")))
-                ).execute().actionGet();
-        assertSearchResponse(response);
-        Terms genres = response.getAggregations().get("genres");
-        Collection<Bucket> genreBuckets = genres.getBuckets();
-        // For this test to be useful we need >1 genre bucket to compare
-        assertThat(genreBuckets.size(), greaterThan(1));
-        double lastMaxPrice = asc ? Double.MIN_VALUE : Double.MAX_VALUE;
-        for (Terms.Bucket genreBucket : genres.getBuckets()) {
-            Sampler sample = genreBucket.getAggregations().get("sample");
-            Max maxPriceInGenre = sample.getAggregations().get("max_price");
-            double price = maxPriceInGenre.getValue();
-            if (asc) {
-                assertThat(price, greaterThanOrEqualTo(lastMaxPrice));
-            } else {
-                assertThat(price, lessThanOrEqualTo(lastMaxPrice));
-            }
-            lastMaxPrice = price;
-        }
-
-    }
-
-    public void testSimpleDiversity() throws Exception {
-        int MAX_DOCS_PER_AUTHOR = 1;
-        DiversifiedAggregatorBuilder sampleAgg = new DiversifiedAggregatorBuilder("sample").shardSize(100);
-        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
-        sampleAgg.subAggregation(terms("authors").field("author"));
-        SearchResponse response = client().prepareSearch("test")
-                .setSearchType(SearchType.QUERY_AND_FETCH)
-                .setQuery(new TermQueryBuilder("genre", "fantasy"))
-                .setFrom(0).setSize(60)
-                .addAggregation(sampleAgg)
-                .execute()
-                .actionGet();
-        assertSearchResponse(response);
-        Sampler sample = response.getAggregations().get("sample");
-        Terms authors = sample.getAggregations().get("authors");
-        Collection<Bucket> testBuckets = authors.getBuckets();
-
-        for (Terms.Bucket testBucket : testBuckets) {
-            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
-        }
-    }
-
-    public void testNestedDiversity() throws Exception {
-        // Test multiple samples gathered under buckets made by a parent agg
-        int MAX_DOCS_PER_AUTHOR = 1;
-        TermsAggregatorBuilder rootTerms = terms("genres").field("genre");
-
-        DiversifiedAggregatorBuilder sampleAgg = new DiversifiedAggregatorBuilder("sample").shardSize(100);
-        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
-        sampleAgg.subAggregation(terms("authors").field("author"));
-
-        rootTerms.subAggregation(sampleAgg);
-        SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH)
-                .addAggregation(rootTerms).execute().actionGet();
-        assertSearchResponse(response);
-        Terms genres = response.getAggregations().get("genres");
-        Collection<Bucket> genreBuckets = genres.getBuckets();
-        for (Terms.Bucket genreBucket : genreBuckets) {
-            Sampler sample = genreBucket.getAggregations().get("sample");
-            Terms authors = sample.getAggregations().get("authors");
-            Collection<Bucket> testBuckets = authors.getBuckets();
-
-            for (Terms.Bucket testBucket : testBuckets) {
-                assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
-            }
-        }
-    }
-
-    public void testNestedSamples() throws Exception {
-        // Test samples nested under samples
-        int MAX_DOCS_PER_AUTHOR = 1;
-        int MAX_DOCS_PER_GENRE = 2;
-        DiversifiedAggregatorBuilder rootSample = new DiversifiedAggregatorBuilder("genreSample").shardSize(100)
-                .field("genre")
-                .maxDocsPerValue(MAX_DOCS_PER_GENRE);
-
-        DiversifiedAggregatorBuilder sampleAgg = new DiversifiedAggregatorBuilder("sample").shardSize(100);
-        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
-        sampleAgg.subAggregation(terms("authors").field("author"));
-        sampleAgg.subAggregation(terms("genres").field("genre"));
-
-        rootSample.subAggregation(sampleAgg);
-        SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH).addAggregation(rootSample)
-                .execute().actionGet();
-        assertSearchResponse(response);
-        Sampler genreSample = response.getAggregations().get("genreSample");
-        Sampler sample = genreSample.getAggregations().get("sample");
-
-        Terms genres = sample.getAggregations().get("genres");
-        Collection<Bucket> testBuckets = genres.getBuckets();
-        for (Terms.Bucket testBucket : testBuckets) {
-            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_GENRE));
-        }
-
-        Terms authors = sample.getAggregations().get("authors");
-        testBuckets = authors.getBuckets();
-        for (Terms.Bucket testBucket : testBuckets) {
-            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
-        }
-    }
-
-    public void testPartiallyUnmappedDiversifyField() throws Exception {
-        // One of the indexes is missing the "author" field used for
-        // diversifying results
-        DiversifiedAggregatorBuilder sampleAgg = new DiversifiedAggregatorBuilder("sample").shardSize(100).field("author")
-                .maxDocsPerValue(1);
-        sampleAgg.subAggregation(terms("authors").field("author"));
-        SearchResponse response = client().prepareSearch("idx_unmapped_author", "test").setSearchType(SearchType.QUERY_AND_FETCH)
-                .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg)
-                .execute().actionGet();
-        assertSearchResponse(response);
-        Sampler sample = response.getAggregations().get("sample");
-        assertThat(sample.getDocCount(), greaterThan(0L));
-        Terms authors = sample.getAggregations().get("authors");
-        assertThat(authors.getBuckets().size(), greaterThan(0));
-    }
-
-    public void testWhollyUnmappedDiversifyField() throws Exception {
-        //All of the indices are missing the "author" field used for diversifying results
-        int MAX_DOCS_PER_AUTHOR = 1;
-        DiversifiedAggregatorBuilder sampleAgg = new DiversifiedAggregatorBuilder("sample").shardSize(100);
-        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
-        sampleAgg.subAggregation(terms("authors").field("author"));
-        SearchResponse response = client().prepareSearch("idx_unmapped", "idx_unmapped_author").setSearchType(SearchType.QUERY_AND_FETCH)
-                .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg).execute().actionGet();
-        assertSearchResponse(response);
-        Sampler sample = response.getAggregations().get("sample");
-        assertThat(sample.getDocCount(), equalTo(0L));
-        Terms authors = sample.getAggregations().get("authors");
-        assertNull(authors);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerTests.java
deleted file mode 100644
index a900496..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerTests.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator.ExecutionMode;
-
-public class DiversifiedSamplerTests extends BaseAggregationTestCase<SamplerAggregator.DiversifiedAggregatorBuilder> {
-
-    @Override
-    protected final SamplerAggregator.DiversifiedAggregatorBuilder createTestAggregatorBuilder() {
-        SamplerAggregator.DiversifiedAggregatorBuilder factory = new SamplerAggregator.DiversifiedAggregatorBuilder("foo");
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            factory.maxDocsPerValue(randomIntBetween(1, 1000));
-        }
-        if (randomBoolean()) {
-            factory.shardSize(randomIntBetween(1, 1000));
-        }
-        if (randomBoolean()) {
-            factory.executionHint(randomFrom(ExecutionMode.values()).toString());
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FilterIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FilterIT.java
index 9a4b5ec..38cc24a 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FilterIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FilterIT.java
@@ -93,7 +93,7 @@ public class FilterIT extends ESIntegTestCase {
 
     public void testSimple() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filter("tag1", termQuery("tag", "tag1")))
+                .addAggregation(filter("tag1").filter(termQuery("tag", "tag1")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -109,7 +109,7 @@ public class FilterIT extends ESIntegTestCase {
     // https://github.com/elasticsearch/elasticsearch/issues/8438
     public void testEmptyFilterDeclarations() throws Exception {
         QueryBuilder emptyFilter = new BoolQueryBuilder();
-        SearchResponse response = client().prepareSearch("idx").addAggregation(filter("tag1", emptyFilter)).execute().actionGet();
+        SearchResponse response = client().prepareSearch("idx").addAggregation(filter("tag1").filter(emptyFilter)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -120,7 +120,8 @@ public class FilterIT extends ESIntegTestCase {
 
     public void testWithSubAggregation() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filter("tag1", termQuery("tag", "tag1"))
+                .addAggregation(filter("tag1")
+                        .filter(termQuery("tag", "tag1"))
                         .subAggregation(avg("avg_value").field("value")))
                 .execute().actionGet();
 
@@ -149,7 +150,7 @@ public class FilterIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(
                         histogram("histo").field("value").interval(2L).subAggregation(
-                                filter("filter", matchAllQuery()))).get();
+                                filter("filter").filter(matchAllQuery()))).get();
 
         assertSearchResponse(response);
 
@@ -167,7 +168,8 @@ public class FilterIT extends ESIntegTestCase {
     public void testWithContextBasedSubAggregation() throws Exception {
         try {
             client().prepareSearch("idx")
-                    .addAggregation(filter("tag1", termQuery("tag", "tag1"))
+                    .addAggregation(filter("tag1")
+                            .filter(termQuery("tag", "tag1"))
                             .subAggregation(avg("avg_value")))
                     .execute().actionGet();
 
@@ -183,7 +185,7 @@ public class FilterIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(filter("filter", matchAllQuery())))
+                        .subAggregation(filter("filter").filter(matchAllQuery())))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java
index bf25201..ab44d6c 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java
@@ -26,7 +26,6 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.BoolQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.bucket.filters.Filters;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.KeyedFilter;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -110,8 +109,11 @@ public class FiltersIT extends ESIntegTestCase {
     }
 
     public void testSimple() throws Exception {
-        SearchResponse response = client().prepareSearch("idx").addAggregation(
-                filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")), new KeyedFilter("tag2", termQuery("tag", "tag2"))))
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(
+                        filters("tags")
+                                .filter("tag1", termQuery("tag", "tag1"))
+                                .filter("tag2", termQuery("tag", "tag2")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -134,10 +136,10 @@ public class FiltersIT extends ESIntegTestCase {
     // See NullPointer issue when filters are empty:
     // https://github.com/elasticsearch/elasticsearch/issues/8438
     public void testEmptyFilterDeclarations() throws Exception {
-        QueryBuilder<?> emptyFilter = new BoolQueryBuilder();
+        QueryBuilder emptyFilter = new BoolQueryBuilder();
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", new KeyedFilter("all", emptyFilter), new KeyedFilter("tag1", termQuery("tag", "tag1"))))
-                .execute().actionGet();
+                .addAggregation(filters("tags").filter("all", emptyFilter).filter("tag1", termQuery("tag", "tag1"))).execute()
+                .actionGet();
 
         assertSearchResponse(response);
 
@@ -153,8 +155,11 @@ public class FiltersIT extends ESIntegTestCase {
 
     public void testWithSubAggregation() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")),
-                        new KeyedFilter("tag2", termQuery("tag", "tag2"))).subAggregation(avg("avg_value").field("value")))
+                .addAggregation(
+                        filters("tags")
+                                .filter("tag1", termQuery("tag", "tag1"))
+                                .filter("tag2", termQuery("tag", "tag2"))
+                                .subAggregation(avg("avg_value").field("value")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -205,7 +210,7 @@ public class FiltersIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(
                         histogram("histo").field("value").interval(2L).subAggregation(
-                                filters("filters", matchAllQuery()))).get();
+                                filters("filters").filter(matchAllQuery()))).get();
 
         assertSearchResponse(response);
 
@@ -227,7 +232,9 @@ public class FiltersIT extends ESIntegTestCase {
         try {
             client().prepareSearch("idx")
                     .addAggregation(
-                    filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")), new KeyedFilter("tag2", termQuery("tag", "tag2")))
+                            filters("tags")
+                                    .filter("tag1", termQuery("tag", "tag1"))
+                                    .filter("tag2", termQuery("tag", "tag2"))
                                     .subAggregation(avg("avg_value"))
                     )
                     .execute().actionGet();
@@ -244,7 +251,7 @@ public class FiltersIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(filters("filters", new KeyedFilter("all", matchAllQuery()))))
+                        .subAggregation(filters("filters").filter("all", matchAllQuery())))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -263,7 +270,11 @@ public class FiltersIT extends ESIntegTestCase {
 
     public void testSimpleNonKeyed() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", termQuery("tag", "tag1"), termQuery("tag", "tag2"))).execute().actionGet();
+                .addAggregation(
+                        filters("tags")
+                                .filter(termQuery("tag", "tag1"))
+                                .filter(termQuery("tag", "tag2")))
+                .execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -286,9 +297,12 @@ public class FiltersIT extends ESIntegTestCase {
     }
 
     public void testOtherBucket() throws Exception {
-        SearchResponse response = client().prepareSearch("idx").addAggregation(
-                filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")), new KeyedFilter("tag2", termQuery("tag", "tag2")))
-                        .otherBucket(true))
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        filters("tags").otherBucket(true)
+                        .filter("tag1", termQuery("tag", "tag1"))
+                        .filter("tag2", termQuery("tag", "tag2")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -313,9 +327,12 @@ public class FiltersIT extends ESIntegTestCase {
     }
 
     public void testOtherNamedBucket() throws Exception {
-        SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")),
-                        new KeyedFilter("tag2", termQuery("tag", "tag2"))).otherBucket(true).otherBucketKey("foobar"))
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        filters("tags").otherBucketKey("foobar")
+                        .filter("tag1", termQuery("tag", "tag1"))
+                        .filter("tag2", termQuery("tag", "tag2")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -341,8 +358,11 @@ public class FiltersIT extends ESIntegTestCase {
 
     public void testOtherNonKeyed() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", termQuery("tag", "tag1"), termQuery("tag", "tag2")).otherBucket(true)).execute()
-                .actionGet();
+                .addAggregation(
+                        filters("tags").otherBucket(true)
+                                .filter(termQuery("tag", "tag1"))
+                                .filter(termQuery("tag", "tag2")))
+                .execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -370,8 +390,10 @@ public class FiltersIT extends ESIntegTestCase {
 
     public void testOtherWithSubAggregation() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filters("tags", new KeyedFilter("tag1", termQuery("tag", "tag1")),
-                        new KeyedFilter("tag2", termQuery("tag", "tag2"))).otherBucket(true)
+                .addAggregation(
+                        filters("tags").otherBucket(true)
+                                .filter("tag1", termQuery("tag", "tag1"))
+                                .filter("tag2", termQuery("tag", "tag2"))
                                 .subAggregation(avg("avg_value").field("value")))
                 .execute().actionGet();
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceIT.java
index 80211eb..5d52d14 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceIT.java
@@ -22,7 +22,6 @@ import org.elasticsearch.Version;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -141,9 +140,10 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
     public void testSimple() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo(500)
                         .addRange(500, 1000)
                         .addUnboundedFrom(1000))
@@ -188,9 +188,10 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
     public void testSimpleWithCustomKeys() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo("ring1", 500)
                         .addRange("ring2", 500, 1000)
                         .addUnboundedFrom("ring3", 1000))
@@ -237,9 +238,10 @@ public class GeoDistanceIT extends ESIntegTestCase {
         client().admin().cluster().prepareHealth("idx_unmapped").setWaitForYellowStatus().execute().actionGet();
 
         SearchResponse response = client().prepareSearch("idx_unmapped")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo(500)
                         .addRange(500, 1000)
                         .addUnboundedFrom(1000))
@@ -284,9 +286,10 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
     public void testPartiallyUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx", "idx_unmapped")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo(500)
                         .addRange(500, 1000)
                         .addUnboundedFrom(1000))
@@ -331,9 +334,10 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
     public void testWithSubAggregation() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo(500)
                         .addRange(500, 1000)
                         .addUnboundedFrom(1000)
@@ -418,7 +422,7 @@ public class GeoDistanceIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(geoDistance("geo_dist", new GeoPoint(52.3760, 4.894)).field("location").addRange("0-100", 0.0, 100.0)))
+                        .subAggregation(geoDistance("geo_dist").field("location").point("52.3760, 4.894").addRange("0-100", 0.0, 100.0)))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -443,10 +447,11 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
     public void testMultiValues() throws Exception {
         SearchResponse response = client().prepareSearch("idx-multi")
-                .addAggregation(geoDistance("amsterdam_rings", new GeoPoint(52.3760, 4.894))
+                .addAggregation(geoDistance("amsterdam_rings")
                         .field("location")
                         .unit(DistanceUnit.KILOMETERS)
                         .distanceType(org.elasticsearch.common.geo.GeoDistance.ARC)
+                        .point("52.3760, 4.894") // coords of amsterdam
                         .addUnboundedTo(500)
                         .addRange(500, 1000)
                         .addUnboundedFrom(1000))
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceRangeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceRangeTests.java
deleted file mode 100644
index 39eccf4..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceRangeTests.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.unit.DistanceUnit;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanceParser.GeoDistanceAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanceParser.Range;
-import org.elasticsearch.test.geo.RandomShapeGenerator;
-
-public class GeoDistanceRangeTests extends BaseAggregationTestCase<GeoDistanceAggregatorBuilder> {
-
-    @Override
-    protected GeoDistanceAggregatorBuilder createTestAggregatorBuilder() {
-        int numRanges = randomIntBetween(1, 10);
-        GeoPoint origin = RandomShapeGenerator.randomPoint(getRandom());
-        GeoDistanceAggregatorBuilder factory = new GeoDistanceAggregatorBuilder("foo", origin);
-        for (int i = 0; i < numRanges; i++) {
-            String key = null;
-            if (randomBoolean()) {
-                key = randomAsciiOfLengthBetween(1, 20);
-            }
-            double from = randomBoolean() ? 0 : randomIntBetween(0, Integer.MAX_VALUE - 1000);
-            double to = randomBoolean() ? Double.POSITIVE_INFINITY
-                    : (Double.compare(from, 0) == 0 ? randomIntBetween(0, Integer.MAX_VALUE)
-                            : randomIntBetween((int) from, Integer.MAX_VALUE));
-            factory.addRange(new Range(key, from, to));
-        }
-        factory.field(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing("0, 0");
-        }
-        if (randomBoolean()) {
-            factory.unit(randomFrom(DistanceUnit.values()));
-        }
-        if (randomBoolean()) {
-            factory.distanceType(randomFrom(GeoDistance.values()));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java
index c8658cc..672447a 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java
@@ -204,7 +204,7 @@ public class GeoHashGridIT extends ESIntegTestCase {
         for (int precision = 1; precision <= GeoHashUtils.PRECISION; precision++) {
             SearchResponse response = client().prepareSearch("idx")
                     .addAggregation(
-                            AggregationBuilders.filter("filtered", bbox)
+                            AggregationBuilders.filter("filtered").filter(bbox)
                                     .subAggregation(
                                             geohashGrid("geohashgrid")
                                                     .field("location")
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java
deleted file mode 100644
index f8f74ed..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridParser.GeoGridAggregatorBuilder;
-
-public class GeoHashGridTests extends BaseAggregationTestCase<GeoGridAggregatorBuilder> {
-
-    @Override
-    protected GeoGridAggregatorBuilder createTestAggregatorBuilder() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        GeoGridAggregatorBuilder factory = new GeoGridAggregatorBuilder(name);
-        if (randomBoolean()) {
-            int precision = randomIntBetween(1, 12);
-            factory.precision(precision);
-        }
-        if (randomBoolean()) {
-            int size = randomInt(5);
-            switch (size) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                size = randomInt();
-                break;
-            }
-            factory.size(size);
-
-        }
-        if (randomBoolean()) {
-            int shardSize = randomInt(5);
-            switch (shardSize) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardSize = randomInt();
-                break;
-            }
-            factory.shardSize(shardSize);
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GlobalTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GlobalTests.java
deleted file mode 100644
index c5dd307..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GlobalTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.global.GlobalAggregator;
-import org.elasticsearch.search.aggregations.bucket.global.GlobalAggregator.GlobalAggregatorBuilder;
-
-public class GlobalTests extends BaseAggregationTestCase<GlobalAggregator.GlobalAggregatorBuilder> {
-
-    @Override
-    protected GlobalAggregatorBuilder createTestAggregatorBuilder() {
-        return new GlobalAggregator.GlobalAggregatorBuilder(randomAsciiOfLengthBetween(3, 20));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java
deleted file mode 100644
index dbb54ce..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
-import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Order;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.HistogramAggregatorBuilder;
-
-public class HistogramTests extends BaseAggregationTestCase<HistogramAggregator.HistogramAggregatorBuilder> {
-
-    @Override
-    protected HistogramAggregatorBuilder createTestAggregatorBuilder() {
-        HistogramAggregatorBuilder factory = new HistogramAggregatorBuilder("foo");
-        factory.field(INT_FIELD_NAME);
-        factory.interval(randomIntBetween(1, 100000));
-        if (randomBoolean()) {
-            long extendedBoundsMin = randomIntBetween(-100000, 100000);
-            long extendedBoundsMax = randomIntBetween((int) extendedBoundsMin, 200000);
-            factory.extendedBounds(new ExtendedBounds(extendedBoundsMin, extendedBoundsMax));
-        }
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.minDocCount(randomIntBetween(0, 100));
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        if (randomBoolean()) {
-            factory.offset(randomIntBetween(0, 100000));
-        }
-        if (randomBoolean()) {
-            int branch = randomInt(5);
-            switch (branch) {
-            case 0:
-                factory.order(Order.COUNT_ASC);
-                break;
-            case 1:
-                factory.order(Order.COUNT_DESC);
-                break;
-            case 2:
-                factory.order(Order.KEY_ASC);
-                break;
-            case 3:
-                factory.order(Order.KEY_DESC);
-                break;
-            case 4:
-                factory.order(Order.aggregation("foo", true));
-                break;
-            case 5:
-                factory.order(Order.aggregation("foo", false));
-                break;
-            }
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java
deleted file mode 100644
index 507e877..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.common.network.Cidrs;
-import org.elasticsearch.index.mapper.ip.IpFieldMapper;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.range.ipv4.IPv4RangeAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.range.ipv4.IPv4RangeAggregatorBuilder.Range;
-
-public class IPv4RangeTests extends BaseAggregationTestCase<IPv4RangeAggregatorBuilder> {
-
-    @Override
-    protected IPv4RangeAggregatorBuilder createTestAggregatorBuilder() {
-        int numRanges = randomIntBetween(1, 10);
-        IPv4RangeAggregatorBuilder factory = new IPv4RangeAggregatorBuilder("foo");
-        for (int i = 0; i < numRanges; i++) {
-            String key = null;
-            if (randomBoolean()) {
-                key = randomAsciiOfLengthBetween(1, 20);
-            }
-            if (randomBoolean()) {
-                double from = randomBoolean() ? Double.NEGATIVE_INFINITY : randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE - 1000);
-                double to = randomBoolean() ? Double.POSITIVE_INFINITY
-                        : (Double.isInfinite(from) ? randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE)
-                                : randomIntBetween((int) from, Integer.MAX_VALUE));
-                if (randomBoolean()) {
-                    factory.addRange(new Range(key, from, to));
-                } else {
-                    String fromAsStr = Double.isInfinite(from) ? null : IpFieldMapper.longToIp((long) from);
-                    String toAsStr = Double.isInfinite(to) ? null : IpFieldMapper.longToIp((long) to);
-                    factory.addRange(new Range(key, fromAsStr, toAsStr));
-                }
-            } else {
-                int mask = randomInt(32);
-                long ipAsLong = randomIntBetween(0, Integer.MAX_VALUE);
-
-                long blockSize = 1L << (32 - mask);
-                ipAsLong = ipAsLong - (ipAsLong & (blockSize - 1));
-                String cidr = Cidrs.createCIDR(ipAsLong, mask);
-                factory.addRange(new Range(key, cidr));
-            }
-        }
-        factory.field(INT_FIELD_NAME);
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/MissingIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/MissingIT.java
index 97eaf2f..85e0c58 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/MissingIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/MissingIT.java
@@ -160,11 +160,32 @@ public class MissingIT extends ESIntegTestCase {
         assertThat((double) missing.getProperty("avg_value.value"), equalTo((double) sum / (numDocsMissing + numDocsUnmapped)));
     }
 
+    public void testWithInheritedSubMissing() throws Exception {
+        SearchResponse response = client().prepareSearch("idx", "unmapped_idx")
+                .addAggregation(missing("top_missing").field("tag")
+                        .subAggregation(missing("sub_missing")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Missing topMissing = response.getAggregations().get("top_missing");
+        assertThat(topMissing, notNullValue());
+        assertThat(topMissing.getName(), equalTo("top_missing"));
+        assertThat(topMissing.getDocCount(), equalTo((long) numDocsMissing + numDocsUnmapped));
+        assertThat(topMissing.getAggregations().asList().isEmpty(), is(false));
+
+        Missing subMissing = topMissing.getAggregations().get("sub_missing");
+        assertThat(subMissing, notNullValue());
+        assertThat(subMissing.getName(), equalTo("sub_missing"));
+        assertThat(subMissing.getDocCount(), equalTo((long) numDocsMissing + numDocsUnmapped));
+    }
+
     public void testEmptyAggregation() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(missing("missing").field("value")))
+                        .subAggregation(missing("missing")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NaNSortingIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NaNSortingIT.java
index d303a2e..bfd6837 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NaNSortingIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NaNSortingIT.java
@@ -26,12 +26,9 @@ import org.elasticsearch.search.aggregations.Aggregation;
 import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.metrics.MetricsAggregationBuilder;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
-import org.elasticsearch.search.aggregations.metrics.avg.AvgAggregator;
 import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStats;
-import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStatsAggregator;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
@@ -48,10 +45,8 @@ public class NaNSortingIT extends ESIntegTestCase {
     private enum SubAggregation {
         AVG("avg") {
             @Override
-            public AvgAggregator.AvgAggregatorBuilder builder() {
-                AvgAggregator.AvgAggregatorBuilder factory = avg(name);
-                factory.field("numeric_field");
-                return factory;
+            public MetricsAggregationBuilder<?> builder() {
+                return avg(name).field("numeric_field");
             }
             @Override
             public double getValue(Aggregation aggregation) {
@@ -60,10 +55,8 @@ public class NaNSortingIT extends ESIntegTestCase {
         },
         VARIANCE("variance") {
             @Override
-            public ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder builder() {
-                ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder factory = extendedStats(name);
-                factory.field("numeric_field");
-                return factory;
+            public MetricsAggregationBuilder<?> builder() {
+                return extendedStats(name).field("numeric_field");
             }
             @Override
             public String sortKey() {
@@ -76,10 +69,8 @@ public class NaNSortingIT extends ESIntegTestCase {
         },
         STD_DEVIATION("std_deviation"){
             @Override
-            public ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder builder() {
-                ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder factory = extendedStats(name);
-                factory.field("numeric_field");
-                return factory;
+            public MetricsAggregationBuilder<?> builder() {
+                return extendedStats(name).field("numeric_field");
             }
             @Override
             public String sortKey() {
@@ -97,7 +88,7 @@ public class NaNSortingIT extends ESIntegTestCase {
 
         public String name;
 
-        public abstract ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, ? extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, ?>> builder();
+        public abstract MetricsAggregationBuilder<?> builder();
 
         public String sortKey() {
             return name;
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NestedIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NestedIT.java
index cd68fab..5317f2e 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NestedIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NestedIT.java
@@ -167,7 +167,7 @@ public class NestedIT extends ESIntegTestCase {
 
     public void testSimple() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(nested("nested", "nested")
+                .addAggregation(nested("nested").path("nested")
                         .subAggregation(stats("nested_value_stats").field("nested.value")))
                 .execute().actionGet();
 
@@ -205,7 +205,7 @@ public class NestedIT extends ESIntegTestCase {
 
     public void testNonExistingNestedField() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("idx")
-                .addAggregation(nested("nested", "value")
+                .addAggregation(nested("nested").path("value")
                         .subAggregation(stats("nested_value_stats").field("nested.value")))
                 .execute().actionGet();
 
@@ -217,7 +217,7 @@ public class NestedIT extends ESIntegTestCase {
 
     public void testNestedWithSubTermsAgg() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(nested("nested", "nested")
+                .addAggregation(nested("nested").path("nested")
                         .subAggregation(terms("values").field("nested.value").size(100)
                                 .collectMode(aggCollectionMode)))
                 .execute().actionGet();
@@ -270,7 +270,7 @@ public class NestedIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(terms("top_values").field("value").size(100)
                         .collectMode(aggCollectionMode)
-                        .subAggregation(nested("nested", "nested")
+                        .subAggregation(nested("nested").path("nested")
                                 .subAggregation(max("max_value").field("nested.value"))))
                 .execute().actionGet();
 
@@ -296,10 +296,10 @@ public class NestedIT extends ESIntegTestCase {
 
     public void testNestNestedAggs() throws Exception {
         SearchResponse response = client().prepareSearch("idx_nested_nested_aggs")
-                .addAggregation(nested("level1", "nested1")
+                .addAggregation(nested("level1").path("nested1")
                         .subAggregation(terms("a").field("nested1.a")
                                 .collectMode(aggCollectionMode)
-                        .subAggregation(nested("level2", "nested1.nested2")
+                                .subAggregation(nested("level2").path("nested1.nested2")
                                         .subAggregation(sum("sum").field("nested1.nested2.b")))))
                 .get();
         assertSearchResponse(response);
@@ -333,7 +333,7 @@ public class NestedIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(nested("nested", "nested")))
+                        .subAggregation(nested("nested").path("nested")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -352,7 +352,7 @@ public class NestedIT extends ESIntegTestCase {
         try {
             client().prepareSearch("idx")
                     .setQuery(matchAllQuery())
-                    .addAggregation(nested("object_field", "incorrect"))
+                    .addAggregation(nested("object_field").path("incorrect"))
                     .execute().actionGet();
             fail();
         } catch (SearchPhaseExecutionException e) {
@@ -407,10 +407,9 @@ public class NestedIT extends ESIntegTestCase {
                         terms("startDate").field("dates.month.start").subAggregation(
                                 terms("endDate").field("dates.month.end").subAggregation(
                                         terms("period").field("dates.month.label").subAggregation(
-                                                nested("ctxt_idfier_nested", "comments")
-                                                .subAggregation(filter("comment_filter", termQuery("comments.identifier", "29111"))
-                                                        .subAggregation(nested("nested_tags", "comments.tags")
-                                                                .subAggregation(
+                                                nested("ctxt_idfier_nested").path("comments").subAggregation(
+                                                        filter("comment_filter").filter(termQuery("comments.identifier", "29111")).subAggregation(
+                                                                nested("nested_tags").path("comments.tags").subAggregation(
                                                                         terms("tag").field("comments.tags.name")
                                                                 )
                                                         )
@@ -489,7 +488,7 @@ public class NestedIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("idx4").setTypes("product")
                 .addAggregation(terms("category").field("categories").subAggregation(
-                        nested("property", "property").subAggregation(
+                        nested("property").path("property").subAggregation(
                                 terms("property_id").field("property.id")
                         )
                 ))
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java
deleted file mode 100644
index 159f491..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.RangeAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-
-public class RangeTests extends BaseAggregationTestCase<RangeAggregator.RangeAggregatorBuilder> {
-
-    @Override
-    protected RangeAggregatorBuilder createTestAggregatorBuilder() {
-        int numRanges = randomIntBetween(1, 10);
-        RangeAggregatorBuilder factory = new RangeAggregatorBuilder("foo");
-        for (int i = 0; i < numRanges; i++) {
-            String key = null;
-            if (randomBoolean()) {
-                key = randomAsciiOfLengthBetween(1, 20);
-            }
-            double from = randomBoolean() ? Double.NEGATIVE_INFINITY : randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE - 1000);
-            double to = randomBoolean() ? Double.POSITIVE_INFINITY
-                    : (Double.isInfinite(from) ? randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE)
-                            : randomIntBetween((int) from, Integer.MAX_VALUE));
-            if (randomBoolean()) {
-                factory.addRange(new Range(key, from, to));
-            } else {
-                String fromAsStr = Double.isInfinite(from) ? null : String.valueOf(from);
-                String toAsStr = Double.isInfinite(to) ? null : String.valueOf(to);
-                factory.addRange(new Range(key, fromAsStr, toAsStr));
-            }
-        }
-        factory.field(INT_FIELD_NAME);
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java
index 5f38f1b..e82a737 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java
@@ -138,7 +138,7 @@ public class ReverseNestedIT extends ESIntegTestCase {
 
     public void testSimpleReverseNestedToRoot() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type1")
-                .addAggregation(nested("nested1", "nested1")
+                .addAggregation(nested("nested1").path("nested1")
                         .subAggregation(
                                 terms("field2").field("nested1.field2")
                                         .subAggregation(
@@ -326,10 +326,10 @@ public class ReverseNestedIT extends ESIntegTestCase {
 
     public void testSimpleNested1ToRootToNested2() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type2")
-                .addAggregation(nested("nested1", "nested1")
+                .addAggregation(nested("nested1").path("nested1")
                                 .subAggregation(
                                         reverseNested("nested1_to_root")
-                                                .subAggregation(nested("root_to_nested2", "nested1.nested2"))
+                                                .subAggregation(nested("root_to_nested2").path("nested1.nested2"))
                                         )
                                 )
                 .get();
@@ -348,7 +348,7 @@ public class ReverseNestedIT extends ESIntegTestCase {
 
     public void testSimpleReverseNestedToNested1() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type2")
-                .addAggregation(nested("nested1", "nested1.nested2")
+                .addAggregation(nested("nested1").path("nested1.nested2")
                                 .subAggregation(
                                         terms("field2").field("nested1.nested2.field2").order(Terms.Order.term(true))
                                                 .collectMode(randomFrom(SubAggCollectionMode.values()))
@@ -470,7 +470,7 @@ public class ReverseNestedIT extends ESIntegTestCase {
     public void testNonExistingNestedField() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(nested("nested2", "nested1.nested2").subAggregation(reverseNested("incorrect").path("nested3")))
+                .addAggregation(nested("nested2").path("nested1.nested2").subAggregation(reverseNested("incorrect").path("nested3")))
                 .execute().actionGet();
 
         Nested nested = searchResponse.getAggregations().get("nested2");
@@ -558,11 +558,11 @@ public class ReverseNestedIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("idx3")
                 .addAggregation(
-                        nested("nested_0", "category").subAggregation(
+                        nested("nested_0").path("category").subAggregation(
                                 terms("group_by_category").field("category.name").subAggregation(
                                         reverseNested("to_root").subAggregation(
-                                                nested("nested_1", "sku").subAggregation(
-                                                        filter("filter_by_sku", termQuery("sku.sku_type", "bar1")).subAggregation(
+                                                nested("nested_1").path("sku").subAggregation(
+                                                        filter("filter_by_sku").filter(termQuery("sku.sku_type", "bar1")).subAggregation(
                                                                 count("sku_count").field("sku.sku_type")
                                                         )
                                                 )
@@ -593,13 +593,13 @@ public class ReverseNestedIT extends ESIntegTestCase {
 
         response = client().prepareSearch("idx3")
                 .addAggregation(
-                        nested("nested_0", "category").subAggregation(
+                        nested("nested_0").path("category").subAggregation(
                                 terms("group_by_category").field("category.name").subAggregation(
                                         reverseNested("to_root").subAggregation(
-                                                nested("nested_1", "sku").subAggregation(
-                                                        filter("filter_by_sku", termQuery("sku.sku_type", "bar1")).subAggregation(
-                                                                nested("nested_2", "sku.colors").subAggregation(
-                                                                        filter("filter_sku_color", termQuery("sku.colors.name", "red")).subAggregation(
+                                                nested("nested_1").path("sku").subAggregation(
+                                                        filter("filter_by_sku").filter(termQuery("sku.sku_type", "bar1")).subAggregation(
+                                                                nested("nested_2").path("sku.colors").subAggregation(
+                                                                        filter("filter_sku_color").filter(termQuery("sku.colors.name", "red")).subAggregation(
                                                                                 reverseNested("reverse_to_sku").path("sku").subAggregation(
                                                                                         count("sku_count").field("sku.sku_type")
                                                                                 )
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerIT.java
index 2a24803..623d27b 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerIT.java
@@ -23,9 +23,11 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.index.query.TermQueryBuilder;
 import org.elasticsearch.search.aggregations.bucket.sampler.Sampler;
+import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
 import org.elasticsearch.test.ESIntegTestCase;
 
@@ -38,8 +40,6 @@ import static org.elasticsearch.search.aggregations.AggregationBuilders.sampler;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.sampler;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.greaterThan;
 import static org.hamcrest.Matchers.greaterThanOrEqualTo;
@@ -123,9 +123,9 @@ public class SamplerIT extends ESIntegTestCase {
 
     }
 
-    public void testSimpleSampler() throws Exception {
-        SamplerAggregator.SamplerAggregatorBuilder sampleAgg = sampler("sample").shardSize(100);
-        sampleAgg.subAggregation(terms("authors").field("author"));
+    public void testNoDiversity() throws Exception {
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
         SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg).execute().actionGet();
         assertSearchResponse(response);
@@ -140,9 +140,89 @@ public class SamplerIT extends ESIntegTestCase {
         assertThat(maxBooksPerAuthor, equalTo(3L));
     }
 
+    public void testSimpleDiversity() throws Exception {
+        int MAX_DOCS_PER_AUTHOR = 1;
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+        SearchResponse response = client().prepareSearch("test")
+                .setSearchType(SearchType.QUERY_AND_FETCH)
+                .setQuery(new TermQueryBuilder("genre", "fantasy"))
+                .setFrom(0).setSize(60)
+                .addAggregation(sampleAgg)
+                .execute()
+                .actionGet();
+        assertSearchResponse(response);
+        Sampler sample = response.getAggregations().get("sample");
+        Terms authors = sample.getAggregations().get("authors");
+        Collection<Bucket> testBuckets = authors.getBuckets();
+
+        for (Terms.Bucket testBucket : testBuckets) {
+            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
+        }
+    }
+
+    public void testNestedDiversity() throws Exception {
+        // Test multiple samples gathered under buckets made by a parent agg
+        int MAX_DOCS_PER_AUTHOR = 1;
+        TermsBuilder rootTerms = new TermsBuilder("genres").field("genre");
+
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+
+        rootTerms.subAggregation(sampleAgg);
+        SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH)
+                .addAggregation(rootTerms).execute().actionGet();
+        assertSearchResponse(response);
+        Terms genres = response.getAggregations().get("genres");
+        Collection<Bucket> genreBuckets = genres.getBuckets();
+        for (Terms.Bucket genreBucket : genreBuckets) {
+            Sampler sample = genreBucket.getAggregations().get("sample");
+            Terms authors = sample.getAggregations().get("authors");
+            Collection<Bucket> testBuckets = authors.getBuckets();
+
+            for (Terms.Bucket testBucket : testBuckets) {
+                assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
+            }
+        }
+    }
+
+    public void testNestedSamples() throws Exception {
+        // Test samples nested under samples
+        int MAX_DOCS_PER_AUTHOR = 1;
+        int MAX_DOCS_PER_GENRE = 2;
+        SamplerAggregationBuilder rootSample = new SamplerAggregationBuilder("genreSample").shardSize(100).field("genre")
+                .maxDocsPerValue(MAX_DOCS_PER_GENRE);
+
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+        sampleAgg.subAggregation(new TermsBuilder("genres").field("genre"));
+
+        rootSample.subAggregation(sampleAgg);
+        SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH).addAggregation(rootSample)
+                .execute().actionGet();
+        assertSearchResponse(response);
+        Sampler genreSample = response.getAggregations().get("genreSample");
+        Sampler sample = genreSample.getAggregations().get("sample");
+
+        Terms genres = sample.getAggregations().get("genres");
+        Collection<Bucket> testBuckets = genres.getBuckets();
+        for (Terms.Bucket testBucket : testBuckets) {
+            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_GENRE));
+        }
+
+        Terms authors = sample.getAggregations().get("authors");
+        testBuckets = authors.getBuckets();
+        for (Terms.Bucket testBucket : testBuckets) {
+            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
+        }
+    }
+
     public void testUnmappedChildAggNoDiversity() throws Exception {
-        SamplerAggregator.SamplerAggregatorBuilder sampleAgg = sampler("sample").shardSize(100);
-        sampleAgg.subAggregation(terms("authors").field("author"));
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
         SearchResponse response = client().prepareSearch("idx_unmapped")
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("genre", "fantasy"))
@@ -158,8 +238,8 @@ public class SamplerIT extends ESIntegTestCase {
     }
 
     public void testPartiallyUnmappedChildAggNoDiversity() throws Exception {
-        SamplerAggregator.SamplerAggregatorBuilder sampleAgg = sampler("sample").shardSize(100);
-        sampleAgg.subAggregation(terms("authors").field("author"));
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
         SearchResponse response = client().prepareSearch("idx_unmapped", "test")
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("genre", "fantasy"))
@@ -174,4 +254,34 @@ public class SamplerIT extends ESIntegTestCase {
         assertThat(authors.getBuckets().size(), greaterThan(0));
     }
 
+    public void testPartiallyUnmappedDiversifyField() throws Exception {
+        // One of the indexes is missing the "author" field used for
+        // diversifying results
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100).field("author").maxDocsPerValue(1);
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+        SearchResponse response = client().prepareSearch("idx_unmapped_author", "test").setSearchType(SearchType.QUERY_AND_FETCH)
+                .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg)
+                .execute().actionGet();
+        assertSearchResponse(response);
+        Sampler sample = response.getAggregations().get("sample");
+        assertThat(sample.getDocCount(), greaterThan(0L));
+        Terms authors = sample.getAggregations().get("authors");
+        assertThat(authors.getBuckets().size(), greaterThan(0));
+    }
+
+    public void testWhollyUnmappedDiversifyField() throws Exception {
+        //All of the indices are missing the "author" field used for diversifying results
+        int MAX_DOCS_PER_AUTHOR = 1;
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+        SearchResponse response = client().prepareSearch("idx_unmapped", "idx_unmapped_author").setSearchType(SearchType.QUERY_AND_FETCH)
+                .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg).execute().actionGet();
+        assertSearchResponse(response);
+        Sampler sample = response.getAggregations().get("sample");
+        assertThat(sample.getDocCount(), equalTo(0L));
+        Terms authors = sample.getAggregations().get("authors");
+        assertNull(authors);
+    }
+
 }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerTests.java
deleted file mode 100644
index dfa2060..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerTests.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator;
-
-public class SamplerTests extends BaseAggregationTestCase<SamplerAggregator.SamplerAggregatorBuilder> {
-
-    @Override
-    protected final SamplerAggregator.SamplerAggregatorBuilder createTestAggregatorBuilder() {
-        SamplerAggregator.SamplerAggregatorBuilder factory = new SamplerAggregator.SamplerAggregatorBuilder("foo");
-        if (randomBoolean()) {
-            factory.shardSize(randomIntBetween(1, 1000));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java
index 8c6f30c..d138c0c 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java
@@ -91,7 +91,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(global("global")
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -104,8 +104,8 @@ public class ShardReduceIT extends ESIntegTestCase {
     public void testFilter() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
-                .addAggregation(filter("filter", QueryBuilders.matchAllQuery())
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                .addAggregation(filter("filter").filter(QueryBuilders.matchAllQuery())
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -119,7 +119,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(missing("missing").field("foobar")
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -133,9 +133,9 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(global("global")
-                        .subAggregation(filter("filter", QueryBuilders.matchAllQuery())
+                        .subAggregation(filter("filter").filter(QueryBuilders.matchAllQuery())
                                 .subAggregation(missing("missing").field("foobar")
-                                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))))
+                                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -150,8 +150,8 @@ public class ShardReduceIT extends ESIntegTestCase {
     public void testNested() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
-                .addAggregation(nested("nested", "nested")
-                        .subAggregation(dateHistogram("histo").field("nested.date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                .addAggregation(nested("nested").path("nested")
+                        .subAggregation(dateHistogram("histo").field("nested.date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -166,7 +166,7 @@ public class ShardReduceIT extends ESIntegTestCase {
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(terms("terms").field("term-s")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -181,7 +181,7 @@ public class ShardReduceIT extends ESIntegTestCase {
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(terms("terms").field("term-l")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -196,7 +196,7 @@ public class ShardReduceIT extends ESIntegTestCase {
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(terms("terms").field("term-d")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -210,7 +210,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(range("range").field("value").addRange("r1", 0, 10)
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -224,7 +224,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(dateRange("range").field("date").addRange("r1", "2014-01-01", "2014-01-10")
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -238,7 +238,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(ipRange("range").field("ip").addRange("r1", "10.0.0.1", "10.0.0.10")
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -252,7 +252,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(histogram("topHisto").field("value").interval(5)
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -265,8 +265,8 @@ public class ShardReduceIT extends ESIntegTestCase {
     public void testDateHistogram() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
-                .addAggregation(dateHistogram("topHisto").field("date").dateHistogramInterval(DateHistogramInterval.MONTH)
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                .addAggregation(dateHistogram("topHisto").field("date").interval(DateHistogramInterval.MONTH)
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -281,7 +281,7 @@ public class ShardReduceIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .setQuery(QueryBuilders.matchAllQuery())
                 .addAggregation(geohashGrid("grid").field("location")
-                        .subAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).minDocCount(0)))
+                        .subAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(0)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsIT.java
index 4c3e548..97a3cfa 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsIT.java
@@ -27,13 +27,14 @@ import org.elasticsearch.index.query.TermQueryBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms.Bucket;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorFactory.ExecutionMode;
+import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.ChiSquare;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.GND;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.MutualInformation;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.PercentageScore;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 
 import java.util.Arrays;
@@ -45,8 +46,6 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.significantTerms;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.hasSize;
 import static org.hamcrest.Matchers.is;
@@ -117,7 +116,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
                            .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -133,8 +132,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "paul"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
-                           .minDocCount(1).includeExclude(new IncludeExclude(null, excludeTerms)))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
+                           .minDocCount(1).exclude(excludeTerms))
                 .execute()
                 .actionGet();
         assertSearchResponse(response);
@@ -146,8 +145,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
     public void testIncludeExclude() throws Exception {
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(new TermQueryBuilder("_all", "weller"))
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
-                        .includeExclude(new IncludeExclude(null, "weller")))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                        .exclude("weller"))
                 .get();
         assertSearchResponse(response);
         SignificantTerms topTerms = response.getAggregations().get("mySignificantTerms");
@@ -165,8 +164,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
 
         response = client().prepareSearch("test")
                 .setQuery(new TermQueryBuilder("_all", "weller"))
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
-                        .includeExclude(new IncludeExclude("weller", null)))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                        .include("weller"))
                 .get();
         assertSearchResponse(response);
         topTerms = response.getAggregations().get("mySignificantTerms");
@@ -182,8 +181,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
         String []incExcTerms={"weller","nosuchterm"};
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(new TermQueryBuilder("_all", "weller"))
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
-                        .includeExclude(new IncludeExclude(null, incExcTerms)))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                        .exclude(incExcTerms))
                 .get();
         assertSearchResponse(response);
         SignificantTerms topTerms = response.getAggregations().get("mySignificantTerms");
@@ -195,8 +194,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
 
         response = client().prepareSearch("test")
                 .setQuery(new TermQueryBuilder("_all", "weller"))
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
-                        .includeExclude(new IncludeExclude(incExcTerms, null)))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                        .include(incExcTerms))
                 .get();
         assertSearchResponse(response);
         topTerms = response.getAggregations().get("mySignificantTerms");
@@ -213,7 +212,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("fact_category").executionHint(randomExecutionHint())
                         .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -227,7 +226,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
                            .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -241,7 +240,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint()).significanceHeuristic(new GND(true))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint()).significanceHeuristic(new GND.GNDBuilder(true))
                         .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -255,7 +254,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint()).significanceHeuristic(new ChiSquare(false,true))
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint()).significanceHeuristic(new ChiSquare.ChiSquareBuilder(false,true))
                         .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -273,8 +272,8 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSize(60)
                 .setExplain(true)
                 .addAggregation(
-                        significantTerms("mySignificantTerms").field("description").executionHint(randomExecutionHint())
-                                .significanceHeuristic(new PercentageScore()).minDocCount(2)).execute().actionGet();
+                        new SignificantTermsBuilder("mySignificantTerms").field("description").executionHint(randomExecutionHint())
+                                .significanceHeuristic(new PercentageScore.PercentageScoreBuilder()).minDocCount(2)).execute().actionGet();
         assertSearchResponse(response);
         SignificantTerms topTerms = response.getAggregations().get("mySignificantTerms");
         checkExpectedStringTermsFound(topTerms);
@@ -289,7 +288,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description")
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description")
                            .minDocCount(2).backgroundFilter(QueryBuilders.termQuery("fact_category", 1)))
                 .execute()
                 .actionGet();
@@ -313,7 +312,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "weller"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description")
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description")
                            .minDocCount(1).backgroundFilter(QueryBuilders.termsQuery("description",  "paul")))
                 .execute()
                 .actionGet();
@@ -336,9 +335,9 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 { "craig", "kelly", "terje", "haakonsen", "burton" }};
         SearchResponse response = client().prepareSearch("test")
                 .setSearchType(SearchType.QUERY_AND_FETCH)
-                .addAggregation(terms("myCategories").field("fact_category").minDocCount(2)
+                .addAggregation(new TermsBuilder("myCategories").field("fact_category").minDocCount(2)
                         .subAggregation(
-                                   significantTerms("mySignificantTerms").field("description")
+                                   new SignificantTermsBuilder("mySignificantTerms").field("description")
                                    .executionHint(randomExecutionHint())
                                    .minDocCount(2)))
                 .execute()
@@ -363,7 +362,7 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms").field("description")
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms").field("description")
                             .executionHint(randomExecutionHint())
                            .minDocCount(2))
                 .execute()
@@ -394,10 +393,10 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms")
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms")
                         .field("description")
                         .executionHint(randomExecutionHint())
-                        .significanceHeuristic(new JLHScore())
+                        .significanceHeuristic(new JLHScore.JLHScoreBuilder())
                         .minDocCount(2))
                 .execute()
                 .actionGet();
@@ -411,10 +410,10 @@ public class SignificantTermsIT extends ESIntegTestCase {
                 .setSearchType(SearchType.QUERY_AND_FETCH)
                 .setQuery(new TermQueryBuilder("_all", "terje"))
                 .setFrom(0).setSize(60).setExplain(true)
-                .addAggregation(significantTerms("mySignificantTerms")
+                .addAggregation(new SignificantTermsBuilder("mySignificantTerms")
                         .field("description")
                         .executionHint(randomExecutionHint())
-                        .significanceHeuristic(new MutualInformation(false, true))
+                        .significanceHeuristic(new MutualInformation.MutualInformationBuilder(false, true))
                         .minDocCount(1))
                 .execute()
                 .actionGet();
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
index d112957..6c1e7df 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
@@ -20,7 +20,6 @@ package org.elasticsearch.search.aggregations.bucket;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -35,20 +34,25 @@ import org.elasticsearch.script.ScriptModule;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.search.SearchModule;
 import org.elasticsearch.search.aggregations.Aggregation;
+import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.filter.InternalFilter;
 import org.elasticsearch.search.aggregations.bucket.script.NativeSignificanceScoreScriptNoParams;
 import org.elasticsearch.search.aggregations.bucket.script.NativeSignificanceScoreScriptWithParams;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.ChiSquare;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.GND;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.MutualInformation;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParser;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicStreams;
 import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
+import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.search.aggregations.bucket.SharedSignificantTermsTestMethods;
 
@@ -65,9 +69,6 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.filter;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.significantTerms;
 import static org.hamcrest.Matchers.closeTo;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.greaterThan;
@@ -98,12 +99,11 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         String settings = "{\"index.number_of_shards\": 1, \"index.number_of_replicas\": 0}";
         SharedSignificantTermsTestMethods.index01Docs(type, settings, this);
         SearchResponse response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(
-                        terms("class")
+                .addAggregation(new TermsBuilder("class")
                         .field(CLASS_FIELD)
-                                .subAggregation((significantTerms("sig_terms"))
+                        .subAggregation((new SignificantTermsBuilder("sig_terms"))
                                 .field(TEXT_FIELD)
-                                .significanceHeuristic(new SimpleHeuristic())
+                                .significanceHeuristic(new SimpleHeuristic.SimpleHeuristicBuilder())
                                 .minDocCount(1)
                         )
                 )
@@ -131,12 +131,11 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         // the reason is that this would trigger toXContent and we would like to check that this has no potential side effects
 
         response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(
-                        terms("class")
+                .addAggregation(new TermsBuilder("class")
                         .field(CLASS_FIELD)
-                                .subAggregation((significantTerms("sig_terms"))
+                        .subAggregation((new SignificantTermsBuilder("sig_terms"))
                                 .field(TEXT_FIELD)
-                                .significanceHeuristic(new SimpleHeuristic())
+                                .significanceHeuristic(new SimpleHeuristic.SimpleHeuristicBuilder())
                                 .minDocCount(1)
                         )
                 )
@@ -164,7 +163,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
     public static class CustomSignificanceHeuristicPlugin extends Plugin {
 
         static {
-            SignificanceHeuristicStreams.registerPrototype(SimpleHeuristic.PROTOTYPE);
+            SignificanceHeuristicStreams.registerStream(SimpleHeuristic.STREAM);
         }
 
         @Override
@@ -178,7 +177,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         }
 
         public void onModule(SearchModule significanceModule) {
-            significanceModule.registerHeuristicParser(new SimpleHeuristic.SimpleHeuristicParser());
+            significanceModule.registerHeuristicParser(SimpleHeuristic.SimpleHeuristicParser.class);
         }
         public void onModule(ScriptModule module) {
             module.registerScript(NativeSignificanceScoreScriptNoParams.NATIVE_SIGNIFICANCE_SCORE_SCRIPT_NO_PARAMS, NativeSignificanceScoreScriptNoParams.Factory.class);
@@ -188,46 +187,24 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
     public static class SimpleHeuristic extends SignificanceHeuristic {
 
-        static final SimpleHeuristic PROTOTYPE = new SimpleHeuristic();
+        protected static final String[] NAMES = {"simple"};
 
-        protected static final ParseField NAMES_FIELD = new ParseField("simple");
+        public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+            @Override
+            public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+                return readFrom(in);
+            }
 
-        @Override
-        public String getWriteableName() {
-            return NAMES_FIELD.getPreferredName();
-        }
+            @Override
+            public String getName() {
+                return NAMES[0];
+            }
+        };
 
-        @Override
-        public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
+        public static SignificanceHeuristic readFrom(StreamInput in) throws IOException {
             return new SimpleHeuristic();
         }
 
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
-            return builder;
-        }
-
-        @Override
-        public int hashCode() {
-            return 1;
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            return true;
-        }
-
         /**
          * @param subsetFreq   The frequency of the term in the selected sample
          * @param subsetSize   The size of the selected sample (typically number of docs)
@@ -240,10 +217,15 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
             return subsetFreq / subsetSize > supersetFreq / supersetSize ? 2.0 : 1.0;
         }
 
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            out.writeString(STREAM.getName());
+        }
+
         public static class SimpleHeuristicParser implements SignificanceHeuristicParser {
 
             @Override
-            public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+            public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                     throws IOException, QueryShardException {
                 parser.nextToken();
                 return new SimpleHeuristic();
@@ -251,7 +233,16 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
             @Override
             public String[] getNames() {
-                return NAMES_FIELD.getAllNamesIncludedDeprecated();
+                return NAMES;
+            }
+        }
+
+        public static class SimpleHeuristicBuilder implements SignificanceHeuristicBuilder {
+
+            @Override
+            public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+                builder.startObject(STREAM.getName()).endObject();
+                return builder;
             }
         }
     }
@@ -261,7 +252,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         String settings = "{\"index.number_of_shards\": 1, \"index.number_of_replicas\": 0}";
         SharedSignificantTermsTestMethods.index01Docs(type, settings, this);
         SearchResponse response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(terms("class").field(CLASS_FIELD).subAggregation(significantTerms("sig_terms").field(TEXT_FIELD)))
+                .addAggregation(new TermsBuilder("class").field(CLASS_FIELD).subAggregation(new SignificantTermsBuilder("sig_terms").field(TEXT_FIELD)))
                 .execute()
                 .actionGet();
         assertSearchResponse(response);
@@ -321,11 +312,10 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         indexRandom(true, false, indexRequestBuilderList);
 
         SearchResponse response1 = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(
-                        terms("class")
+                .addAggregation(new TermsBuilder("class")
                         .field(CLASS_FIELD)
                         .subAggregation(
-                                significantTerms("sig_terms")
+                                new SignificantTermsBuilder("sig_terms")
                                         .field(TEXT_FIELD)
                                         .minDocCount(1)))
                 .execute()
@@ -336,22 +326,22 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         String type = randomBoolean() ? "string" : "long";
         String settings = "{\"index.number_of_shards\": 1, \"index.number_of_replicas\": 0}";
         SharedSignificantTermsTestMethods.index01Docs(type, settings, this);
-        testBackgroundVsSeparateSet(new MutualInformation(true, true), new MutualInformation(true, false));
-        testBackgroundVsSeparateSet(new ChiSquare(true, true), new ChiSquare(true, false));
-        testBackgroundVsSeparateSet(new GND(true), new GND(false));
+        testBackgroundVsSeparateSet(new MutualInformation.MutualInformationBuilder(true, true), new MutualInformation.MutualInformationBuilder(true, false));
+        testBackgroundVsSeparateSet(new ChiSquare.ChiSquareBuilder(true, true), new ChiSquare.ChiSquareBuilder(true, false));
+        testBackgroundVsSeparateSet(new GND.GNDBuilder(true), new GND.GNDBuilder(false));
     }
 
     // compute significance score by
     // 1. terms agg on class and significant terms
     // 2. filter buckets and set the background to the other class and set is_background false
     // both should yield exact same result
-    public void testBackgroundVsSeparateSet(SignificanceHeuristic significanceHeuristicExpectingSuperset, SignificanceHeuristic significanceHeuristicExpectingSeparateSets) throws Exception {
+    public void testBackgroundVsSeparateSet(SignificanceHeuristicBuilder significanceHeuristicExpectingSuperset, SignificanceHeuristicBuilder significanceHeuristicExpectingSeparateSets) throws Exception {
 
         SearchResponse response1 = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(terms("class")
+                .addAggregation(new TermsBuilder("class")
                         .field(CLASS_FIELD)
                         .subAggregation(
-                                significantTerms("sig_terms")
+                                new SignificantTermsBuilder("sig_terms")
                                         .field(TEXT_FIELD)
                                         .minDocCount(1)
                                         .significanceHeuristic(
@@ -360,14 +350,16 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
                 .actionGet();
         assertSearchResponse(response1);
         SearchResponse response2 = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
-                .addAggregation(filter("0", QueryBuilders.termQuery(CLASS_FIELD, "0"))
-                        .subAggregation(significantTerms("sig_terms")
+                .addAggregation((new FilterAggregationBuilder("0"))
+                        .filter(QueryBuilders.termQuery(CLASS_FIELD, "0"))
+                        .subAggregation(new SignificantTermsBuilder("sig_terms")
                                 .field(TEXT_FIELD)
                                 .minDocCount(1)
                                 .backgroundFilter(QueryBuilders.termQuery(CLASS_FIELD, "1"))
                                 .significanceHeuristic(significanceHeuristicExpectingSeparateSets)))
-                .addAggregation(filter("1", QueryBuilders.termQuery(CLASS_FIELD, "1"))
-                        .subAggregation(significantTerms("sig_terms")
+                .addAggregation((new FilterAggregationBuilder("1"))
+                        .filter(QueryBuilders.termQuery(CLASS_FIELD, "1"))
+                        .subAggregation(new SignificantTermsBuilder("sig_terms")
                                 .field(TEXT_FIELD)
                                 .minDocCount(1)
                                 .backgroundFilter(QueryBuilders.termQuery(CLASS_FIELD, "0"))
@@ -396,15 +388,15 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
     public void testScoresEqualForPositiveAndNegative() throws Exception {
         indexEqualTestData();
-        testScoresEqualForPositiveAndNegative(new MutualInformation(true, true));
-        testScoresEqualForPositiveAndNegative(new ChiSquare(true, true));
+        testScoresEqualForPositiveAndNegative(new MutualInformation.MutualInformationBuilder(true, true));
+        testScoresEqualForPositiveAndNegative(new ChiSquare.ChiSquareBuilder(true, true));
     }
 
-    public void testScoresEqualForPositiveAndNegative(SignificanceHeuristic heuristic) throws Exception {
+    public void testScoresEqualForPositiveAndNegative(SignificanceHeuristicBuilder heuristic) throws Exception {
 
         //check that results for both classes are the same with exclude negatives = false and classes are routing ids
         SearchResponse response = client().prepareSearch("test")
-                .addAggregation(terms("class").field("class").subAggregation(significantTerms("mySignificantTerms")
+                .addAggregation(new TermsBuilder("class").field("class").subAggregation(new SignificantTermsBuilder("mySignificantTerms")
                         .field("text")
                         .executionHint(randomExecutionHint())
                         .significanceHeuristic(heuristic)
@@ -461,14 +453,13 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
     public void testScriptScore() throws ExecutionException, InterruptedException, IOException {
         indexRandomFrequencies01(randomBoolean() ? "string" : "long");
-        ScriptHeuristic scriptHeuristic = getScriptSignificanceHeuristic();
+        ScriptHeuristic.ScriptHeuristicBuilder scriptHeuristicBuilder = getScriptSignificanceHeuristicBuilder();
         ensureYellow();
         SearchResponse response = client().prepareSearch(INDEX_NAME)
-                .addAggregation(terms("class").field(CLASS_FIELD)
-                        .subAggregation(significantTerms("mySignificantTerms")
+                .addAggregation(new TermsBuilder("class").field(CLASS_FIELD).subAggregation(new SignificantTermsBuilder("mySignificantTerms")
                         .field(TEXT_FIELD)
                         .executionHint(randomExecutionHint())
-                        .significanceHeuristic(scriptHeuristic)
+                        .significanceHeuristic(scriptHeuristicBuilder)
                         .minDocCount(1).shardSize(2).size(2)))
                 .execute()
                 .actionGet();
@@ -480,7 +471,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         }
     }
 
-    private ScriptHeuristic getScriptSignificanceHeuristic() throws IOException {
+    private ScriptHeuristic.ScriptHeuristicBuilder getScriptSignificanceHeuristicBuilder() throws IOException {
         Script script = null;
         if (randomBoolean()) {
             Map<String, Object> params = null;
@@ -490,9 +481,9 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         } else {
             script = new Script("native_significance_score_script_no_params", ScriptType.INLINE, "native", null);
         }
-        ScriptHeuristic scriptHeuristic = new ScriptHeuristic(script);
+        ScriptHeuristic.ScriptHeuristicBuilder builder = new ScriptHeuristic.ScriptHeuristicBuilder().setScript(script);
 
-        return scriptHeuristic;
+        return builder;
     }
 
     private void indexRandomFrequencies01(String type) throws ExecutionException, InterruptedException {
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsTests.java
deleted file mode 100644
index 1db778c..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsTests.java
+++ /dev/null
@@ -1,224 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.automaton.RegExp;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.ChiSquare;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.GND;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.MutualInformation;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.PercentageScore;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.ExecutionMode;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import java.util.SortedSet;
-import java.util.TreeSet;
-
-public class SignificantTermsTests extends BaseAggregationTestCase<SignificantTermsAggregatorBuilder> {
-
-    private static final String[] executionHints;
-
-    static {
-        ExecutionMode[] executionModes = ExecutionMode.values();
-        executionHints = new String[executionModes.length];
-        for (int i = 0; i < executionModes.length; i++) {
-            executionHints[i] = executionModes[i].toString();
-        }
-    }
-
-    @Override
-    protected SignificantTermsAggregatorBuilder createTestAggregatorBuilder() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        SignificantTermsAggregatorBuilder factory = new SignificantTermsAggregatorBuilder(name, null);
-        String field = randomAsciiOfLengthBetween(3, 20);
-        int randomFieldBranch = randomInt(2);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        default:
-            fail();
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            int size = randomInt(4);
-            switch (size) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                size = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setRequiredSize(size);
-
-        }
-        if (randomBoolean()) {
-            int shardSize = randomInt(4);
-            switch (shardSize) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardSize = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setShardSize(shardSize);
-        }
-        if (randomBoolean()) {
-            int minDocCount = randomInt(4);
-            switch (minDocCount) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                minDocCount = randomInt();
-                break;
-            }
-            factory.bucketCountThresholds().setMinDocCount(minDocCount);
-        }
-        if (randomBoolean()) {
-            int shardMinDocCount = randomInt(4);
-            switch (shardMinDocCount) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardMinDocCount = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setShardMinDocCount(shardMinDocCount);
-        }
-        if (randomBoolean()) {
-            factory.executionHint(randomFrom(executionHints));
-        }
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            IncludeExclude incExc = null;
-            switch (randomInt(5)) {
-            case 0:
-                incExc = new IncludeExclude(new RegExp("foobar"), null);
-                break;
-            case 1:
-                incExc = new IncludeExclude(null, new RegExp("foobaz"));
-                break;
-            case 2:
-                incExc = new IncludeExclude(new RegExp("foobar"), new RegExp("foobaz"));
-                break;
-            case 3:
-                SortedSet<BytesRef> includeValues = new TreeSet<>();
-                int numIncs = randomIntBetween(1, 20);
-                for (int i = 0; i < numIncs; i++) {
-                    includeValues.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                SortedSet<BytesRef> excludeValues = null;
-                incExc = new IncludeExclude(includeValues, excludeValues);
-                break;
-            case 4:
-                SortedSet<BytesRef> includeValues2 = null;
-                SortedSet<BytesRef> excludeValues2 = new TreeSet<>();
-                int numExcs2 = randomIntBetween(1, 20);
-                for (int i = 0; i < numExcs2; i++) {
-                    excludeValues2.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                incExc = new IncludeExclude(includeValues2, excludeValues2);
-                break;
-            case 5:
-                SortedSet<BytesRef> includeValues3 = new TreeSet<>();
-                int numIncs3 = randomIntBetween(1, 20);
-                for (int i = 0; i < numIncs3; i++) {
-                    includeValues3.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                SortedSet<BytesRef> excludeValues3 = new TreeSet<>();
-                int numExcs3 = randomIntBetween(1, 20);
-                for (int i = 0; i < numExcs3; i++) {
-                    excludeValues3.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                incExc = new IncludeExclude(includeValues3, excludeValues3);
-                break;
-            default:
-                fail();
-            }
-            factory.includeExclude(incExc);
-        }
-        if (randomBoolean()) {
-            SignificanceHeuristic significanceHeuristic = null;
-            switch (randomInt(5)) {
-            case 0:
-                significanceHeuristic = PercentageScore.PROTOTYPE;
-                break;
-            case 1:
-                significanceHeuristic = new ChiSquare(randomBoolean(), randomBoolean());
-                break;
-            case 2:
-                significanceHeuristic = new GND(randomBoolean());
-                break;
-            case 3:
-                significanceHeuristic = new MutualInformation(randomBoolean(), randomBoolean());
-                break;
-            case 4:
-                significanceHeuristic = new ScriptHeuristic(new Script("foo"));
-                break;
-            case 5:
-                significanceHeuristic = JLHScore.PROTOTYPE;
-                break;
-            default:
-                fail();
-            }
-            factory.significanceHeuristic(significanceHeuristic);
-        }
-        if (randomBoolean()) {
-            factory.backgroundFilter(QueryBuilders.termsQuery("foo", "bar"));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java
index 4d61f45..9a7b337 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsShardMinDocCountIT.java
@@ -21,10 +21,13 @@ package org.elasticsearch.search.aggregations.bucket;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.index.query.QueryBuilders;
+import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.filter.InternalFilter;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsBuilder;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 
 import java.util.ArrayList;
@@ -34,9 +37,6 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.filter;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.significantTerms;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.hamcrest.Matchers.equalTo;
 
 /**
@@ -71,8 +71,8 @@ public class TermsShardMinDocCountIT extends ESIntegTestCase {
         // first, check that indeed when not setting the shardMinDocCount parameter 0 terms are returned
         SearchResponse response = client().prepareSearch(index)
                 .addAggregation(
-                        (filter("inclass", QueryBuilders.termQuery("class", true)))
-                                .subAggregation(significantTerms("mySignificantTerms").field("text").minDocCount(2).size(2).executionHint(randomExecutionHint()))
+                        (new FilterAggregationBuilder("inclass").filter(QueryBuilders.termQuery("class", true)))
+                                .subAggregation(new SignificantTermsBuilder("mySignificantTerms").field("text").minDocCount(2).size(2).executionHint(randomExecutionHint()))
                 )
                 .execute()
                 .actionGet();
@@ -84,8 +84,8 @@ public class TermsShardMinDocCountIT extends ESIntegTestCase {
 
         response = client().prepareSearch(index)
                 .addAggregation(
-                        (filter("inclass", QueryBuilders.termQuery("class", true)))
-                                .subAggregation(significantTerms("mySignificantTerms").field("text").minDocCount(2).shardMinDocCount(2).size(2).executionHint(randomExecutionHint()))
+                        (new FilterAggregationBuilder("inclass").filter(QueryBuilders.termQuery("class", true)))
+                                .subAggregation(new SignificantTermsBuilder("mySignificantTerms").field("text").minDocCount(2).shardMinDocCount(2).size(2).executionHint(randomExecutionHint()))
                 )
                 .execute()
                 .actionGet();
@@ -127,7 +127,7 @@ public class TermsShardMinDocCountIT extends ESIntegTestCase {
         // first, check that indeed when not setting the shardMinDocCount parameter 0 terms are returned
         SearchResponse response = client().prepareSearch(index)
                 .addAggregation(
-                        terms("myTerms").field("text").minDocCount(2).size(2).executionHint(randomExecutionHint()).order(Terms.Order.term(true))
+                        new TermsBuilder("myTerms").field("text").minDocCount(2).size(2).executionHint(randomExecutionHint()).order(Terms.Order.term(true))
                 )
                 .execute()
                 .actionGet();
@@ -138,7 +138,7 @@ public class TermsShardMinDocCountIT extends ESIntegTestCase {
 
         response = client().prepareSearch(index)
                 .addAggregation(
-                        terms("myTerms").field("text").minDocCount(2).shardMinDocCount(2).size(2).executionHint(randomExecutionHint()).order(Terms.Order.term(true))
+                        new TermsBuilder("myTerms").field("text").minDocCount(2).shardMinDocCount(2).size(2).executionHint(randomExecutionHint()).order(Terms.Order.term(true))
                 )
                 .execute()
                 .actionGet();
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsTests.java
deleted file mode 100644
index ea12f6a..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsTests.java
+++ /dev/null
@@ -1,230 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.automaton.RegExp;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.ExecutionMode;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.SortedSet;
-import java.util.TreeSet;
-
-public class TermsTests extends BaseAggregationTestCase<TermsAggregatorBuilder> {
-
-    private static final String[] executionHints;
-
-    static {
-        ExecutionMode[] executionModes = ExecutionMode.values();
-        executionHints = new String[executionModes.length];
-        for (int i = 0; i < executionModes.length; i++) {
-            executionHints[i] = executionModes[i].toString();
-        }
-    }
-
-    @Override
-    protected TermsAggregatorBuilder createTestAggregatorBuilder() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        TermsAggregatorBuilder factory = new TermsAggregatorBuilder(name, null);
-        String field = randomAsciiOfLengthBetween(3, 20);
-        int randomFieldBranch = randomInt(2);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        default:
-            fail();
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            int size = randomInt(4);
-            switch (size) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                size = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setRequiredSize(size);
-
-        }
-        if (randomBoolean()) {
-            int shardSize = randomInt(4);
-            switch (shardSize) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardSize = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setShardSize(shardSize);
-        }
-        if (randomBoolean()) {
-            int minDocCount = randomInt(4);
-            switch (minDocCount) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                minDocCount = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setMinDocCount(minDocCount);
-        }
-        if (randomBoolean()) {
-            int shardMinDocCount = randomInt(4);
-            switch (shardMinDocCount) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardMinDocCount = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setShardMinDocCount(shardMinDocCount);
-        }
-        if (randomBoolean()) {
-            factory.collectMode(randomFrom(SubAggCollectionMode.values()));
-        }
-        if (randomBoolean()) {
-            factory.executionHint(randomFrom(executionHints));
-        }
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            IncludeExclude incExc = null;
-            switch (randomInt(5)) {
-            case 0:
-                incExc = new IncludeExclude(new RegExp("foobar"), null);
-                break;
-            case 1:
-                incExc = new IncludeExclude(null, new RegExp("foobaz"));
-                break;
-            case 2:
-                incExc = new IncludeExclude(new RegExp("foobar"), new RegExp("foobaz"));
-                break;
-            case 3:
-                SortedSet<BytesRef> includeValues = new TreeSet<>();
-                int numIncs = randomIntBetween(1, 20);
-                for (int i = 0; i < numIncs; i++) {
-                    includeValues.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                SortedSet<BytesRef> excludeValues = null;
-                incExc = new IncludeExclude(includeValues, excludeValues);
-                break;
-            case 4:
-                SortedSet<BytesRef> includeValues2 = null;
-                SortedSet<BytesRef> excludeValues2 = new TreeSet<>();
-                int numExcs2 = randomIntBetween(1, 20);
-                for (int i = 0; i < numExcs2; i++) {
-                    excludeValues2.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                incExc = new IncludeExclude(includeValues2, excludeValues2);
-                break;
-            case 5:
-                SortedSet<BytesRef> includeValues3 = new TreeSet<>();
-                int numIncs3 = randomIntBetween(1, 20);
-                for (int i = 0; i < numIncs3; i++) {
-                    includeValues3.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                SortedSet<BytesRef> excludeValues3 = new TreeSet<>();
-                int numExcs3 = randomIntBetween(1, 20);
-                for (int i = 0; i < numExcs3; i++) {
-                    excludeValues3.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                incExc = new IncludeExclude(includeValues3, excludeValues3);
-                break;
-            default:
-                fail();
-            }
-            factory.includeExclude(incExc);
-        }
-        if (randomBoolean()) {
-            List<Terms.Order> order = randomOrder();
-            factory.order(order);
-        }
-        if (randomBoolean()) {
-            factory.showTermDocCountError(randomBoolean());
-        }
-        return factory;
-    }
-
-    private List<Terms.Order> randomOrder() {
-        List<Terms.Order> orders = new ArrayList<>();
-        switch (randomInt(4)) {
-        case 0:
-            orders.add(Terms.Order.term(randomBoolean()));
-            break;
-        case 1:
-            orders.add(Terms.Order.count(randomBoolean()));
-            break;
-        case 2:
-            orders.add(Terms.Order.aggregation(randomAsciiOfLengthBetween(3, 20), randomBoolean()));
-            break;
-        case 3:
-            orders.add(Terms.Order.aggregation(randomAsciiOfLengthBetween(3, 20), randomAsciiOfLengthBetween(3, 20), randomBoolean()));
-            break;
-        case 4:
-            int numOrders = randomIntBetween(1, 3);
-            for (int i = 0; i < numOrders; i++) {
-                orders.addAll(randomOrder());
-            }
-            break;
-        default:
-            fail();
-        }
-        return orders;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParserTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParserTests.java
index 4ffa860..ce26728 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParserTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParserTests.java
@@ -18,52 +18,40 @@
  */
 package org.elasticsearch.search.aggregations.bucket.geogrid;
 
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.TestSearchContext;
 
 public class GeoHashGridParserTests extends ESTestCase {
     public void testParseValidFromInts() throws Exception {
+        SearchContext searchContext = new TestSearchContext(null);
         int precision = randomIntBetween(1, 12);
         XContentParser stParser = JsonXContent.jsonXContent.createParser(
                 "{\"field\":\"my_loc\", \"precision\":" + precision + ", \"size\": 500, \"shard_size\": 550}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         // can create a factory
-        assertNotNull(parser.parse("geohash_grid", stParser, parseContext));
+        assertNotNull(parser.parse("geohash_grid", stParser, searchContext));
     }
 
     public void testParseValidFromStrings() throws Exception {
+        SearchContext searchContext = new TestSearchContext(null);
         int precision = randomIntBetween(1, 12);
         XContentParser stParser = JsonXContent.jsonXContent.createParser(
                 "{\"field\":\"my_loc\", \"precision\":\"" + precision + "\", \"size\": \"500\", \"shard_size\": \"550\"}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         // can create a factory
-        assertNotNull(parser.parse("geohash_grid", stParser, parseContext));
+        assertNotNull(parser.parse("geohash_grid", stParser, searchContext));
     }
 
     public void testParseErrorOnNonIntPrecision() throws Exception {
+        SearchContext searchContext = new TestSearchContext(null);
         XContentParser stParser = JsonXContent.jsonXContent.createParser("{\"field\":\"my_loc\", \"precision\":\"2.0\"}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         try {
-            parser.parse("geohash_grid", stParser, parseContext);
+            parser.parse("geohash_grid", stParser, searchContext);
             fail();
         } catch (NumberFormatException ex) {
             assertEquals("For input string: \"2.0\"", ex.getMessage());
@@ -71,34 +59,26 @@ public class GeoHashGridParserTests extends ESTestCase {
     }
 
     public void testParseErrorOnBooleanPrecision() throws Exception {
+        SearchContext searchContext = new TestSearchContext(null);
         XContentParser stParser = JsonXContent.jsonXContent.createParser("{\"field\":\"my_loc\", \"precision\":false}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         try {
-            parser.parse("geohash_grid", stParser, parseContext);
+            parser.parse("geohash_grid", stParser, searchContext);
             fail();
-        } catch (ParsingException ex) {
-            assertEquals("Unexpected token VALUE_BOOLEAN [precision] in [geohash_grid].", ex.getMessage());
+        } catch (SearchParseException ex) {
+            assertEquals("Unexpected token VALUE_BOOLEAN in [geohash_grid].", ex.getMessage());
         }
     }
 
     public void testParseErrorOnPrecisionOutOfRange() throws Exception {
+        SearchContext searchContext = new TestSearchContext(null);
         XContentParser stParser = JsonXContent.jsonXContent.createParser("{\"field\":\"my_loc\", \"precision\":\"13\"}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         try {
-            parser.parse("geohash_grid", stParser, parseContext);
+            parser.parse("geohash_grid", stParser, searchContext);
             fail();
         } catch (IllegalArgumentException ex) {
             assertEquals("Invalid geohash aggregation precision of 13. Must be between 1 and 12.", ex.getMessage());
         }
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
index cfe7e00..a9cdb6c 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
@@ -123,11 +123,10 @@ public class NestedAggregatorTests extends ESSingleNodeTestCase {
         AggregationContext context = new AggregationContext(searchContext);
 
         AggregatorFactories.Builder builder = AggregatorFactories.builder();
-        NestedAggregator.NestedAggregatorBuilder factory = new NestedAggregator.NestedAggregatorBuilder("test", "nested_field");
-        builder.addAggregator(factory);
-        AggregatorFactories factories = builder.build(context, null);
+        builder.addAggregator(new NestedAggregator.Factory("test", "nested_field"));
+        AggregatorFactories factories = builder.build();
         searchContext.aggregations(new SearchContextAggregations(factories));
-        Aggregator[] aggs = factories.createTopLevelAggregators();
+        Aggregator[] aggs = factories.createTopLevelAggregators(context);
         BucketCollector collector = BucketCollector.wrap(Arrays.asList(aggs));
         collector.preCollection();
         // A regular search always exclude nested docs, so we use NonNestedDocsFilter.INSTANCE here (otherwise MatchAllDocsQuery would be sufficient)
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedTests.java
deleted file mode 100644
index 5aa26cd..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedTests.java
+++ /dev/null
@@ -1,32 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.nested;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.nested.NestedAggregator.NestedAggregatorBuilder;
-
-public class NestedTests extends BaseAggregationTestCase<NestedAggregator.NestedAggregatorBuilder> {
-
-    @Override
-    protected NestedAggregatorBuilder createTestAggregatorBuilder() {
-        return new NestedAggregatorBuilder(randomAsciiOfLengthBetween(1, 20), randomAsciiOfLengthBetween(3, 40));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedTests.java
deleted file mode 100644
index 56eb412..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedTests.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.nested;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.nested.ReverseNestedAggregator.ReverseNestedAggregatorBuilder;
-
-public class ReverseNestedTests extends BaseAggregationTestCase<ReverseNestedAggregator.ReverseNestedAggregatorBuilder> {
-
-    @Override
-    protected ReverseNestedAggregatorBuilder createTestAggregatorBuilder() {
-        ReverseNestedAggregatorBuilder factory = new ReverseNestedAggregatorBuilder(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.path(randomAsciiOfLengthBetween(3, 40));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java
index a64ccfb..0a094c3 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java
@@ -21,18 +21,13 @@ package org.elasticsearch.search.aggregations.bucket.significant;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.Version;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.InputStreamStreamInput;
 import org.elasticsearch.common.io.stream.OutputStreamStreamOutput;
-import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
 import org.elasticsearch.index.Index;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.QueryParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregations;
@@ -42,6 +37,7 @@ import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHSc
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.MutualInformation;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.PercentageScore;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParser;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParserMapper;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
@@ -67,13 +63,17 @@ import static org.hamcrest.Matchers.greaterThan;
 import static org.hamcrest.Matchers.greaterThanOrEqualTo;
 import static org.hamcrest.Matchers.lessThan;
 import static org.hamcrest.Matchers.lessThanOrEqualTo;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.significantTerms;
 
 /**
  *
  */
 public class SignificanceHeuristicTests extends ESTestCase {
     static class SignificantTermsTestSearchContext extends TestSearchContext {
+
+        public SignificantTermsTestSearchContext() {
+            super(null);
+        }
+
         @Override
         public int numberOfShards() {
             return 1;
@@ -133,7 +133,7 @@ public class SignificanceHeuristicTests extends ESTestCase {
 
     SignificanceHeuristic getRandomSignificanceheuristic() {
         List<SignificanceHeuristic> heuristics = new ArrayList<>();
-        heuristics.add(JLHScore.PROTOTYPE);
+        heuristics.add(JLHScore.INSTANCE);
         heuristics.add(new MutualInformation(randomBoolean(), randomBoolean()));
         heuristics.add(new GND(randomBoolean()));
         heuristics.add(new ChiSquare(randomBoolean(), randomBoolean()));
@@ -196,7 +196,7 @@ public class SignificanceHeuristicTests extends ESTestCase {
     public void testBuilderAndParser() throws Exception {
 
         Set<SignificanceHeuristicParser> parsers = new HashSet<>();
-        SignificanceHeuristicParserMapper heuristicParserMapper = new SignificanceHeuristicParserMapper(parsers);
+        SignificanceHeuristicParserMapper heuristicParserMapper = new SignificanceHeuristicParserMapper(parsers, null);
         SearchContext searchContext = new SignificantTermsTestSearchContext();
 
         // test jlh with string
@@ -210,10 +210,10 @@ public class SignificanceHeuristicTests extends ESTestCase {
         assertThat(parseFromString(heuristicParserMapper, searchContext, "\"chi_square\":{\"include_negatives\": " + includeNegatives + ", \"background_is_superset\":" + backgroundIsSuperset + "}"), equalTo((SignificanceHeuristic) (new ChiSquare(includeNegatives, backgroundIsSuperset))));
 
         // test with builders
-        assertTrue(parseFromBuilder(heuristicParserMapper, searchContext, new JLHScore()) instanceof JLHScore);
-        assertTrue(parseFromBuilder(heuristicParserMapper, searchContext, new GND(backgroundIsSuperset)) instanceof GND);
-        assertThat(parseFromBuilder(heuristicParserMapper, searchContext, new MutualInformation(includeNegatives, backgroundIsSuperset)), equalTo((SignificanceHeuristic) new MutualInformation(includeNegatives, backgroundIsSuperset)));
-        assertThat(parseFromBuilder(heuristicParserMapper, searchContext, new ChiSquare(includeNegatives, backgroundIsSuperset)), equalTo((SignificanceHeuristic) new ChiSquare(includeNegatives, backgroundIsSuperset)));
+        assertTrue(parseFromBuilder(heuristicParserMapper, searchContext, new JLHScore.JLHScoreBuilder()) instanceof JLHScore);
+        assertTrue(parseFromBuilder(heuristicParserMapper, searchContext, new GND.GNDBuilder(backgroundIsSuperset)) instanceof GND);
+        assertThat(parseFromBuilder(heuristicParserMapper, searchContext, new MutualInformation.MutualInformationBuilder(includeNegatives, backgroundIsSuperset)), equalTo((SignificanceHeuristic) new MutualInformation(includeNegatives, backgroundIsSuperset)));
+        assertThat(parseFromBuilder(heuristicParserMapper, searchContext, new ChiSquare.ChiSquareBuilder(includeNegatives, backgroundIsSuperset)), equalTo((SignificanceHeuristic) new ChiSquare(includeNegatives, backgroundIsSuperset)));
 
         // test exceptions
         String faultyHeuristicdefinition = "\"mutual_information\":{\"include_negatives\": false, \"some_unknown_field\": false}";
@@ -233,46 +233,34 @@ public class SignificanceHeuristicTests extends ESTestCase {
         checkParseException(heuristicParserMapper, searchContext, faultyHeuristicdefinition, expectedError);
     }
 
-    protected void checkParseException(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext,
-            String faultyHeuristicDefinition, String expectedError) throws IOException {
-
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, new HashMap<String, QueryParser<?>>());
+    protected void checkParseException(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext, String faultyHeuristicDefinition, String expectedError) throws IOException {
         try {
             XContentParser stParser = JsonXContent.jsonXContent.createParser("{\"field\":\"text\", " + faultyHeuristicDefinition + ",\"min_doc_count\":200}");
-            QueryParseContext parseContext = new QueryParseContext(registry);
-            parseContext.reset(stParser);
-            parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
             stParser.nextToken();
-            new SignificantTermsParser(heuristicParserMapper, registry).parse("testagg", stParser, parseContext);
+            new SignificantTermsParser(heuristicParserMapper).parse("testagg", stParser, searchContext);
             fail();
         } catch (ElasticsearchParseException e) {
             assertTrue(e.getMessage().contains(expectedError));
         }
     }
 
-    protected SignificanceHeuristic parseFromBuilder(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext, SignificanceHeuristic significanceHeuristic) throws IOException {
-        SignificantTermsAggregatorBuilder stBuilder = significantTerms("testagg");
-        stBuilder.significanceHeuristic(significanceHeuristic).field("text").minDocCount(200);
+    protected SignificanceHeuristic parseFromBuilder(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext, SignificanceHeuristicBuilder significanceHeuristicBuilder) throws IOException {
+        SignificantTermsBuilder stBuilder = new SignificantTermsBuilder("testagg");
+        stBuilder.significanceHeuristic(significanceHeuristicBuilder).field("text").minDocCount(200);
         XContentBuilder stXContentBuilder = XContentFactory.jsonBuilder();
         stBuilder.internalXContent(stXContentBuilder, null);
         XContentParser stParser = JsonXContent.jsonXContent.createParser(stXContentBuilder.string());
         return parseSignificanceHeuristic(heuristicParserMapper, searchContext, stParser);
     }
 
-    private SignificanceHeuristic parseSignificanceHeuristic(SignificanceHeuristicParserMapper heuristicParserMapper,
-            SearchContext searchContext, XContentParser stParser) throws IOException {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, new HashMap<String, QueryParser<?>>());
-        QueryParseContext parseContext = new QueryParseContext(registry);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
+    private SignificanceHeuristic parseSignificanceHeuristic(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext, XContentParser stParser) throws IOException {
         stParser.nextToken();
-        SignificantTermsAggregatorBuilder aggregatorFactory = (SignificantTermsAggregatorBuilder) new SignificantTermsParser(
-                heuristicParserMapper, registry).parse("testagg", stParser, parseContext);
+        SignificantTermsAggregatorFactory aggregatorFactory = (SignificantTermsAggregatorFactory) new SignificantTermsParser(heuristicParserMapper).parse("testagg", stParser, searchContext);
         stParser.nextToken();
         assertThat(aggregatorFactory.getBucketCountThresholds().getMinDocCount(), equalTo(200L));
         assertThat(stParser.currentToken(), equalTo(null));
         stParser.close();
-        return aggregatorFactory.significanceHeuristic();
+        return aggregatorFactory.getSignificanceHeuristic();
     }
 
     protected SignificanceHeuristic parseFromString(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext, String heuristicString) throws IOException {
@@ -383,14 +371,14 @@ public class SignificanceHeuristicTests extends ESTestCase {
         testBackgroundAssertions(new MutualInformation(true, true), new MutualInformation(true, false));
         testBackgroundAssertions(new ChiSquare(true, true), new ChiSquare(true, false));
         testBackgroundAssertions(new GND(true), new GND(false));
-        testAssertions(PercentageScore.PROTOTYPE);
-        testAssertions(JLHScore.PROTOTYPE);
+        testAssertions(PercentageScore.INSTANCE);
+        testAssertions(JLHScore.INSTANCE);
     }
 
     public void testBasicScoreProperties() {
-        basicScoreProperties(JLHScore.PROTOTYPE, true);
+        basicScoreProperties(JLHScore.INSTANCE, true);
         basicScoreProperties(new GND(true), true);
-        basicScoreProperties(PercentageScore.PROTOTYPE, true);
+        basicScoreProperties(PercentageScore.INSTANCE, true);
         basicScoreProperties(new MutualInformation(true, true), false);
         basicScoreProperties(new ChiSquare(true, true), false);
     }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AbstractNumericMetricTestCase.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AbstractNumericMetricTestCase.java
deleted file mode 100644
index 58d7fa7..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AbstractNumericMetricTestCase.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
-
-public abstract class AbstractNumericMetricTestCase<AF extends ValuesSourceAggregatorBuilder.LeafOnly<ValuesSource.Numeric, AF>>
-        extends BaseAggregationTestCase<AF> {
-
-    @Override
-    protected final AF createTestAggregatorBuilder() {
-        AF factory = doCreateTestAggregatorFactory();
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-    protected abstract AF doCreateTestAggregatorFactory();
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgIT.java
index d7e8736..2ce78e4 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgIT.java
@@ -72,7 +72,7 @@ public class AvgIT extends AbstractNumericTestCase {
 
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(avg("avg").field("value")))
+                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(avg("avg")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgTests.java
deleted file mode 100644
index df1b78c..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.avg.AvgAggregator;
-
-public class AvgTests extends AbstractNumericMetricTestCase<AvgAggregator.AvgAggregatorBuilder> {
-
-    @Override
-    protected AvgAggregator.AvgAggregatorBuilder doCreateTestAggregatorFactory() {
-        return new AvgAggregator.AvgAggregatorBuilder("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ExtendedStatsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ExtendedStatsTests.java
deleted file mode 100644
index 7a9e30b..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ExtendedStatsTests.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStatsAggregator;
-
-public class ExtendedStatsTests extends AbstractNumericMetricTestCase<ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder> {
-
-    @Override
-    protected ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder doCreateTestAggregatorFactory() {
-        ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder factory = new ExtendedStatsAggregator.ExtendedStatsAggregatorBuilder("foo");
-        if (randomBoolean()) {
-            factory.sigma(randomDoubleBetween(0.0, 10.0, true));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FilterTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FilterTests.java
deleted file mode 100644
index 4b0389b..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FilterTests.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator;
-import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator.FilterAggregatorBuilder;
-
-public class FilterTests extends BaseAggregationTestCase<FilterAggregator.FilterAggregatorBuilder> {
-
-    @Override
-    protected FilterAggregatorBuilder createTestAggregatorBuilder() {
-        FilterAggregatorBuilder factory = new FilterAggregatorBuilder(randomAsciiOfLengthBetween(1, 20),
-                QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20)));
-        // NORELEASE make RandomQueryBuilder work outside of the
-        // AbstractQueryTestCase
-        // builder.query(RandomQueryBuilder.createQuery(getRandom()));
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FiltersTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FiltersTests.java
deleted file mode 100644
index 5ab9109..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FiltersTests.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.FiltersAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.KeyedFilter;
-
-public class FiltersTests extends BaseAggregationTestCase<FiltersAggregator.FiltersAggregatorBuilder> {
-
-    @Override
-    protected FiltersAggregatorBuilder createTestAggregatorBuilder() {
-
-        int size = randomIntBetween(1, 20);
-        FiltersAggregatorBuilder factory;
-        if (randomBoolean()) {
-            KeyedFilter[] filters = new KeyedFilter[size];
-            for (int i = 0; i < size; i++) {
-                // NORELEASE make RandomQueryBuilder work outside of the
-                // AbstractQueryTestCase
-                // builder.query(RandomQueryBuilder.createQuery(getRandom()));
-                filters[i] = new KeyedFilter(randomAsciiOfLengthBetween(1, 20),
-                        QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20)));
-            }
-            factory = new FiltersAggregatorBuilder(randomAsciiOfLengthBetween(1, 20), filters);
-        } else {
-            QueryBuilder<?>[] filters = new QueryBuilder<?>[size];
-            for (int i = 0; i < size; i++) {
-                // NORELEASE make RandomQueryBuilder work outside of the
-                // AbstractQueryTestCase
-                // builder.query(RandomQueryBuilder.createQuery(getRandom()));
-                filters[i] = QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20));
-            }
-            factory = new FiltersAggregatorBuilder(randomAsciiOfLengthBetween(1, 20), filters);
-        }
-        if (randomBoolean()) {
-            factory.otherBucket(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.otherBucketKey(randomAsciiOfLengthBetween(1, 20));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoBoundsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoBoundsTests.java
deleted file mode 100644
index 58fe5ca..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoBoundsTests.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsAggregator;
-import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsAggregator.GeoBoundsAggregatorBuilder;
-
-public class GeoBoundsTests extends BaseAggregationTestCase<GeoBoundsAggregator.GeoBoundsAggregatorBuilder> {
-
-    @Override
-    protected GeoBoundsAggregatorBuilder createTestAggregatorBuilder() {
-        GeoBoundsAggregatorBuilder factory = new GeoBoundsAggregatorBuilder(randomAsciiOfLengthBetween(1, 20));
-        String field = randomAsciiOfLengthBetween(3, 20);
-        factory.field(field);
-        if (randomBoolean()) {
-            factory.wrapLongitude(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing("0,0");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidIT.java
index da86991..8c21cbd 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidIT.java
@@ -147,7 +147,7 @@ public class GeoCentroidIT extends AbstractGeoTestCase {
     public void testSingleValueFieldAsSubAggToGeohashGrid() throws Exception {
         SearchResponse response = client().prepareSearch(HIGH_CARD_IDX_NAME)
                 .addAggregation(geohashGrid("geoGrid").field(SINGLE_VALUED_FIELD_NAME)
-                .subAggregation(geoCentroid(aggName).field(SINGLE_VALUED_FIELD_NAME)))
+                .subAggregation(geoCentroid(aggName)))
                 .execute().actionGet();
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidTests.java
deleted file mode 100644
index 7bf2151..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidTests.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator;
-import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator.GeoCentroidAggregatorBuilder;
-
-public class GeoCentroidTests extends BaseAggregationTestCase<GeoCentroidAggregator.GeoCentroidAggregatorBuilder> {
-
-    @Override
-    protected GeoCentroidAggregatorBuilder createTestAggregatorBuilder() {
-        GeoCentroidAggregatorBuilder factory = new GeoCentroidAggregatorBuilder(randomAsciiOfLengthBetween(1, 20));
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("0,0");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MaxTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MaxTests.java
deleted file mode 100644
index 89b5560..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MaxTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.max.MaxAggregator;
-
-public class MaxTests extends AbstractNumericMetricTestCase<MaxAggregator.MaxAggregatorBuilder> {
-
-    @Override
-    protected MaxAggregator.MaxAggregatorBuilder doCreateTestAggregatorFactory() {
-        return new MaxAggregator.MaxAggregatorBuilder("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MinTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MinTests.java
deleted file mode 100644
index 5ee5db1..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MinTests.java
+++ /dev/null
@@ -1,32 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.min.MinAggregator;
-import org.elasticsearch.search.aggregations.metrics.min.MinAggregator.MinAggregatorBuilder;
-
-public class MinTests extends AbstractNumericMetricTestCase<MinAggregator.MinAggregatorBuilder> {
-
-    @Override
-    protected MinAggregatorBuilder doCreateTestAggregatorFactory() {
-        return new MinAggregator.MinAggregatorBuilder("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MissingTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MissingTests.java
deleted file mode 100644
index 846579f..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MissingTests.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.missing.MissingAggregator;
-
-public class MissingTests extends BaseAggregationTestCase<MissingAggregator.MissingAggregatorBuilder> {
-
-    @Override
-    protected final MissingAggregator.MissingAggregatorBuilder createTestAggregatorBuilder() {
-        MissingAggregator.MissingAggregatorBuilder factory = new MissingAggregator.MissingAggregatorBuilder("foo", null);
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentileRanksTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentileRanksTests.java
deleted file mode 100644
index 4636e4e..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentileRanksTests.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksAggregatorBuilder;
-
-public class PercentileRanksTests extends BaseAggregationTestCase<PercentileRanksAggregatorBuilder> {
-
-    @Override
-    protected PercentileRanksAggregatorBuilder createTestAggregatorBuilder() {
-        PercentileRanksAggregatorBuilder factory = new PercentileRanksAggregatorBuilder(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        int valuesSize = randomIntBetween(1, 20);
-        double[] values = new double[valuesSize];
-        for (int i = 0; i < valuesSize; i++) {
-            values[i] = randomDouble() * 100;
-        }
-        factory.values(values);
-        if (randomBoolean()) {
-            factory.numberOfSignificantValueDigits(randomIntBetween(0, 5));
-        }
-        if (randomBoolean()) {
-            factory.compression(randomIntBetween(1, 50000));
-        }
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentilesTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentilesTests.java
deleted file mode 100644
index 674197c..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/PercentilesTests.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesAggregatorBuilder;
-
-public class PercentilesTests extends BaseAggregationTestCase<PercentilesAggregatorBuilder> {
-
-    @Override
-    protected PercentilesAggregatorBuilder createTestAggregatorBuilder() {
-        PercentilesAggregatorBuilder factory = new PercentilesAggregatorBuilder(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            int percentsSize = randomIntBetween(1, 20);
-            double[] percents = new double[percentsSize];
-            for (int i = 0; i < percentsSize; i++) {
-                percents[i] = randomDouble() * 100;
-            }
-            factory.percentiles(percents);
-        }
-        if (randomBoolean()) {
-            factory.numberOfSignificantValueDigits(randomIntBetween(0, 5));
-        }
-        if (randomBoolean()) {
-            factory.compression(randomIntBetween(1, 50000));
-        }
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            factory.format("###.00");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricTests.java
deleted file mode 100644
index 6d4ae48..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricTests.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator;
-import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.ScriptedMetricAggregatorBuilder;
-
-import java.util.HashMap;
-import java.util.Map;
-
-public class ScriptedMetricTests extends BaseAggregationTestCase<ScriptedMetricAggregator.ScriptedMetricAggregatorBuilder> {
-
-    @Override
-    protected ScriptedMetricAggregatorBuilder createTestAggregatorBuilder() {
-        ScriptedMetricAggregatorBuilder factory = new ScriptedMetricAggregatorBuilder(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.initScript(randomScript("initScript"));
-        }
-        factory.mapScript(randomScript("mapScript"));
-        if (randomBoolean()) {
-            factory.combineScript(randomScript("combineScript"));
-        }
-        if (randomBoolean()) {
-            factory.reduceScript(randomScript("reduceScript"));
-        }
-        if (randomBoolean()) {
-            Map<String, Object> params = new HashMap<String, Object>();
-            params.put("foo", "bar");
-            factory.params(params);
-        }
-        return factory;
-    }
-
-    private Script randomScript(String script) {
-        if (randomBoolean()) {
-            return new Script(script);
-        } else {
-            return new Script(script, randomFrom(ScriptType.values()), randomFrom("my_lang", null), null);
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/StatsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/StatsTests.java
deleted file mode 100644
index eaeca9c..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/StatsTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.stats.StatsAggregator;
-
-public class StatsTests extends AbstractNumericMetricTestCase<StatsAggregator.StatsAggregatorBuilder> {
-
-    @Override
-    protected StatsAggregator.StatsAggregatorBuilder doCreateTestAggregatorFactory() {
-        return new StatsAggregator.StatsAggregatorBuilder("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumIT.java
index adafc55..f2c05ee 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumIT.java
@@ -69,7 +69,7 @@ public class SumIT extends AbstractNumericTestCase {
 
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(sum("sum").field("value")))
+                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(sum("sum")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumTests.java
deleted file mode 100644
index 78eba34..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.sum.SumAggregator;
-
-public class SumTests extends AbstractNumericMetricTestCase<SumAggregator.SumAggregatorBuilder> {
-
-    @Override
-    protected SumAggregator.SumAggregatorBuilder doCreateTestAggregatorFactory() {
-        return new SumAggregator.SumAggregatorBuilder("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
index fe4fafc..90a52ed 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
@@ -256,7 +256,7 @@ public class TopHitsIT extends ESIntegTestCase {
                         .executionHint(randomExecutionHint())
                         .field(TERMS_AGGS_FIELD)
                         .subAggregation(
-                                topHits("hits").sort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
+                                topHits("hits").addSort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
                         )
                 )
                 .get();
@@ -352,7 +352,7 @@ public class TopHitsIT extends ESIntegTestCase {
                         .executionHint(randomExecutionHint())
                         .collectMode(SubAggCollectionMode.BREADTH_FIRST)
                         .field(TERMS_AGGS_FIELD)
-                        .subAggregation(topHits("hits").size(3))
+                        .subAggregation(topHits("hits").setSize(3))
                 ).get();
 
         assertSearchResponse(response);
@@ -403,9 +403,9 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .executionHint(randomExecutionHint())
                                 .field(TERMS_AGGS_FIELD)
                                 .subAggregation(
-                                        topHits("hits").sort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
-                                                .from(from)
-                                                .size(size)
+                                        topHits("hits").addSort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
+                                                .setFrom(from)
+                                                .setSize(size)
                                 )
                 )
                 .get();
@@ -447,7 +447,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .field(TERMS_AGGS_FIELD)
                                 .order(Terms.Order.aggregation("max_sort", false))
                                 .subAggregation(
-                                        topHits("hits").sort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC)).trackScores(true)
+                                        topHits("hits").addSort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC)).setTrackScores(true)
                                 )
                                 .subAggregation(
                                         max("max_sort").field(SORT_FIELD)
@@ -487,7 +487,7 @@ public class TopHitsIT extends ESIntegTestCase {
                 .setQuery(matchQuery("text", "term rare"))
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field("group")
-                                .order(Terms.Order.aggregation("max_score", false)).subAggregation(topHits("hits").size(1))
+                                .order(Terms.Order.aggregation("max_score", false)).subAggregation(topHits("hits").setSize(1))
                                 .subAggregation(max("max_score").field("value"))).get();
         assertSearchResponse(response);
 
@@ -529,14 +529,14 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .executionHint(randomExecutionHint())
                                 .field(TERMS_AGGS_FIELD)
                                 .subAggregation(
-                                        topHits("hits").size(1)
+                                        topHits("hits").setSize(1)
                                             .highlighter(new HighlightBuilder().field("text"))
-                                            .explain(true)
-                                            .field("text")
-                                            .fieldDataField("field1")
-                                            .scriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()))
-                                            .fetchSource("text", null)
-                                            .version(true)
+                                            .setExplain(true)
+                                            .addField("text")
+                                            .addFieldDataField("field1")
+                                            .addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()))
+                                            .setFetchSource("text", null)
+                                            .setVersion(true)
                                 )
                 )
                 .get();
@@ -586,7 +586,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                     .executionHint(randomExecutionHint())
                                     .field(TERMS_AGGS_FIELD)
                                     .subAggregation(
-                                            topHits("hits").sort(SortBuilders.fieldSort("xyz").order(SortOrder.DESC))
+                                            topHits("hits").addSort(SortBuilders.fieldSort("xyz").order(SortOrder.DESC))
                                     )
                     ).get();
             fail();
@@ -650,9 +650,9 @@ public class TopHitsIT extends ESIntegTestCase {
                                     .field("group")
                                     .subAggregation(
                                             topHits("hits")
-                                                    .trackScores(trackScore)
-                                                    .size(1)
-                                                    .sort("_id", SortOrder.DESC)
+                                                    .setTrackScores(trackScore)
+                                                    .setSize(1)
+                                                    .addSort("_id", SortOrder.DESC)
                                     )
                     )
                     .get();
@@ -690,12 +690,13 @@ public class TopHitsIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("articles")
                 .setQuery(matchQuery("title", "title"))
                 .addAggregation(
-                        nested("to-comments", "comments")
+                        nested("to-comments")
+                                .path("comments")
                                 .subAggregation(
                                         terms("users")
                                                 .field("comments.user")
                                                 .subAggregation(
-                                                        topHits("top-comments").sort("comments.date", SortOrder.ASC)
+                                                        topHits("top-comments").addSort("comments.date", SortOrder.ASC)
                                                 )
                                 )
                 )
@@ -740,14 +741,15 @@ public class TopHitsIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("articles")
                 .setQuery(matchQuery("title", "title"))
                 .addAggregation(
-                        nested("to-comments", "comments")
+                        nested("to-comments")
+                                .path("comments")
                                 .subAggregation(
-                                    nested("to-reviewers", "comments.reviewers").subAggregation(
+                                    nested("to-reviewers").path("comments.reviewers").subAggregation(
                                             // Also need to sort on _doc because there are two reviewers with the same name
-                                            topHits("top-reviewers").sort("comments.reviewers.name", SortOrder.ASC).sort("_doc", SortOrder.DESC).size(7)
+                                            topHits("top-reviewers").addSort("comments.reviewers.name", SortOrder.ASC).addSort("_doc", SortOrder.DESC).setSize(7)
                                     )
                                 )
-                                .subAggregation(topHits("top-comments").sort("comments.date", SortOrder.DESC).size(4))
+                                .subAggregation(topHits("top-comments").addSort("comments.date", SortOrder.DESC).setSize(4))
                 ).get();
         assertNoFailures(searchResponse);
 
@@ -846,11 +848,11 @@ public class TopHitsIT extends ESIntegTestCase {
                 .prepareSearch("articles")
                 .setQuery(nestedQuery("comments", matchQuery("comments.message", "comment").queryName("test")))
                 .addAggregation(
-                        nested("to-comments", "comments").subAggregation(
-                                topHits("top-comments").size(1).highlighter(new HighlightBuilder().field(hlField)).explain(true)
-                                                .fieldDataField("comments.user")
-                                        .scriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap())).fetchSource("message", null)
-                                        .version(true).sort("comments.date", SortOrder.ASC))).get();
+                        nested("to-comments").path("comments").subAggregation(
+                                topHits("top-comments").setSize(1).highlighter(new HighlightBuilder().field(hlField)).setExplain(true)
+                                                .addFieldDataField("comments.user")
+                                        .addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap())).setFetchSource("message", null)
+                                        .setVersion(true).addSort("comments.date", SortOrder.ASC))).get();
         assertHitCount(searchResponse, 2);
         Nested nested = searchResponse.getAggregations().get("to-comments");
         assertThat(nested.getDocCount(), equalTo(4L));
@@ -895,10 +897,11 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .interval(5)
                                 .order(Histogram.Order.aggregation("to-comments", true))
                                 .subAggregation(
-                                        nested("to-comments", "comments")
+                                        nested("to-comments")
+                                                .path("comments")
                                                 .subAggregation(topHits("comments")
                                                         .highlighter(new HighlightBuilder().field(new HighlightBuilder.Field("comments.message").highlightQuery(matchQuery("comments.message", "text"))))
-                                                        .sort("comments.id", SortOrder.ASC))
+                                                        .addSort("comments.id", SortOrder.ASC))
                                 )
                 )
                 .get();
@@ -935,7 +938,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .executionHint(randomExecutionHint())
                                 .field(TERMS_AGGS_FIELD)
                                 .subAggregation(
-                                        topHits("hits").size(ArrayUtil.MAX_ARRAY_LENGTH - 1).sort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
+                                        topHits("hits").setSize(ArrayUtil.MAX_ARRAY_LENGTH - 1).addSort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
                                 )
                 )
                 .get();
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsTests.java
deleted file mode 100644
index d353aca..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsTests.java
+++ /dev/null
@@ -1,146 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.index.query.AbstractQueryTestCase;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.tophits.TopHitsAggregator;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
-import org.elasticsearch.search.highlight.HighlightBuilderTests;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
-
-import java.util.ArrayList;
-import java.util.List;;
-
-public class TopHitsTests extends BaseAggregationTestCase<TopHitsAggregator.TopHitsAggregatorBuilder> {
-
-    @Override
-    protected final TopHitsAggregator.TopHitsAggregatorBuilder createTestAggregatorBuilder() {
-        TopHitsAggregator.TopHitsAggregatorBuilder factory = new TopHitsAggregator.TopHitsAggregatorBuilder("foo");
-        if (randomBoolean()) {
-            factory.from(randomIntBetween(0, 10000));
-        }
-        if (randomBoolean()) {
-            factory.size(randomIntBetween(0, 10000));
-        }
-        if (randomBoolean()) {
-            factory.explain(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.version(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.trackScores(randomBoolean());
-        }
-        if (randomBoolean()) {
-            int fieldsSize = randomInt(25);
-            List<String> fields = new ArrayList<>(fieldsSize);
-            for (int i = 0; i < fieldsSize; i++) {
-                fields.add(randomAsciiOfLengthBetween(5, 50));
-            }
-            factory.fields(fields);
-        }
-        if (randomBoolean()) {
-            int fieldDataFieldsSize = randomInt(25);
-            for (int i = 0; i < fieldDataFieldsSize; i++) {
-                factory.fieldDataField(randomAsciiOfLengthBetween(5, 50));
-            }
-        }
-        if (randomBoolean()) {
-            int scriptFieldsSize = randomInt(25);
-            for (int i = 0; i < scriptFieldsSize; i++) {
-                if (randomBoolean()) {
-                    factory.scriptField(randomAsciiOfLengthBetween(5, 50), new Script("foo"), randomBoolean());
-                } else {
-                    factory.scriptField(randomAsciiOfLengthBetween(5, 50), new Script("foo"));
-                }
-            }
-        }
-        if (randomBoolean()) {
-            FetchSourceContext fetchSourceContext;
-            int branch = randomInt(5);
-            String[] includes = new String[randomIntBetween(0, 20)];
-            for (int i = 0; i < includes.length; i++) {
-                includes[i] = randomAsciiOfLengthBetween(5, 20);
-            }
-            String[] excludes = new String[randomIntBetween(0, 20)];
-            for (int i = 0; i < excludes.length; i++) {
-                excludes[i] = randomAsciiOfLengthBetween(5, 20);
-            }
-            switch (branch) {
-            case 0:
-                fetchSourceContext = new FetchSourceContext(randomBoolean());
-                break;
-            case 1:
-                fetchSourceContext = new FetchSourceContext(includes, excludes);
-                break;
-            case 2:
-                fetchSourceContext = new FetchSourceContext(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20));
-                break;
-            case 3:
-                fetchSourceContext = new FetchSourceContext(true, includes, excludes);
-                break;
-            case 4:
-                fetchSourceContext = new FetchSourceContext(includes);
-                break;
-            case 5:
-                fetchSourceContext = new FetchSourceContext(randomAsciiOfLengthBetween(5, 20));
-                break;
-            default:
-                throw new IllegalStateException();
-            }
-            factory.fetchSource(fetchSourceContext);
-        }
-        if (randomBoolean()) {
-            int numSorts = randomIntBetween(1, 5);
-            for (int i = 0; i < numSorts; i++) {
-                int branch = randomInt(5);
-                switch (branch) {
-                case 0:
-                    factory.sort(SortBuilders.fieldSort(randomAsciiOfLengthBetween(5, 20)).order(randomFrom(SortOrder.values())));
-                    break;
-                case 1:
-                    factory.sort(SortBuilders.geoDistanceSort(randomAsciiOfLengthBetween(5, 20))
-                            .geohashes(AbstractQueryTestCase.randomGeohash(1, 12)).order(randomFrom(SortOrder.values())));
-                    break;
-                case 2:
-                    factory.sort(SortBuilders.scoreSort().order(randomFrom(SortOrder.values())));
-                    break;
-                case 3:
-                    factory.sort(SortBuilders.scriptSort(new Script("foo"), "number").order(randomFrom(SortOrder.values())));
-                    break;
-                case 4:
-                    factory.sort(randomAsciiOfLengthBetween(5, 20));
-                    break;
-                case 5:
-                    factory.sort(randomAsciiOfLengthBetween(5, 20), randomFrom(SortOrder.values()));
-                    break;
-                }
-            }
-        }
-        if (randomBoolean()) {
-            factory.highlighter(HighlightBuilderTests.randomHighlighterBuilder());
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountTests.java
deleted file mode 100644
index a942aa4..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountTests.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCountAggregator;
-
-public class ValueCountTests extends BaseAggregationTestCase<ValueCountAggregator.ValueCountAggregatorBuilder> {
-
-    @Override
-    protected final ValueCountAggregator.ValueCountAggregatorBuilder createTestAggregatorBuilder() {
-        ValueCountAggregator.ValueCountAggregatorBuilder factory = new ValueCountAggregator.ValueCountAggregatorBuilder("foo", null);
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityTests.java
deleted file mode 100644
index a769a71..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityTests.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.cardinality;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-
-public class CardinalityTests extends BaseAggregationTestCase<CardinalityAggregatorBuilder> {
-
-    @Override
-    protected final CardinalityAggregatorBuilder createTestAggregatorBuilder() {
-        CardinalityAggregatorBuilder factory = new CardinalityAggregatorBuilder("foo", null);
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethodTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethodTests.java
deleted file mode 100644
index eb08e6f..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethodTests.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.percentiles;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class PercentilesMethodTests extends ESTestCase {
-
-    public void testValidOrdinals() {
-        assertThat(PercentilesMethod.TDIGEST.ordinal(), equalTo(0));
-        assertThat(PercentilesMethod.HDR.ordinal(), equalTo(1));
-    }
-
-    public void testwriteTo() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            PercentilesMethod.TDIGEST.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(0));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            PercentilesMethod.HDR.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(1));
-            }
-        }
-    }
-
-    public void testReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(0);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(PercentilesMethod.TDIGEST.readFrom(in), equalTo(PercentilesMethod.TDIGEST));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(1);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(PercentilesMethod.TDIGEST.readFrom(in), equalTo(PercentilesMethod.HDR));
-            }
-        }
-    }
-
-    public void testInvalidReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(randomIntBetween(2, Integer.MAX_VALUE));
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                PercentilesMethod.TDIGEST.readFrom(in);
-                fail("Expected IOException");
-            } catch(IOException e) {
-                assertThat(e.getMessage(), containsString("Unknown PercentilesMethod ordinal ["));
-            }
-
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/AvgBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/AvgBucketIT.java
index 17f06be..47c0c96 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/AvgBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/AvgBucketIT.java
@@ -21,12 +21,10 @@ package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -94,8 +92,8 @@ public class AvgBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(avgBucket("avg_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(avgBucket("avg_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -132,8 +130,8 @@ public class AvgBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .subAggregation(avgBucket("avg_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(avgBucket("avg_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -175,7 +173,7 @@ public class AvgBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(avgBucket("avg_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(avgBucket("avg_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -214,9 +212,9 @@ public class AvgBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .subAggregation(avgBucket("avg_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(avgBucket("avg_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -267,9 +265,9 @@ public class AvgBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .subAggregation(avgBucket("avg_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(avgBucket("avg_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -313,9 +311,8 @@ public class AvgBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(avgBucket("avg_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(avgBucket("avg_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -340,9 +337,9 @@ public class AvgBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .subAggregation(avgBucket("avg_histo_bucket", "histo>_count")))
-                .addAggregation(avgBucket("avg_terms_bucket", "terms>avg_histo_bucket")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(avgBucket("avg_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(avgBucket("avg_terms_bucket").setBucketsPaths("terms>avg_histo_bucket")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptTests.java
deleted file mode 100644
index d9b5496..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptTests.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptPipelineAggregator.Factory;
-
-import java.util.HashMap;
-import java.util.Map;
-
-public class BucketScriptTests extends BasePipelineAggregationTestCase<BucketScriptPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        Map<String, String> bucketsPaths = new HashMap<>();
-        int numBucketPaths = randomIntBetween(1, 10);
-        for (int i = 0; i < numBucketPaths; i++) {
-            bucketsPaths.put(randomAsciiOfLengthBetween(1, 20), randomAsciiOfLengthBetween(1, 40));
-        }
-        Script script;
-        if (randomBoolean()) {
-            script = new Script("script");
-        } else {
-            Map<String, Object> params = null;
-            if (randomBoolean()) {
-                params = new HashMap<String, Object>();
-                params.put("foo", "bar");
-            }
-            script = new Script("script", randomFrom(ScriptType.values()), randomFrom("my_lang", null), params);
-        }
-        Factory factory = new Factory(name, bucketsPaths, script);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorTests.java
deleted file mode 100644
index 3d5f4d1..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorTests.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.having.BucketSelectorPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.having.BucketSelectorPipelineAggregator.Factory;
-
-import java.util.HashMap;
-import java.util.Map;
-
-public class BucketSelectorTests extends BasePipelineAggregationTestCase<BucketSelectorPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        Map<String, String> bucketsPaths = new HashMap<>();
-        int numBucketPaths = randomIntBetween(1, 10);
-        for (int i = 0; i < numBucketPaths; i++) {
-            bucketsPaths.put(randomAsciiOfLengthBetween(1, 20), randomAsciiOfLengthBetween(1, 40));
-        }
-        Script script;
-        if (randomBoolean()) {
-            script = new Script("script");
-        } else {
-            Map<String, Object> params = null;
-            if (randomBoolean()) {
-                params = new HashMap<String, Object>();
-                params.put("foo", "bar");
-            }
-            script = new Script("script", randomFrom(ScriptType.values()), randomFrom("my_lang", null), params);
-        }
-        Factory factory = new Factory(name, bucketsPaths, script);
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        return factory;
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumIT.java
index 906aa3d..6f10e5d 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumIT.java
@@ -21,7 +21,6 @@ package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
@@ -89,8 +88,8 @@ public class CumulativeSumIT extends ESIntegTestCase {
     public void testDocCount() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
-                                .subAggregation(cumulativeSum("cumulative_sum", "_count"))).execute().actionGet();
+                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
+                                .subAggregation(cumulativeSum("cumulative_sum").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -120,9 +119,9 @@ public class CumulativeSumIT extends ESIntegTestCase {
                 .prepareSearch("idx")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(cumulativeSum("cumulative_sum", "sum"))).execute().actionGet();
+                                .subAggregation(cumulativeSum("cumulative_sum").setBucketsPaths("sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -154,7 +153,7 @@ public class CumulativeSumIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(cumulativeSum("cumulative_sum", "sum"))).execute().actionGet();
+                                .subAggregation(cumulativeSum("cumulative_sum").setBucketsPaths("sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumTests.java
deleted file mode 100644
index 0e0aa2d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumTests.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.cumulativesum.CumulativeSumPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.cumulativesum.CumulativeSumPipelineAggregator.Factory;
-
-public class CumulativeSumTests extends BasePipelineAggregationTestCase<CumulativeSumPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String bucketsPath = randomAsciiOfLengthBetween(3, 20);
-        Factory factory = new Factory(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java
index 73ec2c9..f2da1db 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java
@@ -108,8 +108,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -150,8 +150,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count").unit(DateHistogramInterval.DAY))).execute()
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count").unit(DateHistogramInterval.DAY))).execute()
                 .actionGet();
 
         assertSearchResponse(response);
@@ -195,8 +195,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                            .subAggregation(sum("sum").field("value")).subAggregation(derivative("deriv", "sum")))
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("sum")).subAggregation(sum("sum").field("value")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -264,8 +264,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
-                        dateHistogram("histo").field("dates").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                        dateHistogram("histo").field("dates").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -319,8 +319,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx_unmapped")
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -334,8 +334,8 @@ public class DateDerivativeIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx", "idx_unmapped")
                 .addAggregation(
-                        dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.MONTH).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                        dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH).minDocCount(0)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java
index 14e99f0..6a4f548 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java
@@ -19,13 +19,13 @@
 
 package org.elasticsearch.search.aggregations.pipeline;
 
+import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.Bucket;
@@ -172,8 +172,8 @@ public class DerivativeIT extends ESIntegTestCase {
                 .prepareSearch("idx")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .subAggregation(derivative("deriv", "_count"))
-                                .subAggregation(derivative("2nd_deriv", "deriv"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))
+                                .subAggregation(derivative("2nd_deriv").setBucketsPaths("deriv"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -211,8 +211,8 @@ public class DerivativeIT extends ESIntegTestCase {
                 .prepareSearch("idx")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval).minDocCount(0)
-                                .subAggregation(derivative("deriv", "_count").unit("1ms"))
-                                .subAggregation(derivative("2nd_deriv", "deriv").unit("10ms"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count").unit("1ms"))
+                                .subAggregation(derivative("2nd_deriv").setBucketsPaths("deriv").unit("10ms"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -250,7 +250,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(derivative("deriv", "sum"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -295,7 +295,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
                                 .subAggregation(stats("stats").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(derivative("deriv", "stats.sum"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("stats.sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -339,7 +339,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .prepareSearch("idx_unmapped")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -354,7 +354,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .prepareSearch("idx", "idx_unmapped")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -383,7 +383,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
-                                .subAggregation(derivative("deriv", "_count"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count"))).execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx));
 
@@ -411,8 +411,8 @@ public class DerivativeIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
-                                .extendedBounds(new ExtendedBounds(0L, (long) numBuckets_empty_rnd - 1))
-                                .subAggregation(derivative("deriv", "_count").gapPolicy(randomFrom(GapPolicy.values()))))
+                                .extendedBounds(0L, (long) numBuckets_empty_rnd - 1)
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count").gapPolicy(randomFrom(GapPolicy.values()))))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx_rnd));
@@ -441,7 +441,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
-                                .subAggregation(derivative("deriv", "_count").gapPolicy(GapPolicy.INSERT_ZEROS))).execute()
+                                .subAggregation(derivative("deriv").setBucketsPaths("_count").gapPolicy(GapPolicy.INSERT_ZEROS))).execute()
                                 .actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx));
@@ -471,7 +471,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(derivative("deriv", "sum"))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("sum"))).execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx));
 
@@ -512,7 +512,7 @@ public class DerivativeIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(derivative("deriv", "sum").gapPolicy(GapPolicy.INSERT_ZEROS))).execute()
+                                .subAggregation(derivative("deriv").setBucketsPaths("sum").gapPolicy(GapPolicy.INSERT_ZEROS))).execute()
                 .actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx));
@@ -550,9 +550,9 @@ public class DerivativeIT extends ESIntegTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1)
-                                .extendedBounds(new ExtendedBounds(0L, (long) numBuckets_empty_rnd - 1))
+                                .extendedBounds(0L, (long) numBuckets_empty_rnd - 1)
                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME))
-                                .subAggregation(derivative("deriv", "sum").gapPolicy(gapPolicy))).execute().actionGet();
+                                .subAggregation(derivative("deriv").setBucketsPaths("sum").gapPolicy(gapPolicy))).execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(numDocsEmptyIdx_rnd));
 
@@ -594,21 +594,28 @@ public class DerivativeIT extends ESIntegTestCase {
                                     .field(SINGLE_VALUED_FIELD_NAME)
                                     .interval(interval)
                                     .subAggregation(
-                                            filters("filters", QueryBuilders.termQuery("tag", "foo")).subAggregation(
+                                            filters("filters").filter(QueryBuilders.termQuery("tag", "foo")).subAggregation(
                                                     sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                    .subAggregation(derivative("deriv", "filters>get>sum"))).execute().actionGet();
+                                    .subAggregation(derivative("deriv").setBucketsPaths("filters>get>sum"))).execute().actionGet();
             fail("Expected an Exception but didn't get one");
         } catch (Exception e) {
             Throwable cause = ExceptionsHelper.unwrapCause(e);
             if (cause == null) {
                 throw e;
             } else if (cause instanceof SearchPhaseExecutionException) {
-                SearchPhaseExecutionException spee = (SearchPhaseExecutionException) e;
-                Throwable rootCause = spee.getRootCause();
-                if (!(rootCause instanceof IllegalArgumentException)) {
+                ElasticsearchException[] rootCauses = ((SearchPhaseExecutionException) cause).guessRootCauses();
+                // If there is more than one root cause then something
+                // unexpected happened and we should re-throw the original
+                // exception
+                if (rootCauses.length > 1) {
                     throw e;
                 }
-            } else if (!(cause instanceof IllegalArgumentException)) {
+                ElasticsearchException rootCauseWrapper = rootCauses[0];
+                Throwable rootCause = rootCauseWrapper.getCause();
+                if (rootCause == null || !(rootCause instanceof IllegalArgumentException)) {
+                    throw e;
+                }
+            } else {
                 throw e;
             }
         }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeTests.java
deleted file mode 100644
index 74c58f6..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeTests.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativePipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativePipelineAggregator.Factory;
-
-public class DerivativeTests extends BasePipelineAggregationTestCase<DerivativePipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String bucketsPath = randomAsciiOfLengthBetween(3, 20);
-        Factory factory = new Factory(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                factory.unit(String.valueOf(randomInt()));
-            } else {
-                factory.unit(String.valueOf(randomIntBetween(1, 10) + randomFrom("s", "m", "h", "d", "w", "M", "y")));
-            }
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/ExtendedStatsBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/ExtendedStatsBucketIT.java
index f9b8062..3a82b68 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/ExtendedStatsBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/ExtendedStatsBucketIT.java
@@ -19,16 +19,13 @@
 
 package org.elasticsearch.search.aggregations.pipeline;
 
-import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended.ExtendedStatsBucket;
@@ -97,8 +94,8 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(extendedStatsBucket("extended_stats_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -144,8 +141,8 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(extendedStatsBucket("extended_stats_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -196,7 +193,7 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(extendedStatsBucket("extended_stats_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -244,9 +241,9 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(extendedStatsBucket("extended_stats_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -306,9 +303,9 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(extendedStatsBucket("extended_stats_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -361,9 +358,8 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(extendedStatsBucket("extended_stats_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(extendedStatsBucket("extended_stats_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -389,24 +385,13 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
                                     .order(Order.term(true))
                                     .subAggregation(
                                             histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                    .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                    .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                     .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                    .subAggregation(extendedStatsBucket("extended_stats_bucket", "histo>sum")
-                                            .sigma(-1.0))).execute().actionGet();
+                                    .subAggregation(extendedStatsBucket("extended_stats_bucket")
+                                            .setBucketsPaths("histo>sum").sigma(-1.0))).execute().actionGet();
             fail("Illegal sigma was provided but no exception was thrown.");
-        } catch (Exception e) {
-            Throwable cause = ExceptionsHelper.unwrapCause(e);
-            if (cause == null) {
-                throw e;
-            } else if (cause instanceof SearchPhaseExecutionException) {
-                SearchPhaseExecutionException spee = (SearchPhaseExecutionException) e;
-                Throwable rootCause = spee.getRootCause();
-                if (!(rootCause instanceof IllegalArgumentException)) {
-                    throw e;
-                }
-            } else if (!(cause instanceof IllegalArgumentException)) {
-                throw e;
-            }
+        } catch (SearchPhaseExecutionException exception) {
+            // All good
         }
     }
 
@@ -419,9 +404,9 @@ public class ExtendedStatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(extendedStatsBucket("avg_histo_bucket", "histo>_count")))
-                .addAggregation(extendedStatsBucket("avg_terms_bucket", "terms>avg_histo_bucket.avg")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(extendedStatsBucket("avg_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(extendedStatsBucket("avg_terms_bucket").setBucketsPaths("terms>avg_histo_bucket.avg")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketIT.java
index 434c45d..bf8c23a 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketIT.java
@@ -22,12 +22,10 @@ package org.elasticsearch.search.aggregations.pipeline;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.InternalBucketMetricValue;
@@ -98,8 +96,8 @@ public class MaxBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(maxBucket("max_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(maxBucket("max_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -141,8 +139,8 @@ public class MaxBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(maxBucket("max_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(maxBucket("max_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -189,7 +187,7 @@ public class MaxBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(maxBucket("max_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(maxBucket("max_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -233,9 +231,9 @@ public class MaxBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(maxBucket("max_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(maxBucket("max_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -286,12 +284,13 @@ public class MaxBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
-                        filter("filter", termQuery("tag", "tag0"))
+                        filter("filter")
+                                .filter(termQuery("tag", "tag0"))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(maxBucket("max_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(maxBucket("max_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -338,9 +337,9 @@ public class MaxBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(maxBucket("max_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(maxBucket("max_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -388,9 +387,8 @@ public class MaxBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(maxBucket("max_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(maxBucket("max_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -416,9 +414,9 @@ public class MaxBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(maxBucket("max_histo_bucket", "histo>_count")))
-                .addAggregation(maxBucket("max_terms_bucket", "terms>max_histo_bucket")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(maxBucket("max_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(maxBucket("max_terms_bucket").setBucketsPaths("terms>max_histo_bucket")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MinBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MinBucketIT.java
index 55d4cb0..ff865e2 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MinBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MinBucketIT.java
@@ -21,12 +21,10 @@ package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.InternalBucketMetricValue;
@@ -95,8 +93,8 @@ public class MinBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(minBucket("min_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(minBucket("min_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -138,8 +136,8 @@ public class MinBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(minBucket("min_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(minBucket("min_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -186,7 +184,7 @@ public class MinBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(minBucket("min_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(minBucket("min_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -230,9 +228,9 @@ public class MinBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(minBucket("min_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(minBucket("min_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -288,9 +286,9 @@ public class MinBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(minBucket("min_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(minBucket("min_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -338,9 +336,8 @@ public class MinBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(minBucket("min_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(minBucket("min_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -366,9 +363,9 @@ public class MinBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(minBucket("min_histo_bucket", "histo>_count")))
-                .addAggregation(minBucket("min_terms_bucket", "terms>min_histo_bucket")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(minBucket("min_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(minBucket("min_terms_bucket").setBucketsPaths("terms>min_histo_bucket")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PercentilesBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PercentilesBucketIT.java
index 0e256a8..27f12bd 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PercentilesBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PercentilesBucketIT.java
@@ -20,14 +20,11 @@ package org.elasticsearch.search.aggregations.pipeline;
  */
 
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.percentiles.Percentile;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucket;
@@ -54,7 +51,7 @@ import static org.hamcrest.core.IsNull.notNullValue;
 public class PercentilesBucketIT extends ESIntegTestCase {
 
     private static final String SINGLE_VALUED_FIELD_NAME = "l_value";
-    private static final double[] PERCENTS = {1.0, 25.0, 50.0, 75.0, 99.0};
+    private static final Double[] PERCENTS = {1.0, 25.0, 50.0, 75.0, 99.0};
     static int numDocs;
     static int interval;
     static int minRandomValue;
@@ -99,8 +96,9 @@ public class PercentilesBucketIT extends ESIntegTestCase {
     public void testDocCountopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(percentilesBucket("percentiles_bucket", "histo>_count")
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(percentilesBucket("percentiles_bucket")
+                        .setBucketsPaths("histo>_count")
                         .percents(PERCENTS)).execute().actionGet();
 
         assertSearchResponse(response);
@@ -141,8 +139,9 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(percentilesBucket("percentiles_bucket", "histo>_count")
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(percentilesBucket("percentiles_bucket")
+                                        .setBucketsPaths("histo>_count")
                                         .percents(PERCENTS))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -187,7 +186,8 @@ public class PercentilesBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(percentilesBucket("percentiles_bucket", "terms>sum")
+                .addAggregation(percentilesBucket("percentiles_bucket")
+                        .setBucketsPaths("terms>sum")
                         .percents(PERCENTS)).execute().actionGet();
 
         assertSearchResponse(response);
@@ -224,7 +224,8 @@ public class PercentilesBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(percentilesBucket("percentiles_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(percentilesBucket("percentiles_bucket")
+                        .setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -266,9 +267,10 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(percentilesBucket("percentiles_bucket", "histo>sum")
+                                .subAggregation(percentilesBucket("percentiles_bucket")
+                                        .setBucketsPaths("histo>sum")
                                         .percents(PERCENTS))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -322,9 +324,10 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(percentilesBucket("percentiles_bucket", "histo>sum")
+                                .subAggregation(percentilesBucket("percentiles_bucket")
+                                        .setBucketsPaths("histo>sum")
                                         .gapPolicy(BucketHelpers.GapPolicy.INSERT_ZEROS)
                                         .percents(PERCENTS)))
                 .execute().actionGet();
@@ -372,9 +375,9 @@ public class PercentilesBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(percentilesBucket("percentiles_bucket", "terms>sum")
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(percentilesBucket("percentiles_bucket")
+                        .setBucketsPaths("terms>sum")
                         .percents(PERCENTS)).execute().actionGet();
 
         assertSearchResponse(response);
@@ -395,9 +398,9 @@ public class PercentilesBucketIT extends ESIntegTestCase {
 
     public void testWrongPercents() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(percentilesBucket("percentiles_bucket", "terms>sum")
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(percentilesBucket("percentiles_bucket")
+                        .setBucketsPaths("terms>sum")
                         .percents(PERCENTS)).execute().actionGet();
 
         assertSearchResponse(response);
@@ -421,34 +424,27 @@ public class PercentilesBucketIT extends ESIntegTestCase {
     }
 
     public void testBadPercents() throws Exception {
-        double[] badPercents = {-1.0, 110.0};
+        Double[] badPercents = {-1.0, 110.0};
 
         try {
             client().prepareSearch("idx")
                     .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                    .addAggregation(percentilesBucket("percentiles_bucket", "terms>sum")
+                    .addAggregation(percentilesBucket("percentiles_bucket")
+                            .setBucketsPaths("terms>sum")
                             .percents(badPercents)).execute().actionGet();
 
             fail("Illegal percent's were provided but no exception was thrown.");
-        } catch (Exception e) {
-            Throwable cause = ExceptionsHelper.unwrapCause(e);
-            if (cause == null) {
-                throw e;
-            } else if (cause instanceof SearchPhaseExecutionException) {
-                SearchPhaseExecutionException spee = (SearchPhaseExecutionException) e;
-                Throwable rootCause = spee.getRootCause();
-                if (!(rootCause instanceof IllegalArgumentException)) {
-                    throw e;
-                }
-            } else if (!(cause instanceof IllegalArgumentException)) {
-                throw e;
-            }
+        } catch (SearchPhaseExecutionException exception) {
+            ElasticsearchException[] rootCauses = exception.guessRootCauses();
+            assertThat(rootCauses.length, equalTo(1));
+            ElasticsearchException rootCause = rootCauses[0];
+            assertThat(rootCause.getMessage(), containsString("must only contain non-null doubles from 0.0-100.0 inclusive"));
         }
 
     }
 
     public void testBadPercents_asSubAgg() throws Exception {
-        double[] badPercents = {-1.0, 110.0};
+        Double[] badPercents = {-1.0, 110.0};
 
         try {
             client()
@@ -459,24 +455,17 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(percentilesBucket("percentiles_bucket", "histo>_count")
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(percentilesBucket("percentiles_bucket")
+                                        .setBucketsPaths("histo>_count")
                                         .percents(badPercents))).execute().actionGet();
 
             fail("Illegal percent's were provided but no exception was thrown.");
-        } catch (Exception e) {
-            Throwable cause = ExceptionsHelper.unwrapCause(e);
-            if (cause == null) {
-                throw e;
-            } else if (cause instanceof SearchPhaseExecutionException) {
-                SearchPhaseExecutionException spee = (SearchPhaseExecutionException) e;
-                Throwable rootCause = spee.getRootCause();
-                if (!(rootCause instanceof IllegalArgumentException)) {
-                    throw e;
-                }
-            } else if (!(cause instanceof IllegalArgumentException)) {
-                throw e;
-            }
+        } catch (SearchPhaseExecutionException exception) {
+            ElasticsearchException[] rootCauses = exception.guessRootCauses();
+            assertThat(rootCauses.length, equalTo(1));
+            ElasticsearchException rootCause = rootCauses[0];
+            assertThat(rootCause.getMessage(), containsString("must only contain non-null doubles from 0.0-100.0 inclusive"));
         }
 
     }
@@ -490,9 +479,10 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(percentilesBucket("percentile_histo_bucket", "histo>_count")))
-                .addAggregation(percentilesBucket("percentile_terms_bucket", "terms>percentile_histo_bucket.50")
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(percentilesBucket("percentile_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(percentilesBucket("percentile_terms_bucket")
+                        .setBucketsPaths("terms>percentile_histo_bucket.50")
                         .percents(PERCENTS)).execute().actionGet();
 
         assertSearchResponse(response);
@@ -546,7 +536,7 @@ public class PercentilesBucketIT extends ESIntegTestCase {
     }
 
     public void testNestedWithDecimal() throws Exception {
-        double[] percent = {99.9};
+        Double[] percent = {99.9};
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
@@ -555,10 +545,12 @@ public class PercentilesBucketIT extends ESIntegTestCase {
                                 .order(Terms.Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(percentilesBucket("percentile_histo_bucket", "histo>_count")
-                                        .percents(percent)))
-                .addAggregation(percentilesBucket("percentile_terms_bucket", "terms>percentile_histo_bucket[99.9]")
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(percentilesBucket("percentile_histo_bucket")
+                                        .percents(percent)
+                                        .setBucketsPaths("histo>_count")))
+                .addAggregation(percentilesBucket("percentile_terms_bucket")
+                        .setBucketsPaths("terms>percentile_histo_bucket[99.9]")
                         .percents(percent)).execute().actionGet();
 
         assertSearchResponse(response);
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
index 1739b2b..e962e90 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
@@ -20,11 +20,11 @@
 package org.elasticsearch.search.aggregations.pipeline;
 
 
-import org.elasticsearch.search.aggregations.metrics.avg.AvgAggregator;
-import org.elasticsearch.search.aggregations.metrics.max.MaxAggregator;
-import org.elasticsearch.search.aggregations.metrics.min.MinAggregator;
-import org.elasticsearch.search.aggregations.metrics.sum.SumAggregator;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+import org.elasticsearch.search.aggregations.metrics.avg.AvgBuilder;
+import org.elasticsearch.search.aggregations.metrics.max.MaxBuilder;
+import org.elasticsearch.search.aggregations.metrics.min.MinBuilder;
+import org.elasticsearch.search.aggregations.metrics.sum.SumBuilder;
 import org.elasticsearch.test.ESTestCase;
 
 import java.util.ArrayList;
@@ -109,27 +109,27 @@ public class PipelineAggregationHelperTests extends ESTestCase {
      * @param values Array of values to compute metric for
      * @param metric A metric builder which defines what kind of metric should be returned for the values
      */
-    public static double calculateMetric(double[] values, ValuesSourceAggregatorBuilder<?, ?> metric) {
+    public static double calculateMetric(double[] values, ValuesSourceMetricsAggregationBuilder metric) {
 
-        if (metric instanceof MinAggregator.MinAggregatorBuilder) {
+        if (metric instanceof MinBuilder) {
             double accumulator = Double.POSITIVE_INFINITY;
             for (double value : values) {
                 accumulator = Math.min(accumulator, value);
             }
             return accumulator;
-        } else if (metric instanceof MaxAggregator.MaxAggregatorBuilder) {
+        } else if (metric instanceof MaxBuilder) {
             double accumulator = Double.NEGATIVE_INFINITY;
             for (double value : values) {
                 accumulator = Math.max(accumulator, value);
             }
             return accumulator;
-        } else if (metric instanceof SumAggregator.SumAggregatorBuilder) {
+        } else if (metric instanceof SumBuilder) {
             double accumulator = 0;
             for (double value : values) {
                 accumulator += value;
             }
             return accumulator;
-        } else if (metric instanceof AvgAggregator.AvgAggregatorBuilder) {
+        } else if (metric instanceof AvgBuilder) {
             double accumulator = 0;
             for (double value : values) {
                 accumulator += value;
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SerialDifferenceTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SerialDifferenceTests.java
deleted file mode 100644
index 2d0306c..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SerialDifferenceTests.java
+++ /dev/null
@@ -1,46 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.serialdiff.SerialDiffPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.serialdiff.SerialDiffPipelineAggregator.Factory;
-
-public class SerialDifferenceTests extends BasePipelineAggregationTestCase<SerialDiffPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String bucketsPath = randomAsciiOfLengthBetween(3, 20);
-        Factory factory = new Factory(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        if (randomBoolean()) {
-            factory.lag(randomIntBetween(1, 1000));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/StatsBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/StatsBucketIT.java
index a16a430..949350c 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/StatsBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/StatsBucketIT.java
@@ -21,12 +21,10 @@ package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.StatsBucket;
@@ -95,8 +93,8 @@ public class StatsBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(statsBucket("stats_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(statsBucket("stats_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -139,8 +137,8 @@ public class StatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(statsBucket("stats_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(statsBucket("stats_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -188,7 +186,7 @@ public class StatsBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(statsBucket("stats_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(statsBucket("stats_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -233,9 +231,9 @@ public class StatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(statsBucket("stats_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(statsBucket("stats_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -292,9 +290,9 @@ public class StatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(statsBucket("stats_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(statsBucket("stats_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -344,9 +342,8 @@ public class StatsBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(statsBucket("stats_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(statsBucket("stats_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -371,9 +368,9 @@ public class StatsBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(statsBucket("avg_histo_bucket", "histo>_count")))
-                .addAggregation(statsBucket("avg_terms_bucket", "terms>avg_histo_bucket.avg")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(statsBucket("avg_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(statsBucket("avg_terms_bucket").setBucketsPaths("terms>avg_histo_bucket.avg")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SumBucketIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SumBucketIT.java
index e372f66..0d9a324 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SumBucketIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SumBucketIT.java
@@ -21,12 +21,10 @@ package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -94,8 +92,8 @@ public class SumBucketIT extends ESIntegTestCase {
     public void testDocCountTopLevel() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                        .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                .addAggregation(sumBucket("sum_bucket", "histo>_count")).execute().actionGet();
+                        .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                .addAggregation(sumBucket("sum_bucket").setBucketsPaths("histo>_count")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -129,8 +127,8 @@ public class SumBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(sumBucket("sum_bucket", "histo>_count"))).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(sumBucket("sum_bucket").setBucketsPaths("histo>_count"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -169,7 +167,7 @@ public class SumBucketIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(terms("terms").field("tag").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(sumBucket("sum_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(sumBucket("sum_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -205,9 +203,9 @@ public class SumBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(sumBucket("sum_bucket", "histo>sum"))).execute().actionGet();
+                                .subAggregation(sumBucket("sum_bucket").setBucketsPaths("histo>sum"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -255,9 +253,9 @@ public class SumBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue)
                                                 .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                                .subAggregation(sumBucket("sum_bucket", "histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(sumBucket("sum_bucket").setBucketsPaths("histo>sum").gapPolicy(GapPolicy.INSERT_ZEROS)))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -298,9 +296,8 @@ public class SumBucketIT extends ESIntegTestCase {
 
     public void testNoBuckets() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(terms("terms").field("tag").includeExclude(new IncludeExclude(null, "tag.*"))
-                        .subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
-                .addAggregation(sumBucket("sum_bucket", "terms>sum")).execute().actionGet();
+                .addAggregation(terms("terms").field("tag").exclude("tag.*").subAggregation(sum("sum").field(SINGLE_VALUED_FIELD_NAME)))
+                .addAggregation(sumBucket("sum_bucket").setBucketsPaths("terms>sum")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -325,9 +322,9 @@ public class SumBucketIT extends ESIntegTestCase {
                                 .order(Order.term(true))
                                 .subAggregation(
                                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                                .extendedBounds(new ExtendedBounds((long) minRandomValue, (long) maxRandomValue)))
-                                .subAggregation(sumBucket("sum_histo_bucket", "histo>_count")))
-                .addAggregation(sumBucket("sum_terms_bucket", "terms>sum_histo_bucket")).execute().actionGet();
+                                                .extendedBounds((long) minRandomValue, (long) maxRandomValue))
+                                .subAggregation(sumBucket("sum_histo_bucket").setBucketsPaths("histo>_count")))
+                .addAggregation(sumBucket("sum_terms_bucket").setBucketsPaths("terms>sum_histo_bucket")).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AbstractBucketMetricsTestCase.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AbstractBucketMetricsTestCase.java
deleted file mode 100644
index 5da6238..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AbstractBucketMetricsTestCase.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-
-public abstract class AbstractBucketMetricsTestCase<PAF extends BucketMetricsFactory> extends BasePipelineAggregationTestCase<PAF> {
-
-    @Override
-    protected final PAF createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String bucketsPath = randomAsciiOfLengthBetween(3, 20);
-        PAF factory = doCreateTestAggregatorFactory(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        return factory;
-    }
-
-    protected abstract PAF doCreateTestAggregatorFactory(String name, String bucketsPath);
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AvgBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AvgBucketTests.java
deleted file mode 100644
index 112556d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AvgBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg.AvgBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg.AvgBucketPipelineAggregator.Factory;
-
-public class AvgBucketTests extends AbstractBucketMetricsTestCase<AvgBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        return new Factory(name, bucketsPath);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/ExtendedStatsBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/ExtendedStatsBucketTests.java
deleted file mode 100644
index e467273..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/ExtendedStatsBucketTests.java
+++ /dev/null
@@ -1,37 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended.ExtendedStatsBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended.ExtendedStatsBucketPipelineAggregator.Factory;
-
-public class ExtendedStatsBucketTests extends AbstractBucketMetricsTestCase<ExtendedStatsBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        Factory factory = new Factory(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.sigma(randomDoubleBetween(0.0, 10.0, false));
-        }
-        return factory;
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MaxBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MaxBucketTests.java
deleted file mode 100644
index dd35fbb..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MaxBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max.MaxBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max.MaxBucketPipelineAggregator.Factory;
-
-public class MaxBucketTests extends AbstractBucketMetricsTestCase<MaxBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        return new Factory(name, bucketsPath);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MinBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MinBucketTests.java
deleted file mode 100644
index 745e061..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MinBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketPipelineAggregator.Factory;
-
-public class MinBucketTests extends AbstractBucketMetricsTestCase<MinBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        return new Factory(name, bucketsPath);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/PercentilesBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/PercentilesBucketTests.java
deleted file mode 100644
index 200cd26..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/PercentilesBucketTests.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucketPipelineAggregator.Factory;
-
-public class PercentilesBucketTests extends AbstractBucketMetricsTestCase<PercentilesBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        Factory factory = new Factory(name, bucketsPath);
-        if (randomBoolean()) {
-            int numPercents = randomIntBetween(1, 20);
-            double[] percents = new double[numPercents];
-            for (int i = 0; i < numPercents; i++) {
-                percents[i] = randomDoubleBetween(0.0, 100.0, false);
-            }
-            factory.percents(percents);
-        }
-        return factory;
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/StatsBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/StatsBucketTests.java
deleted file mode 100644
index 22a0774..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/StatsBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.StatsBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.StatsBucketPipelineAggregator.Factory;
-
-public class StatsBucketTests extends AbstractBucketMetricsTestCase<StatsBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        return new Factory(name, bucketsPath);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/SumBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/SumBucketTests.java
deleted file mode 100644
index 02448cd..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/SumBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum.SumBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum.SumBucketPipelineAggregator.Factory;
-
-public class SumBucketTests extends AbstractBucketMetricsTestCase<SumBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String bucketsPath) {
-        return new Factory(name, bucketsPath);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
index 8ae7bd1..3dd578a 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
@@ -23,10 +23,10 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.collect.EvictingQueue;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.Bucket;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregationHelperTests;
@@ -38,8 +38,6 @@ import org.elasticsearch.search.aggregations.pipeline.movavg.models.HoltWintersM
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.LinearModel;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelBuilder;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.SimpleModel;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
 
@@ -79,7 +77,7 @@ public class MovAvgIT extends ESIntegTestCase {
     static int period;
     static HoltWintersModel.SeasonalityType seasonalityType;
     static BucketHelpers.GapPolicy gapPolicy;
-    static ValuesSourceAggregatorBuilder<? extends ValuesSource, ? extends ValuesSourceAggregatorBuilder<?, ?>> metric;
+    static ValuesSourceMetricsAggregationBuilder metric;
     static List<PipelineAggregationHelperTests.MockBucket> mockHisto;
 
     static Map<String, ArrayList<Double>> testValues;
@@ -410,16 +408,18 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts","_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                        .gapPolicy(gapPolicy))
-                                .subAggregation(movingAvg("movavg_values","the_metric")
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -458,16 +458,18 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new LinearModel.LinearModelBuilder())
-                                        .gapPolicy(gapPolicy))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new LinearModel.LinearModelBuilder())
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -506,16 +508,18 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new EwmaModel.EWMAModelBuilder().alpha(alpha))
-                                        .gapPolicy(gapPolicy))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new EwmaModel.EWMAModelBuilder().alpha(alpha))
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -554,16 +558,18 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new HoltLinearModel.HoltLinearModelBuilder().alpha(alpha).beta(beta))
-                                        .gapPolicy(gapPolicy))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new HoltLinearModel.HoltLinearModelBuilder().alpha(alpha).beta(beta))
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -602,20 +608,22 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                 .alpha(alpha).beta(beta).gamma(gamma).period(period).seasonalityType(seasonalityType))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(false))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .minimize(false)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                 .alpha(alpha).beta(beta).gamma(gamma).period(period).seasonalityType(seasonalityType))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(false))
+                                        .minimize(false)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -660,8 +668,8 @@ public class MovAvgIT extends ESIntegTestCase {
                                 .interval(1)
                                 .subAggregation(avg("avg").field(VALUE_FIELD))
                                 .subAggregation(
-                                        movingAvg("movavg_values", "avg").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                                .gapPolicy(gapPolicy).predict(5))).execute().actionGet();
+                                        movingAvg("movavg_values").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
+                                                .gapPolicy(gapPolicy).predict(5).setBucketsPaths("avg"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -706,12 +714,13 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                    .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(0)
                                             .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                            .gapPolicy(gapPolicy))
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("the_metric"))
                     ).execute().actionGet();
             fail("MovingAvg should not accept a window that is zero");
         } catch (SearchPhaseExecutionException e) {
@@ -726,10 +735,11 @@ public class MovAvgIT extends ESIntegTestCase {
                     .addAggregation(
                             range("histo").field(INTERVAL_FIELD).addRange(0, 10)
                                     .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                    .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(0)
                                             .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                            .gapPolicy(gapPolicy))
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("the_metric"))
                     ).execute().actionGet();
             fail("MovingAvg should not accept non-histogram as parent");
 
@@ -744,12 +754,13 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                    .subAggregation(movingAvg("movavg_counts", "_count")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(-10)
                                             .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                            .gapPolicy(gapPolicy))
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("_count"))
                     ).execute().actionGet();
             fail("MovingAvg should not accept a window that is negative");
 
@@ -767,10 +778,11 @@ public class MovAvgIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field("test").interval(interval)
                                 .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -789,10 +801,11 @@ public class MovAvgIT extends ESIntegTestCase {
                 .addAggregation(
                         histogram("histo").field("test").interval(interval)
                                 .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new SimpleModel.SimpleModelBuilder())
                                         .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric")
                                         .predict(numPredictions))
                 ).execute().actionGet();
 
@@ -811,17 +824,18 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                    .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(windowSize)
                                             .modelBuilder(randomModelBuilder())
                                             .gapPolicy(gapPolicy)
-                                            .predict(0))
+                                            .predict(0)
+                                            .setBucketsPaths("the_metric"))
                     ).execute().actionGet();
             fail("MovingAvg should not accept a prediction size that is zero");
 
-        } catch (IllegalArgumentException exception) {
+        } catch (SearchPhaseExecutionException exception) {
             // All Good
         }
     }
@@ -832,17 +846,18 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(randomMetric("the_metric", VALUE_FIELD))
-                                    .subAggregation(movingAvg("movavg_counts", "the_metric")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(windowSize)
                                             .modelBuilder(randomModelBuilder())
                                             .gapPolicy(gapPolicy)
-                                            .predict(-10))
+                                            .predict(-10)
+                                            .setBucketsPaths("the_metric"))
                     ).execute().actionGet();
             fail("MovingAvg should not accept a prediction size that is negative");
 
-        } catch (IllegalArgumentException exception) {
+        } catch (SearchPhaseExecutionException exception) {
             // All Good
         }
     }
@@ -853,18 +868,20 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(metric)
-                                    .subAggregation(movingAvg("movavg_counts", "_count")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(10)
                                             .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                     .alpha(alpha).beta(beta).gamma(gamma).period(20).seasonalityType(seasonalityType))
-                                            .gapPolicy(gapPolicy))
-                                    .subAggregation(movingAvg("movavg_values", "the_metric")
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("_count"))
+                                    .subAggregation(movingAvg("movavg_values")
                                             .window(windowSize)
                                             .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                     .alpha(alpha).beta(beta).gamma(gamma).period(20).seasonalityType(seasonalityType))
-                                            .gapPolicy(gapPolicy))
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("the_metric"))
                     ).execute().actionGet();
         } catch (SearchPhaseExecutionException e) {
             // All good
@@ -881,13 +898,14 @@ public class MovAvgIT extends ESIntegTestCase {
                                 .field(INTERVAL_FIELD)
                                 .interval(1)
                                 .subAggregation(avg("avg").field(VALUE_FIELD))
-                                .subAggregation(derivative("deriv", "avg").gapPolicy(gapPolicy))
+                                .subAggregation(derivative("deriv")
+                                        .setBucketsPaths("avg").gapPolicy(gapPolicy))
                                 .subAggregation(
-                                        movingAvg("avg_movavg", "avg").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                                .gapPolicy(gapPolicy).predict(12))
+                                        movingAvg("avg_movavg").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
+                                                .gapPolicy(gapPolicy).predict(12).setBucketsPaths("avg"))
                                 .subAggregation(
-                                        movingAvg("deriv_movavg", "deriv").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
-                                                .gapPolicy(gapPolicy).predict(12))
+                                        movingAvg("deriv_movavg").window(windowSize).modelBuilder(new SimpleModel.SimpleModelBuilder())
+                                                .gapPolicy(gapPolicy).predict(12).setBucketsPaths("deriv"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -989,12 +1007,13 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(metric)
-                                    .subAggregation(movingAvg("movavg_counts", "_count")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(10)
                                             .modelBuilder(randomModelBuilder(100))
-                                            .gapPolicy(gapPolicy))
+                                            .gapPolicy(gapPolicy)
+                                            .setBucketsPaths("_count"))
                     ).execute().actionGet();
         } catch (SearchPhaseExecutionException e) {
             // All good
@@ -1007,20 +1026,22 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(windowSize)
                                         .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                 .period(period).seasonalityType(seasonalityType))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(true))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .minimize(true)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(windowSize)
                                         .modelBuilder(new HoltWintersModel.HoltWintersModelBuilder()
                                                 .period(period).seasonalityType(seasonalityType))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(true))
+                                        .minimize(true)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -1093,18 +1114,20 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(numBuckets)
                                         .modelBuilder(new HoltLinearModel.HoltLinearModelBuilder().alpha(alpha).beta(beta))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(true))
-                                .subAggregation(movingAvg("movavg_values", "the_metric")
+                                        .minimize(true)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(movingAvg("movavg_values")
                                         .window(numBuckets)
                                         .modelBuilder(new HoltLinearModel.HoltLinearModelBuilder().alpha(alpha).beta(beta))
                                         .gapPolicy(gapPolicy)
-                                        .minimize(true))
+                                        .minimize(true)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -1147,13 +1170,14 @@ public class MovAvgIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(movingAvg("movavg_counts", "_count")
+                                .subAggregation(movingAvg("movavg_counts")
                                         .window(numBuckets)
                                         .modelBuilder(new SimpleModel.SimpleModelBuilder())
                                         .gapPolicy(gapPolicy)
-                                        .minimize(true))
+                                        .minimize(true)
+                                        .setBucketsPaths("_count"))
                 ).execute().actionGet();
             fail("Simple Model cannot be minimized, but an exception was not thrown");
         } catch (SearchPhaseExecutionException e) {
@@ -1165,13 +1189,14 @@ public class MovAvgIT extends ESIntegTestCase {
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                    .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                    .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                     .subAggregation(metric)
-                                    .subAggregation(movingAvg("movavg_counts", "_count")
+                                    .subAggregation(movingAvg("movavg_counts")
                                             .window(numBuckets)
                                             .modelBuilder(new LinearModel.LinearModelBuilder())
                                             .gapPolicy(gapPolicy)
-                                            .minimize(true))
+                                            .minimize(true)
+                                            .setBucketsPaths("_count"))
                     ).execute().actionGet();
             fail("Linear Model cannot be minimized, but an exception was not thrown");
         } catch (SearchPhaseExecutionException e) {
@@ -1195,13 +1220,14 @@ public class MovAvgIT extends ESIntegTestCase {
                         .prepareSearch("idx").setTypes("type")
                         .addAggregation(
                                 histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                        .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                        .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                         .subAggregation(metric)
-                                        .subAggregation(movingAvg("movavg_counts", "_count")
+                                        .subAggregation(movingAvg("movavg_counts")
                                                 .window(numBuckets)
                                                 .modelBuilder(builder)
                                                 .gapPolicy(gapPolicy)
-                                                .minimize(true))
+                                                .minimize(true)
+                                                .setBucketsPaths("_count"))
                         ).execute().actionGet();
             } catch (SearchPhaseExecutionException e) {
                 fail("Model [" + builder.toString() + "] can be minimized, but an exception was thrown");
@@ -1209,6 +1235,43 @@ public class MovAvgIT extends ESIntegTestCase {
         }
     }
 
+    public void testUnrecognizedParams() {
+        MovAvgModelBuilder[] builders = new MovAvgModelBuilder[]{
+                new SimpleModel.SimpleModelBuilder(),
+                new LinearModel.LinearModelBuilder(),
+                new EwmaModel.EWMAModelBuilder(),
+                new HoltLinearModel.HoltLinearModelBuilder(),
+                new HoltWintersModel.HoltWintersModelBuilder()
+        };
+        Map<String, Object> badSettings = new HashMap<>(1);
+        badSettings.put("abc", 1.2);
+
+        for (MovAvgModelBuilder builder : builders) {
+            try {
+                SearchResponse response = client()
+                        .prepareSearch("idx").setTypes("type")
+                        .addAggregation(
+                                histogram("histo").field(INTERVAL_FIELD).interval(interval)
+                                        .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
+                                        .subAggregation(metric)
+                                        .subAggregation(movingAvg("movavg_counts")
+                                                .window(10)
+                                                .modelBuilder(builder)
+                                                .gapPolicy(gapPolicy)
+                                                .settings(badSettings)
+                                                .setBucketsPaths("_count"))
+                        ).execute().actionGet();
+            } catch (SearchPhaseExecutionException e) {
+                // All good
+            }
+        }
+
+
+
+
+    }
+
+
     private void assertValidIterators(Iterator expectedBucketIter, Iterator expectedCountsIter, Iterator expectedValuesIter) {
         if (!expectedBucketIter.hasNext()) {
             fail("`expectedBucketIter` iterator ended before `actual` iterator, size mismatch");
@@ -1292,8 +1355,7 @@ public class MovAvgIT extends ESIntegTestCase {
         }
     }
 
-    private ValuesSourceAggregatorBuilder<? extends ValuesSource, ? extends ValuesSourceAggregatorBuilder<?, ?>> randomMetric(String name,
-            String field) {
+    private ValuesSourceMetricsAggregationBuilder randomMetric(String name, String field) {
         int rand = randomIntBetween(0,3);
 
         switch (rand) {
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java
deleted file mode 100644
index c4bf325..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java
+++ /dev/null
@@ -1,96 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.moving.avg;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.movavg.MovAvgPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.movavg.MovAvgPipelineAggregator.Factory;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.EwmaModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.HoltLinearModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.HoltWintersModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.HoltWintersModel.SeasonalityType;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.LinearModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.SimpleModel;;
-
-public class MovAvgTests extends BasePipelineAggregationTestCase<MovAvgPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String bucketsPath = randomAsciiOfLengthBetween(3, 20);
-        Factory factory = new Factory(name, bucketsPath);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        if (randomBoolean()) {
-            switch (randomInt(4)) {
-            case 0:
-                factory.modelBuilder(new SimpleModel.SimpleModelBuilder());
-                factory.window(randomIntBetween(1, 100));
-                break;
-            case 1:
-                factory.modelBuilder(new LinearModel.LinearModelBuilder());
-                factory.window(randomIntBetween(1, 100));
-                break;
-            case 2:
-                if (randomBoolean()) {
-                    factory.modelBuilder(new EwmaModel.EWMAModelBuilder());
-                    factory.window(randomIntBetween(1, 100));
-                } else {
-                    factory.modelBuilder(new EwmaModel.EWMAModelBuilder().alpha(randomDouble()));
-                    factory.window(randomIntBetween(1, 100));
-                }
-                break;
-            case 3:
-                if (randomBoolean()) {
-                    factory.modelBuilder(new HoltLinearModel.HoltLinearModelBuilder());
-                    factory.window(randomIntBetween(1, 100));
-                } else {
-                    factory.modelBuilder(new HoltLinearModel.HoltLinearModelBuilder().alpha(randomDouble()).beta(randomDouble()));
-                    factory.window(randomIntBetween(1, 100));
-                }
-                break;
-            case 4:
-            default:
-                if (randomBoolean()) {
-                    factory.modelBuilder(new HoltWintersModel.HoltWintersModelBuilder());
-                    factory.window(randomIntBetween(2, 100));
-                } else {
-                    int period = randomIntBetween(1, 100);
-                    factory.modelBuilder(
-                            new HoltWintersModel.HoltWintersModelBuilder().alpha(randomDouble()).beta(randomDouble()).gamma(randomDouble())
-                                    .period(period).seasonalityType(randomFrom(SeasonalityType.values())).pad(randomBoolean()));
-                    factory.window(randomIntBetween(2 * period, 200 * period));
-                }
-                break;
-            }
-        }
-        factory.predict(randomIntBetween(1, 50));
-        if (factory.model().canBeMinimized() && randomBoolean()) {
-            factory.minimize(randomBoolean());
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
index d3b5c80..aebd6a7 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
@@ -23,14 +23,12 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.collect.EvictingQueue;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregationHelperTests;
 import org.elasticsearch.search.aggregations.pipeline.SimpleValue;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
 
@@ -62,7 +60,7 @@ public class SerialDiffIT extends ESIntegTestCase {
     static int numBuckets;
     static int lag;
     static BucketHelpers.GapPolicy gapPolicy;
-    static ValuesSourceAggregatorBuilder<? extends ValuesSource, ? extends ValuesSourceAggregatorBuilder<?, ?>> metric;
+    static ValuesSourceMetricsAggregationBuilder metric;
     static List<PipelineAggregationHelperTests.MockBucket> mockHisto;
 
     static Map<String, ArrayList<Double>> testValues;
@@ -82,7 +80,7 @@ public class SerialDiffIT extends ESIntegTestCase {
         }
     }
 
-    private ValuesSourceAggregatorBuilder<? extends ValuesSource, ? extends ValuesSourceAggregatorBuilder<?, ?>> randomMetric(String name, String field) {
+    private ValuesSourceMetricsAggregationBuilder randomMetric(String name, String field) {
         int rand = randomIntBetween(0,3);
 
         switch (rand) {
@@ -233,14 +231,16 @@ public class SerialDiffIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(diff("diff_counts", "_count")
+                                .subAggregation(diff("diff_counts")
                                         .lag(lag)
-                                        .gapPolicy(gapPolicy))
-                                .subAggregation(diff("diff_values", "the_metric")
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
+                                .subAggregation(diff("diff_values")
                                         .lag(lag)
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("the_metric"))
                 ).execute().actionGet();
 
         assertSearchResponse(response);
@@ -280,11 +280,12 @@ public class SerialDiffIT extends ESIntegTestCase {
                 .prepareSearch("idx").setTypes("type")
                 .addAggregation(
                         histogram("histo").field(INTERVAL_FIELD).interval(interval)
-                                .extendedBounds(new ExtendedBounds(0L, (long) (interval * (numBuckets - 1))))
+                                .extendedBounds(0L, (long) (interval * (numBuckets - 1)))
                                 .subAggregation(metric)
-                                .subAggregation(diff("diff_counts", "_count")
+                                .subAggregation(diff("diff_counts")
                                         .lag(-1)
-                                        .gapPolicy(gapPolicy))
+                                        .gapPolicy(gapPolicy)
+                                        .setBucketsPaths("_count"))
                 ).execute().actionGet();
         } catch (SearchPhaseExecutionException e) {
             assertThat(e.getMessage(), is("all shards failed"));
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/support/ValuesSourceTypeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/support/ValuesSourceTypeTests.java
deleted file mode 100644
index a297181..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/support/ValuesSourceTypeTests.java
+++ /dev/null
@@ -1,109 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.support;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class ValuesSourceTypeTests extends ESTestCase {
-
-    public void testValidOrdinals() {
-        assertThat(ValuesSourceType.ANY.ordinal(), equalTo(0));
-        assertThat(ValuesSourceType.NUMERIC.ordinal(), equalTo(1));
-        assertThat(ValuesSourceType.BYTES.ordinal(), equalTo(2));
-        assertThat(ValuesSourceType.GEOPOINT.ordinal(), equalTo(3));
-    }
-
-    public void testwriteTo() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ValuesSourceType.ANY.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(0));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ValuesSourceType.NUMERIC.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(1));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ValuesSourceType.BYTES.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(2));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ValuesSourceType.GEOPOINT.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(3));
-            }
-        }
-    }
-
-    public void testReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(0);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ValuesSourceType.ANY.readFrom(in), equalTo(ValuesSourceType.ANY));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(1);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ValuesSourceType.ANY.readFrom(in), equalTo(ValuesSourceType.NUMERIC));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(2);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ValuesSourceType.ANY.readFrom(in), equalTo(ValuesSourceType.BYTES));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(3);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ValuesSourceType.ANY.readFrom(in), equalTo(ValuesSourceType.GEOPOINT));
-            }
-        }
-    }
-
-    public void testInvalidReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(randomIntBetween(4, Integer.MAX_VALUE));
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                ValuesSourceType.ANY.readFrom(in);
-                fail("Expected IOException");
-            } catch(IOException e) {
-                assertThat(e.getMessage(), containsString("Unknown ValuesSourceType ordinal ["));
-            }
-
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java b/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
index fec1ca7..fc1ee7a 100644
--- a/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
@@ -344,8 +344,8 @@ public class TransportTwoNodesSearchIT extends ESIntegTestCase {
                 .query(termQuery("multi", "test"))
                 .from(0).size(20).explain(true)
                 .aggregation(AggregationBuilders.global("global").subAggregation(
-                        AggregationBuilders.filter("all", termQuery("multi", "test"))))
-                .aggregation(AggregationBuilders.filter("test1", termQuery("name", "test1")));
+                        AggregationBuilders.filter("all").filter(termQuery("multi", "test"))))
+                .aggregation(AggregationBuilders.filter("test1").filter(termQuery("name", "test1")));
 
         SearchResponse searchResponse = client().search(searchRequest("test").source(sourceBuilder)).actionGet();
         assertNoFailures(searchResponse);
diff --git a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
index 556b764..8521bd2 100644
--- a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
@@ -95,7 +95,7 @@ public class SearchSourceBuilderTests extends ESTestCase {
                         // skip me so we don't need transport
                     }
                     @Override
-                    protected void configureAggs(IndicesQueriesRegistry indicesQueriesRegistry) {
+                    protected void configureAggs() {
                         // skip me so we don't need scripting
                     }
                     @Override
@@ -339,7 +339,7 @@ public class SearchSourceBuilderTests extends ESTestCase {
             // NORELEASE need a random aggregation builder method
             builder.aggregation(AggregationBuilders.avg(randomAsciiOfLengthBetween(5, 20)));
         }
-        if (randomBoolean()) {
+        if (true) {
             // NORELEASE need a method to randomly build content for ext
             XContentBuilder xContentBuilder = XContentFactory.jsonBuilder();
             xContentBuilder.startObject();
diff --git a/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java b/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
index d8f76c0..67420bd 100644
--- a/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
@@ -429,7 +429,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
                 .prepareSearch("test")
                 .setQuery(hasChildQuery("child", boolQuery().should(termQuery("c_field", "red")).should(termQuery("c_field", "yellow"))))
                 .addAggregation(AggregationBuilders.global("global").subAggregation(
-                        AggregationBuilders.filter("filter", boolQuery().should(termQuery("c_field", "red")).should(termQuery("c_field", "yellow"))).subAggregation(
+                        AggregationBuilders.filter("filter").filter(boolQuery().should(termQuery("c_field", "red")).should(termQuery("c_field", "yellow"))).subAggregation(
                                 AggregationBuilders.terms("facet1").field("c_field")))).get();
         assertNoFailures(searchResponse);
         assertThat(searchResponse.getHits().totalHits(), equalTo(2L));
diff --git a/core/src/test/java/org/elasticsearch/search/fetch/FieldDataFieldsTests.java b/core/src/test/java/org/elasticsearch/search/fetch/FieldDataFieldsTests.java
index 7405cee..ba7f70c 100644
--- a/core/src/test/java/org/elasticsearch/search/fetch/FieldDataFieldsTests.java
+++ b/core/src/test/java/org/elasticsearch/search/fetch/FieldDataFieldsTests.java
@@ -40,7 +40,7 @@ public class FieldDataFieldsTests extends ESTestCase {
         parser.nextToken();
         parser.nextToken();
         parser.nextToken();
-        SearchContext context = new TestSearchContext();
+        SearchContext context = new TestSearchContext(null);
         parseElement.parse(parser, context);
     }
 
@@ -52,7 +52,7 @@ public class FieldDataFieldsTests extends ESTestCase {
         parser.nextToken();
         parser.nextToken();
         parser.nextToken();
-        SearchContext context = new TestSearchContext();
+        SearchContext context = new TestSearchContext(null);
         parseElement.parse(parser, context);
     }
 
@@ -69,7 +69,7 @@ public class FieldDataFieldsTests extends ESTestCase {
         parser.nextToken();
         parser.nextToken();
         parser.nextToken();
-        SearchContext context = new TestSearchContext();
+        SearchContext context = new TestSearchContext(null);
         try {
             parseElement.parse(parser, context);
             fail("Expected IllegalStateException");
diff --git a/core/src/test/java/org/elasticsearch/search/query/QueryPhaseTests.java b/core/src/test/java/org/elasticsearch/search/query/QueryPhaseTests.java
index 098825a..9643f24 100644
--- a/core/src/test/java/org/elasticsearch/search/query/QueryPhaseTests.java
+++ b/core/src/test/java/org/elasticsearch/search/query/QueryPhaseTests.java
@@ -51,7 +51,7 @@ import java.util.concurrent.atomic.AtomicBoolean;
 public class QueryPhaseTests extends ESTestCase {
 
     private void countTestCase(Query query, IndexReader reader, boolean shouldCollect) throws Exception {
-        TestSearchContext context = new TestSearchContext();
+        TestSearchContext context = new TestSearchContext(null);
         context.parsedQuery(new ParsedQuery(query));
         context.setSize(0);
 
@@ -120,7 +120,7 @@ public class QueryPhaseTests extends ESTestCase {
     }
 
     public void testPostFilterDisablesCountOptimization() throws Exception {
-        TestSearchContext context = new TestSearchContext();
+        TestSearchContext context = new TestSearchContext(null);
         context.parsedQuery(new ParsedQuery(new MatchAllDocsQuery()));
         context.setSize(0);
 
@@ -143,7 +143,7 @@ public class QueryPhaseTests extends ESTestCase {
     }
 
     public void testMinScoreDisablesCountOptimization() throws Exception {
-        TestSearchContext context = new TestSearchContext();
+        TestSearchContext context = new TestSearchContext(null);
         context.parsedQuery(new ParsedQuery(new MatchAllDocsQuery()));
         context.setSize(0);
 
diff --git a/core/src/test/java/org/elasticsearch/search/sort/SortParserTests.java b/core/src/test/java/org/elasticsearch/search/sort/SortParserTests.java
index e39fd81..cbd7b54 100644
--- a/core/src/test/java/org/elasticsearch/search/sort/SortParserTests.java
+++ b/core/src/test/java/org/elasticsearch/search/sort/SortParserTests.java
@@ -36,7 +36,7 @@ public class SortParserTests extends ESSingleNodeTestCase {
         mapping.startObject().startObject("type").startObject("properties").startObject("location").field("type", "geo_point").endObject().endObject().endObject().endObject();
         IndexService indexService = createIndex("testidx", Settings.settingsBuilder().build(), "type", mapping);
         TestSearchContext context = (TestSearchContext) createSearchContext(indexService);
-        context.setTypes("type");
+        context.getQueryShardContext().setTypes("type");
 
         XContentBuilder sortBuilder = jsonBuilder();
         sortBuilder.startObject();
diff --git a/core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java b/core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java
index 7739124..1df9659 100644
--- a/core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java
+++ b/core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java
@@ -23,8 +23,10 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.search.aggregations.Aggregation;
 import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms;
+import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsBuilder;
 import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Assert;
@@ -38,8 +40,6 @@ import static org.elasticsearch.test.ESIntegTestCase.client;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
 import static org.hamcrest.Matchers.equalTo;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.significantTerms;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 
 public class SharedSignificantTermsTestMethods {
     public static final String INDEX_NAME = "testidx";
@@ -57,9 +57,13 @@ public class SharedSignificantTermsTestMethods {
     }
 
     private static void checkSignificantTermsAggregationCorrect(ESIntegTestCase testCase) {
-        SearchResponse response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE).addAggregation(
-                terms("class").field(CLASS_FIELD).subAggregation(significantTerms("sig_terms").field(TEXT_FIELD)))
-                .execute().actionGet();
+
+        SearchResponse response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
+                .addAggregation(new TermsBuilder("class").field(CLASS_FIELD).subAggregation(
+                        new SignificantTermsBuilder("sig_terms")
+                                .field(TEXT_FIELD)))
+                .execute()
+                .actionGet();
         assertSearchResponse(response);
         StringTerms classes = response.getAggregations().get("class");
         Assert.assertThat(classes.getBuckets().size(), equalTo(2));
diff --git a/docs/reference/aggregations/bucket.asciidoc b/docs/reference/aggregations/bucket.asciidoc
index 66ce2d8..2d185dd 100644
--- a/docs/reference/aggregations/bucket.asciidoc
+++ b/docs/reference/aggregations/bucket.asciidoc
@@ -19,8 +19,6 @@ include::bucket/datehistogram-aggregation.asciidoc[]
 
 include::bucket/daterange-aggregation.asciidoc[]
 
-include::bucket/diversified-sampler-aggregation.asciidoc[]
-
 include::bucket/filter-aggregation.asciidoc[]
 
 include::bucket/filters-aggregation.asciidoc[]
diff --git a/docs/reference/aggregations/bucket/diversified-sampler-aggregation.asciidoc b/docs/reference/aggregations/bucket/diversified-sampler-aggregation.asciidoc
deleted file mode 100644
index 92effce..0000000
--- a/docs/reference/aggregations/bucket/diversified-sampler-aggregation.asciidoc
+++ /dev/null
@@ -1,154 +0,0 @@
-[[search-aggregations-bucket-sampler-aggregation]]
-=== Sampler Aggregation
-
-experimental[]
-
-A filtering aggregation used to limit any sub aggregations' processing to a sample of the top-scoring documents. Diversity settings are 
-used to limit the number of matches that share a common value such as an "author".
-
-.Example use cases:
-* Tightening the focus of analytics to high-relevance matches rather than the potentially very long tail of low-quality matches
-* Removing bias from analytics by ensuring fair representation of content from different sources
-* Reducing the running cost of aggregations that can produce useful results using only samples e.g. `significant_terms`
- 
-
-Example:
-
-[source,js]
---------------------------------------------------
-{
-    "query": {
-        "match": {
-            "text": "iphone"
-        }
-    },
-    "aggs": {
-        "sample": {
-            "sampler": {
-                "shard_size": 200,
-                "field" : "user.id"   
-            },
-            "aggs": {
-                "keywords": {
-                    "significant_terms": {
-                        "field": "text"
-                    }
-                }
-            }
-        }
-    }
-}
---------------------------------------------------
-
-Response:
-
-[source,js]
---------------------------------------------------
-{
-    ...
-        "aggregations": {
-        "sample": {
-            "doc_count": 1000,<1>
-            "keywords": {<2>
-                "doc_count": 1000,
-                "buckets": [
-                    ...
-                    {
-                        "key": "bend",
-                        "doc_count": 58,
-                        "score": 37.982536582524276,
-                        "bg_count": 103
-                    },
-                    ....
-}
---------------------------------------------------
-
-<1> 1000 documents were sampled in total becase we asked for a maximum of 200 from an index with 5 shards. The cost of performing the nested significant_terms aggregation was therefore limited rather than unbounded.
-<2> The results of the significant_terms aggregation are not skewed by any single over-active Twitter user because we asked for a maximum of one tweet from any one user in our sample.
-
-
-==== shard_size
-
-The `shard_size` parameter limits how many top-scoring documents are collected in the sample processed on each shard.
-The default value is 100.
-
-==== Controlling diversity
-=`field` or `script` and `max_docs_per_value` settings are used to control the maximum number of documents collected on any one shard which share a common value.
-The choice of value (e.g. `author`) is loaded from a regular `field` or derived dynamically by a `script`.
-
-The aggregation will throw an error if the choice of field or script produces multiple values for a document.
-It is currently not possible to offer this form of de-duplication using many values, primarily due to concerns over efficiency.
-
-NOTE: Any good market researcher will tell you that when working with samples of data it is important
-that the sample represents a healthy variety of opinions rather than being skewed by any single voice.
-The same is true with aggregations and sampling with these diversify settings can offer a way to remove the bias in your content (an over-populated geography, a large spike in a timeline or an over-active forum spammer).  
-
-==== Field
-
-Controlling diversity using a field:
-
-[source,js]
---------------------------------------------------
-{
-    "aggs" : {
-        "sample" : {
-            "diverisfied_sampler" : {
-                "field" : "author",
-                "max_docs_per_value" : 3
-            }
-        }
-    }
-}
---------------------------------------------------
-
-Note that the `max_docs_per_value` setting applies on a per-shard basis only for the purposes of shard-local sampling.
-It is not intended as a way of providing a global de-duplication feature on search results.
-
-
-
-==== Script
-
-Controlling diversity using a script:
-
-[source,js]
---------------------------------------------------
-{
-    "aggs" : {
-        "sample" : {
-            "diverisfied_sampler" : {
-                "script" : "doc['author'].value + '/' + doc['genre'].value"
-            }
-        }
-    }
-}
---------------------------------------------------
-Note in the above example we chose to use the default `max_docs_per_value` setting of 1 and combine author and genre fields to ensure 
-each shard sample has, at most, one match for an author/genre pair.
-
-
-==== execution_hint
-
-When using the settings to control diversity, the optional `execution_hint` setting can influence the management of the values used for de-duplication.
-Each option will hold up to `shard_size` values in memory while performing de-duplication but the type of value held can be controlled as follows:
- 
- - hold field values directly (`map`)
- - hold ordinals of the field as determined by the Lucene index (`global_ordinals`)
- - hold hashes of the field values - with potential for hash collisions (`bytes_hash`)
- 
-The default setting is to use `global_ordinals` if this information is available from the Lucene index and reverting to `map` if not.
-The `bytes_hash` setting may prove faster in some cases but introduces the possibility of false positives in de-duplication logic due to the possibility of hash collisions.
-Please note that Elasticsearch will ignore the choice of execution hint if it is not applicable and that there is no backward compatibility guarantee on these hints.
-
-==== Limitations
-
-===== Cannot be nested under `breadth_first` aggregations
-Being a quality-based filter the sampler aggregation needs access to the relevance score produced for each document.
-It therefore cannot be nested under a `terms` aggregation which has the `collect_mode` switched from the default `depth_first` mode to `breadth_first` as this discards scores.
-In this situation an error will be thrown.
-
-===== Limited de-dup logic.
-The de-duplication logic in the diversify settings applies only at a shard level so will not apply across shards.
-
-===== No specialized syntax for geo/date fields
-Currently the syntax for defining the diversifying values is defined by a choice of `field` or `script` - there is no added syntactical sugar for expressing geo or date units such as "1w" (1 week).
-This support may be added in a later release and users will currently have to create these sorts of values using a script.
\ No newline at end of file
diff --git a/docs/reference/aggregations/bucket/sampler-aggregation.asciidoc b/docs/reference/aggregations/bucket/sampler-aggregation.asciidoc
index 741edc8..2974270 100644
--- a/docs/reference/aggregations/bucket/sampler-aggregation.asciidoc
+++ b/docs/reference/aggregations/bucket/sampler-aggregation.asciidoc
@@ -4,9 +4,11 @@
 experimental[]
 
 A filtering aggregation used to limit any sub aggregations' processing to a sample of the top-scoring documents.
+Optionally, diversity settings can be used to limit the number of matches that share a common value such as an "author".
 
 .Example use cases:
 * Tightening the focus of analytics to high-relevance matches rather than the potentially very long tail of low-quality matches
+* Removing bias from analytics by ensuring fair representation of content from different sources
 * Reducing the running cost of aggregations that can produce useful results using only samples e.g. `significant_terms`
  
 
@@ -23,7 +25,8 @@ Example:
     "aggs": {
         "sample": {
             "sampler": {
-                "shard_size": 200
+                "shard_size": 200,
+                "field" : "user.id"   
             },
             "aggs": {
                 "keywords": {
@@ -60,7 +63,8 @@ Response:
 }
 --------------------------------------------------
 
-<1> 1000 documents were sampled in total because we asked for a maximum of 200 from an index with 5 shards. The cost of performing the nested significant_terms aggregation was therefore limited rather than unbounded.
+<1> 1000 documents were sampled in total becase we asked for a maximum of 200 from an index with 5 shards. The cost of performing the nested significant_terms aggregation was therefore limited rather than unbounded.
+<2> The results of the significant_terms aggregation are not skewed by any single over-active Twitter user because we asked for a maximum of one tweet from any one user in our sample.
 
 
 ==== shard_size
@@ -68,9 +72,83 @@ Response:
 The `shard_size` parameter limits how many top-scoring documents are collected in the sample processed on each shard.
 The default value is 100.
 
+==== Controlling diversity
+Optionally, you can use the `field` or `script` and `max_docs_per_value` settings to control the maximum number of documents collected on any one shard which share a common value.
+The choice of value (e.g. `author`) is loaded from a regular `field` or derived dynamically by a `script`.
+
+The aggregation will throw an error if the choice of field or script produces multiple values for a document.
+It is currently not possible to offer this form of de-duplication using many values, primarily due to concerns over efficiency.
+
+NOTE: Any good market researcher will tell you that when working with samples of data it is important
+that the sample represents a healthy variety of opinions rather than being skewed by any single voice.
+The same is true with aggregations and sampling with these diversify settings can offer a way to remove the bias in your content (an over-populated geography, a large spike in a timeline or an over-active forum spammer).  
+
+==== Field
+
+Controlling diversity using a field:
+
+[source,js]
+--------------------------------------------------
+{
+    "aggs" : {
+        "sample" : {
+            "sampler" : {
+                "field" : "author",
+                "max_docs_per_value" : 3
+            }
+        }
+    }
+}
+--------------------------------------------------
+
+Note that the `max_docs_per_value` setting applies on a per-shard basis only for the purposes of shard-local sampling.
+It is not intended as a way of providing a global de-duplication feature on search results.
+
+
+
+==== Script
+
+Controlling diversity using a script:
+
+[source,js]
+--------------------------------------------------
+{
+    "aggs" : {
+        "sample" : {
+            "sampler" : {
+                "script" : "doc['author'].value + '/' + doc['genre'].value"
+            }
+        }
+    }
+}
+--------------------------------------------------
+Note in the above example we chose to use the default `max_docs_per_value` setting of 1 and combine author and genre fields to ensure 
+each shard sample has, at most, one match for an author/genre pair.
+
+
+==== execution_hint
+
+When using the settings to control diversity, the optional `execution_hint` setting can influence the management of the values used for de-duplication.
+Each option will hold up to `shard_size` values in memory while performing de-duplication but the type of value held can be controlled as follows:
+ 
+ - hold field values directly (`map`)
+ - hold ordinals of the field as determined by the Lucene index (`global_ordinals`)
+ - hold hashes of the field values - with potential for hash collisions (`bytes_hash`)
+ 
+The default setting is to use `global_ordinals` if this information is available from the Lucene index and reverting to `map` if not.
+The `bytes_hash` setting may prove faster in some cases but introduces the possibility of false positives in de-duplication logic due to the possibility of hash collisions.
+Please note that Elasticsearch will ignore the choice of execution hint if it is not applicable and that there is no backward compatibility guarantee on these hints.
+
 ==== Limitations
 
 ===== Cannot be nested under `breadth_first` aggregations
 Being a quality-based filter the sampler aggregation needs access to the relevance score produced for each document.
 It therefore cannot be nested under a `terms` aggregation which has the `collect_mode` switched from the default `depth_first` mode to `breadth_first` as this discards scores.
-In this situation an error will be thrown.
\ No newline at end of file
+In this situation an error will be thrown.
+
+===== Limited de-dup logic.
+The de-duplication logic in the diversify settings applies only at a shard level so will not apply across shards.
+
+===== No specialized syntax for geo/date fields
+Currently the syntax for defining the diversifying values is defined by a choice of `field` or `script` - there is no added syntactical sugar for expressing geo or date units such as "1w" (1 week).
+This support may be added in a later release and users will currently have to create these sorts of values using a script.
\ No newline at end of file
diff --git a/docs/reference/migration/migrate_3_0.asciidoc b/docs/reference/migration/migrate_3_0.asciidoc
index bae484f..9e95f9d 100644
--- a/docs/reference/migration/migrate_3_0.asciidoc
+++ b/docs/reference/migration/migrate_3_0.asciidoc
@@ -112,6 +112,7 @@ The `GET` HTTP verb for `/_forcemerge` is no longer supported, please use the
 ==== Deprecated queries removed
 
 The following deprecated queries have been removed:
+
 * `filtered`: use `bool` query instead, which supports `filter` clauses too
 * `and`: use `must` clauses in a `bool` query instead
 * `or`: use should clauses in a `bool` query instead
diff --git a/modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java b/modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java
index 5246d0d..a866f33 100644
--- a/modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java
+++ b/modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java
@@ -530,10 +530,9 @@ public class MoreExpressionTests extends ESIntegTestCase {
                                 .subAggregation(sum("twoSum").field("two"))
                                 .subAggregation(sum("threeSum").field("three"))
                                 .subAggregation(sum("fourSum").field("four"))
-                                .subAggregation(bucketScript("totalSum",
-                                    new Script("_value0 + _value1 + _value2", ScriptType.INLINE, ExpressionScriptEngineService.NAME, null),
-                                    "twoSum", "threeSum", "fourSum")))
-                .execute().actionGet();
+                                .subAggregation(
+                                        bucketScript("totalSum").setBucketsPaths("twoSum", "threeSum", "fourSum").script(
+                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, ExpressionScriptEngineService.NAME, null)))).execute().actionGet();
 
         InternalHistogram<Bucket> histogram = response.getAggregations().get("histogram");
         assertThat(histogram, notNullValue());
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketScriptTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketScriptTests.java
index 28549a1..06119fd 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketScriptTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketScriptTests.java
@@ -113,8 +113,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null)
-                                                , "field2Sum", "field3Sum", "field4Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -157,8 +157,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0 + _value1 / _value2", ScriptType.INLINE, null, null),
-                                                "field2Sum", "field3Sum", "field4Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("_value0 + _value1 / _value2", ScriptType.INLINE, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -199,8 +199,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .interval(interval)
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0", ScriptType.INLINE, null, null),
-                                                "field2Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum").script(
+                                                new Script("_value0", ScriptType.INLINE, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -241,7 +241,7 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", bucketsPathsMap, 
+                                        bucketScript("seriesArithmetic").setBucketsPathsMap(bucketsPathsMap ).script(
                                                 new Script("foo + bar + baz", ScriptType.INLINE, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -287,8 +287,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("(_value0 + _value1 + _value2) * factor", ScriptType.INLINE, null, params),
-                                                "field2Sum", "field3Sum", "field4Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("(_value0 + _value1 + _value2) * factor", ScriptType.INLINE, null, params)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -331,8 +331,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null),
-                                                "field2Sum", "field3Sum", "field4Sum").gapPolicy(GapPolicy.INSERT_ZEROS))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null)).gapPolicy(GapPolicy.INSERT_ZEROS))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -377,8 +377,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("my_script", ScriptType.INDEXED, null, null),
-                                                "field2Sum", "field3Sum", "field4Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("my_script", ScriptType.INDEXED, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -421,8 +421,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null),
-                                                "field2Sum", "field3Sum", "field4Sum")))
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null))))
                                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -444,8 +444,8 @@ public class BucketScriptTests extends ESIntegTestCase {
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(sum("field4Sum").field(FIELD_4_NAME))
                                 .subAggregation(
-                                        bucketScript("seriesArithmetic", new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null),
-                                                "field2Sum", "field3Sum", "field4Sum"))).execute().actionGet();
+                                        bucketScript("seriesArithmetic").setBucketsPaths("field2Sum", "field3Sum", "field4Sum").script(
+                                                new Script("_value0 + _value1 + _value2", ScriptType.INLINE, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketSelectorTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketSelectorTests.java
index ffee8c7..e0eb23a 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketSelectorTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BucketSelectorTests.java
@@ -27,7 +27,6 @@ import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.Bucket;
@@ -46,8 +45,8 @@ import java.util.Map;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.histogram;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
-import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders.bucketSelector;
 import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders.derivative;
+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders.having;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.greaterThan;
@@ -117,13 +116,19 @@ public class BucketSelectorTests extends ESIntegTestCase {
     }
 
     public void testInlineScript() {
-        SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(histogram("histo").field(FIELD_1_NAME).interval(interval)
-                        .subAggregation(sum("field2Sum").field(FIELD_2_NAME)).subAggregation(sum("field3Sum").field(FIELD_3_NAME))
-                        .subAggregation(bucketSelector("bucketSelector",
-                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", ScriptType.INLINE, null, null),
-                                "field2Sum", "field3Sum")))
-                .execute().actionGet();
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        histogram("histo")
+                                .field(FIELD_1_NAME)
+                                .interval(interval)
+                                .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
+                                .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
+                                .subAggregation(
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", ScriptType.INLINE,
+                                                        null, null)))).execute()
+                .actionGet();
 
         assertSearchResponse(response);
 
@@ -154,8 +159,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("Double.isNaN(_value0) ? true : (_value0 < 10000)",
-                                                ScriptType.INLINE, null, null), "field2Sum", "field3Sum"))).execute()
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? true : (_value0 < 10000)", ScriptType.INLINE, null,
+                                                        null)))).execute()
                 .actionGet();
 
         assertSearchResponse(response);
@@ -187,8 +193,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("Double.isNaN(_value0) ? false : (_value0 > 10000)",
-                                                ScriptType.INLINE, null, null), "field2Sum", "field3Sum"))).execute().actionGet();
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 > 10000)", ScriptType.INLINE, null,
+                                                        null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -209,8 +216,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("Double.isNaN(_value0) ? false : (_value0 < _value1)",
-                                                ScriptType.INLINE, null, null), "field2Sum", "field3Sum"))).execute().actionGet();
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 < _value1)", ScriptType.INLINE, null,
+                                                        null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -240,8 +248,9 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .interval(interval)
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("Double.isNaN(_value0) ? false : (_value0 > 100)",
-                                                ScriptType.INLINE,null, null), "field2Sum"))).execute().actionGet();
+                                        having("having").setBucketsPaths("field2Sum")
+                                                .script(new Script("Double.isNaN(_value0) ? false : (_value0 > 100)", ScriptType.INLINE,
+                                                        null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -264,12 +273,19 @@ public class BucketSelectorTests extends ESIntegTestCase {
         bucketPathsMap.put("my_value1", "field2Sum");
         bucketPathsMap.put("my_value2", "field3Sum");
 
-        SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(histogram("histo").field(FIELD_1_NAME).interval(interval)
-                        .subAggregation(sum("field2Sum").field(FIELD_2_NAME)).subAggregation(sum("field3Sum").field(FIELD_3_NAME))
-                        .subAggregation(bucketSelector("bucketSelector", bucketPathsMap, new Script(
-                                "Double.isNaN(my_value1) ? false : (my_value1 + my_value2 > 100)", ScriptType.INLINE, null, null))))
-                .execute().actionGet();
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        histogram("histo")
+                                .field(FIELD_1_NAME)
+                                .interval(interval)
+                                .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
+                                .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
+                                .subAggregation(
+                                        having("having").setBucketsPathsMap(bucketPathsMap).script(
+                                                new Script("Double.isNaN(my_value1) ? false : (my_value1 + my_value2 > 100)",
+                                                        ScriptType.INLINE, null, null)))).execute()
+                .actionGet();
 
         assertSearchResponse(response);
 
@@ -293,12 +309,19 @@ public class BucketSelectorTests extends ESIntegTestCase {
     public void testInlineScriptWithParams() {
         Map<String, Object> params = new HashMap<>();
         params.put("threshold", 100);
-        SearchResponse response = client().prepareSearch("idx").addAggregation(histogram("histo").field(FIELD_1_NAME).interval(interval)
-                .subAggregation(sum("field2Sum").field(FIELD_2_NAME)).subAggregation(sum("field3Sum").field(FIELD_3_NAME))
-                .subAggregation(bucketSelector("bucketSelector",
-                        new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > threshold)", ScriptType.INLINE, null, params),
-                        "field2Sum", "field3Sum")))
-                .execute().actionGet();
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        histogram("histo")
+                                .field(FIELD_1_NAME)
+                                .interval(interval)
+                                .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
+                                .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
+                                .subAggregation(
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > threshold)",
+                                                        ScriptType.INLINE, null, params)))).execute()
+                .actionGet();
 
         assertSearchResponse(response);
 
@@ -320,13 +343,17 @@ public class BucketSelectorTests extends ESIntegTestCase {
     }
 
     public void testInlineScriptInsertZeros() {
-        SearchResponse response = client().prepareSearch("idx")
+        SearchResponse response = client()
+                .prepareSearch("idx")
                 .addAggregation(
-                        histogram("histo").field(FIELD_1_NAME).interval(interval).subAggregation(sum("field2Sum").field(FIELD_2_NAME))
+                        histogram("histo")
+                                .field(FIELD_1_NAME)
+                                .interval(interval)
+                                .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
-                                .subAggregation(bucketSelector("bucketSelector",
-                                        new Script("_value0 + _value1 > 100", ScriptType.INLINE, null, null), "field2Sum", "field3Sum")
-                                                .gapPolicy(GapPolicy.INSERT_ZEROS)))
+                                .subAggregation(
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").gapPolicy(GapPolicy.INSERT_ZEROS)
+                                                .script(new Script("_value0 + _value1 > 100", ScriptType.INLINE, null, null))))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -358,8 +385,8 @@ public class BucketSelectorTests extends ESIntegTestCase {
                                 .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
                                 .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
                                 .subAggregation(
-                                        bucketSelector("bucketSelector", new Script("my_script", ScriptType.INDEXED, null, null),
-                                                "field2Sum", "field3Sum"))).execute().actionGet();
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("my_script", ScriptType.INDEXED, null, null)))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -381,13 +408,19 @@ public class BucketSelectorTests extends ESIntegTestCase {
     }
 
     public void testUnmapped() throws Exception {
-        SearchResponse response = client().prepareSearch("idx_unmapped")
-                .addAggregation(histogram("histo").field(FIELD_1_NAME).interval(interval)
-                        .subAggregation(sum("field2Sum").field(FIELD_2_NAME)).subAggregation(sum("field3Sum").field(FIELD_3_NAME))
-                        .subAggregation(bucketSelector("bucketSelector",
-                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", ScriptType.INLINE, null, null),
-                                "field2Sum", "field3Sum")))
-                .execute().actionGet();
+        SearchResponse response = client()
+                .prepareSearch("idx_unmapped")
+                .addAggregation(
+                        histogram("histo")
+                                .field(FIELD_1_NAME)
+                                .interval(interval)
+                                .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
+                                .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
+                                .subAggregation(
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", ScriptType.INLINE,
+                                                        null, null)))).execute()
+                .actionGet();
 
         assertSearchResponse(response);
 
@@ -398,13 +431,19 @@ public class BucketSelectorTests extends ESIntegTestCase {
     }
 
     public void testPartiallyUnmapped() throws Exception {
-        SearchResponse response = client().prepareSearch("idx", "idx_unmapped")
-                .addAggregation(histogram("histo").field(FIELD_1_NAME).interval(interval)
-                        .subAggregation(sum("field2Sum").field(FIELD_2_NAME)).subAggregation(sum("field3Sum").field(FIELD_3_NAME))
-                        .subAggregation(bucketSelector("bucketSelector",
-                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", ScriptType.INLINE, null, null),
-                                "field2Sum", "field3Sum")))
-                .execute().actionGet();
+        SearchResponse response = client()
+                .prepareSearch("idx", "idx_unmapped")
+                .addAggregation(
+                        histogram("histo")
+                                .field(FIELD_1_NAME)
+                                .interval(interval)
+                                .subAggregation(sum("field2Sum").field(FIELD_2_NAME))
+                                .subAggregation(sum("field3Sum").field(FIELD_3_NAME))
+                                .subAggregation(
+                                        having("having").setBucketsPaths("field2Sum", "field3Sum").script(
+                                                new Script("Double.isNaN(_value0) ? false : (_value0 + _value1 > 100)", ScriptType.INLINE,
+                                                        null, null)))).execute()
+                .actionGet();
 
         assertSearchResponse(response);
 
@@ -428,8 +467,8 @@ public class BucketSelectorTests extends ESIntegTestCase {
     public void testEmptyBuckets() {
         SearchResponse response = client().prepareSearch("idx_with_gaps")
                 .addAggregation(histogram("histo").field(FIELD_1_NAME).interval(1)
-                        .subAggregation(histogram("inner_histo").field(FIELD_1_NAME).interval(1).extendedBounds(new ExtendedBounds(1L, 4L))
-                                .minDocCount(0).subAggregation(derivative("derivative", "_count").gapPolicy(GapPolicy.INSERT_ZEROS))))
+                        .subAggregation(histogram("inner_histo").field(FIELD_1_NAME).interval(1).extendedBounds(1L, 4L).minDocCount(0)
+                                .subAggregation(derivative("derivative").setBucketsPaths("_count").gapPolicy(GapPolicy.INSERT_ZEROS))))
                 .execute().actionGet();
 
         assertSearchResponse(response);
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DateRangeTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DateRangeTests.java
index 44f7a93..37b1e11 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DateRangeTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DateRangeTests.java
@@ -26,7 +26,9 @@ import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.range.Range;
 import org.elasticsearch.search.aggregations.bucket.range.Range.Bucket;
-import org.elasticsearch.search.aggregations.bucket.range.date.DateRangeAggregatorBuilder;
+import org.elasticsearch.search.aggregations.bucket.range.date.DateRangeBuilder;
+import org.elasticsearch.search.aggregations.metrics.max.Max;
+import org.elasticsearch.search.aggregations.metrics.min.Min;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
@@ -43,6 +45,8 @@ import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.dateRange;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.histogram;
+import static org.elasticsearch.search.aggregations.AggregationBuilders.max;
+import static org.elasticsearch.search.aggregations.AggregationBuilders.min;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
@@ -108,7 +112,7 @@ public class DateRangeTests extends ESIntegTestCase {
     }
 
     public void testDateMath() throws Exception {
-        DateRangeAggregatorBuilder rangeBuilder = dateRange("range");
+        DateRangeBuilder rangeBuilder = dateRange("range");
         if (randomBoolean()) {
             rangeBuilder.field("date");
         } else {
@@ -452,8 +456,62 @@ public class DateRangeTests extends ESIntegTestCase {
         assertThat((long) propertiesDocCounts[2], equalTo(numDocs - 4L));
     }
 
+    public void testSingleValuedFieldWithSubAggregationInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateRange("range")
+                        .field("date")
+                        .addUnboundedTo("r1", date(2, 15))
+                        .addRange("r2", date(2, 15), date(3, 15))
+                        .addUnboundedFrom("r3", date(3, 15))
+                        .subAggregation(min("min")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
 
 
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(buckets.size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("r1"));
+        assertThat(((DateTime) bucket.getFrom()), nullValue());
+        assertThat(((DateTime) bucket.getTo()), equalTo(date(2, 15)));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("2012-02-15T00:00:00.000Z"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        Min min = bucket.getAggregations().get("min");
+        assertThat(min, notNullValue());
+        assertThat(min.getValue(), equalTo((double) date(1, 2).getMillis()));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("r2"));
+        assertThat(((DateTime) bucket.getFrom()), equalTo(date(2, 15)));
+        assertThat(((DateTime) bucket.getTo()), equalTo(date(3, 15)));
+        assertThat(bucket.getFromAsString(), equalTo("2012-02-15T00:00:00.000Z"));
+        assertThat(bucket.getToAsString(), equalTo("2012-03-15T00:00:00.000Z"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        min = bucket.getAggregations().get("min");
+        assertThat(min, notNullValue());
+        assertThat(min.getValue(), equalTo((double) date(2, 15).getMillis()));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("r3"));
+        assertThat(((DateTime) bucket.getFrom()), equalTo(date(3, 15)));
+        assertThat(((DateTime) bucket.getTo()), nullValue());
+        assertThat(bucket.getFromAsString(), equalTo("2012-03-15T00:00:00.000Z"));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(numDocs - 4L));
+        min = bucket.getAggregations().get("min");
+        assertThat(min, notNullValue());
+        assertThat(min.getValue(), equalTo((double) date(3, 15).getMillis()));
+    }
+
     /*
         Jan 2,  Feb 3,      1
         Feb 2,  Mar 3,      2
@@ -574,10 +632,62 @@ public class DateRangeTests extends ESIntegTestCase {
         Apr 23, May 24      6
      */
 
+    public void testMultiValuedFieldWithValueScriptWithInheritedSubAggregator() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateRange("range")
+                        .field("dates")
+                                .script(new Script("new DateTime(_value.longValue(), DateTimeZone.UTC).plusMonths(1).getMillis()"))
+                                .addUnboundedTo(date(2, 15)).addRange(date(2, 15), date(3, 15)).addUnboundedFrom(date(3, 15))
+                                .subAggregation(max("max"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(buckets.size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-2012-02-15T00:00:00.000Z"));
+        assertThat(((DateTime) bucket.getFrom()), nullValue());
+        assertThat(((DateTime) bucket.getTo()), equalTo(date(2, 15)));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("2012-02-15T00:00:00.000Z"));
+        assertThat(bucket.getDocCount(), equalTo(1L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) date(3, 3).getMillis()));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("2012-02-15T00:00:00.000Z-2012-03-15T00:00:00.000Z"));
+        assertThat(((DateTime) bucket.getFrom()), equalTo(date(2, 15)));
+        assertThat(((DateTime) bucket.getTo()), equalTo(date(3, 15)));
+        assertThat(bucket.getFromAsString(), equalTo("2012-02-15T00:00:00.000Z"));
+        assertThat(bucket.getToAsString(), equalTo("2012-03-15T00:00:00.000Z"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) date(4, 3).getMillis()));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("2012-03-15T00:00:00.000Z-*"));
+        assertThat(((DateTime) bucket.getFrom()), equalTo(date(3, 15)));
+        assertThat(((DateTime) bucket.getTo()), nullValue());
+        assertThat(bucket.getFromAsString(), equalTo("2012-03-15T00:00:00.000Z"));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(numDocs - 1L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+    }
+
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateRange("range")
-                        .script(new Script("doc['date'].value"))
+.script(new Script("doc['date'].value"))
                         .addUnboundedTo(date(2, 15))
                         .addRange(date(2, 15), date(3, 15))
                         .addUnboundedFrom(date(3, 15)))
@@ -620,7 +730,57 @@ public class DateRangeTests extends ESIntegTestCase {
         assertThat(bucket.getDocCount(), equalTo(numDocs - 4L));
     }
 
+    public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        dateRange("range").script(new Script("doc['date'].value")).addUnboundedTo(date(2, 15))
+                                .addRange(date(2, 15), date(3, 15)).addUnboundedFrom(date(3, 15)).subAggregation(max("max"))).execute()
+                .actionGet();
 
+        assertSearchResponse(response);
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(buckets.size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-2012-02-15T00:00:00.000Z"));
+        assertThat(((DateTime) bucket.getFrom()), nullValue());
+        assertThat(((DateTime) bucket.getTo()), equalTo(date(2, 15)));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("2012-02-15T00:00:00.000Z"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) date(2, 2).getMillis()));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("2012-02-15T00:00:00.000Z-2012-03-15T00:00:00.000Z"));
+        assertThat(((DateTime) bucket.getFrom()), equalTo(date(2, 15)));
+        assertThat(((DateTime) bucket.getTo()), equalTo(date(3, 15)));
+        assertThat(bucket.getFromAsString(), equalTo("2012-02-15T00:00:00.000Z"));
+        assertThat(bucket.getToAsString(), equalTo("2012-03-15T00:00:00.000Z"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) date(3, 2).getMillis()));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("2012-03-15T00:00:00.000Z-*"));
+        assertThat(((DateTime) bucket.getFrom()), equalTo(date(3, 15)));
+        assertThat(((DateTime) bucket.getTo()), nullValue());
+        assertThat(bucket.getFromAsString(), equalTo("2012-03-15T00:00:00.000Z"));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(numDocs - 4L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+    }
 
 
 
@@ -676,6 +836,58 @@ public class DateRangeTests extends ESIntegTestCase {
         assertThat(bucket.getDocCount(), equalTo(numDocs - 2L));
     }
 
+    public void testScriptMultiValuedWithAggregatorInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateRange("range")
+.script(new Script("doc['dates'].values")).addUnboundedTo(date(2, 15))
+                                .addRange(date(2, 15), date(3, 15)).addUnboundedFrom(date(3, 15)).subAggregation(min("min"))).execute()
+                .actionGet();
+
+        assertSearchResponse(response);
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(buckets.size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-2012-02-15T00:00:00.000Z"));
+        assertThat(((DateTime) bucket.getFrom()), nullValue());
+        assertThat(((DateTime) bucket.getTo()), equalTo(date(2, 15)));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("2012-02-15T00:00:00.000Z"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        Min min = bucket.getAggregations().get("min");
+        assertThat(min, notNullValue());
+        assertThat(min.getValue(), equalTo((double) date(1, 2).getMillis()));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("2012-02-15T00:00:00.000Z-2012-03-15T00:00:00.000Z"));
+        assertThat(((DateTime) bucket.getFrom()), equalTo(date(2, 15)));
+        assertThat(((DateTime) bucket.getTo()), equalTo(date(3, 15)));
+        assertThat(bucket.getFromAsString(), equalTo("2012-02-15T00:00:00.000Z"));
+        assertThat(bucket.getToAsString(), equalTo("2012-03-15T00:00:00.000Z"));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+        min = bucket.getAggregations().get("min");
+        assertThat(min, notNullValue());
+        assertThat(min.getValue(), equalTo((double) date(2, 2).getMillis()));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("2012-03-15T00:00:00.000Z-*"));
+        assertThat(((DateTime) bucket.getFrom()), equalTo(date(3, 15)));
+        assertThat(((DateTime) bucket.getTo()), nullValue());
+        assertThat(bucket.getFromAsString(), equalTo("2012-03-15T00:00:00.000Z"));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(numDocs - 2L));
+        min = bucket.getAggregations().get("min");
+        assertThat(min, notNullValue());
+        assertThat(min.getValue(), equalTo((double) date(2, 15).getMillis()));
+    }
+
     public void testUnmapped() throws Exception {
         client().admin().cluster().prepareHealth("idx_unmapped").setWaitForYellowStatus().execute().actionGet();
 
@@ -819,8 +1031,7 @@ public class DateRangeTests extends ESIntegTestCase {
     public void testEmptyAggregation() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(dateRange("date_range").field("value").addRange("0-1", 0, 1)))
+                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(dateRange("date_range").addRange("0-1", 0, 1)))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java
index ced13b8..749eab9 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java
@@ -31,7 +31,6 @@ import org.elasticsearch.search.aggregations.bucket.AbstractTermsTestCase;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
 import org.elasticsearch.search.aggregations.metrics.stats.Stats;
@@ -314,7 +313,8 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
         SearchResponse response = client().prepareSearch("idx").setTypes("type")
                 .addAggregation(terms("terms")
                         .field(SINGLE_VALUED_FIELD_NAME)
-                        .includeExclude(new IncludeExclude(includes, excludes))
+                        .include(includes)
+                        .exclude(excludes)
                         .collectMode(randomFrom(SubAggCollectionMode.values())))
                 .execute().actionGet();
         assertSearchResponse(response);
@@ -416,6 +416,34 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testSingleValuedFieldWithSubAggregationInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                .addAggregation(terms("terms")
+                        .field(SINGLE_VALUED_FIELD_NAME)
+                        .collectMode(randomFrom(SubAggCollectionMode.values()))
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("" + (double) i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("" + (double) i));
+            assertThat(bucket.getKeyAsNumber().intValue(), equalTo(i));
+            assertThat(bucket.getDocCount(), equalTo(1L));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            assertThat(sum.getValue(), equalTo((double) i));
+        }
+    }
+
     public void testSingleValuedFieldWithValueScript() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type")
                 .addAggregation(terms("terms")
@@ -538,6 +566,43 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
 
     */
 
+    public void testMultiValuedFieldWithValueScriptWithInheritedSubAggregator() throws Exception {
+        SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                .addAggregation(terms("terms")
+                        .field(MULTI_VALUED_FIELD_NAME)
+                        .collectMode(randomFrom(SubAggCollectionMode.values()))
+                                .script(new Script("_value + 1"))
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(6));
+
+        for (int i = 0; i < 6; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("" + (i + 1d));
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("" + (i+1d)));
+            assertThat(bucket.getKeyAsNumber().doubleValue(), equalTo(i + 1d));
+            final long count = i == 0 || i == 5 ? 1 : 2;
+            double s = 0;
+            for (int j = 0; j < NUM_DOCS; ++j) {
+                if (i == j || i == j+1) {
+                    s += j + 1;
+                    s += j+1 + 1;
+                }
+            }
+            assertThat(bucket.getDocCount(), equalTo(count));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            assertThat(sum.getValue(), equalTo(s));
+        }
+    }
+
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -562,6 +627,34 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                .addAggregation(terms("terms")
+                        .field(SINGLE_VALUED_FIELD_NAME)
+                        .collectMode(randomFrom(SubAggCollectionMode.values()))
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("" + (double) i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("" + (double) i));
+            assertThat(bucket.getKeyAsNumber().intValue(), equalTo(i));
+            assertThat(bucket.getDocCount(), equalTo(1L));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            assertThat(sum.getValue(), equalTo((double) i));
+        }
+    }
+
     public void testScriptMultiValued() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -590,6 +683,65 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testScriptMultiValuedWithAggregatorInheritedNoExplicitType() throws Exception {
+        // since no type is explicitly defined, es will assume all values returned by the script to be strings (bytes),
+        // so the aggregation should fail, since the "sum" aggregation can only operation on numeric values.
+
+        try {
+
+            SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                    .addAggregation(terms("terms")
+                            .collectMode(randomFrom(SubAggCollectionMode.values()))
+                                    .script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "']"))
+                            .subAggregation(sum("sum")))
+                    .execute().actionGet();
+
+
+            fail("expected to fail as sub-aggregation sum requires a numeric value source context, but there is none");
+
+        } catch (Exception e) {
+            // expected
+        }
+
+    }
+
+    public void testScriptMultiValuedWithAggregatorInheritedWithExplicitType() throws Exception {
+        SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                .addAggregation(terms("terms")
+                        .collectMode(randomFrom(SubAggCollectionMode.values()))
+                                .script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "']"))
+                        .valueType(Terms.ValueType.DOUBLE)
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(6));
+
+        for (int i = 0; i < 6; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("" + i + ".0");
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("" + i + ".0"));
+            assertThat(bucket.getKeyAsNumber().intValue(), equalTo(i));
+            final long count = i == 0 || i == 5 ? 1 : 2;
+            double s = 0;
+            for (int j = 0; j < NUM_DOCS; ++j) {
+                if (i == j || i == j+1) {
+                    s += j;
+                    s += j+1;
+                }
+            }
+            assertThat(bucket.getDocCount(), equalTo(count));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            assertThat(sum.getValue(), equalTo(s));
+        }
+    }
+
     public void testUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx_unmapped").setTypes("type")
                 .addAggregation(terms("terms")
@@ -635,7 +787,7 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1L).minDocCount(0)
-                        .subAggregation(terms("terms").field(SINGLE_VALUED_FIELD_NAME)))
+                        .subAggregation(terms("terms")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -732,7 +884,7 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
                 .addAggregation(
                         terms("num_tags").field("num_tag").collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .order(Terms.Order.aggregation("filter", asc))
-                                .subAggregation(filter("filter", QueryBuilders.matchAllQuery()))).execute().actionGet();
+                                .subAggregation(filter("filter").filter(QueryBuilders.matchAllQuery()))).execute().actionGet();
 
 
         assertSearchResponse(response);
@@ -772,8 +924,8 @@ public class DoubleTermsTests extends AbstractTermsTestCase {
                                 .collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .order(Terms.Order.aggregation("filter1>filter2>max", asc))
                                 .subAggregation(
-                                        filter("filter1", QueryBuilders.matchAllQuery()).subAggregation(
-                                                filter("filter2", QueryBuilders.matchAllQuery()).subAggregation(
+                                        filter("filter1").filter(QueryBuilders.matchAllQuery()).subAggregation(
+                                                filter("filter2").filter(QueryBuilders.matchAllQuery()).subAggregation(
                                                         max("max").field(SINGLE_VALUED_FIELD_NAME))))).execute().actionGet();
 
 
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/EquivalenceTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/EquivalenceTests.java
index 032e340..3c062f8 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/EquivalenceTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/EquivalenceTests.java
@@ -35,8 +35,8 @@ import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.range.Range;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
 import org.elasticsearch.search.aggregations.bucket.range.Range.Bucket;
+import org.elasticsearch.search.aggregations.bucket.range.RangeBuilder;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
@@ -122,7 +122,7 @@ public class EquivalenceTests extends ESIntegTestCase {
             }
         }
 
-        RangeAggregator.RangeAggregatorBuilder query = range("range").field("values");
+        RangeBuilder query = range("range").field("values");
         for (int i = 0; i < ranges.length; ++i) {
             String key = Integer.toString(i);
             if (ranges[i][0] == Double.NEGATIVE_INFINITY) {
@@ -143,7 +143,7 @@ public class EquivalenceTests extends ESIntegTestCase {
             if (ranges[i][1] != Double.POSITIVE_INFINITY){
                 filter = filter.to(ranges[i][1]);
             }
-            reqBuilder = reqBuilder.addAggregation(filter("filter" + i, filter));
+            reqBuilder = reqBuilder.addAggregation(filter("filter" + i).filter(filter));
         }
 
         SearchResponse resp = reqBuilder.execute().actionGet();
@@ -236,16 +236,15 @@ public class EquivalenceTests extends ESIntegTestCase {
 
         assertNoFailures(client().admin().indices().prepareRefresh("idx").setIndicesOptions(IndicesOptions.lenientExpandOpen()).execute().get());
 
-        TermsAggregatorFactory.ExecutionMode[] globalOrdinalModes = new TermsAggregatorFactory.ExecutionMode[] {
-                TermsAggregatorFactory.ExecutionMode.GLOBAL_ORDINALS_HASH, TermsAggregatorFactory.ExecutionMode.GLOBAL_ORDINALS
+        TermsAggregatorFactory.ExecutionMode[] globalOrdinalModes = new TermsAggregatorFactory.ExecutionMode[]{
+                TermsAggregatorFactory.ExecutionMode.GLOBAL_ORDINALS_HASH,
+                TermsAggregatorFactory.ExecutionMode.GLOBAL_ORDINALS
         };
 
         SearchResponse resp = client().prepareSearch("idx")
                 .addAggregation(terms("long").field("long_values").size(maxNumTerms).collectMode(randomFrom(SubAggCollectionMode.values())).subAggregation(min("min").field("num")))
                 .addAggregation(terms("double").field("double_values").size(maxNumTerms).collectMode(randomFrom(SubAggCollectionMode.values())).subAggregation(max("max").field("num")))
-                .addAggregation(terms("string_map").field("string_values").collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .executionHint(TermsAggregatorFactory.ExecutionMode.MAP.toString()).size(maxNumTerms)
-                        .subAggregation(stats("stats").field("num")))
+                .addAggregation(terms("string_map").field("string_values").collectMode(randomFrom(SubAggCollectionMode.values())).executionHint(TermsAggregatorFactory.ExecutionMode.MAP.toString()).size(maxNumTerms).subAggregation(stats("stats").field("num")))
                 .addAggregation(terms("string_global_ordinals").field("string_values").collectMode(randomFrom(SubAggCollectionMode.values())).executionHint(globalOrdinalModes[randomInt(globalOrdinalModes.length - 1)].toString()).size(maxNumTerms).subAggregation(extendedStats("stats").field("num")))
                 .addAggregation(terms("string_global_ordinals_doc_values").field("string_values.doc_values").collectMode(randomFrom(SubAggCollectionMode.values())).executionHint(globalOrdinalModes[randomInt(globalOrdinalModes.length - 1)].toString()).size(maxNumTerms).subAggregation(extendedStats("stats").field("num")))
                 .execute().actionGet();
@@ -353,7 +352,7 @@ public class EquivalenceTests extends ESIntegTestCase {
         indexRandom(true, client().prepareIndex("idx", "type").setSource("f", value));
         ensureYellow("idx"); // only one document let's make sure all shards have an active primary
         SearchResponse response = client().prepareSearch("idx")
-                .addAggregation(filter("filter", QueryBuilders.matchAllQuery())
+                .addAggregation(filter("filter").filter(QueryBuilders.matchAllQuery())
                 .subAggregation(range("range")
                         .field("f")
                         .addUnboundedTo(6)
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ExtendedStatsTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ExtendedStatsTests.java
index 7d018ad..855c13b 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ExtendedStatsTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ExtendedStatsTests.java
@@ -71,8 +71,7 @@ public class ExtendedStatsTests extends AbstractNumericTestCase {
     public void testEmptyAggregation() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(
-                        histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(extendedStats("stats").field("value")))
+                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(extendedStats("stats")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentileRanksTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentileRanksTests.java
index 4fe11c6..ff4f5d6 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentileRanksTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentileRanksTests.java
@@ -77,7 +77,7 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
             }
         }
         Arrays.sort(percents);
-        Loggers.getLogger(HDRPercentileRanksTests.class).info("Using values={}", Arrays.toString(percents));
+        Loggers.getLogger(HDRPercentileRanksTests.class).info("Using percentiles={}", Arrays.toString(percents));
         return percents;
     }
 
@@ -85,8 +85,8 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
         return randomIntBetween(0, 5);
     }
 
-    private void assertConsistent(double[] pcts, PercentileRanks values, long minValue, long maxValue, int numberSigDigits) {
-        final List<Percentile> percentileList = iterableAsArrayList(values);
+    private void assertConsistent(double[] pcts, PercentileRanks percentiles, long minValue, long maxValue, int numberSigDigits) {
+        final List<Percentile> percentileList = iterableAsArrayList(percentiles);
         assertEquals(pcts.length, percentileList.size());
         for (int i = 0; i < pcts.length; ++i) {
             final Percentile percentile = percentileList.get(i);
@@ -121,9 +121,8 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                                 .interval(1L)
                                 .minDocCount(0)
                                 .subAggregation(
-                                        percentileRanks("percentile_ranks").field("value").method(PercentilesMethod.HDR)
-                                        .numberOfSignificantValueDigits(sigDigits).values(10, 15)))
-                .execute().actionGet();
+                                        percentileRanks("percentile_ranks").method(PercentilesMethod.HDR)
+                                                .numberOfSignificantValueDigits(sigDigits).percentiles(10, 15))).execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
         Histogram histo = searchResponse.getAggregations().get("histo");
@@ -146,8 +145,7 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").values(0, 10, 15, 100))
-                .execute().actionGet();
+                                .field("value").percentiles(0, 10, 15, 100)).execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(0L));
 
@@ -169,13 +167,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").values(pcts))
-                .execute().actionGet();
+                                .field("value").percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue, sigDigits);
     }
 
     @Override
@@ -188,8 +185,7 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .addAggregation(
                         global("global").subAggregation(
                                 percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                                .field("value").values(pcts)))
-                .execute().actionGet();
+                                        .field("value").percentiles(pcts))).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
@@ -200,10 +196,10 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
         assertThat(global.getAggregations(), notNullValue());
         assertThat(global.getAggregations().asMap().size(), equalTo(1));
 
-        PercentileRanks values = global.getAggregations().get("percentile_ranks");
-        assertThat(values, notNullValue());
-        assertThat(values.getName(), equalTo("percentile_ranks"));
-        assertThat((PercentileRanks) global.getProperty("percentile_ranks"), sameInstance(values));
+        PercentileRanks percentiles = global.getAggregations().get("percentile_ranks");
+        assertThat(percentiles, notNullValue());
+        assertThat(percentiles.getName(), equalTo("percentile_ranks"));
+        assertThat((PercentileRanks) global.getProperty("percentile_ranks"), sameInstance(percentiles));
 
     }
 
@@ -215,13 +211,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").values(pcts))
-                .execute().actionGet();
+                                .field("value").percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue, sigDigits);
     }
 
     @Override
@@ -233,13 +228,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").values(pcts))
-                .execute().actionGet();
+                                .field("value").percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue, sigDigits);
     }
 
     @Override
@@ -251,13 +245,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").script(new Script("_value - 1")).values(pcts))
-                .execute().actionGet();
+                                .field("value").script(new Script("_value - 1")).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1, sigDigits);
     }
 
     @Override
@@ -271,13 +264,13 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("value").script(new Script("_value - dec", ScriptType.INLINE, null, params)).values(pcts))
+                                .field("value").script(new Script("_value - dec", ScriptType.INLINE, null, params)).percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1, sigDigits);
     }
 
     @Override
@@ -289,13 +282,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("values").values(pcts))
-                .execute().actionGet();
+                                .field("values").percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues, maxValues, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues, maxValues, sigDigits);
     }
 
     @Override
@@ -307,13 +299,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("values").script(new Script("_value - 1")).values(pcts))
-                .execute().actionGet();
+                                .field("values").script(new Script("_value - 1")).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1, sigDigits);
     }
 
     public void testMultiValuedFieldWithValueScriptReverse() throws Exception {
@@ -324,13 +315,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("values").script(new Script("20 - _value")).values(pcts))
-                .execute().actionGet();
+                                .field("values").script(new Script("20 - _value")).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, 20 - maxValues, 20 - minValues, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, 20 - maxValues, 20 - minValues, sigDigits);
     }
 
     @Override
@@ -344,13 +334,13 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .field("values").script(new Script("_value - dec", ScriptType.INLINE, null, params)).values(pcts))
+                                .field("values").script(new Script("_value - dec", ScriptType.INLINE, null, params)).percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1, sigDigits);
     }
 
     @Override
@@ -362,13 +352,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .script(new Script("doc['value'].value")).values(pcts))
-                .execute().actionGet();
+                                .script(new Script("doc['value'].value")).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue, sigDigits);
     }
 
     @Override
@@ -382,13 +371,13 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .script(new Script("doc['value'].value - dec", ScriptType.INLINE, null, params)).values(pcts))
+                                .script(new Script("doc['value'].value - dec", ScriptType.INLINE, null, params)).percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1, sigDigits);
     }
 
     @Override
@@ -400,13 +389,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         percentileRanks("percentile_ranks").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
-                        .script(new Script("doc['values'].values")).values(pcts))
-                .execute().actionGet();
+                                .script(new Script("doc['values'].values")).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues, maxValues, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues, maxValues, sigDigits);
     }
 
     @Override
@@ -424,14 +412,12 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                                 .numberOfSignificantValueDigits(sigDigits)
                                 .script(new Script(
                                         "List values = doc['values'].values; double[] res = new double[values.size()]; for (int i = 0; i < res.length; i++) { res[i] = values.get(i) - dec; }; return res;",
-                                ScriptType.INLINE, null, params))
-                        .values(pcts))
-                .execute().actionGet();
+                                        ScriptType.INLINE, null, params)).percentiles(pcts)).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1, sigDigits);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1, sigDigits);
     }
 
     public void testOrderBySubAggregation() {
@@ -443,8 +429,8 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
                 .addAggregation(
                         histogram("histo").field("value").interval(2L)
                                 .subAggregation(
-                                        percentileRanks("percentile_ranks").field("value").method(PercentilesMethod.HDR)
-                                .numberOfSignificantValueDigits(sigDigits).values(99))
+                                        percentileRanks("percentile_ranks").method(PercentilesMethod.HDR)
+                                                .numberOfSignificantValueDigits(sigDigits).percentiles(99))
                                 .order(Order.aggregation("percentile_ranks", "99", asc))).execute().actionGet();
 
         assertHitCount(searchResponse, 10);
@@ -452,8 +438,8 @@ public class HDRPercentileRanksTests extends AbstractNumericTestCase {
         Histogram histo = searchResponse.getAggregations().get("histo");
         double previous = asc ? Double.NEGATIVE_INFINITY : Double.POSITIVE_INFINITY;
         for (Histogram.Bucket bucket : histo.getBuckets()) {
-            PercentileRanks values = bucket.getAggregations().get("percentile_ranks");
-            double p99 = values.percent(99);
+            PercentileRanks percentiles = bucket.getAggregations().get("percentile_ranks");
+            double p99 = percentiles.percent(99);
             if (asc) {
                 assertThat(p99, greaterThanOrEqualTo(previous));
             } else {
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentilesTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentilesTests.java
index aec3c43..8f9370e 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentilesTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HDRPercentilesTests.java
@@ -122,9 +122,7 @@ public class HDRPercentilesTests extends AbstractNumericTestCase {
                                 .interval(1L)
                                 .minDocCount(0)
                                 .subAggregation(
-                                        percentiles("percentiles").field("value")
-                                                .numberOfSignificantValueDigits(sigDigits)
-                                                .method(PercentilesMethod.HDR)
+                                        percentiles("percentiles").numberOfSignificantValueDigits(sigDigits).method(PercentilesMethod.HDR)
                                                 .percentiles(10,
                                                 15))).execute().actionGet();
 
@@ -421,9 +419,7 @@ public class HDRPercentilesTests extends AbstractNumericTestCase {
                 .addAggregation(
                         histogram("histo").field("value").interval(2L)
                                 .subAggregation(
-                                        percentiles("percentiles").field("value")
-                                                .method(PercentilesMethod.HDR)
-                                                .numberOfSignificantValueDigits(sigDigits)
+                                        percentiles("percentiles").method(PercentilesMethod.HDR).numberOfSignificantValueDigits(sigDigits)
                                                 .percentiles(99))
                                 .order(Order.aggregation("percentiles", "99", asc))).execute().actionGet();
 
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java
index fb709c3..87bcc22 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java
@@ -25,10 +25,11 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.groovy.GroovyPlugin;
+import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Bucket;
+import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
 import org.elasticsearch.search.aggregations.metrics.stats.Stats;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
@@ -38,6 +39,7 @@ import org.hamcrest.Matchers;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
+import java.util.Iterator;
 import java.util.List;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
@@ -47,6 +49,7 @@ import static org.elasticsearch.search.aggregations.AggregationBuilders.histogra
 import static org.elasticsearch.search.aggregations.AggregationBuilders.max;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.stats;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
+import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
 import static org.hamcrest.Matchers.containsString;
@@ -355,6 +358,40 @@ public class HistogramTests extends ESIntegTestCase {
         }
     }
 
+    public void testSingleValuedFieldWithSubAggregationInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        assertThat(histo.getBuckets().size(), equalTo(numValueBuckets));
+
+        // TODO: use diamond once JI-9019884 is fixed
+        List<Histogram.Bucket> buckets = new ArrayList<Histogram.Bucket>(histo.getBuckets());
+        for (int i = 0; i < numValueBuckets; ++i) {
+            Histogram.Bucket bucket = buckets.get(i);
+            assertThat(bucket, notNullValue());
+            assertThat(((Number) bucket.getKey()).longValue(), equalTo((long) i * interval));
+            assertThat(bucket.getDocCount(), equalTo(valueCounts[i]));
+            assertThat(bucket.getAggregations().asList().isEmpty(), is(false));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            long s = 0;
+            for (int j = 0; j < numDocs; ++j) {
+                if ((j + 1) / interval == i) {
+                    s += j + 1;
+                }
+            }
+            assertThat(sum.getValue(), equalTo((double) s));
+        }
+    }
+
     public void testSingleValuedFieldOrderedBySubAggregationAsc() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval).order(Histogram.Order.aggregation("sum", true))
@@ -435,6 +472,46 @@ public class HistogramTests extends ESIntegTestCase {
         }
     }
 
+    public void testSingleValuedFieldOrderedByMultiValuedSubAggregationAscInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval).order(Histogram.Order.aggregation("stats.sum", true))
+                        .subAggregation(stats("stats")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        assertThat(histo.getBuckets().size(), equalTo(numValueBuckets));
+
+        LongHashSet visited = new LongHashSet();
+        double previousSum = Double.NEGATIVE_INFINITY;
+        // TODO: use diamond once JI-9019884 is fixed
+        List<Histogram.Bucket> buckets = new ArrayList<Histogram.Bucket>(histo.getBuckets());
+        for (int i = 0; i < numValueBuckets; ++i) {
+            Histogram.Bucket bucket = buckets.get(i);
+            assertThat(bucket, notNullValue());
+            long key = ((Number) bucket.getKey()).longValue();
+            assertTrue(visited.add(key));
+            int b = (int) (key / interval);
+            assertThat(bucket.getDocCount(), equalTo(valueCounts[b]));
+            assertThat(bucket.getAggregations().asList().isEmpty(), is(false));
+            Stats stats = bucket.getAggregations().get("stats");
+            assertThat(stats, notNullValue());
+            long s = 0;
+            for (int j = 0; j < numDocs; ++j) {
+                if ((j + 1) / interval == b) {
+                    s += j + 1;
+                }
+            }
+            assertThat(stats.getSum(), equalTo((double) s));
+            assertThat(stats.getSum(), greaterThanOrEqualTo(previousSum));
+            previousSum = s;
+        }
+    }
+
     public void testSingleValuedFieldOrderedByMultiValuedSubAggregationDesc() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval).order(Histogram.Order.aggregation("stats.sum", false))
@@ -479,7 +556,7 @@ public class HistogramTests extends ESIntegTestCase {
         boolean asc = randomBoolean();
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval).order(Histogram.Order.aggregation("filter>max", asc))
-                        .subAggregation(filter("filter", matchAllQuery())
+                        .subAggregation(filter("filter").filter(matchAllQuery())
                                 .subAggregation(max("max").field(SINGLE_VALUED_FIELD_NAME))))
                 .execute().actionGet();
 
@@ -619,6 +696,55 @@ public class HistogramTests extends ESIntegTestCase {
         }
     }
 
+    public void testMultiValuedFieldWithValueScriptWithInheritedSubAggregator() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(
+                        histogram("histo")
+                                .field(MULTI_VALUED_FIELD_NAME)
+                                .script(new Script("_value + 1"))
+                                .interval(interval)
+                                .subAggregation(
+                                        terms(MULTI_VALUED_FIELD_NAME).collectMode(randomFrom(SubAggCollectionMode.values())).order(
+                                                Terms.Order.term(true)))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        final int numBuckets = (numDocs + 2) / interval - 2 / interval + 1;
+        final long[] counts = new long[(numDocs + 2) / interval + 1];
+        for (int i = 0; i < numDocs; ++i) {
+            final int bucket1 = (i + 2) / interval;
+            final int bucket2 = (i + 3) / interval;
+            ++counts[bucket1];
+            if (bucket1 != bucket2) {
+                ++counts[bucket2];
+            }
+        }
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        List<? extends Bucket> buckets = histo.getBuckets();
+        assertThat(buckets.size(), equalTo(numBuckets));
+
+        for (int i = 0; i < numBuckets; i++) {
+            Histogram.Bucket bucket = buckets.get(i);
+            assertThat(bucket, notNullValue());
+            int key = ((2 / interval) + i) * interval;
+            assertThat(((Number) bucket.getKey()).longValue(), equalTo((long) key));
+            assertThat(bucket.getDocCount(), equalTo(counts[key / interval]));
+            Terms terms = bucket.getAggregations().get(MULTI_VALUED_FIELD_NAME);
+            assertThat(terms, notNullValue());
+            assertThat(terms.getName(), equalTo(MULTI_VALUED_FIELD_NAME));
+            int minTerm = Math.max(2, key - 1);
+            int maxTerm = Math.min(numDocs + 2, (key / interval + 1) * interval);
+            assertThat(terms.getBuckets().size(), equalTo(maxTerm - minTerm + 1));
+            Iterator<Terms.Bucket> iter = terms.getBuckets().iterator();
+            for (int j = minTerm; j <= maxTerm; ++j) {
+                assertThat(iter.next().getKeyAsNumber().longValue(), equalTo((long) j));
+            }
+        }
+    }
+
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").script(new Script("doc['" + SINGLE_VALUED_FIELD_NAME + "'].value")).interval(interval))
@@ -640,6 +766,40 @@ public class HistogramTests extends ESIntegTestCase {
         }
     }
 
+    public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        histogram("histo").script(new Script("doc['" + SINGLE_VALUED_FIELD_NAME + "'].value")).interval(interval)
+                                .subAggregation(sum("sum"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        assertThat(histo.getBuckets().size(), equalTo(numValueBuckets));
+
+        // TODO: use diamond once JI-9019884 is fixed
+        List<Histogram.Bucket> buckets = new ArrayList<Histogram.Bucket>(histo.getBuckets());
+        for (int i = 0; i < numValueBuckets; ++i) {
+            Histogram.Bucket bucket = buckets.get(i);
+            assertThat(bucket, notNullValue());
+            assertThat(((Number) bucket.getKey()).longValue(), equalTo((long) i * interval));
+            assertThat(bucket.getDocCount(), equalTo(valueCounts[i]));
+            assertThat(bucket.getAggregations().asList().isEmpty(), is(false));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            long s = 0;
+            for (int j = 0; j < numDocs; ++j) {
+                if ((j + 1) / interval == i) {
+                    s += j + 1;
+                }
+            }
+            assertThat(sum.getValue(), equalTo((double) s));
+        }
+    }
+
     public void testScriptMultiValued() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(histogram("histo").script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "']")).interval(interval))
@@ -661,6 +821,40 @@ public class HistogramTests extends ESIntegTestCase {
         }
     }
 
+    public void testScriptMultiValuedWithAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        histogram("histo").script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "']")).interval(interval)
+                                .subAggregation(sum("sum"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Histogram histo = response.getAggregations().get("histo");
+        assertThat(histo, notNullValue());
+        assertThat(histo.getName(), equalTo("histo"));
+        List<? extends Bucket> buckets = histo.getBuckets();
+        assertThat(buckets.size(), equalTo(numValuesBuckets));
+
+        for (int i = 0; i < numValuesBuckets; ++i) {
+            Histogram.Bucket bucket = buckets.get(i);
+            assertThat(bucket, notNullValue());
+            assertThat(((Number) bucket.getKey()).longValue(), equalTo((long) i * interval));
+            assertThat(bucket.getDocCount(), equalTo(valuesCounts[i]));
+            assertThat(bucket.getAggregations().asList().isEmpty(), is(false));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            long s = 0;
+            for (int j = 0; j < numDocs; ++j) {
+                if ((j + 1) / interval == i || (j + 2) / interval == i) {
+                    s += j + 1;
+                    s += j + 2;
+                }
+            }
+            assertThat(sum.getValue(), equalTo((double) s));
+        }
+    }
+
     public void testUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx_unmapped")
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval))
@@ -702,7 +896,7 @@ public class HistogramTests extends ESIntegTestCase {
                 .prepareSearch("idx", "idx_unmapped")
                 .addAggregation(
                         histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(interval)
-                                .extendedBounds(new ExtendedBounds((long) -1 * 2 * interval, (long) valueCounts.length * interval))).execute().actionGet();
+                                .extendedBounds((long) -1 * 2 * interval, (long) valueCounts.length * interval)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -734,7 +928,7 @@ public class HistogramTests extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1L).minDocCount(0)
-                        .subAggregation(histogram("sub_histo").field("SINGLE_VALUED_FIELD_NAME").interval(1L)))
+                        .subAggregation(histogram("sub_histo").interval(1L)))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -792,7 +986,7 @@ public class HistogramTests extends ESIntegTestCase {
                             .field(SINGLE_VALUED_FIELD_NAME)
                             .interval(interval)
                             .minDocCount(0)
-                            .extendedBounds(new ExtendedBounds(boundsMin, boundsMax)))
+                            .extendedBounds(boundsMin, boundsMax))
                     .execute().actionGet();
 
             if (invalidBoundsError) {
@@ -835,7 +1029,7 @@ public class HistogramTests extends ESIntegTestCase {
                     .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(-1).minDocCount(0)).execute().actionGet();
             fail();
         } catch (SearchPhaseExecutionException e) {
-            assertThat(e.toString(), containsString("[interval] must be 1 or greater for histogram aggregation [histo]"));
+            assertThat(e.toString(), containsString("Missing required field [interval]"));
         }
     }
 }
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IPv4RangeTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IPv4RangeTests.java
index 1fb16f6..b1861b5 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IPv4RangeTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IPv4RangeTests.java
@@ -27,6 +27,7 @@ import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.range.Range;
 import org.elasticsearch.search.aggregations.bucket.range.Range.Bucket;
+import org.elasticsearch.search.aggregations.metrics.max.Max;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
@@ -40,6 +41,7 @@ import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.histogram;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.ipRange;
+import static org.elasticsearch.search.aggregations.AggregationBuilders.max;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
@@ -315,6 +317,62 @@ public class IPv4RangeTests extends ESIntegTestCase {
         assertThat((double) propertiesCounts[2], equalTo((double) 55 * 3));
     }
 
+    public void testSingleValuedFieldWithSubAggregationInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(ipRange("range")
+                        .field("ip")
+                        .addUnboundedTo("10.0.0.100")
+                        .addRange("10.0.0.100", "10.0.0.200")
+                        .addUnboundedFrom("10.0.0.200")
+                        .subAggregation(max("max")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(range.getBuckets().size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-10.0.0.100"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(Double.NEGATIVE_INFINITY));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("10.0.0.100"));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.100")));
+        assertThat(bucket.getDocCount(), equalTo(100L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.99")));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("10.0.0.100-10.0.0.200"));
+        assertThat(bucket.getFromAsString(), equalTo("10.0.0.100"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.100")));
+        assertThat(bucket.getToAsString(), equalTo("10.0.0.200"));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.200")));
+        assertThat(bucket.getDocCount(), equalTo(100L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.199")));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("10.0.0.200-*"));
+        assertThat(bucket.getFromAsString(), equalTo("10.0.0.200"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.200")));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(55L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.254")));
+    }
+
     public void testSingleValuedFieldWithValueScript() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -465,6 +523,59 @@ public class IPv4RangeTests extends ESIntegTestCase {
         assertThat(bucket.getDocCount(), equalTo(56L));
     }
 
+    public void testMultiValuedFieldWithValueScriptWithInheritedSubAggregator() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        ipRange("range").field("ips").script(new Script("_value")).addUnboundedTo("10.0.0.100")
+                                .addRange("10.0.0.100", "10.0.0.200").addUnboundedFrom("10.0.0.200").subAggregation(max("max"))).execute()
+                .actionGet();
+
+        assertSearchResponse(response);
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(range.getBuckets().size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-10.0.0.100"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(Double.NEGATIVE_INFINITY));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("10.0.0.100"));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.100")));
+        assertThat(bucket.getDocCount(), equalTo(100L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, Matchers.notNullValue());
+        assertThat((long) max.getValue(), equalTo(IpFieldMapper.ipToLong("10.0.0.100")));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("10.0.0.100-10.0.0.200"));
+        assertThat(bucket.getFromAsString(), equalTo("10.0.0.100"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.100")));
+        assertThat(bucket.getToAsString(), equalTo("10.0.0.200"));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.200")));
+        assertThat(bucket.getDocCount(), equalTo(101L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, Matchers.notNullValue());
+        assertThat((long) max.getValue(), equalTo(IpFieldMapper.ipToLong("10.0.0.200")));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("10.0.0.200-*"));
+        assertThat(bucket.getFromAsString(), equalTo("10.0.0.200"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.200")));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(56L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, Matchers.notNullValue());
+        assertThat((long) max.getValue(), equalTo(IpFieldMapper.ipToLong("10.0.0.255")));
+    }
+
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -509,6 +620,60 @@ public class IPv4RangeTests extends ESIntegTestCase {
         assertThat(bucket.getDocCount(), equalTo(55L));
     }
 
+    public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        ipRange("range").script(new Script("doc['ip'].value")).addUnboundedTo("10.0.0.100")
+                                .addRange("10.0.0.100", "10.0.0.200").addUnboundedFrom("10.0.0.200").subAggregation(max("max"))).execute()
+                .actionGet();
+
+        assertSearchResponse(response);
+
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(range.getBuckets().size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-10.0.0.100"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(Double.NEGATIVE_INFINITY));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("10.0.0.100"));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.100")));
+        assertThat(bucket.getDocCount(), equalTo(100L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.99")));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("10.0.0.100-10.0.0.200"));
+        assertThat(bucket.getFromAsString(), equalTo("10.0.0.100"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.100")));
+        assertThat(bucket.getToAsString(), equalTo("10.0.0.200"));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.200")));
+        assertThat(bucket.getDocCount(), equalTo(100L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.199")));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("10.0.0.200-*"));
+        assertThat(bucket.getFromAsString(), equalTo("10.0.0.200"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.200")));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(55L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, notNullValue());
+        assertThat(max.getValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.254")));
+    }
+
     public void testScriptMultiValued() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -553,6 +718,60 @@ public class IPv4RangeTests extends ESIntegTestCase {
         assertThat(bucket.getDocCount(), equalTo(56L));
     }
 
+    public void testScriptMultiValuedWithAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        ipRange("range").script(new Script("doc['ips'].values")).addUnboundedTo("10.0.0.100")
+                                .addRange("10.0.0.100", "10.0.0.200").addUnboundedFrom("10.0.0.200").subAggregation(max("max"))).execute()
+                .actionGet();
+
+        assertSearchResponse(response);
+
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(range.getBuckets().size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-10.0.0.100"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(Double.NEGATIVE_INFINITY));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("10.0.0.100"));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.100")));
+        assertThat(bucket.getDocCount(), equalTo(100L));
+        Max max = bucket.getAggregations().get("max");
+        assertThat(max, Matchers.notNullValue());
+        assertThat((long) max.getValue(), equalTo(IpFieldMapper.ipToLong("10.0.0.100")));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("10.0.0.100-10.0.0.200"));
+        assertThat(bucket.getFromAsString(), equalTo("10.0.0.100"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.100")));
+        assertThat(bucket.getToAsString(), equalTo("10.0.0.200"));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.200")));
+        assertThat(bucket.getDocCount(), equalTo(101L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, Matchers.notNullValue());
+        assertThat((long) max.getValue(), equalTo(IpFieldMapper.ipToLong("10.0.0.200")));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("10.0.0.200-*"));
+        assertThat(bucket.getFromAsString(), equalTo("10.0.0.200"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo((double) IpFieldMapper.ipToLong("10.0.0.200")));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(56L));
+        max = bucket.getAggregations().get("max");
+        assertThat(max, Matchers.notNullValue());
+        assertThat((long) max.getValue(), equalTo(IpFieldMapper.ipToLong("10.0.0.255")));
+    }
+
     public void testUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx_unmapped")
                 .addAggregation(ipRange("range")
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/LongTermsTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/LongTermsTests.java
index 65f2881..fcff4b9 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/LongTermsTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/LongTermsTests.java
@@ -30,7 +30,6 @@ import org.elasticsearch.search.aggregations.bucket.AbstractTermsTestCase;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
 import org.elasticsearch.search.aggregations.metrics.stats.Stats;
@@ -290,7 +289,8 @@ public class LongTermsTests extends AbstractTermsTestCase {
         SearchResponse response = client().prepareSearch("idx").setTypes("type")
                 .addAggregation(terms("terms")
                         .field(SINGLE_VALUED_FIELD_NAME)
-                        .includeExclude(new IncludeExclude(includes, excludes))
+                        .include(includes)
+                        .exclude(excludes)
                         .collectMode(randomFrom(SubAggCollectionMode.values())))
                 .execute().actionGet();
         assertSearchResponse(response);
@@ -416,6 +416,34 @@ public class LongTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testSingleValuedFieldWithSubAggregationInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                .addAggregation(terms("terms")
+                        .field(SINGLE_VALUED_FIELD_NAME)
+                        .collectMode(randomFrom(SubAggCollectionMode.values()))
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("" + i));
+            assertThat(bucket.getKeyAsNumber().intValue(), equalTo(i));
+            assertThat(bucket.getDocCount(), equalTo(1L));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            assertThat(sum.getValue(), equalTo((double) i));
+        }
+    }
+
     public void testSingleValuedFieldWithValueScript() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type")
                 .addAggregation(terms("terms")
@@ -538,6 +566,43 @@ public class LongTermsTests extends AbstractTermsTestCase {
 
     */
 
+    public void testMultiValuedFieldWithValueScriptWithInheritedSubAggregator() throws Exception {
+        SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                .addAggregation(terms("terms")
+                        .field(MULTI_VALUED_FIELD_NAME)
+                        .collectMode(randomFrom(SubAggCollectionMode.values()))
+                                .script(new Script("_value + 1"))
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(6));
+
+        for (int i = 0; i < 6; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("" + (i + 1d));
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("" + (i+1d)));
+            assertThat(bucket.getKeyAsNumber().intValue(), equalTo(i+1));
+            final long count = i == 0 || i == 5 ? 1 : 2;
+            double s = 0;
+            for (int j = 0; j < NUM_DOCS; ++j) {
+                if (i == j || i == j+1) {
+                    s += j + 1;
+                    s += j+1 + 1;
+                }
+            }
+            assertThat(bucket.getDocCount(), equalTo(count));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            assertThat(sum.getValue(), equalTo(s));
+        }
+    }
+
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type")
                 .addAggregation(terms("terms")
@@ -563,6 +628,34 @@ public class LongTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                .addAggregation(terms("terms")
+                        .field(SINGLE_VALUED_FIELD_NAME)
+                        .collectMode(randomFrom(SubAggCollectionMode.values()))
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("" + i));
+            assertThat(bucket.getKeyAsNumber().intValue(), equalTo(i));
+            assertThat(bucket.getDocCount(), equalTo(1L));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            assertThat(sum.getValue(), equalTo((double) i));
+        }
+    }
+
     public void testScriptMultiValued() throws Exception {
         SearchResponse response = client().prepareSearch("idx").setTypes("type")
                 .addAggregation(terms("terms")
@@ -592,6 +685,59 @@ public class LongTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testScriptMultiValuedWithAggregatorInheritedNoExplicitType() throws Exception {
+        // since no type ie explicitly defined, es will assume all values returned by the script to be strings (bytes),
+        // so the aggregation should fail, since the "sum" aggregation can only operation on numeric values.
+        try {
+            client().prepareSearch("idx").setTypes("type")
+                    .addAggregation(terms("terms")
+                            .collectMode(randomFrom(SubAggCollectionMode.values()))
+                                    .script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "']"))
+                            .subAggregation(sum("sum")))
+                    .execute().actionGet();
+            fail("expected to fail as sub-aggregation sum requires a numeric value source context, but there is none");
+        } catch (Exception e) {
+            // expected
+        }
+    }
+
+    public void testScriptMultiValuedWithAggregatorInheritedWithExplicitType() throws Exception {
+        SearchResponse response = client().prepareSearch("idx").setTypes("type")
+                .addAggregation(terms("terms")
+                        .collectMode(randomFrom(SubAggCollectionMode.values()))
+                                .script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "']"))
+                        .valueType(Terms.ValueType.LONG)
+                        .subAggregation(sum("sum")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(6));
+
+        for (int i = 0; i < 6; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("" + i));
+            assertThat(bucket.getKeyAsNumber().intValue(), equalTo(i));
+            final long count = i == 0 || i == 5 ? 1 : 2;
+            double s = 0;
+            for (int j = 0; j < NUM_DOCS; ++j) {
+                if (i == j || i == j+1) {
+                    s += j;
+                    s += j+1;
+                }
+            }
+            assertThat(bucket.getDocCount(), equalTo(count));
+            Sum sum = bucket.getAggregations().get("sum");
+            assertThat(sum, notNullValue());
+            assertThat(sum.getValue(), equalTo(s));
+        }
+    }
+
     public void testUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx_unmapped").setTypes("type")
                 .addAggregation(terms("terms")
@@ -637,7 +783,7 @@ public class LongTermsTests extends AbstractTermsTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1L).minDocCount(0)
-                        .subAggregation(terms("terms").field(SINGLE_VALUED_FIELD_NAME)))
+                        .subAggregation(terms("terms")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -730,7 +876,7 @@ public class LongTermsTests extends AbstractTermsTestCase {
                         .field("num_tag")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
                         .order(Terms.Order.aggregation("filter", asc))
-.subAggregation(filter("filter", QueryBuilders.matchAllQuery()))
+                        .subAggregation(filter("filter").filter(QueryBuilders.matchAllQuery()))
                 ).execute().actionGet();
 
 
@@ -767,8 +913,8 @@ public class LongTermsTests extends AbstractTermsTestCase {
                         .field("num_tag")
                         .collectMode(randomFrom(SubAggCollectionMode.values()))
                         .order(Terms.Order.aggregation("filter1>filter2>max", asc))
-                .subAggregation(filter("filter1", QueryBuilders.matchAllQuery()).subAggregation(
-                        filter("filter2", QueryBuilders.matchAllQuery())
+                        .subAggregation(filter("filter1").filter(QueryBuilders.matchAllQuery())
+                                .subAggregation(filter("filter2").filter(QueryBuilders.matchAllQuery())
                                         .subAggregation(max("max").field(SINGLE_VALUED_FIELD_NAME))))
                 ).execute().actionGet();
 
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MaxTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MaxTests.java
index ee20202..d2e0d20 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MaxTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MaxTests.java
@@ -54,7 +54,7 @@ public class MaxTests extends AbstractNumericTestCase {
     public void testEmptyAggregation() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(max("max").field("value")))
+                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(max("max")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinDocCountTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinDocCountTests.java
index 9a92a61..9b3e1a3 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinDocCountTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinDocCountTests.java
@@ -35,8 +35,7 @@ import org.elasticsearch.search.aggregations.bucket.AbstractTermsTestCase;
 import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorBuilder;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
@@ -111,17 +110,17 @@ public class MinDocCountTests extends AbstractTermsTestCase {
     private enum Script {
         NO {
             @Override
-            TermsAggregatorBuilder apply(TermsAggregatorBuilder builder, String field) {
+            TermsBuilder apply(TermsBuilder builder, String field) {
                 return builder.field(field);
             }
         },
         YES {
             @Override
-            TermsAggregatorBuilder apply(TermsAggregatorBuilder builder, String field) {
+            TermsBuilder apply(TermsBuilder builder, String field) {
                 return builder.script(new org.elasticsearch.script.Script("doc['" + field + "'].values"));
             }
         };
-        abstract TermsAggregatorBuilder apply(TermsAggregatorBuilder builder, String field);
+        abstract TermsBuilder apply(TermsBuilder builder, String field);
     }
 
     // check that terms2 is a subset of terms1
@@ -298,7 +297,7 @@ public class MinDocCountTests extends AbstractTermsTestCase {
                             .executionHint(randomExecutionHint())
                             .order(order)
                             .size(size)
-                            .includeExclude(include == null ? null : new IncludeExclude(include, null))
+                            .include(include)
                             .shardSize(cardinality + randomInt(10))
                             .minDocCount(minDocCount)).request();
             final SearchResponse response = client().search(request).get();
@@ -378,7 +377,7 @@ public class MinDocCountTests extends AbstractTermsTestCase {
         final SearchResponse allResponse = client().prepareSearch("idx").setTypes("type")
                 .setSize(0)
                 .setQuery(QUERY)
-                .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).order(order).minDocCount(0))
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).order(order).minDocCount(0))
                 .execute().actionGet();
 
         final Histogram allHisto = allResponse.getAggregations().get("histo");
@@ -387,7 +386,7 @@ public class MinDocCountTests extends AbstractTermsTestCase {
             final SearchResponse response = client().prepareSearch("idx").setTypes("type")
                     .setSize(0)
                     .setQuery(QUERY)
-                    .addAggregation(dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.DAY).order(order).minDocCount(minDocCount))
+                    .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).order(order).minDocCount(minDocCount))
                     .execute().actionGet();
             assertSubset(allHisto, (Histogram) response.getAggregations().get("histo"), minDocCount);
         }
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinTests.java
index 36bf5c2..caa7b2f 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/MinTests.java
@@ -54,7 +54,7 @@ public class MinTests extends AbstractNumericTestCase {
     public void testEmptyAggregation() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(min("min").field("value")))
+                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(min("min")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/RangeTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/RangeTests.java
index 9955d24..725751f 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/RangeTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/RangeTests.java
@@ -28,6 +28,7 @@ import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.range.Range;
 import org.elasticsearch.search.aggregations.bucket.range.Range.Bucket;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
@@ -39,9 +40,10 @@ import java.util.List;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
+import static org.elasticsearch.search.aggregations.AggregationBuilders.avg;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.histogram;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.range;
+import static org.elasticsearch.search.aggregations.AggregationBuilders.sum;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
 import static org.hamcrest.Matchers.equalTo;
@@ -361,6 +363,66 @@ public class RangeTests extends ESIntegTestCase {
         assertThat((double) propertiesCounts[2], equalTo((double) total));
     }
 
+    public void testSingleValuedFieldWithSubAggregationInherited() throws Exception {
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(range("range")
+                        .field(SINGLE_VALUED_FIELD_NAME)
+                        .addUnboundedTo(3)
+                        .addRange(3, 6)
+                        .addUnboundedFrom(6)
+                        .subAggregation(avg("avg")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(range.getBuckets().size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-3.0"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(Double.NEGATIVE_INFINITY));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(3.0));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("3.0"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        Avg avg = bucket.getAggregations().get("avg");
+        assertThat(avg, notNullValue());
+        assertThat(avg.getValue(), equalTo(1.5)); // (1 + 2) / 2
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("3.0-6.0"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(3.0));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(6.0));
+        assertThat(bucket.getFromAsString(), equalTo("3.0"));
+        assertThat(bucket.getToAsString(), equalTo("6.0"));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+        avg = bucket.getAggregations().get("avg");
+        assertThat(avg, notNullValue());
+        assertThat(avg.getValue(), equalTo(4.0)); // (3 + 4 + 5) / 3
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("6.0-*"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(6.0));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY));
+        assertThat(bucket.getFromAsString(), equalTo("6.0"));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(numDocs - 5L));
+        avg = bucket.getAggregations().get("avg");
+        assertThat(avg, notNullValue());
+        long total = 0;
+        for (int i = 5; i < numDocs; ++i) {
+            total += i + 1;
+        }
+        assertThat(avg.getValue(), equalTo((double) total / (numDocs - 5))); // (6 + 7 + 8 + 9 + 10) / 5
+    }
+
     public void testSingleValuedFieldWithValueScript() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -537,6 +599,66 @@ public class RangeTests extends ESIntegTestCase {
     r3: 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12
      */
 
+    public void testMultiValuedFieldWithValueScriptWithInheritedSubAggregator() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        range("range").field(MULTI_VALUED_FIELD_NAME).script(new Script("_value + 1")).addUnboundedTo(3).addRange(3, 6)
+                                .addUnboundedFrom(6).subAggregation(sum("sum"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(range.getBuckets().size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-3.0"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(Double.NEGATIVE_INFINITY));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(3.0));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("3.0"));
+        assertThat(bucket.getDocCount(), equalTo(1L));
+        Sum sum = bucket.getAggregations().get("sum");
+        assertThat(sum, notNullValue());
+        assertThat(sum.getName(), equalTo("sum"));
+        assertThat(sum.getValue(), equalTo(2d+3d));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("3.0-6.0"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(3.0));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(6.0));
+        assertThat(bucket.getFromAsString(), equalTo("3.0"));
+        assertThat(bucket.getToAsString(), equalTo("6.0"));
+        assertThat(bucket.getDocCount(), equalTo(4L));
+        sum = bucket.getAggregations().get("sum");
+        assertThat(sum, notNullValue());
+        assertThat(sum.getName(), equalTo("sum"));
+        assertThat(sum.getValue(), equalTo((double) 2+3+3+4+4+5+5+6));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("6.0-*"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(6.0));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY));
+        assertThat(bucket.getFromAsString(), equalTo("6.0"));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(numDocs - 3L));
+        sum = bucket.getAggregations().get("sum");
+        assertThat(sum, notNullValue());
+        assertThat(sum.getName(), equalTo("sum"));
+        long total = 0;
+        for (int i = 3; i < numDocs; ++i) {
+            total += ((i + 1) + 1) + ((i + 1) + 2);
+        }
+        assertThat(sum.getValue(), equalTo((double) total));
+    }
+
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -581,6 +703,63 @@ public class RangeTests extends ESIntegTestCase {
         assertThat(bucket.getDocCount(), equalTo(numDocs - 5L));
     }
 
+    public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        range("range").script(new Script("doc['" + SINGLE_VALUED_FIELD_NAME + "'].value")).addUnboundedTo(3).addRange(3, 6)
+                                .addUnboundedFrom(6).subAggregation(avg("avg"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(range.getBuckets().size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("*-3.0"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(Double.NEGATIVE_INFINITY));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(3.0));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("3.0"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        Avg avg = bucket.getAggregations().get("avg");
+        assertThat(avg, notNullValue());
+        assertThat(avg.getValue(), equalTo(1.5)); // (1 + 2) / 2
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("3.0-6.0"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(3.0));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(6.0));
+        assertThat(bucket.getFromAsString(), equalTo("3.0"));
+        assertThat(bucket.getToAsString(), equalTo("6.0"));
+        assertThat(bucket.getDocCount(), equalTo(3L));
+        avg = bucket.getAggregations().get("avg");
+        assertThat(avg, notNullValue());
+        assertThat(avg.getValue(), equalTo(4.0)); // (3 + 4 + 5) / 3
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("6.0-*"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(6.0));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY));
+        assertThat(bucket.getFromAsString(), equalTo("6.0"));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(numDocs - 5L));
+        avg = bucket.getAggregations().get("avg");
+        assertThat(avg, notNullValue());
+        long total = 0;
+        for (int i = 5; i < numDocs; ++i) {
+            total += i + 1;
+        }
+        assertThat(avg.getValue(), equalTo((double) total / (numDocs - 5))); // (6 + 7 + 8 + 9 + 10) / 5
+    }
+
     public void testEmptyRange() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(range("range")
@@ -678,6 +857,66 @@ public class RangeTests extends ESIntegTestCase {
     r3: 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11
      */
 
+    public void testScriptMultiValuedWithAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .addAggregation(
+                        range("range").script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "'].values")).addUnboundedTo("r1", 3)
+                                .addRange("r2", 3, 6).addUnboundedFrom("r3", 6).subAggregation(sum("sum"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+
+        Range range = response.getAggregations().get("range");
+        assertThat(range, notNullValue());
+        assertThat(range.getName(), equalTo("range"));
+        List<? extends Bucket> buckets = range.getBuckets();
+        assertThat(range.getBuckets().size(), equalTo(3));
+
+        Range.Bucket bucket = buckets.get(0);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("r1"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(Double.NEGATIVE_INFINITY));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(3.0));
+        assertThat(bucket.getFromAsString(), nullValue());
+        assertThat(bucket.getToAsString(), equalTo("3.0"));
+        assertThat(bucket.getDocCount(), equalTo(2L));
+        Sum sum = bucket.getAggregations().get("sum");
+        assertThat(sum, notNullValue());
+        assertThat(sum.getName(), equalTo("sum"));
+        assertThat(sum.getValue(), equalTo((double) 1+2+2+3));
+
+        bucket = buckets.get(1);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("r2"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(3.0));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(6.0));
+        assertThat(bucket.getFromAsString(), equalTo("3.0"));
+        assertThat(bucket.getToAsString(), equalTo("6.0"));
+        assertThat(bucket.getDocCount(), equalTo(4L));
+        sum = bucket.getAggregations().get("sum");
+        assertThat(sum, notNullValue());
+        assertThat(sum.getName(), equalTo("sum"));
+        assertThat(sum.getValue(), equalTo((double) 2+3+3+4+4+5+5+6));
+
+        bucket = buckets.get(2);
+        assertThat(bucket, notNullValue());
+        assertThat((String) bucket.getKey(), equalTo("r3"));
+        assertThat(((Number) bucket.getFrom()).doubleValue(), equalTo(6.0));
+        assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY));
+        assertThat(bucket.getFromAsString(), equalTo("6.0"));
+        assertThat(bucket.getToAsString(), nullValue());
+        assertThat(bucket.getDocCount(), equalTo(numDocs - 4L));
+        sum = bucket.getAggregations().get("sum");
+        assertThat(sum, notNullValue());
+        assertThat(sum.getName(), equalTo("sum"));
+        long total = 0;
+        for (int i = 4; i < numDocs; ++i) {
+            total += (i + 1) + (i + 2);
+        }
+        assertThat(sum.getValue(), equalTo((double) total));
+    }
+
     public void testUnmapped() throws Exception {
         SearchResponse response = client().prepareSearch("idx_unmapped")
                 .addAggregation(range("range")
@@ -832,7 +1071,7 @@ public class RangeTests extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1L).minDocCount(0)
-                        .subAggregation(range("range").field(SINGLE_VALUED_FIELD_NAME).addRange("0-2", 0.0, 2.0)))
+                        .subAggregation(range("range").addRange("0-2", 0.0, 2.0)))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StatsTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StatsTests.java
index 4934d2a..2f53d5c 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StatsTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StatsTests.java
@@ -57,7 +57,7 @@ public class StatsTests extends AbstractNumericTestCase {
     public void testEmptyAggregation() throws Exception {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
-                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(stats("stats").field("value")))
+                .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0).subAggregation(stats("stats")))
                 .execute().actionGet();
 
         assertShardExecutionState(searchResponse, 0);
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StringTermsTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StringTermsTests.java
index edf8be4..1f43794 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StringTermsTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/StringTermsTests.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.messy.tests;
 
-import org.apache.lucene.util.automaton.RegExp;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
@@ -37,7 +36,6 @@ import org.elasticsearch.search.aggregations.bucket.filter.Filter;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.ExecutionMode;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.search.aggregations.metrics.stats.Stats;
 import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStats;
@@ -275,8 +273,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values())).includeExclude(new IncludeExclude("val00.+", null)))
-                .execute().actionGet();
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).include("val00.+")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -300,8 +297,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .includeExclude(new IncludeExclude("val00.+", "(val000|val001)")))
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).include("val00.+").exclude("(val000|val001)"))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -326,9 +322,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values()))
-                        .includeExclude(new IncludeExclude(null, new RegExp("val0[1-9]+.+"))))
-                .execute().actionGet();
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).exclude("val0[1-9]+.+")).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -353,8 +347,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values())).includeExclude(new IncludeExclude(incVals, null)))
-                .execute().actionGet();
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).include(incVals)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -381,8 +374,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values())).includeExclude(new IncludeExclude(incVals, excVals)))
-                .execute()
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).include(incVals).exclude(excVals)).execute()
                 .actionGet();
 
         assertSearchResponse(response);
@@ -405,8 +397,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .setTypes("high_card_type")
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
-                        .collectMode(randomFrom(SubAggCollectionMode.values())).includeExclude(new IncludeExclude(null, excVals)))
-                .execute().actionGet();
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).exclude(excVals)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -536,6 +527,33 @@ public class StringTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testSingleValuedFieldWithSubAggregation_Inherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .setTypes("type")
+                .addAggregation(
+                        terms("terms").executionHint(randomExecutionHint()).field(SINGLE_VALUED_FIELD_NAME)
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).subAggregation(count("count"))).execute()
+                .actionGet();
+
+        assertSearchResponse(response);
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("val" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("val" + i));
+            assertThat(bucket.getDocCount(), equalTo(1L));
+            ValueCount valueCount = bucket.getAggregations().get("count");
+            assertThat(valueCount, notNullValue());
+            assertThat(valueCount.getValue(), equalTo(1L));
+        }
+    }
+
     public void testSingleValuedFieldWithValueScript() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -676,6 +694,40 @@ public class StringTermsTests extends AbstractTermsTestCase {
      * doc_count: 1 - val_count: 2
      */
 
+    public void testMultiValuedFieldWithValueScriptWithInheritedSubAggregator() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .setTypes("type")
+                .addAggregation(
+                        terms("terms").executionHint(randomExecutionHint()).field(MULTI_VALUED_FIELD_NAME)
+                                .collectMode(randomFrom(SubAggCollectionMode.values())).script(new Script("'foo_' + _value"))
+                                .subAggregation(count("count"))).execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(6));
+
+        for (int i = 0; i < 6; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("foo_val" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("foo_val" + i));
+            if (i == 0 | i == 5) {
+                assertThat(bucket.getDocCount(), equalTo(1L));
+                ValueCount valueCount = bucket.getAggregations().get("count");
+                assertThat(valueCount, notNullValue());
+                assertThat(valueCount.getValue(), equalTo(2L));
+            } else {
+                assertThat(bucket.getDocCount(), equalTo(2L));
+                ValueCount valueCount = bucket.getAggregations().get("count");
+                assertThat(valueCount, notNullValue());
+                assertThat("term[" + key(bucket) + "]", valueCount.getValue(), equalTo(4L));
+            }
+        }
+    }
+
     public void testScriptSingleValue() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -722,6 +774,33 @@ public class StringTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testScriptSingleValueWithSubAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .setTypes("type")
+                .addAggregation(
+                        terms("terms").collectMode(randomFrom(SubAggCollectionMode.values())).executionHint(randomExecutionHint())
+                                .script(new Script("doc['" + SINGLE_VALUED_FIELD_NAME + "'].value")).subAggregation(count("count")))
+                .execute().actionGet();
+
+        assertSearchResponse(response);
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(5));
+
+        for (int i = 0; i < 5; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("val" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("val" + i));
+            assertThat(bucket.getDocCount(), equalTo(1L));
+            ValueCount valueCount = bucket.getAggregations().get("count");
+            assertThat(valueCount, notNullValue());
+            assertThat(valueCount.getValue(), equalTo(1L));
+        }
+    }
+
     public void testScriptMultiValued() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx")
@@ -749,6 +828,40 @@ public class StringTermsTests extends AbstractTermsTestCase {
         }
     }
 
+    public void testScriptMultiValuedWithAggregatorInherited() throws Exception {
+        SearchResponse response = client()
+                .prepareSearch("idx")
+                .setTypes("type")
+                .addAggregation(
+                        terms("terms").collectMode(randomFrom(SubAggCollectionMode.values())).executionHint(randomExecutionHint())
+                                .script(new Script("doc['" + MULTI_VALUED_FIELD_NAME + "']")).subAggregation(count("count"))).execute()
+                .actionGet();
+
+        assertSearchResponse(response);
+
+        Terms terms = response.getAggregations().get("terms");
+        assertThat(terms, notNullValue());
+        assertThat(terms.getName(), equalTo("terms"));
+        assertThat(terms.getBuckets().size(), equalTo(6));
+
+        for (int i = 0; i < 6; i++) {
+            Terms.Bucket bucket = terms.getBucketByKey("val" + i);
+            assertThat(bucket, notNullValue());
+            assertThat(key(bucket), equalTo("val" + i));
+            if (i == 0 | i == 5) {
+                assertThat(bucket.getDocCount(), equalTo(1L));
+                ValueCount valueCount = bucket.getAggregations().get("count");
+                assertThat(valueCount, notNullValue());
+                assertThat(valueCount.getValue(), equalTo(2L));
+            } else {
+                assertThat(bucket.getDocCount(), equalTo(2L));
+                ValueCount valueCount = bucket.getAggregations().get("count");
+                assertThat(valueCount, notNullValue());
+                assertThat(valueCount.getValue(), equalTo(4L));
+            }
+        }
+    }
+
     public void testUnmapped() throws Exception {
         SearchResponse response = client()
                 .prepareSearch("idx_unmapped")
@@ -794,7 +907,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .prepareSearch("idx")
                 .setTypes("type")
                 .addAggregation(
-                        filter("filter", termQuery(MULTI_VALUED_FIELD_NAME, "val3")).subAggregation(
+                        filter("filter").filter(termQuery(MULTI_VALUED_FIELD_NAME, "val3")).subAggregation(
                                 terms("terms").field(MULTI_VALUED_FIELD_NAME).collectMode(randomFrom(SubAggCollectionMode.values()))))
                 .execute().actionGet();
 
@@ -820,7 +933,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(
-                        histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1L).minDocCount(0).subAggregation(terms("terms").field("value")))
+                        histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(1L).minDocCount(0).subAggregation(terms("terms")))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -901,7 +1014,7 @@ public class StringTermsTests extends AbstractTermsTestCase {
                 .addAggregation(
                         terms("tags").executionHint(randomExecutionHint()).field("tag")
                                 .collectMode(randomFrom(SubAggCollectionMode.values())).order(Terms.Order.aggregation("filter", asc))
-                                .subAggregation(filter("filter", QueryBuilders.matchAllQuery()))).execute().actionGet();
+                                .subAggregation(filter("filter").filter(QueryBuilders.matchAllQuery()))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -941,8 +1054,8 @@ public class StringTermsTests extends AbstractTermsTestCase {
                                 .collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .order(Terms.Order.aggregation("filter1>filter2>stats.max", asc))
                                 .subAggregation(
-                                        filter("filter1", QueryBuilders.matchAllQuery()).subAggregation(
-                                                filter("filter2", QueryBuilders.matchAllQuery()).subAggregation(
+                                        filter("filter1").filter(QueryBuilders.matchAllQuery()).subAggregation(
+                                                filter("filter2").filter(QueryBuilders.matchAllQuery()).subAggregation(
                                                         stats("stats").field("i"))))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -1004,8 +1117,8 @@ public class StringTermsTests extends AbstractTermsTestCase {
                                 .collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .order(Terms.Order.aggregation("filter1>" + filter2Name + ">" + statsName + ".max", asc))
                                 .subAggregation(
-                                        filter("filter1", QueryBuilders.matchAllQuery()).subAggregation(
-                                                filter(filter2Name, QueryBuilders.matchAllQuery()).subAggregation(
+                                        filter("filter1").filter(QueryBuilders.matchAllQuery()).subAggregation(
+                                                filter(filter2Name).filter(QueryBuilders.matchAllQuery()).subAggregation(
                                                         stats(statsName).field("i"))))).execute().actionGet();
 
         assertSearchResponse(response);
@@ -1067,8 +1180,8 @@ public class StringTermsTests extends AbstractTermsTestCase {
                                 .collectMode(randomFrom(SubAggCollectionMode.values()))
                                 .order(Terms.Order.aggregation("filter1>" + filter2Name + ">" + statsName + "[max]", asc))
                                 .subAggregation(
-                                        filter("filter1", QueryBuilders.matchAllQuery()).subAggregation(
-                                                filter(filter2Name, QueryBuilders.matchAllQuery()).subAggregation(
+                                        filter("filter1").filter(QueryBuilders.matchAllQuery()).subAggregation(
+                                                filter(filter2Name).filter(QueryBuilders.matchAllQuery()).subAggregation(
                                                         stats(statsName).field("i"))))).execute().actionGet();
 
         assertSearchResponse(response);
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentileRanksTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentileRanksTests.java
index 540d0cf..0a2d103 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentileRanksTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentileRanksTests.java
@@ -31,7 +31,7 @@ import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Order;
 import org.elasticsearch.search.aggregations.metrics.AbstractNumericTestCase;
 import org.elasticsearch.search.aggregations.metrics.percentiles.Percentile;
 import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanks;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksBuilder;
 
 import java.util.Arrays;
 import java.util.Collection;
@@ -78,19 +78,19 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
             }
         }
         Arrays.sort(percents);
-        Loggers.getLogger(TDigestPercentileRanksTests.class).info("Using values={}", Arrays.toString(percents));
+        Loggers.getLogger(TDigestPercentileRanksTests.class).info("Using percentiles={}", Arrays.toString(percents));
         return percents;
     }
 
-    private static PercentileRanksAggregatorBuilder randomCompression(PercentileRanksAggregatorBuilder builder) {
+    private static PercentileRanksBuilder randomCompression(PercentileRanksBuilder builder) {
         if (randomBoolean()) {
             builder.compression(randomIntBetween(20, 120) + randomDouble());
         }
         return builder;
     }
 
-    private void assertConsistent(double[] pcts, PercentileRanks values, long minValue, long maxValue) {
-        final List<Percentile> percentileList = CollectionUtils.iterableAsArrayList(values);
+    private void assertConsistent(double[] pcts, PercentileRanks percentiles, long minValue, long maxValue) {
+        final List<Percentile> percentileList = CollectionUtils.iterableAsArrayList(percentiles);
         assertEquals(pcts.length, percentileList.size());
         for (int i = 0; i < pcts.length; ++i) {
             final Percentile percentile = percentileList.get(i);
@@ -116,8 +116,8 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(randomCompression(percentileRanks("percentile_ranks").field("value"))
-                                .values(10, 15)))
+                        .subAggregation(randomCompression(percentileRanks("percentile_ranks"))
+                                .percentiles(10, 15)))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L));
@@ -139,7 +139,7 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
                         .field("value")
-                        .values(0, 10, 15, 100))
+                        .percentiles(0, 10, 15, 100))
                 .execute().actionGet();
 
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(0L));
@@ -160,13 +160,13 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
                         .field("value")
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue);
     }
 
     @Override
@@ -177,7 +177,7 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         global("global").subAggregation(
-                                randomCompression(percentileRanks("percentile_ranks")).field("value").values(pcts))).execute()
+                                randomCompression(percentileRanks("percentile_ranks")).field("value").percentiles(pcts))).execute()
                 .actionGet();
 
         assertHitCount(searchResponse, 10);
@@ -189,10 +189,10 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         assertThat(global.getAggregations(), notNullValue());
         assertThat(global.getAggregations().asMap().size(), equalTo(1));
 
-        PercentileRanks values = global.getAggregations().get("percentile_ranks");
-        assertThat(values, notNullValue());
-        assertThat(values.getName(), equalTo("percentile_ranks"));
-        assertThat((PercentileRanks) global.getProperty("percentile_ranks"), sameInstance(values));
+        PercentileRanks percentiles = global.getAggregations().get("percentile_ranks");
+        assertThat(percentiles, notNullValue());
+        assertThat(percentiles.getName(), equalTo("percentile_ranks"));
+        assertThat((PercentileRanks) global.getProperty("percentile_ranks"), sameInstance(percentiles));
 
     }
 
@@ -202,13 +202,13 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
                         .field("value")
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue);
     }
 
     @Override
@@ -218,13 +218,13 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
                         .field("value")
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue);
     }
 
     @Override
@@ -233,14 +233,14 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .field("value").script(new Script("_value - 1"))
-                        .values(pcts))
+.field("value").script(new Script("_value - 1"))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1);
     }
 
     @Override
@@ -251,15 +251,15 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .field("value")
+.field("value")
                                 .script(new Script("_value - dec", ScriptType.INLINE, null, params))
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1);
     }
 
     @Override
@@ -269,13 +269,13 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
                         .field("values")
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues, maxValues);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues, maxValues);
     }
 
     @Override
@@ -284,14 +284,14 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .field("values").script(new Script("_value - 1"))
-                        .values(pcts))
+.field("values").script(new Script("_value - 1"))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1);
     }
 
     public void testMultiValuedFieldWithValueScriptReverse() throws Exception {
@@ -299,14 +299,14 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .field("values").script(new Script("_value * -1"))
-                        .values(pcts))
+.field("values").script(new Script("_value * -1"))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, -maxValues, -minValues);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, -maxValues, -minValues);
     }
 
     @Override
@@ -317,15 +317,15 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .field("values")
+.field("values")
                                 .script(new Script("_value - dec", ScriptType.INLINE, null, params))
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1);
     }
 
     @Override
@@ -334,14 +334,14 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .script(new Script("doc['value'].value"))
-                        .values(pcts))
+.script(new Script("doc['value'].value"))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue, maxValue);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue, maxValue);
     }
 
     @Override
@@ -352,15 +352,15 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .script(
+.script(
                                 new Script("doc['value'].value - dec", ScriptType.INLINE, null, params))
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValue - 1, maxValue - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValue - 1, maxValue - 1);
     }
 
     @Override
@@ -369,14 +369,14 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(randomCompression(percentileRanks("percentile_ranks"))
-                        .script(new Script("doc['values'].values"))
-                        .values(pcts))
+.script(new Script("doc['values'].values"))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues, maxValues);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues, maxValues);
     }
 
     @Override
@@ -390,13 +390,13 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                                 .script(new Script(
                                         "List values = doc['values'].values; double[] res = new double[values.size()]; for (int i = 0; i < res.length; i++) { res[i] = values.get(i) - dec; }; return res;",
                                         ScriptType.INLINE, null, params))
-                        .values(pcts))
+                        .percentiles(pcts))
                 .execute().actionGet();
 
         assertHitCount(searchResponse, 10);
 
-        final PercentileRanks values = searchResponse.getAggregations().get("percentile_ranks");
-        assertConsistent(pcts, values, minValues - 1, maxValues - 1);
+        final PercentileRanks percentiles = searchResponse.getAggregations().get("percentile_ranks");
+        assertConsistent(pcts, percentiles, minValues - 1, maxValues - 1);
     }
 
     public void testOrderBySubAggregation() {
@@ -405,7 +405,7 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field("value").interval(2L)
-                            .subAggregation(randomCompression(percentileRanks("percentile_ranks").field("value").values(99)))
+                            .subAggregation(randomCompression(percentileRanks("percentile_ranks").percentiles(99)))
                             .order(Order.aggregation("percentile_ranks", "99", asc)))
                 .execute().actionGet();
 
@@ -414,8 +414,8 @@ public class TDigestPercentileRanksTests extends AbstractNumericTestCase {
         Histogram histo = searchResponse.getAggregations().get("histo");
         double previous = asc ? Double.NEGATIVE_INFINITY : Double.POSITIVE_INFINITY;
         for (Histogram.Bucket bucket : histo.getBuckets()) {
-            PercentileRanks values = bucket.getAggregations().get("percentile_ranks");
-            double p99 = values.percent(99);
+            PercentileRanks percentiles = bucket.getAggregations().get("percentile_ranks");
+            double p99 = percentiles.percent(99);
             if (asc) {
                 assertThat(p99, greaterThanOrEqualTo(previous));
             } else {
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentilesTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentilesTests.java
index 1bf0b00..0fa5f66 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentilesTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentilesTests.java
@@ -31,7 +31,8 @@ import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Order;
 import org.elasticsearch.search.aggregations.metrics.AbstractNumericTestCase;
 import org.elasticsearch.search.aggregations.metrics.percentiles.Percentile;
 import org.elasticsearch.search.aggregations.metrics.percentiles.Percentiles;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesAggregatorBuilder;
+import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesBuilder;
+
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
@@ -80,7 +81,7 @@ public class TDigestPercentilesTests extends AbstractNumericTestCase {
         return percentiles;
     }
 
-    private static PercentilesAggregatorBuilder randomCompression(PercentilesAggregatorBuilder builder) {
+    private static PercentilesBuilder randomCompression(PercentilesBuilder builder) {
         if (randomBoolean()) {
             builder.compression(randomIntBetween(20, 120) + randomDouble());
         }
@@ -115,7 +116,7 @@ public class TDigestPercentilesTests extends AbstractNumericTestCase {
         SearchResponse searchResponse = client().prepareSearch("empty_bucket_idx")
                 .setQuery(matchAllQuery())
                 .addAggregation(histogram("histo").field("value").interval(1L).minDocCount(0)
-                        .subAggregation(randomCompression(percentiles("percentiles").field("value"))
+                        .subAggregation(randomCompression(percentiles("percentiles"))
                                 .percentiles(10, 15)))
                 .execute().actionGet();
 
@@ -388,7 +389,7 @@ public class TDigestPercentilesTests extends AbstractNumericTestCase {
                 .setQuery(matchAllQuery())
                 .addAggregation(
                         histogram("histo").field("value").interval(2L)
-                        .subAggregation(randomCompression(percentiles("percentiles").field("value").percentiles(99)))
+                            .subAggregation(randomCompression(percentiles("percentiles").percentiles(99)))
                             .order(Order.aggregation("percentiles", "99", asc)))
                 .execute().actionGet();
 
diff --git a/plugins/mapper-murmur3/src/test/java/org/elasticsearch/index/mapper/murmur3/Murmur3FieldMapperTests.java b/plugins/mapper-murmur3/src/test/java/org/elasticsearch/index/mapper/murmur3/Murmur3FieldMapperTests.java
index 1da604c..072c0db 100644
--- a/plugins/mapper-murmur3/src/test/java/org/elasticsearch/index/mapper/murmur3/Murmur3FieldMapperTests.java
+++ b/plugins/mapper-murmur3/src/test/java/org/elasticsearch/index/mapper/murmur3/Murmur3FieldMapperTests.java
@@ -60,7 +60,7 @@ public class Murmur3FieldMapperTests extends ESSingleNodeTestCase {
                 Collections.singletonMap(Murmur3FieldMapper.CONTENT_TYPE, new Murmur3FieldMapper.TypeParser()),
                 Collections.emptyMap());
         parser = new DocumentMapperParser(indexService.getIndexSettings(), indexService.mapperService(),
-        indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::getQueryShardContext);
+        indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::newQueryShardContext);
     }
 
     public void testDefaults() throws Exception {
@@ -136,7 +136,7 @@ public class Murmur3FieldMapperTests extends ESSingleNodeTestCase {
         Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_4_2.id).build();
         indexService = createIndex("test_bwc", settings);
         parser = new DocumentMapperParser(indexService.getIndexSettings(), indexService.mapperService(),
-                indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::getQueryShardContext);
+                indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::newQueryShardContext);
         String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
             .startObject("properties").startObject("field")
                 .field("type", "murmur3")
@@ -152,7 +152,7 @@ public class Murmur3FieldMapperTests extends ESSingleNodeTestCase {
         Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_4_2.id).build();
         indexService = createIndex("test_bwc", settings);
         parser = new DocumentMapperParser(indexService.getIndexSettings(), indexService.mapperService(),
-        indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::getQueryShardContext);
+        indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::newQueryShardContext);
         String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
             .startObject("properties").startObject("field")
             .field("type", "murmur3")
diff --git a/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java b/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java
index a44dddd..d6b64df 100644
--- a/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java
+++ b/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java
@@ -66,7 +66,7 @@ public class SizeMappingTests extends ESSingleNodeTestCase {
         Map<String, MetadataFieldMapper.TypeParser> metadataMappers = new HashMap<>();
         IndicesModule indices = new IndicesModule();
         indices.registerMetadataMapper(SizeFieldMapper.NAME, new SizeFieldMapper.TypeParser());
-        mapperService = new MapperService(indexService.getIndexSettings(), indexService.analysisService(), indexService.similarityService(), indices.getMapperRegistry(), indexService::getQueryShardContext);
+        mapperService = new MapperService(indexService.getIndexSettings(), indexService.analysisService(), indexService.similarityService(), indices.getMapperRegistry(), indexService::newQueryShardContext);
         parser = mapperService.documentMapperParser();
     }
 
@@ -98,7 +98,7 @@ public class SizeMappingTests extends ESSingleNodeTestCase {
                 Collections.emptyMap(),
                 Collections.singletonMap(SizeFieldMapper.NAME, new SizeFieldMapper.TypeParser()));
         parser = new DocumentMapperParser(indexService.getIndexSettings(), mapperService,
-                indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::getQueryShardContext);
+                indexService.analysisService(), indexService.similarityService(), mapperRegistry, indexService::newQueryShardContext);
         DocumentMapper docMapper = parser.parse("type", new CompressedXContent(mapping));
 
         BytesReference source = XContentFactory.jsonBuilder()
diff --git a/test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java b/test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java
index d24baeb..bb03cf8 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java
@@ -515,10 +515,10 @@ public abstract class ESIntegTestCase extends ESTestCase {
 
     private static Settings.Builder setRandomIndexTranslogSettings(Random random, Settings.Builder builder) {
         if (random.nextBoolean()) {
-            builder.put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey(), new ByteSizeValue(RandomInts.randomIntBetween(random, 1, 300), ByteSizeUnit.MB));
+            builder.put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(RandomInts.randomIntBetween(random, 1, 300), ByteSizeUnit.MB));
         }
         if (random.nextBoolean()) {
-            builder.put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)); // just don't flush
+            builder.put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB)); // just don't flush
         }
         if (random.nextBoolean()) {
             builder.put(IndexSettings.INDEX_TRANSLOG_DURABILITY_SETTING.getKey(), RandomPicks.randomFrom(random, Translog.Durability.values()));
diff --git a/test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java b/test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java
index 1f9c420..303203c 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java
@@ -18,13 +18,6 @@
  */
 package org.elasticsearch.test;
 
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.FieldDoc;
 import org.apache.lucene.search.Query;
@@ -33,7 +26,6 @@ import org.apache.lucene.util.Counter;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.analysis.AnalysisService;
@@ -44,17 +36,16 @@ import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.ParsedQuery;
+import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
 import org.elasticsearch.search.fetch.script.ScriptFieldsContext;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 import org.elasticsearch.search.highlight.SearchContextHighlight;
@@ -69,7 +60,9 @@ import org.elasticsearch.search.rescore.RescoreSearchContext;
 import org.elasticsearch.search.suggest.SuggestionSearchContext;
 import org.elasticsearch.threadpool.ThreadPool;
 
-import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
 
 public class TestSearchContext extends SearchContext {
 
@@ -83,6 +76,7 @@ public class TestSearchContext extends SearchContext {
     final IndexShard indexShard;
     final Counter timeEstimateCounter = Counter.newCounter();
     final QuerySearchResult queryResult = new QuerySearchResult();
+    final QueryShardContext queryShardContext;
     ScriptService scriptService;
     ParsedQuery originalQuery;
     ParsedQuery postFilter;
@@ -92,7 +86,6 @@ public class TestSearchContext extends SearchContext {
     ContextIndexSearcher searcher;
     int size;
     private int terminateAfter = DEFAULT_TERMINATE_AFTER;
-    private String[] types;
     private SearchContextAggregations aggregations;
 
     private final long originNanoTime = System.nanoTime();
@@ -108,9 +101,10 @@ public class TestSearchContext extends SearchContext {
         this.threadPool = threadPool;
         this.indexShard = indexService.getShardOrNull(0);
         this.scriptService = scriptService;
+        queryShardContext = indexService.newQueryShardContext();
     }
 
-    public TestSearchContext() {
+    public TestSearchContext(QueryShardContext queryShardContext) {
         super(ParseFieldMatcher.STRICT);
         this.pageCacheRecycler = null;
         this.bigArrays = null;
@@ -120,10 +114,7 @@ public class TestSearchContext extends SearchContext {
         this.fixedBitSetFilterCache = null;
         this.indexShard = null;
         scriptService = null;
-    }
-
-    public void setTypes(String... types) {
-        this.types = types;
+        this.queryShardContext = queryShardContext;
     }
 
     @Override
@@ -171,16 +162,6 @@ public class TestSearchContext extends SearchContext {
     }
 
     @Override
-    public boolean hasTypes() {
-        return false;
-    }
-
-    @Override
-    public String[] types() {
-        return new String[0];
-    }
-
-    @Override
     public float queryBoost() {
         return 0;
     }
@@ -559,10 +540,6 @@ public class TestSearchContext extends SearchContext {
         return null;
     }
 
-    @Override
-    public FetchPhase fetchPhase() {
-        return null;
-    }
 
     @Override
     public MappedFieldType smartNameFieldType(String name) {
@@ -597,4 +574,9 @@ public class TestSearchContext extends SearchContext {
     @Override
     public Map<Class<?>, Collector> queryCollectors() {return queryCollectors;}
 
+    @Override
+    public QueryShardContext getQueryShardContext() {
+        return queryShardContext;
+    }
+
 }
